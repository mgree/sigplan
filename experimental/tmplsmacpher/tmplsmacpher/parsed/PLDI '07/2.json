{"article_publication_date": "06-10-2007", "fulltext": "\n Automatically Classifying Benign and Harmful Data Races Using Replay Analysis Satish Narayanasamy \n, Zhenghao Wang , Jordan Tigani , Andrew Edwards , Brad Calder Microsoft University of California, San \nDiego Abstract Many concurrency bugs in multi-threaded programs are due to data races. There have been \nmany efforts to develop static and dynamic mechanisms to automatically .nd the data races. Most of the \nprior work has focused on .nding the data races and eliminating the false positives. In this paper, we \ninstead focus on a dynamic analysis technique to automatically classify the data races into two categories \n the data races that are potentially benign and the data races that are po\u00adtentially harmful. A harmful \ndata race is a real bug that needs to be .xed. This classi.cation is needed to focus the triaging effort \non those data races that are potentially harmful. Without prioritiz\u00ading the data races we have found \nthat there are too many data races to triage. Our second focus is to automatically provide to the de\u00adveloper \na reproducible scenario of the data race, which allows the developer to understand the different effects \nof a harmful data race on a program s execution. To achieve the above, we record a multi-threaded program \ns execution in a replay log. The replay log is used to replay the multi\u00adthreaded program, and during \nreplay we .nd the data races using a happens-before based algorithm. To automatically classify if a data \nrace that we .nd is potentially benign or potentially harmful, we replay the execution twice for a given \ndata race one for each possible order between the con.icting memory operations. If the two replays for \nthe two orders produce the same result, then we classify the data race to be potentially benign. We discuss \nour experiences in using our replay based dynamic data race checker on several Microsoft applications. \nCategories and Subject Descriptors D. Software [D.2 Software Engineering]: D.2.5 Testing and Debugging \n Debugging aids General Terms Algorithms, Experimentation, Reliability, Veri.\u00adcation Keywords Benign \nData Races, Replay, Concurrency Bugs 1. Introduction Automatically detecting data races is a very hard \nproblem. Data race detection tools, even the dynamic analysis tools, tend to report a large number of \ndata races. But only a handful of them are harmful. A harmful data race is one that is a source of a \nconcurrency Permission to make digital or hard copies of all or part of this work for personal or classroom \nuse is granted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. PLDI \n07 June 11 13, 2007, San Diego, California, USA. Copyright c . 2007 ACM 978-1-59593-633-2/07/0006. . \n. $5.00 bug, which can affect the correctness of a program s execution. A developer will consider .xing \nonly the harmful data races. Ideally, an automatic race detection tool should report only the harmful \ndata races to a developer. However, many existing tools report data races that can never occur at all. \nSuch data races are the false positives. Even if we manage to eliminate all the false positives, not \nall of the remaining true data races are harmful. In fact, only a small fraction of the true data races \nare actually harmful. In our work, we found that only 10% of the true data races are harmful. The remaining \n90% were all benign data races -the data races that do not compromise program s correctness. Thus, the \nset of true data races is still too large for developers to manually examine and triage. The .rst goal \nof our paper is to .nd and report only the poten\u00adtially harmful data races, in order to improve programmer \nproduc\u00adtivity. To achieve this, we propose a mechanism to automatically classify the true data races \ninto potentially benign and potentially harmful. This allows the developers to prioritize the data races \nthat need be triaged. We .nd that reporting accurate information about the potentially harmful data races \nis very important because triag\u00ading a data race bug is a tedious exercise. Triaging a data race bug is \ndif.cult for the following reasons: Requires Domain Expertise: Effects of a data race are hard to understand, \nas it involves analyzing multiple program states across multiple threads. Domain expertise is usually \nrequired to understand if a data race is benign or harmful in our production code.  Time Consuming: \nThe number of true data races is too large for the developers to go through and triage them all. It can \nbe extremely time consuming for a developer to triage a data race, and those with the domain expertise \nare often very busy.  Hard to Figure Out: Even if someone with domain expertise examines the data race, \nthey tend to incorrectly believe it is benign when it is actually harmful, or vice versa.  Besides \n.nding the harmful data races accurately, another prob\u00adlem is generating information that will help convince \nthe developer about the existence of a data race bug. Therefore, our second goal is to generate a concrete \nreproducible scenario for a potentially harm\u00adful data race. The reproducible scenario helps the programmer \nin debugging and understanding the harmful effects of the data race reported. In this paper, we take \nimportant steps towards meeting both of the above two goals. We have developed a dynamic data race detection \ntool that can automatically classify the data races into potentially benign and potentially harmful categories. \nAn integral part of our solution is the ability to record a program s execution in a replay log and replay \nthe program s execution using the log. The proposed dynamic data race detection analysis is performed \nof.ine, during replay. In addition to .nding the potentially harmful data races, the analysis also produces \nuseful information for each data race. Using that information, a developer can replay a program execution \nin two different ways the original execution order and an alternative order, which has the order of \nthe two memory operations involved in the data race reversed. The two replays help the developer understand \nhow a potentially harmful data race can produce different results based on the different interleavings \nbetween the two racing memory operations. Data Race Detection: We focus on building a race detection \ntool with no false positives. Our data race detection algorithm is based on the happens-before relationship \n[17]. We determine that there is a data race between two memory operations executed in two different \nthreads if (a) at least one of them is a write, and (b) there is no synchronization operation executed \nbetween the two memory operations (that is, there is no happens-before relation to provide an order for \nthe two operations). Going by this strict de.nition of what a data race is, a happens-before based algorithm \ndoes not report any false positives. However, a happens-before algorithm still reports a large number \nof data races, out of which many are benign. In order for our tool to be used in practice, we need to \nprioritize the data races so that a developer can focus on .xing and understanding the potentially harmful \ndata race bugs. Data Race Classi.cation: Our approach automatically clas\u00adsi.es data races by leveraging \nthe ability to replay the program s execution. The key concept behind our analysis is as follows. For \na data race, the checker analysis tool replays the execution twice for the two different orders between \nthe memory operations involved in the data race. If the two replays produce the same result, then the \nchecker determines that the data race is potentially benign. Oth\u00aderwise, it classi.es the data race as \npotentially harmful. The data races that our tool marks as potentially benign are not examined by the \ndevelopers, but only those marked as potentially harmful are examined. We keep track of the results of \nthis analysis for each pair of memory operations involved in a data race. There can be many instances \nof the same data race during a program s execution and across several different executions. By analyzing \nthese instances we can observe several effects of the data race. Thereby, we get a much clearer picture \nabout how to classify the data race. Data Race Report: At the end of our analysis, we provide the developer \nwith precise information about the effects of each potentially harmful data race. The information includes \nthe replay log and the two memory orders that were analyzed by the checker for the data race. One of \nthe replays will produce the correct result and the other will produce a different result. Thus, a developer \nhas the precise information about the memory operations involved in the data race, and also has the ability \nto replay the program in two different ways (two ways are the original execution order and the alternative \norder that is possible due to the data race) and understand the effects of the data race. If the same \ndata race had occurred multiple times within the same or different execution scenario, we provide information \nfor all of those instances to help the developer understand the various possible effects of a particular \ndata race. iDNA and Usage Model: We perform our dynamic data race analysis using the reply tool called \niDNA developed by Bhansali et al. [3]. iDNA provides the ability to record a program s execution in a \nreplay log. Using the replay log, iDNA can replay a multi\u00adthreaded program s execution, even in the presence \nof all forms of non-determinism, including system interactions (system calls, interrupts, DMAs etc.) \nand multi-processor interactions. We extend iDNA to provide the ability to replay with two different \nthread interleavings for a data race, and provide the ability to examine the results of both orderings \nto see if they result in the same execution. In using our approach in a development environment, iDNA \nis .rst used to gather the replay logs for the product s test scenarios, with an overhead of about 10x \non average [3]. We then run our off-line replay analysis to .nd all of the data races, and the off\u00adline \nanalysis classi.es the data races into potentially benign or potentially harmful. For the potentially \nharmful ones, we provide at least two replay scenarios that will show how the data race can result in \ntwo different outcomes. This information coupled with the ability to do reverse execution (also called \ntime travel debugging) using iDNA [3] for the replays, provides a powerful platform for the developers \nto examine the potentially harmful data races. In striving to achieve the above two goals, our approach \ncan have incorrect classi.cations. We may classify some data races as potentially benign when they are \nharmful or vice versa. If we classify a benign data race as potentially harmful, then we end up using \nprecious developer s time. But once those races are manually identi.ed as benign, they are marked as \nbenign to prevent them from being classi.ed as potentially harmful in the future analysis. If we classify \na harmful data race as benign, they will not be examined by the developer. However, later on, when analyzing \na different test case, the analysis may .nd an instance of the data race that exposes it as potentially \nharmful. The data race will then be re-classi.ed and reported to the developer. Thus, the more the number \nof test cases analyzed, the more likely harmful data races will be discovered (which is true for any \ndynamic analysis tool). This is a trade-off between coverage and accuracy that we make during development, \nbecause there are too many true data races found, and most of them are benign. To summarize, this paper \nmakes the following contributions: We present a happens-before based dynamic analysis algorithm to .nd \na set of data races using the iDNA replay framework. The happens-before based analysis does not report \nfalse positives. However, the analysis still reports a lot of data races, most of which are benign. \n To our knowledge, we are the .rst to focus on the problem of automatically classifying the true data \nraces into potentially benign and potentially harmful. This classi.cation directs the focus of the developer \non triaging the potentially harmful data races. The key idea is that for a data race to be harmful, the \ntwo different possible memory orders for the data race should produce different results. We describe \nhow we extended the iDNA framework to replay the alternative memory orderings, and to determine if they \nresult in the same execution or not. Then for each potentially harmful data race, we provide a replay \nscenario that allows the developer to understand the effects of the data race.  We discuss our experiences \nin using our dynamic race classi\u00ad.cation approach on an extensively stress-tested build of Mi\u00adcrosoft \ns Windows Vista and Internet Explorer. Our proposed technique was able to automatically .lter out over \nhalf of the real benign data races, classifying them as potentially benign, which can be ignored by the \ndevelopers. In addition all of the harmful data races were correctly classi.ed as potentially harm\u00adful, \nwhich were reported to the developers and they all have been .xed in the production code.  2. Prior \nWork The focus of this paper is to accurately classify data races in parallel applications based on the \nshared memory programming model. Prior work on data race detection can be broken into solutions based \non static analysis and solutions based on dynamic analysis. 2.1 Static Analysis Data races can be found \nusing type-based static analysis tech\u00adniques [4, 14]. A type-based technique requires the programmer \nto specify the type of the synchronization operations [4, 14]. Au\u00adtomatically inferring information about \nthe synchronization oper\u00adations is dif.cult and there are some techniques that address this problem [32]. \nStatic analysis can also be done using model check\u00ading techniques like BLAST [15] and KISS [28]. Model \nchecking techniques can handle various synchronization idioms and can also produce counterexamples. The \nlimitation of the model checking techniques is that the analysis algorithm does not scale well for large \nprograms, which can limit their use. There are techniques that statically implement a lockset [33] based \nalgorithm [35, 13, 26]. Naik et al. [20] recently proposed an analysis method that consists of a set \nof techniques that are applied in series like reachability and alias analysis to reduce the number of \nfalse data races. The primary limitation of the static analysis techniques is their accuracy in terms \nof the number of false positives reported. Also, an even bigger problem (which is true even for existing \ndynamic anal\u00adysis techniques) is that, among the true data races reported, a large proportion of them \nare benign data races. Benign data races are very hard to distinguish from the harmful data races during \nstatic analysis. For example, in one of the very recent proposals [20], for one program jdbm, the analysis \nreturned 91 data races (not false positives but real data races), but only 2 of them were found to be \nharmful. Static analysis techniques address this problem with manual annotations, but they require the \nprogrammer to get the an\u00adnotation right, and there is a signi.cant amount of existing code existing without \nannotations. We focus on a dynamic analysis technique, since it can signi.\u00adcantly reduce the number of \ncandidate data races that need to be ex\u00adamined. The trade-off of course is that, the coverage will be \nlower than the static techniques. Also, we can generate different replay scenarios for a data race found \nduring a dynamic analysis, which the user can use to understand the possible effects of the data race \non a program s execution.  2.2 Dynamic Analysis Dynamic analysis can be done either on-line or off-line. \nWe .rst ex\u00adamine the trade-offs between the two approaches. Then we describe the dynamic race detection \ntechniques in more detail and place our work in context. 2.2.1 When the Analysis is Performed: On-line \nVersus Off-line Analysis A program s execution can be analyzed on-line when the program is executing \nto detect the data races. This approach incurs runtime overhead, and hence the dynamic analysis needs \nto be ef.cient in terms of performance. A majority of prior dynamic race detection techniques have focused \non detecting data races on-line, either with instrumentation support [33] and with hardware support [1, \n29]. There have also been attempts to ameliorate the performance cost of dynamic analysis using static \noptimizations [11, 38, 23, 25]. Alternatively, if we can ef.ciently record suf.cient information about \na program s execution to allow us to deterministically replay the execution, then we can do off-line \nanalysis. The advantage of off-line analysis over on-line analysis is that the analysis itself does not \nhave to be as performance ef.cient as it is has to be for on\u00adline analysis. Only the recording part needs \nto be ef.cient. We can perform (many) sophisticated time consuming dynamic analysis over a recorded program \ns execution off-line. Also, the result of the analysis can enable the developer to examine the source \nof the data race by replaying the program. There have been a few techniques that looked at doing off-line \nanalysis [7, 30] to detect data races. However, both RaceFrontier [7] and RecPlay [30] do not attempt \nto record the non-deterministic in\u00adteractions between the threads. As a result, they are limited in their \nanalysis in that they are able to detect only the .rst data race in the recorded program execution. In \ncontrast, we use the iDNA [3] infrastructure, which enables us to replay multi-threaded programs across \nall forms of non-determinism, including non-deterministic shared memory multiprocessor interactions. \nThis allows us to ex\u00adamine all the data races in the recorded program execution.  2.2.2 How the Analysis \nis Performed: Happens-Before Versus Lockset Dynamic race detection algorithms can be broadly classi.ed \ninto happens-before based algorithms [17, 21, 1, 6, 10, 8, 34, 24, 31, 19], lockset based algorithms \n[33, 36, 22, 2] and hybrid algorithms that combine the two [11, 38, 23, 25]. One class of data race detectors \nuse the lockset algorithm. The lockset algorithm checks whether each shared variable in a program is \nconsistently guarded by at least one lock. Eraser [33] implements the lockset algorithm using instrumentation \nto dynamically .nd the data races during a program s execution. This algorithm has been extended to object-oriented \nlanguages [25] and improved for precision and performance [2, 22, 36, 5]. The lockset algorithm is essentially \na heuristics based algorithm and hence reports data races that can never occur at all (that is, it can \nreport false positives). A recent work [18] reports that a lockset algorithm resulted in thousands of \nfalse positives for scienti.c applications. There are race detectors that use the happens-before algorithm. \nThe happens-before algorithm checks whether con.icting accesses to shared variables in a program are \nordered by an explicit synchro\u00adnization operation or not. Many dynamic race detectors implement the happens-before \nalgorithm in software [31] . Hardware [19, 27] and Distributed-Shared-Memory [24, 29] implementations \nwere also proposed to reduce the runtime overhead of these detectors. A recent hardware based proposal \ncalled ReEnact [27] detects data races using happens-before relation on-the-.y. Upon detection of a data \nrace, it can rollback to a previous checkpoint and replay the execution. During replay, it tries to avoid \nthe data race detected in the previous execution. The advantage of using a happens-before algorithm is \nthat it can detect the data races with no false positives because the analysis is based on whether there \nare two unordered con.icting memory operations or not. However, the resulting cov\u00aderage can be less than \nthe lockset algorithm. It is also possible to combine these two algorithms [11, 38, 23, 25] to get coverage \nclose to a lockset algorithm, and at the same time reduce false positives using happens-before relations. \nThese prior dynamic data race detectors have not yet focused on classifying real data races as potentially \nbenign versus harmful data races. For example, RaceTrack [38] found 48 warnings in CLR regression test \nsuite out of which there were 8 false positives. But more importantly, 32 were benign data races and \nonly 8 were found to be harmful during manual inspection. Distinguishing between the benign and the harmful \ndata races is a hard problem. To our knowledge no prior work has attempted to automatically identify \nthe potentially benign data races, which is the focus of our work. If can do that, then we can direct \nthe developers s effort towards triaging the potentially harmful data races. The focus in building our \ntool was to provide as much accurate information as possible. That is why we chose to use a happens\u00adbefore \nbased data race detection algorithm, since it does not report any false positive. Neverthless, our analysis \ncan also be used for analyzing the data races reported by a lockset based algorithm and its variations. \nThe analysis should be able to .lter out the benign data races and also the false positives produced \nby those algorithms.  2.3 Atomicity Violation Detection There have also been work on .nding concurrency \nbugs by check\u00ading for atomicity violations [2, 12, 37, 18]. If we can know which regions of code need \nto be executed atomically, then we can ver\u00adify the atomicity properties either statically [2] or dynamically \n[12, 37, 18]. Also, there has been work on inferring the set of locks that need to be acquired to enforce \nthe atomicity speci.ed by the pro\u00adgrammer [16]. Any violation of atomicity is a source of a bug, but \nevery data race is not necessarily harmful. So checking for atom\u00adicity violations is more effective than \n.nding data races. However, determining the atomic regions in itself is a signi.cant challenge. Many \ntechniques require the programmer to explicitly specify the atomic regions through annotations [2, 12, \n16]. In SVD [37] and AVIO [18], the authors used heuristics to infer the atomic regions automatically. \nThese methods are heuristic based, and as a result they report a high number of false positives when \na code region is incorrectly determined to be atomic. 3. Finding Happens-before Replay Data Races In \nthis section, we provide a brief overview of the iDNA [3] record and replay mechanism. We then discuss \na happens-before based data race detection algorithm that we implemented by extending the iDNA replayer. \nOur happens-before based data race detector does not report false positives. In other words, if our detector \n.nds a data race in a program s execution, then it guarantees that there is at least one instance of \nthe data race in the execution. 3.1 iDNA Recorder iDNA [3] provides the ability to record a multi-threaded \nprogram s execution in a replay log, which can be used to replay the execution. Here we will brie.y discuss \nhow iDNA works, but more details can be found in [3]. iDNA uses a load-based checkpointing scheme to \nrecord a pro\u00adgram s execution. Let us .rst just consider a single threaded ap\u00adplication. At the beginning \nof a checkpoint, iDNA records the ar\u00adchitectural state consisting of the values in the registers and \nthe program counter. And then during the program s execution, iDNA dynamically instruments the load instructions \nand records their val\u00adues. The log size generated is reduced using a compression mecha\u00adnism [3]. Recording \nthe values of load instructions executed by a program automatically takes care of all forms of non-determinism, \ninclud\u00ading system interactions (system calls, interrupts, DMAs) and multi\u00adthreaded interactions (even \nwhen multiple threads are executing on multiple processors). For example, if a system call or an interrupt \nmodi.es a memory location, the program needs to load the value from the memory location before it can \nuse the value. Therefore, recording the values of the load instructions is suf.cient to cap\u00adture the \nsystem interactions. Even DMAs that concurrently modify the program s memory state can be taken care \nof by logging the load values. Also, in the case of multi-threaded programs, multiple threads can be \nconcurrently modifying a shared memory location, but as long as we record the load values for a particular \nthread, we can replay that thread. Note, iDNA does not log every load value. It records only the load \nthat accesses a memory location for the .rst time. In addition, if a memory value is modi.ed by the external \nsystem (DMA, system call, another thread, etc.) outside of the thread, then the value of the subsequent \nload to that location is logged. iDNA also correctly deals with the dynamically loaded libraries and \nself\u00admodifying code. A detailed explanation on how all this is done ef.ciently can be found in [3]. \n 3.2 Sequencers for Multi-Threaded Programs In the case of multi-threaded programs, a replay log is recorded \nfor each thread individually. As we described above, the replay log for a thread contains the initial \narchitectural state of the thread and all the load values that are necessary to replay that thread correctly. \nEven if other threads are concurrently modifying the shared mem\u00adory locations, it does not affect how \na thread is replayed. Because, iDNA logs the values of all the required load instructions in the re\u00adplay \nlog. Thus, using the replay log of a thread, we can replay the thread exactly how it was executed during \nthe original execution. However, to aid debugging and to enable multi-threaded pro\u00adgram analysis, we \nwant the ability to deterministically replay the thread interactions observed during the original execution. \niDNA provides this functionality by recording what are called Se\u00adquencers. A sequencer log consists of \na global time-stamp value that is maintained by iDNA (one global time-stamp counter main\u00adtained across \nall the threads). The global time-stamp is incremented whenever a sequencer is logged in the replay log \nof any thread. A sequencer is recorded when a synchronization instruction or a system call is executed. \niDNA dynamically instruments the instructions with the lock pre.x to recognize the synchronization operations. \nWhenever a synchronization operation is executed by a thread, a sequencer is logged. Since each sequencer \nconsists of a time-stamp that is incremented monotonically, there exists a total order between all the \nsequencers recorded across all the threads. Figure 1 shows an example for how sequencers are recorded \nin the replay log of each thread. The sequencers are labeled as S1, S2, etc. For the example, assume \nthat Si > Sj if i>j. With this log, we can determine that all the memory operations that were executed \nbefore the sequencer S1 in the thread T2, should have been executed before all the memory operations \nthat were executed after the sequencer S3 in the thread T1 (because time-stamp for S3 is greater than \nS1). 3.3 iDNA Replayer To replay a thread using a replay log, .rst the architectural state of a thread \ncomprising of the registers and the program counter are initialized with the information read from the \nlog. iDNA records both the code and the data that is read by a thread. So during replay, the execution \nstarts from the instruction pointed to by the program counter. The load instructions are then dynamically \ninstrumented so that the iDNA replayer can make sure that replayed loads return the correct values as \nrecorded in the replay log. In the case of multi-threaded programs, one sequencing re\u00adgion is replayed \nat a time. A sequencing region consists of the sequence of instructions executed between two consecutive \nse\u00adquencers logged in an iDNA log for a thread. For the example in Figure 1, the instructions executed \nbetween S3-S5 constitute a se\u00adquencing region. One sequencing region is replayed at a time and it is \nchosen from one of the thread as follows. The sequencing region that has the smallest starting sequencers \namong all the sequencing regions that are yet to be replayed is chosen for replay. For exam\u00adple, in the \nFigure 1, the sequencing region S1 - S4 is replayed before S2 - S6. After replaying S2 - S6, the region \nS3 - S5 is replayed and so on. 3.4 Finding Happens-Before Data Races Using iDNA, we replay a multi-threaded \nprogram s execution us\u00ading the above sequencers. During replay, we modi.ed iDNA to an\u00adalyze the program \ns execution to .nd data races between sequence regions. To .nd the data races, we use the sequencers \nrecorded in the iDNA traces. Using the sequencers, we can determine the overlap\u00adping sequencing regions \nacross different threads in a multi-threaded program execution. For example, in the Figure 1, the instructions \nexecuted between S3 - S5 constitute a sequencing region. It over\u00adlaps with the sequencing regions S1 \n- S4 and S4 - S7 in the thread T2, and also with the sequencing region S2 - S6 in the thread T3. In other \nwords, there is no happens-before relationship T1 T1 T2 T3 T2 T1 T2 S1 S2 W S3 S4R R W S5 S6 S7  \nFigure 1. Happens-before based race detection during replay using sequencers in the replay log. between \nthe memory operations executed in the overlapping se\u00adquencing regions. We then detect a data race using \nthe following happens-before algorithm. If we .nd two memory operations in two overlapping sequencing \nregions, and at-least one of them is a write, then we consider that the two memory operations to be involved \nin a data race. There is a data race between those two memory operations, because there is no sequencer \nseparating the two in time to spec\u00adify an order between them. If there is no sequencer between two memory \noperations, then it implies that there was no synchroniza\u00adtion operation that was executed during the \nprogram s execution to guard the shared memory accesses. Therefore, there is a data race between the \ntwo memory operations. 4. Classifying Data Races by Replaying Both Orderings In the previous section, \nwe described how we .nd a set of data races in a given program s execution by looking for memory op\u00aderations \nthat are not ordered by a happens-before relation. In this section, we present a replay-based dynamic \nanalysis algorithm that automatically classi.es data races as either potentially benign and potentially \nharmful. 4.1 Overview Consider a data race between two memory operations executed in a particular execution \nof a multi-threaded program. The two memory operations involved in the con.ict would have been executed \nin a particular order during the original execution recorded by iDNA. During replay, there are two possible \norderings for the two memory operations involved in the data race. Our analysis, experi\u00adments by replaying \nthe two different orders for those memory op\u00aderations and examines the outcome of the two executions. \nIf the outcomes are the same, then we classify that instance of the data race as potentially benign. \nOtherwise, the data race is classi.ed as potentially harmful, which will then be examined by the developer. \nFigure 2 shows a piece of code to illustrate how the proposed analysis works. This is a sanitized example \nof one of the harmful R1(2) R1(2) W1(1) W1(1) Thread 1 and 2 R2(1) R3(1) executes: {  foo->refCnt--; \n  If(foo->refCnt == 0)    free(foo); R3(1) W2(0) R4(0) FREE R(0) W2(0) FREE R4(0) } FREE (a) Replay \n1 (b) Replay 2 No bug exposed Bug exposed Figure 2. Race Detection Example data races found during \nour analysis on production code. The ex\u00adample code essentially decrements a reference counter value. \nThen, it reads the reference counter value. If the value is zero it frees the memory pointed to by the \nvariable foo . Assume that there are two threads executing the same piece of code in parallel and that \nthe programmer, by mistake, did not use any synchronization oper\u00adations to guarantee the correct parallel \nexecution. The .gure also shows two possible orderings for the memory operations when this piece of code \nis concurrently executed by two threads. The values of the memory operations are shown inside the parenthesis. \nFigure 2(a) shows the order observed during recording. Fortunately, for this ordering, the atomicity \nof the operations was not violated and hence the program executes correctly. However, during our dynamic \nanalysis, we will detect data races between the read and the write operations executed in the two threads. \nFor example, there is a data race between the read R2 in thread T1, and the write W2 in thread T2. During \ndynamic analysis, we can replay for the two possible orders between these two memory operations. One \nreplay will be the same as the one observed during the original execution that was recorded as shown \nin Figure 2(a). Another possible order is shown in Figure 2(b). In the latter order, the R2 is replayed \nafter W2. During that replay, we will catch a null pointer violation, when the replay tries to free the \nlocation foo in the thread T1. Thus, we determine that the data race is potentially harmful. In contrast, \nwhen we examine the two orderings, if they evaluate to the same result, then we classify that instance \nof the data race as potentially benign. However, even if one of the instance of the data race was found \nto be potentially harmful, then we classify the data race to be potentially harmful. We next describe \nhow we perform our replay analysis to determine if the two orderings arrive at the same result or not. \n 4.2 Mechanism for Alternative Replay The above discussion assumes that it is possible to replay the \ntwo possible memory orders. We had to add the following support to provide this functionality on top \nof iDNA. Our algorithm analyzes each data race in isolation. For a given data race, the goal is to replay \nthe two possible memory orders between the two memory operations involved in the data race. The two memory \noperations involved in the data race are part of two sequencing regions in two different threads (a sequencing \nregion constitutes the instructions executed between two sequencers in a thread, which we described in \nSection 3). By replaying the program s execution, we can determine all the instructions executed in each \nof the two sequencing regions that contain the data race. Using this information, we know which two dynamic \ninstructions are the data race being considered, and we can examine both orders of those two instructions \nduring replay. We replay both threads for the region up until we get to the data race instruction in \neach thread. We then can replay the two orders to examine the differences exhibited by the data race. \nThe .rst order we call the original order, since it matches the values seen during the original logged \nexecution, and the second order we call the alternative order. In order to execute the instructions in \nthe two sequencing re\u00adgions for the two orders, we added to iDNA the ability to create a virtual processor. \nThe virtual processor allows us to start with a set of sequences across the threads and execute multiple \ndiffer\u00adent realities starting at that set of sequences. A virtual processor is created to execute the \noriginal and alternative replay orders. The virtual processor is initialized with the live-in memory \nvalues and the register states of the two threads. We orchestrate the execution of the two threads in \nthe virtual processor to obey the ordering for the instructions involved in the data race. Whenever a \nmemory lo\u00adcation is read for the .rst time in the virtual processor, the virtual processor copies the \nvalue from the live-in memory. Then from that point on, the reads and writes to that memory location \nwill be to the local copy in the virtual processor. 4.2.1 Alternative Replay Failure While executing \nthe alternative ordering, the replayer may come across a memory reference to an address not seen when \nthe orig\u00adinal log was taken, or it may come across a control .ow change. The address may not have been \nlogged or it may have been changed during replay, so we do not know what the value is. For the control \n.ow change, it may jump to a piece of code that was not recorded as part of the logging or to an illegal \naddress. In our current imple\u00admentation, we classify all of these as replay failures. They are an indication \nthat execution has changed enough from the alternative order that the data race is potentially harmful. \nEven so, we are look\u00ading at trying to log enough information to allow replay to continue in the face \nof both of these for the .nal version of our tool.  4.3 Classifying Data Races After we have replayed \nthe original and alternative orderings in the virtual processors, we compare the register and memory \nlive-outs at the end of the sequencing regions to classify the data race as potentially benign or potentially \nharmful. A data race between two memory operations may occur many times during our analysis, and we examine \neach of those as a sep\u00adarate data race instance. Our current approach .ags a data race in\u00adstance as potentially \nbenign only if the two replays result in ex\u00adactly the same application state (both memory live-outs and \nregister state) at the end of the replay. Otherwise, the data race instance is considered to be potentially \nharmful. The potentially harmful con\u00adsist of the data races where the alternative replay resulted in \ndif\u00adferent state, and also those that had a replay failure as described above. After all of the instances \nfor a data race have been examined, we classify the data race as potentially benign only if all of its \ninstances are classi.ed as potentially benign. Otherwise the data race is classi.ed as potentially harmful. \nThe data races classi.ed as potentially benign are guaranteed to be benign for the test scenarios we \nexamined, but they are not guaranteed to be benign for all possible scenarios. Another instance of the \ndata race not captured in our replay logs between the same two memory operations may prove to be harmful. \nTo add more con.dence to our classi.cation, several instances of the same data race should try to be \nfound in the same execution or across the different test scenarios. If the replay analysis determines \nthe data race to be potentially benign in all those instances, then we will have greater con.dence that \nthe data race is probably benign. The greater the number of instances studied, the greater is the con.dence \nthat a data race is benign. For those data races that are classi.ed as potentially harmful the two replays \nwill enable the developer to have a better understanding of the effects of the data race that is reported. \nThe two replays will show the differences in the outcomes of the program for the two different memory \norders between the memory operations involved in the data race. 4.4 Advantages There are some advantages \nwith our replay based analysis: Our analysis is at the instruction level and is not dependent on the \nspeci.c synchronization methods. As a result, it is applica\u00adble to programs written in any language as \nit is agnostic to the synchronization methods used in the language. Our instruction based happens-before \nanalysis does incur a heavy performance overhead. However, this is not a serious concern for our ap\u00adproach \nbecause we perform our analysis off-line during replay, where we can take more time to do the dynamic \nanalysis.  Unlike traditional approaches where it is hard to determine the possible effects of a data \nrace, we will have two possible executions for the data race and produce the output for those executions. \nThe ability to replay and see the differences in output between the two executions is of great value \nfor the developer to understand the potential data race.  5. Results In this section, we discuss our \nexperiences in using our tool to classify the data races. 5.1 Methodology We collected replay logs for \n18 different executions of various services in Windows Vista and the Internet Explorer. Among the 18 \nexecutions that we studied, the happens-before based algorithm that we described in Section 3 returned \n16,642 instances of data race con.icts. Out of these 16,642 instances there were only 68 unique data \nraces. The reason is that a data race (between the same two memory instructions in different threads) \noccurred more than once in the same execution or in different scenarios. For this study, we went through \nthe painstaking effort to manually examine every single data race to determine if it was actually benign \nor harmful. All of the data races that were identi.ed as truly harmful have been .xed in the production \ncode. The average log size for the replay logs collected using iDNA was about 0.8 bit per instruction. \nThe total storage space required for the logs was 3.1 GB, which captured 33 billion instructions executed \nacross all the different executions that we studied (about 96 MB to record a billion instructions). By \ncompressing the log sizes using the Windows zip utility, we reduced the log sizes to about 0.3 bit per \ninstruction. To get an estimate for the time overhead for recording, replay\u00ading and analyzing programs \nwe studied an execution of Internet Explorer, where we accessed a website and browsed through a few pages. \nThis study was carried out on a Pentium 4 Xeon 2.2GHz processor with 1GB RAM. The runtime performance \noverhead to collect the replay logs using iDNA [3] was about 6x when com\u00adpared to the native execution. \nThe iDNA replayer can replay the recorded execution with a performance overhead of 10x on aver\u00adage (relative \nto the native execution). The execution had spawned Potentially Benign Potentially Harmful Real Benign \nReal Harmful Real Benign Real Harmful Total No State Change 32 0 - - 32 State Change - - 15 2 17 Replay \nFailure - - 14 5 19 Total 32 0 29 7 68 Table 1. Data Race Classi.cation 27 threads. When we ran our \nhappens-before based race detection analysis, we found 2,196 instances of various data races. The over\u00adhead \nof executing the off-line happens-before race detection anal\u00adysis was about 45x. The overhead of executing \nthe replay analysis that we described in Section 4 to classify the data races was about 280x when compared \nto the native execution.  5.2 Data Race Classi.cation Results 5.2.1 Outcomes of Replay Analysis We performed \nthe replay-based data race classi.cation analysis that we described in Section 4 over all the instances \nof the data races that were found using the happens-before algorithm. There are three possible outcomes \nwhen we perform the replay based analysis for an instance of a data race. The two replays for an instance \nmay produce the same live-out. We call this outcome No-State-Change, because the memory order does not \naffect the state of the program s execution. Another possible outcome is that the two replays might produce \ndifferent live-outs. We call this outcome State-Change. Finally, for some instances of data races we \nmay encounter a replay failure while replaying for the alternative order for the reasons that we described \nin Section 4. We call this outcome Replay-Failure. Note, that a replay failure is a good indicator that \nthe data race is likely to cause a change in the program s state (in other words, the outcome is similar \nto State-Change). There can be many instances for a given unique (static) data race. The .nal classi.cation \nfor a data race classi.es the data race as No-State-Change only if all of its instances are No-State-Change. \nIf for any instance of the data race, the outcome was a State-Change, then we place the data race in \nthe State-Change group. All of the remaining unique data races are classi.ed as Replay-Failure. These \nare the data races for which none of the instances were classi.ed as State-Change and at least one of \nthe instances was classi.ed as Replay-Failure. 5.2.2 Data Race Classi.cation Table 1 presents the classi.cation \nfor all the unique static data races (a data race between the same two static instructions) that we studied. \nThe rows in the table correspond to one of the three outcomes of the automatic replay analysis that we \njust described. Based on the outcomes of the replay analysis for all the in\u00adstances of a data race, our \nreplay checker classi.es the data race as either Potentially-Benign or Potentially-Harmful. These two \nclas\u00adsi.cations are shown in the table as the two aggregate columns. All data races classi.ed as No-State-Change \nare potentially benign, and all data races classi.ed as State-Change or Replay-Failure are classi.ed \nas potentially harmful. Table 1 splits the potentially benign and harmful columns fur\u00adther into two groups: \nReal-Benign and Real-Harmful. The sub\u00adcolumns correspond to the manual classi.cation. In addition to \nthe automatic classi.cation, we also manually triaged each data race to determine if they were really \nbenign or harmful. This was done to determine the accuracy of the automatic classi.cation.  5.2.3 Potentially \nBenign Data Races Table 1 shows that out of the total 68 data races that we studied, 32 data races fell \ninto the No-State-Change group. Since none of the instances of these data races can cause a state change \nor a replay failure, our automatic analysis classi.ed these 32 data races as potentially benign. We manually \nveri.ed each of these data races and found that they were all indeed benign. None of them were found \nto be harmful. 5.2.4 Potentially Harmful Data Races The data races accounted for in the second and the \nthird rows in the Table 1 were classi.ed as potentially harmful. The reason behind this classi.cation \nis that, in at-least one instance of a data race, if the outcome of the replay analysis was either a \nstate change or a replay failure then it has the potential to be harmful. Based on this classi.cation \nthe automatic replay analysis classi.ed 36 data races (17+19) to be potentially harmful. Seven among \nthe 36 data races were found to be harmful through manual inspection, as listed in the sub-column named \nReal-Harmful under the Potentially-Harmful column. The auto\u00admatic analysis correctly classi.ed all the \nreal harmful data races that it analyzed as potentially harmful. Two of these harmful data races are \nsimilar to the reference counting example that we dis\u00adcussed in Section 4. However, as we can see from \nthe table not all of the potentially harmful races were found to be harmful in our manual classi.ca\u00adtion. \nThe sub-column named Real-Benign under the Potentially-Harmful column shows that 29 data races that were \nclassi.ed as potentially harmful are actually benign. The following are the two main reasons for the \nmisclassi.cation. Misclassi.cation Due to Approximate Computation: By manually inspecting these 29 data \nraces, we found that 23 of them actually affect the execution of the program. As a result, our re\u00adplay \nanalysis will .nd a state change or a replay failure for most of the instances of these data races. Therefore, \nthey were classi.ed as potentially harmful. We took these potentially harmful data races to the developers. \nThey described that these data races were left in the production code, because they chose to tolerate \nthe effects of the data race rather than synchronize the code and lose perfor\u00admance. A good example where \nthis kind of optimization is possible is the code region that was used to update a data structure main\u00adtaining \nstatistics. In that case, the programmer consciously chose to gather approximate statistics and avoid \nthe performance overhead required to accurately gather them. Another example is where the variable s \nvalue is used to make decisions that can affect only the performance and not correctness (e.g., time-stamp \nvalue used for making decisions on what to replace from a software cache). To optimize the synchronization \noverhead, programmers may choose to not synchronize operations on values such as time-stamps and statistics \nwherever appropriate. Since these data races were in\u00adtended by the programmers, the are classi.ed as \nReal-Benign, even though they can change the program s execution. Misclassi.cation Due to Replayer Limitation: \nWe now focus on the remaining 6 Real-Benign data races of the 29 data races that were incorrectly classi.ed \nas Potentially-Harmful. When we manually triaged the six data races, we found them to be benign. Unlike \nthe other 23 data races that we discussed earlier, these six data races did not affect the output or \nthe state of the program. The reason why these six data races still got classi.ed as Potentially-Harmful \nis that for at-least one of their instances, the outcome of the replay analysis was Replay-Failure. The \nreplay failure was to due to the reasons that we described in Section 4.2.1. When we manually analyzed \nthese 6 replay failures, we actually found that the execution of the program wouldn t have been affected \nhad the replays proceeded without failing. By adding additional support in iDNA to execute down unseen \ncontrol paths, we should be able to correctly classify these six data races as no-state change and thereby \nclassify them as potentially benign. In conclusion, our approach classi.ed 47% of the data races as potentially \nbenign and they were all benign (none of them were harmful). Out of the other 53% of the data races that \nwere classi.ed as potentially harmful, only 20% of the 53% were found to be harmful.  5.3 Results for \nEach Dynamic Data Race Instance Let us now discuss the results for the each of the instances that we \nanalyzed for every static data race. We will also discuss the type of outcome that we obtained from the \nreplay analysis for each instance. Figure 3 shows the number of instances that we analyzed for each of \nthe 32 data races that were of the type No-State-Change, which we classi.ed as Potentially-Benign. The \nnumber of instances for each unique data race varied from about 50 instances to just one instance. The \ngreater the number of instances that we analyze and classify as No-State-Change, the greater the con.dence \nwe have in classifying them as Potentially-Benign. Figure 4 shows the number of instances that we analyzed \nfor each harmful data race. As we can see, for some of the harmful data races we analyzed several thousand \ninstances. However, only one in ten of those instances caused a replay failure or a state change. This \nshows that it is important to see those data races multiple times in order to catch them as Potentially-Harmful. \nFigure 5 shows the number of instances that we studied for the data races that we considered to be Potentially-Harmful, \nbut when we analyzed them manually we found them to be Real-Benign. The main cause for this misclassi.cation \nare the data races due to approximate computation, which we described in Section 5.2.4.  5.4 Reasons \nfor Benign Data Races In this section, we describe the categories of benign data races that we were able \nto automatically classify as potentially benign. 1. User Constructed Synchronization: Programmers may \ncon\u00adstruct their own synchronization primitives without using fences or the atomic operations provided \nin the instruction set archi\u00adtecture. For example, a garbage collector can maintain the ref\u00aderence counts \nfor concurrent objects without using locks [9]. It is dif.cult to automatically infer the user constructed \nsynchro\u00adnization operations and so iDNA does not log a sequencer for a user constructed synchronization \noperation during the log\u00adging run. Because of this reason, the happens-before algorithm, will incorrectly \nclassify a race between two user constructed synchronization operations, which is essentially correct \nsyn\u00adchronization, as a data race. 2. Double Checks: Double checks are used to optimize the syn\u00adchronization \noverhead. A typical example for a double check is:   Figure 3. Statistics for the unique data races \nclassi.ed as Potentially-Benign. Every instance of these data races resulted in No-State-Change and were \nactually Real-Benign. Total number of instances for each such data race are shown. Unique Data Races \n Figure 4. Statistics for the unique data races that were classi.ed as Potentially-Harmful and they were \nfound to be Real-Harmful. Results are shown for total number of instances, and also for the number of \ninstances that resulted in a State-Change or Replay-Failure. Figure 5. Statistics for the unique data \nraces that were classi.ed as Potentially-Harmful, but they were actually Real-Benign. Results are shown \nfor total number of instances, and also for the number of instances that resulted in a State-Change or \nReplay-Failure. # Races User Constructed Synchronization 8 Double Checks 3 Both Values Valid 5 Redundant \nWrites 13 Disjoint bit manipulation 9 Approximate Computation 23 Table 2. Benign Data Races. if(a) { \nlock (..) { if(a) ... } } The read in the .rst check is not synchronized and so there can be a data race \ninvolving the read, but the data race is benign. 3. Both Values are Valid: Let us consider a data race \nbetween a read and a write operation. We found many instances where it is correct for the read operation \nto return either the old or the updated value (old value is the value in memory before the write and \nupdated value is the value in memory after the write). For example, when a buffer is shared between the \nproducer and the consumer it can be correctly synchronized without using synchronization primitives. \nThe producer writes to a buffer and increments the number of writes. The consumer reads the number of \nwrites, and if it is greater than the number of entries it has consumed so far (referred to by a local \nvariable), then it consumes an entry from the buffer. After consuming a value it updates its local variable \nrepresenting the number of entries consumed. Without explicit synchronization, it is possible that the \nconsumer might read a stale value for the buffer size. But that is .ne, since it will just force the \nconsumer to wait longer. In another example, a shared variable was checked to decide which of the two \nversions of a function need to be used for do\u00ading a particular computation. Both the functions do exactly \nthe same computation, but with different performance characteris\u00adtics. The shared variable is written \nby another thread to specify the version of the function to be used. However, the read and the write \nneed not be synchronized, because both versions will pro\u00adduce correct results, though one version might \nperform slower than the other. Similar to this, we found the case where it just mattered if the memory \nvalue zero or non-zero. The code was valid for multiple writers setting the memory value to non-zero \nwithout any synchronization, and it did not matter if the non-zero value written was the same. 4. Redundant \nWrites: If a write operation writes the same old value that already resides in the memory location then \nthe data race between the write and a read operation in another thread will be benign (thus it can be \nconsidered as a special case of the previous category in the sense that both the old and the updated \nvalues are correct values to return for a read operation). In one of the programs we studied, we found \nthat a thread was writing its process identi.er returned by a system call to a shared variable read by \nanother thread. The writes were redundant and did not affect the correctness of the program execution. \n 5. Disjoint Bit Manipulation: There can be data races between two memory operations where the programmer \nknows for sure that the two operations use or modify different bits in a shared variable. Programmers \ntend to use multiple bits in the same variable in order to optimize for performance.  Table 2 shows \nthe number of data races that we studied for each of the above categories of benign data races. It also \nshows that there were 23 data races that were due to approximate com\u00adputation, which were mis-classi.ed \nby the replay analysis. As we mentioned in Section 5.2.4, there were six other benign data races that \nwere misclassi.ed as Potentially-Benign. These six were due to those benign data races that can affect \nthe control .ow of the pro\u00adgram s execution. The rest of the benign data races were correctly classi.ed \nas Potentially-Benign and the reasons for why they were benign are shown in Table 2. 6. Conclusion In \nthis paper, we focused on automatically .nding the potentially harmful data races. The happens-before \nalgorithm that we used does not report false positives, but it still yields a large number of true data \nraces, out of which 90% are benign. To reduce the triage effort, we needed to automatically identify \nand .lter the data races that are potentially benign. We built our dynamic analysis mechanism on top \nof iDNA, which provided us the ability to record and replay a program s exe\u00adcution. To automatically \n.nd out if a data race is potentially benign or not, the replay based checker replays the execution twice, \none for each possible order between the con.icting memory operations. If the two replays for the two \norders produce the same result, then the checker classi.es the data race as potentially benign. In addition \nto reporting harmful data races, the analysis also pro\u00adduces very useful information to assist a programmer \nin debugging the data race. For every data race, the checker dumps out the replay log along with the \nmemory orders corresponding to the data race. Using that information, a programmer can replay the program \nin two different ways and understand the effects of different memory orders that are possible due to \nthe data race. This information can be a signi.cant aid to the developer. We discussed our experiences \nin using our dynamic race clas\u00adsi.cation approach on an extensively stress-tested build of Mi\u00adcrosoft \ns Windows Vista and Internet Explorer. Our proposed tech\u00adnique was able to automatically .lter out over \nhalf of the real be\u00adnign data races, by classifying them as potentially benign, which can be ignored \nby the developers. In addition, all of the real harm\u00adful data races were correctly classi.ed as potentially \nharmful. The harmful data races that we found were reported to the developers and all of them have been \n.xed in the production code. Acknowledgments We would like to thank the anonymous reviewers for providing \nvaluable feedback on this paper. This work was funded by grants from Intel and Microsoft. References \n[1] S. V. Adve, M. D. Hill, B. P. Miller, and R. H. B. Netzer. Detecting data races on weak memory systems. \nIn ISCA 91: Proceedings of the 18th Annual International Symposium on Computer architecture, 1991. [2] \nR. Agarwal, A. Sasturkar, L. Wang, and S. D. Stoller. Optimized run\u00adtime race detection and atomicity \nchecking using partial discovered types. In In Proceedings of the 20th IEEE/ACM International Conference \non Automated Software Engineering, pages 233 242, 2005. [3] S. Bhansali, W. Chen, S. de Jong, A. Edwards, \nand M. Drinic. Framework for instruction-level tracing and analysis of programs. In Second International \nConference on Virtual Execution Environments, June 2006. [4] C. Boyapati, R. Lee, and M. Rinard. Ownership \ntypes for safe programming: Preventing data races and deadlocks. In Object-Oriented Programming Systems, \nLanguages, and Applications, 2002. [5] J. D. Choi, K. Lee, A. Loginov, R. O Callahan, V. Sarkar, and \nM. Srid\u00adharan. Ef.cient and precise datarace detection for multithreaded object-oriented programs. In \nPLDI 02: Proceedings of the ACM SIGPLAN 2002 Conference on Programming language design and implementation, \npages 258 269, New York, NY, USA, 2002. ACM Press. [6] J. D. Choi, B. P. Miller, and R. H. B. Netzer. \nTechniques for debugging parallel programs with .owback analysis. ACM Transactions on Programming Languages \nand Systems, 13(4):491 530, 1991. [7] J. D. Choi and S. L. Min. Race frontier: reproducing data races \nin parallel-program debugging. In PPOPP 91: Proceedings of the third ACM SIGPLAN symposium on Principles \nand practice of parallel programming, pages 145 154, 1991. [8] J. M. Crummey. On-the-.y detection of \ndata races for programs with nested fork-join parallelism. In Supercomputing 91: Proceedings of the 1991 \nACM/IEEE conference on Supercomputing, pages 24 33, 1991. [9] D. L. Detlefs, P. A. Martin, M. Moir, and \nJr. G. L. Steele. Lock\u00adfree reference counting. In PODC 01: Proceedings of the twentieth annual ACM symposium \non Principles of distributed computing, pages 190 199, 2001. [10] A. Dinning and E. Schonberg. An empirical \ncomparison of monitoring algorithms for access anomaly detection. In PPOPP 90: Proceedings of the second \nACM SIGPLAN symposium on Principles &#38; practice of parallel programming, pages 1 10, 1990. [11] A. \nDinning and E. Schonberg. Detecting access anomalies in programs with critical sections. In PADD 91: \nProceedings of the 1991 ACM/ONR workshop on Parallel and distributed debugging, pages 85 96, 1991. [12] \nT. Elmas, S. Tasiran, and S. Qadeer. Vyrd: verifying concurrent programs by runtime re.nement-violation \ndetection. In PLDI, 2005. [13] D. Engler and K. Ashcraft. Racerx: effective, static detection of race \nconditions and deadlocks. In SOSP 03: Proceedings of the nineteenth ACM symposium on Operating systems \nprinciples, pages 237 252, 2003. [14] C. Flanagan and S. N. Freund. Type-based race detection for java. \nIn PLDI 00: Proceedings of the ACM SIGPLAN 2000 conference on Programming language design and implementation, \npages 219 232, 2000. [15] T. A. Henzinger, R. Jhala, and R. Majumdar. Race checking by context inference. \nIn PLDI 04: Proceedings of the ACM SIGPLAN 2004 conference on Programming language design and implementation, \npages 1 13, 2004. [16] M. Hicks, J. S. Foster, and P. Pratikakis. Inferring locking for atomic sections. \nIn Proceedings of the ACM SIGPLAN Workshop on Languages, Compilers, and Hardware Support for Transactional \nComputing (TRANSACT), June 2006. [17] L. Lamport. Time, clocks, and the ordering of events in a distributed \nsystem. Communications of the ACM, 21(7):558 565, 1978. [18] S. Lu, J. Tucek, F. Qin, and Y. Zhou. Avio: \ndetecting atomicity violations via access interleaving invariants. In ASPLOS-XII: Proceedings of the \n12th international conference on Architectural support for programming languages and operating systems, \npages 37 48, 2006. [19] S. L. Min and J.-D. Choi. An ef.cient cache-based access anomaly detection scheme. \nIn Proceedings of the 4th International Conference on Architectural Support for Programming Languages \nand Operating System (ASPLOS), pages 235 244, 1991. [20] M. Naik, A. Aiken, and J. Whaley. Effective \nstatic race detection for java. In PLDI 06: Proceedings of the 2006 ACM SIGPLAN conference on Programming \nlanguage design and implementation, pages 308 319, 2006. [21] R. H. B. Netzer. Optimal tracing and replay \nfor debugging shared\u00admemory parallel programs. In Proceedings of the ACM/ONR Workshop on Parallel and \nDistributed Debugging, pages 1 11, 1993. [22] H. Nishiyama. Detecting data races using dynamic escape \nanalysis based on read barrier. Third Virtual Machine Research &#38; Technology Symposium, pages 127 \n138, May 2004. [23] R. O Callahan and J. D. Choi. Hybrid dynamic data race detection. In PPoPP 03: Proceedings \nof the ninth ACM SIGPLAN symposium on Principles and practice of parallel programming, pages 167 178, \n2003. [24] D. Perkovic and P. J. Keleher. Online data-race detection via coherency guarantees. In OSDI, \npages 47 57, 1996. [25] E. Pozniansky and A. Schuster. Ef.cient on-the-.y data race detection in multithreaded \nc++ programs. In PPoPP 03: Proceedings of the ninth ACM SIGPLAN symposium on Principles and practice \nof parallel programming, pages 179 190, 2003. [26] P. Pratikakis, J. S. Foster, and M. Hicks. Locksmith: \ncontext-sensitive correlation analysis for race detection. In PLDI 06: Proceedings of the 2006 ACM SIGPLAN \nconference on Programming language design and implementation, pages 320 331, 2006. [27] M. Prvulovic \nand J. Torrelas. Reenact: Using thread-level speculation mechanisms to debug data races in multithreaded \ncodes. In 30th Annual International Symposium on Computer Architecture, San Diego, CA, June 2003. [28] \nS. Qadeer and D. Wu. Kiss: keep it simple and sequential. In PLDI 04: Proceedings of the ACM SIGPLAN \n2004 conference on Programming language design and implementation, pages 14 24, 2004. [29] B. Richards \nand J. R. Larus. Protocol based data race detection. In Proceedings of the SIGMETRICS Symposium on Parallel \nand Distributed Tools, pages 40 47. ACM Press, 1998. [30] M. Ronsse and K. de Bosschere. Recplay: A fully \nintegrated practical record/replay system. ACM Transactions on Computer Systems, 17(2):133 152, 5 1999. \n[31] M. Ronsse and K. de Bosschere. Non-intrusive on-the-.y data race detection using execution replay. \nIn Proceedings of Automated and Algorithmic Debugging, Nov 2000. [32] A. Sasturkar, R. Agarwal, L. Wang, \nand S. D. Stoller. Automated type-based analysis of data races and atomicity. In PPoPP 05: Proceedings \nof the tenth ACM SIGPLAN symposium on Principles and practice of parallel programming, pages 83 94, 2005. \n[33] S. Savage, M. Burrows, G. Nelson, P. Sobalvarro, and T. Anderson. Eraser: A dynamic data race detector \nfor multithreaded programs. ACM Transactions on Computer Systems, 15(4):391 411, 1997. [34] E. Schonberg. \nOn-the-.y detection of access anomalies. In Proceedings of the ACM SIGPLAN 89 Conference on Programming \nLanguage Design and Implementation (PLDI), 1989. [35] N. Sterling. Warlock -a static data race analysis \ntool. In Proceedings of the USENIX Winter Technical Conference, pages 97 106, 1993. [36] C. von Praun \nand T. R. Gross. Object race detection. In OOPSLA 01: Proceedings of the 16th ACM SIGPLAN conference \non Object oriented programming, systems, languages, and applications, pages 70 82, 2001. [37] M. Xu, \nR. Bodik, and M. D. Hill. A serializability violation detector for shared-memory server programs. In \nACM SIGPLAN 2005 Conference on Programming Language Design and Implementation (PLDI), 2005. [38] Y. Yu, \nT. Rodeheffer, and W. Chen. Racetrack: ef.cient detection of data race conditions via adaptive tracking. \nIn SOSP 05: Proceedings of the twentieth ACM symposium on Operating systems principles, pages 221 234, \n2005.   \n\t\t\t", "proc_id": "1250734", "abstract": "<p>Many concurrency bugs in multi-threaded programs are due to dataraces. There have been many efforts to develop static and dynamic mechanisms to automatically find the data races. Most of the prior work has focused on finding the data races and eliminating the false positives.</p> <p>In this paper, we instead focus on a dynamic analysis technique to automatically classify the data races into two categories - the dataraces that are potentially benign and the data races that are potentially harmful. A harmful data race is a real bug that needs to be fixed. This classification is needed to focus the triaging effort on those data races that are potentially harmful. Without prioritizing the data races we have found that there are too many data races to triage. Our second focus is to automatically provide to the developer a reproducible scenario of the data race, which allows the developer to understand the different effects of a harmful data race on a program's execution.</p> <p>To achieve the above, we record a multi-threaded program's execution in a replay log. The replay log is used to replay the multi-threaded program, and during replay we find the data races using a happens-before based algorithm. To automatically classify if a data race that we find is potentially benign or potentially harmful, were play the execution twice for a given data race - one for each possible order between the conflicting memory operations. If the two replays for the two orders produce the same result, then we classify the data race to be potentially benign. We discuss our experiences in using our replay based dynamic data race checker on several Microsoft applications.</p>", "authors": [{"name": "Satish Narayanasamy", "author_profile_id": "81100556410", "affiliation": "UC San Diego, La Jolla, CA", "person_id": "P540960", "email_address": "", "orcid_id": ""}, {"name": "Zhenghao Wang", "author_profile_id": "81547596556", "affiliation": "Microsoft, Redmond, WA", "person_id": "PP33034444", "email_address": "", "orcid_id": ""}, {"name": "Jordan Tigani", "author_profile_id": "81331505572", "affiliation": "Microsoft, Redmond, WA", "person_id": "P871677", "email_address": "", "orcid_id": ""}, {"name": "Andrew Edwards", "author_profile_id": "81331491856", "affiliation": "Microsoft, Redmond, WA", "person_id": "PP33032221", "email_address": "", "orcid_id": ""}, {"name": "Brad Calder", "author_profile_id": "81100088945", "affiliation": "Microsoft, Redmond, WA", "person_id": "PP17000250", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1250734.1250738", "year": "2007", "article_id": "1250738", "conference": "PLDI", "title": "Automatically classifying benign and harmful data races using replay analysis", "url": "http://dl.acm.org/citation.cfm?id=1250738"}