{"article_publication_date": "06-10-2007", "fulltext": "\n Improved Error Reporting for Software that Uses Black-Box Components Jungwoo Ha Christopher J. Rossbach \nJason V. Davis Indrajit Roy Hany E. Ramadan Donald E. Porter David L. Chen Emmett Witchel Department \nof Computer Sciences The University of Texas at Austin {habals,rossbach,jdavis,indrajit,ramadan,porterde,dlcc,witchel}@cs.utexas.edu \nAbstract An error occurs when software cannot complete a requested action as a result of some problem \nwith its input, con.guration, or environ\u00adment. A high-quality error report allows a user to understand \nand correct the problem. Unfortunately, the quality of error reports has been decreasing as software \nbecomes more complex and layered. End-users take the cryptic error messages given to them by pro\u00adgrams \nand struggle to .x their problems using search engines and support websites. Developers cannot improve \ntheir error messages when they receive an ambiguous or otherwise insuf.cient error in\u00addicator from a \nblack-box software component. We introduce Clarify, a system that improves error reporting by classifying \napplication behavior. Clarify uses minimally invasive monitoring to generate a behavior pro.le, which \nis a summary of the program s execution history. A machine learning classi.er uses the behavior pro.le \nto classify the application s behavior, thereby enabling a more precise error report than the output \nof the application itself. We evaluate a prototype Clarify system on ambiguous error messages generated \nby large, modern applications like gcc,La-TeX, and the Linux kernel. For a performance cost of less than \n1% on user applications and 4.7% on the Linux kernel, the prototype correctly disambiguates at least \n85% of application behaviors that result in ambiguous error reports. This accuracy does not degrade signi.cantly \nwith more behaviors: a Clarify classi.er for 81 La-TeX error messages is at most 2.5% less accurate than \na classi.er for 27 LaTeX error messages. Finally, we show that without any hu\u00adman effort to build a classi.er, \nClarify can provide nearest-neighbor software support, where users who experience a problem are told \nabout 5 other users who might have had the same problem. On av\u00aderage 2.3 of the 5 users that Clarify \nidenti.es have experienced the same problem. Categories and Subject Descriptors D.2.7 [Distribution, \nMainte\u00adnance, and Enhancement]: Enhancement General Terms Documentation, Management, Reliability Keywords \nSoftware support, Error report, Pro.ling, Classi.ca\u00adtion, Machine learning Permission to make digital \nor hard copies of all or part of this work for personal or classroom use is granted without fee provided \nthat copies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. PLDI 07 June 11 13, 2007, San Diego, California, \nUSA. Copyright c . 2007 ACM 978-1-59593-633-2/07/0006. . . $5.00 1. Introduction Bad error reporting \nis more than an inconvenience for most users. A large part of modern software support cost comes from \ntime wasted with bad error messages, which we de.ne as any message that does not provide suf.cient information \nfor a user to .x the problem in a timely fashion. One recent study concluded that up to 25 percent of \na system administrator s time may be spent following blind al\u00adleys suggested by poorly constructed and \nunclear messages [6]. The time and expertise required to administer modern computing sys\u00adtems is causing \nthe cost of administrating, con.guring and updating a machine to surpass the cost of the hardware [22]. \nImproving error reporting will keep down computer ownership costs and improve end-user satisfaction. \nAn error or error behavior is any program behavior that is not a successful completion of a task speci.ed \nby a user. Errors include bugs, which are program behaviors that do not match a program s speci.cation. \nIt is also an error when a program fails a consistency check on its inputs possibly because the user \nentered bad input, or mis-con.gured the system. Errors cause programs to produce error reports, which \nare usually text messages or dialog boxes that inform the user that the requested action will not complete. \nCrashes and hangs cause the program to output the null error message. The user must interpret an error \nreport to .gure out how to get the program to complete her request, often resorting to search engines \nand support websites (like support.microsoft.com) for more information. Consider the following model \nof error reporting. A given appli\u00adcation has a set E of errors, and a set R of error reports. Unfor\u00adtunately, \none element r . R can correspond to multiple elements e . E because an error report is often ambiguous \nacross multiple causes. For example, the Linux operating system uses the return code EEXIST to signal \ndiverse error conditions, such as an attempt to create a .le whose name already exists in a directory, \nor an at\u00adtempt to put a rule in a routing table that con.icts with the routing table s current state. \nDe.ne S as a set of vectors of runtime statis\u00adtics about an application. Then the tuple (r, s)|r . R, \ns . S could uniquely determine the proper e . E, even though r alone fails. In fact, r might not be needed \nat all, s alone might suf.ce. We introduce Clarify, a system to improve error reporting. Clar\u00adify consists \nof two parts: a runtime to monitor a black-box software component, and a classi.er to interpret the output \nof the runtime. Clarify monitors the program using minimally invasive techniques like reading the program \ns memory or counting function calls. The Clarify runtime outputs a behavior pro.le (the s . S). Clarify \ns users collect the behavior pro.les generated when the program ex\u00adperiences a particular error, and \ntrain a machine learning classi.er that recognizes the application s error behaviors. These users also \nClarify monitors application (A) machine-learning model construction training deployed machine learning \nalgorithm + human labeled profiles (B) behavior classification machine learning classifier  Figure \n1. Clarify consists of a runtime monitor and a machine\u00adlearning classi.er. The rectangles represent processes \nconsuming and producing data. The Clarify runtime monitors a black-box component to generate a behavior \npro.le that summarizes the ex\u00adecution history of the component. Section (A) shows a machine learning \nclassi.er, trained of.ine from behavior pro.les. Section (B) shows the trained classi.er classifying \nbehavior pro.les to pro\u00adduce improved error reports. write an improved error report that describes the \nerror behavior and how to .x or work around it (the e . E). Classi.er training is done by a small minority \nof technically-savvy Clarify users such as support engineers who reproduce user problems in-house. End\u00adusers \nget the improved error reports by classifying their behavior pro.le. Clarify reduces the problem of improving \nerror reports to the problem of classifying error behaviors. Figure 1 shows the major components of Clarify: \nthe runtime and the machine-learning classi.er. Each time the black-box com\u00adponent executes, the Clarify \nruntime generates a behavior pro.le. The behavior pro.le includes information about the control .ow or \ndata values of the program execution. Simple examples of a be\u00adhavior pro.le would include counts of each \nfunction execution, or counts of how often each function returned zero. Training the machine learning \nclassi.er happens in section (A) of Figure 1. A machine learning algorithm takes labeled behav\u00adior pro.les \nas input and produces a classi.er. The classi.er takes a behavior pro.le as input and outputs a label. \nA label could be a non-ambiguous error code, or a lengthy description of the prob\u00adlem and how to resolve \nit. The classi.er improves error reports be\u00adcause users can train the classi.er to recognize very speci.c \nerrors that have a generic error report. In settings where labeled data is not available, Clarify employs \na nearest-neighbor software support method. Here, users are paired with others who have experienced similar \nerrors. Non-technical end-users get improved error reports from Clar\u00adify in section (B) of Figure 1. \nClarify classi.es an end-user s behav\u00adior pro.le, giving them more precise information about their error \nand how to resolve it. The machine learning classi.er uses features from the behavior pro.le to determine \nthe error classi.cation. A feature is the value of a particular statistic, like the number of times the \nfunction decode audio frame was called in an execution of an mp3 player application. A value of zero \ncan indicate an error where no audio frames were ever played. The contributions of this paper are: A \nsystem that combines runtime monitoring and machine learn\u00ading in a novel way to improve error reports \nof black-box soft\u00adware components.  A new pro.ling technique called call-tree pro.ling,that rep\u00adresents \nsoftware behaviors more accurately, on average, than existing pro.ling techniques such as function pro.ling, \nor path pro.ling.   Evaluation of a Clarify prototype on large, mature programs that currently produce \nunclear error messages and confusing er\u00adror behavior, such as the gcc compiler, and the Linux operating \nsystem. Our evaluation includes an in-lab deployment of Clar\u00adify.  Introduction of nearest-neighbor \nsoftware support, where users are paired with other users who have experienced the same problem.  The \nnext section provides an example use of Clarify that moti\u00advates the design presented in Sections 3 5. \nSection 6 describes our benchmarks and the ambiguous errors they report and Section 7 contains the evaluation \nof the Clarify prototype. Section 8 reviews related work and Section 9 concludes. 2. Improving error \nreports with Clarify: an example This section provides an example to elucidate the motivation for Clarify \nand the bene.t to its users. The example also provides mo\u00adtivation for the sections that discuss Clarify \ns design (Sections 3 5). 2.1 Clarify scenario mpg321 is a popular command-line mp3 player written by \nJoe Drew that is included in many Linux distributions. Software sup\u00adport options for mpg321 are limited. \nThere is a support forum on SourceForge, and a mailing list for noti.cation of new releases. Ad\u00additionally, \nusers are invited to email Joe Drew directly. The support forum has many requests for help with zero \nreplies. Recent requests include some with titles, Problem playing mp3 s and no sound. mpg321 tends to \nfail without printing diagnostic messages. Imagine two users, SmartyP and Grandpa, who will use Clarify \nto improve the error reports of mpg321. We will assume that SmartyP posted the message about problems \nplaying mp3s and that Grandpa posted the no sound problem (Note, user names have been changed, but posting \nsubject lines have not). SmartyP has .gured out his problem, and wants to donate his solution to the \nmpg321 support community using Clarify. SmartyP found that his mp3 audio data was corrupt, which does \nindeed cause mpg321 to run without producing audible output. Such a problem could occur if SmartyP were \nstoring his .les on a .ash drive that was failing. Clarify enables Grandpa, a non-technical user, to \nbene.t from the diagnosis of a technical user SmartyP. Step 1: The Clarify runtime. We assume that SmartyP \nand Grandpa have Clarify-enabled versions of mpg321, i.e., the bi\u00adnaries are already linked with the \nClarify runtime. Modifying an application to make it produce a behavior pro.le does not require source \ncode, so it is reasonable to assume that Clarify-enabled bi\u00adnaries can be distributed along standard \nsoftware distribution chan\u00adnels. Step 2: Collect behavior pro.les. With the Clarify runtime, SmartyP \nruns mpg321 on a few corrupt mp3 .les. The interface is simple: when the software fails it queries the \nuser about what went wrong. SmartyP can enter, mpg321 produces no sound output due to corrupted audio \nframe data in the source mp3 .les. Check your mp3 .les because their contents are probably corrupt. The \nClarify\u00adenabled binary then uploads the problem description and behavior pro.les generated by the executions \nthat fail to the SourceForge support site. Step 3: Train a classi.er. A moderator for the SourceForge \nsupport site would read SmartyP s error description and group SmartyP s behavior pro.les with the pro.les \nof other users who experienced the same problem. Having a human in the loop ensures that the language \nin the error report is clear and understandable and guards against malicious or inept users. If less \ncentral authority is desired, a peer reputation system can replace a human moderator. The support site \nsoftware (or moderator) will build a new classi\u00ad.er from the behavior pro.les submitted by users. Users \nonly need upload their pro.les, they do not build classi.ers. There are many policies for managing the \nclassi.er build such as doing it for every new error report, or building it once a day. Step 4: Use the \nclassi.er. Instead of posting no sound to the SourceForge support forum, Grandpa runs his Clarify-enabled \nbinary. When Grandpa fails to hear any sound from mpg321, he hits a special help key which uploads his \nbehavior pro.le to the SourceForge support website. The site classi.es Grandpa s behavior pro.le and \nprovides him with SmartyP s detailed error description, telling him that his mp3 .le has corrupt data. \n 2.2 Discussion By classifying program behavior, Clarify enables a user commu\u00adnity to improve software \nsupport. It also enables software vendors to improve software support. Microsoft has built distributed \nlabel\u00ading of problem reports into Windows Vista. In the documentation for the new Problem reports and \nsolutions control panel item, Microsoft says it can ask end-users to provide additional details about \ntheir problem to create a solution that can be provided to other users [18]. The Clarify scenario presented \nhas most computation occurring on the server, but classi.cation can happen on a client, if the client \nhas the latest classi.er de.nition from the server. Clients might periodically connect to the server \nto download classi.er updates, like modern virus checkers update virus de.nition .les. Once the information \nis cached locally, a client can diagnose errors without connecting to the network. SmartyP treats mpg321 \nlike a black box. He does not change the error reports generated by the source code. Maybe such changes \nwould be accepted by Joe Drew in a timely fashion, but maybe not. Programs developed by more people or \ncommercial organizations are dif.cult or impossible for an end-user to change. The example motivates \nthe following questions, which we ad\u00address in succeeding sections. Feature collection (Section 3). What \ninformation does the Clar\u00adify runtime collect? This section describes alternatives for the con\u00adtents \nof behavior pro.les. Deployment and security (Section 4). Can Grandpa run a min\u00adimally instrumented executable \nthat is fast enough for daily use, but produces behavior pro.les of suf.cient detail to disambiguate \ncur\u00adrently known errors? Does SmartyP s contribution to the support site mean that Grandpa can .gure \nout what kind of mp3s SmartyP listens to? Minimizing human effort (Section 5). Clarify can give Smar-tyP \ns email address to Grandpa, even before SmartyP contributed his labeled examples (or even .gured out \nwhat his problem is), be\u00adcause it can detect similarity between user executions even without a trained \nmachine learning classi.er. SmartyP only uploads a few behavior pro.les, because he assumes that other \nwill also upload pro.les. A classi.er trained on diverse examples usually general\u00adizes better than one \ntrained on homogeneous examples. Section 7.4 measures how many labeled pro.les are necessary to train \nan ac\u00adcurate classi.er (in our experiments, mpg321 requires 38 pro.les per error type).  3. Behavior \npro.les The Clarify runtime should collect the most expressive runtime fea\u00adtures at the lowest cost. \nExpressive features are those that a machine learning algorithm can use to discriminate different error \nbehaviors robustly. Intuitively, expressive features capture details of control .ow or important data \nvalues that are caused by a particular er-Table 1. Summary of the type of feature that is collected by \nthe Clarify runtime. Behavior Pro.le Key Value     ror behavior. For instance, an incorrectly formatted \nURL passed to a web browser can be correlated with the execution of functions that attempt every possible \ninterpretation of the input URL before declaring the error. Programs often have error-reporting routines, \nso one might think that the execution of such routines is a sure.re indication of an error behavior. \nHowever, highly mature and factored pro\u00adgrams, like gcc, reuse error-reporting code for other purposes, \nsuch as producing warnings during correct compilation. In every non-trivial program we have examined, \nsimple correlations be\u00adtween an error condition and the execution of a given function or the presence \nof a given return code do not hold. Clarify collects feature counts from black-box components us\u00ading \ncode instrumentation that does not require source code. Re\u00adcent binary-to-binary translators like Traceback \n[3] (static) and Dynamo(RIO) [4, 11] (dynamic), and .ne-grained instrumentation systems like the OS-level \ninstrumentation tool KernInst [37], Sun s DTrace [13], or Linux s kprobes [24] (all dynamic) provide \nthe op\u00adportunity to insert a small amount of instrumentation code to user\u00adlevel applications or the operating \nsystem with very low execution\u00adtime cost. Clarify must limit the number of features it collects. Error \nbe\u00adhavior is usually correlated with a small number of features, so collecting large numbers of features \nrequires the machine learning algorithm winnow the large set of features down to the relevant few. Having \nmore than about 70,000 features pushes the limits of many machine learning algorithms often causing address \nspace ex\u00adhaustion and unreasonable runtimes. This section discusses Clar\u00adify s strategy for collecting \ninformation about control .ow and data values. 3.1 Control .ow Clarify counts features that are related \nto control .ow because control .ow is a good indicator of program behavior. In general, the more information \nClarify collects about control .ow, the more accurate its model of program behavior, but this accuracy \ncomes at the price of greater CPU and memory overhead. One form of behavior pro.ling counts the execution \nof function call sites. Another counts intra-procedural paths using path pro.l\u00ading [5]. Paths encode \nmore information about control .ow, but they are more expensive to collect than function counts. Clarify \nalso in\u00adtroduces a new pro.ling method called call-tree pro.ling that sum\u00admarizes the calling behavior \nof a function and its caller. The call\u00ading behavior contains some of the intraprocedural control .ow \nthat program paths represent, but it is less computationally intensive to gather. Clarify uses counts \nbecause counts preserve rare events. Often a program will make a unique sequence of function calls before \noutputting a cryptic error report or crashing. Clarify uses those unique calls as the signature of the \nbehavior. Some systems use Figure 2. An example of call-tree pro.ling. The left side of the diagram is \nthe rightmost subtree of the dynamic call tree with arrows pointing in the direction of function calls. \nThe right side is the CTP feature that is collected when function C returns. The CTP feature is combination \nof call sequences of function C and its caller A. event probabilities [8], which penalize the importance \nof rare code paths, especially for programs that run for long periods of time. We evaluate a number of \napproaches to behavior pro.les that have different tradeoffs for performance overhead and level of execution \ndetail. 3.1.1 Function and call-site pro.ling The .rst method uses function pro.ling (FP) (sometimes \ncalled function call pro.ling [31]). Each function has a counter that is incremented when the function \nis executed. The order in which the functions are called is not retained. Function pro.ling is ef.cient \nand tends to be accurate when each behavior has a set of unique functions associated with it. The second \nmethod is call-site pro.ling (CSP). This is similar to FP but the counter is associated with each call \nsite, rather than with the call target. For direct calls, CSP differentiates among call sites, while \nFP does not. 3.1.2 Path pro.ling The third control-.ow based behavior pro.ling method is path pro.ling \n(PP) as described by Ball and Larus [5]. Each program path within a procedure (unique sequence of basic \nblocks) has a counter that is incremented when the path is executed. Path pro.ling distinguishes amongst \nprogram behaviors that result in different control .ow within a function (intra-procedural control .ow), \nsomething that function pro.ling cannot do. 3.1.3 Call-tree pro.ling The fourth control-.ow based pro.ling \ntechnique is call-tree pro\u00ad.ling (CTP). Since each function in a program is one processing step, the \ndynamic call tree is a good representation of the program behavior. However, the size of the whole dynamic \ncall tree is enor\u00admous, it is impractical to use it for the classi.cation directly. CTP counts the number \nof times a particular calling sequence occurs in the current function and its caller. It counts the sequence \nat every function return or loop backedge. CTP is an approximation to the subtree of depth 2 in the call \ntree. Figure 2 shows an example of a dynamic call tree and the CTP pattern that is counted. Each arrow \nindicates the call direction and the left sibling is called before the right sibling. The tree shown \nis where A calls B (B may call other functions but that behavior is ignored by CTP) and then calls C, \nand C calls D and then E. When function C returns the call pattern for C is (C (D E)), and the call pattern \nfor C s caller A is (A (B C)). Therefore, CTP increments by one a counter for the entire pattern of C \nand its caller, (C (D E)),(A (B C)). To implement CTP ef.ciently, each function gets a CTP bitvec\u00adtor, \nwhere each corresponds to a call site in the function. To reduce the number of bits used, a bit is assigned \nonly once per basic block because calls in a basic block happens in the same order. Some bits are shared \nfor basic blocks that cannot be called together in a sin\u00adgle path. When a function returns, CTP increments \na counter for the concatenation of the function s and its caller s bitvector. CTP also increments the \ncounter on loop backedges, clearing the current function s bitvector. In this way, CTP bitvectors remain \ncompact. Path pro.ling is able to preserve more .ne-grained information about paths within the function \nthan CTP s bitvector, but CTP preserves more information about calling context by concatenating the caller \ns bitvector. Because the order of the function calls can be decoded of.ine with the control-.ow graph \nand the bitvector, CTP is distinct from calling context trees [2, 42] which are lossy with respect to \ncalling sequence. Experimental results in section 7 show that CTP supports high classi.cation accuracy. \n 3.2 Data Data values can provide robust characterization of error behavior, though a na\u00a8ive implementation \ncan greatly increase the number of features thereby canceling any bene.t. For instance, to associate \nreturn values with their call sites, Clarify can count call site, return value pairs. A function that \nhas 100 distinct return values will increase the number of features by 100. Such encodings increase the \ncomplexity of the classi.cation task considerably as machine learning algorithms have performance and \naccuracy problems when confronted with large numbers of features. Predication is a standard technique \nto reduce the feature space of data values [27]. We de.ne nine predicates which are applied to Clarify \ndata and return values; the predicates map raw values to feature values. The predicates indicate whether \nthe raw value is equal to zero, equal to 1 or -1, is a small or large positive or negative integer, or \nis a pointer to the stack or heap. The thresholds for small and large positive and negative integers \nare arbitrary: any value with absolute value less than 100 is small, any value with absolute value greater \nthan 100 that is neither a stack or heap pointer is large . 3.2.1 Call-site pro.ling with predicated \nreturn values Call-site pro.ling with predicated return values (CSRV) counts pairs of call sites and \npredicated return values. If call-site A returns 255 one hundred times and returns -1 once, then the \nfeature <A, large int> has a count of 100 and the feature <A, equals -1> has a count of 1. 3.2.2 Stack \nscraping Stack scraping (SS) is a behavior pro.le that relies only on the dynamic data values from an \nexecution instance, rather than on control .ow. The insight behind stack scraping is that the stack contains \ncontrol .ow history in the form of return addresses (some of them residual in memory below the current \nstack pointer) and status information like function return codes. At the moment the program returns an \nerror code, its execution is paused, the range of memory allocated to the program stack is traversed, \nand a feature vector representing that instance of execu\u00adtion is created by applying predication to each \nword in the stack range. The representation trades some .delity for convenience and compactness, compared \nto instrumentation-based control .ow his\u00adtories. The scraper obtains the stack and heap bounds dynamically \n(from /proc/pid/maps on Linux) so it can differentiate point\u00aders to the stack and pointers to the heap. \nStack scraping is unique in Clarify feature sources in that it does not require instrumentation of the \nsource program. It imposes very little runtime overhead, but it is also the least accurate feature source. \n  4. Deployment issues This section discusses the different deployment scenarios for Clar\u00adify, and addresses \nsecurity issues of a Clarify deployment. 4.1 Forensic vs. live deployments Clarify can be deployed in \ntwo ways: to improve any error report a program can give (live deployment), or to improve a .xed set \nof error reports (forensic deployment). A live deployment will instru\u00adment an entire executable, sacri.cing \nsome performance to collect data about the entire application s behavior. A forensic deployment only \ncollects data that is known to help disambiguate a .xed set of error reports. 4.2 Security Clarify improves \nsoftware support, but raises security issues for users and software vendors. Users would like to keep \ntheir data and the way they use software private. Vendors do not want to divulge information about the \nstructure, control .ow, and support history of their product to users or competitors. Current software \nsupport systems suffer from this problem. In Windows Vista, there is a new control panel item called, \nProb\u00adlem reports and solutions, [17] which is a re.nement of the cur\u00adrent Windows support dialog. When \na program malfunctions, it can send a partial memory dump to Microsoft and Microsoft can send the user \na better error report. However the dump sent to Microsoft can contain arbitrarily sensitive data (e.g., \npasswords, credit card information, etc.). Microsoft s privacy statement currently discour\u00adages users \nwho are concerned about privacy from using their ser\u00advice [16]. Clarify decision trees can be evaluated \non Clarify behavior pro\u00ad.les in such a way that the end-user learns only the information re\u00adlated to \nhis error (and nothing about the software support history or control .ow of the application). The software \nvendor who provides the decision tree learns nothing about the user s execution. The se\u00adcurity details \nare in a separate paper [9], but the system allows trees with 255 nodes and 1,000 attributes to be securely \nevaluated for 28 seconds of online computation and 4.5 MB of bandwidth for the vendor, and 48 seconds \nof online computation time and 1.5 MB of bandwidth for the user.  5. Minimizing human effort Clarify \nrequires humans to label or generate examples of faulty error reports in order to train a machine learning \nclassi.er. Even without a classi.er, Clarify should help users. We describe nearest\u00adneighbor software \nsupport, an execution mode Clarify uses when it has no classi.er. The section next describes distributing \nthe work of labeling pro.les among a software support community. 5.1 Nearest neighbor software support \nClarify needs a certain number of labeled examples to build an accurate machine learning classi.er (exact \nnumbers are problem\u00addependent and quanti.ed in Section 7.4). Before it has trained a classi.er, Clarify \nuses nearest-neighbor search to match similar behavior pro.les. For instance, users of mpg321 can give \ntheir email addresses to a a support website. If a user has a problem that she does not understand, she \nsends her behavior pro.le to the site which runs Clarify. The site returns the emails of 5 other users \nwho opted in to the system and who likely experienced the same application behavior. (The system might \ngive out a particular email address only 3 times and take other steps to make sure participants are not \noverwhelmed with email or put on spam lists.) As the results in Section 7.6 show, nearest-neighbor search \nis sometimes highly effective, but it is not as accurate in general as building a classi.er. 5.2 Labeling \nbehavior pro.les The Clarify classi.er must be built from labeled behavior pro.les. There are three ways \nthis labeling can be done. Members of a support organization can do all labeling. This approach is human \nresource intensive, but provides high-quality labeling.  End-users can label their pro.les, distributing \nthe work across many more people, but enabling malicious or inept users to add noise in the form of incorrect \nlabels. End-user contributions can be graded by support staff or by peer reputation (like what is done \non current support websites).  Support engineers can write scripts to generate many variant inputs for \neach problem. All inputs exercise the same problem, so they all share the same label. We use this method \nto evaluate Clarify. It requires the most expertise, and the inputs are not guaranteed to accurately \nmodel real-life inputs.   6. Benchmarks Clarify is intended to improve the error reporting of complex, \nblack-box software components. To evaluate Clarify, we choose benchmarks that are common, heavily-used \nprograms for which non-exotic error conditions lead to misleading or non-existent error messages. That \ncommon utilities provide shoddy error reporting makes clear the motivation for Clarify. We also use Clarify \non programs that span the kernel/user boundary, containing user-level code that interacts with kernel \nmodules. Interaction across a protection boundary creates chal\u00adlenges for error reporting due to .xed \ninterfaces and the dif.culty of passing memory objects across the boundary. This section summarizes the \nbenchmarks and the kinds of prob\u00adlematic errors they report. We explain the behavior underlying the error \nreports it is this underlying behavior that Clarify is intended to discover. 6.1 User-level programs \ngcc. The GNU C compiler is a popular compiler, containing both hand-written and automatically generated \nsource code. Our ex\u00adperiments use version 3.1, executing only the compiler (the cc1 phase), using the \n.i .le output of the pre-processor, drawn from a pool of 4,070 .les pre-processed from the Linux kernel \n2.6.13 dis\u00adtribution. A corruptor script randomly modi.es correct source code to exhibit mistakes from \nfour error classes: adding a semicolon af\u00adter an if() that has an else clause, causing the compiler to \nfail on the else; omitting the closing curly bracket of a switch block causing an end of .le error; deletion \nof a semicolon, yielding a generic syntax error, often on a very different line from the removed semicolon; \nmisspelling a keyword which also generates a generic syntax error. All error classes result in confusing \nand imprecise er\u00adror messages. mpg321. mpg321 is an mp3 player for Linux. This benchmark has three failure \nmodes: .le format error (e.g. trying to play a wav .le as if it were an mp3), corrupted tag (mp3 metadata \nis stored in ID3 format tags, e.g., artist name), corrupted frames (mp3 frame data is corrupt). The Clarify \nclassi.er distinguishes between these three failure modes and normal execution. The application itself \ndoes not give any consistent error message for any of these error cases. LaTeX. Latex is a typesetting \nprogram widely used by the re\u00adsearch community. Its error reporting is known to be obscure. Rub\u00adber [34] \nis a tool that .lters LaTeX s output to make it more com\u00adprehensible to the user. However, many of LaTeX \ns error messages are generic and many have varied root causes, making it dif.cult for users to understand \nwhat went wrong and .x it. Our LaTeX benchmark has 26 ambiguous error cases, too many to summarize here, \nso we describe one illustrative example. A website [15] contains an explanation of all the classes. If \na table, array or eqnarray has more separator char\u00adacters (ampersands) than columns, LaTeX prints the \nobscure error message, !Extra alignment tab has been changed to \\cr . Most LaTeX books and most LaTeX \nsupport websites recommend check\u00ading the number of ampersands if a user receives this error. Some websites \nand books are helpful enough to suggest a missing end of row symbol \\\\ on the previous line. While forgetting \nthe double backslash will cause the error report, the error report is not unique: misuse of the \\cline \ncommand (a directive that draws a horizon\u00adtal line in the table) will result in the same message if one \nof the arguments to \\cline refers to a non-existent column in the table. Users who make the \\cline mistake \nget an error message that al\u00admost all support options say are due to one of two possible causes, even \nthough there is a third possible cause. Error reports are biased to their most likely cause, leaving \na user who executes a less likely scenario scratching her head, potentially for a long time. 6.2 Kernel \nbenchmarks To evaluate the applicability of Clarify across the user/kernel boundary, we chose three benchmarks \nthat depend on both user\u00adspace applications and kernel modules: iptables, iproute, and mount. iptables. \niptables is a popular open source Linux appli\u00adcation that does packet .ltering, network address translation, \nand other packet mangling. The policies for these operations are in kernel-space data structures, while \nthe user application is an in\u00adterface for the end-user. The error reporting interface between the kernel \nand the user is netlink. netlink simpli.es the interaction between the kernel and userspace, allowing \nanyone to create a kernel module and use the error reporting infrastructure. But netlink makes the error \nreporting interface rigid, forcing the kernel to reuse error codes like EEXIST.The EEXIST code means \nboth that a .le the user tried to create already exists, and that a new packet handling rule creates \na con.ict with the current rules. This ambiguity is especially confusing when an attempt to add a new \npacket handling rule returns the string, File exists because that is the default string for the EEXIST \nerror code in the C runtime library. The .rst behavior class for this benchmark includes the misuse of \ntable SNAT, DNAT,and SAME, all of which produce the generic Invalid arguments error. The second class \nis the misuse of MARK as a jump target, the third class is absence of the kernel module that is necessary \nto handle the user s request, and the fourth error class is using a forwarding chain name that does not \nexist. The kernel returns the same error code for the last three classes, which causes the application \nto print, No chain/target/match by that name . iproute. iproute controls the contents of the kernel routing \ntables. It has similar problems reporting errors as does iptables because it also uses the netlink error \nreporting standard. The .rst error class is adding routing rules that con.ict with existing rules; the \nsecond is adding an IP address that con.icts with existing IP address; the third is entry of a con.icting \nrouting table entry that should produce an error, but does not due to a bug in the kernel module. The \nerror message for both the .rst and the second error classes are RTNETLINK answers: File exists due to \nthe use of the overloaded return code EEXIST. nfs mount. Mounting a remote NFS server is a complicated \nop\u00aderation involving different kernel subsystems and cross-machine communication. It is no wonder that \nthe error reports generated from mount can be cryptic. The .rst error class is specifying the wrong port \nnumber, which produces the unrelated error messages NFSv3 not supported! or Can t read superblock . The \nsecond error class is a TCP/UDP mismatch between the server and the client, and the third error class \nis when the server is down. In both cases, the mount program prints RPC: Program not registered . App. \ninst. Er FP CSP CSRV PP CTP Table 2. Sizes of the Clarify behavior pro.les for each benchmark. The second \nand third columns show the number of instances (pro\u00adgram executions) and the number of error classes \nfor each bench\u00admark. The remaining columns show the number of features for each behavior representation. \nSS is not shown in the table since it always has 9 features. Kernel utilities only generate the .rst \ntwo behavior pro.les due to limitations in how the kernel can be instrumented. This error message makes \nsome sense because the remote proce\u00addure call daemon cannot .nd the proper program to handle the user \ns request, but this might not be obvious to normal end-users. The fourth error class is NFS version con.guration \nmismatch be\u00adtween the server and the client. We tested with NFSv2 and NFSv3. The error message is RPC: \nProgram/version mismatch; low ver\u00adsion = 1, high version = 2 . While the problem detected the NFS version \nmismatch, the error message reports the wrong version numbers which is likely to confuse a user diagnosing \nthe problem. 6.3 Complexity of Clarify benchmark dataset Table 2 summarizes the complexity of the Clarify \nbenchmark dataset. Each program has at least three ambiguous or mislead\u00ading error classes and one normal \nclass. Latex27 has 26 ambiguous error classes and 1 normal class. In general, more accurate pro.les have \nmore features. For instance, there are 533 functions in latex, but 6,802 call sites, and call-site pro.ling \nis more accurate than function pro.ling. Our benchmarks all have approximately equal number of in\u00adstances \nper error type. This distribution is not intended to model the frequency of bugs occurring in the .eld, \nbut rather trains the classi.er to distinguish among the given cases.  7. Evaluation We evaluate Clarify \naccording to four criteria: accuracy, perfor\u00admance, training cost, and scalability. First, Clarify must \ncorrectly classify program behaviors that share ambiguous error messages. Accuracy is summarized by the \nratio of behavior pro.les correctly classi.ed to the total number of pro.les (Section 7.1). A perfect \nclassi.er would correctly identify each error scenario from the be\u00adhavior pro.le for each benchmark. \nAs further validation of our clas\u00adsi.cation models, we examine the decision trees generated by Clar\u00adify \nin Section 7.3. We show that the tree tests program features that intuitively correlate with the observed \nbehavior. The accuracy of Clarify must come at an acceptable perfor\u00admance cost, which is measured in \nSection 7.2. A successful deploy\u00adment of the Clarify system should incur minimal overhead costs. Labeled \nexamples can be expensive to collect, as determining the error type of a given instance can require considerable \nhuman effort. Section 7.4 shows how many labeled behavior pro.les are required to generate a Clarify \nclassi.er. In the absence of any labeled data, Clarify employs a nearest-neighbor algorithm, where users \nare paired with other users who have experienced the same problem (Section 7.6). Section 7.7 shows data \nabout the use of Clarify as deployed in our lab.  CSP CTP App. Forensic Live Forensic Live  latex 0.6% \n5.3% 1.1% 97% mpg321 0.3% 1.2% 1.3% 67% gcc 1.0% 7.0% 9.9% 110% iptables 1.1% 3.2% N/A N/A iproute2 4.7% \n7.6% N/A N/A mount 1.1% 3.1% N/A N/A Table 3. Slowdown of programs running under the Clarify run\u00adtime \nusing CSP and CTP for a forensic deployment (which can only classify errors known during training), and \na live deployment (which can classify new errors found after deployment). Finally, section 7.5 examines \nhow the accuracy of Clarify s classi.ers scale with the number of error classes. The robustness of the \nClarify classi.ers is demonstrated by the relatively high accuracy obtained for the latex benchmark with \n81 classes. 7.1 Classi.cation accuracy Clarify uses decision trees to classify. Decision trees are nested \nif\u00adthen-else statements where each leaf corresponds to a single class prediction. An advantage of decision \ntrees (over more continuous methods like support vector machines) is their ease of interpreta\u00adtion. It \nis possible for a software engineer to validate the classi.er based on knowledge of program structure. \nFurther, in the context of our experiments with Clarify, decision trees are as accurate as other machine-learning \nmethods. Although Clarify s instrumenta\u00adtion computes thousands of features that describe each program \nex\u00adecution, the task of classifying error messages can be accomplished by analyzing only a few features. \nThis can be seen through the rel\u00adatively small size and high accuracy of Clarify s decision trees. In \ncontrast, methods that optimize over the entire feature set e.g. lo\u00adgistic regression or support vector \nmachines tend to yield over\u00ad.tted models with lower accuracy. Other algorithms that optimize over only \na subset of features, such as rule learning and boosted decision stumps, yield classi.ers we found to \nbe competitive with decision trees. Figure 3 shows the accuracy of user and kernel benchmarks, for several \ndifferent behavior representations. These tables report accuracy using 5-fold cross validation, a standard \ntechnique for evaluating classi.ers. The dataset is partitioned into .ve sections, the classi.er is trained \nand tested .ve times; it is trained on four sections of the data and its accuracy tested on the remaining \n.fth. The average of these .ve tests is the reported accuracy of the classi.er. The decision trees are \nbuilt using an implementation of the C4.5 algorithm [32] found in the WEKA machine learning package [39]. \nCall-tree pro.ling (CTP) demonstrates the best overall accuracy. Call-site pro.ling (CSP), path pro.ling \n(PP) and CTP have an accuracy of over 85% on every user-level benchmark, and call\u00adsite pro.ling has over \n85% accuracy for kernel benchmarks. 85% accuracy is a signi.cant help for improving error reports. To \nevaluate sampling, we present results for sampling FP and CSP, with a sampling rate of 10% (which is \ngenerous for sys\u00adtems that use sampling [27]). For example, the sampled function counts record one of \nevery ten function calls, uniformly at random. The sampled results are the stippled part of each bar, \nachieving lower classi.cation accuracy than non-sampled data for almost ev\u00adery benchmarks. The poor accuracy \nof sampling con.rms our intu\u00adition that sampling is the wrong approach for classifying program behavior, \nbecause Clarify must be sensitive to rare events. 7.2 Performance Table 3 shows the performance of live \nand forensic deployments of call-site pro.ling. All timing runs are on a dual-processor Intel Xeon 3.0GHz \nwith 2GB of RAM. Because there is no freely avail\u00adable static binary translator for the x86 architecture, \nthe experiment modi.es the assembly code of the programs to count call sites in exactly the way a binary \nmodi.cation tool would do it. On the x86 a count with a known address can be incremented with a single \ninstruction. The counters reside in a memory mapped .le, so the results can be collected after program \ntermination. Each benchmark runs several inputs to obtain a running time that is long enough to measure \naccurately: gcc compiles the 23 largest .i .les from the Linux 2.6.16 distribution, mpg321 decodes 256 \nframes of 200 mp3 .les, and LaTeX processes 5 .les with a total of 27,587 lines. We average the user \ntime of three executions. The remaining rows in Table 3 show benchmarks run on the 2.6.17 version of \nthe Linux kernel. The kernel behavior pro.le is built us\u00ading the kprobes [24], a dynamic instrumentation \npackage that is standard in Linux. Kprobes uses breakpoints so it is a more ex\u00adpensive form of instrumentation. \nWe use it to collect only function pro.ling and call-site pro.ling. Performance overhead is low for call-site \npro.ling, both for forensic and live deployments. The live deployment overhead for call-site pro.ling \nis modest, less than 7.6%. The live deployment overhead for CTP is much higher. Live deployment requires \nin\u00adstrumenting the entire binary, while forensic deployment chooses features that training runs indicate \nare necessary to disambiguate a known set of problems and that are cheap to collect, e.g., they reside \nin functions that are called infrequently. We use a published machine learning algorithm [19] that uses \ntraining data to .nd the minimum cost tree whose accuracy is within 1% with our cost\u00adoblivious tree. \nThe increase in performance from live to forensic for CTP is dramatic. The overhead of CSP is smaller \nto begin with, so the reduction is smaller, but the forensic overhead of CSP for user-level programs \nis less than 1%. The high cost of breakpoints in the kernel accounts for the higher overhead relative \nto user-level programs. Forensic deployment is an effective means of deploy\u00ading richer behavior pro.les \nlike CTP at reasonable levels of perfor\u00admance cost.  7.3 Verifying the machine learning model Machine \nlearning algorithms train classi.ers without any domain knowledge regarding the underlying semantics \nof the program s be\u00adhavior. It is possible for a classi.er to fail miserably on unseen data because the \nclassi.er examines features that are semantically un\u00adrelated to the behavior it classi.es. To make sure \nthat Clarify clas\u00adsi.ers use program features that intuitively relate to the behaviors they classify, \nwe examined several classi.ers by hand. Classi.ers trained using function pro.ling and call-tree pro.ling \nfor the mp3 player mpg321 are shown in Figure 4. The trees show how each behavior pro.le provides different \nclues to the classi.er about the same underlying behavior. The function pro.ling tree is composed of \na simpler set of rules that depict differences in control .ow across the four error classes. At the root \nof the tree, the function mad layer III provides near perfect discriminative information for the wav \nerror class: the mad layer III routine is part of the libmad library and is called when the audio frame \ndecoder runs. Since the wav format is among the formats not supported by mpg321, it will not success\u00adfully \ndecode any audio frames, and the libmad library will never call mad layer III.The id3 tag delete routine \ndifferenti\u00adates between the corrupted tag and and other classes. The ID3 tag parser in the libid3tag \nlibrary dynamically allocates memory to represent tags and frees them with id3 tag delete.If tag pars\u00ading \nfails, the memory for a tag is not allocated. Since no tag parsing      100 100 95 Accuracy (%) \n 95 90 FP FP90 8585 80 80 75 75 70 70 65 6065 5560 Accuracy (%) CSP  CSP PP  CTP CSRV 50 \n55  SS 40 45 50 45  35 latex81 latex26 gcc mpg321 iptables iproute2 mount Figure 3. The .gure shows \nthe accuracy of the classi.er used to distinguish the error cases, based on behavior pro.les, for each \nbenchmark. For each benchmark a classi.er is built using different behavior pro.les: function pro.ling \n(FP), call-site pro.ling (CSP), path pro.ling (PP), call-tree pro.ling (CTP), call-site pro.ling with \npredicated return values (CSRV), and stack scraping (SS). The .gure also presents sampled versions of \nfunction pro.ling and call site pro.ling with a sampling rate of 10% (in the stippled, lower bar in the \nstacked FP or CSP entry). succeeds in the corrupted frames case, id3 tag delete is never # Classes Accuracy \nCreation Time  called to free the tag memory, making its absence discriminative for that class. The \nlibmad audio library s default error handler error default is used if the application does not specify \none. mpg321 does not specify its own error handler, so the presence of the function indicates corrupted \naudio frames, and its absence indicates the corrupted id3 tags case. Finally, III freqinver, which performs \nsubband frequency inversion for odd sample lines, is called very frequently as part of the normal process \nof decoding audio frame data. When there are corrupted frames, this function is called less frequently, \nand the decision tree algorithm .nds an ap\u00adpropriate threshold value to separate the normal from the \ncorrupted case. The decision tree built on call-tree pro.ling data has a richer combination of data sources \nthan function pro.ling. Call-tree pro\u00ad.ling uses the presence of the libmad library function III side\u00adinfo \n(which decodes frame side information from a bitstream) calling the utility function mad bit read as \nan indicator of suc\u00adcessful audio frame decoding. The lack of that calling pattern reliably indicates \na .le format error. The corrupted frames class is once again differentiated from the normal class by \na thresh\u00adold value on a subtree of libmad functions that will only be called during successful decoding \nof audio frame data, such as III scalefactors, the discrete cosine transform function fastsdct, III huffdecode, \nand so on. The libmad func\u00adtion scan encapsulates the process of reading mp3 .les. A CTP rule (decoded \nbitvector) wherein scan calls a function that calls a number of low-level stream manipulation routines \nsuch as mad\u00ad bit read,and mad timer set, and so on, provides discrimi\u00adnative power in combination with \na similarly complex control .ow pattern in main for the corrupted tags error class. The decision tree \nnode whose CTP rule involves main, id3 get tag,and so on differentiates between normal and error conditions \nfor the handling of ID3 tags, while the decision tree node whose CTP rule involves scan discriminates \nbetween successful and unsuccessful audio decoding. The high level pattern exposed by these rules is \nthe combination of failed ID3 tag parsing with successful audio decoding, which precisely describes the \ncorrupted tag error class.  7.4 How many labeled behavior pro.les are needed? The classi.ers used by \nClarify are trained with labeled behavior pro.les. Labeling pro.les generally requires human effort, \nso it should be minimized. In general, classi.ers trained with fewer labeled training instances will \nresult in less accurate models. In this  Table 4. The accuracy and time to create the classi.er as \nthe number of behaviors is increased in the LaTeX benchmark. section, we investigate the tradeoff between \nclassi.cation accuracy and the amount of training data used in building the classi.er. Figure 5 plots \nthe classi.cation accuracy of the latex benchmark as a function of the number of instances used in training \n(the benchmark includes 75 of the 81 distinct error classes) . The C4.5 algorithm used to build the decision \ntree is surprisingly robust: with as few as 15 examples per class, the algorithm achieves an accuracy \nof 86%. Looking at the legend in Figure 5, we can see that to achieve accuracy within 1% of the maximum, \nonly a small subset of the training data is required. For example, gcc needs only 105 instances to attain \nthe accuracy level of 88.9% which is within 1% of the accuracy reached when we use all of the 300 available \nexamples per class. A human does not need to label each behavior pro.le individ\u00adually. For our training \nsets we use a script to induce errors in the program input, producing large numbers of training examples \nwith little human involvement. However, inducing errors by a script is not necessarily an accurate model \nfor the errors that Clarify would see in deployment. 7.5 Scalability In this section we analyze how \nClarify scales as the number of error classes increases. LaTeX has 247 unique error messages, and we \nevaluate the scalability on 81 behavior classes about one third of all possible LaTeX errors. Table 4 \nshows how model creation time and classi.cation ac\u00adcuracy scale as the number of error classes increases. \nWe consider subsets of error classes with varying sizes. For each size, we picked 10 random subsets of \nerror classes and ran our experiments. The curves shown in the graph are the average of the results for \neach size. We can see that as the number of error classes increases the accuracy drops from 97.8% to \n93.6%. This decrease is acceptable      Function pro.ling Call-tree pro.ling Figure 4. Decision \ntrees produced for the mpg321 benchmark. Dotted lines are taken when the normalized count of the feature \nvalue is less than or equal to a threshold, while the solid line is taken when it is greater than the \nthreshold. The threshold is determined automatically for each benchmark by the decision tree algorithm, \nand can be different for each node in the tree. Clear boxes are features. FP features are normalized \nfunction counts, and call-tree pro.ling features are normalized counts of CTP subtrees (represented by \nthe symbolic tree names in brackets, with function names for nodes in each call tree). Shaded boxes are \nerror classes. considering that the number of behaviors has increased by a factor of 8. The training \ntime of the model increases from under 30 minutes to more than 18 hours as the number of error classes \nincreases from 10 to 81. This increase does not hinder scalability since the model is trained of.ine. \nOf greater practical concern is the execution time needed to evaluate the decision tree, as this largely \ndetermines the amount of processing done at the client end. Our experiments show that the time to execute \nthe models averaged 10ns, with a maximum of 21ns, which is imperceptible for almost any application. \nClarify scales to nearly one hundred error behaviors without much loss in accuracy or substantial increase \nin processing time. 7.6 Nearest neighbor software support In contrast to the decision trees used by \nClarify s classi.ers which rely on only a small subset of all features, nearest-neighbor al\u00adgorithms \nrely on averages over all features. For example, the Eu\u00adclidean distance between two instances, a popular \nmetric used for nearest-neighbor searches, is a function of the average of the squares of the difference \nbetween each pair of feature values. Such Percentage Accuracy 100 90 80 70 60 50 40 30 20 10 0 Figure \n5. The curve shows how the accuracy increases as the num\u00adber of training instances per error class is \nincreased. The dataset is the latex benchmark with 75 classes. The text on the graph gives the minimum \nnumber of training instances needed for a benchmark to achieve accuracy within 1% of accuracy obtained \nusing all the training data. App. FP CSP CTP CSRV Table 5. In the absence of labeled training data, \nClarify uses a nearest-neighbor algorithm with linear-regression based feature scaling. This table shows \nthe expected number of correctly classi\u00ad.ed neighbors for a .ve-nearest-neighbor search. distance functions \nare particularly susceptible to differences of scale among the various features. In Clarify, features \ntake on vastly different scales: some features may have a count under ten, while others may have upwards \nof one million occurrences. Furthermore, for some features, the count is a function of the length of \nthe program execution, and for others it is independent of program execution. For example, a parsing-related \nfeature for gcc will be called many more times for a longer .le that contains many repetitions of particular \nconstruct, than a shorter .le. Some sections of code e.g. initialization functions will be called a (roughly) \nconstant number of times and thus will take on values independent of the program execution length. To \novercome such scaling challenges, Clarify employs a linear regression-based feature scaling method. For \neach feature y,a least-squares line is .tted to correlate each feature value with its corresponding program \nexecution length x (de.ned as the sum of all feature values of a given execution instance). The feature \nvalue is normalized to be the scaled difference between the feature value y and the .tted feature value \nf(x). The scaling factor is determined such that the variance of each resulting feature is one. We note \nthat for features that have no correlation with program length, the linear regression step will have \nno effect on the .nal normalized feature values. Table 5 gives the expected number of correctly classi.ed \nneigh\u00adbors for a nearest neighbor search returning .ve neighbors. Eu\u00adclidean distance is used. For some \nbenchmarks with many classes (the LaTeX benchmark in table 5 has 27 classes), accuracy of the nearest-neighbor \nsearch is somewhat lower. In such cases, a larger number of neighbors should be returned. 7.7 Deployment \nTo begin understanding the performance of Clarify in a deployed environment, we created a version of \nLaTeX that includes static instrumentation and a small runtime to generate call-site informa\u00adtion. We \ndeployed the version of LaTeX to a user base of 6 users over a period of 3 weeks. Our deployed version \nof LaTeX encoun\u00adtered 57 distinct error inputs ranging over 17 error classes and was able to classify \n46% (26/57) of them correctly. LaTeX has 247 er\u00adror messages the experiment was not limited to unclear \nor am\u00adbiguous messages. Classifying nearly 50% of a program s behavior correctly is much more dif.cult \nthan disambiguating a small num\u00adberoferrorbehaviors.  8. Related work We .rst contrast Clarify to several \nsystems that appear similar. Clarify improves error reporting by classifying program behavior, it does \nnot .nd program bugs [20, 27, 1, 14]. An ambiguous error message or return code might meet the speci.cation \nfor a program (e.g., the netlink standard for error reporting). Clarify does not attempt to .nd the root \ncause of program faults [12, 31], miscon.gurations [38, 25], or program crashes [10, 29, 30, 7]. Its \naim is to classify the application behavior to help the developer or end-user get better error reports \nwhen these events happen. The remainder of this section compares Clarify with problem di\u00adagnosis systems, \nand systems that classify program behavior. Clar\u00adify does help software problem diagnosis and it classi.es \nprogram behavior. 8.1 Problem diagnosis systems A group at Microsoft Research correlates low-level system \nevents with error reports to automate problem diagnosis [40, 41], just as Clarify does. They currently \nfocus only on forensic deployments (in our terminology), and on building models from sequences of system \ncalls. Clarify uses control-.ow and data from the program, which allows it to deal with errors that involve \nonly user code. Ph [36] also uses sequences of system calls to build a model, though their model detects \nhost intrusions. While system calls are a good representation of certain types of program behavior, many \nprograms make few systems calls (e.g., SPEC). Because every named system call has wrapper functions from \nuser-space libraries, Clarify can detect system calls by detecting function calls to the wrapper functions, \ngiving it a richer input source to determine program behavior. Statistical bug isolation [27, 26] correlates \nlow-level applica\u00adtion behavior with application behavior (bugs) and builds a model, as Clarify does. \nStatistical bug isolation requires a special compiler to insert invariant checks into the program, while \nClarify records a small amount of control-.ow and data continuously. Statistical bug isolation samples \nthe invariants it inserts to get good perfor\u00admance. Section 7.1 demonstrates a sharp loss of accuracy \nif Clar\u00adify uses sampling. Statistical bug isolation must eliminate sub-bug and super-bug predictors; \nClarify has an analogous struggle to gain enough training instances to isolate the program behavior created \nby the error condition. The systems could be used together to gather statistical data on crashes and \nprovide better error messages for crashes and other misbehaviors. DIDUCE [20] uses dynamic program invariants \nto detect pro\u00adgram behavioral anomalies. The anomalies can indicate program bugs, but at a performance \nslowdown of 6 20\u00d7. Clarify is much faster and can classify program behavior that is not anomalous. Stack \nbacktraces are used by many remote diagnostic systems like Dr. Watson [29], Microsoft s online crash \nanalysis [30] and GNOME s bug-buddy [7]. IBM has a system to classify stack backtraces harvested on a \ncrash [10], and the technology has been deployed in their TrapFinder tool. Their motivation is similar \nto Clarify s reduce the human effort needed to match problems from different program executions. Clarify \ndiagnoses a wider range of problems than crashes, and it operates on behavior pro.les, which are a richer \nsource of data than stack backtraces. 8.2 Classifying program behavior Classifying program behavior \nhas received attention in the software engineering literature. Podgurski et al. [31] identify a similar \nmo\u00adtivation to Clarify and they also investigate gcc behavior. Clar\u00adify is more accurate (over 85 100% \naccurate, as compared to 24 96%), and is more of a complete system, designed to address the problem of \nimproving error reporting. Bowring et al. [8] models software behavior as Markov models using control \n.ow between basic blocks and then uses active learning to cluster the models. Markov models use probabilities \nwhich make them insensitive to rare events. Clarify needs sensitivity to rare events because rare events \noften characterize an error behavior see the sampling re\u00adsults in Section 7.1. Bowring et. al. evaluate \ntheir method on 33 versions of SPACE, which is a very small 6,200 line program. Liu et al. [28] use program \nbehavior graphs as features for a machine-learning model just as Clarify uses data related to pro\u00adgram \ncontrol .ow. The number of program behavior graphs grows quickly with program size, and can become computationally \nin\u00adtractable even for the small Siemens programs [23] used to evaluate their method. SimPoint [35] characterizes \nthe phase behavior of applications using basic block execution counts to maintain the accuracy of architectural \nsimulation while executing fewer instructions. The types of program behavior it detects are coarse-grained \nand occurs over much longer time windows than the errors that Clarify de\u00adtects. SimPoint can reduce its \ndataset to 15 dimensions and main\u00adtain phase-detection accuracy. Clarify s classi.ers must be sensitive \nto small, localized changes in behavior that form the signature of an error behavior. As seen in Table \n2, Clarify s representations have tens of thousands of features. We veri.ed that using random projec\u00adtion \nto reduce the feature count, like SimPoint does, dramatically reduces Clarify s accuracy. Program paths \n[5] have been used to analyze runtime program behavior. Path Spectra [33] approximate an execution s \nbehavior with the occurrence (or frequency) of the individual paths. Spectral differences have been used \nto identify the portions of a program s execution that differ with different inputs, notably, during \nY2K test\u00ading [21]. Path Spectra focused on identifying path differences be\u00adtween several program runs, \nwhereas Clarify s novel use of path pro.ling uses machine learning to identify which paths are com\u00admon \nto each error class. Clarify s call-site pro.ling is much more ef.cient and nearly as accurate as path \npro.ling.  9. Conclusion We present Clarify, a system that improves the error reporting of black-box \nsystems, e.g., third-party libraries, the operating system, and external programs. Our Clarify prototype \naccurately and ef\u00ad.ciently classi.es the behavior of all of these systems, enabling improved error reporting. \n  Acknowledgments Thanks to William Cook for help with writing. Thanks to Peter Stone, Raymond Mooney \nand Kathryn McKinley for feedback on earlier drafts of the paper. This research has been supported by \na gift from Microsoft s Phoenix compiler group, a DARPA grant from the architectures for cognitive information \nprocessing program, and by NSF grant CNS-0615104. References [1] M. K. Aguilera, J. C. Mogul, J. L. \nWiener, P. Reynolds, and A. Muthitacharoen. Performance debugging for distributed systems of black boxes. \nIn SOSP, Bolton Landing, NY, Oct. 2003. [2] G. Ammons, T. Ball, and J. R. Larus. Exploiting hardware \nperformacne counters with .ow and context sensitive pro.ling. In PLDI 97, pages 4 16, June 1997. [3] \nAndrew Ayers, Christopher Metcalf, Junghwan Rhee, Richard Schooler, Anant Agarwal, and Emmett Witchel. \nTraceback: First fault diagnosis by reconstruction of distributed control .ow. In PLDI, June 2005. [4] \nV. Bala, E. Duesterwald, and S. Banerjia. Dynamo: a transparent dynamic optimization system. In PLDI, \npages 1 12, 2000. [5] T. Ball and J. R. Larus. Ef.cient path pro.ling. In MICRO, 1996. [6] R. Barrett, \nE. Haber, E. Kandogan, P. P. Maglio, M. Prabaker, and L. A. Takayama. Field studies of computer system \nadministrators: Analysis of system management tools and practices. In ACM CSCW (Computer-supported Cooperative \nWork), 2004. [7] J. Berkman. Bug-buddy GNOME bug-reporting utility, 2004. http://directory.fsf.org/ \nAll_Packages_in_Directory/bugbuddy.html. [8] J. F. Bowring, J. M. Rehg, and M. J. Harrold. Active learning \nfor automatic classi.cation of software behavior. In ISSTA, Jul 2004. [9] Justin Brickell, Donald E. \nPorter, Vitaly Shmatikov, and Emmett Witchel. Secure remote software diagnostics, Under review. [10] \nM. Brodie, Sheng Ma, G. Lohman, L. Mignet, N. Modani, M. Wild\u00ading, J. Champlin, and P. Sohn. Quickly \n.nding known software problems via automated symptom matching. In ICAC 05, pages 101 110, 2005. [11] \nDerek Bruening, Timothy Garnett, and Saman Amarasinghe. An infrastructure for adaptive dynamic optimization. \nIn CGO-03, 2003. [12] Y. Brun and M. D. Ernst. Finding latent code errors via machine learning over program \nexecutions. In ICSE, 2004. [13] Bryan Cantrill and Mike Shapiro and Adam Leventhal. Dtrace, 2006. http://www.genunix.org/wiki/index.php/ \nDTrace_FAQ. [14] Trishul M. Chilimbi and Vinod Ganapathy. Heapmd: Identifying heap-based bugs using anomaly \ndetection. In ASPLOS 06, 2006. [15] Latex Error Classes. http://www.cs.utexas.edu/users/ habals/clarify/latex_errors.html, \n2006. [16] Microsoft corporation. Privacy statement for the microsoft error reporting service, 2006. \n[17] Microsoft corporation. Reporting and solving computer problems, 2006. [18] Microsoft Corporation. \nWhat information is sent to Microsoft when I report a problem?, 2006. [19] Jason V. Davis, Jungwoo Ha, \nChristopher J. Rossbach, Hany E. Ramadan, and Emmett Witchel. Cost-sensitive decision tree learning for \nforensic classi.cation. In ECML, 2006. [20] S. Hangal and M. S. Lam. Tracking down software bugs using \nautomatic anomaly detection. In ICSE, 2002. [21] M. J. Harrold, G. Rothermel, K. Sayre, R. Wu, and L. \nYi. An empirical investigation of the relationship between fault-revealing test behavior and differences \nin program spectra. In Journal of Software Testing, Veri.cation and Reliability, vol 10, no 3, 2000. \n[22] J. Humphreys and V. Turner. On-demand enterprises and utility computing: A current market assessment \nand outlook. Technical report, IDC, Jul 2004. [23] M. Hutchins, H. Foster, T. Goradia, and T. Ostrand. \nExperiments on the effectiveness of data.ow-and control.ow-based test adequacy criteria. In ICSE, 1994. \n[24] Jim Keniston and Prasanna S Panchamukhi. Kernel Probes (Kprobes), 2006. Documentation/kprobes.txt. \n[25] N. Lao, J. Wen, W. Ma, and Y. Wang. Combining high level symptom descriptions and low level state \ninformation for con.guration fault diagnosis. In LISA, 2004. [26] B. Liblit, A. Aiken, A.X. Zheng, and \nM. I. Jordan. Bug isolation via remote program sampling. In PLDI, 2003. [27] B. Liblit, M. Naik, A. X. \nZheng, A. Aiken, and M. I. Jordan. Scalable statistical bug isolation. In PLDI, 2005. [28] C. Liu, X. \nYang, H.Yu, J. Han, and P. S. Yu. Mining behavior graphs for backtrace of noncrashing bugs. In Proc. \nof 2005 SIAM Int. Conf. on Data Mining (SDM05), 2005. [29] Microsoft Corporation. Dr. Watson Overview, \n2002. http://www. microsoft.com/TechNet/prodtechnol/winxppro/ proddocs/drwatson_overview.asp. [30] Microsoft \nCorporation. Online Crash Analysis, 2004. http:// oca.microsoft.com/. [31] A. Podgurski, D. Leon, P. \nFrancis, W. Masri, M. Minch, J. Sun, and B. Wang. Automated support for classifying software failure \nreports. In ICSE, 2003. [32] R. Quinlan. C4.5: programs for machine learning. Morgan Kaufmann Publishers, \n1992. [33] T. Reps, T. Ball, M. Das, and J. Larus. The use of program pro.ling for software maintenance \nwith applications to the year 2000 problem. In M. Jazayeri and H. Schauer, editors, ESEC/FSE 97, pages \n432 449. Springer Verlag, 1997. [34] Rubber. http://www.pps.jussieu.fr/ beffara/soft/ rubber, 2007. [35] \nTimothy Sherwood, Erez Perelman, Greg Hamerly, and Brad Calder. Automatically characterizing large scale \nprogram behavior. In ASPLOS, Oct 2002. [36] A. Somayaji and S. Forrest. Automated response using system-call \ndelays. In Proceedings of 9th Usenix Security Symposium, August 2000. [37] Ariel Tamches and Barton P. \nMiller. Fine-grained dynamic instrumentation of commodity operating system kernels. In OSDI, pages 117 \n130, 1999. [38] H. J. Wang, J. C. Platt, Y. Chen, R. Zhang, and Y. Wang. Automatic miscon.guration troubleshooting \nwith PeerPressure. In OSDI, 2004. [39] I. Witten and E. Frank. Data Mining: Practical machine learning \ntools with Java implementations. Morgan Kaufmann, San Francisco, 2000. [40] C. Yuan, N. Lao, J. Wen, \nJ. Li, Z. Zhang, Y. Wang, and W. Ma. Automated known problem diagnosis with event traces. MSR-TR\u00ad2005-81, \n2005. [41] C. Yuan, N. Lao, J. Wen, J. Li, Z. Zhang, Y. Wang, and W. Ma. Automated known problem diagnosis \nwith event traces. In EuroSys, 2006. [42] Xiaotong Zhuang, Mauricio J. Serrano, Harold W. Cain, and Jong-Deok \nChoi. Accurate, ef.cient, and adaptive calling context pro.ling. In PLDI, 2006.  \n\t\t\t", "proc_id": "1250734", "abstract": "<p>An error occurs when software cannot complete a requested action as a result of some problem with its input, configuration, or environment. A high-quality error report allows a user to understand and correct the problem. Unfortunately, the quality of error reports has been decreasing as software becomes more complex and layered. End-users take the cryptic error messages given to them by programsand struggle to fix their problems using search engines and support websites. Developers cannot improve their error messages when they receive an ambiguous or otherwise insufficient error indicator from a black-box software component.</p> <p>We introduce Clarify, a system that improves error reporting by classifying application behavior. Clarify uses minimally invasive monitoring to generate a <i>behavior profile</i>, which is a summary of the program's execution history. A machine learning classifier uses the behavior profile to classify the application's behavior, thereby enabling a more precise error report than the output of the application itself.</p> <p>We evaluate a prototype Clarify system on ambiguous error messages generated by large, modern applications like gcc, La-TeX, and the Linux kernel. For a performance cost of less than 1% on user applications and 4.7% on the Linux kernel, the proto type correctly disambiguates at least 85% of application behaviors that result in ambiguous error reports. This accuracy does not degrade significantly with more behaviors: a Clarify classifier for 81 La-TeX error messages is at most 2.5% less accurate than a classifier for 27 LaTeX error messages. Finally, we show that without any human effort to build a classifier, Clarify can provide nearest-neighbor software support, where users who experience a problem are told about 5 other users who might have had the same problem. On average 2.3 of the 5 users that Clarify identifies have experienced the same problem.</p>", "authors": [{"name": "Jungwoo Ha", "author_profile_id": "81331493734", "affiliation": "The University of Texas at Austin, Austin, TX", "person_id": "P871678", "email_address": "", "orcid_id": ""}, {"name": "Christopher J. Rossbach", "author_profile_id": "81331502710", "affiliation": "The University of Texas at Austin, Austin, TX", "person_id": "P871620", "email_address": "", "orcid_id": ""}, {"name": "Jason V. Davis", "author_profile_id": "81317487671", "affiliation": "The University of Texas at Austin, Austin, TX", "person_id": "P798503", "email_address": "", "orcid_id": ""}, {"name": "Indrajit Roy", "author_profile_id": "81332524691", "affiliation": "The University of Texas at Austin, Austin, TX", "person_id": "PP39072136", "email_address": "", "orcid_id": ""}, {"name": "Hany E. Ramadan", "author_profile_id": "81330497010", "affiliation": "The University of Texas at Austin, Austin, TX", "person_id": "P871627", "email_address": "", "orcid_id": ""}, {"name": "Donald E. Porter", "author_profile_id": "81100437540", "affiliation": "The University of Texas at Austin, Austin, TX", "person_id": "PP40034546", "email_address": "", "orcid_id": ""}, {"name": "David L. Chen", "author_profile_id": "81544150456", "affiliation": "The University of Texas at Austin, Austin, TX", "person_id": "P871669", "email_address": "", "orcid_id": ""}, {"name": "Emmett Witchel", "author_profile_id": "81100230582", "affiliation": "The University of Texas at Austin, Austin, TX", "person_id": "PP14089909", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1250734.1250747", "year": "2007", "article_id": "1250747", "conference": "PLDI", "title": "Improved error reporting for software that uses black-box components", "url": "http://dl.acm.org/citation.cfm?id=1250747"}