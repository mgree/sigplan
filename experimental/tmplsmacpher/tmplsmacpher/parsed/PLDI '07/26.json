{"article_publication_date": "06-10-2007", "fulltext": "\n The Ant and the Grasshopper: Fast and Accurate Pointer Analysis for Millions of Lines of Code Ben Hardekopf \nUniversity of Texas at Austin benh@cs.utexas.edu Abstract Pointer information is a prerequisite for \nmost program analyses, and the quality of this information can greatly affect their precision and performance. \nInclusion-based (i.e. Andersen-style) pointer analysis is an important point in the space of pointer \nanalyses, offering a potential sweet-spot in the trade-off between precision and performance. However, \ncurrent techniques for inclusion-based pointer analysis can have dif.culties delivering on this potential. \nWe introduce and evaluate two novel techniques for inclusion\u00adbased pointer analysis one lazy, one eager1 \nthat signi.cantly improve upon the current state-of-the-art without impacting pre\u00adcision. These techniques \nfocus on the problem of online cycle de\u00adtection, a critical optimization for scaling such analyses. Using \na suite of six open-source C programs, which range in size from 169K to 2.17M LOC, we compare our techniques \nagainst the three best inclusion-based analyses described by Heintze and Tardieu [11], by Pearce et al. \n[21], and by Berndl et al. [4]. The combination of our two techniques results in an algorithm which is \non average 3.2\u00d7 faster than Heintze and Tardieu s algorithm, 6.4\u00d7 faster than Pearce et al. s algorithm, \nand 20.6\u00d7 faster than Berndl et al. s al\u00adgorithm. We also investigate the use of different data structures \nto repre\u00adsent points-to sets, examining the impact on both performance and memory consumption. We compare \na sparse-bitmap implementa\u00adtion used in the GCC compiler with a BDD-based implementation, and we .nd \nthat the BDD implementation is on average 2\u00d7 slower than using sparse bitmaps but uses 5.5\u00d7 less memory. \nCategories and Subject Descriptors F.3.2 [Logics and Meanings of Programs]: Semantics of Programming \nLanguages Program analysis General Terms Algorithms, Performance Keywords pointer analysis 1. Introduction \nPointer information is a prerequisite for most program analyses, in\u00adcluding modern whole-program analyses \nsuch as program veri.ca\u00adtion and program understanding. The precision and performance of 1 Hence the \nreference to Aesop s fable The Ant and the Grasshopper [1]. Permission to make digital or hard copies \nof all or part of this work for personal or classroom use is granted without fee provided that copies \nare not made or distributed for pro.t or commercial advantage and that copies bear this notice and the \nfull citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. PLDI 07 June 11 13, 2007, San Diego, California, \nUSA. Copyright c . 2007 ACM 978-1-59593-633-2/07/0006. . . $5.00 Calvin Lin University of Texas at Austin \n lin@cs.utexas.edu these client analyses depend heavily on the precision of the pointer information \nthat they re given [24]. Unfortunately, precise pointer analysis is NP-hard [14] any practical pointer \nanalysis must ap\u00adproximate the exact solution. There are a number of different ap\u00adproximations that can \nbe made, each with its own trade-off between precision and performance [12]. The most precise analyses \nare .ow-sensitive respecting control\u00ad.ow dependencies and context-sensitive respecting the seman\u00adtics \nof function calls. Despite a great deal of work on both .ow\u00adsensitive and context-sensitive algorithms \n[6, 8, 13, 15, 20, 27, 28, 29, 30], none has been shown to scale to programs with millions of lines of \ncode, and most have dif.culty scaling to 100,000 lines of code. If .ow-and context-sensitivity aren t \nfeasible for large pro\u00adgrams, we re left to consider .ow-and context-insensitive analy\u00adses. The most \nprecise member of this class is inclusion-based (i.e., Andersen-style) pointer analysis [2], which is \nclosely related to computing the dynamic transitive closure of a graph. Inclusion con\u00adstraints are generated \nfrom the program code and used to construct a constraint graph, with nodes to represent each program \nvari\u00adable and edges to represent inclusion constraints between the vari\u00adables. Indirect constraints those \ninvolving pointer dereferences can t be represented, since points-to information isn t yet available. \nPoints-to information is gathered by computing the transitive clo\u00adsure of the graph; as more information \nis gained, new edges are added to the constraint graph to represent the indirect constraints. The transitive \nclosure of the .nal graph yields the points-to solu\u00adtion. The exact algorithm is explained in Section \n3. Inclusion-based pointer analysis has a complexity of O(n 3);the key to scaling it is to reduce the \ninput size i.e. make n smaller while maintaining soundness. The primary method for reducing n is online \ncycle detection: the analysis looks for cycles in the constraint graph and collapses their components \ninto single nodes. Because the algorithm computes the transitive closure, all nodes in the same cycle \nare guaranteed to have identical points-to sets and can safely be collapsed together. The method used \nto .nd and collapse cycles during the analysis has a signi.cant impact on the algorithm s performance. \nIn this paper we introduce a new inclusion-based pointer anal\u00adysis algorithm that employs a novel method \nof detecting cycles called Lazy Cycle Detection (LCD). Rather than aggressively seek\u00ading out cycles in \nthe constraint graph, LCD piggybacks on top of the transitive closure computation, identifying potential \ncycles based on their effect identical points-to sets. This lazy nature sig\u00adni.cantly reduces the overhead \nof online cycle detection. We also introduce a second method for detecting cycles called Hybrid Cycle \nDetection (HCD). Hybrid Cycle Detection of.oads work to a linear-time of.ine analysis a static analysis \ndone prior to the actual pointer analysis. The actual pointer analysis is then able to detect cycles \nwithout performing any graph traversal. Thus, HCD eagerly pays a small up-front cost to avoid a large \namount of later work. While HCD can be used on its own, its true power lies in the fact that it can easily \nbe combined with other inclusion-based pointer analyses to signi.cantly improve their performance. We \ncompare our new techniques against a diverse group of inclusion-based pointer analyses representing the \ncurrent state\u00adof-the-art. This group includes algorithms due to Heintze and Tardieu [11] (HT), Pearce \net al. [21] (PKH), and Berndl et al. [4] (BLQ). All of these algorithms are explained in Section 2. This \npaper makes the following contributions to inclusion-based pointer analysis: We introduce Lazy Cycle \nDetection, which recognizes that the effects of cycles identical points-to sets can be used to de\u00adtect \nthem with extremely low overhead. On average LCD is faster than all three current state-of-the-art inclusion-based \nanalyses: 1.05\u00d7 faster than HT, 2.1\u00d7 faster than PKH, and 6.8\u00d7 faster than BLQ.  We introduce Hybrid \nCycle Detection, which dramatically re\u00adduces the overhead of online cycle detection by carefully par\u00adtitioning \nthe task into of.ine and online analyses. On average HCD improves the performance of HT by 3.2\u00d7,PKH by \n5\u00d7, BLQ by 1.1\u00d7,and LCDby3.2\u00d7. HCD is the .rst cycle de\u00adtection technique that has been shown to be practical \nfor BDD\u00adbased program analyses like BLQ.  We provide the .rst empirical comparison of the three current \nstate-of-the-art inclusion-based pointer analysis algorithms, namely, HT, PKH, and BLQ. We .nd that HT \nis the fastest 1.9\u00d7 faster than PKH and 6.5\u00d7 faster than BLQ.  We demonstrate that an algorithm that \ncombines Lazy Cy\u00adcle Detection and Hybrid Cycle Detection (LCD+HCD) is the fastest of the algorithms \nthat we studied and can easily scale to programs consisting of over a million lines of code. It is on \nav\u00aderage 3.2\u00d7 faster than HT, 6.4\u00d7 faster than PKH, and 20.6\u00d7 faster than BLQ.  We investigate the memory \nconsumption of the various analy\u00adses, experimenting with two different data structures for repre\u00adsenting \npoints-to sets: sparse bitmaps as currently used in the GCC compiler, and a BDD-based representation. \nFor the algo\u00adrithms that we study, we .nd that the BDD-based representation is an average of 2\u00d7 slower \nthan sparse bitmaps but uses 5.5\u00d7 less memory.  The rest of this paper is organized as follows. In Section \n2 we place our techniques in the context of prior work. Section 3 pro\u00advides background about inclusion-based \npointer analysis. Section 4 describes our two new techniques for detecting cycles, and Sec\u00adtion 5 presents \nour experimental evaluation.  2. Related Work Inclusion-based pointer analysis is described by Andersen \nin his Ph.D. thesis [2], in which he formulates the problem in terms of type theory. The algorithm presented \nin the thesis solves the in\u00adclusion constraints in a fairly naive manner by repeatedly iterating through \na constraint vector. Cycle detection is not mentioned. There have been several signi.cant updates since \nthat time. Faehndrich et al. [9] represent the constraints using a graph and formulate the problem as \ncomputing the dynamic transitive closure of that graph. This work introduces a method for partial online \ncycle detection and demonstrates that cycle detection is critical for scalability. A depth-.rst search \nof the graph is performed upon every edge insertion, but the search is arti.cially restricted for the \nsake of performance, making cycle detection incomplete. Heintze and Tardieu introduce a new algorithm \nfor computing the dynamic transitive closure [11]. As new inclusion edges are added to the constraint \ngraph from the indirect constraints, their corresponding new transitive edges are not added to the graph. \nIn\u00adstead, the constraint graph retains its pre-transitive form. During the analysis, indirect constraints \nare resolved via reachability queries on the graph. Cycle detection is performed as a side-effect of \nthese queries. The main drawback to this technique is unavoidable redun\u00addant work it is impossible to \nknow whether a reachability query will encounter a newly-added inclusion edge (inserted earlier due to \nsome other indirect constraint) until after it completes, which means that potentially redundant queries \nmust still be carried out on the off-chance that a new edge will be encountered. Heintze and Tardieu \nreport excellent results, analyzing a C program with 1.3M LOC in less than a second, but these results \nare for a .eld\u00adbased implementation. A .eld-based analysis treats each .eld of a struct as its own variable \nassignments to x.f, y.f,and (*z).f are all treated as assignments to a variable f, which tends to de\u00adcrease \nboth the size of the input to the pointer analysis and the number of dereferenced variables (an important \nindicator of perfor\u00admance). Field-based analysis is unsound for C programs, and while such an analysis \nis appropriate for the work described by Heintze and Tardieu (the client is a dependency analysis that \nis itself .eld\u00adbased), it is inappropriate for many others. For the results in this paper, we use a .eld-insensitive \nversion of their algorithm, which is dramatically slower than the .eld-based version2. Pearce et al. \nhave proposed two different approaches to inclusion\u00adbased analysis, both of which differ from Heintze \nand Tardieu in that they maintain the explicit transitive closure of the constraint graph. Pearce et \nal. .rst proposed an analysis [22] that uses a more ef.cient algorithm for online cycle detection than \nthat introduced by Faehndrich et al. [9]. In order to avoid cycle detection at every edge insertion, \nthe algorithm dynamically maintains a topologi\u00adcal ordering of the constraint graph. Only a newly-inserted \nedge that violates the current ordering could possibly create a cycle, so only in this case are cycle \ndetection and topological re-ordering performed. This algorithm proves to still have too much overhead, \nso Pearce et al. later proposed a new and more ef.cient algorithm [21]. Rather than detect cycles at \nevery edge insertion, the entire constraint graph is periodically swept to detect and collapse any cycles \nthat have formed since the last sweep. It is this algorithm that we evaluate in this paper. Berndl et \nal. [4] describe a .eld-sensitive inclusion-based pointer analysis for Java that uses BDDs [5] to represent \nboth the constraint graph and the points-to solution. BDDs have been exten\u00adsively used in model checking \nas a way to represent large graphs in a very compact form that allows for fast manipulation. Berndl et \nal. were one of the .rst to use BDDs for pointer analysis. The analysis they describe is speci.c to the \nJava language; it also doesn t handle indirect calls because it depends on a prior analysis to construct \nthe complete call-graph. The version of the algorithm that we use in this paper is a .eld-insensitive \nanalysis for C programs that does handle indirect function calls. Because Andersen-style analysis was \npreviously considered to be non-scalable, other algorithms, including Steensgaard s near\u00adlinear time \nanalysis [25] and Das One-Level Flow analysis [7], have been proposed to improve performance by sacri.cing \neven more precision. While Steensgaard s analysis has much greater im\u00adprecision than inclusion-based \nanalysis, Das reports that for C pro\u00adgrams One-Level Flow analysis has precision very close to that of \n2 To ensure that the performance difference is in fact due to .eld\u00adinsensitivity, we also benchmarked \na .eld-based version of our HT im\u00adplementation. We observed comparable performance to that reported by \nHeintze and Tardieu [11]. Constraint Type Program Code Constraint Meaning Base Simple a =&#38;b a =b \na .{b}a .b loc(b).pts(a) pts(a).pts(b) Complex1 a =*b a .*b .v .pts(b):pts(a).pts(v) Complex2 *a =b *a \n.b .v .pts(a):pts(v).pts(b) Table 1. Constraint Types inclusion-based analysis. This precision is based \non the assump\u00adtion that multi-level pointers are less frequent and less important than single-level pointers, \nwhich Das experiments indicate is usu\u00adally (though not always) true for C, but which may not be true \nfor other languages such as Java and C++. In addition, for the sake of performance, Das conservatively \nuni.es non-equivalent variables, much like Steensgaard s analysis; this uni.cation makes it dif.cult \nto trace dependency chains among variables. Dependency chains are very useful for understanding the results \nof program analyses such as program veri.cation and program understanding, and also for use in automatic \ntools such as Broadway [10]. Inclusion-based pointer analysis is a better choice than either Steensgaard \ns analy\u00adsis or One-Level Flow, if it can be made to run in reasonable time even on large programs with \nmillions of lines of code; this is the challenge that we address in this paper. In the other direction \nof increasing precision, there have been several attempts to scale a context-sensitive version of inclusion\u00adbased \npointer analysis. One of the fastest of these attempts is the the algorithm by Whaley et al. [28], which \nuses BDDs to scale a context-sensitive, .ow-insensitive pointer analysis for Java to almost 700K LOC \n(measuring bytecode rather than source lines). However, Whaley et al. s algorithm is only context-sensitive \nfor top-level variables, meaning that all variables in the heap are treated context-insensitively; also, \nits ef.ciency depends heavily on certain characteristics of the Java language attempts to use the same \ntechnique for analyzing programs in C have shown greatly reduced performance [3]. Nystrom et al. [20] \npresent a context-sensitive algorithm based on the insight that inlining all function calls makes a context\u00adinsensitive \nanalysis equivalent to a context-sensitive analysis of the original program. Of course, inlining all \nfunction calls can increase the program size exponentially, but intelligent heuristics can make exponential \ngrowth extremely unlikely. An important building block of this approach is context-insensitive inclusion\u00adbased \nanalysis it is used while inlining the functions and also for analyzing the resulting program. Nystrom \net al. manage to scale the context-sensitive analysis to a C program with 200K LOC. The new techniques \ndescribed in this paper could be used to scale their algorithm even further.  3. Background Inclusion-based \npointer analysis is a set-constraint problem. A linear pass through the program code generates three \ntypes of constraints base, simple,and complex [11]. We eliminate nested pointer dereferences by introducing \nauxiliary variables and con\u00adstraints, leaving only one pointer dereference per constraint. Table 1 demonstrates \nthe three types of constraints, how they are derived from the program code, and what the constraints \nmean. For a vari\u00adable v, pts(v)represents v s points-to set and loc(v)represents the memory location \ndenoted by v. Following the example of prior work in this area [9, 11, 21, 4], we solve the set-constraint \nproblem by computing the dynamic transitive closure of a constraint graph. The constraint graph G has \none node for each program variable. For each simple constraint a .b, G has a directed edge b .a. Each \nnode also has a points-to let G =<V,E > W .V while W =\u00d8do n .SELECT-FROM(W ) for each v .pts(n)do for \neach constraint a .*n do if v .a/.E then E .E .{v .a} W .W .{v}  for each constraint *n .b do if b \n.v/.E then E .E .{b .v} W .W .{b} for each n .z .E do pts(z).pts(z).pts(n) if pts(z)changed then W .W \n.{z} Figure 1. Dynamic Transitive Closure set associated with it, initialized using the base constraints: \nfor each base constraint a .{b}, node a s points-to set contains loc(b).The complex constraints are not \nexplicitly represented in the graph; they are maintained in a separate list. To solve the constraints \nwe compute the transitive closure of G by propagating points-to information along its edges. As we update \nthe points-to sets, we must also add new edges to represent the complex constraints. For each constraint \na .*b and each loc(v) . pts(b), we add a new edge v . a. Similarly, for each constraint *a .b and each \nloc(v) .pts(a), we add a new edge b .v. Figure 1 shows a basic worklist algorithm that maintains the \nexplicit transitive closure of G. The worklist is initialized with all nodes in G that have a non-empty \npoints-to set. For each node n taken off the worklist, we proceed in two steps: 1. For each loc(v) .pts(n): \nfor each constraint a .*n add an edge v .a, and for each constraint *n .b addanedge b .v. Any node that \nhas had a new outgoing edge added is inserted into the worklist. 2. For each outgoing edge n . v, propagate \npts(n)to node v,  i.e. pts(v):=pts(v).pts(n). Any node whose points-to set has been modi.ed is inserted \ninto the worklist. The algorithm is presented as it is for clarity of exposition; various optimizations \nare possible to improve its performance.  4. Our Solutions The algorithm shown in Figure 1 computes \nthe dynamic transitive closure of the constraint graph but makes no attempt to detect cy\u00adcles. The particular \nmethod used for detecting cycles will in large part determine the ef.ciency of the analysis in fact, \nwithout cycle detection our larger benchmarks run out of memory before com\u00adpleting, even on a machine \nwith 2GB of memory. When perform\u00ading online cycle detection, there is a tension between searching for \ncycles too early which leads to the overhead of repeatedly sweep\u00ading the constraint graph and searching \nfor cycles too late which reduces the bene.ts of cycle elimination because points-to infor\u00admation can \nbe redundantly propagated around cycles before they are detected. We now present two new approaches for \nonline cycle detection that balance this tension in different ways. 4.1 Lazy Cycle Detection Cycles in \nthe constraint graph can be collapsed because nodes in the same cycle are guaranteed to have identical \npoints-to sets. We use this fact to create a heuristic for cycle detection: before propagating points-to \ninformation across an edge of the constraint graph, we check to see if the source and destination already \nhave equal points\u00adto sets; if so then we use a depth-.rst search to check for a possible cycle. This \ntechnique is lazy because rather than trying to detect cycles when they are created, i.e. when the .nal \nedge is inserted that completes the cycle, it waits until the effect of the cycle identical points-to \nsets becomes evident. The advantage of this technique is that we only attempt to detect cycles when we \nare likely to .nd them. A potential disadvantage is that cycles may be detected well after they are formed, \nsince we must wait for the points-to information to propagate all the way around the cycle before we \ncan detect it. The accuracy of this technique depends upon the assumption that two nodes usually have \nidentical points-to sets only because they are in the same cycle; otherwise it would waste time trying \nto detect non-existent cycles. One additional re.nement is neces\u00adsary to bolster this assumption and \nmake the technique relatively precise: we never trigger cycle detection on the same edge twice. We thus \navoid making repeated cycle detection attempts involving nodes with identical points-to sets that are \nnot in a cycle. This addi\u00adtional restriction implies that Lazy Cycle Detection is incomplete it is not \nguaranteed to .nd all cycles in the constraint graph. The Lazy Cycle Detection algorithm is shown in \nFigure 2. Be\u00adfore we propagate a points-to set from one node to another, we check to see if two conditions \nare met: (1) the points-to sets are identical; and (2) we haven t triggered a search on this edge previ\u00adously. \nIf these conditions are met, then we trigger cycle detection rooted at the destination node. If there \nexists a cycle, we collapse together all the nodes involved; otherwise we remember this edge so that \nwe won t repeat the attempt later.  4.2 Hybrid Cycle Detection Cycle detection can be done of.ine, in \na static analysis prior to the actual pointer analysis, such as with Of.ine Variable Substitu\u00adtion described \nby Rountev et al. [23]. However, many cycles don t exist in the initial constraint graph and only appear \nas new edges are added during the pointer analysis itself, thus the need for on\u00adline cycle detection \ntechniques such as Lazy Cycle Detection. The drawback to online cycle detection is that it requires traversing \nthe constraint graph multiple times searching for cycles; these repeated traversals can become extremely \nexpensive. Hybrid Cycle Detec\u00adtion (HCD) is so-called because it combines both of.ine and online analyses \nto detect cycles, thereby getting the best of both worlds detecting cycles created online during the \npointer analysis, without requiring any traversal of the constraint graph. We now describe the HCD of.ine \nanalysis, which is a linear\u00adtime static analysis done prior to the actual pointer analysis. We build \nan of.ine version of the constraint graph, with one node for each program variable plus an additional \nref node for each variable dereferenced in the constraints (e.g. *n). There is a directed edge for each \nsimple and complex constraint: a .b yields edge b .a, a .*b yields edge *b .a,and *a .b yields edge b \n.*a.Base constraints are ignored. Figure 3 illustrates this process. let G =<V,E > R .\u00d8 W .V while W \n=\u00d8do n .SELECT-FROM(W ) for each v .pts(n)do for each constraint a .*n do if v .a/.E then E .E .{v .a} \nW .W .{v}  for each constraint *n .b do if b .v/.E then E .E .{b .v} W .W .{b}  for each n .z .E do \nif pts(z)=pts(n).n .z/.R then DETECT-AND-COLLAPSE-CYCLES(z) R .R .{n .z} pts(z).pts(z).pts(n) if pts(z)changed \nthen W .W .{z} Figure 2. Lazy Cycle Detection a =&#38;c; d =c; b =*a; *a =b; (a) Program a .{c} d .c \n b .*a (c) Of.ine Constraint Graph *a .b  (b) Constraints Figure 3. HCD Of.ine Analysis Example: (a) \nProgram code; (b) constraints generated from the program code; (c) the of.ine con\u00adstraint graph corresponding \nto the constraints. Note that *a and b are in a cycle together; from this we can infer that in the online \nconstraint graph, b will be in a cycle with all the variables in a s points-to set. Once the graph is \nbuilt we detect strongly-connected compo\u00adnents (SCCs) using Tarjan s linear-time algorithm [26]. Any \nSCCs containing only non-ref nodes can be collapsed immediately. SCCs containing ref nodes are more problematic: \na ref node in the of.ine constraint graph is a stand-in for a variable s unknown points-to set, e.g. \nthe ref node *n stands for whatever n s points-to set will be when the pointer analysis is complete. \nAn SCC containing a ref node such as *n actually means that n s points-to set is part of the SCC; but \nsince we don t yet know what that points-to set will be, we can t collapse that SCC. The of.ine analysis \nknows which vari\u00adables points-to sets will be part of an SCC, while the online anal\u00adysis (i.e. the pointer \nanalysis) knows the variables actual points-to sets. The purpose of Hybrid Cycle Detection is to bridge \nthis gap. Figure 4 shows how the online analysis is affected when an SCC contains a ref node in the of.ine \nconstraint graph. a .{c} (a) Points-to Info  (b) Before edges added (c) After edges added Figure 4. \nHCD Online Analysis Example: (a) The initial points-to information from the constraints in Figure 3; \n(b) the online con\u00adstraint graph before any edges are added; (c) the online constraint graph after the \nedges are added due to the complex constraints in Figure 3. Note that c and b are now in a cycle together. \n let G =<V,E > W .V while W = \u00d8do n .SELECT-FROM(W ) if (n, a) .L then for each v .pts(n) do COLLAPSE(v,a) \nW .W .{a}  for each v .pts(n) do for each constraint a .*n do if v .a/.E then E .E .{v .a} W .W .{v} \n  for each constraint *n .b do if b .v/.E then E .E .{b .v} W .W .{b} for each n .z .E do pts(z) .pts(z) \n.pts(n) if pts(z) changed then W .W .{z} Figure 5. Hybrid Cycle Detection We .nish the of.ine analysis \nby looking for SCCs in the of.ine constraint graph that consist of more than one node and that also contain \nat least one ref node. Because there are no constraints of the form *p .*q, no ref node can have a re.exive \nedge and any non-trivial SCC containing a ref node must also contain a non-ref node. For each SCC of \ninterest we select one non-ref node b,and for each ref node *a in the same SCC, we store the tuple (a, \nb) in a list L. This tuple signi.es to the online analysis that a s points\u00adto set belongs in an SCC with \nb, and therefore everything in a s points-to set can safely be collapsed with b. The online analysis \nis shown in Figure 5. The algorithm is similar to the basic algorithm shown in Figure 1, except when \nprocessing node n we .rst check L for a tuple of the form (n, a).If one is found then we preemptively \ncollapse together node a and all members of n s points-to set, knowing that they belong to the same cycle. \nFor simplicity s sake the pseudo-code ignores some obvious optimizations. Hybrid Cycle Detection is not \nguaranteed to .nd all cycles in the online constraint graph, only those that can be inferred from the \nof.ine version of the graph. Those cycles that it does .nd, however, are discovered at the earliest possible \nopportunity and without requiring any traversal of the constraint graph. In addition, while HCD can be \nused on its own as shown in Figure 5, it can also be easily combined with other algorithms such as HT, \nPKH, BLQ, and LCD to enhance their performance.  5. Evaluation 5.1 Methodology To compare the various \ninclusion-based pointer analyses, we imple\u00adment .eld-insensitive versions of .ve main algorithms: Heintze \nand Tardieu (HT), Berndl et al. (BLQ), Pearce et al. (PKH), Lazy Cycle Detection (LCD), and Hybrid Cycle \nDetection (HCD). We also im\u00adplement four additional algorithms by integrating HCD with four of the main \nalgorithms: HT+HCD, PKH+HCD, BLQ+HCD, and LCD+HCD. The algorithms are written in C++ and handle all as\u00adpects \nof the C language except for varargs. They use as many com\u00admon components as possible to provide a fair \ncomparison, and they have all been highly optimized. The source code is available from the authors upon \nrequest. Some highlights of the implementations include: Indirect function calls are handled as described \nby Pearce et al [21]. Function parameters are numbered contiguously start\u00ading immediately after their \ncorresponding function variable, and when resolving indirect calls they are accessed as offsets to that \nfunction variable.  Cycles are detected using Nuutila et al. s [19] variant of Tar\u00adjan s algorithm, \nand they are collapsed using a union-.nd data structure with both union-by-rank and path compression \nheuris\u00adtics.  BLQ uses the incrementalization optimization described by Berndl et al. [4]. We use the \nBuDDy BDD library [16] to im\u00adplement BDDs.  LCD and HCD are both worklist algorithms we use the work\u00adlist \nstrategy LRF,3 suggested by Pearce et al. [22], to priori\u00adtize the worklist. We also divide the worklist \ninto two sections, current and next, as described by Nielson et al. [18]; items are selected from current \nand pushed onto next,and the two are swapped when current becomes empty. For our benchmarks, the divided \nworklist yields signi.cantly better performance than a single worklist.  Aside from BLQ, all the algorithms \nuse sparse bitmaps to im\u00adplement both the constraint graph and the points-to sets. The sparse bitmap \nimplementation is taken from the GCC 4.1.1 compiler.  We also experiment with the use of BDDs to represent \nthe points-to sets. Unlike BLQ, which stores the entire points-to solution in a single BDD, we give each \nvariable its own BDD to store its individual points-to set. For example, if a .{b, c}and d .{c, e}, BLQ \nwould have a single BDD representing the set of tuples {(a, b), (a, c), (d, c), (d, e)}. Instead, we \ngive a a BDD representing the set {b, c} and we give d aBDD representing the set {c, e}. The use of BDDs \ninstead of sparse bitmaps is a simple modi.cation that requires minimal changes to the code.  The benchmarks \nfor our experiments are described in Table 2. Emacs is a text editor; Ghostscript is a postscript viewer; \nGimp is an image manipulation program; Insight is a GUI overlaid on 3 Least Recently Fired the node processed \nfurthest back in time is given priority. Name LOC Original Constraints Reduced Constraints Base Simple \nComplex Emacs-21.4a 169K 83,213 21,460 4,088 11,095 6,277 Ghostscript-8.15 242K 169,312 67,310 12,154 \n25,880 29,276 Gimp-2.2.8 554K 411,783 96,483 17,083 43,878 35,522 Insight-6.5 603K 243,404 85,375 13,198 \n35,382 36,795 Wine-0.9.21 1,338K 713,065 171,237 39,166 62,499 69,572 Linux-2.4.26 2,172K 574,788 203,733 \n25,678 77,936 100,119 Table 2. Benchmarks: For each benchmark we show the number of lines of code (computed \nas the number of non-blank, non-comment lines in the source .les), the original number of constraints \ngenerated using CIL, the reduced number of constraints after being pre-processed, and a break-down of \nthe forms of the reduced constraints. top of the gdb debugger; Wine is a Windows emulator; and Linux \nis the Linux operating system kernel. The constraint generator is separate from the constraint solvers: \nwe generate constraints from the benchmarks using the CIL C front-end [17], ignoring any assignments \ninvolving types too small to hold a pointer. External library calls are summarized using hand-crafted \nfunction stubs. We pre-process the resulting constraint .les using a variant of Of.ine Variable Substitution \n[23], which reduces the number of constraints by 60 77%. This pre-processing step takes less than a second \nfor Emacs and Ghostscript, and between 1 and 3 seconds for Gimp, Insight, Wine, and Linux. The results \nreported are for these reduced constraint .les; they include everything from reading in the constraint \n.le from disk, creating the initial constraint graph, and solving that graph. We run the experiments \non a dual-core 1.83 GHz processor with 2 GB of memory, using the Ubuntu 6.10 Linux distribution. Though \nthe processor is dual-core, the executables themselves are single\u00adthreaded. All executables are compiled \nusing gcc-4.1.1 and the  O3 optimization .ag. We repeat each experiment three times and report the smallest \ntime; all the experiments have very low variance in performance.  5.2 Time and Memory Consumption Table \n3 shows the performance of the various algorithms. The times for HCD s of.ine analysis are shown separately \nand not in\u00adcluded in the times for the various algorithms using HCD they are small enough to be essentially \nnegligible. Table 4 shows the memory consumption of the algorithms. Figure 6 graphically com\u00adpares (using \na log-scale) the performance of our combined algo\u00adrithm LCD+HCD the fastest of all the algorithms against \nthe current state-of-the-art algorithms. All these numbers were gath\u00adered using the sparse-bitmap implementations \nof the algorithms (except for BLQ). BLQ s memory allocation is fairly constant across all the bench\u00admarks. \nWe allocate an initial pool of memory for the BDDs, which dominates the memory usage and is independent \nof benchmark size. While we can decrease the initial pool size for the smaller benchmarks without decreasing \nperformance, there is no easy way to calculate the minimum pool size for a speci.c benchmark, so for \nall the benchmarks we use the smallest pool size that doesn t impair the performance of our largest benchmark. \nIt is interesting to note the vast difference in analysis time be\u00adtween Wine and Linux for all algorithms \nother than BLQ. While Wine has 32.5K fewer constraints than Linux, it takes 1.7 7.3\u00d7 longer to be analyzed, \ndepending on the algorithm used. This dis\u00adcrepancy points out the danger in using the size of the initial \ninput to predict performance when other factors can have at least as much impact. Wine is a case in point: \nwhile its initial constraint graph is smaller than that of Linux, its .nal constraint graph at the end \nof the analysis is an order-of-magnitude larger than that of Linux, due mostly to Wine s larger average \npoints-to set size. BLQ doesn t dis-Figure 6. Performance (in seconds) of our new combined algo\u00adrithm \n(LCD+HCD) versus three state-of-the art inclusion-based al\u00adgorithms. Note that the Y-axis is log-scale. \n play this same behavior, because of its radically different analysis mechanism that uses BDDs and because \nit lacks cycle detection. Comparing HT, PKH, BLQ, LCD, and HCD. Figure 7 compares the performance of \nthe main algorithms by normalizing the times for HT, PKH, BLQ, and HCD by that of LCD. Focusing on the \ncurrent state-of-the-art algorithms, HT is clearly the fastest, being 1.9\u00d7 faster than PKH and 6.5\u00d7 faster \nthan BLQ. LCD is on average 1.05\u00d7 faster than HT and uses 1.2\u00d7 less memory. HCD runs out of memory for \nWine, but excluding that benchmark it is on average 1.8\u00d7 slower than HT and 1.9\u00d7 faster than PKH, using \n1.4\u00d7 more memory than HT. Effects of HCD. Figure 8 normalizes the performance of the main algorithms \nby that of their HCD-enhanced counterparts. On av\u00aderage, the use of HCD increases HT performance by 3.2\u00d7,PKH \nperformance by 5\u00d7, BLQ performance by 1.1\u00d7,and LCDper\u00adformance by 3.2\u00d7. HCD also leads to a small decrease \nin mem\u00adory consumption for all the algorithms except BLQ it decreases memory consumption by 1.2\u00d7 for \nHT, by 1.1\u00d7 for PKH, and by 1.02\u00d7 for LCD. Most of the memory used by these algorithms comes from the \nrepresentation of points-to sets. HCD improves performance by .nding and collapsing cycles much earlier \nthan normal, but it doesn t actually .nd many more cycles than were already detected without using HCD, \nso it doesn t signi.cantly re\u00adduce the number of points-to sets that need to be maintained. HCD doesn \nt improve BLQ s performance by much because even though no extra effort is required to .nd cycles, there \nis still some overhead involved in collapsing those cycles. Also, the performance of BLQ depends on the \nsizes of the BDD representations of the constraint Table 3. Performance (in seconds), using bitmaps for \npoints-to sets. The HCD-Of.ine analysis is reported separately and not included in the times for those \nalgorithms using HCD. The HCD algorithm runs out of memory on the Wine benchmark. Emacs Ghostscript Gimp \nInsight Wine Linux HCD-Of.ine 0.05 0.17 0.26 0.23 0.51 0.62 HT 1.66 12.03 59.00 42.49 1,388.51 393.30 \nPKH 2.05 20.05 92.30 117.88 1,946.16 1,181.59 BLQ 4.74 121.60 167.56 265.94 5,117.64 5,144.29 LCD 3.07 \n15.23 39.50 39.02 1,157.10 327.65 HCD 0.46 49.55 59.70 73.92 OOM 659.74 HT+HCD 0.46 7.29 11.94 14.82 \n643.89 102.77 PKH+HCD 0.46 10.52 17.12 21.91 838.08 114.45 BLQ+HCD 5.81 115.00 173.46 257.05 4,211.71 \n4,581.91 LCD+HCD 0.56 7.99 12.50 15.97 492.40 86.74 Emacs Ghostscript Gimp Insight Wine Linux HT 17.7 \n84.9 279.0 231.5 1,867.2 901.3 PKH 17.6 83.9 269.5 194.7 1,448.3 840.7 BLQ 215.6 216.1 216.2 216.1 216.2 \n216.2 LCD 14.3 74.6 269.0 184.4 1,465.1 830.1 HCD 18.1 138.7 416.1 290.5 OOM 1,301.5 HT+HCD 12.4 80.8 \n253.9 186.5 1,391.4 842.5 PKH+HCD 13.9 79.1 264.6 186.0 1,430.2 807.5 BLQ+HCD 215.8 216.2 216.2 216.2 \n216.2 216.2 LCD+HCD 13.9 73.5 263.9 183.6 1,406.4 807.9 Table 4. Memory consumption (in megabytes), \nusing bitmaps for points-to sets..  10 8 6 4 Normalized Time Normalized Time 2 0 Figure 7. Performance \ncomparison of individual benchmarks, where performance is normalized against LCD. HCD runs out of memory \nfor Wine, so there is no HCD bar for that benchmark. and points-to graphs, and because of the properties \nof BDDs, re\u00admoving edges from the constraint graph can potentially increase the size of the constraint \ngraph BDD. The combination of our two new algorithms, LCD+HCD, yields the fastest algorithm among all \nthose studied: It is 3.2\u00d7 faster than HT, 6.4\u00d7 faster than PKH, and 20.6\u00d7 faster than BLQ.  5.3 Understanding \nthe Results There are a number of factors that determine the relative perfor\u00admance of these algorithms, \nbut three of the most important are: (1) the number of nodes collapsed due to strongly-connected com-Figure \n8. Performance comparison of the individual benchmarks, where the performance of each main algorithm \nis normalized against its respective HCD-enhanced counterpart. ponents; (2) the number of nodes searched \nduring the depth-.rst traversals of the constraint graph; and (3) the number of propa\u00adgations of points-to \ninformation across the edges of the constraint graph. The number of nodes collapsed is important because \nit reduces both the number of nodes and the number of edges in the constraint graph; the more nodes that \nare collapsed, the smaller the input and the more ef.cient the algorithm. The depth-.rst searches are \npure overhead due to cycle detec\u00adtion. As long as roughly as many cycles are being detected, then the \nfewer nodes that are searched the better. Emacs Ghostscript Gimp Insight Wine Linux HT 3.44 18.55 46.98 \n65.00 1,551.89 419.38 PKH 4.23 19.55 81.53 96.50 1,172.15 801.13 LCD 4.96 19.34 47.29 64.57 1,213.43 \n380.26 HCD 3.96 24.65 49.11 65.01 731.20 267.69 HT+HCD 2.58 15.65 33.69 42.33 737.37 209.90 PKH+HCD 3.06 \n14.70 33.71 43.20 744.35 172.43 LCD+HCD 3.09 13.69 33.04 43.17 625.82 183.97 Table 5. Performance (in \nseconds), using BDDs for points-to sets. Emacs Ghostscript Gimp Insight Wine Linux HT 33.1 49.3 100.7 \n100.0 811.2 274.3 PKH 33.2 33.6 50.4 66.8 226.4 182.1 LCD 33.2 33.2 40.1 33.9 251.1 73.5 HCD 33.1 37.1 \n36.8 37.0 239.6 65.8 HT+HCD 33.1 37.8 51.2 53.9 410.6 100.7 PKH+HCD 33.1 33.2 36.0 33.2 103.9 45.2 LCD+HCD \n33.1 33.2 33.2 33.2 173.6 42.6 Table 6. Memory consumption (in megabytes), using BDDs for points-to \nsets. The number of points-to information propagations is an impor\u00adtant metric because propagation is \none of the most expensive opera\u00adtions in the analysis. It is strongly in.uenced by both the number of \ncycles collapsed and by how quickly they are collapsed. If a cycle is not detected quickly, then points-to \ninformation could be redun\u00addantly circulated around the cycle a number of times. We now examine these \nthree quantities to help explain the per\u00adformance results seen in the previous section. Due to its radically \ndifferent analysis mechanism, we don t include BLQ in this exam\u00adination.4 Nodes Collapsed. PKH is the \nonly algorithm guaranteed to detect all strongly-connected components in the constraint graph; how\u00adever, \nHT and LCD both do a very good job of .nding and col\u00adlapsing cycles for each benchmark they detect and \ncollapse over 99% of the nodes collapsed by PKH. HCD by itself doesn t do as well, collapsing only 46 \n74% of the nodes collapsed by PKH. This de.ciency is primarily responsible for HCD s greater memory con\u00adsumption. \nNodes Searched. HCD is, of course, the most ef.cient algorithm in terms of searching the constraint graph, \nsince it doesn t search at all. HT is the next most ef.cient algorithm, because it only searches the \nsubset of the graph necessary for resolving indirect constraints. PKH searches 2.6\u00d7 as many nodes as \nHT, as it period\u00adically searches the entire graph for cycles. LCD is the least ef.cient, searching 8\u00d7 \nas many nodes as HT. Propagations. LCD has the fewest propagations, showing that its greater effort at \nsearching for cycles pays off by .nding those cycles earlier than HT or PKH. HT has 1.8\u00d7 as many propagations, \nand PKH has 2.2\u00d7 as many. Since they both .nd as many cycles as LCD (as shown by the number of nodes \ncollapsed), this difference is due to the relative amount of time it takes for each of the 4 It is dif.cult \nto .nd statistics to directly explain BLQ s performance rela\u00adtive to HT, PKH, LCD, and HCD. It doesn \nt use cycle detection, so it adds orders of magnitude more edges to the constraint graph but propagation \nof points-to information is done simultaneously across all the edges using BDD operations, and the performance \nof the algorithm is due more to how well the BDDs compress the constraint and points-to graphs than anything \nelse. algorithms to .nd cycles. HCD has the most propagations, 5.2\u00d7 as many as LCD. HCD .nds cycles \nas soon as they are formed, so it .nds them much faster than LCD does, but as shown above, it .nds substantially \nfewer cycles than the other algorithms. Effects of HCD. The main bene.t of combining HCD with the other \nalgorithms is that it helps these algorithms .nd cycles much sooner than they would on their own. While \nit does little to increase the number of nodes collapsed or decrease the number of nodes searched, it \ngreatly decreases the number of propagations, because cycles are collapsed before the points-to information \nhas a chance to propagate around the cycles. The addition of HCD decreases the number of propagations \nby 10\u00d7 for HT and by 7.4\u00d7 for both PKH and LCD. Discussion. Despite its lazy nature, LCD searches more \nnodes than either HT or PKH, and it propagates less points-to information than either as well. It appears \nthat being more aggressive pays off, which naturally leads to the question: could we do better by being \neven more aggressive? However, past experience has shown that we must carefully balance the work we do \ntoo much aggression can lead to overhead that overwhelms any bene.ts it may provide. This point is shown \nin both Faehndrich et al. s algorithm [9] and Pearce et al. s original algorithm [22]. Both of these \nalgorithms are very aggressive in seeking out cycles, and both are an order of magnitude slower than \nany of the algorithms evaluated in this paper. 5.4 Representing Points-to Sets Table 4 shows that the \nmemory consumption of all the algorithms that use sparse bitmaps is extremely high. Pro.ling reveals \nthat the majority of this memory usage comes from the bit-map rep\u00adresentation of points-to sets. BLQ, \non the other hand, uses rela\u00adtively little memory even for the largest benchmarks, due to its use of \nBDDs. It is thus natural to wonder how the other algo\u00adrithms would compare in terms of both analysis \ntime and mem\u00adory consumption if they were to instead use BDDs to represent points-to sets. Tables 5 and \n6 show the performance and memory consump\u00adtion of the modi.ed algorithms. Figure 9 graphically shows \nthe performance cost of the modi.ed algorithms by normalizing them  When BDDs are used, HCD is less \neffective in improving per\u00adformance than it was when using bitmaps because HCD decreases the number of \npropagations required, but using BDDs already makes propagation a fairly cheap operation. However, with \nBDDs, HCD s effect on memory consumption is much more noticeable, since the constraint graph represents \na much larger proportion of the memory usage.  6. Conclusion We have signi.cantly improved upon the \ncurrent state-of-the-art in inclusion-based pointer analysis by introducing two novel tech\u00adniques: Lazy \nCycle Detection (LCD) and Hybrid Cycle Detection (HCD). As their names suggest, both techniques improve \nthe ef.\u00adciency and effectiveness of online cycle detection, which is criti\u00adcal to the scalability of \nall inclusion-based pointer analyses. Lazy Figure 9. Performances of the BDD-based implementations nor\u00ad \nmalized by their bitmap-based counterparts, averaged over all the Cycle Detection detects cycles based \non their effects on points-to sets, piggybacking on top of the transitive closure computation that is \ninherent to this type of analysis. Its lazy nature yields a highly ef.cient algorithm. Hybrid Cycle Detection \ntakes a different ap\u00ad benchmarks.  proach, paying a tiny up-front cost to perform an of.ine analysis \nthat allows the subsequent online analysis to detect cycles without ever having to traverse the constraint \ngraph. Hybrid Cycle Detec\u00adtion can be used to enhance other algorithms for inclusion-based pointer analysis, \nsigni.cantly improving their performance. Our re\u00adsults show that the combination of LCD and HCD is on \naverage the most ef.cient of all the algorithms we studied. On our suite of six large open source C benchmarks, \nwhich range in size from 169K to 2.17M lines of code, the LCD+HCD algorithm is an average of 3.2\u00d7 faster \nthan the Heintze and Tardieu algorithm, 6.4\u00d7 faster than the Pearce et al. algorithm, and 20.6\u00d7 faster \nthan the Berndl et al. algorithm. We have also investigated the use of different data structures to represent \npoints-to sets, examining the impact on both perfor\u00admance and memory consumption. In particular, we have \ncompared the sparse-bitmap implementation used in the GCC open-source compiler with a BDD-based implementation, \nand we have found that the BDD implementation is on average 2\u00d7 slower but uses 5.5\u00d7 less memory. Figure \n10. Memory consumption of the bitmap-based implemen\u00ad tations normalized by their BDD-based counterparts, \naveraged over all the benchmarks. Many program analyses that require pointer information sac\u00adri.ce precision \nin the pointer analysis for the sake of reasonable performance. This performance is the attraction of \nanalyses such as Steensgaard s near-linear-time analysis [25] and Das One-Level Flow analysis [7]. However, \nthe precision of subsequent program by their bitmap-based counterparts, and Figure 10 shows the mem\u00ad \nory savings by normalizing the bitmap-based algorithms by their BDD-based counterparts. As with BLQ, \nwe allocate an initial pool of memory for the BDDs that is independent of the benchmark size, which is \nwhy memory consumption actually increases for the smallest benchmark, Emacs, and never goes lower than \n33.1MB for any benchmark. On average, the use of BDDs increases running time by 2\u00d7 while it decreases \nmemory usage by 5.5\u00d7. Most of the extra time comes from a single function, bdd allsat, which is used \nto extract all the elements of a set contained in a given BDD. This function is used when iterating through \na variable s points-to set while adding new edges according to the complex constraints. However, both \nPKH and HCD are actually faster with BDDs on all benchmarks except for Emacs (Figure 9 shows that they \nare slower on average, but this is solely because of Emacs). These are the two algorithms that propagate \nthe most points-to information across constraint edges. BDDs make this operation much faster than using \nsparse bitmaps, and this advantage makes up for the extra time taken by bdd allsat. analysis is often \nlimited by the precision of the pointer informa\u00adtion used [24], so it behooves an analysis to use the \nmost precise pointer information that it can reasonably acquire. Our work has made inclusion-based pointer \nanalysis a reasonable choice even for applications with millions of lines of code.  Acknowledgments \nWe thank Brandon Streiff and Luke Robison for their help in conducting experiments, Dan Berlin for his \nhelp with the GCC compiler internals, and Sam Guyer, E Lewis, Kathryn McKinley, and the anonymous reviewers \nfor their helpful comments on early drafts of this paper. This work was supported by NSF grants ACI\u00ad0313263 \nand CNS-0509354 and by an IBM Faculty Partnership Award. References [1] Aesop. The Ant and the Grasshopper, \nfrom Aesop s Fables. Greece, 6th century BC. [2] Lars Ole Andersen. Program Analysis and Specialization \nfor the C Programming Language. PhD thesis, DIKU, University of Copenhagen, May 1994.  [3] Dzintars \nAvots, Michael Dalton, V. Benjamin Livshits, and Monica S. Lam. Improving software security with a C \npointer analysis. In 27th International Conference on Software Engineering (ICSE), pages 332 341, 2005. \n[4] Marc Berndl, Ondrej Lhotak, Feng Qian, Laurie Hendren, and Navindra Umanee. Points-to analysis using \nBDDs. In Programming Language Design and Implementation (PLDI), pages 103 114, 2003. [5] Randal E. Bryant. \nGraph-based algorithms for Boolean function manipulation. IEEE Transactions on Computers, C-35(8):677 \n691, August 1986. [6] Jong-Deok Choi, Michael Burke, and Paul Carini. Ef.cient .ow\u00adsensitive interprocedural \ncomputation of pointer-induced aliases and side effects. In Principles of Programming Languages (POPL), \npages 232 245, 1993. [7] Manuvir Das. Uni.cation-based pointer analysis with directional assignments. \nIn Programming Language Design and Implementation (PLDI), pages 35 46, 2000. [8] Maryam Emami, Rakesh \nGhiya, and Laurie J. Hendren. Context\u00adsensitive interprocedural points-to analysis in the presence of \nfunction pointers. In Programming Language Design and Implementation (PLDI), pages 242 256, 1994. [9] \nManuel Faehndrich, Jeffrey S. Foster, Zhendong Su, and Alexander Aiken. Partial online cycle elimination \nin inclusion constraint graphs. In Programming Language Design and Implementation (PLDI), pages 85 96, \n1998. [10] Samuel Z. Guyer and Calvin Lin. Error checking with client-driven pointer analysis. Science \nof Computer Programming, 58(1-2):83 114, 2005. [11] Nevin Heintze and Olivier Tardieu. Ultra-fast aliasing \nanalysis using CLA: A million lines of C code in a second. In Programming Language Design and Implementation \n(PLDI), pages 24 34, 2001. [12] Michael Hind. Pointer analysis: haven t we solved this problem yet? In \nWorkshop on Program Analysis for Software Tools and Engineering (PASTE), pages 54 61, 2001. [13] Michael \nHind, Michael Burke, Paul Carini, and Jong-Deok Choi. Interprocedural pointer alias analysis. ACM Transactions \non Programming Languages and Systems, 21(4):848 894, 1999. [14] William Landi and Barbara G. Ryder. Pointer-induced \naliasing: a problem taxonomy. In Symposium on Principles of Programming Languages (POPL), pages 93 103, \n1991. [15] William Landi and Barbara G. Ryder. A safe approximate algorithm for interprocedural pointer \naliasing. In Programming Language Design and Implementation (PLDI), pages 235 248, 1992. [16] J. Lind-Nielson. \nBuDDy, a binary decision package. http://www.itu.dk/ research/buddy/. [17] George C. Necula, Scott McPeak, \nShree Prakash Rahul, and Westley Weimer. CIL: Intermediate language and tools for analysis and transformation \nof C programs. In Computational Complexity, pages 213 228, 2002. [18] F. Nielson, H. R. Nielson, and \nC. L. Hankin. Principles of Program Analysis. Springer-Verlag, 1999. [19] Esko Nuutila and Eljas Soisalon-Soininen. \nOn .nding the strong components in a directed graph. Technical Report TKO-B94, Helsinki University of \nTechnology, Laboratory of Information Processing Science, 1995. [20] Erik M. Nystrom, Hong-Seok Kim, \nand Wen mei W. Hwu. Bottom-up and top-down context-sensitive summary-based pointer analysis. In International \nSymposium on Static Analysis, pages 165 180, 2004. [21] David Pearce, Paul Kelly, and Chris Hankin. Ef.cient \n.eld-sensitive pointer analysis for C. In ACM workshop on Program Analysis for Software Tools and Engineering \n(PASTE), pages 37 42, 2004. [22] David J. Pearce, Paul H. J. Kelly, and Chris Hankin. Online cycle detection \nand difference propagation for pointer analysis. In 3rd International IEEE Workshop on Source Code Analysis \nand Manipulation (SCAM), pages 3 12, 2003. [23] Atanas Rountev and Satish Chandra. Off-line variable \nsubstitution for scaling points-to analysis. In Programming Language Design and Implementation (PLDI), \npages 47 56, 2000. [24] M. Shapiro and S. Horwitz. The effects of the precision of pointer analysis. \nLecture Notes in Computer Science, 1302:16 34, 1997. [25] Bjarne Steensgaard. Points-to analysis in almost \nlinear time. In Symposium on Principles of Programming Languages (POPL), pages 32 41, 1996. [26] Robert \nTarjan. Depth-.rst search and linear graph algorithms. SIAM J. Comput., 1(2):146 160, June 1972. [27] \nTeck Bok Tok, Samuel Z. Guyer, and Calvin Lin. Ef.cient .ow\u00adsensitive interprocedural data-.ow analysis \nin the presence of pointers. In 15th International Conference on Compiler Construction (CC), pages 17 \n31, 2006. [28] John Whaley and Monica S. Lam. Cloning-based context-sensitive pointer alias analysis. \nIn Programming Language Design and Implementation (PLDI), pages 131 144, 2004. [29] Robert P. Wilson \nand Monica S. Lam. Ef.cient context-sensitive pointer analysis for c programs. In Programming Language \nDesign and Implementation (PLDI), pages 1 12, 1995. [30] Jianwen Zhu and Silvian Calman. Symbolic pointer \nanalysis revisited. In Programming Language Design and Implementation (PLDI), pages 145 157, 2004.  \n\t\t\t", "proc_id": "1250734", "abstract": "<p>Pointer information is a prerequisite for most program analyses, and the quality of this information can greatly affect their precision and performance. Inclusion-based (i.e. Andersen-style) pointer analysis is an important point in the space of pointer analyses, offering a potential sweet-spot in the trade-off between precision and performance. However, current techniques for inclusion-based pointer analysis can have difficulties delivering on this potential.</p> <p>We introduce and evaluate two novel techniques for inclusion-based pointer analysis---one lazy, one eager<sup>1</sup>---that significantly improve upon the current state-of-the-art without impacting precision. These techniques focus on the problem of online cycle detection, a critical optimization for scaling such analyses. Using a suite of six open-source C programs, which range in size from 169K to 2.17M LOC, we compare our techniques against the three best inclusion-based analyses--described by Heintze and Tardieu [11], by Pearce et al. [21], and by Berndl et al. [4]. The combination of our two techniques results in an algorithm which is on average 3.2 xfaster than Heintze and Tardieu's algorithm, 6.4 xfaster than Pearce et al.'s algorithm, and 20.6 faster than Berndl et al.'s algorithm.</p> <p>We also investigate the use of different data structures to represent points-to sets, examining the impact on both performance and memory consumption. We compare a sparse-bitmap implementation used in the GCC compiler with a BDD-based implementation, and we find that the BDD implementation is on average 2x slower than using sparse bitmaps but uses 5.5x less memory.</p>", "authors": [{"name": "Ben Hardekopf", "author_profile_id": "81331494259", "affiliation": "University of Texas at Austin, Austin, TX", "person_id": "PP33032316", "email_address": "", "orcid_id": ""}, {"name": "Calvin Lin", "author_profile_id": "81451598438", "affiliation": "University of Texas at Austin, Austin, TX", "person_id": "PP95031702", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1250734.1250767", "year": "2007", "article_id": "1250767", "conference": "PLDI", "title": "The ant and the grasshopper: fast and accurate pointer analysis for millions of lines of code", "url": "http://dl.acm.org/citation.cfm?id=1250767"}