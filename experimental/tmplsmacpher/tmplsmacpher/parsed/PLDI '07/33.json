{"article_publication_date": "06-10-2007", "fulltext": "\n Of.ine Compression for On-Chip RAM Nathan Cooprider John Regehr School of Computing, University of Utah \n {coop,regehr}@cs.utah.edu Abstract We present of.ine RAM compression, an automated source-to\u00adsource \ntransformation that reduces a program s data size. Statically allocated scalars, pointers, structures, \nand arrays are encoded and packed based on the results of a whole-program analysis in the value set and \npointer set domains. We target embedded software written in C that relies heavily on static memory allocation \nand runs on Harvard-architecture microcontrollers supporting just a few KB of on-chip RAM. On a collection \nof embedded applications for AVR microcontrollers, our transformation reduces RAM usage by an average \nof 12%, in addition to a 10% reduction through a dead\u00addata elimination pass that is also driven by our \nwhole-program analysis, for a total RAM savings of 22%. We also developed a technique for giving developers \naccess to a .exible spectrum of tradeoffs between RAM consumption, ROM consumption, and CPU ef.ciency. \nThis technique is based on a model for estimating the cost/bene.t ratio of compressing each variable \nand then selec\u00adtively compressing only those variables that present a good value proposition in terms \nof the desired tradeoffs. Categories and Subject Descriptors C.3 [Special-purpose and Application-based \nSystems]: Real-time and Embedded Systems; D.3.4 [Programming Languages]: Processors optimization General \nTerms Performance, Languages Keywords data compression, embedded software, memory opti\u00admization, static \nanalysis, TinyOS, sensor networks  1. Introduction In 2004, 6.8 billion microcontroller units (MCUs) \nwere shipped [5]: more than one per person on Earth. MCUs are small systems-on\u00adchip that permit software \ncontrol to be added to embedded devices at very low cost: a few dollars or less. Sophisticated electronic \nsystems, such as those running a modern automobile, rely heavily on MCUs. For example, in 2002 a high-end \ncar contained more than 100 processors [4]. RAM constraints are a .rst-order concern for developers. \nA typ\u00adical MCU has 0.01 100 KB of RAM, 4 32 times as much ROM as RAM, and no .oating point unit, memory \nmanagement unit, or caches. We refer to program memory as ROM since it is treated as such by applications, \neven though it can usually be written to (but slowly and in blocks, in the common case of .ash memory). \nPro- Permission to make digital or hard copies of all or part of this work for personal or classroom \nuse is granted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. PLDI \n07 June 11 13, 2007, San Diego, California, USA. Copyright c . 2007 ACM 978-1-59593-633-2/07/0006. . \n. $5.00 grams running on microcontrollers are usually limited to on-chip RAM. The cost of adding off-chip \nRAM is high, and many MCUs do not even provide an external memory bus. RAM constraints are one of the \nmain reasons why MCU software does not commonly take advantage of useful abstractions such as threads \nand a heap. We developed of.ine RAM compression, a source-to-source transformation that reduces the amount \nof memory used by a C program by encoding and bit-level packing global scalars, pointers, arrays, and \nstructures based on the results of static whole-program analysis. The goal of our work is to reduce the \nRAM usage of ex\u00adisting embedded software without a great deal of overhead and in a way that is predictable \nat compile time. Our work targets legacy C code and reduces RAM requirements automatically and transpar\u00adently. \nCompile-time predictability rules out online RAM compres\u00adsion, a family of techniques that .nd and exploit \nredundant data as a system executes [7, 32]. 1.1 Fundamental observations The following properties of \nembedded systems motivate our work. RAM is used inef.ciently. Although an n-bit machine word can store \n2n distinct values, in practice a typical word allocated by a program stores many fewer values. For example, \nBrooks and Martonosi [8] found that roughly 50% of the instructions [in SPECint95] had both operands \nless than or equal to 16 bits. To verify that small embedded systems behave similarly, we instru\u00admented \na simulator for Mica2 sensor network nodes to keep track of the number of distinct values stored in each \nbyte of RAM. Bytes are a reasonable unit of measurement since Mica2s are based on the 8-bit AVR architecture. \nWe then ran a collection of sensor net\u00adwork applications in the simulator. We found that, on average, \na byte of RAM used to store a global variable (or part of one) took on just under four values over the \nexecution of an application. In other words, six out of every eight bits of RAM allocated to global variables \nare effectively unused. On-chip RAM is persistently scarce in low-end systems. RAM is, and always has \nbeen, in short supply for small embedded sys\u00adtems that are constrained by power, size, and unit cost. \nFurther\u00admore, it is not at all clear that RAM constraints are going to go away in the foreseeable future. \nAs transistor cost decreases, it be\u00adcomes possible not only to create more capable processors at a given \nprice point, but also to create cheaper, lower-power proces\u00adsors with a given amount of RAM. Since many \nsectors of the em\u00adbedded market are extremely sensitive to unit cost, developers must often choose the \nsmallest, cheapest, and lowest-power MCU that meets their needs. The decreasing cost of MCUs with a given \nset of capabilities supports the development of new applications, such as sensor networks, that were \nnot previously feasible. Value functions for resource use are discontinuous. For desktop and workstation \napplications, value functions for resource use are generally continuous in the sense that a small increase \nin resource Figure 1. Characteristics and prices of some members of Atmel s AVR family, a popular line \nof 8-bit MCUs. Prices are from http: //digikey.com and other sources, for quantities of 100 or more. \nDevice ROM RAM Ratio Price ATtiny24 2 KB 128 B 16:1 $0.70 ATtiny45 4 KB 256 B 16:1 $0.97 ATmega48 4 KB \n512 B 8:1 $1.50 ATmega8 8 KB 1024 B 8:1 $2.06 ATmega32 32 KB 2048 B 8:1 $4.75 ATmega128 128 KB 4096 B \n32:1 $8.75 ATmega256 256 KB 8192 B 32:1 $10.66 use maps to a small decrease in utility. In contrast, \ngiven an embed\u00added processor with .xed amounts of on-chip RAM and ROM: A program requiring too much \nof either kind of storage simply cannot be run: it has no value.  Once a system .ts into RAM and ROM, \nfurther optimization provides no additional value.  These discontinuities imply that it is important \nto be able to trade off between resources like RAM size, ROM size, and CPU use. The ratio of ROM to RAM \non the MCUs in Figure 1 gives us a general idea of how much ROM we should be willing to sacri.ce to save \na byte of RAM. Of course, the actual exchange rate for a given system depends on what its limiting resource \nis. Manual RAM optimization is dif.cult. Most embedded software developers including ourselves have had \nthe experience of run\u00adning out of RAM with features still left to implement. Manually reducing RAM usage \nis dif.cult and error-prone. Furthermore, it is not forward-thinking: highly RAM-optimized software often \nbe\u00adcomes over-specialized and dif.cult to maintain or reuse.  1.2 Bene.ts From the point of view of \nan embedded software developer, our work has two concrete bene.ts. First, it can support basing a prod\u00aduct \non a less expensive, more energy-ef.cient MCU with less RAM. For example, the Robot2 application that \nis part of our benchmark suite (see Section 6) requires 368 bytes of RAM and runs on an Atmel ATmega8535 \nMCU with 512 bytes of RAM. The compressed version of Robot2 uses 209 bytes and therefore would .t onto \na cheaper part with 256 bytes of RAM. Figure 1 gives an idea of the potential cost savings. Savings of \npennies or dollars can add up to real money in the high-volume, low-margin markets for which MCUs are \ndesigned. The second important bene.t of our work is that it can enable substantially more functionality \nto be placed on an MCU with a given amount of RAM. For example, consider a sensor network developer who \nwishes to add network reprogramming [16] and link-level packet encryption [17] to an existing TinyOS \napplication that already uses the entire 4 KB of RAM on a Mica2 sensor network node. Since these features \nare orthogonal to application logic, both are designed as transparent add-ons. Obviously this only works \nif enough memory is available. Across a set of representative TinyOS applications, our tool reduces RAM \nusage by an average of 19% more than enough to support the addition of network reprogramming, which requires \n84 bytes of RAM (2.1% of the total), and encryption, which requires 256 bytes of RAM (6.3% of the total). \nWithout our work, this developer would be forced to manually reduce RAM usage by a total of 340 bytes: \nan unpleasant proposition at best.  1.3 Contributions Our research has three main contributions: 1. \nOf.ine RAM compression, a new technique for automatically reducing RAM usage of embedded software using \nwhole\u00adprogram analysis and source-to-source translation.  2. A novel technique supporting .exible tradeoffs \nbetween RAM size, ROM size, and CPU ef.ciency by estimating the cost/bene.t ratio of compressing each \nvariable and then selectively com\u00adpressing only the most pro.table, up to a user-con.gurable threshold. \n 3. A tool, CComp, that implements of.ine RAM compression and tradeoff-aware compilation. Although CComp \ncurrently targets C code for AVR processors, only a small part of our tool is architecture-speci.c. CComp \nis available at http: //www.cs.utah.edu/~coop/research/ccomp/ as open\u00adsource software.   2. Background: \nMicrocontroller-Based Embedded Software Software for MCUs is somewhat different from general-purpose \napplication code. These characteristics are largely programming\u00adlanguage independent, having more to \ndo with requirements of the domain and properties of the platform. Below are some key features of microcontroller \nsoftware; two represent complications that we are forced to handle and one is a restriction that we can \nexploit. Software is interrupt-driven. Interrupts are the only form of con\u00adcurrency on many MCU-based \nsystems, where they serve as an ef.cient alternative to threads. Interrupt-driven software uses di\u00adverse \nsynchronization idioms, some based on disabling interrupts and others based on volatile variables and \ndeveloper knowledge of hardware-atomic memory operations. The implication for our work is that we need \nto provide a sound data.ow analysis of global vari\u00adables even in the presence of unstructured, user-de.ned \nsynchro\u00adnization primitives. Locality is irrelevant. MCUs have relatively fast memory, no caches or TLBs, \nand minimal (three stage or smaller) pipelines. Thus, there are no performance gains to be had by improving \nspatial locality through RAM compression and, furthermore, it is dif.cult to hide the overhead of compressing \nand uncompressing data. The implication is that code implementing compression must be carefully tuned \nand that we must invest effort in optimizations. Memory allocation is static. The only form of dynamic \nmemory allocation on most MCU-based systems is the call stack. When RAM is very constrained, heaps are \nunacceptably unpredictable (e.g., fragmentation is dif.cult to reason about and allocations may fail). \nThe implication is that our pointer analysis can be in terms of static memory objects and stack objects. \n 3. Static Analysis CComp is our tool that performs of.ine RAM compression through static analysis and \nsource-to-source transformation. It borrows heavily from cXprop, our existing whole-program data.ow ana\u00adlyzer \nfor C [12]. cXprop is itself built upon CIL [20], a parser, typechecker, and intermediate representation \nfor C. Figure 2 shows the role of CComp in the toolchain. CComp is sound under a standard set of assumptions. \nMainly, the analyzed program must not perform out-of-bounds memory accesses, and inline assembly must \nbe well-behaved (no side effects visible at the source level). C source code executable Figure 2. Toolchain \nfor of.ine RAM compression This section describes both the previous analysis features that we borrowed \nfrom cXprop and the new ones implemented by CComp. 3.1 Inlining Before CComp proper runs, we run the \ncode being analyzed through an aggressive source-to-source function inlining pass. Our inliner is based \non a model of code bloat; its goal is to minimize the size of the eventual object code. Inlining increases \nthe size of functions, which improves the precision of our context-insensitive data.ow analysis. 3.2 \nValue set analysis Of.ine RAM compression is driven by the value set abstract do\u00admain, where abstract \nvalues are bounded-size sets of concrete val\u00adues. From cXprop, CComp inherited transfer functions not \nonly for the value set domain but also for the interval and bitwise domains. The interval domain represents \nabstract values as integer ranges, for example [16..200]. In the bitwise domain, each bit of a value \nis independently . (unknown), 0, or 1. Our choice of the value set domain was pragmatic: of the domains \nwe implemented, it pro\u00adduces the best results. Interval-domain analysis of our benchmark programs supports \n1.8% RAM compression and bitwise analysis supports 2.2% RAM compression, whereas value set analysis sup\u00adports \n12% RAM compression. Note, however, that our implemen\u00adtations of these three domains are not directly \ncomparable: we have extensively tuned our value set implementation for precision, for example by adding \nbackwards transfer functions that gain informa\u00adtion from branches. The maximum value set size is con.gurable; \nfor this work, we set it to 16. Larger sizes resulted in signi.cant analysis slowdown and only a minor \nincrease in precision.  3.3 Pointer set analysis The points-to analysis in CComp is based on a pointer \nset analy\u00adsis that maintains explicit sets of locations that each pointer may alias. This analysis is \nfairly precise because it is .ow-sensitive and .eld-sensitive. The pointer set domain works well for \nMCU-based applications because the set of items that are potentially pointed to is generally fairly small. \nA pointer set of cardinality one indicates a must-alias relationship, supporting strong updates of pointed-to \nvalues (a strong update replaces the existing state at a storage loca\u00adtion, whereas a weak update merges \nthe new information with the old). The pointer set domain, illustrated in Figure 3, is analogous to the \nvalue set domain except that it contains a special not-NULL element that permits the domain to represent \nthe (common) case where nothing is known about a pointer s value except that it can\u00adnot be NULL. not-NULL \n . Figure 3. Pointer set lattice for three variables and maximum set size of two 3.4 Concurrency analysis \nCComp can soundly analyze global variables even in the presence of interrupt-driven concurrency. It does \nthis using a two-element lattice to conservatively detect racing variables that are accessed directly \nor indirectly by multiple .ows (i.e., either by multiple interrupt handlers, or else by one interrupt \nhandler and by the non\u00adinterrupt context) and whose accesses are not uniformly protected by disabling \ninterrupts. Racing variables are considered to have the value . at all program points. Non-racing variables \nare always manipulated atomically and can be modeled soundly by identifying all program points where \ninterrupts might become enabled, and then adding .ow edges from these locations to the start of all interrupt \nhandlers. Data.ow analysis of racing variables can be performed us\u00ading essentially the same technique, \nwhere each access to a rac\u00ading variable is considered to be an atomic access, after which .ow edges to \ninterrupts must be added. We leave this feature in CComp turned off by default because in our experience \nit does not signi.cantly increase precision and because it is potentially un\u00adsound when language-level \nmemory operations are compiled down to non-atomic instruction sequences. 3.5 Integrated analyses A well-known \nway to avoid phase-ordering problems in compil\u00aders is to create integrated super-analyses that run multiple \nanal\u00adyses concurrently and permit them to exchange information with each other. For example, conditional \nconstant propagation [31] in\u00adtegrates constant propagation and dead code detection. CComp in\u00adtegrates \nall of its main analyses: value set analysis, dead code de\u00adtection, pointer set analysis, and concurrency \nanalysis. Although we have not quanti.ed the bene.ts of integrating these analyses, it seems clear that, \nat least in some cases, there was no reasonable alternative. For example, concurrency analysis cannot \nrun before pointer analysis (in this case aliases cannot be tracked accurately, forcing a highly pessimistic \nrace analysis) or after it (racing point\u00aders will have been analyzed unsoundly). 3.6 Whole-program optimization \nThe results of CComp s interprocedural data.ow analysis are used to perform interprocedural constant \npropagation, pointer optimiza\u00adtions, dead code elimination, dead data elimination, redundant syn\u00adchronization \nelimination, copy propagation, and branch, switch, and jump optimizations. 3.7 Modeling volatile variables \nThe C99 standard states that: An object that has volatile-quali.ed type may be modi.ed in ways unknown \nto the implementation or have other un\u00ad known side effects. In practice, volatile variables are not subject \nto most compiler optimizations. Similarly, early versions of CComp did not attempt to model volatiles, \ntreating them as perpetually .. Eventually, we realized not only that bottom-valued global vari\u00adables \nwere hurting CComp s precision (they tend to be contagious) but also that many volatile variables can \nbe analyzed soundly. The insight is that volatiles are only opaque at the language level. When non-portable \nplatform-level properties are known, different uses of the volatile quali.er can be identi.ed: 1. Storage \nlocations corresponding to device registers are volatile in the most literal sense: their values can \nchange at any time and both reads and writes may be side-effecting. Data .owing through these locations \ncannot be analyzed by CComp. 2. Global variables in regular RAM are often declared as volatile to prevent \ncompiler-level caching of values. These variables cannot, in fact, be modi.ed except by stores in the \nprogram which may occur in interrupt handlers. C does not have a se\u00admantics for interrupts, but CComp \ndoes, and so it can safety analyze and even compress volatile global variables.  CComp identi.es references \nto device registers heuristically, by matching against the two important idioms for accessing hardware \ndevices from C code. The .rst method is portable: a constant integer is cast into a pointer-to-volatile \nand then dereferenced. The second method is non-portable: gcc permits variables to be bound to speci.c \nmemory locations occupied by device registers. We claim that data.ow analysis through volatiles is sound \nunder the assumption that our heuristic correctly identi.es accesses to device registers.  3.8 Analyzing \narrays CComp represents each array using a single abstract value that tracks the greatest lower bound \nof any value stored in any element of the array. We had two reasons for choosing a collapsed array representation. \nFirst, it is often the case that the elements of a given array store similar values. For example, a common \nuse of arrays in networked embedded applications is to store a queue of pointers to packet buffers. In \nthis case, the pointer set stored in each element of the array is the same the set of buffers so our \nanalysis loses nothing compared to a more precise array model. Second, collapsed arrays are ef.cient. \n 3.9 Scalar replacement of aggregates CComp compresses values found in C structs by splitting them into \nscalars and then attempting to compress the scalars. We created a scalar replacement pass that avoids \nhazards such as address-taken structs and structs inside unions. It also takes care to retain call-by\u00advalue \nsemantics for arrays formerly inside structs. In C, structs are passed by value while arrays due to being \ntreated like constant pointers are passed by reference.  3.10 Emulating external functions cXprop treated \nexternal functions (those whose code is unavailable at analysis time) as safe or unsafe. Functions are \nunsafe by default, meaning that they must be modeled conservatively: when called they kill all address-taken \nvariables and furthermore they are as\u00adsumed to call back into address-taken functions in the application. \nA safe function is one like scanf that affects only program state passed to it through pointers. CComp \nimproves upon this analysis by adding two new categories of external functions. A pure func\u00adtion, such \nas strlen, has no side effects. An interpreted function has a user-de.ned effect on program state. For \nexample, we in\u00adterpret calls to memset in order to perform array initialization. We could almost never \ncompress an array before supporting this idiom. // compression function for 16-bit variables unsigned \nchar __f_16 (uint16_t * table, uint16_t val) { unsigned int index; for (index=0; ; index++) { if (pgm_read_word_near \n(table + index) == val) return index; } } // decompression function for 16-bit variables uint16_t __finv_16 \n(uint16_t * table, unsigned char index) { return pgm_read_word_near (table + index); } Figure 4. Compression \nand decompression functions based on ta\u00adble lookup. The function pgm read word near is an AVR primi\u00adtive \nfor accessing values from ROM.  4. Compressing RAM This section describes the theory and practice of \nof.ine RAM compression, including some optimizations. 4.1 Theory Let xbe a variable in a given program \nthat occupies nbits. Since the embedded systems under consideration use static memory al\u00adlocation, we \ncan speak of variables rather than more general mem\u00adory objects. Let Vx be a conservative estimate of \nthe set of val\u00adues that can be stored into xacross all possible executions. In the general case (i.e., \nin future implementations of of.ine RAM com\u00adpression), Vx could be computed by a type-based analysis, \nby a constraint-based analysis, by exhaustive testing, etc., as opposed to being computed using value \nset or pointer set analysis. Of.ine RAM compression can in principle be performed when |Vx| < 2n . However, \nin practice it should be the case that llog2 |Vx|l <n. In other words, compressing xby itself should \nresult in a savings of at least one bit. Exploiting the restricted size of Vx may be dif.cult because \nit may not be easy to represent the actual values in Vx compactly. A general solution is to .nd another \nset Cx with the same cardinality as Vx, and also to .nd a function fx that is a one-to-one and onto mapping \nfrom Vx to Cx. Then, fx is a compression function and its inverse fx -1 is a decompression function (one-to-one \nand onto functions can always be inverted). 4.2 Finding and implementing fx and fx -1 For each compressed \nvariable x,we.nd fx and fx -1 as follows: 0 = y<2[log2 |Vx|1 1. If xis an integer type and .y. Vx,, then \nthe trivial compression function fx(y)= ycan be used. Across our set of benchmark applications, 65% of \ncompressed variables fall into this case. 2. Otherwise, we let the elements of Cx be 0..|Vx|- 1 and \nlet fx and fx -1 be lookups in a compression table stored in ROM. The compression table is simply an \narray storing all members of Vx.  Figure 4 depicts our compression table lookup functions. De\u00adcompression \nuses the compressed value as an index into the com\u00adpression table, and compression involves a linear \nscan of the com\u00adpression table. Empirically, on the AVR MCUs that we use to eval\u00aduate of.ine RAM compression, \nlinear scan is faster than binary search, on average, for value set sizes up to 19. Another argument \nagainst binary search is that since CComp operates by source-to\u00adsource transformation, we are unable \nto order pointers by value. As we did this work, other implementations of compression and decompression \nsuggested themselves. For example, if x is an integer type and we can .nd a constant c such that .y . \nVx,c = y<c+2[log2 |Vx|1, then fx(y)= y - c is a valid compression function. However, we did not implement \nthis because we observed few cases where it would help.  4.3 Program transformation For each compressed \nvariable x, CComp performs the following steps: 1. If xrequires a compression table, allocate the table \nin ROM. 2. Convert x s initializer, if any, into the compressed domain. 3. Allocate space for xin a \nglobal compressed struct as a bit.eld of llog2 |Vx|l bits. 4. Rewrite all loads and stores of xto access \nthe compressed bit\u00ad.eld and go through compression and decompression functions.  CComp does not attempt \nto compress racing variables; this nat\u00adurally falls out of the concurrency analysis described in Section \n3.4 that treats racing variables as . at all program points. CComp also does not attempt to compress \n.oating point values. This was a de\u00adsign simpli.cation that we made based on the lack of .oating point \ncode in the MCU-based applications that we target. 4.4 An example Figure 5 illustrates RAM compression \nusing code from the stan\u00addard TinyOS application BlinkTask. The TOSH queue data struc\u00adture shown in Figure \n5(a) is at the core of the TinyOS task sched\u00aduler, which supports deferred function calls that help developers \navoid placing lengthy computations in interrupt handlers. Elements of the task queue are 16-bit function \npointers on the AVR archi\u00adtecture. Through static analysis, CComp shows that elements of the task queue \nhave pointer sets of size four. The contents of this pointer set are used to generate the compression \ntable shown in Fig\u00adure 5(b). Since each element of the task queue can be represented using two bits, \nan 8:1 compression ratio is achieved, saving a total of 14 bytes. Figure 5(c) shows part of the compressed \ndata region for the BlinkTask application. Finally, Figure 5(d) shows how the original application reads \na function pointer from the task queue and Figure 5(e) shows the transformed code emitted by CComp. \n4.5 A global synchronization optimization Since compressed data is accessed through non-atomic bit.eld \nop\u00aderations, an unprotected global compressed data region is effec\u00adtively a big racing variable. To avoid \nthe overhead of disabling in\u00adterrupts to protect accesses to compressed data, we instead decided to segregate \nthe compressed data into two parts: one containing variables whose accesses are protected by disabling \ninterrupts, the other containing variables that do not need to be protected because they are not accessed \nby concurrent .ows. Neither of the segregated compressed data regions requires explicit synchronization. \n 4.6 A global layout optimization As shown in Figure 5(c), we represent compressed RAM using C s bit.eld \nconstruct. The cost of a bit.eld access depends on the align\u00adment and size of the .eld being accessed. \nThere are three major cases for bit.eld sizes of less than a word: a bit.eld aligned on a word boundary, \na bit.eld that is unaligned and does not span multi\u00adple words, and a bit.eld that is unaligned and spans \nwords. Figure 6 summarizes the code size and performance costs of these different cases for our toolchain. \nOther architectures and compilers would (a) Original declaration of the task queue data structure: typedef \nstruct { void (*tp)(void); } TOSH_sched_entry_T; volatile TOSH_sched_entry_T TOSH_queue[8]; (b) Compression \ntable for the task queue (the progmem attribute places constant data into ROM): unsigned short const \n__attribute__((__progmem__)) __valueset_3[4] = { NULL, &#38;BlinkTaskM$processing, &#38;TimerM$HandleFire, \n&#38;TimerM$signalOneTimer }; (c) The compressed task queue is element f9 of the global com\u00adpressed data \nregion, which has room for eight two-bit com\u00adpressed values: struct __compressed { char f9[2] ; unsigned \nchar f0 : 3; unsigned char f1 : 3; unsigned char f7 : 1; ... }; (d) Original code for reading the head \nof the task queue: func = TOSH_queue[old_full].tp; (e) Code for reading the head of the compressed queue \n(the 2 passed to the array read function indicates that compressed entries are two bits long): __tmp \n= __array_read (__compressed.f9, old_full, 2); func = __finv_16 (__valueset_3, __tmp); Figure 5. Compression \ntransformation example for the main TinyOS 1.x scheduler data structure: a FIFO queue of function pointers \nfor deferred interrupt processing access type read write bytes cycles bytes cycles aligned 5 10 9 18 \nunaligned 7.2 14.4 11.2 22.4 spanning 13 26 21 42 Figure 6. Average bit.eld access costs for the AVR \nprocessor and gcc have different costs, but we would expect the ratios to be roughly the same since the \nunaligned and spanning cases fundamentally ne\u00adcessitate extra ALU and memory operations. Rather than \nattempting to compute an optimal layout for com\u00adpressed data, we developed an ef.cient greedy heuristic \nthat at\u00adtempts to meet the following two goals. First, the compressed vari\u00adables with the most static \naccesses should be byte-aligned. On the AVR architecture, words are bytes. Second, no compressed variable \nshould span multiple bytes. The heuristic operates as follows: 1. For each positive integer nless than \nthe number of compressed variables: (a) Align on byte boundaries the n variables with the largest number \nof static accesses. (b) Starting with the largest (compressed size) remaining vari\u00adable, pack it into \nthe structure under the constraint that it may not span two bytes. A variable is placed in the .rst lo\u00adcation \nwhere it .ts, extending the structure if it .ts nowhere else. This step is repeated until there are no \nremaining vari\u00adables to pack. (c) Fail if there is any wasted space in the packed structure, oth\u00aderwise \nsucceed. Note that it is possible for there to be holes in the packed structure without wasting space. \nInstead of al\u00adlowing the compiler-added padding at the end of a struct for alignment purposes, we disperse \nthe padding throughout the struct.  2. Choose the succeeding result for the largest n. In practice this \nheuristic works well, and in fact it almost always succeeds in byte-aligning the maximum possible number \nof high\u00adpriority compressed variables (that is, one per byte of compressed data). The heuristic can fail \nin the situation where it is forced to choose between wasting RAM and permitting a variable to span multiple \nbytes, but we have not yet seen this happen for any input that occurs in practice. If it does fail, we \nwarn the user and back off to the unordered compressed structure.  4.7 Local optimizations We implemented \nseveral additional optimizations. First, duplicate compression tables are eliminated, saving ROM. Second, \nwhen an application stores a constant into a compressed variable, the compression table lookup can be \nperformed at compile time. gcc cannot perform this optimization on its own. Finally, we have gcc inline \nour compression and decompression functions. Since these functions are small, this gives a reasonable \nspeedup with minimal code bloat.  5. Tradeoff-Aware RAM Compression This section explores the idea of \ncomputing a cost/bene.t ratio for each compressible variable and then selectively compressing only a \nhighly pro.table subset of the compressible variables. In other words, we give users a RAM compression \nknob to turn, where the 100% setting compresses all compressible variables, the 0% setting performs no \ncompression, and at points in between the system attempts to give up as little ROM or CPU time as possible \nwhile achieving the speci.ed level of compression. Tradeoff-aware compilation can help developers exploit \nthe discontinuous nature of value functions for resource use (Section 1.1) by, for example, compiling \nan application such that it just barely .ts into RAM, into ROM, or into an execution time budget. Our \nRAM/ROM tradeoff is based on static information while our RAM/CPU tradeoff is based on pro.le information. \nWe could have made the RAM/CPU tradeoff by using standard heuristics based on static information (e.g., \neach loop executes 10 times ). However, we were reluctant to do so because in an interrupt-driven system, \nthe dynamic execution count of each function is strongly dependent on the relative frequencies with which \ndifferent inter\u00adrupts .re. The key to producing good tradeoffs is to accurately estimate the cost/bene.t \nratio of compressing each variable. We compute this ratio as follows: 6 X 1 Cost/Bene.t Ratio = Ci(Ai \n+ BiV ) (1) Su - Sc i=1 Su is original size of the compressible object, Sc is the compressed size, C \nis an access pro.le: a vector of the static or dynamic counts of the basic operations required to compress \nthe variable, A and B are vectors of platform-speci.c, empirically determined cost pa\u00adFigure 7. Two sets \nof parameters that can be plugged into Equa\u00adtion 1 to support either trading RAM for ROM or RAM for CPU \ncycles access type size cost (bytes) speed cost (cycles) bit.eld read 6.1 12.2 bit.eld write 10.1 20.2 \narray read 40 90 array write 60 120 decompress 24 16 compress 14 20 + 9.5V  rameters, and V is the cardinality \nof the variable s value set. Fig\u00adure 7 shows two sets of parameters, one in terms of ROM cost and the \nother in terms of CPU cost, which we computed for the AVR processor. Most of the B constants are zero, \nmeaning that opera\u00adtions have constant cost. The exception is the cycle cost to com\u00adpress a value, which \ninvolves a linear scan of the compression table as shown in Figure 4. In some cases these costs are average \n.gures across several datatypes. For example, since AVR is an 8-bit archi\u00adtecture, compressing a char \nis cheaper than compressing an int. We could have made this information more .ne-grained by break\u00ading \nout more sub-cases, but we judged that this had diminishing returns. The static count of each basic compression \nand decompression operation naturally falls out of the compilation process. These counts are taken after \nthe optimizations described in Section 4 have been performed. We measured dynamic operation counts by \nrunning applications in a modi.ed version of the Avrora sensor network simulator [29]. 6. Evaluation \nWe evaluate of.ine RAM compression by answering several ques\u00adtions: How much RAM is saved? What is the \ncost in terms of ROM usage and execution time? Are tradeoffs between resources effec\u00adtive? What is the \ncost in terms of analysis time? Our evaluation is based on a collection of embedded applica\u00adtions for \nAtmel AVR 8-bit microcontrollers: Two robot control applications, Robot1 and Robot2, emitted by KESO \n[27], an ahead-of-time Java-to-C compiler for con\u00adstrained embedded systems. They are 2526 and 3675 lines \nof code (LOC), respectively.  An avionics control application, UAVcontrol, for a small un\u00admanned aerial \nvehicle developed by the Paparazzi project [22] (4215 LOC).  Eleven sensor networking applications from \nTinyOS [15] 1.1, emitted by the nesC [13] compiler (5800 39000 LOC).  In all cases our baseline is \nthe RAM usage, ROM usage, and CPU usage of the out-of-the-box version of the application as compiled \nby avr-gcc version 3.4.3. For the TinyOS applications, we used Avrora [29], a cycle-accurate sensor network \nsimulator, to measure ef.ciency in terms of duty cycle the fraction of time the processor is active. \nThis is a good metric because it directly correlates to energy usage and hence system lifetime: a primary \nconsideration for sensor networks. We did not measure the ef.ciency of the KESO or Paparazzi applications; \nthey run on custom hardware platforms with many peripherals. Extending Avrora to simulate enough hardware \nthat we could run these applications was beyond the scope of our work. CComp does not change the semantics \nof an application unless it contains analysis or transformation bugs. To defend against this possibility, \nwe validated many of our compressed applications. This duty cycle code size data size Change from optimization \n** * 40% 30% 20% 10% 0% -10% -20% -30% -40% Robot1 Robot2IdentUAVcontrolOscilloscopeGenericBaseRfmToLedsTestTinySecAVERAGETestTimeStampingTestDrip \nSurgeCntToLedsAndRfmSenseToRfmHighFrequencySamplingTinyDB  was not totally straightforward because \nunlike benchmarks, real embedded applications do not simply transform inputs into outputs. Our validation \nwas by hand, by checking that applications exhibited the expected behavior. For example, for Surge, a \nmultihop routing application, we ensured that routes were formed and packets were properly forwarded \nacross the network to the base station. We did not validate the applications for which we have no simulation \nenvironment: Robot1, Robot2, and UAVcontrol. 6.1 Effectiveness of optimizations Section 4.2 mentions \nthat across our benchmark suite, 65% of com\u00adpressed variables meet our criteria for using simple compression \nand decompression functions, as opposed to indirection through a ROM-based compression table. On average, \nthis optimization re\u00adduces code size by 11% and duty cycle by 22%. Intelligent layout of bit.elds in \nthe global compressed data structure (Section 4.6) reduces code size by 2.7% and duty cycle by 1.9%. \n 6.2 Per-application resource usage results Figures 8 11 compare applications processed by CComp against \na baseline of out-of-the-box applications in terms of code size, data size, and duty cycle. Asterisks \nin these .gures mark data points that are unavailable because we cannot simulate some applications. Figure \n8 shows that when RAM compression is disabled, CComp reduces usage of all three resources through the \noptimiza\u00adtions described in Section 3.6. On the other hand, Figure 9 shows that maximum RAM compression \nresults in signi.cantly greater RAM savings: 22%, as opposed to 10% in Figure 8. The problematic increase \nin average duty cycle shown in Fig\u00adure 9 indicates that RAM compression can be expensive when hot variables \nare compressed. The general-purpose solution to this problem is tradeoff-aware compilation. 6.3 Results \nfrom tradeoffs Figures 10 and 11 show the effect on our benchmark applications of turning the RAM compression \nknob down to 90%. That is, reducing RAM compression to 10% below the maximum in order to buy as much \ncode size or duty cycle as possible. Figures 12 and 13 show the full spectrum of tradeoffs from 0% to \n100% RAM compression. The most important thing to notice about these graphs is that sacri.cing a small \namount of RAM buys a major decrease in duty cycle and a signi.cant decrease in code size. The steepness \nof these curves near 100% RAM compression indicates that our cost functions work well. Figure 14 is a \ndifferent way to look at the data from Figures 12 and 13. The diamond shape traced by the data points \nin this para\u00admetric plot provides additional validation that our tradeoff strate\u00adgies are working properly. \n 6.4 Analysis time The median time to apply of.ine RAM compression to members of our benchmark suite \nis 62 s. The minimum is 2 s and the maximum is 94 minutes. Only two applications (TestTinySec and TinyDB) \nrequire more than .ve minutes. CComp is a research prototype and we have not optimized it for speed. \n  7. Related Work There is substantial body of literature on compiler-based RAM op\u00adtimizations. Here \nwe discuss representative publications from sev\u00aderal categories of research that are related to ours. \nWe do not dis\u00adcuss the literature on code compression and code size optimization, which is largely orthogonal \nto our work. Compiler-based of.ine RAM size optimization. Ananian and Ri\u00adnard [1] perform static bitwidth \nanalysis and .eld packing for Java  Percent of compressible RAM compressed  Percent of compressible \nRAM compressed  Percent change in duty cycle objects (among other techniques less closely related to \nour work). Zhang and Gupta [33] use memory pro.le data to .nd limited\u00adbitwidth heap data that can be \npacked into less space. Lattner and Adve [18] save RAM through a transformation that makes it pos\u00adsible \nto use 32-bit pointers on a per-data-structure basis, on archi\u00adtectures with 64-bit native pointers. \nChanet et al. [10] apply whole\u00adprogram optimization to the Linux kernel at link time, reducing both RAM \nand ROM usage. Virgil [28] has a number of of.ine RAM optimizations including reachable members analysis, \nrefer\u00adence compression, and moving constant data into ROM. Our work exploits the same basic insight as \nthese previous ef\u00adforts, but it differs in several key ways. First, we have taken a whole-system approach \nto compressing RAM for legacy C appli\u00adcations in the presence of interrupt-driven concurrency. Most pre\u00advious \nwork has focused on benchmarks rather than attacking actual embedded applications. Second, our value \nset and pointer set do\u00admains appear to work signi.cantly better than do the interval and bitwise domains \nused to analyze bitwidth in most previous work. Third, we perform tight bit-level packing across multiple \nvariables to achieve good savings on very small platforms. Fourth, we com\u00adpress scalars, pointers, structures, \nand arrays. Most previous work has focused on some subset of these kinds of data. Finally, we have carefully \nfocused on optimizations and tradeoffs to avoid slowing down and bloating applications unacceptably. \nThis focus was nec\u00adessary due to the constrained nature of the hardware platforms that we target and \nalso because on cacheless systems there is no possi\u00adbility of hiding overheads by improving locality. \nA separate body of work performs of.ine RAM optimization using techniques that exploit the structure \nof runtime systems rather than (or in addition to) exploiting the structure of application data. Barthelmann \n[6] describes inter-task register allocation, a global optimization that saves RAM used to store thread \ncontexts. Our previous work [25] addressed the problem of reducing stack mem\u00adory requirements through \nselective function inlining and by restrict\u00ading preemption relations that lead to large stacks. Grunwald \nand Neves [14] save RAM by allocating stack frames on the heap, on demand, using whole-program optimization \nto reduce the number of stack checks and to make context switches faster. We believe these techniques \nto be complementary to CComp: they exploit dif\u00adferent underlying sources of savings than we do. Static \nbitwidth analysis. A number of researchers, including Razdan and Smith [24], Stephenson et al. [26], \nBudiu and Gold\u00adstein [9], Ananian and Rinard [1], Verbrugge et al. [30], and our\u00adselves [12] have developed \ncompiler analyses to .nd unused parts of memory objects. Our research builds on this previous work, in\u00adnovating \nin a few areas such as analyzing data.ow through volatile variables. Online RAM size optimization. The \nconstrained nature of RAM in embedded systems is well known and a number of research ef\u00adforts have addressed \nthis problem using online schemes that dy\u00adnamically recover unused or poorly used space. Biswas et al. \n[7] and Middha et al. [19] use compiler-driven techniques to blur the lines between different storage \nregions. This permits, for exam\u00adple, stacks to over.ow into unused parts of the heap, globals, or other \nstacks. CRAMES [32] saves RAM by applying standard data compression techniques to swapped-out virtual \npages, based on the idea that these pages are likely to remain unused for some time. MEMMU [2] provides \non-line compression for systems without a memory management unit, such as wireless sensor network nodes. \nOzturk et al. [21] compress data buffers in embedded applications. In contrast with our work, these techniques \nare opportunistic and not guaranteed to work well for any particular run of a system. Online RAM optimizations \nare most suitable for larger embedded platforms with RAM sized in hundreds of KB or MB where statistically \nthere are always enough opportunities for compres\u00adsion to provide good results. Also, most online RAM \noptimiza\u00adtions incur unpredictable execution time overheads (for example, uncompressing a memory page \non demand) and therefore may not be applicable to real-time systems. RAM layout optimizations. A signi.cant \nbody of literature ex\u00adists on changing the layout of objects in memory to improve per\u00adformance, usually \nby improving spatial locality to reduce cache and TLB misses. Good examples include Chilimbi et al. s \nwork on cache-conscious structure layout [11] and Rabbah and Palem s work on data remapping [23]. This \ntype of research relates mostly to our compressed structure layout work in Section 4.6. As far as we \nknow, no existing work has addressed the problem of bit.eld layout to minimize code size or execution \ntime, as we have. Value-set-based pointer analysis. Abstract values in our value set and pointer set \ndomains are sets of explicit concrete values. Balakr\u00adishnan and Reps [3] used value set analysis to analyze \npointers in executable code, but they use the term differently than we do. Their abstract values are \nsets of reduced interval congruences a highly specialized domain tuned to match x86 addressing modes. \n 8. Conclusions We developed a novel method for of.ine RAM compression in em\u00adbedded software that employs \nstatic whole-program analysis in the value set and pointer set domains, followed by source-to-source \ntransformation. We have shown that across a collection of embed\u00added applications targeting small microcontrollers, \ncompression re\u00adduces an application s RAM requirements by an average of 12%, in addition to a 10% savings \nthrough a dead data elimination pass that is also driven by our whole-program analysis. This result is \nsig\u00adni.cant because RAM is often the limiting resource for embedded software developers, and because \nthe programs that we started with had already been tuned for memory ef.ciency. Our second main contribution \nis a tradeoff-aware compilation technique that, given a goal in terms of RAM savings, attempts to meet \nthat goal while giving up as little code size or as few processor cycles as possible. Finally, we have \ncreated a tool, CComp, that implements of.ine RAM compression and tradeoff-aware compilation for embedded \nC programs. Acknowledgments We thank Eric Eide, Alastair Reid, and Bjorn De Sutter for their helpful \ncomments on drafts of this paper. This work is supported by National Science Foundation CAREER Award \nCNS-0448047. References [1] C. Scott Ananian and Martin Rinard. Data size optimizations for Java programs. \nIn Proc. of the 2003 Conf. on Languages, Compilers, and Tools for Embedded Systems (LCTES), pages 59 \n68, San Diego, CA, June 2003. [2] Lan S. Bai, Lei Yang, and Robert P. Dick. Automated compile\u00adtime and \nrun-time techniques to increase usable memory in MMU\u00adless embedded systems. In Proc. of the Intl. Conf. \non Compilers, Architecture, and Synthesis for Embedded Systems (CASES), Seoul, Korea, October 2006. [3] \nGogul Balakrishnan and Thomas Reps. Analyzing memory accesses in x86 executables. In Proc. of the Intl. \nConf. on Compiler Construction (CC), pages 5 23, Bonita Springs, FL, April 2004. [4] Ross Bannatyne. \nMicrocontrollers for the automobile. Micro Control Journal, 2004. http://www.mcjournal.com/articles/ \narc105/arc105.htm. [5] Max Baron and Cheryl Cadden. Strong growth to continue for MCU market, 2005. http://www.instat.com/press.asp?ID= \n1445&#38;sku=IN0502457SI. [6] Volker Barthelmann. Inter-task register-allocation for static operating \nsystems. In Proc. of the Joint Conf. on Languages, Compilers, and Tools for Embedded Systems (LCTES) \nand Software and Compilers for Embedded Systems (SCOPES), pages 149 154, Berlin, Germany, June 2002. \n[7] Surupa Biswas, Matthew Simpson, and Rajeev Barua. Memory over\u00ad.ow protection for embedded systems \nusing run-time checks, reuse and compression. In Proc. of the Conf. on Compilers, Architecture, and Synthesis \nfor Embedded Systems (CASES), Washington, DC, September 2004. [8] David Brooks and Margaret Martonosi. \nDynamically exploiting narrow width operands to improve processor power and performance. In Proc. of \nthe 5th Intl. Symp. on High Performance Computer Architecture (HPCA), Orlando, FL, January 1999. [9] \nMihai Budiu, Majd Sakr, Kip Walker, and Seth Copen Goldstein. BitValue inference: Detecting and exploiting \nnarrow bitwidth computations. In Proc. of the European Conf. on Parallel Processing (EUROPAR),M\u00a8 unich, \nGermany, August 2000. [10] Dominique Chanet, Bjorn De Sutter, Bruno De Bus, Ludo Van Put, and Koen De \nBosschere. System-wide compaction and specialization of the Linux kernel. In Proc. of the 2005 Conf. \non Languages, Compilers, and Tools for Embedded Systems (LCTES), Chicago, IL, June 2005. [11] Trishul \nM. Chilimbi, Bob Davidson, and James R. Larus. Cache\u00adconscious structure de.nition. In Proc. of the ACM \nSIGPLAN 1999 Conf. on Programming Language Design and Implementation (PLDI), pages 13 24, Atlanta, GA, \nMay 1999. [12] Nathan Cooprider and John Regehr. Pluggable abstract domains for analyzing embedded software. \nIn Proc. of the 2006 Conf. on Languages, Compilers, and Tools for Embedded Systems (LCTES), pages 44 \n53, Ottawa, Canada, June 2006. [13] David Gay, Phil Levis, Robert von Behren, Matt Welsh, Eric Brewer, \nand David Culler. The nesC language: A holistic approach to networked embedded systems. In Proc. of the \nConf. on Programming Language Design and Implementation (PLDI), pages 1 11, San Diego, CA, June 2003. \n[14] Dirk Grunwald and Richard Neves. Whole-program optimization for time and space ef.cient threads. \nIn Proc. of the 7th Intl. Conf. on Architectural Support for Programming Languages and Operating Systems \n(ASPLOS), pages 50 59, Cambridge, MA, October 1996. [15] Jason Hill, Robert Szewczyk, Alec Woo, Seth \nHollar, David Culler, and Kristofer Pister. System architecture directions for networked sensors. In \nProc. of the 9th Intl. Conf. on Architectural Support for Programming Languages and Operating Systems \n(ASPLOS), pages 93 104, Cambridge, MA, November 2000. [16] Jonathan W. Hui and David Culler. The dynamic \nbehavior of a data dissemination protocol for network programming at scale. In Proc. of the 2nd ACM Conf. \non Embedded Networked Sensor Systems (SenSys), pages 81 94, 2004. [17] Chris Karlof, Naveen Sastry, and \nDavid Wagner. TinySec: A link layer security architecture for wireless sensor networks. In Proc. of the \n2nd ACM Conf. on Embedded Networked Sensor Systems (SenSys), Baltimore, MD, November 2004. [18] Chris \nLattner and Vikram Adve. Transparent pointer compression for linked data structures. In Proc. of the \nACM Workshop on Memory System Performance (MSP), Chicago, IL, June 2005. [19] Bhuvan Middha, Matthew \nSimpson, and Rajeev Barua. MTSS: Multi task stack sharing for embedded systems. In Proc. of the Intl. \nConf. on Compilers, Architecture, and Synthesis for Embedded Systems (CASES), San Francisco, CA, September \n2005. [20] George C. Necula, Scott McPeak, S. P. Rahul, and Westley Weimer. CIL: Intermediate language \nand tools for analysis and transformation of C programs. In Proc. of the Intl. Conf. on Compiler Construction \n(CC), pages 213 228, Grenoble, France, April 2002. [21] Ozcan Ozturk, Mahmut Kandemir, and Mary Jane \nIrwin. Increasing on-chip memory space utilization for embedded chip multiprocessors through data compression. \nIn Proc. of the 3rd Intl. Conf. on Hardware/Software Codesign and System Synthesis (CODES+ISSS), pages \n87 92, Jersey City, NJ, September 2005. [22] The Paparazzi project, 2006. http://www.nongnu.org/ paparazzi. \n[23] Rodric M. Rabbah and Krishna V. Palem. Data remapping for design space optimization of embedded \nmemory systems. ACM Trans. Embedded Computing Systems (TECS), 2(2):1 32, May 2003. [24] Rahul Razdan \nand Michael D. Smith. A high-performance microar\u00adchitecture with hardware-programmable functional units. \nIn Proc. of the 27th Intl. Symp. on Microarchitecture (MICRO), pages 172 180, San Jose, CA, November \n1994. [25] John Regehr, Alastair Reid, and Kirk Webb. Eliminating stack over.ow by abstract interpretation. \nACM Transactions on Embedded Computing Systems (TECS), 4(4):751 778, November 2005. [26] Mark Stephenson, \nJonathan Babb, and Saman Amarasinghe. Bitwidth analysis with application to silicon compilation. In Proc. \nof the Conf. on Programming Language Design and Implementation (PLDI), pages 108 120, Vancouver, Canada, \nJune 2000. [27] Michael Stilkerich, Christian Wawersich, Wolfgang Schr\u00a8oder-Preikschat, Andreas Gal, \nand Michael Franz. An OSEK/VDX API for Java. In Proc. of the 3rd Workshop on Programming Languages and \nOperating Systems (PLOS), San Jose, CA, October 2006. [28] Ben L. Titzer. Virgil: Objects on the head \nof a pin. In Proc. of the ACM Conf. on Object-Oriented Programming, Systems, Languages, and Applications \n(OOPSLA), Portland, OR, October 2006. [29] Ben L. Titzer, Daniel Lee, and Jens Palsberg. Avrora: Scalable \nsensor network simulation with precise timing. In Proc. of the 4th Intl. Conf. on Information Processing \nin Sensor Networks (IPSN), Los Angeles, CA, April 2005. [30] Clark Verbrugge, Phong Co, and Laurie Hendren. \nGeneralized constant propagation: A study in C. In Proc. of the Intl. Conf. on Compiler Construction \n(CC), Link\u00a8oping, Sweden, April 1996. [31] Mark N. Wegman and F. Kenneth Zadeck. Constant propagation \nwith conditional branches. ACM Transactions on Programming Languages and Systems (TOPLAS), 13(2):181 \n210, April 1991. [32] Lei Yang, Robert P. Dick, Haris Lekatsas, and Srimat Chakradhar. On-line memory \ncompression for embedded systems. ACM Trans. Embedded Computing Systems (TECS), 2006. [33] Youtao Zhang \nand Rajiv Gupta. Compressing heap data for improved memory performance. Software Practice and Experience, \n36(10):1081 1111, August 2006. \n\t\t\t", "proc_id": "1250734", "abstract": "<p>We present <i>offline RAM compression</i>, an automated source-to-source transformation that reduces a program's data size. Statically allocated scalars, pointers, structures, and arrays are encoded and packed based on the results of a whole-program analysis in the value set and pointer set domains. We target embedded software written in C that relies heavily on static memory allocation and runs on Harvard-architecture microcontrollers supporting just a few KB of on-chip RAM. On a collection of embedded applications for AVR microcontrollers, our transformation reduces RAM usage by an average of 12%, in addition to a 10% reduction through a dead-data elimination pass that is also driven by our whole-program analysis, for a total RAM savings of 22%. We also developeda technique for giving developers access to a flexible spectrum of tradeoffs between RAM consumption, ROM consumption, and CPU efficiency. This technique is based on a model for estimating the cost/benefit ratio of compressing each variable and then selectively compressing only those variables that present a good value proposition in terms of the desired tradeoffs.</p>", "authors": [{"name": "Nathan Dean Cooprider", "author_profile_id": "81314488266", "affiliation": "University of Utah, Salt Lake City, UT", "person_id": "P871687", "email_address": "", "orcid_id": ""}, {"name": "John David Regehr", "author_profile_id": "81100459621", "affiliation": "University of Utah, Salt Lake City, UT", "person_id": "P615812", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1250734.1250776", "year": "2007", "article_id": "1250776", "conference": "PLDI", "title": "Offline compression for on-chip ram", "url": "http://dl.acm.org/citation.cfm?id=1250776"}