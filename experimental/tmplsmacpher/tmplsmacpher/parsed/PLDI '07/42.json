{"article_publication_date": "06-10-2007", "fulltext": "\n CGCExplorer: A Semi-Automated Search Procedure for Provably Correct Concurrent Collectors Martin T. \nVechev Eran Yahav David F. Bacon Noam Rinetzky Cambridge University IBM Research IBM Research Tel Aviv \nUniversity Abstract Concurrent garbage collectors are notoriously hard to design, im\u00adplement, and verify. \nWe present a framework for the automatic ex\u00adploration of a space of concurrent mark-and-sweep collectors. \nIn our framework, the designer speci.es a set of building blocks from which algorithms can be constructed. \nThese blocks re.ect the designer s insights about the coordination between the collector and the mutator. \nGiven a set of building blocks, our framework au\u00adtomatically explores a space of algorithms, using model \nchecking with abstraction to verify algorithms in the space. We capture the intuition behind some common \nmark-and-sweep algorithms using a set of building blocks. We utilize our framework to automatically explore \na space of more than 1, 600, 000 algo\u00adrithms built from these blocks, and derive over 100 correct .ne\u00adgrained \nalgorithms with various space, synchronization, and preci\u00adsion tradeoffs. Some tasks are best done by \nmachine, while others are best done by human insight; and a properly designed system will .nd the right \nbalance. D. Knuth Categories and Subject Descriptors D.1.3 [Concurrent Pro\u00adgramming]; D.2.4 [Program \nVeri.cation]; D.4.2 [Storage Man\u00adagement]: garbage collection General Terms Veri.cation, Algorithms Keywords \nconcurrent garbage collection, concurrent algorithms, veri.cation, synthesis 1. Introduction The design, \nimplementation, and veri.cation of concurrent garbage collection algorithms are important and challenging \ntasks: As garbage-collected languages like Java and C# become more and more widely used, the long pauses \ncaused by traditional syn\u00adchronous ( stop the world ) collection are unacceptable in many domains. Unfortunately, \nconcurrent collectors are extremely com\u00adplex and error-prone. This work attempts to provide a systematic \nmethod for the de\u00adsign and veri.cation of concurrent mark and sweep collection al\u00adgorithms. In our approach, \nthe algorithm designer provides the Permission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior \nspeci.c permission and/or a fee. PLDI 07 June 11 13, 2007, San Diego, California, USA. Copyright c . \n2007 ACM 978-1-59593-633-2/07/0006. . . $5.00. insights behind the algorithm and utilizes our work to \nautomati\u00adcally turn these insights into provably correct collection algorithms. More speci.cally, the \ndesigner speci.es a set of building blocks that re.ect her insights about the coordination between the \ncollec\u00adtor and the mutator (the metadata they share and some constraints on how it may be used). The \nsystem then automatically explores the induced space of algorithms, outputting a number of different \ncor\u00adrect collectors. The correctness of the results is ensured by using specially-designed model-checking \nwith abstraction. When a full coverage of the induced space turns out to be infeasible, we expect the \ndesigner to simplify the search. Interestingly, we found that this form of an iterative design can also \nbene.t the designer: she can start the exploration by specifying only a few building blocks and placing \nsevere constraints; use our framework; study the resulting algorithm; detect a common pattern; and reuse \nour framework with a more sophisticated input constraining our system to respect the common pattern. \nUsing this form of a staged design, we were able to discover over 100 new (correct) algorithm variations \nwhere the only atomicity constraints are at the granularity of a single building block. Technically, \nour work builds on the parametric concurrent mark\u00adand-sweep collection framework of [24]. Their framework \nallowed the derivation of many abstract algorithms by starting from a single simple, very precise, but \nmaximally atomic algorithm instantiation. While the algorithms were simple to derive and prove, they \nwere not directly implementable in an ef.cient manner. In particular, the mutator write barrier, and \nthe individual marking steps were imple\u00admented using large, heavy-weight, atomic sections. Moreover, \nthe time complexity of the algorithms was not linear, their space con\u00adsumption would exceed that of the \nheap being collected by a sig\u00adni.cant constant factor, and their synchronization overhead would be very \nhigh. In this work, we concentrate on transforming a subset of the ab\u00adstract algorithms from [24] into \ncorrect and more lightweight algo\u00adrithms. Speci.cally, we transform the large, heavy-weight, atomic sections \nthat form the core of the algorithm of [24] to use a small amount of per-object metadata and perform \nonly a limited amount of computation on each step, resulting in algorithms that are linear in the size \nof the heap and the amount of mutation. Our work provides a signi.cant step towards the synthesis of \nconcurrent collectors. We take this step in a modest setting in which the system is comprised of a single \ncollector and a single mutator. We also limit the scope of the synthesis to the mark step of mark-and-sweep \nalgorithms, because this is the part which is most dif.cult to construct correctly. The .nal step in \ngenerating an actual implementation would be to parameterize the abstract costs of this model by machine\u00addependent \nparameters and the expected application parameters. This information could then be used to automatically \nselect an algorithm for a speci.c application and to turn the building blocks into machine code. We leave \nthis as future work. 1.1 Related Work There has been little work on using formal techniques to automati\u00adcally \ndiscover and verify interesting and practical concurrent algo\u00adrithms. However, in the .eld of garbage \ncollection there are several works using formal approaches to verify algorithms. The works of [8, 9] \nde.ne a framework to describe genera\u00adtional and conservative collectors. However, it only deals with \nstop\u00adthe-world algorithms. [9] presents a transformational approach us\u00ading the SETL wide spectrum language \nto specify an initially cor\u00adrect and inef.cient implementation of a stop-the-world collector. Through \nloop fusion and formal differentiation transformations, they obtain a more precise implementation of \na well-known stop\u00adthe-world algorithm. In contrast, we derive concurrent collectors. In [5], a stop-the-world \ncollector is modeled, but not veri.ed using CCS. In [4], separation logic is used to prove the correctness \nof a stop-the-world copying garbage collector. Several works formally verify the correctness of Ben-Ari \ns and Dijkstra s algorithms presented in [3, 10]. Ben-Ari s algorithm has made simpli.cations to Dijkstra \ns algorithm with the sole purpose of having an algorithm which is easier to prove. The quadratic complexity \nof these algorithms is largely due to author s intention of making these algorithms simple and easy to \nverify. In [11], Dijkstra s algorithm is veri.ed using the Owicki-Gries logic. A correction to the article \nwas published in [12]. In [18], Ben-Ari s algorithm is veri.ed for both single and multi-mutator systems \nusing Owicki-Gries s logic in the Isabelle/HOL theorem prover [17]. In the work of [14], again Ben-Ari \ns algorithm is veri\u00ad.ed using the PVS theorem proving system. Similar work has been done by [19], where \nhe proves Ben-Ari s algorithm but this time in Boyer-Moore s theorem prover. In [15], Dijkstra s algorithm \nhas been veri.ed again in the PVS theorem prover. The paper of [6], proves Ben-Ari s algorithm using \nthe B and Coq systems. In [22], the authors use model checking with abstraction to verify Dijkstra s \nand Yuasa s algorithms. In addition, they use a search technique to generate algorithms (and then verify \nthem). However, the algorithm space they consider is rather limited, and the algorithms they derive do \nnot vary in concurrency. The work of [16] on superoptimization .nds the shortest in\u00adstruction sequence \nto compute a function. The state space of these algorithms is bounded, while in our problem the state \nspace is un\u00adbounded. In the work of [1], the authors deal with mutual exclusion al\u00adgorithms. They perform \nsyntactic exploration and discover various interesting algorithms, some of which are better than known \nsolu\u00adtions under the given space constraints. The authors employ a cus\u00adtom model checker and various \nheuristics to reduce the state space. However, the state space of these algorithms is bounded a priori, \nwhile in our problem the state space is unbounded. In sketching [21], the user provides a reference program \nof the desired implementation and some sketches which partially speci.es certain optimized functions. \nThe sketching compiler automatically .lls in the missing low-level details to create an optimized imple\u00admentation. \nSketching has been used to synthesize several bitstream program implementing cryptographic ciphers. In \ncontrast, our work can be seen as a specialized synthesizer applicable to the domain of concurrent mark-and-sweep \ncollectors. 1.2 Main Results The contributions of this work can be summarized as follows: We de.ne \na search procedure that explores a space of concur\u00adrent garbage collection algorithms built from a set \nof building blocks provided by an algorithm designer.  We provide a set of building blocks that captures \nthe intuition behind some common mark-and-sweep algorithms.  We explore the induced space of algorithms \nand discover over 100 non-trivial solutions.  We de.ne a model checking (with abstraction) procedure \nthat automatically veri.es the correctness of our algorithms.  While the search procedure is general, \nthe abstraction used for verifying the discovered algorithms may be speci.c to the choice of building \nblocks. In this paper, we present an abstraction that is appropriate for the subspace of mark-and-sweep \nalgorithms we explore. Due to space restrictions, we only give an informal description of the abstraction. \nA detailed formal description is givenin[23]. Outline. The rest of the paper is organized as follows: \nSection 2 reviews the framework of [24]; Sections 3-5 gradually present the core of our framework and \nthe new algorithms; Section 6 described the abstraction used for the model-checking algorithm; and Section \n7 concludes the paper.  2. The Log-based Parametric Collector In this section we brie.y review the \nparametric concurrent collec\u00adtion algorithm of [24], which serves as a starting point for our work. \n2.1 Log-based Concurrent Collectors The algorithm is de.ned using a standard concrete semantics. For \nsimplicity, we assume that all objects have the same set Fields of .eld identi.ers. We denote by VarId \nthe set of local variable identi.ers. We denote by locs an unbounded set of dynamically allocated heap \nlocations, and by Val = locs .{null}the possible values to which .elds and variables may be mapped. A \nprogram state keeps track of the program locations of the mutator and the collector, a set of allocated \nobjects (L . locs), an environment mapping local variables to values (.: VarId .Val), and a mapping from \n.elds of allocated objects to values (h: locs . Fields . Val). For convenience, we use obj .f to denote \nthe value h(obj )(f). All reachable objects are reachable from a .nite set of R .L root objects, denoted \nroot1,... ,rootR. No particular structure is assumed regarding the initial root set for collection. Thus \nit could consist of just a single pointer, the stack of one thread, or the stacks of multiple threads. \n(For simplicity, stack frames are treated as heap allocated objects.) The collection algorithm uses an \ninteraction log to record infor\u00admation about the combined behavior of the collector and the muta\u00adtor. \nThis log is used by the collection algorithm to select the objects to be marked. The interaction log \nis a sequence of log entries of the following kinds: (i) a tracing entry recording a tracing action of \nthe collector as it traverses the heap during the marking phase; (ii) a mutation entry recording a pointer \nredirection action by the mutator; (iii) an allocation entry recording an allocation of a new object \nby the mutator. This is formally de.ned as follows: DEFINITION 2.1. A log entry is a tuple (k, source, \nfld, old, new). {T, M, A}\u00d7L \u00d7Fields \u00d7Val \u00d7Val,where: k identi.es the kind of action as one of tracing, \nmutation, or allocation, denoted by T, M, and A, respectively.  source is the object affected by the \naction.  fld is the .eld of source affected by the action. old is the value of the .eld source.fld prior \nto the action.  new is the value of source.fld subsequent to the action.  Tracing actions do not change \nthe structure of the heap; therefore old = new for all tracing entries. Allocation actions allocate the \nobject new, which must not appear previously in the trace. The following selectors are used to access \nthe components of a given log entry tuple, t = (k,s,f,o,n): t.kind = k, t.source = s, t.fld = f, t.old \n= o,and t.new = n. collect() { atomic 2 marked .{root1,...,rootR} S 4 pending . fields(x) x.marked log \n.E do { mark() addOrigins() } while (?)  atomic \u00bb addOrigins() mark() sweep() } (SKL) Collector skeleton \n mark() {while (pending )= \u00d8{(obj, fld) . removeElement(pending) atomic \u00bb dst .obj.fld log .log (T,obj,fld,dst,dst) \nif (dst null .dst.marked){ = marked .. {dst}pending .. fields(dst) }}} (MRK) Collector tracing addOrigins() \n{ atomic [ origins .expose(log) \\marked marked .. origins x.origins S pending .. fields(x) } mutate(source, \nfld, new) {  atomic 2 old .source.fld 4 log .log (M, source, fld, old, new) source.fld .new } mutateAlloc(source, \nfld) { atomic 2 new .alloc new object 6 old .source.fld 4 log .log (A, source, fld, old, new) source.fld \n.new } (MUT) Mutator write and allocation barriers (ORG) Collector adding origins Figure 1. A log-based \nparametric Mark-and-Sweep algorithm.  2.2 The Parametric Algorithm Fig. 1 presents the pseudo-code for \na parametric concurrent mark\u00adand-sweep collector. The operation of this collector is de.ned over a pre.x \nof the interaction log, recording the collector and mutator interaction. (In the .gure, we use log t \nto denote a concatenation of t at the end of log.) The parametric collection algorithm does not specify \nhow ob\u00adjects are selected to be marked. Instead, the parameter function ex\u00adpose determines how the collector \nhandles objects that may be hid\u00adden due to concurrent mutations. The algorithm, however, does re\u00adstrict \nconcurrency by assuming that write barriers are atomic with respect to collector operations. Effectively, \nthis means that a col\u00adlector cannot preempt a mutator during a write barrier. The collection cycle of \nthe algorithm is described in the collect procedure. The collection cycle consists of two phases: (i) \nthe marking phase, in which the collector marks potentially live ob\u00adjects; (ii) the sweeping phase, in \nwhich unmarked objects are re\u00adclaimed. The collection cycle starts by atomically selecting the set of \nroot objects as origins. After selecting the root objects as origins, the collector proceeds by repeatedly \ntracing heap objects and marking them (mark procedure), and adding origins to be considered by the collector \ndue to concurrent mutations performed by the mutator (addOrigins procedure). These two steps are repeated \nuntil a non-deterministic choice (denoted by ? in the .gure) triggers a move to an atomic phase in which \nthe remaining origins and objects to be marked are processed atomically. This atomic phase guarantees \nthe termination of the algorithm, and is in line with some practical collector implementations (e.g., \n[2]). After the marking phase has completed, the sweep phase re\u00adclaims all objects that are not marked. \nNeither [24] nor we specify how the sweep operation proceeds, except to ensure that there is the proper \nsynchronization between the mark and sweep phases. Marking Traversal:The mark procedure implements a \ncollec\u00adtor traversal of the heap. (obj, fld)denotes the .eld fld of an ob\u00adject obj. fields(obj)denotes \nthe set of all object .elds for a given object obj. The procedure uses a set pending of pending .elds \nto be traversed, and performs a transitive traversal of the heap by it\u00aderatively removing an object .eld \nfrom pending and tracing from it. Whenever an object .eld is traced-from, the procedure inserts a tracing \nentry into the log. When the traced object .eld points to an unmarked object, the object is marked, and \nits .elds are added to the pending set. During this traversal, the mutator might concurrently modify \nthe heap. These concurrent mutations might cause reachable objects to be hidden from the traversal, and \nthus may remain unmarked by the current traversal. The Collector Wavefront: All collectors discussed \nin this pa\u00adper rely on cooperation between the collector and the mutator to guarantee correctness in \nthe presence of concurrency. A key part of the cooperation is tracking the progress of the collector \nthrough the heap, since mutations can be treated differently depending on whether they happened in the \nportion of the heap already scanned by the collector (behind the wavefront) or not yet scanned (ahead \nof the wavefront). The progress of the collector is tracked by track\u00ading the set of object .elds (that \nis, not the values of the pointers in those .elds) that have been traced by the collector thus far. DEFINITION \n2.2. Given a log pre.x P , the set of object-.elds that have been traced by collector operations in P \nis: W(P )= {(Pi.source, Pi.fld)|Pi.kind =T .0=i< |P |},where Pi denotes the ith log entry in P . We say \nthat an object .eld (o, f) is behind the wavefront when (o, f).W(P ), and ahead of the wavefront when \n(o, f).W (P ). Adding Origins:The addOrigins procedure uses the inter\u00adaction log to select a set of additional \nobjects to be considered as origins. When this procedure is invoked by the collector, it is possible \nthat a number of reachable pointers were hidden by the mutator behind the wavefront during the mark procedure. \nThe addOrigins procedure .nds a safe over-approximation of these hidden, but reachable objects. Thecoreof \naddOrigins is the atomic call to expose.The latter takes a log pre.x and returns a set of objects that \nshould be considered as additional origins. Each object returned by expose is then marked, and its .elds \nare inserted into the pending set. Mutator Barriers:Fig.1(MUT) shows the write-barrier and allocation-barrier \nused by the mutator. The procedure mutate is called by the mutator to update a pointer in the heap. The \nprocedure mutateAlloc is called by the mutator to allocate a new object and store it in the given .eld. \nTo collaborate with the collector, the mutator barriers append their actions to the interaction log. \nWhen the mutator performs an assignment source.fld .new with new = null, we say that a pointer is installed \nfrom (source, fld)to new. When the object .eld (source, fld)is be\u00adhind the wavefront, we say that the \npointer is installed behind the wavefront. Otherwise, we say that the pointer is installed ahead of the \nwavefront. Similarly, whenever we assign a value to a .eld (source, fld) containing an existing pointer, \nwe say that the exist\u00ading pointer is deleted.If the .eld (source, fld) is ahead (behind) of the wavefront, \nwe say that the pointer is deleted ahead (behind) of the wavefront. Counting-Based Collection: We now \ndescribe an approach for exposing hidden objects, which is based on counting the number of references \nto an object from behind the wavefront. The mutator count is the number of pointers to an object from \nobject .elds behind the wavefront. This quantity is computed with respect to a given wavefront. Note \nthat this is not a general form of reference counting. To compute the mutator count from a given log \npre.x P , we de.ne the mutator-count increment and decrement as follows (pre(P, i) denotes P s pre.x \nof length i): M+(o, P )= |{Pi | Pi.kind .{M, A}. Pi.new = o . (Pi.source, Pi.fld) .W(pre(P, i)) . 0 = \ni< |P |}| M-(o, P )= |{Pi | Pi.kind .{M, A}. Pi.old = o . (Pi.source, Pi.fld) .W(pre(P, i)) . 0 = i< \n|P |}| The value M+(o, P ) is the number of new references intro\u00adduced by the mutator from object .elds \nthat are behind the wave\u00adfront. Similarly, the value M-(o, P ) is the number of references removed by \nthe mutator from object .elds behind of the wave\u00adfront. The mutator count M(o, P ) is computed by combining \nthe mutator-count increments and decrements as follows: M(o, P )= M+(o, P ) - M-(o, P ) A counting-based \ncollector can be instantiated using the follow\u00ading expose c function. expose c(P )= {n | n = Pi.new . \nM(n, P ) > 0 . 0 = i< |P |} In real systems, the count maintained for an object is usually very small. \nTherefore, it would be wasteful to have a very large reference count per object. We therefore introduce \na threshold for the count. The threshold limits the mutator count to a maximum value, at which it sticks \nand is not subsequently decremented. This allows counts to be implemented with a .xed (small) number \nof bits while still maintaining the correctness properties provided by reference counting.  3. From \nLog-based to Log-free Algorithms In this section, we perform several manual steps that allow us to move \naway from the log-centric model of our algorithms and move towards a more practical model that allows \nsome of the collector\u00adrelated computation to take place in the write barrier. Speci.cally, we show how \nto get a collection algorithm that does not use a log by retaining the collector skeleton, shown in Fig. \n1(SKL), and .xing certain design decisions regarding the im\u00adplementation of mark, addOrigins and mutate. \nPractically, this means adding additional metadata (state) per object. Note that Fig. 1 is exactly the \nsame algorithm discussed in [24] and is re\u00adpeated here for convenience. The algorithm obtained in this \nsection is shown in Fig. 2. In this .gure, parts (MRK), (ORG) and (MUT) of Fig. 1 are modi.ed from being \nlog based to being state based, as will be shown below. The collector skeleton of Fig. 1(SKL) is identical \nfor both al\u00adgorithms and is not shown in Fig. 2. The code for (MRK), (ORG), and (MUT) shown in Fig. 1 \nis converted to the code shown in the corresponding parts of Fig. 2. For clarity of presentation we do \nnot show the necessary range checks for an over.ow on increment (operation new.MC ++). One can assume \nthe operations new.MC ++ and new.MC -- are no longer activated if over.ow of the count has occurred. \nSimilarly, we do not show that all operations in the barrier but the actual .eld assignment are predicated \non their target object being non-null and unmarked. The following manual steps are taken to perform the \nconversion. Update the Collector Wavefront On-the-.y: Rather than re\u00adcomputing the wavefront each time \nit is used by expose,the col\u00adlector keeps track of the wavefront and updates it incrementally in the \nmark procedure. Technically, the incremental wavefront can be implemented by storing a designated bit \non each .eld recording whether the .eld has been scanned by the collector. We denote the wavefront bit \nfor a .eld fld by WFfld. Introduce a Marked Bit: Instead of recording the marked objects in a collector \ndata-structure, we use a bit on every object to record whether the object has been marked by the collector. \nWe use the .eld mark to hold the mark bit of an object. Constrain Object Processing Order: In the log-based \nalgorithm, the .elds of an object are processed in an arbitrary order (i.e. pendingF ields). There are \ntwo sources of non-determinism here. First, it is possible to process .elds of different objects in arbitrary \norder and second, it is possible to process .elds of the same object in arbitrary order. This step eliminates \nboth sources of non-determinism by pro\u00adcessing all .elds of an object in a prede.ned order. This allows \nus to maintain a pending set of objects, rather than a pending set of .elds, and hence use less space. \n(In Fig. 2, we use the variable pending instead of pendingF ields to denote the set of pending objects.) \nTechnically, we assume that .elds within an object are identi.ed by a number and are processed in an \nincreasing order. Moving Computation from the Collector to the Mutator:In this paper, we will concentrate \nsolely on the counting-based algorithms from [24] which allocate black objects. To avoid re-computation \nof the mutator count, we introduce a counter MC for each object. This counter can be stored in the object \nheader. The MC counter is updated incrementally by the mutator s write-barrier. Because both wavefront \nand count information are stored in heap objects and updated incrementally, the log used in the log\u00adbased \nalgorithm of Fig. 1 is reduced to a set of objects for which the count should be re-inspected when the \ntracing is over (at the point of expose). We denote this set of objects by cand,as these objects are \ncandidates for being exposed. Next, the expose function is adapted as well. We denote the adaptation \nof the expose function that only reinspects the object counts by . expose. The adapted expose function \nis de.ned as fol\u00adlows: . expose(cand)= {o | o.MC > 0 . o . cand} So far, we have only de.ned . expose \nas a function over the candidate set cand. However, since we are interested in exploring versions of \n. expose that are not fully-atomic, we need to describe it operationally. We choose to represent the \ncandidate set cand usinganarray, and the . expose operation via a traversal of the array (atomically \nat this point, but we relax atomicity constraints in later sections). The variable nextcand represents \nthe current size of the candidate set. It is updated by the mutator when it inserts references into cand \nand is read by addOrigins when it processes the cand set. In our experiments we have simpli.ed the problem \nby assuming that the set cand only grows. In a more practical implementation the collector would need \nto update nextcand when it .nishes processing the set, e.g. by resetting it to zero. Separate Shared \nComputation from Local Computation:Ini\u00adtially mark and addOrigins consisted of updates to both heap and \nlocal information (e.g. pendingF ields is a variable local to the collector). We would like to separate \nthese two because we are interested in concentrating and varying only the part which is dif.\u00adcult to \ncreate and prove correct: the code accessing the shared data. Fig. 2 shows the routines performing collector-local \ncompu\u00adtation: mark,and processObject in the (MRK) part, and mark() { while (pending =\u00d8) { obj .removeElement(pending) \nprocessObject(obj) }} processObject(obj) { for (fld =0;fld < |Fields|;fld ++) {dst .markStep(obj,fld) \nif (dst =null) pending .. {dst}}} markStep(obj,fld) { atomic 2 dst .obj.fld 6 obj.WFfld .true 6 6 if \n(dst =null .\u00acdst.mark) 4 dst.mark .true else dst .null return dst } (MRK) Collector tracing addOrigins() \n{ atomic 2 for (i =0;i< |nextcand|;i ++){ 6 dst .addOriginStep(i) 6 6 if(dst =null) 4 pending .. {dst}}} \naddOriginStep(i) {dst .cand[i] dst.inLog .false if (dst.MC > 0.\u00acdst.mark) dst.mark .true else dst .null \n return dst } (ORG) Collector adding origins mutate(source, fld, new) {  atomic 2 if(source.W Ffld){old \n.source.fld 6 6 new.MC ++ 6 6 old.MC -- 6 6 if(\u00acnew.inLog){ 6 6 cand[nextcand ++]=new 6 6 new.inLog .true \n4 }} source.fld .new } mutateAlloc(source, fld) {new .alloc new object mutate(source, fld,new) } (MUT) \nMutator barriers Figure 2. Atomic log-free mark-and-sweep collector with metadata stored on heap objects \nand mutator-computation of the mutator count. addOrigins in the (ORG) part. These procedure do not directly \nread and write to heap objects, and use the procedures markStep and addOriginStep to access shared heap \ndata. Part (MUT) of the .gure shows the mutator routines: the write barrier procedure mutate that accesses \nshared data and the allo\u00adcation barrier mutateAlloc. In the .gure, we show the procedures that mutate \nshared heap data with a shaded background. These are the procedures that will be automatically synthesized \nby our framework. It is worth noting that addOrigins and mutate are access\u00ading a shared variable nextcand. \nWe will revisit this point in Sec\u00adtion 5.5. Although the procedure addOrigins is atomic, we chose to \nshow the splitting of addOrigins and addOriginStep here as in Section 5, we will be concentrating on \nremoving atomicity for both addOrigins and addOriginStep. To summarize, the resulting algorithm has the \nfollowing char\u00adacteristics: space overhead: the algorithm uses: (i) one bit per object-.eld in order \nto record the collector s progress through the heap (wavefront); (ii) one mark bit per object; (iii) \none inLog bit per object recording whether the object is in the candidate set; (iv) a counter per object \nwhose number of bits depends on the counting threshold.  concurrency: the algorithm uses an atomic write-barrier, \nand two atomic collector steps: markStep and addOrigins. In effect, the collector cannot perform a marking \nstep or addOrigins while the write barrier is executing, and the mutator cannot mutate the heap in the \nduration of a tracing step (tracing a single .eld) or during addOrigins.  precision: this algorithm \nhas the same precision as the log\u00adbased algorithm. All algorithms we derive in this work are of identical \nor lesser precision than the algorithm we obtained in this section.  In this section we obtained an \nalgorithm that uses large coarse\u00adgrained atomic routines: markStep, mutate and addOrigins. The key question \nthat we address in the rest of the paper is: what are the (correct) alternative implementations in terms \nof con\u00adcurrency to the three coarse-grained routines described above? Can we obtain other less-atomic \nimplementations of these three rou\u00adtines? Can we .nd implementations that use no atomics at all? This \nis a challenging question because the collector and the mutator con\u00adcurrently manipulate shared metadata. \nWe approach this question systematically, and, using our auto\u00admated exploration procedure, .nd a number \nof interesting correct collection algorithms.  4. The Exploration Framework In this section, we describe \nour semi-automated framework for ex\u00adploring a space of collection algorithms. In Section 5, we put the \nframework to work, and derive a variety of collection algorithms by systematically breaking the coarse-grained \natomicity of the al\u00adgorithm described in Section 3. Our framework is composed of a search algorithm and \na veri.\u00adcation procedure. The search algorithm attempts to verify the cor\u00adrectness of algorithms in the \nspace as they are being explored. In this section, we describe the search algorithm, and treat the ver\u00adi.cation \nprocedure as a black box (the details of the veri.cation procedure are described in Section 6). Our search \nprocedure looks for correct implementations of three procedures: markStep, addOriginStep and mutate. \n4.1 Atomicity We use the key word atomic to denote that a sequence of operations should not be interrupted. \nWe will be using the terms more atomic, most atomic and less atomic with respect to collection algorithms. \nWe de.ne these relations in terms of interleavings. We de.ne an interleaving of an algorithm in the standard \nway: a sequence of collector and mutator operations, where each operation is an execution of a building \nblock as de.ned in the next subsection. We denote the set of interleavings for an algorithm C1 by int(C1). \nWe note that only algorithms comprised of the same building blocks and in the same order (but, possibly, \nwith different atomicity constraints) are comparable by this partial order. Therefore, given two collector \nalgorithms C1 and C2, we say that C1 is more atomic than C2, when int(C1) . int(C2). Subsequently, given \na set of collection algorithms S, c . S is the most atomic if for any other c' . S, c' = c,and c is more \natomic than c'. The de.nitions for less atomic and least atomic are similar. Collector BB# Building Block \nMeaning C1 dst .o.fld scan .eld C2 if (dst .= null .\u00acdst.mark) mark an object which is not dst.mark .true \nalready marked else dst .null C3 addOrigins() process candidate objects C4 o.W Ffld .true notify .eld \nis read Mutator BB# Building Block Meaning M1 old .o.fld read .eld M2 o.fld .new write .eld M3 if(val) \no.MC .o.MC + 1 conditional inc of count M4 if(val) o.MC .o.MC -1 conditional dec of count M5 cand .cand \n.{o} records object as candidate M6 val .o.W Ffld check if .eld is read Table 1. Building blocks forming \nthe algorithm of Fig. 2. 4.2 Input The input of the exploration framework is the designer speci.cation \nwhich is comprised of a set of building blocks and, optionally, some constraints over them. Building \nBlocks: The building blocks can be viewed as the state\u00adments of a domain speci.c language for constructing \nconcurrent collectors. In our framework, the building blocks have the property that they can be implemented \nvia a small number of machine in\u00adstructions. Generally, any building block that references storage, where \nthe storage may be accessed concurrently (due mutator\u00adcollector concurrency) contains at most two memory \naccesses (al\u00adthough it may operate upon any number of local values). However, it is possible to have \nbuilding blocks which consist of higher num\u00adbers of memory accesses (generally, enclosed in atomic sections.) \nFor example, Table 1 shows a choice of building blocks that make up the coarse-grained atomic algorithm \nof Fig. 2. Initially, we start by treating the updating of the cand set as an atomic oper\u00adation (in mutate). \nWe also treat the collector s addOrigins as atomic. Therefore, these two steps are represented as single \natomic building blocks, respectively M5 and C3, in Table 1. In Section 5, we will reduce the atomicity \nof addOrigins further. We denote the set of all speci.ed building blocks by Blocks = CBlocks . MBlocks,where \nCBlocks and MBlocks denote the (disjoint sets of) building blocks performed by the collector and the \nmutator, respectively. Constraints: Our framework supports two forms of (optional) constraints: ordering \nconstraints and atomicity constraints. The collector ordering constraints require that the collector \ns blocks are executed in a certain order. They should form a partial order over the building blocks. \nThe same property holds for mutator ordering constraints. The atomicity constraints specify which blocks \nshould be ex\u00adecuted atomically. They should form an equivalence relation over the building blocks. Both \nforms of constraints do not allow building blocks from the collector and the mutator to occur in the \nsame constraint. That is, we cannot have an ordering constraint which speci.es that C2 oc\u00adcurs after \nM1, but we can have an ordering constraint that speci.es that C2 occurs after C1. Providing constraints \nserves several pur\u00adposes. First, without some constraints, exploration my be infeasible as there may \nbe too many points in the space to verify. Second, even if it is feasible, it is still possible to reduces \nthe size of the algorithm space and thus makes the exploration more ef.cient. Third, con\u00adstraints may \nbe a natural way for the designer to express insights that he is aware of. set explore(Blocks, Constraints) \n{correctSet = \u00d8; algSpace = genAll(Blocks, Constraints); while (algSpace .= \u00d8) { alg = pickandRemoveMostAtomic(algSpace); \nif (verify(alg)) { // propagate correctness correctSet = correctSet .moreAtomic(alg,algSpace); algSpace \n= algSpace \\moreAtomic(alg,algSpace); correctSet = correctSet .{alg}; } else {// propagate failure algSpace \n= algSpace \\{alg}; algSpace = algSpace \\lessAtomic(alg,algSpace); }} return correctSet; } Figure 3. Exploration \nProcedure. 4.3 Exploration Fig. 3 shows the algorithm we use for exploring the algorithm space. explore \nuses the designer s speci.cation to automatically and systematically explore the space of collection \nalgorithms that can be de.ned using the input blocks and constraints. Algorithm Generation:The explore \nprocedure starts by in\u00advoking genAll which initializes the algSpace set of algorithms for exploration \nby enumerating all combinations of building blocks and atomicity constraints that satisfy the input constraints. \ngenAll generates all the algorithms in the space using the fol\u00adlowing two operations: (i) sequential \ncomposition, i.e., placing an ordering constraint on building blocks within the collector (and respectively \nwithin the mutator); (ii) combining operations into atomic sequences (effectively melding building blocks \ntogether). For ef.ciency, genAll performs a lazy enumeration and not a sin\u00adgle eager enumeration step. \nThis is more ef.cient because it saves storage and improves the search. That is, as the search proceeds, \nit is possible to infer that certain algorithms are correct without need\u00ading to enumerate them. For example, \nan algorithm A is correct if a less atomic version B is also correct, and hence we can infer B s correctness \nwithout needing to explicitly enumerate it. Exploration:The explore procedure iteratively processes the \nset algSpace and tries to verify each algorithm by calling a model checker (procedure verify). When veri.cation \nsucceeds for an algorithm alg, it is added to the set of correct algorithms, and its correctness is used \nto imply the correctness of all algorithms that aremore atomicthan alg. This is done by adding the algorithms \nthat are more atomic than alg to the set of correct algorithms, and removing them from the algSpace set, \nas they are guaranteed to be correct and need not be considered further. If the veri.cation of alg fails, \nexplore removes all less atomic variants of alg from the algSpace set, as they are guaranteed to be incorrect. \nThe procedure pickandRemoveMostAtomic is a particular heuristic choice that we made in our exploration. \nThe exploration always selects the most constrained algorithm in terms of atomicity from the algSpace \nset, and tries to .nd correct algorithms that are less constrained. Effectively, we are expecting most \nvariations to be incorrect and expect to propagate the incorrectness to less atomic variations. It is \ncertainly possible to have a hybrid approach where we start from the least atomic algorithm and expect \nit to be correct so that we propagate that information to the more atomic variations. At any rate, such \ntechniques could be used to reduce the number of algorithms that need to be checked by the model checker \nand hence explore larger algorithm spaces. Additionally, there is an opportunity for running explore \non several processors. Note that the search algorithm is parametric in the set of build\u00ading blocks. That \nis, we can add additional building blocks and auto\u00admatically explore a new algorithm space. However, \nthe veri.cation procedure is not parametric and would need to be adapted. Optimizations: As mentioned \nearlier, we assume that the .elds of an object are processed in increasing order. Experimentally, the \nutilization of this assumption reduces the state-space and speeds up the checking of the algorithm. Moreover, \nthe propagation of cor\u00adrectness and failure reduce the number of calls to the model checker by more than \n90%, which is important because the verify pro\u00adcedure dominates the total exploration time. To limit \nthe search space, the exploration system checks for algorithms where every building block is used once. \nIf a designer wishes a block to be used more than once, she would have to specify that as a separate \nblock. For ef.ciency, we also equate algorithms that differ only in the order of (non data dependant) \nbuilding blocks that are executed atomically, that is atomic [ab] is equivalent to atomic [ba] if building \nblocks a and b are not data dependent. 4.4 Output The exploration procedure returns a set of correct \nconcurrent col\u00adlectors. Every algorithm is a pair of a write-barrier description and a collector-step \ndescription. The collector-step consists of the proce\u00addures markStep and addOriginStep. In cases where \nwe con\u00adsider addOrigins to be atomic, the collector step will be simply consist of the markStep. The \nalgorithms are encoded using a symbolic representation. A symbolic write-barrier description consists \nof: a sequence of mutator blocks and some atomicity constraints over them (i.e., an equivalence relation). \nA symbolic collector-step description is similar, but uses collector blocks. The order of blocks in the \nsequence re.ects the (total) order in which these blocks are executed. The atomicity constraints speci\u00ad.es \nwhich blocks should be executed atomically. These constraints re.ne the designer s speci.cation. The \nalgorithm also respects the data dependencies between building blocks and does not explore orders in \nwhich building blocks use uninitialized values. Given two blocks B1,B2 . Blocks and a sequence seq,we \nwrite B1 <B2 when B1 precedes B2 in seq. Note this forms a partial order. Given two blocks B1,B2 . Blocks \nand a set of atomicity constraints acs . Blocks \u00d7 Blocks, we write [B1,B2] when (B1,B2) . acs, denoting \nthe fact that acs requires that B1 and B2 are executed atomically. Note that this is an equivalence relation. \n  5. Discovering Algorithms In this section we describe how we used the automated system to synthesize \na number of interesting .ne-grained synchronization al\u00adgorithms. We show several possible scenarios that \nwe experimented with and the results that were obtained from each. This section demonstrates the process \nthat a typical user of the system would follow in order to synthesize algorithms. In this ex\u00adample the \nsystem is used in a staged manner, utilizing the feedback of the system over a particular set of building \nblocks in order to reduce the search space for a more re.ned set of building blocks. In that sense, we \nderived variations of algorithms incrementally. As we progress through this section, we describe how \nwe used the sys\u00adtem to explore various sets of building blocks. We summarize the results presented in \nthis section in Table 2. The last four columns of the table show the number of algorithms explored at \neach step (To\u00adtal), the number of algorithms model-checked by the checking pro\u00adcedure (SPIN), the number \nof correct algorithms found (Correct), Table 2. Results of Exploration. Experiments performed on a ma\u00adchine \nwith a 3.8 Ghz Xeon processor and 8 Gb memory running version 4 of the RedHat Linux operating system. \nSec. Run Total SPIN Correct Time (min.) 5.1 (a) 306 45 1 2 5.3 (b) 2744 162 2 34 5.4 (c) 12 7 2 1 (d) \n592 146 14 56 (e) 32 26 1 1 5.5 (f) 3024 550 80 212 (g) - - - T/O (h) 6144 127 10 39 (i) 1,624,320 1833 \n6 2072 (j) 364,032 288 0 39 mutate4() markStep4() old .o.fld detectedAtomic 2 atomicdetectedAtomic 2 \n\u00bb 6 dst .o.fld o.fld .new 6 66 . if (dst = null . w.o.WF fld 66 66 \u00acdst.mark) atomic 64 \u00bb6 dst.mark .true \nif(w) new.MC++ 4 else dst .null if(w) cand .cand .{new} o.WF fld.true if(w) old.MC return dst Figure \n4. The least atomic algorithm that is automatically detected by our system from the building blocks of \nTable 1 (depicted as run (a) in Table 2). and the exploration time (in minutes). The difference between \nthe columns Total and SPIN is that the correctness or failure of many algorithms (from Total) can be \ndeduced without invoking the SPIN checker (as discussed in the previous section). It should be noted \nthat the Correct column indicates the number of least atomic cor\u00adrect algorithms. More atomic algorithms \nare trivially correct and their number is not shown in this column. As noted previously, the running \ntimes are affected by our particular choice of exploration. Different choices such as starting from the \nleast atomic algorithm or a hybrid approach will yield different running times. In our experi\u00adments we \nchecked the safety property of the algorithms, i.e., that all reachable objects are marked and processed \nat the end of marking. Further details of the veri.cation are available in Section 6. As users of the \nsystem, we start the process with some intuition as to what are reasonable building blocks from which \nan algorithm could be constructed, but without knowing the full details of how to put these blocks together. \nThis process is similar to the idea of sketching. Different users of the system may come up with different \nsuch insights/building blocks. As mentioned in Section 3, the algorithm skeleton has already been split \ninto local and shared parts manually as shown in Fig. 2. In what follows, we will describe how we used \nto system to .ll the shared routines (shaded in Fig. 2) with correct code. 5.1 Exploration: Starting \nPoint The starting point of our search is the coarse-grained algorithm of Fig. 2. There are two main \ncauses of mutator-collector interference in this algorithm. The .rst is the interaction between markStep \nand mutate. The second is the interaction between addOrigins and mutate. As a .rst step, we decide to \ntry and resolve the interfer\u00adence between markStep and mutate,and leave addOrigins atomic. That is, addOrigins \nis a single building block. Note that although no operation in mutate can preempt an atomic addOrigins,making \nmutate non-atomic will allow for it to be preempted by addOrigins. The building blocks comprising the \nshaded parts of the algo\u00adrithm of Fig. 2 were already shown in Table 1. Our .rst natural choice is to \nrun the system with this set. However, before break\u00ading the atomic operations into elementary units (loads/stores), \nwe .rst try and maintain more coarse, higher-level , operations. This is done by providing the system \nwith user-de.ned atomicity con\u00adstraints that are guaranteed to be maintained during exploration. For \nthe collector, we make building blocks C1and C2of Table 1 a sin\u00adgle atomic operation, expressed as readMarkT \narget =[C1C2]. That is, the reading of the .eld and the marking of the object who is pointed to by this \n.eld are executed atomically. For the mutator, we make the increment of the mutator count and the logging \nof the object execute atomically, expressed as incAndLog =[M3M5]. Given these constraints over the building \nblocks, we ran the system and the least atomic algorithm found is shown in Fig. 4. In this paper, in \nall .gures presenting algorithms, we use atomic to denote an initial atomicity constraint provided by \nuser, and detectedAtomic to denote atomicity constraints detected as required by the system. The details \nof this execution can be seen as run (a) of Table 2. The detectedAtomic constraints in the resulting \nalgorithm effec\u00adtively suggests that the mutator should store the target object and read the wavefront \natomically. Similarly, the collector should read the .eld and set its progress atomically. Initially, \nwe were surprised that the setting of the .eld and the reading of the wavefront in the mutator had to \nbe atomic, even more so because the system had found the processing of the .eld was done atomically in \nthe collector, so the mutator could not pre\u00adempt the collector during the processing of a .eld. To .gure \nout why detectedAtomic in the mutator was necessary, we examined a counterexample generated by SPIN for \nan algorithm without this detectedAtomic. It indicated a subtle interleaving where an incor\u00adrect decrement \nof a mutator count was taking place. The system was helpful in quickly discarding such candidates and \n.nding the correct alternatives. Our automatic exploration procedure is exhaustive. Thus, it is guaranteed \nthat the algorithm of Fig. 4 is the least atomic algorithm that the system could .nd under the provided \nset of building blocks and user-provided atomicity constraints. However, the user may still be unsatis.ed \n(as she should be) with the current result. She may want to derive an algorithm with fewer atomicity \nconstraints. 5.2 Manual Step: Adding New Building Blocks At this point, new insights and new building \nblocks are needed in order to continue the derivation and a human is required to generate an insight. \nBut how can we generate an insight which is as generic as possible, that is, not garbage collection dependent? \nWe asked a natural question of what would happen if more state is injected into the system? Can it be \nused to reduce atomicity? And if so, how should the state be introduced? To answer these questions, we \ndistinguish between two types of building blocks: Core blocks -describe the core operations performed \nby the collector and the mutator, such as the collector reading and markinganobject (readMarkT arget) \nor the mutator storing a pointer in the heap (M2). One can think of these as operations which cannot \nbe avoided and exist even in sequential collectors where the mutator cannot preempt the collector during \nmarking. Collector BB# Building Block Meaning C4S o.Sfld . true notify .eld scan starts C4E o.Efld . \ntrue notify .eld scan ends Mutator BB# Building Block Meaning M6S val . o.Sfld check if .eld scan started \nM6E val . o.Efld check if .eld scan ended Table 3. Re.ned building blocks, replacing the blocks C4and \nM6 of Table 1, and recording both the beginning and the ending of the wavefront progress over a .eld. \nAuxiliary blocks -describe the operations on collector metadata such as the mutator count .eld in each \nobject, or the progress of the collector through the heap via modifying the wavefront. The next step \ncan be thought of as adding new metadata, that is, new auxiliary building blocks. The question of where \nto add this metadata is a central one. Our insight was to add state which captures the history of the \nexecution of the core operations, that is, the starting and the ending of each core operation. One can \nthink of the current per-.eld wavefront bit as a way of notifying the mutator that the .eld has been \nread by the collector. However, the role of this bit relies on the atomicity of the collector sequence. \nWhen the sequence is atomic, the wavefront variable is observed by the mutator if and only if the .eld \nhas been scanned by the collector. If the collector sequence becomes non-atomic, the wavefront variable \ncan no longer convey the same semantics as it is updated separately from the scanning of the .eld. Therefore, \nwe add state to capture the history of the core oper\u00adations. For each .eld in an object we introduce \na new bit per .eld Sfld, which noti.es the mutator when the collector starts process\u00ading the .eld fld. \nThe original wavefront bit retains its semantics that when it is set, the .eld has been read by the collector. \nTo re.ect the fact that we now have two .elds, we rename the original wave\u00adfront bit from Wfld to Efld. \nThese two .elds make the progress of the collector more precisely observable to the mutator. The new \nstate also implies that we need new auxiliary building blocks for reading and writing this state (the \ncollector writes and the mutator reads this state). The additional blocks are shown in Table 3. In the \nfuture, we would like to see the system automatically apply various transformation rules such as the \none described here (adding state in a speci.c place). However, at this stage, the ad\u00addition of auxiliary \nstate is done manually. Next, we proceed by showing how to use these building blocks in order to further \nreduce atomicity. 5.3 Exploration: Another Try Before proceeding, we add an ordering constraint over \nthe collec\u00adtor s blocks which re.ects our insight about notifying the start and the end of a core operation: \nCO1: o.Sfld . true < ReadMarkTarget< o.Efld . true In addition, we also added the data-dependent constraints \nthat decrementing mutator count is dependent on the value of Efld (i.e. mutator building block M4is enabled \nonly if Efld is true). We ran the system again using the additional building blocks and the CO1constraint. \nThe details of this invocation can be seen in Table 2 as run (b). Exploration of the algorithm space \nyielded two least atomic correct algorithms (i.e. where no detectedAtomic was required). Fig. 5 shows \none of these algorithms. The other algorithm is similar and is obtained by swapping the .rst two lines \nof mutate. mutate5() markStep5() old .obj.fld o.Sfld .true e .obj.Efld atomic 2 obj.fld .new dst .o.fld \ns .obj.Sfld 6 if (dst .= null .\u00acdst.mark) 4 atomic dst.mark .true \u00bb if(s) new.MC++ else dst .null if(s) \ncand .cand .{new} o.Efld .true if(e) old.MC return dst Figure 5. The least atomic algorithm that can \nbe derived from the building blocks of Table 1 re.ned by the blocks of Table 3. The following three mutator \nconstraints describe the mutate procedure of the two least atomic algorithms just discovered: DMC1: end \n. o.Efld <o.fld . new DMC2: o.fld . new < start . o.Sfld DMC3: incAndLog < if(e)old.MC The .rst two \nconstraints state that the mutator processes an object .eld in the opposite order from the collector. \nThe third constraint is between mutator operations. It is an interesting one because it says that although \naddOrigins is atomic, the sys\u00adtem requires the logging of an object to occur before a poten\u00adtial mutator \ncount decrement of another or same object. This is due to the fact that although addOrigins cannot be \npreempted by mutate, it can still preempt mutate. In fact, we also per\u00adformed another experiment where \naddOrigins could not pre\u00adempt mutate and mutate could not preempt addOrigins by using a global synchronization \nbarrier in the skeleton before each addOrigins starts. With this more global constraint we actually derived \n(not surprisingly) more correct algorithms. That is, we were able to discover the .rst two constraints \nDMC1and DMC2,but not DMC3. The experiment suggested that what look like small changes in the skeleton \ncan affect algorithm correctness in sub\u00adtle ways. Reasoning about these changes manually in the presence \nof concurrency is very dif.cult for a human. Moreover, .nding all possible correct solutions after a \nsmall change is made and express\u00ading all of the resulting solutions concisely as a set of constraints \nis nearly impossible to correctly reason about manually. 5.4 Abstracting State At this stage, our exploration \nhas produced a non-atomic mutate and markStep in the building blocks we have provided, but with an atomic \naddOrigins. Before we continue searching for less atomic algorithms, a question to ask is: can we achieve \nthe same non-atomicity with less auxiliary state and if so, is there some property of the algorithms \nthat is affected by using less state? We decide to try and answer this question by experimenting with \nour system. Naturally, we concentrate on auxiliary state and abstracting the collector progress. Abstracting \ncollector progress simply means the mutator does not observe such progress precisely. We explore three \ncorner cases of this abstraction, although more variations could be introduced. The results of these \nthree experi\u00adments are shown in Table 2: 1. Removal of Sfld state from every .eld is shown as run (d). \n 2. Removal of Efld state from every .eld is shown as run (e). 3. Removal of Sfld and Efld from every \n.eld is shown as run (c).  Removal of state can be thought of as the mutator building blocks M6S and \nM4E simply returning true or false.Thatis, they are unable to observe the collector progress precisely \nand are forced to return a safe approximation, which for M6S is true and for M4E is false. It may seem \nthat this process of abstracting state contradicts the previous step of introducing additional state \nby suggesting to now remove the same state from every .eld. However, there is a fundamental difference. \nEarlier, both the increment of the mutator count and the decrement depended on the same bit Wfld.By splitting \nthis bit into two bits Sfld and Efld, the increment and decrement no longer depend on the same state. \nTherefore, even when we remove the Sfld bit from every .eld, the result is not the same as having only \nthe Wfld, because the resulting mutator incrementing operations fundamentally depend on different states. \nThe removal of only the Sfld bit, as suggested by the .rst case, led to the generation of 14 correct \nand least atomic variations. That is, the system did not detect that more atomics were necessary than \nthose already provided in the building blocks. All 14 variations can be expectedly described with the \ntwo constraints presented earlier: DMC1 and DMC3. The constraint DMC2 is not necessary anymore because \nM6S always returns true in this case. When we removed the Efld bit only, as suggested in the sec\u00adond \ncase, the system generated only one correct least atomic varia\u00adtion. This result is represented by the \nconstraint DMC2presented earlier, which is expected because M4E always returns false and hence the constraints \nDMC1and DMC3are unnecessary. Note that unlike in Fig. 4 where we needed detectedAtomic,in the two examples \nso far, we found algorithms which do not require such a constraint. This is interesting because they \nuse the same state size, namely a bit per .eld. As mentioned, the key difference is the meaning of this \nstate and what actions are triggered by the mutator upon reading it. In our third experiment, in which \nboth bits were removed, the system found two correct least atomic variations. The system did not derive \nany required constraints. That is, the algorithms are strictly described by their input data-.ow constraints. \nEffectively, the algorithms always increment the mutator count, and never decrement it. Certainly, even \nwithout running the system, we could have de\u00adduced the derived constraints from these three runs by simply \nelim\u00adinating the corresponding constraint when the building block is known to return a constant value \n(e.g. true or false). However, the particular searching order depends on the expertise of the user and \nhence it is possible the user simply did not reach the original three constraints (DMC1,DMC2and DMC3) \nin their design, but managed to think of a variation which had the Sfld bit only. It is interesting to \nmention that the third variation with the least state (i.e. both bits removed) is a symbolic algorithm \nwhich is very similar to the original Dijkstra algorithm. However, it is well-known that the order of \nstatements (that is, the store on the new target and the coloring of the target) in the original Dijkstra \nalgorithm is crucial to correctness. The reason why the order is critical there is due to the fact that \nwhile in the write barrier (e.g. mutate), it is possible for one collection cycle to .nish and another \ncollection cycle to start. Such an interleaving is not possible in our system and in most practical systems \nwhich require a stopping phase for each thread in order to mark their root set.  5.5 Final Step: Reducing \nAtomicity In Candidate Processing We resume our search for a non-atomic collector. Our starting point \nare the building blocks described in Table 3 and the results ob\u00adtained in Section 5.3. Recall that the \nmutator logging of objects and collector s addOrigins are still atomic operations (build\u00ading blocks M5and \nC3). Our next task is splitting these blocks into separate smaller building blocks. In particular, we \n.rst manu\u00adally make addOrigins non-atomic by removing the atomic sec\u00adtion around it and placing it inside \naddOriginStep,so now the whole of addOriginStep is atomic, but addOrigins is not. The splitting amounts \nto simply removing an atomicity constraint.  Collector BB# C3A C3B Building Block o . cand[i] o.inLog \n. false Meaning C3C if (\u00aco.mark . o.o.mark . true MC > 0) Mutator mark object BB# M5A M5B M5C Building \nBlock if (v1) cand[nextif (v1) o.inLog . if (v1) v2 . o.in cand++] . o true Log Meaning read object \nfrom cand set reset inLog bit add object to cand set inLog bit read inLog bit Table 4. Re.ned building \nblocks, replacing the blocks C3 and M5 of Table 1, explicitly handling the candidate set cand.  Collector \nBB# C5 Building Block gc state . phase Meaning write gc state. phase Mutator is either trace or expose \nBB# Building Block Meaning M7 M8 if (v1) v2 . gc state if (v1) o.MC . (val = trace)? o.MC +1 : max read \ngc state increment or over.ow the mutator count.  Table 5. Additional building blocks to Table 4. Therefore, \nthe new collector building blocks are basically those of addOriginStep. These re.ned building blocks \nare shown in Ta\u00adble 4. Similarly, the building block M5 is split into several smaller pieces manually. \nIn fact, these pieces of M5 were always present, but were not shown earlier as logging was executed atomically. \nIn addition, we also remove the user provided atomicity constraint of incAndLog,sonow M3 (mutator increments) \nand mutator log\u00adging are no longer constrained to execute atomically. Therefore, the only atomicity constraints \nleft in the mutator are those forming the individual building blocks. For example, in block M5A, we log \nand then increment nextcand atomically. In the collector, as before, readMarkT arget forms a single coarse-level \nbuilding block. We ran the system with these new building blocks and fewer constraints. This run is depicted \nas run (j) in Table 2. Unfortunately the exploration could not .nd a correct algorithm. Despite the fact \nthat exploration was not successful, there is still a key point to be made. The main reason we were able \nto explore over 350, 000 variations in less than 40 minutes is because we used two of the ordering constraints \nthat we discovered in Section 5.3: DMC1 and DMC2. In this section our .nal goal is to have no detectedAtomic \nbetween building blocks and therefore we knew that constraints DMC1 and DMC2 must hold if the result \nis to be non-atomic (as we already know they are required for dealing with the interference between the \nmutate and markStep). In this sense, we are using the system in a feedback-directed manner, inserting \na constraint from a previous stage in order to direct the search more accurately. In our experience, \nsuch informed guidance is vital in obtaining realistic running times. At this point, the system has deduced \nthat even with an atomic addOriginStep, mutate and markStep, it is still not possi\u00adble to produce even \na single correct algorithm. Again, an insight is clearly needed. We manually studied why an algorithm \nfails and .gured out the problem to be due to mutator counts going up and down while interleaving with \nthe execution of addOrigins. It is possible to construct an example where addOrigins never terminates, \nbecause an object is continuously being re-logged during its execution. To solve this problem, we look \nfor a way to guarantee progress of addOrigins and avoid a potentially in.nite log while still guaranteeing \nsafety. We decide to follow the same pattern we took in Section 5.2 and add some auxiliary control state \nto the collector. Therefore, all we add is a .ag (a variable) which signals whether the collector is \nin the marking phase or whether it is in the addOrigins phase. We refer to this variable as gc state \nand the corresponding build\u00ading blocks to read and write this variable are shown in Table 5. For the \nmutator, building block M3 which performs increments is subsumed by block M8.In M8, the mutator checks \nwhether the gc state has been set to expose and if so, the mutator count over\u00ad.ows and hence can no longer \nbe decremented. To fundamentally understand how gc state came into existence, we note that ide\u00adally, \nwe would have added start and end bits to each log .eld and explored the possible results. The gc state \nvariable is in fact an abstraction of the collector progress information through the log (similar to \nthe abstraction of the collector progress through the heap discussed in Section 5.4). We ran the system \nagain with the additional building blocks. Run (i) in Table 2 shows the details of this run. This run \nwas split into four parts and we report the maximum time it took for a single part to complete. Note \nthat the time it took for this run to complete is much longer than the other runs because the model exploration \nrequires memory which is close to the physical memory of the machine. It took 7.8GB for the checking \nof the largest model and the available physical memory was 8GB. The exploration found six correct algorithms. \nFor two of these algorithms, the system did not require any usage of detectedAtomic at all, both for \nthe mutator part and for the collector part. One of these two non\u00adatomic algorithms (instantiated inside \nthe skeleton) is shown in Fig. 6. The second algorithm can be obtained by swapping the .rst two building \nblocks of mutate. Note that addOrigins has no enclosing atomic section as well. The state variable gc \nstate is set to tracing whenever the collector begins the tracing phase, and is set to expose before \naddOrigins begins. It is set back to tracing when addOrigins .nishes. Our system dictated that all six \ncorrect algorithms must comply with the following derived mutator ordering constraints: DMC4: M8 <M5C \nDMC5: M5A <M4 In fact these two constraints are simply re.nements of constraint DMC3 presented earlier. \nThe .rst constraint indicates that an in\u00adcrement/over.ow of a mutator count should occur before checking \nwhether the object is in the candidate set. The second one states that the store of the object in the \ncandidate set should occur before the decrement of its count. The other four algorithms look like the \none in Fig. 6, except that M5B is moved either after M5A or after M4. The system states that if such \nmovement of M5B occurs, thenanatomicsectionis required around C3B and C3C (i.e. detectedAtomic). As in \nSection 5.4, we also experimented with the system by abstracting the progress of the collector. That \nis, by removing either Sfld, Efld bits or both from every .eld. Run (h) in Table 2 re.ects the removal \nof Efld and run (f) re.ects the removal of both. Run (h) indicated that no detectedAtomic was necessary \nin both the mutator or the collector, so the only constraints required are the initial data-dependency \nconstraints. For run (f), the system detected one derived constraint M8 <M5A, indicating that the increment \nof the mutator count had to occur before the logging of the object. Case (g) (removal of Sfld only) timed \nout.  5.6 Future Steps Although the algorithm obtained in Fig. 6 did not require de\u00adtectedAtomic constraints, \nit still has atomic inside the code (due    (MRK) Collector tracing addOrigins() {gc state .expose \nfor (i =0;i< |nextcand|;i ++) { dst .addOriginStep(i) if (dst =null) pending .. {dst} } gc state .trace \n } addOriginStep(i) { dst .cand[i] dst.inLog .false atomic 2 if (dst.MC > 0.\u00acdst.mark) 4 dst.mark \n.true else dst .null return dst }  (ORG) Collector adding origins (MUT) Mutator barriers Figure 6. \nThe most .ne-grained algorithm derived in our framework. to the designer requesting them). However, note \nthat there is no building block which is executed atomically that is accessing two or more variables \nwhere both variables can be read and written by both the collector and the mutator. The next step in \nthe exploration would be to remove these .nal atomicity constraints and check the algorithm. However, \nthe algorithm in Fig. 6 already requires 7.8GB and our machine had 8GB, so it would be impossible to \ncheck the resulting less atomic algorithm. It would be interesting to continue the exploration either \nafter applying various engineering optimiza\u00adtions to the model to make it smaller or simply after more \nmemory is obtained. Moreover, although our proofs and derivations are for algo\u00adrithms allocating black, \nthis restriction is only necessary because we are more general than practical algorithms which perform \na stop-the-world phase for stack rescanning. Assuming such a syn\u00adchronization phase undoubtedly will \nlead to more interesting vari\u00adations as well as being able to handle white objects. Further, we would \nbe interested in seeing this work extended for multiple mu\u00adtators or write barriers other than counting \nas presented in [24]. This would certainly require more sophisticated abstraction as well more memory \nto verify the results.  6. Verifying Derived Algorithms We verify the derived algorithms using model \nchecking with ab\u00adstraction. Automatically verifying arbitrary concurrent garbage col\u00adlection algorithms, \nwith all of their details, is a challenging task. The alert reader may therefore .nd the above claim \nsurprising. However, the reason that our veri.cation attempt is successful, is that we operate within \nthe boundaries of our limited framework. In particular, we are making the following assumptions: The \nalgorithm handles a single collector and a single mutator.  The implemented algorithm is an attempted \nimplementation of a counting algorithm, where the counting threshold is known.  The algorithm skeleton \nis .xed, and the operations performed by the skeleton are known to be correct. For example, we as\u00adsume \nthat basic stop-the-world tracing is implemented correctly (i.e., the trace procedure marks all the objects \nthat are reachable from the pending set when it executes without interruptions).  The algorithm uses \na synchronization barrier before moving to the sweep phase, so mutations are required to terminate before \nthe collector ends the mark phase.  Therefore our procedure veri.es that using the derived write\u00adbarrier \nand collector-step inside a given (correct) skeleton yields a correct algorithm. In this section, we \ndescribe key aspects of the veri.cation process. 6.1 Veri.cation Problem: Safety of the Derived Algorithms \nThe correctness of the derived algorithms is speci.ed by the fol\u00adlowing safety invariant: DEFINITION \n6.1 (Safety Invariant). When the last execution of addOrigins terminates, that is, before sweep starts, \nall reach\u00adable objects are marked. Using the safety invariant as the veri.cation goal requires reason\u00ading \nabout reachability properties in arbitrary heaps undergoing arbi\u00adtrary mutations while taking into account \nthe interference between the mutator s write barrier and the collector s tracing of the heap and processing \nof the log. Reasoning about reachability properties in such a setting is very challenging. However, we \navoid the need to about reason reachability properties using the following obser\u00advation: For the safety \ninvariant to be violated, there must exist an object o that is reachable but not marked. If such an object \no exists, there exists an object o ' such that o ' is directly pointed to by a black object, but o ' \nis not marked. This observation allows to establish that the safety invariant holds by verifying that \nthe following local safety invariant holds DEFINITION 6.2 (Local Safety Invariant). When the last execu\u00adtion \nof addOrigins terminates, that is, before sweep starts, every object which is pointed to by a black object, \nis marked. We verify that the local safety property holds by non-deterministically selecting a tracked \nobject and checking whether any interaction of mutator and collector can cause this object to violate \nthe local safety invariant. This selection is similar to the choice of a single tracked object in e.g., \n[7, 25, 13]. 6.2 Abstraction for Model Checking We verify that the local safety invariant holds using \nmodel checking with abstraction. Our abstraction represents an unbounded number of heap locations by \na bounded abstract representation. We parti\u00adtion the heap into equivalence classes based on properties \nof heap locations. Intuitively, our abstraction partitions the heap in a way that distinguishes several \nclasses of locations, including: locations that have already been read by the collector (scanned locations). \n locations marked by the collector (marked locations)  header locations with different mutator counts. \n Our abstraction is sound, so when we successfully verify an algorithm, it is indeed guaranteed to be \ncorrect. However, when we are unable to establish the correctness of an algorithm under our abstraction, \nit is possible that the algorithm is still correct, but our abstraction is insuf.cient for showing its \ncorrectness (hence we may have missed some correct algorithms). To maintain suf.ciently precise abstraction \nwe identify .elds that were read by the collector (scanned) and then updated by the mutator to point \nto the tracked object. We re.ne the abstraction of the scanned location by keeping these .elds distinct, \nwhile the mutator count (MC) of the tracked object has not over.owed. Tracking these .elds allows us \nto precisely handle decrements of the mutator count. A key observation that we are using in order to \nbound the number of these locations, is that there are only k relevant pointers that can be installed \npointing to the tracked object before its count over.ows. Because we assume that k is known, we are able \nto precisely track only a bounded number of speci.c locations, while aggressively abstracting the rest \nof the heap. Technically, we use the framework of [20] to describe the heap and properties of heap locations \nusing .rst-order logic. We then hand-code the abstraction into SPIN to achieve a more ef.cient implementation. \n 7. Conclusion We present a framework for synthesizing provably correct count\u00ading based concurrent mark \nand sweep collection algorithms. Our framework utilizes a user-provided set of building blocks to au\u00adtomatically \nexplore a space of algorithms, using model checking with abstraction to verify algorithms in the space. \nUsing our frame\u00adwork, we were able to discover several interesting .ne-grained al\u00adgorithms. In future \nwork, we plan to investigate the possibility of applying our techniques in other domains. We believe \nthat the key reason for the success of our framework is that it found a good balance between the tasks \nperformed by a human and the tasks per\u00adformed by the machine.  References [1] BAR-DAVID,Y., AND TAUBENFELD, \nG. Automatic discovery of mutual exclusion algorithms. In Proceedings of the 22nd Annual Symposium on \nPrinciples of Distributed Computing (2003). [2] BARABASH,K., OSSIA,Y., AND PETRANK, E. Mostly concurrent \ngarbage collection revisited. In Proceedings of the 18th ACM conference on Object-oriented programing, \nsystems, languages, and applications (2003). [3] BEN-ARI, M. Algorithms for on-the-.y garbage collection. \nACM Trans. Program. Lang. Syst. 6, 3 (1984). [4] BIRKEDAL,L., TORP-SMITH,N., AND REYNOLDS, J. C. Local \nreasoning about a copying garbage collector. In Proceedings of the 31st ACM Symposium on Principles of \nProgramming Languages (2004). [5] BOWMAN,H., DERRICK,J., AND JONES, R. E. Modelling garbage collection \nalgorithms. In Proceedings of International Computing Symposium (1994). [6] BURDY, L. B vs. Coq to prove \na garbage collector. In the 14th International Conference on Theorem Proving in Higher Order Logics: \nSupplemental Proceedings (2001). [7] DAS,M., LERNER,S., AND SEIGLE, M. ESP: Path-sensitive program veri.cation \nin polynomial time. In Proceedings of the ACM SIGPLAN 2002 Conference on Programming Language Design \nand Implementation (2002). [8] DEMMERS,A., WEISER,M., HAYES ,B., BOEHM,H., BOBROW, D., AND SHENKER, S. \nCombining generational and conservative garbage collection: framework and implementations. In Proceedings \nof the 17th ACM symposium on Principles of programming languages (1990). [9] DEWAR, R.B.K., SHIRAR,M., \nAND WEIXELBAUM, E. Transfor\u00admational derivation of a garbage collection algorithm. ACM Trans. Program. \nLang. Syst. 4, 4 (1982). [10] DIJKSTRA,E. W., LAMPORT,L., MARTIN,A. J., SCHOLTEN, C. S., AND STEFFENS, \nE. F. M. On-the-.y garbage collection: an exercise in cooperation. Commun. ACM 21, 11 (1978). [11] GRIES, \nD. An exercise in proving parallel programs correct. Commun. ACM 20, 12 (1977). [12] GRIES, D. Corrigendum. \nCommun. ACM 21, 12 (December 1978), 1048. [13] HACKETT,B., AND RUGINA, R. Region-based shape analysis \nwith tracked locations. In Proceedings of the 32nd ACM Symposium on Principles of Programming Languages \n(2005), ACM. [14] HAVELUND, K. Mechanical veri.cation of a garbage collector. In Fourth International \nWorkshop on Formal Methods for Parallel Programming: Theory and Applications (1999). [15] JACKSON, P. \nB. Verifying a garbage collection algorithm. In Theorem Proving in Higher Order Logics, 11th International \nConference (1998). [16] MASSALIN, H. Superoptimizer: a look at the smallest program. In the 2nd International \nConference on Architectural Support for Programming Languages and Operating Systems (1987). [17] PAULSON,L. \nIsabelle: A Generic Theorem Prover, vol. 828 of Lecture Notes in Computer Science. 1994. [18] PRENSA \nNIETO,L., AND ESPARZA, J. Verifying single and multi\u00admutator garbage collectors with Owicki/Gries in \nIsabelle/HOL. In Mathematical Foundations of Computer Science (2000). [19] RUSSINOFF, D. M. A mechanically \nveri.ed incremental garbage collector. Formal Aspects of Computing 6, 4 (1994). [20] SAGIV,M., REPS,T., \nAND WILHELM, R. Parametric shape analysis via 3-valued logic. ACM Trans. on Prog. Lang. and Systems 24,3 \n(2002). [21] SOLAR-LEZAMA,A., RABBAH,R. M., BOD\u00b4IK,R., AND EBCIOGLU, K. Programming by sketching for \nbit-streaming pro\u00adgrams. In Proceedings of the ACM Conference on Programming Language Design and Implementation \n(2005). [22] TAKAHASHI,K. Abstraction and Search in Veri.cation by State Exploration. PhD thesis, University \nof Tokyo, Jan. 2002. [23] VECHEV,M. Derivation And Evaluation Of Concurrent Collectors. PhD thesis, University \nof Cambridge, 2007. [24] VECHEV,M.T., YAHAV,E., AND BACON, D. F. Correctness\u00adpreserving derivation of \nconcurrent garbage collection algorithms. In Proceedings of the ACM Conference on Programming Language \nDesign and Implementation (2006). [25] YAHAV,E., AND RAMALINGAM, G. Verifying safety properties using \nseparation and heterogeneous abstractions. In Proceedings of the ACM conference on Programming language \ndesign and implementation (2004). \n\t\t\t", "proc_id": "1250734", "abstract": "<p>Concurrent garbage collectors are notoriously hard to design, implement, and verify. We present a framework for the automatic exploration of a space of concurrent mark-and-sweep collectors. In our framework, the designer specifies a set of \"building blocks\" from which algorithms can be constructed. These blocks reflect the designer's insights about the coordination between the collector and the mutator. Given a set of building blocks, our framework automatically explores a space of algorithms, using model checking with abstraction to verify algorithms in the space.</p> <p>We capture the intuition behind some common mark-and-sweep algorithms using a set of building blocks. We utilize our framework to automatically explore a space of more than 1,600,000 algorithms built from these blocks, and derive over 100 correct fine-grained algorithms with various space, synchronization, and precision tradeoffs.</p>", "authors": [{"name": "Martin T. Vechev", "author_profile_id": "81100269652", "affiliation": "Cambridge University, Cambridge, England UK", "person_id": "P699721", "email_address": "", "orcid_id": ""}, {"name": "Eran Yahav", "author_profile_id": "81100285431", "affiliation": "IBM Research, Hawthorne, NY", "person_id": "PP35027073", "email_address": "", "orcid_id": ""}, {"name": "David F. Bacon", "author_profile_id": "81100628167", "affiliation": "IBM Research, Hawthorne, NY", "person_id": "P60470", "email_address": "", "orcid_id": ""}, {"name": "Noam Rinetzky", "author_profile_id": "81100129514", "affiliation": "Tel Aviv University, Tel Aviv, Israel", "person_id": "P483756", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1250734.1250787", "year": "2007", "article_id": "1250787", "conference": "PLDI", "title": "CGCExplorer: a semi-automated search procedure for provably correct concurrent collectors", "url": "http://dl.acm.org/citation.cfm?id=1250787"}