{"article_publication_date": "06-10-2007", "fulltext": "\n Software Behavior OrientedParallelization Chen Ding *, Xipeng Shen , KirkKelsey , ChrisTice , Ruke Huang*, \nand Chengliang Zhang Computer Science Dept., University of Rochester Computer Science Dept., CollegeofWilliam \nand Mary * Microsoft Corporation Abstract Many sequential applications are dif.cult to parallelize because \nof unpredictable control .ow, indirect data access, and input\u00addependent parallelism. Thesedif.cultiesledustobuilda \nsoftware system for behavior oriented parallelization (BOP), which allowsa program to be parallelized \nbased on partial information about pro\u00adgram behavior, for example, a user reading just part of the source \ncode, or a pro.ling tool examining merely one or few executions. The basis of BOP is programmable software \nspeculation, where a user or an analysis tool marks possibly parallel regions in the code, and the run-time \nsystemexecutes these regions speculatively. It is imperative to protect the entire address space during \nspecula\u00adtion. The main goal of the paper is to demonstrate that the gen\u00aderal protection can be made cost \neffective by three novel tech\u00adniques: programmable speculation, critical-path minimization, and value-based \ncorrectness checking. On a recently acquired multi\u00adcore, multi-processor PC, the BOP system reduced the \nend-to-end executiontimebyintegerfactorsforaLisp interpreter,adata com\u00adpressor, a language parser, and \na scienti.c library, with no change to the underlying hardware or operating system. Categories and Subject \nDescriptors D.1.2[ProgrammingTech\u00adniques]: Concurrent Programming parallel programming; D.3.4 [Programming \nLanguages]: Processors optimization, compilers General Terms Languages, Performance Keywords speculative \nparallelization, program behavior Dedication This paper is dedicated to the memory of Ken Kennedy, who \npassed away on February 7, for his lasting lead\u00adership in parallel computing research. 1. Introduction \nManyexisting programshave dynamic high-level parallelism, such as,a compression tool that processes databufferbybuffer, \nan En\u00adglish parser parsing sentence by sentence, and an interpreter in\u00adterpreting expression by expression. \nThey are complex and may make extensive use of bit-level operations, unrestricted pointers, exception \nhandling, custom memory management, and third-party Permission to make digital or hard copies of all \nor part of this work for personal or classroom use is granted without fee provided that copies are not \nmade or distributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page.To copyotherwise, to republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. PLDI 07 June 11 13, 2007, San Diego, California, USA. Copyright \nc &#38;#169; 2007ACM 978-1-59593-633-2/07/0006...$5.00 while (1) {get work(); ... BeginPPR(1); ... work(x); \nBeginPPR(1); EndPPR(1); step1(); ... step2(); BeginPPR(2); EndPPR(1); work(y); ... EndPPR(2); } ... Figure \n1. possible loop paral- Figure 2. possible function lelism parallelism libraries. The unknown data access \nand control .ow make such ap\u00adplications dif.cult if not impossible to parallelize in a fully au\u00adtomatic \nfashion. On the other hand, manual parallelization is a daunting task for complexprograms, pre-existing \nones in particular. Moreover, many programs have input-dependent behavior where both the degree and the \ngranularity of parallelism are not guaran\u00adteed or even predictable. The complexity and the uncertain \nperfor\u00admancegain make it dif.cult to justify the investment of time and the risk of error. We address \nthese problems with behavior oriented paralleliza\u00adtion(BOP). Here the behavior is de.ned as the set of \nall possible executions of a program. The goal of BOP is to improve the (part of) executions that contain \ncoarse-grain, possibly input-dependent parallelism, while guaranteeing correctness and basic ef.ciencyfor \nother cases. Unlike traditional code-based approaches that exploit the invariance in all behavior, a \nbehavior basedsystem utilizes par\u00adtial informationto incrementally parallelizeaprogramorto stream\u00adline \nit for common uses. Thenewsystemletsauserorapro.lingtoolto suggest possibly parallelregions (PPR)inaprogrambymarking \nthe start and the end of the region with matching markers: BeginPPR(p) and EndPPR(p). Figures 1 and 2 \nshow the marking of possible (pipelined) loop parallelism and possible function parallelism respectively. \nPPR is region-based (which is different from communication-based do-across [1, 10]), the parallelism \nis likely but not de.nite (which is different from future and parallel section constructs [17, 27]),andaregionmayhave \nunpredictable entriesorexits (whichis different from transactions [19]). To support possible parallelism,BOP \nprotects the entire address space by dividing it into possibly shared and privatizable subsets and monitoring \nand replicating them accordingly. The virtual mem\u00adory (VM) protection mechanism in modern operating systems \ncan be readilyused for this purpose.For BOP,the VM protection effects on-demand data replication and \nsupports complete rollback. The process-based protection has a high overhead. However, much of it is \ninherently unavoidable for a software scheme to support unpredictable computations. A major goal of this \npaper is to show that general protection can be made cost effective by mainly three techniques. First, \nmost overheads starting, checking, and committing are placed on the speculative path, while the system \nuses unmon\u00aditored, non-speculative execution to ensure basic progress if all speculation should fail. \nWe will show how the system holds a tournament between speculativeand non-speculativeexecutions, so that \nit uses the speculative result only when the parallel execution .nishes not only correctlybut alsofaster \nthan the sequentialexe\u00adcution. The parallelized programin theworst caseis asfast as its ingmarkersareexecutedthe \nsame numberof times.Forexample,a longjmpinthe middleofaparallelregionmay causetheexecution to back out \nand re-enter. The BOP system constructs a sequence of non-overlapping PPR instances using a dynamic \nscope. Based on the order the PPR markers are executed at run time, BOP dynamically delineates the boundary \nof PPR instances. While many options are possible, in the current design we use the following scheme: \nAt any point t, the next PPR instance starts from the .rst start marker operation BeginPPR(p) after t \nand then ends at the .rst end marker operation EndPPR(p) after the BeginPPR(p).For example, assume the \nprogram unmodi.ed sequential version. has two PPR regions P and Q marked by m bP , m eP , m bQ , and \nm eQ . Second, BOP uses value-based checking, which is more general If the program, from the start t0, \nexecutes the markers six times than dependence-based checking (also known as Bernstein condi-from t1 \nto t6 as follows: tions [1, 3]). It permits parallel execution in the presence of true t0 t1 t2 t3 t4 \nt5 t6 dependences and it is one of the main differences between process-m bP m bP m eP m bQ m eP m eQ \nbased BOP and existing thread-based systems (as discussed in Sec\u00adtion 2.5). In the paper we give a proof \nof its correctness and show how BOP bounds and hides its run-time costs. In implementation, BOP differs \nfrom prior techniques of specu\u00adlative execution and current techniques of transactional memory in that \ntheoverheadof BOP depends on the size of accessed data rather than the length of the parallel execution. \nConsequently, the cost is negligible if the size of the parallel task is suf.ciently large. This leads \nto its third feature: BOP is programmable.Wewill describe its pro.ling technique and user interface for \n.nding appropriate paral\u00adlel regions. BOP inherits two inherent limitations of speculation: the execu\u00adtionmaybefasterbutnot \nmoreef.cient (becauseofextraCPU, memory, and energy usage), and the speculative region cannot in\u00advoke \ngeneral forms of I/O and other operations with unrecoverable side effects. However, the main advantage \nof BOP is ease of pro\u00adgramming.Parallelizingacomplexapplication will perhapsalways require non-trivial \ncode changes,but the process is greatlyfacili\u00adtatedbythe system becausethe programmerno longer needsto \nun\u00adderstand the entire application, spend time in parallel programming or debugging, or worry about the \nnegative performance impact of parallelization. Compared to thread-based techniques, BOP has the overhead \nof general protection and the problem offalse sharing. However, the overhead of the system is automatically \nhidden by the granularity of tasks regardless of their parallelism. Furthermore, BOP can be more ef.cient \nbecause it uses unmodi.ed, fully optimized sequen\u00adtial code, while explicit threading and its compiler \nsupport are of\u00adten restrained due to concerns over the weak memory consistency on modern processors. \nAs we will see in the evaluation section, the generality and basic ef.ciencymake it possible for large, \nexist\u00ading software to bene.t from parallel execution, that is, to be paral\u00adlelized without being paralyzed. \n2. Behavior-orientedParallelization 2.1 PossiblyParallel Regions The PPR markers are written as BeginPPR(p) \nand EndPPR(p), where p is a unique identi.er. At a start marker, BOP forks a process that jumps to the \nmatching end marker and speculatively executes from there. While multiple BeginPPR(p) mayexistinthe code, \nEndPPR(p) must be unique for the same p. The matching markers can only be inserted into the same function. \nTheexact code sequenceinC language is as follows. BeginPPR(p):if (BeginPPR(p)==1) goto EndPPR p;  EndPPR(p):EndPPR(p); \nEndPPR p:;  At the presence of unpredictable control .ows, there is no guar\u00adanteethatastartmarkerisfollowedbyitsendmarker,orthe \nmatch-Two dynamic PPR instances are from t1 to t3 and from t4 to t6, which will be run in parallel. The \nother fragments of the execution will be run sequentially, although the part from t3 to t4 is also speculative. \nNote that the above scheme is only one example of the manychoices the run-time system can take. In principle, \nBOP may dynamically explore different schemes in the same execution. Compared to the static and hierarchical \nscopes used by most parallel constructs, the dynamic scope lacks the structured paral\u00adlelism to model \ncomplex task graphs and data .ows. While it is not a good .t for static parallelism, it is a useful solution \nfor the extreme case of dynamic parallelism in unfamiliar code. Acoarse-grain task often executes thousands \nof lines of code, communicates through dynamic data structures, and has non-local control .ows (exceptions). \nFunctions may be called through indi\u00adrect pointers, so parallel regions may be interleaved instead of \nbeing disjoint. Some of the non-local error handling may be frequent, for example, an interpreter encountering \nsyntax errors. In addition, ex\u00adceptions are often used idiomatically,forexample, resizingavector upon \nan out-of-bound access. Some non-local jumps are rare.For example, the commonly used gzip program has \nerror checking and abnormalexitinthe compression code. Althoughin ourexperience no error has ever happened, \nif one cannot prove the absence of er\u00adror in gzip (or other sizeable software), dynamicscopes such as \nPPR can be used to parallelize the common cases while guarding against unpredictable or unknown entries \nand exits. Since the PPR markers can be inserted anywhere in a program andexecutedinanyorderat run-time,the \nsystem tolerates incorrect marking of parallelism, which can easily happen when the region is markedbya \npro.ling tool based onafew inputs orgivenbya user unfamiliar with the code. The markers are programmable \nhints, so are other parts of the interface, where the quality of hints affects the parallelismbut not \nthe correctness nor theworst-case performance. 2.2 TheParallel Ensemble The BOP systemuses concurrentexecutionstohidethe \nspeculation overhead offthecritical path,which determines theworst-case per\u00adformance where all speculationfails \nand the program runs sequen\u00adtially. 2.2.1 Lead and Spec Processes Theexecution starts as the lead process, \nwhich continuestoexecute the program non-speculatively until the program exits. At a (pre\u00adspeci.ed) speculation \ndepth k,up tok processes are usedtoexecute the next k PPR instances.Foramachine with p available processors, \nthe speculation depth is set to p - 1 to make the full use of the CPU resource (assuming CPU is the bottleneck). \nFigure3 illustrates the run-time setupby anexample.Part(a) shows the sequential execution of three PPR \ninstances, P , Q, and R.Part(b)showsthe speculativeexecution.Whenthelead process Table 1. BOP actions \nfor unexpected behavior behavior prog. exit or error unexpected PPR markers lead exit continue understudy \nexit continue spec(s) abort speculation continue  2.2.2 Understudy: Non-speculative Re-execution BOP \nassumes that the probability, the size, and the overhead of parallelism are all unpredictable. The understudy \nprovides a safety net not only for correctness (when speculationfails)but also for performance(when speculationistooslow).For \nperformance, BOP holdsatwo-team race betweenthe non-speculative understudyand the speculative processes. \nThe non-speculative team represents the worst-case perfor\u00admance or the critical path. If all speculation \nfails, it sequentially executesthe program.Aswewillexplaininthenextpart,theover\u00adhead for the lead process \nonly consists of the page-based write monitoring for the .rst PPR instance. The understudy runs as the \noriginalcode withoutanymonitoring.Asa result,ifthe granularity of PPR instance is large or when the speculation \ndepth is high, the worst-case running time should be almost identical to that of the unmodi.ed sequential \nexecution. On the other hand, whenever the speculation .nishesfaster than the understudy,it meansa perfor\u00admance \nimprovement over the would-be sequential execution. reaches the start marker of P , m bP , it forks the \n.rst speculation process, spec 1, and continues to execute the .rst PPR instance. Spec 1 jumps to the \nend marker of P and executes from there. When spec 1 reaches the start of Q, m bQ , it forks the second \nspeculation process, spec 2, which jumps ahead to execute from the end of Q. At the end of P , the lead \nprocess starts the understudy process, which re-executes the following code non-speculatively. Then it \nwaits for spec 1 to .nish, checks for con.icts, and if no con.ict is detected, commits its changes to \nspec 1, which assumes the role of the lead process so later speculation processes are handled recur\u00ad \nsively in a similar manner. The kth spec is checked and combined after the .rst k-1 spec processes commit. \nWhen multiple spec pro\u00ad cesses are used, the data copying is delayed until the last commit. The changed \ndata is copied only once instead of multiple times in a rolling commit. Ourimplementationof these stepsis \nshown later in Section 3.1. The performance bene.t of understudy comes at the cost of potentially redundant \ncomputation. However, the cost is at most one re-execution for each speculatively executed PPR, regardless \nof the depth of the speculation. With the understudy, the worst-case parallel running time is mostly \nequal to the sequential time. One may argue that this can be easily done by running the sequential version \nside by side in a sequential-parallel race. The difference is that the BOP system is running a relay \nrace for every group of PPR instances. At the whole-program level it is sequential-parallel collaboration \nrather than competition because the winner of each relay joins together to make thefastest time.Every \ntime counts when speculation runs faster, and no penalty when it runs slower. In addition, the parallel \nrun shares read-only data in cache and memory, while multiple se\u00ad quential runsdo not. Finally, runningtwo \ninstancesofa programis not always possible for a utilityprogram, since the communication with the outside \nworld often cannot be undone. In BOP, unrecover- The speculation runs slower than the normal execution \nbecause of the startup, checking, and commit costs. The costs may be much higher in process-based systems \nthan in thread-based systems. In the example in Figure 3(b), the startup and commit costs, shown as gray \nbars, are so high that the parallelexecutionof spec1.nishes slower than the sequential understudy. However, \nby that time spec 2has .nished and is ready to commit. The second commit .nishes before the understudy \n.nishes, so spec2aborts the understudy and becomes the next lead process. BOP executes PPR instances \nin a pipeline and shares the basic property of pipelining: if there is an in.nite number of PPRs, the \naverage.nishtimeis determinedbythe startingtimenotthelength of each speculation. In other words, the \nparallel speed is limited only by the speed of the startup and the size of the sequential region outside \nPPR. The delays during and after speculation do not affect the steady-state performance. This may be \ncounter intuitive at .rst because the commit time does not matter even though it is sequentiallydone.IntheexampleinFigure3(b),spec2has \nsimilar high startup and commit costsbut they overlap with the costs of spec 1. In experiments with real \nprograms, if the improvement jumps after a small increase in the speculation depth, it usually indicates \na high speculation overhead. able I/O and system calls are placed outside the parallel region. 2.2.3 \nExpecting the Unexpected Figure3shows theexpected behavior when anexecutionof PPR runs from BeginPPR \nto EndPPR.In general, theexecution may reach an exit (normal or abnormal) or an unexpected PPR marker. \nTa\u00adble1shows the actions of the three types of processes when they encounter an exit, error, or unexpected \nPPR markers. The abortby spec inTable1is conservative.Forexample, spec\u00adulation may correctly hit a normal \nexit, so an alternative scheme maydelaythe abortandsalvagetheworkifit turnsout correct.We favor the conservative \ndesign for performance. Although it may re\u00adcompute useful work, the checking and commit cost cannot delay \nthe critical path. The speculation process may allocate an excessive amount of memory and attempt unrecoverable \noperations such as I/O, other OS calls, or user interactions. The speculation is aborted upon .le reads, \nsystem calls, and memory allocation over a threshold. The .le output is buffered and is either written \nout or discarded at the commit point. Additional engineering can support regular .le I/O. The current \nimplementation supports stdout and stderr for the pragmatic purpose of debugging and verifying the output. \n 2.2.4 Strong Isolation We say thatBOP uses strong isolation because the intermediate re\u00adsults of the \nlead process are not made visible to speculation pro\u00adcesses until the lead process .nishes the .rst PPR. \nStrong isolation comes naturally with process-based protection. It is a basic differ\u00adence between BOP \nand thread-based systems, where the updates of one thread are visible to other threads.We call it weak \nisolation. We discuss the control aspect of the difference here and complete the rest of comparisons \nin Section 2.5 after we describe the data protection. Weak isolation allows opportunistic parallelism \nbetween two dependent threads, if the source of the dependence happens to be executed before the sink. \nIn the BOP system, such parallelism can be made explicit and deterministic using PPR directives by placing \ndependent operations outside the PPR region (for example, in Figure 3, the code outside PPR executes \nsequentially). At the loop level, the most common dependence comes from the update oftheloop indexvariable.With \nPPR, the loop control can be easily excluded from the parallel region and the pipelined parallelism is \nde.nite instead of opportunistic. The second difference is that strong isolation does not need syn\u00adchronization \nduring the parallelexecutionbut weak isolation needs to synchronize between the lead and the spec processes \nwhen com\u00admunicating the updates between the two. Since the synchroniza\u00adtion delaysthe non-speculativeexecution,itadds \nvisibleoverheads (when speculationfails)tothe thread-based systemsbutnotto BOP. Although strong isolation \ndelays data updates, it detects specu\u00adlationfailure and success before the speculation ends. Likesystems \nwith weak isolation, strong isolation detects con.icts as theyhap\u00adpen because all access maps are visible \nto all processes for reads (each process can only update its own map during the parallel exe\u00adcution). \nAfter the .rst PPR, strong isolation can check for correct\u00adness before the next speculation .nishes by \nstopping the specula\u00adtion, checking for con.icts, and communicating data updates. As a design choice, \nBOP does not abort speculation early because of the property of pipelined parallelism, explained at the \nend of Sec\u00adtion 2.2.1. The speculation process, no matter how slow, may im\u00adprove the program speed, when \nenough of them work together.  2.3 Checkingfor Correctness The BOP system guarantees that the same result \nis produced as in the sequential execution if the speculation succeeds. It par\u00adtitions the address space \nof a running program into three dis\u00adjoint groups: shared, checked, and private. More formally, we say \nDall = Dshared + Dchecked + Dprivate, and any two of Dshared,Dchecked, and Dprivate do not overlap. For \nthe following discussion we consider two concurrent processes the lead process thatexecutes the current \nPPR instance, and the spec process that executes the next PPR instance and the code in between. The cases \nfor k (k> 1)speculation processes can be proved inductively since theycommit in a sequence in the BOP \nsystem. 2.3.1 Three types of data protection Page-based protection of shared data All program data are \nshared at BeginPPR by default and protected at page granularity. During execution, the system records \nthe location and size of all global variables and the range of dynamic memory allocation. At BeginPPR, \nthe system turns off write permission for the lead pro\u00adcess and read/write permission for the spec processes. \nIt installs customized page-fault handlers that raise the permission to read or write upon the .rst read \nor write access. At the same time, the han\u00addler records which page has what type of access by which process. \nAt the commit time, each spec process is checked in an increasing order, the kth processfailsif and onlyifa \npageis writtenby the shared = GetTable(); ... while (...) { ... BeginPPR(1) ... if (...) checked = checked \n+ Search(shared, x) Insert(private, new Node(checked)) ... if (!error) Reset(checked) ... EndPPR(1) ... \n} Figure 4. Examples of shared, checked, and private data lead process and the previous k -1 spec processesbut \nreadby spec k. If speculation succeeds, the modi.ed pages are merged into a single address space at the \ncommit point. By using Unix processes for speculation, the BOP system elim\u00adinates all anti-and output \ndependences through the replication of the address space and detects true dependences at run-time. An \nex\u00adampleis thevariable shared in Figure 4. It may point to some large dictionary data structures.Page-based \nprotection allows concurrent executions as long asalaterPPR does not need the entries produced by a previous \nPPR. The condition is signi.cantly weaker than the Bernstein condi\u00adtion [3], which requires that no twoconcurrent \ncomputations access the same data if at least one of the two writes to it. The additional parallelism \nis due to the replication of modi.ed data, which re\u00admoves anti-and output dependences.Forexample, the \nwrite access by spec k to a page does not invalidate earlier spec processes that read concurrently from \nthe same (logical) page. Page-based protection has been widely used for supporting dis\u00adtributed shared \nmemory [22,23] and manyother purposes including race detection [28]. While these systems enforce parallel \nconsis\u00adtencyamong concurrent computations, the BOP system checks for dependence violation when running \na sequential program. A common problem in page-level protection is false sharing. We alleviate the problem \nby allocating each global variable on its own page(s). Writes to different parts of a page may be detected \nby checking the difference [22] at the end of PPR. In addition, the shared data is never mixed with checked \nand private data (to be described next) on the same page, although at run time newly allocated heap data \nare private at .rst and then converted to shared data at EndPPR. Value-based checking Dependence checking \nis based on data ac\u00adcessnotdatavalue.Itissuf.cientbutnot necessaryfor correctness. Consider the variable \nchecked in Figure 4, which causes true de\u00adpendences as both the current and next PPR instances may read \nand modify it. On the other hand, the reset statement at the end may re\u00adinstall the old value as checked \nhad at the beginning. The parallel execution is still correct despite of the true dependence violation. \nThis case is called a silent dependence [31]. Thereisoftenno guaranteethatthevalueofavariableisresetby \nEndPPR.In theexample, the reset depends ona.ag, so the silence is conditional. Even after a reset, the \nvalue may be modi.ed by pointer indirection. Finally, the rest operation may assign different values \nat different times. Hence run-time checking is necessary. For global variables, the size is statically \nknown, so the BOP system allocates checked variables in a contiguous region, makes a copyof their value \nat the BeginPPR of the lead process, and checks their value at the EndPPR.For dynamic data, the system \nneeds to knowthe rangeof addressesand performsthe same checking steps. Checked data are found through \npro.ling analysis or identi.edbya user (described more in Section 2.6). Since the values are checked, \nincorrect hints would not compromise correctness. In addition, a checked variable does not have to return \nto its initial value in every PPR instance. Speculation still bene.ts if the value remains constant for \njust two consecutive PPR instances. Most silent dependences come from implicit re-initialization. Some \nexamples are that the loop level increments and decrements when a compiler compiles a function, the traversed \nbits of the objects in a graph are set and reset during a depth-.rst search, and thework-listis .lled \nand emptiedina scheduling pass.We classify these variables as checked data, which may take the same value \nat BeginPPR and EndPPR, in other words, the PPR execution may have no visible effect on the variable. \nThe shared data and checked data have a signi.cant overlap, which are the data that are either read only \nor untouched by the parallel processes. We classify them as checked if their size is small; otherwise, \nthey are shared. A problem is when different partsofastructureoran array requiredifferent protection \nschemes. Structure splitting, when possible, may alleviate the problem. The correctness of checked data \nis not obvious because their intermediate values may be used to compute other values that are not checked. \nSection 2.4 presents a formal proof of the correctness to show how the three protection schemes work \ntogether to cast a complete shield against concurrency errors. Likely private data The third group is \nprivate data, which is initialized before being used and therefore causes no con.ict. In Figure 4, if \nprivate is always initialized before it is used, the access in the current PPR cannot affect the result \nof the next PPR, so any true dependence cause by it can be ignored. Private data come from three sources. \nThe .rst is the program stack,which includeslocalvariablesthatareeitherread-onlyinthe PPR or always initialized \nbefore use. Data.owanalysis can identify privatizable variables [4, 16]. When the two conditions cannot \nbe guaranteed by compiler analysis, for example, due to unknown control .ow or the address of a local \nvariable escaping into the program heap, we can rede.ne the local variable to be a global variable and \nclassify it as shared data.For recursive functions, we can either use a stack of pages or disable the \nPPR. The second sourceisglobalvariablesand arraysthatarealways initialized before the use in the PPR. \nThe standard technique to de\u00adtectthisisinter-proceduralkill analysis[1].In general,a compiler may notalways \nascertain all casesof initialization.For global data whose access is statically known in a program, the \ncompiler auto\u00admatically inserts calls after the initialization assignment or loop to classify the data \nas private at run time. Any access by the specula\u00adtion process before the initialization causes it to \nbe treated as shared data.For (non-aggregate)datathatmaybe accessedbypointers,the systemplacesitonasinglepageand \ntreatsitasshareduntilthe.rst access. Additionally, we allow the user to specify the list of vari\u00adables \nthat are known to be written before read in PPR. These vari\u00adables are reinitializedto zeroatthe startofa \nPPR instance. Since we cannot guarantee write-.rst access in all cases, we call this group likely private \ndata. The third type of private date is newly allocated data in a PPR instance. Before BeginPPR, the \nlead process reserves regions of memory for speculation processes. Speculation would abort if it allocates \nmore than the capacity of the region. The main process does not allocate into the region, so at EndPPR, \nits newly allocated data can be merged with the data from the speculation process. For programs that \nusegarbage collection, we encapsulate the heap region of spec processes, which we will describe when \ndiscussing the test of a lisp interpreter. Another solution is to ignore GC, which, if happens during \na PPR instance, will cause speculation tofail becauseofthe manychangesitmakestothe shared data. Overheads \non the critical path The three data protection schemes are summarized and compared in Table 2. We now \ndiscuss their overheads. Most speculation costs the forking of speculation pro\u00adcesses, the change of \nprotection, data replication and read and write monitoring,the checkingof access maps for con.icts, the \nmerging of modi.ed pages, and the competition between the understudy and the spec processes are offthe \ncritical path. Therefore, the relation between the worst-case running time T max parallel and the time \nof un\u00admodi.ed sequential program Tseq is T max parallel = Tseq +c1 * (Sshared/Spage) +c2 * (Smodified \nby 1st ppr + Schecked) The two terms after Tseq are the cost from data monitoring and copying on the \ncritical path, as we explain next. For monitoring, at the start ofPPR, the lead process needs to set \nand reset the write protection and the access map for shared data (including global variables) before \nand after the .rst PPR instance. The number of pages is the size of shared data Sshared divided by the \npage size Spage, and the cost per page is a constant c1. During the instance,a write pagefaultis incurred \nforevery pageof shared data modi.ed in the .rst PPR instance. The constant per page cost is negligible \ncompared to the cost of copying a modi.ed page. Two types of copying costs may appear on the critical \npath. The .rst is for pages of shared data modi.ed by the lead process in the .rst PPR instance and (among \nthose) pages modi.ed again by the understudy. The second is taking the snapshot of checked data. The \ncost in the above formula is the worst case. The copy-on-write mechanism in modern OS may hide most of \nboth costs. Data copying may hurt locality across PPR boundaries, although the locality within is preserved. \nThe footprint of a speculative run is larger than the sequential run as modi.ed data are replicated. \nHowever, the read-only data is shared by all processes in main memory and in shared cache (that is physically \nindexed). As a result, the footprint may be much smaller than running k copies of a program.  2.4 The \nCorrectness Proof It is suf.cient to prove the correctness for a single instance of the parallel execution \nbetween two PPR instances. We .rst de.ne an abstract model of an execution. memory Vx: a set of variables. \nVall represents all variables in memory. memory state SVt :the content ofV at time t.For ease of reading \nwe use Sxt (rather than SVt x )to denote the state ofVx at t. instruction rx:the instructions we consider \nare the markers of the two PPRs, P and Q, P b , P e , Qb, and Qe (corresponding to beb e mP , mP , mQ, \nand mQ in Section 2.1). P and Q can be the same region. execution state (rx,SVt ): a point in execution \nwhere the current instruction is rx and the state is SVt . execution (r1,St1 =p all): a continuous execution \nof a all) . (r2,St2 process p (which can be either seq, lead or spec) from instruc\u00adtion r1 and state \nSt1 to the next occurrence of r2 at the state all St2 all. Figure5showstheparallelexecutionandthe statesofthe \nlead and the spec processes at different times. If a parallel execution passes the three data protection \nschemes, all program variables in our abstract model can be partitioned into the following categories: \n Vwf :variables whose .rst access by spec is a write. wf stands for write .rst.  Vexcl lead:variables \naccessed onlybylead whenexecuting the .rst PPR instance P .  Table 2. Three types of data protection \ntype shared data Dshared checked data Dchecked (likely) private data Dprivate protection Not written \nby lead and read by spec Value atBeginPPR is the same at EndPPR in lead. Concurrent read/write allowed. \nno read before 1st write in spec. Concurrent read/write allowed. granularity page/element element element \nneeded support compiler, pro.ler, run-time compiler, pro.ler, run-time compiler (run-time) overhead on \ncritical path 1fault per mod. page copy-on-write copy-on-write copy-on-write  Figure 5. The states of \nthe sequential and parallel execution Vexcl spec:variables accessed only by spec.  Vchk:the remaining \nvariables.chk stands for checked.  Vchk = Vall - Vwf - Vexcl lead - Vexcl spec Examining Table 2, we \nsee that Dshared contains data that are either accessed by only one process(Vexcl lead and Vexcl spec), \nwritten before read in spec (Vwf ), read only in both processes or not accessedby either(Vchk). Dprivate \ncontains data either in Vwf or Vchk. Dchecked is a subset of Vchk. In addition, the following two conditions \nare met upon a successful speculation. 1. the lead process reaches the end of P at P e, and the spec \nprocess, after leaving P e, executes the two markers of Q, Qb and then Qe . 2. the state of Vchk is \nthe same at the two ends of P (but it may  = Slead change in the middle), that is, Sinit chk chk . To \nanalyze correctness, we examine the states of the sequential execution, Sinit at P b and Sseq at Qe of \nthe sequential process , Slead at seq, and the states of the parallel execution, Sinit at P b P e of \nthe lead process and Sinit at P e and Sspec at Qe of the spec process. These states are illustrated in \nFigure 5. The concluding state of the parallel execution, Sparallel at Qe , is a combination of Slead \nand Sspec after the successful specula\u00adtion.Tobeexact, the merging step copies the modi.ed pages from \nthe lead process to the spec process, so parallel = Sspec lead S+ S excl lead all-excl lead In the following \nproof, we de.ne each operation rt by its inputs and outputs. All inputs occur before anyoutput. The inputs \nare the read set R(rt). The outputs include the write set W (rt) and the next instruction to execute, \nrt+1 1. 1An operationis an instanceofa program instruction.For the simplicityof the presentation, we \noverload the symbol rx as both the static instruction and its dynamic instances.To distinguish in the \ntext, we callthe former an instruction and the latter an operation, so we may have only one instruction \nrx but anynumber of operationsrx. THEOREM 1 (Correctness). If the spec process reaches the end marker \nof Q, and the protection inTable2passes, the speculation is correct, because the sequential execution \nwould also reach Qe with a state Sseq = Sparallel, assuming that both the sequential and the parallel \nexecutions start with the same state, Sinit at P b . spec ,Sinit Proof Consider the speculative execution, \n(P e)=. ,Sspec (Qe), for the part of the sequential execution, (P e,Smid) seq (Qe,Sseq). =. We denote \nthe correct sequen\u00adtial execution as pe,r1,r2, \u00b7\u00b7\u00b7 and the speculative execution as ;;; pe,r1,r2, \u00b7\u00b7\u00b7 \n.Weprovebycontradiction thatevery operation rt in the speculativeexecution mustbe identical to rt in \nthe sequential ; execution in the sense that rt and rt are the same instruction, they read and write \nthe same variables with the same values, and they move next to the same instruction rt+1. ; Assume the \ntwo sequences are not identical and let rt be the .rst instruction that produces a different value than \nrt, either by modifying a different variable, the same variable with a different ; value, or moving \nnext to a different instruction. Since rt and rt are the same instruction, the difference in output must \nbe due to a difference in the input. ; Suppose rt and rt read a variable v but see different valuesv \nand v;. Since the values cannot differ if the last writes do not exist, ; let rv and rv be the previous \nwrite operations that produce v and ; v . ;; The operation rv can happen either in spec before rt or \nin the lead process as the last write to v. We show neither of the two ; cases is possible. First, if \nrv happens in spec, then it must produce ; the same output as rv as per our assumption that rt is the \n.rst to ; deviate. Second, rv is part of lead and produces a value not visible ; to spec. Consider \nthe only way v canbe accessed. Since(rv is the last write so) v is read before being modi.ed in spec, \nit does not belong to Vwf or Vexcl lead. Neither is it in Vexcl spec since it is modi.ed in the lead \nprocess. The only case left is for v to belong = V init to Vchk. Since V lead , after the last write \nthe value of v is chk chk restored to the beginning state where spec starts and consequently ; cannot \ncause rt in spec to see a different value as rt does in the ; sequential run. Therefore rt and rt cannot \nhave different inputs and produce different outputs, and the speculative and sequential executions must \nbe identical. = Sseq WenowshowthatSparallel is correct, that is, Sparallel . Since spec reads and writes \ncorrectvalues, Vwf , Vexcl spec, and the accessed part of Vchk are correct. Vexcl lead is also correct \nbecause ofthecopyingofthe theirvaluesatthe commit time.The remaining part of Vchk is not accessedby lead \nor spec and still holds the same value as Sinit. It follows that the two states Sparallel and Sseq are \nidentical. The proof is similar to that of the Fundamental Theorem of De\u00adpendence(Sec. 2.2.3 in [1]). \nWhile the proof in the book deals with statement reordering, the proof here deals with region reordering \nandvalue-based checking.It rules outtwocommon concerns. First, the intermediate values of checked data \nnever lead to incorrect re\u00adsults in unchecked data. Second, the data protection always ensures the correct \ncontrol .ow by speculation. In BOP, the three checking schemes work together to ensure these strong guarantees. \nTable 3. Comparisons between strong and weak isolation during speculation strong weak data updates visible \nto outside no yes overall overhead proportional to data size data use synchronization on critical path \nnone needed hardware memory consistency independent dependent support value-based checking yes no detect \nspec failure early yes yes can certify spec success early yes yes  2.5 Comparisons with Thread-based \nSystems Strong and weak isolation as discussed in Section 2.2.4 is a basic difference between process-based \nBOP and thread-based systems that include most hardware and software speculation and transac\u00adtional memory \ntechniques. The previous section discussed the con\u00adtrol aspect. Here we discuss the data protection and \nsystem imple\u00admentation. The comparisons are summarizedinTable3. Weak isolation needs concurrent access \nto both program data and system data. It needs synchronization to eliminate race con\u00additions between \nparallel threads and between the program and the run-time system. The problem is complicated if the hardware \nuses weak memory consistency, which does not guarantee correct re\u00adsults without explicit synchronization, \nif the memory operations are reorderedbythe compilerandthe hardware.Infact, concurrent threads lacka \nwell-de.ned memory model[5].Arecent loop-level speculation system avoids race conditions and reduces \nthe num\u00adberof critical sections(to1)by carefullyorderingthesystemcode based onasequential memory consistencymodel \nand adding mem\u00adory directives to enforce the order under relaxed consistencymod\u00adels [9]. In BOP, parallel \nprocesses are logically separated. The correct\u00adness check is done sequentially in rolling commits with \na complete guaranteeas statedin Theorem1. Thereisno synchronizationover\u00adhead on the critical path. The \ncompiler and hardware are free to reorder program operations as theydo for a sequential program. Thread-based \nsystems do not yet support general value-based checking. When data updates are visible, the intermediate \nvalue of a checked variable can be seen by a concurrent thread and the effect cannot be easily undone \neven if the variable resumes the ini\u00adtial value afterwards. This is widely known as the ABAproblem, where \na thread may be mistakenly holding different data by the same pointer (see [18] for a solution). In hardware, \na correct value prediction may cause a thread to read at a wrong time and vio\u00adlate the sequential consistency, \nso value prediction requires careful extra tracking by hardware [24]. No software speculation systems \nwe know usevalue-based checking.With strong isolationin BOP, the intermediatevaluesof checkedvariableshavenoeffecton \nother processes,sovalue-based checkingisnotonly correctbutalsoadds little cost on the critical path. \nValue-based checking is different from value-speci.c dynamic compilation (for example in DyC [13]), which \n.nds values that are constant for a region of the code rather than values that are the same at speci.c \npoints of an execution (and can change arbitrar\u00adily between these points). It is different from a silent \nwrite, which writesthe samevalueastheprevious writetothevariable.Our soft\u00adware checking happens once \nper PPR for a global set of data, and the correctness is independent of the memory consistencymodel of \nthe hardware. Most previous techniques monitor data at the granularity of array elements, objects, and \ncache blocks; BOP uses pages for heap dataand paddedvariablesfor global data.Paging supportis more ef.cient \nfor monitoring unknown data structuresbut ittakes more timetosetupthe permissions.Italsogivesrisetofalse \nsharing.  Figure 6. An illustration of the parallelism analysis. Region\u00adcarried dependences, shown as \ndotted edges, cause speculation to fail. Loop-carried dependences are marked with a bar. The cost of \npage-based monitoring is proportional to the size of accessed data (for the overhead on the critical \npath it is the size of modi.ed data) rather than the number of accesses as in thread\u00adbased systems, making \npage-based protection especially suitable for coarse-grain parallelism. 2.6 Programming using PPR 2.6.1 \nThe Pro.ling Analysis Of.ine pro.ling analysis .rst identi.es the high-level phase struc\u00adtureofaprogram[32,33].Thenit \nuses dependence pro.lingto.nd the phase with thelargest portion of run-time instructions that can be \nexecuted in parallel as the PPR. At the same time, it classi.es program data into shared, checked and \nprivate categories based on their behavior in the pro.ling run. For example consider a PPR within a loop. \nWe call the rest of the loop the control code. Figure 6 shows an example PPR region.For each possible \nregion duringa training run, an instance is parallelizable if there is no dependence that originates \nfrom this instance region to later regions or later instances of the same region. Figure6shows most typesof \ndependences. While not all types are shown for simplicity, all three types of region-carried dependences, \nwhich would prohibit parallelization, are shown as dotted edges. Loop-carried dependences, marked by \na bar, are not the same as region-carried dependences. In addition to the parallelism, we need to analyze \nthe size of possible PPR instances. The parallelism of a region is then measured by the total size of \nits parallel instances. We want to .nd the largest of possible parallel regions. Abrute force method \nwould test all possible regions, which is `\u00b4 nn(n-1) 2 = 2 for a loop body with n statements. Alternatively \nwe can usea graph model.We de.ne each statementin the loop body as an elementary region. We construct \na graph in which a node represents an elementary region, and a directed edge exists betweentwonodesiftheyhaveadependence.We.rst.ndstrongly \nconnected components and collapse each into a single node. A node cannot belong to PPR ifithasa loop-carried \ndependence.The largest PPR is the set of nodes that represent a continuous region that has no outgoing \ndependences. The time complexity is linear to the size of the graph, which in the worst case is the same \nas complete enumeration. Phase analysis identi.es coarse-grain tasks such as locality phases in scienti.c \nprograms [33] and variable-length phases in compilers, interpreters and data transcoding programs [32]. \nThe analysis problem is similar to the one illustrated in Figure 6 ex\u00adcept that the boundary of possible \nregions are phase markers. Each region is continuous at run time even though the code for it may not \nbe contiguous in the program. Infact, the same program code may belong to both PPR and control. 2.6.2 \nThe Programming Interface BOP can also be invoked manually. The programming interface has three parts. \nThe .rst is the PPR markers, which can be practically peppered anywhere in anyquantity in a program. \nThe second is a list of global and static variables that are write .rst (privatizable) and checked. The \nprogrammer speci.es the place where the vari\u00adables are initialized, and the system treats the data as \nshared until the initialization. The third component of the interface is the run-time feedback to the \nuser. When speculationfails, the system outputs the causeof thefailure,in particular, the memory page \nthat receives con.icting accesses.In our current implementation, globalvariables are placed on separate \nmemory pages by the compiler. As a result, the system can output the exact name of the global variable \nwhen it causes a con.ict.A user can thenexamine the code and remove the con.ict by marking the variable \nprivatizable or moving the dependence out of the parallel region. Three featuresoftheAPIare especially \nusefulforworkingwith large, unfamiliar code. First, the user does not write a parallel pro\u00adgram and never \nneeds parallel debugging. Second, the user paral\u00adlelizes a program step by step as hidden dependences \nare discov\u00adered and removed one by one. Finally, the user can parallelize a programfora subsetof inputs \nratherthanall inputs.The program can run in parallel even if it has latent dependences. 3. Evaluation \n 3.1 Implementation and Experimental Setup We have implemented the compiler support in Gcc 4.0.1. After \nhigh-level program optimization passes but before machine code generation, the compiler converts global \nvariables to use dynamic allocation for proper protection.Wedid not implement the compiler analysis for \nlocal variables. Instead the system privatizes all stack data. All global and heap data are protected. \nEach global variable is allocated on separate page(s) to reduce false sharing. We im\u00adplemented similar \nsystems using two binary instrumentors, which donot require program sourcebutoffernoeasywayof relocating \nglobal data, tracking register dependences, or .nding the cause of con.icts at the source level. The \nBOP run-time is implemented as a statically linked library. For lack of space, we only include a part \nof the pseudo-code in Figure 7. It shows the basic steps taken when a process reaches EndPPR. Partial \ncommit or early error detection requires further re.nements. The listed code has three key comments in \nitalics. The .rst two mark the .nish line of the sequential-parallel race. The third comment marks the \nimportant fall-through case of the if statement, where a speculation process, after passing the cor\u00adrectness \ncheck, implicitly becomes the lead process and continues the rolling commit with the next speculation \nprocess. Note that the commit procedure also includes actions invoked by signals, which are not shownin \nthe pseudo-code.Forexample, the concede sig\u00adnal from the understudy will cause all except the new lead \nprocess to exit. The implementation uses shared memory for storing snap\u00adshots, access maps, and for copying \ndata at a commit. The rest of communication is done by signals. No locks are used. We have implemented \nan instrumentor and a behavior analyzer. The instrumentor, also based on Gcc 4.0.1, collects complete \npro\u00adgram traces with unique identi.ers for instructions, data accesses, and memory and register variables, \nso the behavior analyzer can track all data dependences and identify PPR. In BOP, the original lead process \nmay die long before the pro\u00adgram ends, since each successful speculation produces a new lead subroutine \nBOP EndPPR(): switch (myStatus) case understudy: undyWorkCount++ if (undyWorkCount<specDepth) return \nthe .nish line of the sequential execution myStatus = lead undyWorkCount =0 AbortSpeculation() CommitOutput() \n return case spec: WaitForLeadDoneSignal() SendSpecDoneSignal() if (FoundCon.icts()) AbortSpeculation() \nif (lastSpec) ClearProtection() PullChangedDataFromPipe() WaitEnsureUnderstudyCreated() SignalUnderstudySpecSuccess() \nWaitForUnderstudyToConcede() the .nish line of the parallel execution myStatus = lead CommitOutput() \nreturn else fall through case lead: if (myStatus == lead) ClearProtection() StartUnderstudy() SignalLeadDone() \nWaitSpecDoneSignal() PushChangedDataToPipe() Figure 7. Pseudo-code executed at the end of a PPR region \n (forexamplein Figure3).For one momentwe thoughtwehadan incredible speedup. Now each parallelized program \nstarts with a timing process that forks the .rst lead process and waits until the last process is over \n(when a lead process hits a program exit). In\u00adstead of counting the time for all processes, we use the \nwall-clock timeof the timing process, which includes theOSoverheads.We use multiple runs on an unloaded \nsystem. We use GNU Gcc 4.0.1 with -O3 .ag for all programs.We use a new Dell PowerEdge 6850 machine (after \ninstalling a 250V power supply) with four dual-core Intel 3.40 GHz Xeon 7140M processors (a total of \n8 CPUs), 16MB cache, and 4GB physical memory. 3.2 Gzip v1.2.4byJ. Gailly Gzip takes one or more .les \nas input and compresses them one by one using the Lempel-Zivcoding algorithm (LZ77).Weuseversion 1.2.4 \navailable from the SPEC 2000 benchmark suite. Much of the 8616-lineCcode performs bit-level operations, \nsome through in\u00adline assembly. Thekernelwas based on an earlier implementation on 16-bit machines. We \ndid not specify spec so the program behaves as a normal compressor rather than a benchmark program (which \narti.cially lengthens the inputby replication). We make two parallel versions, one by automatic methods, \nand the other by hand. In the .rst one, BeginPPR and EndPPR are automatically inserted before reading \na .le and after the output of the compressed .le (for this one we allow .le I/O in the PPR), and the \nvariables and allocation sites are classi.ed through pro.ling analysis. The second parallel version compresses \na single .le in parallel. The sequential code compresses one buffer at a time and stores the results \nuntil the output buffer is full. We manually placed PPR around thebuffer loop and speci.ed the set of \nlikely private variables through the program interface described in Section 2.6. The program returned \ncorrect resultsbut speculationfailed because of con.icts caused by two variables, unsigned short bi buf \nand intbi valid ,as detectedbythe run-time monitoring. Thetwovariablesareusedinonlythreeshort functions.Inspect\u00ading \nthe code, we realized that the compression algorithm produced bits, not bytes, and the two variables \nstored the partial byte of the lastbuffer. The dependencewas hidden below layersof code and among 104 \nglobalvariables,but the run-time analyzer enabled us to quickly uncover the hidden dependence.We .rst \ntried to .ll the byte, as the program does for the last byte. However, the result .le couldnotbe decompressed.Infact,asingleextraor \nerrorbitwould render the output .le meaningless to the decompressor. Our second solutionisto compressbuffersin \nparalleland concatenatethe com\u00adpressed bits afterwards. This requires tensof linesof coding,butit was \nsequential programmingandwasdonebyoneofthe authorsin one day. Inter-.le parallel gzip performs similarly \nas one may get from invoking multiple gzip programs. The intra-.le compression per\u00admits single-.le compression \nto use multiple processors. We test bop-gzip on a single 84MB .le (the Gcc4.0.1 tar .le) and com\u00adpares \nthe running times of the unmodi.ed sequential code and the BOP version with threespeculation depths. \nEachPPR instance com\u00adpressesabout10MBoftheinput.le.Theexecutiontimeis stablein sequential runsbutvariesby \nas much as 67%in parallel runs, soin the following table we include the result of six consecutive tests \nof each version and compute the speedup based on the average time. version sequen\u00ad speculation depth \ntial 1 3 7 times (sec) 8.46, 8.56, 8.50, 8.51 8.53, 8.48 7.29, 7.71 7.32, 7.47 5.70, 7.02 5.38, 5.49, \n4.16, 5.71 5.33, 5.56 4.80, 4.47, 4.49, 3.10 2.88, 4.88 avg time 8.51 7.09 5.27 4.10 avg speedup 1.00 \n1.20 1.61 2.08 With 2, 4, and 8 processors, the parallel compression gains speedups of 1.20, 1.61, and \n2.08. The 8-way gzip is twice asfast. The compression is now slightly faster than decompression (by gunzip), \nwhich is unusual for such a tool. The critical path of bop\u00adgzip,when all speculationfails, runs slightlyfaster \nthan the sequen\u00adtial version because of the effect of prefetching by the speculation. The BOP run-time \nallocates 138 4KB pages for the 104 global variables, 19200 pages for shared heap data, and 9701 pages \nfor likely write-.rst data. The commit operations copy a total of 2597, 3912, and 4504 pages respectively \nduring the parallelexecution for the three speculation depths.  3.3 Sleator-Temperley LinkParser v2.1 \nAccording to the Spec2K web site, The parser has a dictionary of about 60000 word forms. It has coverage \nof a wide variety of syntactic constructions, including many rare and idiomatic ones. ... It is able \nto handle unknown vocabulary, and make intelligent guesses from context about the syntactic categories \nof unknown words. It is not clear in the documentation or the 11,391 lines of its C code whether the \nparsing of sentences can be done in parallel.Infact,they are not.IfaPPR instance parsesa command sentence \nwhich changes the parsing environment, e.g. turning on or offthe echo mode, the next PPR instance cannot \nbe speculatively executed. This is a typical example of dynamic parallelism. The parallelism analyzer \nidenti.es the sentence-parsing loop. We manually strip-minetheloop(fora recenttextbook description see \n[1]) to create a larger PPR. The data are then classi.ed auto\u00admatically. During the training run, 16 \nvariables are always written .rst by the speculation process during training, 117 variables al\u00adwayshavethe \nsamevalueatthetwoendsofa PPR instance, and 35 variables are shared. The system allocates 132 pages of \nshared data and 75 pages private and checked data. We test the parallel parser using 1022 sentences \nobtained by replicating SPEC95 train input twice. When each PPR includes the parsing of 10 sentences, \nthe sequential run takes 11.34 second, and the parallel runs show speedup of 1.13, 1.62 and 2.12 with \na few failed speculations due to the dynamic parallelism. We have studied the parallel performance as \na function of the PPR size (i.e. the number of sentences per PPR)but have to leave it out for lack of \nspace. The total number of pages being copied during commits varies between0 and 21 pages depending on \nthe size of PPR and the depth of speculation. version sequen\u00ad speculation depth tial 1 3 7 times (sec) \n11.35, 11.37 11.34 10.06, 10.06 10.07 7.03, 7.01 7.04 5.34, 5.35 5.34 speedup 1.00 1.13 1.62 2.12  \n3.4 XLisp Interpreter v1.6byD.M. Betz According to its author in 1985, Xlisp is a small implementation \nof lisp with object-oriented programming. We use the codeavailable as part of the SPEC 1995 benchmark \nsuite, which has 25 .les and 7616 lines of C code. The main function has two control loops, one for reading \nexpressions from the keyboard and the other for batch processing from a .le. By hand we mark the body \nof the batch loop as a PPR. Through the programminginterface described in Section 2.6, we identify5likelyprivatizablevariables: \nbuf for copying string constants gspre.x for generated name strings xlfsize for counting the string length \nin a print call xlsample the vestige of a deleted feature called oscheck xltrace intermediate results \nfor debugging 5checked variables: xlstack current stack pointer, restored after an evaluation xlenv \ncurrent environment, restored after an evaluation xlcontext the setjumpbuffer forexception handling xlvalue \nwould-be exception value xlplevel parenthesis nesting level, for command prompt and one reduction variable, \ngccalls, which counts the number of garbage collections.Wedo not know much about the restof the87 global \nvariables (including function pointers) except that they are all monitored by BOP. The so-parallelized \nxlisp runs .ne until a garbage collection, which changes the heap and always kills the speculation.To \nsolve the problem, we have revised the xlisp mark-sweep collector for BOP, whichwe describevery brie.y \nhere.Thekeyideaisto insu\u00adlate the effect of GC, so it can be done concurrently without caus\u00ading unnecessary \ncon.icts. Each PPR uses a separate page-aligned region.At the start (after forkingbut before data protection),a \nPPR instance runs a marking pass over the entire heap and records all reachable objects in a start list. \nDuring the PPR, it allocates new objects inside the region.Atagarbage collection,it marks just ob\u00adjects \ninside the regionbut it traverses the start list as an additional set of root pointers. It frees an object \nif it is inside the region. At the end, it performs GC again, so only the pages with live objects are \ncopied at the commit. The code changes include three new global variablesand12 statements, countedbythe \nnumberof semi-colons, for region-basedGC, mostly for collecting and traversing the start list and resetting \nthe MARK .ags in its nodes. The region-based mark-sweep has non-trivial costs at the start and the end \nof PPR. In the middle it may not be as ef.cient because it may not collect all garbage (as some nodes \nin the start list would have become unreachable in the sequential run). These costs depend on the input. \nIn addition, the regions will accumulate long\u00adlive data, which leads to morefalse alerts fromfalse sharing. \nThe evaluation may trigger an exception and an early exit from PPR, so the content of checked variables \nmay not be restored even for parallel expressions. Therefore, one cannot decide a priori whether the \nchance of parallelism and its likely bene.t would outweigh the overhead. However, these are the exact \nproblems that BOP is designed to address with the streamlined critical path and the on\u00adline sequential-parallel \nrace. To test thebop-lisp interpreter, we use an input from SPEC95, which in .ve expressions computes \nall positions of n queens on an n \u00d7 n board. When n is 9, the sequential run takes 2.36 seconds using \nthe base collector and 2.25 seconds using the region-based collector (whicheffectively hasa larger heapbut \nstill needsover 4028garbage collections for nine 10K-node regions).We modify four lines of the lisp program, \nso the problem is solved by 13 expressions,9parallel and4sequential. The dependence between sequentialexpressionsis \nautomatically detectedduetothe con.icts in the internal data structureof the interpreter.Toavoidfalse \nalerts whenexecuting the parallelexpressions, we disable the monitoring oftheheapinthe followingexperiment.We \ntest three speculation depths three times each, and the (idealized) results are: version sequen\u00ad speculation \ndepth tial 1 3 7 times (sec) 2.25, 2.27, 2.26 1.50, 1.48 1.47 .95, .94, .94 .68, .68, .68 speedup 1.00 \n1.53 2.39 3.31 The last row shows that the speedup, if we pick the lowest time from three runs, is 1.53 \nwith2processors, 2.39 with4processors, and 3.31 with 8 processors. Failed speculations account for 0.02 \nsecond of the parallel time. Compared to the other test programs, bop-lisp has a small memory footprint. \nIn the test run, it allocates 103 pages for monitored global variables, 273 pages for the heap (currentlynot \nmonitoredtoavoidthefalse alert),and111for other unmonitored data.  3.5 Comparison with Threaded Intel \nMathKernel Library The Intel Math Kernel Library 9.0 (MKL) provides highly opti\u00admized, processor-speci.c, \nand multi-threaded routines speci.cally for Intel processors. The library includes Linear Algebra Pack\u00adage \n(LAPACK) routines used for, among other things, solving sys\u00adtems of linear equations. In this experiment \nwe compare the per\u00adformance of solving eight independent systems of equations using the dgesv routine. \nMKLexploits thread-level parallelisminsidebut not across library calls. We set the number of threads \nusing the OMP NUM THREADS environment variable. BOP, on the other hand, can speculatively solve the systems \nin parallel using the se\u00adquential version of the library (by setting OMP NUM THREADS to 1). Since the \nprogram data are protected, BOP monitors inter\u00adsystem dependences (which may arise when the same array \nis passed to multiple calls) and guarantees program correctness if speculation succeeds.We compare the \nspeed of speculative paral\u00adlel invocations of unparallelized dgesv with the speed of sequential invocation \nof the hand-crafted parallel implementation. The experiment was conducted over the range of 500 to 4500, \nin increments of 500, equations per system.For each, the number of threads in the MKL-only implementation \ntested was 1, 2, 4, and 8.FortheBOPandMKL implementation,thelevelsof speculation testedwas0,1,3, and7.To \nreduce the monitoring cost, the BOP version makes a copy of the matrices and pass the unmonitored temporary \ncopies to dgesv. The memory usage includes 32 pages for globalvariables, 3920to 316K pagesfor monitored \nmatricesof coef.cients, and the same amount for their unmonitored temporary Figure 8. Solving8systemsof \nlinear equations with Intel MKL  twin. The number of copied pages during commits ranges from0 to 107. \nThe performance comparison is shown in Figure 8. Zero\u00adspeculation BOP is slightlyfaster than single-threaded \nMKL pos\u00adsibly due to the difference in memory allocation. bop-mkl depth1 and omp-mkl thread2 perform \nsimilarly, with the MKL-only im\u00adplementation achieving at most an 18% increase in operations per second \nfor 1000 equations.For bop-mkl depth3 and bop-mkl depth 7, the runtime overhead of the BOP system prevents \nspeedups for roughly 1750 and 1300 number of equations, respectively, and be\u00adlow. However, above these \nranges the course-grained parallelism provided by BOP is able to outperform the .ne-grained, thread\u00adlevel \nparallelism of the MKL library. Increases between 15% and 20% are seen for bop-mkl depth7 compared to \nomp-mkl thread8 and increases between 7% and 11% are seen for bop-mkl depth3 compared to omp-mkl thread4. \nThe comparison with threaded MKL helps to understand the overhead of process-based BOP, in particular \nits relation with the size of parallel tasks and the speculation depth. The results demon\u00adstrate the \nproperty explained in Section 2.2.1: the overhead be\u00adcomes less if the granularity is large or if the \nspeculation depth is high.For 1500 equations,3speculation processes perform 10% slower than 4-thread \nMKLbecause of the overhead. However, for the same input size, the greater parallelism from7speculation \npro\u00adcesses, more than compensates for the overhead and produces an improvementof16%over 8-threadMKL.Wehavealso \ntested BOP against another scienti.c library, the threadedATLAS, and found similar results, althoughATLAS \nis slower than MKL on our test machine. 4. Related work Parallel languages Most languages let a user \nspecify the unit of parallel execution explicitly, for example, pcall and future in Multilisp [17], parallel \nloop and section in OpenMP [27], SIMD modelof UPC[7] and Co-arrayFortran [26]. Theyrequire de.nite parallelism \nenclosed in regions or procedures with predictable en\u00adtries and exits. The annotations are binding because \ntheyaffect the program correctness. In data parallel languages such as HPF [1] and array languages such \nas the publiclyavailable Single-assignmentC (SaC) [14], the parallelismis implicitbuta user has limited \ncontrol over the partition of program computation. In comparison, PPR re\u00adgions are not bindingbut allowa \nuser to directly specify the unit of parallel execution. As such it allows computation partitioning based \non the partial information about program behavior, for ex\u00adample, a user reading part of the source code, \nor a pro.ling tool examining a few inputs. For general-purpose imperative programs, the synchronization \nis either static (e.g. in OpenMP) or dynamic based on the run\u00adtime access of the shared data. The access \ncan be speci.ed as programmed (and typed) descriptions as in Jade [30]. BOP assumes alldataare sharedbydefaultand \nuses pro.ling analysis,soitneeds little or no user speci.cation. In addition, the checked and private \ndata are suggested through non-binding hints, which allow partial and incorrect speci.cations. The disadvantage \nis the additional cost of data protection and hint checking, which BOP hides on the speculative path. \nSoftware speculation Automatic loop-level software speculation is pioneered by the lazy privatizing doall \n(LPD) test [29]. LPD has two separate phases: the marking phase executes the loop and records access \nto shared arrays in a set of shadow arrays, and the analysis phase then checks for dependence between \nanytwo itera\u00adtions. Later techniques speculatively privatize shared arrays (to al\u00adlow forfalse dependences) \nand combine the marking and checking phases(to guarantee progress)[9,11,15].Previous systemsalsoad\u00address \nissuesof parallel reduction[15,29]anddifferent strategiesof loop scheduling [9].Aweaker typeof software \nspeculationis used for disk prefetching, where only the data access of the speculation needs to be protected \n(through compiler-inserted checks) [6].We observe the prefetching effect in one of our tests, gzip. Recently \ntwo programmable systems are developed: safe fu\u00adture in Java [38] and ordered transactions in X10 [36]. \nThe .rst is designed for (type-safe) Java programs and is supported entirely in software, utilizing read/write \nbarriers of the virtual machine to monitor data access, object replication to eliminate false depen\u00addences, \nand byte-code rewriting to save program states. At the syntax level, the difference between procedure-based \nfuture and region-based PPR is super.cial but as a programming construct, they differ in two important \naspects. First, PPR supports unsafe languages with unconstrained control .ows. Second, the commit point \nis implicit in PPR but explicit in future (i.e. get). Ordered transactions rely on hardware transactional \nmemory support for ef\u00ad.ciency and correctnessbut provide pro.ling analysis to identify parallel regions.A \nuser manually classi.es program data into .ve categories. It does not use value-based checking, nor does \nthe sys\u00adtem checks the correctness of the data classi.cation. Our pro.ling analysis is similar to theirs \nin purposebut uses phase analysis to identify program-level regions. At the implementation level, the \nBOP system is unique in that the speculation overhead is proportional to the size of program data, while \nthe cost of other systems, including the transactional memory, is proportional to the frequency of data \naccess. Among software techniques, BOP is the .rst to speculatively privatize the entire address space \nand apply speculative execution beyond the traditional loop-level parallelism by address the problems \nof un\u00adpredictable code, task size, parallelism, and speculation overhead. The main appeal of programmable \nspeculation is that the spec\u00adi.cation of parallelism is purely a performance hint and has no ef\u00adfect \non program correctness. The BOP system attempts to demon\u00adstrate that this new style of parallel programming \ncan be ef.ciently supported for a more general class of applications. Hardware thread-level speculation \n(TLS) Hardware-based thread-level speculation is among the .rst to automatically exploit loop-and method-level \nparallelism in integer code. An on-line survey cited 105 related papers [20]. An early effort is described \nin [34]. The term lead thread is used in [35]. Since speculative states are buffered in hardware, the \nsize of threads is usually no more than thousands of instructions. A recent study classi.es existing \nloop-level techniques as control, data, or value speculation and shows that the maximal speedup is 12% \non average for SPEC2Kint assuming no speculation overhead and unlimited computing resources [21]. The \nlimited potential at the loop level suggests that speculation needs to be applied at larger granularity \nto fully utilize multi-processor machines. Software transactional memory Transactional memory allows \nthe programmer to identify sections of a parallel program that must be executed atomically, and which \nthe system may choose to speculativelyexecutein parallel [19].Transaction semantics, which requires a \nserializable result, is less restrictive than parallelization, which requires observational equivalence \nor the same result as the original sequential execution. Like transactions, PPR regions do not guarantee \nparallelism. Unlike transactions, PPR regions do not affect the meaning of a program. Since incorrectly \ninserted regions do not break a program, PPR is easier to use for a user or a tool to parallelize an \nunfamiliar program. At the implementation level, serializibility checking requires the monitoring of \nboth data reads and writes, so it is more costly than the run-time dependence checking. The additional \n.exibility is useful for supporting parallel reduction, but it is not strictly necessary for parallelization \nas it is for concurrencyproblems such as on-line ticket booking. Current transactional memory systems \nmonitor data accesses rather than values for con.ict detection. Run-time data monitoring For large programs \nusing complex data, per-access monitoring causes slow-downs often in integer multiples, as reported for \ndata breakpoints and on-the-.y data race detection, even after removing as manychecks as possible by \nad\u00advanced compiler analysis [25, 28, 37]. It is dif.cult for dynamic speculation to afford such slowdown \nand be practical. Run-time sampling basedondata[12]orcode[2,8] areef.cientbutdoesnot monitoring all program \naccesses. BOP uses page-based monitoring for shared data to trade precision for ef.ciency (without compro\u00admising \ncorrectness)andtoboundthecostbythesizeofdata rather than the frequencyof access. 5. Summary With programmable \ndynamic PPR regions, strong isolation dur\u00ading speculation, minimal critical path, and value-based correctness \nchecking, BOP enables parallelization based on the partialinforma\u00adtionof program behavior.Wehavebuilta \nprototype implementa\u00adtion including a parallelism analyzer, a compiler, and a run-time system and have \nparallelized a set of non-trivial applications, most of them have not been parallelized (or known to \nbe parallelizable) before this work. On a 8-CPU machine, their end-to-end speedis improvedby integerfactors. \nBOP is best suited for parallelizing large, existing code with a minimal effort. Known dependences, such \nas error handling and garbage collection, can stay in code as long as theyhappen rarely. Parallelization \ncan be done in incremental steps by removing de\u00adpendencesonebyoneas detectedbythe run-time feedbacks.Atno \npoint does a programmer need to perform parallel debugging. Acknowledgment Kai Shen suggested letting \nthe parent process exitasthewayto implementthe commit.AlokGargparticipatedin the pilot study of the BOP \nsystem. Xiao Zhang helped to collect the MKL results. The modi.ed lisp program was named in celebration \nofTuolumne Gildea.We thank Michael Scott and the anonymous refereesfor their commentsonthe earlierversionsofthe \npaper.The researchis supportedbythe National ScienceFoundation (Contract No. CNS-0509270, CCR-0238176), \nan IBM CAS Fellowship, two grants from Microsoft Research. References [1] R. Allen and K. Kennedy. Optimizing \nCompilers for Modern Architectures:ADependence-based Approach. Morgan Kaufmann Publishers, October 2001. \n[2] M. Arnold and B. G. Ryder. Aframework for reducing the cost of instrumented code. In ProceedingsoftheACM \nSIGPLAN Conference on Programming Language Design and Implementation, Snowbird, Utah, June 2001. [3] \nA. J. Bernstein. Analysis of programs for parallel processing. IEEE Transactions on Electronic Computers, \n15(5):757 763, 1966. [4] W. Blume et al. Parallel programming with polaris. IEEE Computer, 29(12):77 \n81, December 1996. [5] H.-J. Boehm. Threads cannot be implemented as a library. In Proceedings of the \nACM SIGPLAN Conference on Programming Language Design and Implementation, pages 261 268, 2005. [6]F.W.ChangandG.A. \nGibson. Automatici/ohint generationthrough speculativeexecution. In Proceedings of the Symposium on Operating \nSystems Design and Implementation, 1999. [7]W. Chen,C. Iancu, andK.Yelick. Communication optimizations \nfor .ne-grained UPC applications. In Proceedings of International Conference onParallel Architectures \nand CompilationTechniques, St. Louis, MO, 2005. [8] T. M. Chilimbi and M. Hirzel. Dynamic hot data stream \nprefetching for general-purpose programs. In Proceedingsof theACM SIGPLAN Conference on Programming Language \nDesign and Implementation, Berlin, Germany, June 2002. [9]M.H.CintraandD.R. Llanos.Designspaceexplorationofasoftware \nspeculative parallelization scheme. IEEETransactions onParallel and Distributed Systems, 16(6):562 576, \n2005. [10] R. Cytron. Doacross: Beyond vectorization for multiprocessors. In Proceedings of the 1986 \nInternational Conference on Parallel Processing, St. Charles, IL, August 1986. [11]F.Dang,H.Yu,andL. \nRauchwerger. The R-LRPD test: Speculative parallelization of partially parallel loops. Technical report, \nCS Dept., Texas A&#38;M University, College Station, TX, 2002. [12] C. Ding and K.Kennedy. Improving \ncache performance in dynamic applications through data and computation reorganization at run time. In \nProceedings of theACM SIGPLAN Conference on Programming Language Design and Implementation, Atlanta, \nGA, May 1999. [13] B. Grant, M. Philipose, M. Mock, C. Chambers, and S. J. Eggers. An evaluation of staged \nrun-time optimizations in DyC. In Proceedings oftheACM SIGPLAN ConferenceonProgramming LanguageDesign \nand Implementation, Atlanta, Georgia, May 1999. [14] C. Grelck and S.-B. Scholz. SAC from high-level \nprogramming with arraystoef.cient parallelexecution. Parallel Processing Letters, 13(3):401 412, 2003. \n[15] M. Gupta and R. Nim. Techniques for run-time parallelization of loops. In Proceedings of SC 98, \n1998. [16] M. Hall, S. Amarasinghe, B. Murphy, S. Liao, and M. Lam. Interprocedural parallelization analysis \nin SUIF. ACM Trans. Program. Lang. Syst., 27(4):662 731, 2005. [17] R. H. Halstead. Multilisp: a language \nfor concurrent symbolic computation. ACMTransactions on Programming Languages and Systems(TOPLAS), 7(4):501 \n538, 1985. [18] M. Herlihy, V. Luchangco, M. Moir, and W. N. Scherer III. Software transactional memory \nfor dynamic-sized data structures. In Proceedings of the 22th PODC, pages 92 101, Boston, MA, July 2003. \n[19] M. Herlihyand J. E. Moss. Transactional memory: Architectural support for lock-free data structures. \nIn Proceedings of the International Symposium on Co mputer Architecture, San Diego, CA, May 1993. [20] \nA. Kejariwal and A. Nicolau. Reading list of performance analysis, speculative execution. http://www.ics.uci.edu/akejariw/SpeculativeExecutionReadingList.pdf. \n[21] A.Kejariwal, X.Tian,W. Li, M. Girkar, S.Kozhukhov, H. Saito, U. Banerjee, A. Nicolau, A. V. Veidenbaum, \nand C. D. Poly\u00adchronopoulos. On the performance potential of different types of speculative thread-level \nparallelism. In ProceedingsofACM Interna\u00adtional Conference on Supercomputing, June 2006. [22]P.Keleher,A. \nCox,S.Dwarkadas, andW.Zwaenepoel.TreadMarks: Distributed shared memory on standard workstations and operating \nsystems. In Proceedings of the 1994Winter USENIX Conference, 1994. [23] K. Li. SharedVirtual Memory on \nLoosely Coupled Multiprocessors. PhD thesis, Dept.of Computer Science,Yale University,NewHaven, CT, September \n1986. [24]M.K. Martin,D.J.Sorin,H.V.Cain,M.D.Hill,andM.H. Lipasti. Correctly implementing value prediction \nin microprocessors that support multithreading or multiprocessing. In Proceedings of the International \nSymposium on Microarchitecture (MICRO-34), 2001. [25] J. Mellor-Crummey. Compile-time support for ef.cient \ndata race detection in shared memory parallel programs. Technical Report CRPC-TR92232, Rice University, \nSeptember 1992. [26] R. W. Numrich and J. K. Reid. Co-array Fortran for parallel programming. ACMFortranForum, \n17(2):1 31, August 1998. [27] OpenMP application program interface, version 2.5, May 2005. http://www.openmp.org/drupal/mp-documents/spec25.pdf. \n[28]D.PerkovicandP.J.Keleher.Aprotocol-centric approachto on-the\u00ad.y race detection. IEEETransactions \nonParallel and Distributed Systems, 11(10):1058 1072, 2000. [29] L. Rauchwerger and D. Padua. The LRPD \ntest: Speculative run-time parallelization of loops with privatization and reduction parallelization. \nIn Proceedingsof theACM SIGPLAN Conference on Programming Language Design and Implementation, La Jolla, \nCA, June 1995. [30] M. C. Rinard and M. S. Lam. The design, implementation, and evaluation of Jade. ACMTransactions \non Programming Languages and Systems(TOPLAS), 20(3):483 545, 1998. [31] X. Shen and C. Ding. Parallelization \nof utility programs based on behavior phase analysis. In Proceedings of the International Workshop on \nLanguages and Compilers for Parallel Computing, Hawthorne, NY, 2005. short paper. [32] X. Shen, C. Ding, \nS. Dwarkadas, and M. L. Scott. Characterizing phasesin service-oriented applications. Technical Report \nTR 848, Department of Computer Science, University of Rochester,November 2004. [33] X. Shen, Y. Zhong, \nand C. Ding. Locality phase prediction. In Proceedings of the Eleventh International Conference on Architect \nural Support for Programming Languages and Operating Systems (ASPLOS XI), Boston, MA, 2004. [34] G. S. \nSohi, S. E. Breach, and T. N. Vijaykumar. Multiscalar processors. In Proceedings of the International \nSymposium on Computer Architecture, 1995. [35] J.G. Steffan,C. Colohan,A. Zhai, andT.C.Mowry. TheSTAMPede \napproach to thread-level speculation. ACMTransactions on Computer Systems, 23(3):253 300, 2005. [36] \nC. von Praun, L. Ceze, and C. Cascaval. Implicit parallelism with ordered transactions. In Proceedings \nof theACM SIGPLAN Symposium on Principles Practice ofParallel Programming, March 2007. [37] R.Wahbe, \nS. Lucco, and S. L. Graham. Practical data breakpoints: design and implementation. In Proceedings of \ntheACM SIGPLAN Conference on Programming Language Design and Implementation, Albuquerque, NM, June 1993. \n[38]A.Welc,S.Jagannathan,andA.L. Hosking. Safe futuresforjava.In Proceedings of OOPSLA, pages 439 453, \n2005.   \n\t\t\t", "proc_id": "1250734", "abstract": "<p>Many sequential applications are difficult to parallelize because of unpredictable control flow, indirect data access, and input-dependent parallelism. These difficulties led us to build a software system for behavior oriented parallelization (BOP), which allows a program to be parallelized based on partial information about program behavior, for example, a user reading just part of the source code, or a profiling tool examining merely one or few executions.</p> <p>The basis of BOP is programmable software speculation, where a user or an analysis tool marks possibly parallel regions in the code, and the run-time system executes these regions speculatively. It is imperative to protect the entire address space during speculation. The main goal of the paper is to demonstrate that the general protection can be made cost effective by three novel techniques: programmable speculation, critical-path minimization, and value-based correctness checking. On a recently acquired multi-core, multi-processor PC, the BOP system reduced the end-to-end execution time by integer factors for a Lisp interpreter, a data compressor, a language parser, and a scientific library, with no change to the underlying hardware or operating system.</p>", "authors": [{"name": "Chen Ding", "author_profile_id": "81309499457", "affiliation": "University of Rochester, Rochester, NY", "person_id": "PP39066596", "email_address": "", "orcid_id": ""}, {"name": "Xipeng Shen", "author_profile_id": "81452603368", "affiliation": "College of William and Mary, Williamsburg, VA", "person_id": "PP39057352", "email_address": "", "orcid_id": ""}, {"name": "Kirk Kelsey", "author_profile_id": "81314489982", "affiliation": "University of Rochester, Rochester, NY", "person_id": "P789563", "email_address": "", "orcid_id": ""}, {"name": "Chris Tice", "author_profile_id": "81331505658", "affiliation": "University of Rochester, Rochester, NY", "person_id": "P871668", "email_address": "", "orcid_id": ""}, {"name": "Ruke Huang", "author_profile_id": "81331494494", "affiliation": "Microsoft, Redmond, WA", "person_id": "P871689", "email_address": "", "orcid_id": ""}, {"name": "Chengliang Zhang", "author_profile_id": "81409597461", "affiliation": "University of Rochester, Rochester, NY", "person_id": "PP39068786", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1250734.1250760", "year": "2007", "article_id": "1250760", "conference": "PLDI", "title": "Software behavior oriented parallelization", "url": "http://dl.acm.org/citation.cfm?id=1250760"}