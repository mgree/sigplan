{"article_publication_date": "06-10-2007", "fulltext": "\n EXOCHI: Architecture and Programming Environment for A Heterogeneous Multi-core Multithreaded System \n Perry H. Wang1, Jamison D. Collins1, Gautham N. Chinya1, Hong Jiang2,XinminTian3 Milind Girkar3,Nick \nY.Yang2, Guei-Yuan Lueh2, and Hong Wang1 Microarchitecture Research Lab, Microprocessor Technology Labs, \nIntel Corporation1 Graphics Architecture, Chipset Group, Intel Corporation2 Intel Compiler Lab, Software \nSolutions Group, Intel Corporation3 Contact: perry.wang@intel.com Abstract Future mainstream microprocessors \nwill likely integrate special\u00adized accelerators, such as GPUs, onto a single die to achieve bet\u00adter performance \nand power ef.ciency. However, it remains a keen challenge to program such a heterogeneous multi-core \nplatform, since these specialized accelerators feature ISAs and functional\u00adity that are signi.cantly \ndifferent from the general purpose CPU cores. In this paper, we present EXOCHI: (1) Exoskeleton Se\u00adquencer \n(EXO), an architecture to represent heterogeneous accel\u00aderators as ISA-based MIMD architecture resources, \nand a shared virtual memory heterogeneous multithreaded program execution model that tightly couples \nspecialized accelerator cores with gen\u00aderal purpose CPU cores, and (2) C for Heterogeneous Integration \n(CHI), an integrated C/C++ programming environment that sup\u00adports accelerator-speci.c inline assembly \nand domain-speci.c lan\u00adguages. The CHI compiler extends the OpenMP pragma for hetero\u00adgeneous multithreading \nprogramming, and produces a single fat bi\u00adnary with code sections corresponding to different instruction \nsets. The runtime can judiciously spread parallel computation across the heterogeneous cores to optimize \nperformance and power. We have prototyped the EXO architecture on a physical het\u00aderogeneous platform \nconsisting of an Intel R2 Duo pro\u00ad \u00ae CoreTM cessor and an 8-core 32-thread Intel R \u00ae Graphics Media Accelera\u00adtor \nX3000. In addition, we have implemented the CHI integrated programming environment with the Intel R \u00ae \nC++ Compiler, run\u00adtime toolset, and debugger. On the EXO prototype system, we have enhanced a suite of \nproduction-quality media kernels for video and image processing to utilize the accelerator through the \nCHI programming interface, achieving signi.cant speedup (1.41X to 10.97X) over execution on the IA32 \nCPU alone. Categories and Subject Descriptors C.1.4 [Processor Architec\u00adtures]: Parallel Architectures; \nD.3.4 [Programming Languages]: Processors Compilers General Terms Performance, Design, Languages Keywords \nHeterogeneous multi-cores, GPU, OpenMP Permission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior \nspeci.c permission and/or a fee. PLDI 07 June 11 13, 2007, San Diego, California, USA. Copyright c &#38;#169; \n2007 ACM 978-1-59593-633-2/07/0006. . . $5.00 1. Introduction The relentless pace of Moore s Law will \nlead to mainstream multi\u00adcore microprocessor designs with extensive on-die integration of a large number \nof cores [14]. Fundamentally, to scale multi-core processor designs to incorporate a large number of \ncores, ultra low EPI (Energy Per Instruction) cores are essential [10]. For example, to achieve a 20X \nimprovement (e.g., from 5GOPS to 100GOPS) in throughput performance while staying below the power envelope \nof 150W, the building-block cores must have an average EPI of approximately 1nJ. The EPI for the Intel \nR \u00ae CoreTM 2 Duo processor core [31] is approximately 10nJ while the EPI for the 8-core 32\u00adthread Intel \nR \u00ae Graphics Media Accelerator X3000 [15] is only 0.3nJ. One approach to improving EPI by an order of \nmagnitude is through heterogeneous multi-core design, in which some cores vary in functionality, instruction \nset (ISA), performance, power, and energy ef.ciency [2, 17]. The key challenge then becomes how to accomplish \nsuch heterogeneous integration and achieve high performance while still maintaining the look-and-feel \nof the classic mainstream IA32-based programming models and software ecosystem. In this paper, we present \nEXOCHI: Exoskeleton Sequencer (EXO), an architecture to represent heterogeneous accelerators as ISA-based \nMIMD architectural resources, and C for Heteroge\u00adneous Integration (CHI), a programming environment that \nsup\u00adports tightly-coupled integration of heterogeneous cores. The EXO architecture supports the familiar \nPOSIX shared virtual memory multithreaded programming model for heterogeneous cores. Like application-managed \nsequencers in the Multiple Instruction Stream Processor (MISP) architecture [11], the non-IA32 cores \nare archi\u00adtecturally exposed to the programmer as a new form of sequencer resource. They can be regarded \nessentially as application-level MIMD functional units on which user-level threads, or shreds,en\u00adcoded \nin the accelerator-speci.c ISA can execute. Having a shared virtual address space between the IA32 sequencer \nand accelerator sequencers facilitates code and data sharing and harmonizes coop\u00aderation between the \nconcurrent shreds of different ISAs. The CHI integrated programming environment allows an ap\u00adplication \ndeveloper to inline blocks of accelerator-speci.c assem\u00adbly or domain-speci.c language with traditional \nC/C++ code. The CHI compiler produces a single fat binary consisting of executable code sections corresponding \nto the different ISAs. CHI further ex\u00adtends the OpenMP pragmas [25, 26, 30] to allow the program\u00admer \nto express thread-level parallelism by demarcating parallel regions of code targeting non-IA32 accelerators. \nThe CHI exten\u00adsions to OpenMP support both fork-join and producer-consumer parallelism among the accelerator \nshreds and between the IA32 shreds and the accelerator shreds. The CHI runtime can judiciously spread \nthe shreds across the heterogeneous sequencers dynamically to maximize throughput performance while minimizing \npower. This paper makes the following contributions: We describe the EXO architecture and the CHI programming \nenvironment that support shared virtual memory multithreaded programs for a heterogeneous multi-core \nprocessor.  We detail a heterogeneous multi-core prototype of the EXO ar\u00adchitecture consisting of an \nIntel R2 Duo [31] proces\u00ad  \u00ae CoreTM sor and an 8-core 32-thread Intel R \u00ae Graphics Media Accelerator \n(GMA) X3000 [15]. We present an implementation of the CHI programming envi\u00adronment based on the Intel \nR \u00ae C++ Compiler [29] that supports the seamless integration of accelerator-speci.c assembler and domain-speci.c \nlanguages.  We report signi.cant performance gains for a set of production\u00adquality media-processing \nkernels by employing heterogeneous shreds on the GMA X3000, achieving speedups of up to 10.97X.  The \nrest of the paper is organized as follows. Section 2 reviews related work. Section 3 introduces the EXO \narchitecture and de\u00adscribes a physical prototype system using an Intel Core 2 Duo pro\u00adcessor and an Intel \nGMA X3000. Section 4 presents the CHI inte\u00adgrated programming environment and details the extensions \nto the OpenMP pragmas for heterogeneous multithreading support in the Intel C++ Compiler and the supporting \nCHI runtime software sys\u00adtem. Section 5 evaluates the performance of CHI-enabled media\u00adprocessing kernels \non the EXO hardware prototype. Section 6 con\u00adcludes.  2. Related Work There has been a rich body of \nresearch on heterogeneous acceler\u00adation. In most published work, the execution models usually fall into \ntwo categories: (1) an ISA-based tightly-coupled approach, or (2) a device driver-based loosely-coupled \nexecution model. An ex\u00adample of the tightly-coupled approach is the SCP architecture [7] in which a custom \nISA extension represents the operations imple\u00admented by a hardware accelerator attached to the CPU. The \nCPU is then responsible for sequencing, decoding and dispatching each co-processor instruction, stalling \nuntil the co-processor execution completes. This approach resembles the classic x87 escape-wait style \nco-processor instruction execution where the co-processor does not sequence instructions independently \nfrom the CPU. Examples of the second category include most known GPGPU infrastructures [9, 22]. GPGPU \nuses the massive computational power of a modern GPU, normally dedicated to render graphics op\u00aderations, \nto accelerate general purpose computation. In this line of work, depicted in Figure 1(a), the CPU resources \n(cores and mem\u00adory) are managed by the OS, and the GPU resources are separately managed by vender-supplied \ndevice drivers. Applications and de\u00advice drivers run in separate address spaces, and consequently, the \ndata communication and synchronization between them are usually carried out in coarse granularity through \nexplicit data copying via device driver APIs. In the EXOCHI framework depicted in Figure 1(b), the EXO \narchitecture supports an execution model with a shared virtual ad\u00address space and a POSIX multithreaded \nprogramming model for the OS-managed IA32 sequencer and application-managed non-IA32 accelerator sequencers. \nEXO differs from the existing tightly\u00adcoupled approaches (category 1) by allowing independent sequenc\u00ading \nand concurrent execution of multiple instruction streams on multiple sequencers within a single OS thread \ncontext. EXO also differs from the loosely-coupled, driver-based approaches (cate- (a) GPGPU (b) EXOCHI \nFigure 1. Alternate Programming Environments gory 2) by directly exposing the heterogeneous sequencers \nto appli\u00adcation programs and by supporting a shared virtual address space amongst these sequencers. Without \nrequiring an OS device driver to manage the accelerators, EXOCHI s user-level runtime can be used to \nschedule shreds and coordinate light-weight inter-shred data communication ef.ciently through shared \nvirtual memory. With direct architectural support for shared virtual memory, EXOCHI avoids the signi.cant \nburden of emulating shared memory in soft\u00adware and orchestrating DMA data transfers to provide the illusion \nof a shared memory multithreaded programming model, as with CELL [5]. Clearly, the programming models \nfor most prior research on heterogeneous acceleration have been signi.cantly in.uenced by the underlying \narchitecture. For example, different levels of GPU abstractions have been introduced in various GPGPU \nprogramming models. These abstractions range from allowing to-the-metal pro\u00adgramming in the GPU s assembly \nlanguage, to high-level language extensions with full-.edged runtime systems that abstract away any implementation \nor vendor-speci.c GPU hardware intricacy. As il\u00adlustrated in Figure 1(a), at the lowest level of abstraction, \nan infras\u00adtructure like the Data Parallel Virtual Machine (DPVM) [24] allows developers to program the \nGPU in its native assembly language by exposing the instruction set and by providing a set of special \ndevice driver APIs. However, DPVM exposes the GPU as a device that op\u00aderates in a separate address space \nand requires the programmer to explicitly manage the GPU s device-speci.c hardware. Therefore, data communication \nbetween the application program and the GPU driver code still occurs through data copying. In other approaches, \nthe GPU can also be programmed using domain-speci.c APIs, such as OpenGL or DirectX, or through a higher-level \ndomain-speci.c programming language, such as the OpenGL Shader Language [6] or Cg [19]. This approach \nprovides a higher level abstraction of the GPU by hiding both the internal ISA and the device-speci.c \nGPU hardware complexity. However, the APIs and shader language restrictions prede.ned for the graph\u00adics \nprocessing domain may become cumbersome when used to ex\u00adpress an algorithm in another domain. For example, \nin order to at\u00adtain GPU acceleration, the programmer needs to remap the data structure representation \nof the application to the native data types for OpenGL and DirectX, e.g., vertices and pixels. The program\u00admer \nalso needs to convert the algorithm into a computationally\u00adequivalent GPU operation, e.g., mapping an \ninteger sorting opera\u00adtion to a texture sampling operation. At the next level of abstraction are the \nstreaming program\u00adming models, where the GPU is abstracted as a stream processor; such an approach is \nused in Brook [1], CUDA [3], StreamIt [28], StreamC [16], SPUR [32], and Sh [20]. With this approach, \nthe pro\u00adgrammer is presented with more general purpose language syntax to support data structures like \nthose in high level languages such as C/C++. While this approach provides the programmer a much more \nproductive programming environment to produce code for the GPU hardware, the GPU still works as a device. \nAs examples of the loosely-coupled execution model (category 2), these streaming programming environments \nalso lack ef.cient integration with the traditional CPU-based programming environment and ecosystem. \nFurther up the abstraction hierarchy are domain-speci.c vir\u00adtual machines, such as Stanford Stream Virtual \nMachine [18] and RapidMind Streaming Execution Manager [21] for stream pro\u00adgramming, Microsoft Accelerator \n[27] for data parallelism explo\u00adration, and PeakStream Virtual Machine [23] for HPC accelera\u00adtion. By \nusing GPU-optimized math library APIs provided by this approach, the programmer does not need to program \nthe GPU di\u00adrectly, but can still take advantage of GPU acceleration. All these prior works treat the \nCPU and GPU as essentially sep\u00adarate, loosely-coupled entities until the highest level of abstraction \nis reached, as illustrated in Figure 1(a). EXO, in contrast, tightly couples the heterogeneous sequencers \nwith the OS-managed IA32 sequencer and can potentially provide a much leaner software run\u00adtime stack \nfor better performance, as illustrated in Figure 1(b). In addition, by supporting the shared virtual \nmemory heterogeneous multithreaded execution model, the CHI integrated programming environment facilitates \nthe application developer to inline blocks of accelerator-speci.c assembly or domain-speci.c languages \nwithin traditional C/C++ code. This allows performance-sensitive parts of an algorithm to be optimized \nfor the accelerator ISA just as Intel s SSE ISA extensions are traditionally used in implementing a high \nperformance math library. CHI s extensions to OpenMP allow pro\u00adgrammers to express the underlying thread-level \nparallelism in a familiar parallel programming environment.  3. EXO Architecture Architecturally, Exoskeleton \nSequencer (EXO) extends the MISP architecture [11] in three signi.cant ways: (1) MISP exoskeleton (2) \naddress translation remapping (ATR), and (3) collaborative exception handling (CEH). With this architectural \nsupport, EXO fundamentally enables a powerful shared virtual memory heteroge\u00adneous multithreaded programming \nmodel, despite ISA differences between the IA32 sequencer and the exo-sequencers. 3.1 MISP Exoskeleton \nEXO provides a minimal architectural wrapper , or exoskele\u00adton, to make a non-IA32 heterogeneous accelerator \nsequencer conform to the MISP inter-sequencer signaling mechanism. With this exoskeleton, the accelerator \nsequencer can be exposed as an application-managed sequencer, even though it has a different ISA from \nIA32. To distinguish from an application-managed IA32 se\u00adquencer, we call such heterogeneous accelerator \nsequencers exo\u00adsequencers. The exoskeleton supports interaction with the OS\u00admanaged IA32 sequencer through \neither initiating or responding to inter-sequencer user-level interrupts. With this enhancement, the \ncode on an OS-managed IA32 sequencer can use MISP s SIGNAL instruction to dispatch shreds of a non-IA32 \nISA to run on the exo-sequencers. This demands no additional OS support beyond MISP s requirements. \n 3.2 Address Translation Remapping To support shared virtual memory between the OS-managed IA32 sequencer \nand the exo-sequencers, EXO provides an address trans\u00adlation remapping (ATR) mechanism to allow the IA32 \nsequencer to handle page faults on behalf of the exo-sequencers. Maintaining a shared virtual address \nspace between two se\u00adquencers requires the same virtual address to be resolved to the Figure 2. ATR \nand CEH between Heterogeneous Sequencers same physical memory address on both sequencers. Among se\u00adquencers \nof the same architecture, this is accomplished by having the sequencers utilize the same page table for \naddress translation. In a heterogeneous multi-core with IA32 sequencers and non-IA32 exo-sequencers however, \nthe page table format understood by each sequencer may differ. Directly accessing the IA32 page table \nis not an option for the exo-sequencers in such cases. For example, the internal TLB of the Intel GMA \nX3000 assumes the industry stan\u00addard GPU driver-oriented page table format, which is different from the \nIA32 page table formats. Without signi.cant hardware changes, the exo-sequencers cannot use the IA32 \npage table to service TLB misses. EXO solves this problem with its ATR mechanism. With ATR, when an exo-sequencer \nincurs a translation miss, it suspends shred execution and signals the IA32 sequencer to request proxy \nexe\u00adcution [11] in order to service that TLB miss or page fault. Like MISP, upon receiving the proxy \nrequest as a user-level interrupt, the IA32 shred transfers control to a proxy handler that will touch \nthe virtual address on behalf of the exo-sequencer. Once the page fault is serviced on the IA32 sequencer, \nhowever, unlike MISP, ATR will transcode the IA32 page table entry to the format of the exo-sequencer \ns page table entry before inserting the entry to the exo-sequencer s TLB. The exo-sequencer s TLB will \npoint to the same physical page as the IA32 s TLB, and can directly access the needed data. The exo-sequencer \nthen resumes execution. As shown in Figure 2, an address translation remapping mechanism is respon\u00adsible \nfor remapping the IA32 page entry to the native format on the accelerator. The shared virtual memory \nspace for heterogeneous sequencers provides many bene.ts over the alternative approaches described in \nSection 2. It provides the essential architectural foundation to extend the classic shared memory multithreaded \nprogramming paradigm to heterogeneous multi-core processors. With a shared virtual address space, shreds \nfrom a single memory image exe\u00adcutable running on IA32 sequencers and exo-sequencers can per\u00adform data \ncommunication and synchronization in familiar and ef.\u00adcient ways, e.g., without having to resort to explicit \ndata copying as is necessary in the loosely-coupled approach. It is important to note that even though \nATR provides the nec\u00adessary architectural support for a shared virtual address space, ATR by itself does \nnot guarantee or require cache coherence between the IA32 sequencer and an exo-sequencer. In the absence \nof hardware support for cache coherence between the IA32 sequencer and an exo-sequencer, it is the responsibility \nof the programmer to use crit\u00adical sections to protect other IA32 shreds from reading or writing the \ndata being processed by shreds on the exo-sequencers. When an IA32 shred hands off a shared data structure \nto a shred on an exo-sequencer to process, the IA32 shred must .rst .ush its cache to commit any dirty \nlines to main memory. Similarly, when the exo\u00adsequencer shred completes its computation, it also needs \nto .ush its cache before releasing a semaphore to the IA32 sequencer. Clearly, with full cache coherence \nsupport between the IA32 sequencer and the exo-sequencer, the programmer s work can be Command 8 cores, \n4 hw threads/core (32 exo-sequencers) Figure 3. Intel Graphics Media Accelerator X3000 greatly eased. \nIn particular, there is no need to use critical sections to ensure mutual exclusion on reads to the shared \nworking set. This enables more concurrency between shreds on the IA32 sequencer and the exo-sequencer. \nIn Section 5.2, we provide a more quanti\u00adtative analysis of the bene.t of shared virtual memory support \nand the impact of cache coherence.  3.3 Collaborative Exception Handling As with page faults, execution \non the exo-sequencers could poten\u00adtially incur exceptions or faults that require OS services. In con\u00adventional \nMISP, if an exception occurs on an application-managed sequencer, the instruction causing the exception \ncan be replayed on the OS-managed sequencer through proxy execution. However, when the exception occurs \non a non-IA32 exo-sequencer, the fault\u00ading instruction cannot simply be replayed on the IA32 CPU se\u00adquencer. \nBecause the exo-sequencer uses a different ISA, the fault\u00ading instruction might have a data type that \nis not supported by IA32 ISA directly, or the exo-sequencer may require a different exception handling \nconvention. To address this, EXO adds hardware support for collaborative exception handling (CEH) and \na software-based exception handling mechanism, which allows faults or exceptions that occur on the exo-sequencer \nto be handled by the OS by proxy on the OS-managed IA32 sequencer. Through CEH, an exception is handled \nin a similar fashion to a TLB miss. For example, as shown in Figure 2, when a double pre\u00adcision .oating \npoint vector instruction on an exo-sequencer incurs an exception, the exo-sequencer .rst signals the \nIA32 sequencer, as it does with ATR. The IA32 sequencer then functions as the proxy for the exo-sequencer \nby invoking an application-level handler to emulate the faulting vector instruction or use an OS service \nsuch as structured exception handling (SEH) to provide full IEEE compli\u00adant handling of the exception \non the particular excepting scalar el\u00adement. Once the exception is handled on the IA32 sequencer, CEH \nensures the result is updated in the exo-sequencer before resuming execution.  3.4 EXO Hardware Prototype \nWe prototyped the EXO architecture enhancements using an In\u00adtel Santa Rosa platform [13]. The system \nconsists of an Intel Core 2 Duo [31] processor and an Intel 965G Express chipset [12], which contains \nthe integrated Intel Graphics Media Accelerator X3000 [15]. Figure 3 shows a high-level view of the GMA \nX3000 hardware. The GMA X3000 contains eight programmable, general purpose graphics media accelerator \ncores, called Execution Units (EU), each of which supports four hardware thread contexts. From the programmer \ns perspective, 32 exo-sequencers are available. We use a custom emulation .rmware that uses an IA32 CPU \ncore as the OS-managed sequencer and uses the 32 GMA X3000 sequencers as exo-sequencers. The .rmware \nimplements all essential architec\u00adtural extensions required by the EXO architecture, including MISP exoskeleton, \nATR and CEH. A shred for the GMA X3000 exo-sequencer can be created ei\u00adther by an IA32 shred or spawned \nfrom another GMA X3000 shred. Once created, GMA X3000 shreds are scheduled in a software work queue in \nshared virtual memory like POSIX threads. The work queue can have a far greater number of shreds than \nthe num\u00adber of GMA X3000 exo-sequencers. The emulation .rmware is re\u00adsponsible for translating a shred \ndescriptor, which includes shred continuation information like instruction and data pointers to the shared \nmemory, into implementation-speci.c hardware commands that the GMA X3000 exo-sequencers can consume and \nexecute. The emulation layer hides all device-speci.c hardware details from the programmer. On the GMA \nX3000, one shred can write directly to another shred s register .le to facilitate inter-shred communication. \nThis creates a producer-consumer relationship between shreds and en\u00adables the development of very sophisticated \nyet ef.cient multi\u00adthreaded algorithms. The X3000 ISA is optimized for data-and thread-level parallelism \nand each exo-sequencer supports wide SIMD operations on up to 16 data elements in parallel. The X3000 \nISA also features both specialized instructions for media process\u00ading and a full complement of control \n.ow mechanisms. The exo\u00adsequencers share access to specialized, .xed function hardware that can execute \nperformance-critical tasks, such as texture sampling and scattering/gathering memory operations. The \nfour exo-sequencers, physically implemented in each GMA X3000 core, alternate fetching through .y-weight \nswitch-on-stall multithreading. As each exo-sequencer fetches and retires instruc\u00adtions in-order, the \ncore s .ne-grained thread multiplexing capabil\u00adity plays a critical role in sustaining throughput performance. \nFor example, a stall in one exo-sequencer due to an instruction depen\u00addency or cache miss is mitigated \nby preferentially fetching from one of the other three exo-sequencers bound to the same core.  4. CHI \nProgramming Environment C for Heterogeneous Integration (CHI) is designed to provide an IA32 look-and-feel \nprogramming environment to support user\u00adlevel multi-shredding on heterogeneous sequencers. In the CHI \ninfrastructure, we enhance the Intel C++ Compiler to support accelerator-speci.c inline assembly within \nthe C/C++ source. In addition, we extend OpenMP pragmas to support heterogeneous multi-shredding, and \nprovide the related runtime support. The run\u00adtime library is responsible for judiciously scheduling heteroge\u00adneous \nshreds across the exo-sequencers. The compiler can also em\u00adbed debugging information for different ISAs \nin a single binary. Such information can be used by an enhanced version of the Intel Debugger (IDB) to \nenable source-level debugging for both C/C++ code on the IA32 CPU target and the accelerator-speci.c \ncode on the accelerator target. Figure 4 depicts the overall CHI compilation infrastructure. Three new \ncapabilities are provided in the CHI com\u00adpiler to allow programmers to express multi-shredded computation \nfor the heterogeneous exo-sequencers in the C/C++ source code: A method to specify a region of accelerator-speci.c \ncomputa\u00adtion in either inline assembly or domain-speci.c language.  A method to specify fork-join or \nproducer-consumer style shred-level parallel execution for the inline accelerator-speci.c code region \nwith OpenMP pragmas.  A method to specify input and output memory regions and live\u00adin values for the \naccelerator-speci.c code region.  4.1 Inline Accelerator Assembly Support C/C++ provides a facility \nto inline assembly code blocks directly within the high-level source code. This capability provides pro\u00adgrammers \naccess to new instructions or processor features not exposed through the compiler and allows the most \nperformance\u00adcritical parts of a program to be custom optimized in assembly. #pragma omp parallel target( \n) __asm { }  .code <call to runtime> .data .special_section <accelerator-specific binary> Figure 4. \nCHI Compilation Flow This inline assembly construct can be naturally extended to pro\u00advide accelerator-speci.c \ninline assembly support. Many variants of asm keyword and syntax exist. In CHI we adopt the Microsoft \nMASM syntax, i.e., asm {asm statements;}where brackets are used to enclose the assembly statements. asm \nis the keyword that indicates the enclosed block of code is a spe\u00adcial assembly block written speci.cally \nfor the given accelerator ISA. The asm statements enclosed in the ensuing brackets are com\u00adpiled into \nan accelerator-speci.c executable binary. The target ISA for the asm statements is speci.ed through the \nenclosing OpenMP pragma with the target clause, which is described in Section 4.2. As shown in Figure \n4, a separate accelerator-speci.c assem\u00adbler is dynamically linked with the Intel compiler. Similar to \ntra\u00additional inline assembly, this accelerator-speci.c assembler gen\u00aderates code for the target ISA by \ntranslating the inline assembly instructions enclosed in the brackets into binary code, and resolv\u00ading \nsymbolic names for memory locations and other entities refer\u00adenced within the assembly block. After the \nassembler compiles the assembly block, the resulting binary code is embedded in a spe\u00adcial code section \nof the executable indexed with a unique identi.er. The .nal executable is a fat binary, consisting of \nbinary code sec\u00adtions corresponding to different ISAs. With a similar inline com\u00adpilation mechanism, \nthe CHI compiler also supports integration of a domain-speci.c high-level language for programming the \nGMA X3000 hardware.  4.2 OpenMP Parallel Pragma Extension Traditionally, the OpenMP parallel directive \nis used to demar\u00adcate a program region for fork-join parallel thread execution. When such a construct \nis encountered, a number of threads (the thread team) is spawned to execute the dynamic extent of a parallel \nre\u00adgion. This team of threads, including the main thread that spawned them, participates in the parallel \ncomputation. At the conclusion of the parallel region, the main thread waits at an implied barrier until \nall threads in the thread team complete execution. The main thread then resumes serial execution. The \nprogrammer can use additional clauses to specify attributes for the thread team; for example, the num \nthreads clause indicates the number of threads to create. CHI extends the OpenMP parallel pragma. The \nconstruct for generating heterogeneous shreds of an accelerator-speci.c in\u00adstruction set is outlined \nin Figure 5(a). The target clause speci\u00ad.es the particular accelerator instruction set used within the \nparal\u00ad    #pragma omp parallel target(targetISA) [clause[[,]clause] ] structured-block Where clause \ncan be any of the following: firstprivate(variable-list) private(variable-list) shared(variable-ptr-list) \ndescriptor(descriptor-ptr-list) num_threads(integer-expression) master_nowait (a) Parallel specification \nin fork-join threading model #pragma intel omp taskq target(targetISA) [clause[[,]clause] ] structured-block \n Where clause can be any of the following: firstprivate(variable-list) private(variable-list) shared(variable-ptr-list) \ndescriptor(descriptor-ptr-list) num_threads(integer-expression) master_nowait (b) Queue specification \nin producer-consumer threading model #pragma intel omp task target(targetISA) [clause[[,]clause] ] structured-block \n Where clause can be any of the following: captureprivate(variable-list) shared(variable-ptr-list) descriptor(descriptor-ptr-list) \n (c) Task specification in producer-consumer threading model Figure 5. CHI Extensions to OpenMP Pragmas \n lel region. The compiler inserts appropriate calls to the CHI run\u00adtime layer to enable judicious dynamic \nshred scheduling and dis\u00adpatching onto the targeted exo-sequencers. When the main IA32 shred encounters \nan accelerator-speci.c parallel construct with the target(targetISA ) clause, the IA32 shred spawns a \nteam of num threads heterogeneous shreds for the parallel region, where each shred eventually executes \nthe enclosed assembly block on an exo-sequencer. By default, the main IA32 shred waits at the end of \nthe construct until it is noti.ed by the CHI runtime of the comple\u00adtion of all heterogeneous shreds. \nSimilar to the traditional nowait clause, an optional master nowait clause allows the main IA32 shred \nto continue execution past the construct after spawning the team of heterogeneous shreds, without having \nto wait for their completion. This allows concurrent execution on both the IA32 se\u00adquencer and its exo-sequencers. \nThe CHI runtime is responsible for asynchronously notifying the IA32 sequencer of the eventual completion \nof all heterogeneous shreds. This concurrency model presents an interesting opportunity for managing \nparallelism. For example, the programmer may use the heterogeneous shreds to pro\u00adcess two thirds of an \nimage while using the main IA32 shred to pro\u00adcess the rest of the image in parallel. Section 5.3 further \nquanti.es the bene.t of such cooperative heterogeneous multi-shredding. As in standard OpenMP, data communication \nbetween the IA32 main shred and the heterogeneous shreds can be speci.ed via data clauses, namely, shared, \nprivate,and firstprivate.In the proposed extension to the OpenMP parallel directive for hetero\u00adgeneous \nshreds, the semantics of these clauses remains identical to their respective meaning for homogeneous \nsymmetric multiproces\u00adsors, in both syntax and spirit. By de.nition, for each variable spec\u00adi.ed in the \nshared clause, all shreds in a team can access the same memory area. For each variable speci.ed in the \nfirstprivate clause, a private copy-constructed variable is created for all shreds with the same value. \nWhen the parallel for loop is used, the compiler parses the loop construct and, for the variables speci.ed \nin the private clause, each shred context is initialized with differ\u00adent copy-constructed variable values \nevaluated by each loop itera\u00adtion. When the team of shreds is launched, each shred executes the same \ncode in the pragma. CHI also introduces a new descriptor clause to allow programmer to optionally associate \nan accelerator\u00adspeci.c descriptor to a variable enumerated in the shared clause. We discuss this further \nin Section 4.4.  For accelerators highly optimized for thread-level parallelism, such as the GMA X3000, \nthe number of heterogeneous shreds launched may be quite large. For example, as shown in Table 2, a linear \n.lter algorithm for image smoothing, representative of many media .ltering operations that are embarrassingly \nparallel, can launch thousands to tens of thousands of shreds. Each sub\u00adunit (e.g. an 8x8 macroblock) \nin the input image can be indepen\u00addently processed by a separate parallel shred. Through the use of the \nparallel pragma in CHI, the starting coordinates of the mac\u00adroblock for each shred to operate on is calculated \nbased on the loop index and passed in through the private clause, while the en\u00adtire source image is shared \namong all heterogeneous shreds via the shared clause. The smoothing constants are then passed to each \nshred via the firstprivate clause. With this OpenMP extension, the programmer can use OpenMP to express \nthe algorithmic thread\u00adlevel parallelism without having to worry about implementation speci.c details \non how the compiler and runtime translate the par\u00adallel construct into shreds and how these shreds are \nscheduled to run on the exo-sequencers.  4.3 OpenMP Work-Queuing Extension The fork-join model of the \nOpenMP parallel pragma is an ideal .t for supporting parallel execution of independent threads. In order \nto support concurrent threads with intricate dynamic inter-thread dependencies (e.g., due to the use \nof irregular data structures), the Intel C++ Compiler supports irregular parallelism through two spe\u00adcial \nOpenMP pragmas taskq and task [26].InCHI,we further enhance the compiler and runtime to support inter-shred \ndependen\u00adcies among heterogeneous shreds using these pragmas. The work-queuing model supports the producer-consumer \n(or pipeline) form of thread level parallelism. It is a .exible mechanism for specifying units of work \nthat are not pre-computed at the start of the work-queuing construct. The construct is implemented by \nspecifying the environment (taskq) and the units of work (task) separately. A taskq pragma creates an \nempty queue of tasks. The code inside a taskq block is executed serially. Any task pragma construct encountered \nwhile executing a taskq block speci.es that the enclosed work is associated with the queue. A taskq pragma \nmay be nested within either a taskq block or a task block; in both cases a subordinate queue is formed. \nThe parallel taskq construct and the task construct for an exo-sequencer are outlined in Figure 5(b) \nand Figure 5(c). When an IA32 shred encounters a taskq pragma construct with the target(targetISA ) clause, \nthe IA32 shred calls to the CHI runtime to select a shred as the root shred. The root shred sequen\u00adtially \nexecutes the while or for loop within the taskq construct, and for each task pragma encountered the CHI \nruntime creates a child shred of the root shred and enqueues it into the queue asso\u00adciated with the taskq.The \ncaptureprivate clause creates a pri\u00advate copy-constructed version for each object in variable-list for \nthe task at the time the task is enqueued. While the extension to the OpenMP parallel pragma is use\u00adful \nfor exploiting large numbers of independent shreds through the fork-join model, the taskq and task work-queuing \nconstructs en\u00adsure that the producer-consumer dependency can be honored be\u00adtween GMA X3000 shreds as \nwell. An example that uses such a construct is a deblocking .lter for video processing. The recent video \ncoding standards including H.264/AVC follow the block\u00adbased hybrid coding approach, in which each picture \nis represented and processed in block-shaped units. As in block-based transfor\u00admation and quantization, \nthe block-based processing may induce more distortion in the boundaries of blocks. Deblocking .ltering \n  #1 chi alloc desc(targetISA,ptr,mode,width,height)  #2 chi free desc(targetISA,desc)  #3 chi modify \ndesc(targetISA,desc,attrib id,value)  #4 chi set feature(targetISA,feature id,value)  #5 chi set \nfeature pershred(targetISA,shr id,feature id,value)    Table 1. CHI APIs for Programming an Exo-sequencer \nof targetISA has been used to diminish this kind of block artifact. The deblock\u00ading .ltering in H.264/AVC \nis computationally intensive with com\u00adplicated data access patterns. In order to ensure the quality of \nthe .ltering, the deblocking algorithm requires macroblocks to be pro\u00adcessed in a particular order; for \nexample, a macroblock will not be processed until its left and upper neighboring macroblocks have been \ncompletely processed. Such inter-shred dependency can be easily supported by the work-queuing extension \nin CHI. 4.4 CHI Runtime Support The CHI runtime is a software library that translates the programmer\u00adspeci.ed \nOpenMP directives into primitives to create and manage shreds that can carry out the parallel execution \non the heteroge\u00adneous multi-core target. Table 1 lists the main APIs for the CHI runtime. Like conventional \nOpenMP runtimes, the CHI runtime layer provides a layer of abstraction that hides the details of man\u00adaging \nthe exo-sequencers from users of the OpenMP pragmas. Built on the EXO architecture, the runtime layer \nuses the MISP archi\u00adtectural support for user-level inter-sequencer communication and proxy execution. \nLike the Shredlib runtime for MISP [11], the CHI runtime is responsible for scheduling and dispatching \nthe hetero\u00adgeneous shreds for execution and for handling exceptions that may occur on the exo-sequencers. \nThe compiler translates CHI s OpenMP parallel pragma ex\u00adtensions to a series of calls to the runtime \nlayer, which is responsi\u00adble for appropriately con.guring the accelerator hardware for par\u00adallel execution. \nThe accelerator-speci.c assembly block is replaced with a call into a CHI runtime service that is responsible \nfor locat\u00ading the corresponding accelerator binary code in the fat binary. The CHI runtime service then \ninitiates the parallel execution of the het\u00aderogeneous shreds by dispatching shred continuations to the \nexo\u00adsequencers through the SIGNAL instruction. Another critical task for the CHI runtime is to manage \ndata communication between the IA32 and the exo-sequencers, which may have different views on the same \nshared virtual memory object speci.ed by shared data clause, as described in Section 4.2. General purpose \nCPU architectures, such as the IA32 family, view the virtual address space as a contiguous one-dimensional \nlin\u00adear uniform memory space. For such architectures a memory object (e.g., a global or local variable) \nin C/C++ can be easily bound to an operand in the inline assembly via a register move or a load instruc\u00adtion. \nHowever, domain-optimized accelerators may view memory in a signi.cantly different way than the general \npurpose CPU [8]. For instance, the GMA X3000 is optimized for 2-D image and video media processing. It \naccesses virtual memory via surfaces, which are two-dimensional blocks of memory. Con.guring surface \ninformation such as the tiling format is important for achieving the best possible performance in media \nacceleration code. Even within the same application, one surface can have signi.cantly different attributes \nfrom other surfaces. In order to allow the accelerator more ef.cient access to the C/C++ variables speci.ed \nby the shared data clause, programmers can use the CHI runtime APIs to convey accelerator-speci.c access \ninformation through data structures known as descriptors. Descrip\u00adtors are used by the accelerator to \ninterpret the attributes of the shared variables that are accessed by the shreds. The .rst three 1. A_desc \n= chi_alloc_desc(X3000, A, CHI_INPUT, n, 1); 2. B_desc = chi_alloc_desc(X3000, B, CHI_INPUT, n, 1); \n 3. C_desc = chi_alloc_desc(X3000, C, CHI_OUTPUT, n, 1); 4. #pragma omp parallel target(X3000) shared(A, \nB, C) 5. descriptor(A_desc,B_desc,C_desc) private(i) master_nowait 6. { 7. for (i=0; i<n/8; i++) \n8. __asm 9. { 10. shl.1.w vr1 = i, 3 11. ld.8.dw [vr2..vr9] = (A, vr1, 0) 12. ld.8.dw [vr10..vr17] \n= (B, vr1, 0) 13. add.8.dw [vr18..r25] = [vr2..vr9], [vr10..vr17] 14. st.8.dw (C, vr1, 0) = [vr18..vr25] \n 15. } 16. } 17. #pragma omp parallel for shared(D,E,F) private(i) 18. { 19. for (i=0; i<n; i++) \n 20. F[i] = D[i] + E[i]; 21. }  Figure 6. CHI Code Example with GMA X3000 Pseudo-code APIs listed in \nTable 1 are used for managing descriptors. API #1 allocates a descriptor by specifying the input/output \nmode, as well as the width and height of the surface. API #2 deallocates the exist\u00ading descriptor, and \nAPI #3 is a general API to modify the descriptor from its default attributes. CHI provides two generic \nruntime APIs for programmers to specify and access specialized features on accelerators. API #4 makes \na global change to all exo-sequencers states, which ap\u00adply to all heterogeneous shreds created. API #5 \nchanges an exo\u00adsequencer s state on a per shred basis. An application can directly utilize new hardware \nfeatures simply by making the appropriate call in the source .le, without requiring any changes to the \ncom\u00adpiler.  4.5 Debugging Tools In the CHI programming environment, the C/C++ compiler, inline assembler \nand domain-speci.c language compiler together produce comprehensive source-level debugging information \nthat maps each accelerator-speci.c instruction to source code. We extend the Intel Debugger (IDB) and \nthe CHI runtime layer to create a debugging environment for the application programmer. The enhanced \nversion of the Intel Debugger is capable of debugging code that is running on the IA32 sequencers as \nwell as the shreds that are running on the exo-sequencers. The debugger extensions consist of two parts. \nThe .rst part is the introduction of commands to set breakpoints, single\u00adstep, and examine state on the \nGMA X3000 exo-sequencers. The second part comprises the enhancements in the debugger and the CHI run \ntime layer so they can communicate debugging informa\u00adtion to one another. This debugger is essential \nto provide the IA32 look-and-feel in CHI for productive development of heterogeneous multi-shredded code. \n 4.6 Putting It All Together Figure 6 shows an example of C code using the extended OpenMP pragmas and \nCHI runtime APIs for a heterogeneous target consist\u00ading of an IA32 sequencer and GMA X3000 exo-sequencers. \nThe example depicts a simple addition of two vectors (A and B) with the results written to a third vector \nC. The C code (lines 1-16) uses the extended version of the OpenMP parallel pragma to perform the vector \naddition by spreading the computation to n/8 shreds, with each shred operating on eight elements in the \nvector using 8-wide SIMD instructions in the GMA X3000 ISA. In this example, the binding of C variables \nA, B, C to the inline assembly GMA3000 shred kernel code is done via the shared data clause of the parallel \npragma as speci.ed in line 4. Lines 1-3 illustrate the use of the CHI runtime API #1 to describe the \nadditional information on the surface memory area for the GMA X3000. The surfaces in this example are \ncategorized as either input or output, and described as vector arrays with width of n and height of 1. \nThe resulting descriptors for the shared variables can be communicated via the descriptor data clause \nof the parallel pragma as speci.ed in line 5. Before forking the heterogeneous shreds, the CHI runtime \ninspects these descriptors and con.gures the accelerator appropriately. The private clause speci.es the \nloop index i as an input value for each shred. The example creates n/8 GMA X3000 shreds, each executing \nthe asm block. Each shred operates on the same three input vectors, but accesses distinct regions of \nthe surfaces, depending on the per-shred input value i. The .rst line of assembly (line 10) shifts the \ninput value i left by 3 bits to calculate the index into the surface and locates the block area to perform \ncomputation (line 11\u00ad12). After the vector add is executed (line 13), the result is written to the output \nsurface (line 14). The master nowait clause allows the IA32 sequencer to continue executing the traditional \nOpenMP code (line 17-21) to perform vector add of different arrays without waiting for the pending GMA \nX3000 shreds to .nish, thus creating the parallel execution of both IA32 and GMA X3000 shreds.  5. Performance \nEvaluation The EXOCHI framework described in this paper has already been deployed within Intel for successful \ndevelopment of production\u00adquality, GMA X3000 media-processing kernels and other work\u00adloads of growing \nimportance [4]. We select a representative sub\u00adset of these kernels as benchmarks for performance evaluation. \nTa\u00adble 2 summarizes these kernels and inputs. From left to right the columns indicate the kernel name \nand abbreviation, the input data set, a brief description of the kernel, and .nally the total number \nof shreds spawned per kernel execution. These kernels exhibit a signi.cant amount of data-and thread-level \nparallelism, and thus, readily lend themselves to ef.cient execution on the GMA X3000 exo-sequencers. \nImplementation of these kernels is made easy due to special GMA X3000 ISA features optimized for media \nprocessing. The key ISA features include wide SIMD instructions, predication support, and a large register \n.le of 64 to 128 vector registers for each GMA X3000 exo-sequencer. With CHI, programmers can directly \nuse the GMA X3000 ISA features via inline assembly in C/C++ code as if they are traditional ISA extensions \nto IA32 like SSE. By providing such IA32 look-and-feel, CHI enables highly productive development of \nheterogeneous multi-shredded code. All benchmarks are compiled with the enhanced version of the Intel \nC++ Compiler using the most aggressive optimization settings ( fast Qprof use). These compiler optimizations \ninclude auto\u00advectorization, pro.le-guided optimization, and tune speci.cally for the Intel Core 2 Duo \nprocessor used in the EXO prototype system. LinearFilter, SepiaTone and FGT make use of the optimized \nand SSE-enhanced Intel IPP library, and the other benchmarks were manually tuned and SSE-optimized. Performance \nresults measure the wall clock execution time. 5.1 Performance Speedup on GMA X3000 Exo-sequencers over \nIA32 Sequencer Figure 7 shows the speedup achieved over IA32 sequencer ex\u00adecution by executing media \nkernels on the GMA X3000 exo\u00adsequencers. Signi.cant speedup is achieved, ranging from 1.41X for BOB up \nto 10.97X for Bicubic. Two factors are crucial in achieving this high throughput performance on the GMA \nX3000 exo-sequencers. Most important is the availability of abundant shred-level parallelism. As each \nGMA X3000 exo-sequencer sup\u00adports only in-order execution within a shred, the accelerator relies   \nKernel (Abbreviation) Data size Description #GMA X3000 Shreds Linear Filter 640x480 image Compute output \npixel as average of input 6,480 (LinearFilter) 2000x2000 image pixel and eight surrounding pixels 83,500 \nSepia Tone 640x480 image Modify RGB values to arti.cially age image 4,800 (SepiaTone) 2000x2000 image \n62,500 Film Grain Technology (FGT) 1024x768 image Apply arti.cial .lm grain .lter from H.264 standard \n96 Bicubic Scaling (Bicubic) Scale 30 frames 360x240 to 720x480 Scale video using bicubic .lter 2,700 \nKalman 30 frames 512x256 Video noise reduction .lter 4,096 (Kalman) 30 frames 2048x1024 65,536 Film Mode \nDetection (FMD) 60 frames 720x480 Detect video cadence so inverse telecine can be applied 1,276 Alpha \nBlending (AlphaBlend) Blend 64x32 image onto 720x480 Bi-linear scale 64x32 image up to 720x480 and blend \nwith 720x480 image 2,700 De-interlace BOB Avg (BOB) 30 frames 720x480 De-interlace video by averaging \nnearby pixels within a .eld to compute missing scanlines 2,700 Advanced De-interlacing (ADVDI) 30 frames \n720x480 Computationally intensive advanced de\u00adinterlacing .lter with motion detection 2,700 ProcAmp (ProcAmp) \n30 frames 720x480 Simple linear modi.cation to YUV values for color correction 2,700  GMA X3000 s wide \nSIMD execution bandwidth and its generous Table 2. Media-Processing Kernels  Factor Speedup over IA32 \nSequencer 12 11 10 9 8 7 6 5 4 3 2 1 Figure 7. Speedup from Execution on GMA X3000 Exo\u00adsequencers over \nIA32 Sequencer on the presence of multiple concurrent shreds to cover up stalls incurred in one shred \nby switching to another shred. A second, but related issue is the need to maximize cache hit rate and \nthe memory bandwidth utilization. The GMA X3000 supports simultaneous ex\u00adecution of 32 hardware threads, \neach of which might be reading and writing multiple data streams. The CHI runtime allows program\u00admers \nto carefully orchestrate shred scheduling to ensure shreds ac\u00adcessing adjacent or overlapping macroblocks \nare ordered closely together in the work queue so as to take advantage of spatial and temporal localities. \nOther than support for thread-level parallelism, the GMA X3000 ISA also provides strong support for data-level \nparallelism. It features signi.cantly wider SIMD operations (8 to 16-wide vec\u00adtor) than the SSE on today \ns IA32 CPU. LinearFilter, ProcAmp and SepiaTone are able to take advantage of such ISA support by aggressively \nunrolling loops. AlphaBlending bene.ts from the ability to access the texture sampler .xed function unit; \nin the absence of a texture sampler the IA32 sequencer code has to em\u00adulate this behavior in software. \nBicubic bene.ts both from the number of general purpose registers (64 to 128). The lone exception is \nBOB, which achieves a meager, albeit non\u00adtrivial, speedup of 1.41X. Compared to the other kernels studied, \nthis kernel is the least computationally intensive. Instead, it is pri\u00admarily bandwidth-bound, and bene.ts \nmuch less from the greater aggregate execution rate of the GMA X3000 than the other kernels. 5.2 Impact \nof Data Copying Versus Shared Virtual Address Space In general, the performance improvement by using \nan accelerator is determined not only by the accelerator architecture but also by the data communication \noverhead between the CPU and accelerator. This overhead varies greatly depending on the memory model \nbe\u00adtween the CPU and the accelerator. Figure 7 shows overall perfor\u00admance improvement achieved with a \ncache coherent shared virtual memory model between the IA32 sequencer and the GMA X3000 exo-sequencers. \nIn the absence of cache coherence or shared mem\u00adory, the data communication overheads can signi.cantly \ndegrade the speedup achieved by accelerating the computation. In Figure 8 we contrast performance impacts \nfor three memory model con.g\u00adurations. The .rst con.guration, Data Copy, assumes a model without shared \nvirtual memory and no cache coherence between the IA32 sequencer and the GMA X3000 exo-sequencers. Consequently, \ndata communication between IA32 shred and GMA X3000 shreds requires explicit data copying, for which \nwe assume a 3.1GB/s data copy rate. This corresponds to an aggressive data copy rate using an SSE-enhanced \nmemory copy routine when copying data from a cacheable memory source to a destination region marked as \nuncacheable, write-combining memory. The Core 2 Duo pro\u00adcessor features special write-combining buffers \nthat allow aggres\u00adsive burst mode transfers when copying from cacheable memory to write-combining memory. \nDue to the lack of shared virtual mem\u00adory, the inter-shred communication between the IA32 shred and GMA \nX3000 shreds resemble that of traditional message passing communication between processes from different \naddress spaces. The second con.guration, Non-CC Shared, assumes a shared virtual address space but without \ncache coherency between the IA32 sequencer an the GMA X3000 exo-sequencers. Data copying can be avoided \nin this case as both the IA32 sequencer and GMA Figure 8. Impact of Shared Virtual Memory X3000 exo-sequencers \ncan access the identical physical memory location for the same virtual address. Memory writes performed \nby the IA32 sequencer or the GMA X3000 exo-sequencers may not be visible to the other until after a cache \n.ush operation, which forces any dirty cache lines to be written back to main memory. However, data communication \ncan still be accomplished by passing a pointer to a shared data structure between the IA32 sequencer \nand an GMA X3000 exo-sequencer as long as cache .ush operations are appropriately invoked. Due to lack \nof cache coherence, the IA32 shred and the GMA X3000 shreds need to use critical sections to enforce \nmutually exclusive access to shared data structures. The semaphore on the critical section will not be \nreleased until the GMA X3000 exo-sequencers completely .ush the dirty lines into the memory. The third \ncon.guration, CC Shared, models a cache-coherent shared virtual address space, which is the con.guration \nassumed in Section 5.1. In this model, between the IA32 shred and the GMA X3000 shreds, the data communication \nbecomes much more ef\u00ad.cient. Similarly, the synchronization on mutual access to shared data structure \nis also made much easier to the programmers. For example, while critical sections are still necessary \nto provide mu\u00adtual exclusion on writes to a shared variable, one shred can always read the shared variables \nthat are updated by the other shreds. This allows more execution concurrency between shreds. The performance \ndata in Figure 8 demonstrate the bene.ts of a shared virtual address space compared to data copying. \nWhile sig\u00adni.cant performance improvement is still possible even with data copying, for computationally \nintensive kernels (e.g., bicubic and ADVDI), the gains are signi.cantly reduced from the original CC \nShared con.guration in cases such as LinearFilter and BOB.For benchmarks in which the GMA X3000 performs \nlittle computation on the loaded input data, the time to copy data between separate address spaces represents \na signi.cant fraction of the processing time. Even with a highly optimized implementation on the latest \nIA32 Core 2 Duo, the data copying achieves only 70.5% of that seen for a coherent shared virtual address \nspace. The cost of copying data can be ameliorated if the IA32 se\u00adquencer and the GMA X3000 exo-sequencers \noperate within a shared virtual address space, even if cache coherency is not sup\u00adported. The time required \nto .ush caches is still nontrivial, how\u00adever, and the lack of coherency (Non-CC Shared ) stillyields \n85.3% of the performance achieved with full cache coherency. Support for cache coherence improves performance \nbecause the cache .ush op\u00aderation is not needed to synchronize memory accesses. 1. n=800; 2. GMA_iters \n= 600; 3. IN_desc = chi_alloc_desc(X3000, IN, CHI_INPUT, n, 1); 4. OUT_desc = chi_alloc_desc(X3000, \nOUT, CHI_OUTPUT, n, 1); 5. #pragma omp parallel target(X3000) shared(IN, OUT) 6. descriptor(IN_desc,OUT_desc) \nprivate(i) master_nowait 7. { 8. for (i=0; i<GMA_iters; i++) 9. __asm 10. { 11. ... 12. } 13. \n} 14. #pragma omp parallel for shared(IN, OUT) private(i) 15. { 16. for (i=GMA_iters; i<n; i++) 17. \n... 18. }  Figure 9. Cooperative Execution Code Example which Executes 600 Loop Iterations on GMA X3000 \nExo-sequencers and 200 Loop Iterations on the IA32 Sequencer For the Non-CC Shared con.guration, when \nan IA32 shred spawns GMA X3000 shreds, it may appear necessary to .ush the IA32 sequencer s cache fully \nbefore any GMA X3000 shred can be launched. In reality the majority of the cache .ush operation on the \nIA32 sequencer can be overlapped with parallel shred execution on the GMA X3000 exo-sequencers if cache \n.ush operations and shred launches can be interleaved. As each exo-sequencer shred only reads and writes \na tiny portion of each data buffer (e.g.,a 16 pixel by 16 pixel macroblock), as long as that data has \nbeen .ushed back to memory by the IA32 producer shred, the exo\u00adsequencer consumer shred for that macroblock \ncan be launched and can execute safely. Additional cache .ush operations can then proceed in parallel \nwith useful work being performed in parallel on the exo-sequencers. For example, in a system where the \ncache .ush operation has not been optimized and only writes data back to memory at 2GB/s (not shown), \nexecuting LinearFilter yields a speedup of only 3.15X over IA32 sequencer execution. This occurs if the \nentire cache .ush cost for a 640x480 RGB input must be paid up front before any exo-sequencer shreds \nare spawned. However, the initial 32 exo-sequencer shreds (which fully populate the GMA X3000 exo-sequencers) \naccess less than 1% of the total input data. By .ushing just this necessary data initially, and .ushing \nthe remaining data in parallel with execution on the exo-sequencers in the system, performance very close \nto a cache-coherent shared virtual memory con.guration can be achieved without hardware support for cache \ncoherency. This intelligent cache .ushing scheme can be carried out by the CHI runtime support, transparent \nto the programmer. A shared virtual address space, with or without cache coher\u00adence, offers performance \nbene.ts by eliminating the cost of copy\u00ading data between virtual address spaces. More importantly, how\u00adever, \na shared virtual address space allows the IA32 sequencer and the GMA X3000 exo-sequencers to closely \ncooperate in the ex\u00adecution of a given workload. We discuss this approach further in Section 5.3. 5.3 \nCooperative Execution between Heterogeneous Sequencers The shared virtual address space enabled by EXO \nallows the IA32 sequencer and GMA X3000 exo-sequencers to simultaneously op\u00aderate on the same data. Because \nof this, further speedup is possible by dividing available work between the IA32 sequencer and the GMA \nX3000 sequencers. Figure 9 illustrates such an example. For each targeted ISA, the programmer provides \na separate version of the code to execute a parallel loop, and divides those loop itera\u00adtions which should \nexecute on each type of sequencer. By using the Figure 10. Cooperative Multi-shredding Between IA32 Sequencer \nand GMA X3000 Exo-sequencers  master nowait clause, cooperative heterogeneous parallel execu\u00adtion can \nbe initiated. Figure 10 shows the performance improvement when work is divided between the IA32 sequencer \nand the GMA X3000 exo\u00adsequencers using four different work partitions: (1) 0% of the work is executed \nby the IA32 sequencer, (2) a static partition where 10% of the work is executed by the IA32 sequencer, \n(3) a static partition where 25% of the work is executed by the IA32 sequencer and (4) an oracle work \npartition which optimally distributes the work so that both the IA32 sequencer and GMA X3000 exo-sequencers \n.nish execution as close to the same time as possible. The height of each bar shows the execution time \nrelative to execution on the IA32 sequencer alone, and each bar is divided according to the time when \nthe IA32 sequencer, the GMA X3000 exo-sequencers, or both are busy. This data assumes a cache-coherent \nshared virtual address space, but by carefully dividing the work among sequencers this technique can \nalso be applicable even if cache coherency is not supported. For kernels in which the IA32 sequencer \nperforms well, sig\u00adni.cant speedup is possible, and BOB, which shows the best rela\u00adtive IA32 sequencer \nperformance, achieves up to 38% for the or\u00adacle scheme. Bicubic, on the other hand, sees an improvement \nof only 8% for the oracle case. While cooperative parallel execu\u00adtion can yield further speedup in all \ncases, performance is sensitive to work imbalance. In certain cases with poor divisions of labor, e.g., \nBicubic in partition (3), the performance from cooperative execution is worse than simply executing on \nthe GMA X3000 exo\u00adsequencers. Unfortunately, determining the appropriate work divi\u00adsion apriori can be \nchallenging, and will only get more complex as the numbers and types of heterogeneous sequencers available \ncontinue to grow. This challenge can be overcome, however, by extending the CHI multi-shredding runtime \nto support a dynamic work distribution policy. To implement dynamic heterogeneous work scheduling, the \nprogrammer can provide a separate version of the code to execute an individual loop iteration for each \ntargeted ISA. At runtime, the multi-shredding runtime creates multiple candidate shred continua\u00adtions, \none for each targeted ISA, for each loop iteration. The multi\u00adshredding runtime then divides the parallel \nloop iterations among the sequencers in the system. Whenever a sequencer completes its assigned work \nit requests additional work of the runtime. Both fork-join and producer-consumer style parallelism can \nbe utilized. If cache coherency is supported, it is easy to extend the tra\u00additional shared-memory synchronization \nprimitives, such as locks and mutexes, allowing .ne-grained cooperative execution between the heterogeneous \nsequencers. In systems without cache coherency, mutual exclusion must be ensured in data accesses by \nthe IA32 se\u00adquencer and the GMA X3000 exo-sequencers. However, this can be maintained by selectively \n.ushing the necessary data prior to spawning any exo-sequencer shred to consume data produced by the \nIA32 sequencer. Dynamic work scheduling is an area of ongo\u00ading work.  6. Conclusion As long as Moore \ns Law continues, the level of on-die device in\u00adtegration will continue to grow. Thus, due to the desire \nfor de\u00adsign .exibility, the need to amortize cost across multiple uses, and the necessity to observe \npower constraints, future micropro\u00adcessor designs will bring more heterogeneous building blocks on\u00addie. \nThis level of on-die integration presents challenges as well as opportunities to both microprocessor \narchitecture and the soft\u00adware stack. In particular, this integration will inevitably bring about salient \nre-architecting of the processor hardware hierarchy, e.g., re-partitioning of power budget to introduce \nnew features, and shedding legacy interfaces to achieve ef.ciency. Re-partitioning the hardware hierarchy \ncan signi.cantly affect the software stack. On the one hand, there is a need to cope with legacy software \necosystem constraints. On the other hand, a higher degree of on\u00addie hardware integration makes it possible \nto present traditionally system-level resources as user-level architecture resources, allow\u00ading a much \ntighter coupling between applications and hardware resources. Consequently, hardware resource management \ncan be changed from centralized OS control to application-level control. Such changes can reduce the \nbloated software stack between appli\u00adcation programs and silicon. In this paper we present the EXO MIMD \nextension to the IA32 ISA to expose heterogeneous cores as application-level architecture resources and \nprovide shared virtual memory to support the classic multi-shredded programming model for the heterogeneous \nmulti\u00adcore. The EXO architecture allows application programs to directly use heterogeneous hardware as \nMIMD functional units while re\u00adquiring minimal additional dependency on the existing OS ecosys\u00adtem. In \naddition, in order to take advantage of the rich ecosystem legacy for IA32 software development, the \nCHI programming en\u00advironment provides an IA32 look-and-feel by extending the Intel C++ Compiler, OpenMP \nruntime and debugger toolchains to sup\u00adport user-level heterogeneous multi-shredding. Since its develop\u00adment, \nEXOCHI has been used in Intel s production media kernel development. Based on extensive feedback from \ndevelopers, there is strong evidence that the IA32 look-and-feel of the programming environment has signi.cantly \nimproved productivity over prior de\u00advice driver-based development environments.  Acknowledgments We \nwould like to thank John Shen, Rich Hankins, Lisa Pearce, Porus Khajotia, Prasoonkumar Surti, Bob Dreyer, \nSang-hee Lee, Katen Shah, Mike Dwyer, Yi-jen Chiu, Lian Tang, Igor Kozint\u00adsev, Xintian Wu, Bevin Brett, \nSusan Macchia, Ping Liu, Nenad Ukropina, Todd Schwartz, Jenny Nieh, David Sehr, Wei Li and San\u00adjiv Shah \nfor the productive collaboration throughout the EXOCHI project. We also appreciate the support from Shekhar \nBorkar, Joe Schutz, Tom Piazza, Justin Rattner, Jim Held, Steve Pawlowski, Kevin J. Smith, Bill Savage, \nKetan Paranjape, Raj Hazra, Alan Crouch, Bryant Bigbee, Wilf Pinfold, Dave Shinsel, Ajay Bhatt, Doug \nCarmean, Per Hammarlund, and Dion Rodgers. In addition, we would like to thank Anne Bracy, Ethan Schuchman, \nAnkur Khandelwal, and the anonymous reviewers whose valuable feed\u00adback has helped the authors greatly \nimprove the quality of this pa\u00adper. References [1] I. Buck, T. Foley, D. Horn, J. Sugerman, K. Fatahalian, \nM. Houston, and P. Hanrahan. Brook for GPUs: Stream Computing on Graphics Hardware. ACM Transactions \non Graphics, 23(3):777 786, 2004. [2] CPU+GPU integration. http://www.google.com/search?hl=en&#38;lr=&#38; \nrls=GGLG%2CGGLG%2005-47%2CGGLG%3Aen&#38;q=intel+amd+ nvidia+ati+cpu+gp%u+integrated+&#38;btnG=Search. \n[3] CUDA. http://developer.nvidia.com/object/cuda.html. [4] P. Dubey. Recognition, Mining and Synthesis \nMoves Computers to the Era of Tera. Technology@Intel Magazine, February 2005. [5] A. Eichenberger, K. \nO Brien, K. O Brien, P. Wu, T. Chen, P. Oden, D. Prener, J. Shepherd, B. So, Z. Sura, A. Wang, T. Zhang, \nP. Zhao, and  M. Gschwind. Optimizing Compiler for the CELL Processor. In Pro\u00adceedings of the 14th international \nConference on Parallel Architectures and Compilation Techniques, 2005. [6] GLSL OpenGL Shading Language. \nwww.wikipedia.org/wiki/GLSL. [7] R. Gonzalez. A Software-con.gurable Processor Architecture. IEEE Micro, \npages 42 51, Sept-Oct 2006. [8] N. Govindaraju, S. Larsen, J. Gray, and D. Manocha. A Memory Model for \nScienti.c Algorithms on Graphics Processor. In IEEE Supercomput\u00ading, 2006. [9] GPGPU: General Purpose \nComputation using Graphics Hardware. www.gpgpu.org. [10] E. Grochowski and M. Annavaram. Energy per Instruction \nTrends in Intel Microprocessors. Technology@Intel Magazine, March 2006. [11] R. Hankins, G. Chinya, J. \nCollins, P. Wang, R. Rakvic, H. Wang, and J. Shen. Multiple Instruction Stream Processor. In Proceedings \nof the 33rd International Symposium on Computer Architecture, June 2006. [12] Intel G965 Express Chipset. \nhttp://www.intel.com/products/chipsets/ g965/prod brief.pdf. [13] Intel Santa Rosa Platform. http://www.intel.com/pressroom/archive/ \nreleases/20060307corp b.htm. [14] Tera-scale Research Prototype: Connecting 80 Simple Sores on a Single \nTest Chip. ftp://download.intel.com/research/platform/terascale/ tera-scaleresearchprototypebackgrounder.pdf. \n[15] Intels Next Generation Integrated Graphics Architecture Intel Graph\u00adics Media Accelerator X3000 \nand 3000. Intel Corporation, 2006. [16] U. Kapasi, S. Rixner, W. Dally, B. Khailany, J. Ahn, P. Mattson, \nand J. Owens. Programmable Stream Processors. IEEE Computer, 2003.  [17] R. Kumar, D. Tullsen, P. Ranganathan, \nN. Jouppi, and K. Farkas. Single-ISA Heterogeneous Multi-Core Architectures for Multithreaded Workload \nPerformance. In Proceedings of the 31st International Sym\u00adposium on Computer Architecture, June 2004. \n  [18] F. Labonte, P. Mattson, W. Thies, I. Buck, C. Kozyrakis, and M. Horowitz. The Stream Virtual \nMachine. In Proceedings of the 13th International Conference on Parallel Architectures and Compila\u00adtion \nTechniques, 2004. [19] W. Mark, R. Glanville, K. Akeley, and M. Kilgard. Cg: A System for Programming \nGraphics Hardware in a C-like Language. ACM Transac\u00adtions on Graphics, (3):896 907, 2003. [20] M. McCool \nand S. Toit. Metaprogramming GPUs with Sh. A K Peters, 2004. [21] M. McCool, K. Wadleigh, B. Henderson, \nand H. Y. Lin. Performance Evaluation of GPUs using the RapidMind Development Platform. In Proceedings \nof the 20th International Conference on Supercomputing, 2006. [22] J. Owens, D. Luebke, N. Govindaraju, \nM. Harris, J. Kr\u00a8uger, A. Lefohn, and T. Purcell. A Survey of General-Purpose Computation on Graphics \nHardware. In Eurographics, August 2005. [23] The PeakStream Platform: High Productivity Software Development \nfor Multi-core Processors. PeakStream Inc, 2006. [24] M. Segal and M. Peercy. A Performance-Oriented \nData Parallel Virtual Machine for GPUs. In SIGGRAPH, 2006. [25] S. Shah, G. Haab, P. Petersen, and J. \nThroop. Flexible control struc\u00adtures for parallelism in OpenMP. In First European Workshop on OpenMP, \nSeptember 1999. [26] E. Su, X. Tian, M. Girkar, G. Haab, S. Shah, and P. Petersen. Compiler Support of \nthe Workqueuing Execution Model for Intel SMP Architec\u00adtures. In Proceedings of the 4th European Workshop \non OpenMP, 2002. [27] D. Tarditi, S. Puri, and J. Oglesby. Accelerator: Using Data Parallelism to Program \nGPUs for General-Purpose Uses. In Proceedings of the 12th International Conference on Architectural Support \nfor Programming Languages and Operating Systems, October 2006. [28] W. Thies, M. Karczmarek, and S. Amarasinghe. \nStreamIt: A Language for Streaming Applications. In Computational Complexity, 2002. [29] X. Tian, A. \nBik, M. Girkar, P. Grey, H. Saito, and E. Su. Intel OpenMP C++/Fortran Compiler for Hyper-Threading Technology: \nImplementa\u00adtion and Performance. Intel Technology Journal, Q1 2002. [30] X. Tian, M. Girkar, S. Shah, \nD. Armstrong, E. Su, and P. Petersen. Compiler and Runtime Support for Running OpenMP Programs on Pen\u00adtium \nand Itanium Architectures. In Proceedings of the 17th International Symposium on Parallel and Distributed \nProcessing, April 2003. [31] O. Wechsler. Inside Intel Core Microarchitecture: Setting New Stan\u00addards \nfor Energy-ef.cient Performance. Technology@Intel Magazine, 2006. [32] D. Zhang, Z. Li, H. Song, and \nL. Liu. A Programming Model for an Embedded Media Processing Architecture. In Embedded Computer Systems: \nArchitecture, Modeling, and Simulation, 2005. \n\t\t\t", "proc_id": "1250734", "abstract": "<p>Future mainstream microprocessors will likely integrate specialized accelerators, such as GPUs, onto a single die to achieve better performance and power efficiency. However, it remains a keen challenge to program such a heterogeneous multicore platform, since these specialized accelerators feature ISAs and functionality that are significantly different from the general purpose CPU cores. In this paper, we present EXOCHI: (1) <i>Exoskeleton Sequencer</i>(EXO), an architecture to represent heterogeneous acceleratorsas ISA-based MIMD architecture resources, and a shared virtual memory heterogeneous multithreaded program execution model that tightly couples specialized accelerator cores with generalpurpose CPU cores, and (2) <i>C for Heterogeneous Integration</i>(CHI), an integrated C/C++ programming environment that supports accelerator-specific inline assembly and domain-specific languages. The CHI compiler extends the OpenMP pragma for heterogeneous multithreading programming, and produces a single fat binary with code sections corresponding to different instruction sets. The runtime can judiciously spread parallel computation across the heterogeneous cores to optimize performance and power.</p> <p>We have prototyped the EXO architecture on a physical heterogeneous platform consisting of an Intel&#174; Core&#8482; 2 Duo processor and an 8-core 32-thread Intel&#174; Graphics Media Accelerator X3000. In addition, we have implemented the CHI integrated programming environment with the Intel&#174; C++ Compiler, runtime toolset, and debugger. On the EXO prototype system, we have enhanced a suite of production-quality media kernels for video and image processing to utilize the accelerator through the CHI programming interface, achieving significant speedup (1.41X to10.97X) over execution on the IA32 CPU alone.</p>", "authors": [{"name": "Perry H. Wang", "author_profile_id": "81451596193", "affiliation": "Intel, Santa Clara, CA", "person_id": "P348276", "email_address": "", "orcid_id": ""}, {"name": "Jamison D. Collins", "author_profile_id": "81100445511", "affiliation": "Intel, Santa Clara, CA", "person_id": "PP39043095", "email_address": "", "orcid_id": ""}, {"name": "Gautham N. Chinya", "author_profile_id": "81314481867", "affiliation": "Intel, Hillsboro, OR", "person_id": "P787683", "email_address": "", "orcid_id": ""}, {"name": "Hong Jiang", "author_profile_id": "81381604596", "affiliation": "Intel, Folsom, CA", "person_id": "PP40034247", "email_address": "", "orcid_id": ""}, {"name": "Xinmin Tian", "author_profile_id": "81100106742", "affiliation": "Intel, Santa Clara, CA", "person_id": "P302299", "email_address": "", "orcid_id": ""}, {"name": "Milind Girkar", "author_profile_id": "81100144792", "affiliation": "Intel, Santa Clara, CA", "person_id": "P201003", "email_address": "", "orcid_id": ""}, {"name": "Nick Y. Yang", "author_profile_id": "81539079556", "affiliation": "Intel, Folsom, CA", "person_id": "PP33034499", "email_address": "", "orcid_id": ""}, {"name": "Guei-Yuan Lueh", "author_profile_id": "81448594033", "affiliation": "Intel, Santa Clara, CA", "person_id": "PP94030244", "email_address": "", "orcid_id": ""}, {"name": "Hong Wang", "author_profile_id": "81350588664", "affiliation": "Intel, Santa Clara, CA", "person_id": "PP39033138", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1250734.1250753", "year": "2007", "article_id": "1250753", "conference": "PLDI", "title": "EXOCHI: architecture and programming environment for a heterogeneous multi-core multithreaded system", "url": "http://dl.acm.org/citation.cfm?id=1250753"}