{"article_publication_date": "06-10-2007", "fulltext": "\n Reliable and Ef.cient Programming Abstractions for Wireless Sensor Networks Nupur Kothari* Ramakrishna \nGummadi * Todd Millstein Ramesh Govindan University of Southern California University of California, \nLos Angeles University of Southern California {nkothari, gummadi}@usc.edu todd@cs.ucla.edu ramesh@usc.edu \n Abstract It is currently dif.cult to build practical and reliable programming systems out of distributed \nand resource-constrained sensor devices. The state of the art in today s sensornet programming is centered \naround a component-based language called nesC. nesC is a node\u00adlevel language a program is written for \nan individual node in the network and nesC programs use the services of an operating sys\u00adtem called TinyOS. \nWe are pursuing an approach to programming sensor networks that signi.cantly raises the level of abstraction \nover this practice. The critical change is one of perspective: rather than writing programs from the \npoint of view of an individual node, programmers implement a central program that conceptually has access \nto the entire network. This approach pushes to the compiler the task of producing node-level programs \nthat implement the de\u00adsired behavior. We present the Pleiades programming language, its compiler, and \nits runtime. The Pleiades language extends the C language with constructs that allow programmers to name \nand access node-local state within the network and to specify simple forms of concur\u00adrent execution. \nThe compiler and runtime system cooperate to im\u00adplement Pleiades programs ef.ciently and reliably. First, \nthe com\u00adpiler employs a novel program analysis to translate Pleiades pro\u00adgrams into message-ef.cient \nunits of work implemented in nesC. The Pleiades runtime system orchestrates execution of these units, \nusing TinyOS services, across a network of sensor nodes. Sec\u00adond, the compiler and runtime system employ \nnovel locking, dead\u00adlock detection, and deadlock recovery algorithms that guarantee serializability in \nthe face of concurrent execution. We illustrate the readability, reliability and ef.ciency bene.ts of \nthe Pleiades language through detailed experiments, and demonstrate that the Pleiades implementation \nof a realistic application performs similar to a hand-coded nesC version that contains more than ten \ntimes as much code. Categories and Subject Descriptors D.1.3 [Programming Tech\u00adniques]: Concurrent Programming \nDistributed programming; D.3.2 [Programming Languages]: Language Classi.cation Specialized application \nlanguages * Primary authors. http://kairos.usc.edu This material is based in part upon work supported \nby the National Science Foundation under Grant Nos. 0520299, 0121778, 0427202 and 0545850. Any opinions, \n.ndings, and conclusions or recommendations expressed in this material are those of the author(s) and \ndo not necessarily re.ect the views of the National Science Foundation. Permission to make digital or \nhard copies of all or part of this work for personal or classroom use is granted without fee provided \nthat copies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. PLDI 07 June 11 13, 2007, San Diego, California, \nUSA. Copyright c . 2007 ACM 978-1-59593-633-2/07/0006. . . $5.00 General Terms Performance, Design, Languages, \nReliability, Experimentation Keywords Wireless Sensor Networks, Macroprogramming, En\u00adergy Ef.ciency, \nSerializability, Deadlocks 1. Introduction Wireless sensor networks consist of a system of distributed \nsen\u00adsors embedded in the physical world. They are increasingly used in scienti.c and commercial applications \n[15, 28]. However, con\u00ad structing practical and reliable wirelessly-networked systems out of them is \nstill a signi.cant challenge. This is because the programmer must cope with severe resource, bandwidth, \nand power constraints on the sensor nodes as well as the challenges of distributed sys\u00adtems, such as \nthe need to maintain consistency and synchronization among numerous, asynchronous loosely coupled nodes. \nCurrent practice in sensor network programming uses a a highly concurrent dialect of C called nesC [5], \nwhich is a node-level lan\u00adguage a nesC program is written for an individual node in the network. nesC \nstatically detects potential race conditions and op\u00adtimizes hardware resources using whole-program analysis. \nnesC programs use the services of the TinyOS operating system [11], which provides basic runtime support \nfor statically linked pro\u00adgrams. TinyOS exposes an event-driven execution and scheduling model and provides \na library of reusable low-level components that encapsulate widely used functionality, such as timers \nand ra\u00addios. TinyOS was designed for ef.cient execution on low-power, limited-memory sensor nodes called \nmotes. nesC and TinyOS provide abstractions and libraries that sim\u00adplify node-level sensor-network application \nprogramming, but en\u00adsuring the ef.ciency and reliability of sensor network applications is still tedious \nand error prone (Section 2). For example, the pro\u00ad grammer must manually decompose a high-level distributed \nalgo\u00adrithm into programs for each individual sensor node, must ensure that these programs ef.ciently \ncommunicate with one another, must implement any necessary data consistency and control-.ow syn\u00adchronization \nprotocols among these node-level programs, and must explicitly manage resources at each node. We are \npursuing an alternative approach to programming sensor networks that signi.cantly raises the level of \nabstraction over cur\u00adrent practice. The critical change is one of perspective: rather than writing programs \nfrom the point of view of an individual node in the network, programmers implement a central program \nthat con\u00adceptually has access to the entire network. This change allows a programmer to focus attention \non the higher-level algorithmics of an application, and the compiler automatically generates the node\u00adlevel \nprograms that properly and ef.ciently implement the appli\u00adcation on the network. In the literature, this \nstyle of programming sensor networks is known as macroprogramming [29]. We have instantiated our macroprogramming \napproach in the context of a modest extension to C called Pleiades, which aug\u00adments C with constructs \nfor addressing the nodes in a network and accessing local state from individual nodes. These features \nallow programmers to naturally express the global intent of their sensor\u00adnetwork programs without worrying \nabout the low-level details of inter-node communication and node-level resource management. By default, \na Pleiades program is de.ned to have a sequential thread of control, which provides a simple semantics \nfor program\u00admers to understand and reason about. However, Pleiades includes a novel language construct \nfor parallel iteration called cfor,which can be used, for example, to iterate concurrently over all the \nnodes in the network or all one-hop neighbors of a particular node. The Pleiades compiler translates \nPleiades programs into node\u00adlevel nesC programs that can be directly linked with standard TinyOS components \nand the Pleiades runtime system and exe\u00adcuted over a network of sensor motes. The key technical challenge \nfor Pleiades is the need to automatically implement high-level cen\u00adtralized programs in an ef.cient and \nreliable manner on the nodes in the network. The Pleiades compiler and runtime system coop\u00aderate to meet \nthis challenge in a practical manner (Section 3). This paper makes the following contributions: Automatic \nprogram partitioning and migration for minimiz\u00ading energy consumption. Energy ef.ciency is of primary \ncon\u00adcern for sensor nodes because they are typically battery-powered. Wireless communication consumes \nsigni.cant battery energy, and so it is critical to minimize communication costs among nodes. Pleiades \nuses a novel combination of static and dynamic information in order to determine at which node to execute \neach statement of a Pleiades program. A compile-time analysis .rst partitions a program s statements \ninto nodecuts, each represent\u00ading a unit of work to be executed on a single node. The runtime system \nthen uses knowledge of the actual nodes involved in a nodecut s computation to determine at which node \nit should be executed in order to minimize the communication overhead.  An easy-to-use and reliable \nconcurrency primitive. Concur\u00adrent execution is a natural component of sensor network applica\u00adtions, \nsince each sensor node can execute code in parallel. How\u00adever, with concurrency comes the potential for \nsubtle errors in synchronization that can affect application reliability. To sup\u00adport concurrency while \nensuring reliability, the Pleiades runtime system guarantees serializability for each cfor: the effect \nof a cfor loop always corresponds to some sequential execution of the loop. To achieve this semantics, \nthe runtime system automat\u00adically synchronizes access to variables among cfor iterations via locks, alleviating \nthe programmer of this burden. Locking has the potential to cause deadlocks, so the compiler and runtime \nsystem also support a novel distributed deadlock detection and recovery algorithm for cfors.  A mote-based \nimplementation and its evaluation. We have implemented Pleiades on the widely used, but highly memory\u00adconstrained, \nmote platform. The motes we use have 10kB RAM for program variables and 48kB ROM for compiled code. Our \nimplementation generates event-driven node-level nesC code that is conceptually similar to what a programmer \nwould manually write today. We evaluate three applications belonging to three different classes (Section \n4). We .rst compare the performance of a sophisticated pursuit-evasion game macroprogram with that of \na hand-coded nesC version written by others [7]. We .nd that the Pleiades program is signi.cantly more \ncompact (the source code size less than 10% as large), well-structured, and easy to understand. At the \nsame time, the Pleiades implementation has comparable performance with the native nesC implementation. \nWe then evaluate a car parking application that requires a strict  notion of consistency and show that \nthe Pleiades implementation of the concurrent execution is reliable. We .nally demonstrate the utility \nof control .ow migration within a simple network information gathering example. Researchers have previously \nexplored abstractions for program\u00adming sensor networks in the aggregate [8, 25, 29], as well as intermediate \nprogram representations to support compilation of such programs [23]. However, to our knowledge, a self-contained \nmacroprogramming system for motes one that generates the com\u00adplete code necessary for stand-alone execution \nhas not previously been explored or reported on. Pleiades is also related to research on parallel and \ndistributed systems. Unlike traditional parallel sys\u00adtems and research on automatic parallelization, \nwe are primar\u00adily interested in achieving high task-level parallelism rather than data parallelism, given \nthe loosely coupled and asynchronous na\u00adture of sensor networks. Further, we target concurrency support \ntoward minimizing energy consumption rather than latency, since sensor networks are primarily power constrained. \nUnlike traditional distributed systems, Pleiades features a centralized programming model and pushes \nthe burden of concurrency control and synchro\u00adnization to the compiler and runtime. A more detailed comparison \nwith related work is presented in Section 5.  2. The Pleiades Language 2.1 Design Rationale Pleiades \nis designed to provide a simple programming model that addresses the challenges and requirements of sensor \nnetwork pro\u00adgramming. Pleiades sequential semantics makes programs easy to understand and is natural \nwhen programming sensor networks in a centralized fashion. Concurrency is introduced in a simple manner \nappropriate to the domain, via the cfor construct for node itera\u00adtion. At the same time, the sequential \nsemantics is still appropriate for the purpose of programmer understanding, because Pleiades ensures \nserializability of cfors. This strong form of consistency and reliability is important for a growing \nclass of sensor network applications, like car parking and the part of an application respon\u00adsible for \nbuilding a routing tree across the nodes. For these kinds of applications, we argue that Pleiades s sequential \nsemantics is the right one. We have also used Pleiades for applications such as rout\u00ading, localization, \ntime synchronization and data collection, which require consistency for at least some program variables. \nTo our knowledge, no other macroprogramming system guarantees even weak forms of consistency. While Pleiades \nprovides a sequential semantics, it nonetheless ef.ciently and naturally supports event-driven execution. \nPleiades has special language support for sensors and timers that provides a synchronous abstraction \nfor event-driven execution. The syn\u00adchronous semantics is easy for programmers to understand and .ts \nwell with the sequential nature of a Pleiades program. Under the covers, the language constructs are \ncompiled to ef.cient event\u00addriven nesC code.  2.2 Parking Cars with Pleiades We illustrate the language \nfeatures of Pleiades and the bene.ts they provide over node-level nesC programs through a small but realistic \nexample application. It involves low cost wireless sensors that are deployed on streets in a city to \nhelp drivers .nd a free space. (According to recent surveys [27], searching for a free parking spot already \naccounts for up to 45% of vehicular traf.c in some metropolitan areas.) Each space on the street has \nan associated sensor node that maintains the space s status (free or occupied). The goal is to identify \na sensor node with a free spot that is as close to the desired destination of the driver as possible. \nFor ease of explanation, we de.ne distance by hop count in the network, but 1: #include \"pleiades.h\" \n 2: boolean nodelocal isfree=TRUE; 3: nodeset nodelocal neighbors; 4: node nodelocal neighborIter; \n 5: void reserve(pos dst) { 6: boolean reserved=FALSE; 7: node nodeIter,reservedNode=NULL; 8: node \nn=closest_node(dst); 9: nodeset loose nToExamine=add_node(n, empty_nodeset()); 10: nodeset loose nExamined=empty_nodeset(); \n 11: if(isfree@n) { 12: reserved=TRUE; reservedNode=n; 13: isfree@n=FALSE; 14: return; 15: } 16: \nwhile(!reserved &#38;&#38; !empty(nToExamine)){ 17: cfor(nodeIter=get_first(nToExamine);nodeIter!=NULL; \nnodeIter = get_next(nToExamine)){ 18: neighbors@nodeIter=get_neighbors(nodeIter);  19: for(neighborIter@nodeIter=get_first(neighbors@nodeIter); \nneighborIter@nodeIter!=NULL; neighborIter@nodeIter=get_next(neighbors@nodeIter)){ 20: if(!member(neighborIter@nodeIter,nExamined)) \n 21: add_node(neighborIter@nodeIter,nToExamine); 22: } 23: if(isfree@nodeIter){ 24: if(!reserved){ \n 25: reserved=TRUE; reservedNode=nodeIter; 26: isfree@nodeIter=FALSE; 27: break; 28: } 29: } 30: \nremove_node(nodeIter,nToExamine); 31: add_node(nodeIter,nExamined); 32: } 33: } 34:} Figure 1. A \nstreet-parking application in Pleiades. it is straightforward to base this on physical distance. We consider \nan implementation of this application in Pleiades as well as two node-level versions written in nesC \n[5]. We show that the Pleiades version is simultaneously readable, reliable, and ef.cient. Each of the \ntwo nesC versions is more complex and provides reliability or ef.ciency, but not both simultaneously. \nFigure 1 shows the key procedure that makes up a version of the street-parking application written in \nPleiades. When a car arrives near the deployed area, a space near the driver s indicated desti\u00adnation \nis found and reserved for it by invoking reserve, passing the car s desired location. The reserve procedure \n.nds the closest sensor node to the desired destination and checks if its space is free. If so, the space \nis reserved for the car. If not, the node s neighbors are recursively and concurrently checked. The code \nin Figure 1 makes critical use of Pleiades s centralized view of a sensor network. We describe the associated \nlanguage constructs in turn. Node Naming. Pleiades provides a set of language constructs that allow programmers \nto easily access nodes and node-local state in a high-level, centralized, and topology-independent manner. \nThe node type provides an abstraction of a single network node, and the nodeset type provides an iterator \nabstraction for an unordered collection of nodes. For example, variable n (line 8) in reserve holds the \nnode that is closest to the desired position (the code for the closest node function is not shown), and \nnToExamine (line 9) maintains the set of nodes that should be checked to see if the associated space \nis free. The set of currently available nodes in the network is returned by invoking get network nodes(), \nwhich returns a nodeset. Pleiades also provides a get neighbors(n) procedure that re\u00adturns a nodeset \ncontaining n s current one-hop radio neighbors. In Figure 1, the reserve procedure uses get neighbors \n(line 18) to add an examined node s neighbors to the nToExamine set. The Pleiades runtime implements \nget neighbors by maintaining a set of sensor nodes that are reachable through wireless broadcast. Node-Local \nVariables. Pleiades extends standard C variable naming to address node-local state. This facility allows \nprogram\u00admers to naturally express distributed computations and eliminates the need for programmers to \nmanually implement inter-node data access and communication. Node-local variables are declared as ordinary \nC variables but include the attribute nodelocal,asshown for the isfree variable (line 2) in Figure 1. \nThe attribute indicates that there is one version of the variable per node in the network. A node-local \nvariable is addressed inside a Pleiades program using a new expression var@e,where var is a nodelocal \nvariable and e is an expression of type node. For example, the reserve procedure uses this syntax to \ncheck if each node in nToExamine is free (line 23). An expression of the form var@e can appear anywhere \nthat a C l-value can appear; in particular, a node-local variable can be updated through assignment. \nAll variables not annotated as nodelocal are treated as ordi\u00adnary C variables, whose scope and lifetime \nrespect C s standard se\u00admantics. In Pleiades, we call these central variables, to distinguish them from \nnode-local variables. In our example code, reserved is a central variable (line 6), which is therefore \nshared across all nodes in the network. Concurrency. By default, a Pleiades program has a sequential \nexecution semantics. However, Pleiades also provides a simple form of programmer-directed concurrency. \nThe cfor loop is like an ordinary for loop but allows for concurrent execution of the loop s iterations. \nA cfor loop can iterate over any nodeset,and the loop body will be executed concurrently for each node \nin the set. For example, the reserve procedure in Figure 1 concurrently iterates over the nodes in nToExamine \n(line 17), in order to check if any of these nodes is free. While concurrency is often essential to achieve \ngood perfor\u00admance, it can cause subtle errors that are dif.cult to understand and debug. For example, \na purely concurrent semantics of the cfor in reserve can easily cause multiple free nodes to read a value \nof false for the reserved .ag. This will have the effect of mak\u00ading each such node believe that it has \nbeen selected for the new car and is therefore no longer free. To help programmers obtain the bene.ts \nof concurrency while maintaining reliability, the Pleiades compiler and runtime system ensure that the \nexecution of a cfor is always serializable: the effect of a cfor always corresponds to some sequential \nexecution of the loop. In reserve, serializability ensures that only one free node will reserve itself \nfor the new car; the other free nodes will see the updated value of the reserved .ag at that point. Section \n3.2 explains our algorithm for ensuring serializability for cfor loops. Pleiades allows cfors to be arbitrarily \nnested. The serializabil\u00adity semantics of a single cfor is naturally extended for nested cfors. Intuitively, \nthe inner cfor is serialized as part of the iter\u00adation of the serialized outer cfor. So, in Figure 1, \nthe program\u00ad mer could have replaced the simple for in line 19 with a cfor, and the execution would be \ncorrect. It would also increase the available concurrency because multiple threads from the nested cfor \niterations would be active at a node. However, in this case, it would not be ef.cient to use a cfor because \nthe message and latency overheads involved in starting and terminating the concur\u00adrent threads and remotely \naccessing nExamined and nToExamine would offset the potential concurrency gain from executing on mul\u00adtiple \nneighboring nodes of nodeIter. In general, a programmer must weigh the bene.ts of .ne-grained concurrency \nthrough nested cfors against the start-up and .nalization overheads of such con\u00adcurrency. Loose Variables. \nWhile serializability provides strong guaran\u00adtees on the behavior of cfor loops, sensor network applications \noften have variables that do not need serializability semantics and can obtain timeliness and message \nef.ciency bene.ts by using a looser consistency model. Examples include routing beacons that are used \nto maintain trees for sensor data collection, and sensor val\u00adues that need to be .ltered or smoothed \nusing samples from neigh\u00adboring nodes. Pleiades lets a programmer annotate such variables as loose, in \nwhich case accesses to these variables are not synchro\u00adnized within a cfor. The consistency model used \nfor loose vari\u00adables closely follows release consistency semantics [13]. Writes to a loose variable can \nbe re-ordered. The beginning of a new cfor statement or the end of any active cfor statement act as synchro\u00adnization \nvariables, ensuring that the current thread of control has no more outstanding writes. In Figure 1, variables \nnToExamine and nExamined are anno\u00adtated as loose (lines 9 and 10) in order to gain additional con\u00adcurrency \nand avoid lock overhead on them. These annotations are based on the two observations that it is safe \nto examine a node in nToExamine multiple times, and that only a cfor itera\u00adtion on nodeIter can remove \nthe candidate node nodeIter from nToExamine. Alternatively, the programmer can derive the same concurrency \nin this case without using loose by temporarily stor\u00ading the set of nodes that would be added to nToExamine \nin line 21 and deferring the add node operations on this set until after statement 31. In general, the \nprogrammer can derive maximum concurrency while ensuring serializability by organizing her code so that \nwrites on serialized variables happen toward the end of a cfor. By default, loose variables are still \nreliably accessed, but the programmer can further annotate a loose variable to be unreliable, so that \nthe implementation can use the wireless broadcast facility. In Section 4, we evaluate the street parking \nexample with reliable loose variables and a separate application that primarily uses unre\u00adliable loose \nvariables. Automatic Control Flow Migration. Ultimately a centralized Pleiades program must be executed \nat the individual nodes of the network. As described in Section 3, the Pleiades implementation automatically \npartitions a Pleiades program into units of work to be executed on individual nodes and determines the \nbest node on which to execute each unit of work in order to minimize communi\u00adcation costs. For example, \nthe .rst .ve statements of the code (lines 6 10) execute at the node invoking reserve. The implementation \nthen migrates the execution of statements in lines 11 16 to node n. This is because it is cheaper to \nsimply transfer control to n than to .rst read isfree@n and later write it back if necessary. Similarly, \neach iteration of the cfor loop will execute at the node identi.ed by the current value of nodeIter (line \n17). While it does not hap\u00adpen in this example, the execution of a single cfor iteration can also successively \nmigrate to other nodes.  2.3 Parking Cars with nesC Pleiades provides several important advantages over \nthe traditional node-level programming for sensor networks in use today. To make things concrete, we \nconsider how the street-parking algorithm would be implemented in nesC. We describe two different nesC \nimplementations: a centralized version that is relatively simple and reliable but highly inef.cient, \nand a more complex distributed ver\u00adsion that is ef.cient but unreliable. In contrast, the Pleiades version \nis both reliable and ef.cient. 2.3.1 A Centralized nesC Implementation First, it is possible to implement \na centralized version of the algo\u00adrithm in nesC, wherein most of the algorithm is executed on a single \nnode. The major advantage of this approach is its relative simplic\u00adity for programmers. However, this \nversion is extremely inef.cient in terms of both message cost and latency. Figure 2 shows the core functions \nthat comprise such a program. The overall logic is similar to that of the Pleiades version from Figure \n1. However, program\u00ad mers must explicitly manage the details of inter-node communica\u00adtion. Because nesC \nuses an asynchronous, split-phase approach to such communication [5], the application s logic must be \npartitioned across multiple callback functions at remote read/write boundaries. The control .ow is as \nfollows. A task reserve (line 9) is spawned on the node closest to the car, which, in turn, calls the \nclosest node function (line 10) in the Topology component (this component is not shown). Since all tasks \nin nesC run to comple\u00adtion, and since Topology.closest node performs a split-phase lookup operation for \nthe desired closest node, the callback func\u00adtion found node is later invoked by Topology (line 12). The \ncall\u00adback creates a new task transfer control (line 14), which ulti\u00admately triggers doReserve on the \nclosest node (line 21). The rest of the algorithm then runs centrally on the closest node. doReserve,executingon \nclosest, either .nds itself free (line 22) or creates the nToExamine set with its current neighbor set \n(line 26). Next, it concurrently and asynchronously reads isfrees at nToExamine (line 27) using aread \nof the RemoteRW component (not shown). When the asynchronous read completes, it signals aread done (line \n29), and continue reserve is called (line 30). Such reads are locally cached in the RemoteRW component, \nso that continue reserve can synchronously read them in line 37. If no node with a free spot is found \n(lines 37 41), more neighboring nodes of the current nodes are searched using another asynchronous read \n(line 42), which, ultimately calls build more nodes (line 31). Since the code is executed on a single \nnode, this approach maintains a relatively straightforward structure, similar to that of the Pleiades \ncode. The main drawback of this approach to node\u00adlevel programming is inef.ciency. Message cost is high \nbecause isfree of every node is centrally fetched and checked from a single node. In contrast, the Pleiades \nversion from Figure 1 uses a cfor to allow each node to locally process its own data, using the code \nmigration techniques described in Section 3. Thus, even for small example topologies of two-hop radius, \nit can be shown that the Pleiades version requires around half the messages required by the nesC version; \nthis message count for Pleiades includes all control overhead for code migration and for ensuring serializability \nof cfors. The concurrent cfor iterations in Pleiades also .nd a free spot earlier than is possible in \nthe nesC version. In the nesC version, continue reserve in line 42 waits on RemoteRW.aread for all remote \nneighbors in nToExamine to be asynchronously read, and build more nodes in line 51 similarly waits until \nall remote isfreesin nToExamine are read.  2.3.2 A Distributed nesC Implementation The Pleiades version \nof car parking in Figure 1 does a breadth\u00ad .rst search around the closest node, moving to the next depth \nin a distributed fashion only if no free slot is found in the current one. Unfortunately, a distributed \nimplementation in nesC that provides thesamebehaviorasthe Pleiades version would be exceedingly complex. \nSuch an implementation would require the programmer to manually implement many of the same concurrency \ncontrol tech\u00adniques that Pleiades automatically implements for cfors, as dis\u00adcussed in Section 3.2. For \nexample, to ensure that exactly one free 1: module ReserveM { 2: uses{...} 3: provides { ... } 4: \n} implementation { 5: nodeset nToExamine, nExamined; 6: boolean reserved, isfree, is_remote_free; \n7: node closest, reserved_node, req, iter, iter1; 8: pos dst; 9: task void reserve() { 10: call Topology.closest_node(dst); \n 11: } 12: event void Topology.found_node(node n){ 13: closest = n; req=TOS_LOCAL_ADDRESS; 14: post \ntransfer_control(); 15: } 16: task void transfer_control() { 17: uint8_t i; 18: //Trigger remote \ndoReserve() at closest node 19: //Also, send req and closest node values 20: } 21: task void \ndoReserve() { 22: if (isfree) { 23: reserved_node=TOS_LOCAL_ADDRESS; 24: call MsgInt.send_reply(req,FOUND);} \n 25: else { 26: nToExamine=call Topology.get_neighbors(); 27: call RemoteRW.aread(nToExamine,ISFREE); \n} 28: } 29: event void RemoteRW.aread_done(done_t done) { 30: if (done==ISFREE) continue_reserve(); \n 31: else if (done==NEIGHBORS) build_more_nodes(); 32: } 33: void continue_reserve() { 34: for(iter=get_first(nToExamine);iter!=NULL; \niter=get_next(nToExamine)) { 35: remove_node(iter, nToExamine); 36: add_node(iter, nExamined); 37: \nif(is_remote_free=call RemoteRW.read(iter,ISFREE)){ 38: reserved_node=iter; reserved=TRUE; 39: call \nRemoteRW.awrite(iter,ISFREE,0); } 40: } 41: if (!reserved) 42: call RemoteRW.aread(nToExamine,NEIGHBORS); \n 43: } 44: void build_more_nodes(){ 45: nodeset nl; 46: for(iter=get_first(nToExamine);iter!=NULL; \niter=get_next(nToExamine)) { 47: nl=(call RemoteRW.read(iter,NEIGHBORS)); 48: for(iter1=get_first(nl); \niter1!=NULL; iter1=get_next(nl)) 49: if(!member(iter1,nExamined)) 50: add_node(iter1,nToExamine); } \n 51: call RemoteRW.aread(nToExamine,ISFREE); 52: } 53:}  1: module ReserveM { 2: uses{...} 3: provides \n{ ... } 4: } implementation { 5: boolean isfree, seen, reserved; 6: pos dst; 7: node start_node[], \nreq, orig, reserved_node; 8: uint8_t cnt_start_node, hopcount; 9: task void reserve() { 10: call Topology.closest_node(dst); \n11:} 12:event void Topology.found_node(node n){ 13: orig=TOS_LOCAL_ADDRESS; 14: start_node[0]=n, req=n, \nhopcount=HOP_MAX; 15: cnt_start_node=1; 16: post transfer_control(); 17:} 18:task void transfer_control() \n{ 19: uint8_t i; 20: for (i=0;i<cnt_start_node;i++) { 21: //Trigger remote doReserve() at every start_node[i] \n 22: //Also, send each node our req, orig, hopcount values 23: } 24:} 25:task void doReserve(){ 26: \nif(!seen) {seen=TRUE;} 27: if (isfree &#38;&#38; !seen){ 28: reserved_node=TOS_LOCAL_ADDRESS; 29: \nisfree=FALSE; 30: call MsgInt.send_reply(req,FOUND); } 31: else flood_neighbors(); 32:} 33:void flood_neighbors() \n{ 34: nodeset nl=Topology.get_neighbors(); 35: node iter; 36: hopcount--; 37: if (hopcount>0) { \n38: cnt_start_node=0; 39: for (iter=get_first(nl);iter!=NULL;iter=get_next(nl)) 40: start_node[cnt_start_node++]=iter; \n 41: post transfer_control(); } 42:} 43:event void MsgInt.receive_reply(node rep,msg_t msg){ 44: if \n(msg==FOUND) { 45: if (!reserved){ 46: reserved_node=rep; 47: call MsgInt.send_reply(rep,ACCEPT); \n 48: call MsgInt.send_reply(orig,FOUND); } 49: else call MsgInt.send_reply(rep,REJECT); } 50: else \nif(msg==REJECT){isfree=TRUE;} 51:} 52:} //end implementation Figure 2. Reliable but inef.cient street-parking \nin nesC. Figure 3. Ef.cient but unreliable street-parking in nesC. space is reserved for a car, the programmer \nwould have to imple\u00adment a form of distributed locking for conceptually central vari\u00adables. In general \nthe use of locking would then require manual sup\u00adport for distributed deadlock detection or avoidance. \nSimilarly, to ensure that the closest free space is always found, the programmer would have to manually \nsynchronize execution across the nodes in the network, to ensure that a depth d is completely explored \nbefore moving on to depth d + 1. Therefore, in practice a distributed version in nesC would forgo synchronization, \nas shown in Figure 3. Here we do a distributed .ooding-based search around the closest node, in order \nto .nd a free spot. The control .ow is as follows. After reserve is invoked (line 9), doReserve is ultimately \ntriggered, in a manner similar to the previous version. The only difference here is that doReserve may \nbe active at multiple nodes that receive the .ooding request and may be activated multiple times by several \nneighbors (lines 39 41). Since a node must process a request exactly once even if its doReserve is triggered \nmultiple times by its neighbors, doReserve uses a .ag seen (line 26) to ignore all but the .rst request. \nTo limit the number of duplicate requests at a node, the code also suppresses broadcasts to neighbors \nwhen the hopcount reaches 0 (line 37). This is an effective technique when the network diam\u00adeter is unknown \nand when we want to ensure the .ooded requests prefer shorter hops from the .ooding initiator (node req \nin line 14). receive reply (line 43) is a callback that is invoked by the local message interface component \nMsgInt (not shown) whenever a remote node sends a message. When a spot is found at a remote node, it \nsends FOUND to the .ooding initiator (line 30), which re\u00adjects all but the .rst successfully replying \nnode (lines 45 49). If a remote node is rejected, it sets itself back to free (line 50). As described \nearlier, the Pleiades version performs a breadth\u00ad.rst search on the topology, distributedly determining \nif there is a freeslotatdepth d before moving on to depth d +1. By contrast, the .ooding approach starts \nup the free-slot determination concurrently at all network nodes by .ooding the transfer of control. \nGiven this distinction, two things follow. First, the Pleiades approach is always more message ef.cient, \nsince it avoids multiple requests to the same node. Second, the .ooding approach has lower latency, since \nit can .nd a spot more quickly when the free spot is far away. The .ooding approach is also much more \nef.cient in terms of both messaging costs and latency than the centralized nesC version shown in Section \n2.3.1. Despite the latency advantage, the code in Figure 3 is signi.\u00ad cantly less understandable and \nreliable than the Pleiades version. The programmer is responsible for explicitly managing the com\u00admunication \namong nodes. For ef.ciency, this requires maintaining information about hop counts and other network \ndetails. It also re\u00adquires that conceptually central variables be packaged up and passed among the nodes \nexplicitly, taking care to maintain consis\u00adtency. For example, a special protocol is used in receive \nreply (lines 44 50) to ensure a consistent view of the reserved .ag, in order to avoid having multiple \nnodes be reserved for the same car. Similarly, in transfer control (lines 21 22), a node explicitly sends \nthe values of the node originating the request and the node closest to the destination that initiated \nthe search. In the Pleiades version, the combination of central variables and cfors takes care of these \nlow-level details automatically. Finally, the .ooding ver\u00adsion, unlike the other two versions, makes \nno guarantee that the .rst node to reply is the topologically closest node. So, if we want it to reliably \nreturn only a closest node, the req node executing MsgInt.receive reply (line 43) must wait for an indeterminable \namount of time before accepting a replying node, negating the la\u00adtency advantage.  2.4 Other Features \nof Pleiades Pleiades includes other language constructs to support the imple\u00admentation of common sensor \nnetwork idioms, which we brie.y de\u00adscribe. Sensors and Timers. As mentioned earlier, Pleiades uses spe\u00adcial \nkinds of variables as an abstraction for sensors, which are crit\u00adical components of sensor-network applications. \nSensor readings are asynchronous events, and Pleiades provides a facility to syn\u00adchronously wait for \nsuch an event to occur. In particular, Pleiades s wait function takes a sensor variable and returns when \nthe sensor takes a reading. At that point, the associated variable contains the most recent reading and \nthe program can take appropriate action. For example, this mechanism is used in order for the car-parking \napplication to wait for noti.cation that a parked car has left its spot, at which point the spot s sensor \nsets its associated isfree variable de.ned in line 2 of Figure 1 to TRUE (this operation is not shown), \nso that it can once again service remote reserve requests. A similar technique is used to model timers, \nwhich .re at some user-speci.ed rate. Modules. A Pleiades program consists of a number of modules, which \nare executed concurrently. Each module encapsulates a log\u00adically independent application-level computation, \nsuch as building a shortest path tree rooted at a given node, computing an aggregate, or routing application \ndata to a given node. A module is a set of functions that can invoke each other and de.ne and use global \nand local variables of both central and nodelocal type. Since modules are meant to be independent tasks, \nwe currently provide no syn\u00adchronization among modules.  3. Implementation This section describes the \nPleiades compiler and runtime system. The Pleiades compiler is built as an extension to the CIL infras\u00adtructure \nfor C analysis and transformation [21]. Our compiler ac\u00ad cepts a Pleiades program as input and produces \nnode-level nesC code that can be linked with standard TinyOS components and the Pleiades runtime system. \nThe Pleiades runtime system is a col\u00adlection of TinyOS modules that orchestrates the execution of the \ncompiler-generated nesC code across the nodes in the network. The Pleiades compiler and runtime cooperate \nto tackle two key technical challenges. First, they must partition a Pleiades program into chunks that \ncan be executed on individual nodes and determine at which node to run each chunk, striving to minimize \ncommuni\u00adcation costs. Second, they must provide concurrent but serializable execution of cfors. We discuss \neach challenge in turn. 3.1 Program Partitioning and Migration Partitioning. The Pleiades compiler performs \na data.ow analy\u00adsis in order to partition a Pleiades program into a set of nodecuts. Each nodecut is \nthen converted into a nesC task [5], to be executed by the Pleiades runtime system on a single node in \nthe network. At one extreme, one could consider the entire Pleiades program to be a single nodecut and \nexecute it at one node, fetching node-local and central variables from other nodes as needed (moving \nthe data to the computation). The other extreme would be to consider each instruction in the Pleiades \nprogram as its own nodecut, executing it on the node whose local variables are used in the computation \n(moving the computation to the data). Both of these strategies lead to generated code that has high messaging \noverhead and high la\u00adtency, in the .rst case due to the on-the-.y fetching of individual variables, and \nin the second case due to the per-instruction migra\u00adtion of the thread of control. We adopt a compilation \nstrategy for Pleiades that lies in be\u00adtween these two extremes, involving both control .ow migration \nand data movement. A nodecut can include any number of state\u00adments, but it must have the property that \njust before it is to be ex\u00adecuted, the runtime system can determine the location of all the node-local \nvariables needed for the nodecut s execution. We there\u00adfore de.ne a nodecut as a subgraph of a program \ns control-.ow graph (CFG) such that for every expression of the form var@e in the subgraph, the l-values \nin e have no reaching de.nitions within that subgraph. Given this property, the runtime system can retrieve \nall the nec\u00adessary node-local and central variables concurrently, before begin\u00adning execution of a nodecut, \nwhich improves the latency immensely over the .rst strategy above. At the same time, because the runtime \nsystem has information about the required node-local variables, it can determine the best node (in terms \nof messaging costs) at which to execute the nodecut, thereby obtaining the bene.ts of the sec\u00adond strategy \nabove without the latency and message costs of per\u00adstatement migration. Intuitively, the goal is to make \neach nodecut as large as possible, in order to minimize the control and data costs associated with a \nmi\u00adgration. Since a nodecut runs to its completion without any further communication, this approach would \nstatically minimize the total communication cost of a program. We make the goal of minimiz\u00ading migrations \nprecise by striving to minimize the total number of edges in the program s CFG that cross from one nodecut \nto another, since each such edge represents a migration of the dynamic thread of control from one sensor \nnode to another. This optimization prob\u00adlem is exactly equivalent to the directed unweighted multi-cut \nprob\u00adlem, which is known to be NP-complete [1]. Therefore, instead of .nding the optimal partition of \na CFG into nodecuts, the Pleiades compiler uses a heuristic algorithm that works well in practice, as \nshown in Section 4. The algorithm starts by assuming that all CFG nodes are in the same nodecut and does \na forward traversal through the CFG, creat\u00ading new nodecuts along the way. For each CFG node n containing \nan expression of the form var@e, we .nd all reaching de.nitions of the l-values in e and collect the \nsubset R of such de.nitions that oc\u00adcur within n s nodecut. If R is nonempty, we induce a new nodecut \nby .nding a CFG node d that dominates node n and post-dominates all of the nodes in R. Node d then becomes \nthe entry node of the new nodecut. Any such node d can be used, but our implementation uses simple heuristics \nthat attempt to keep the bodies of condition\u00adals and loops in the same nodecut whenever possible. The \nimple\u00admentation also uses heuristics to increase the potential for concur\u00adrency. For example, the body \nof a cfor is always partitioned into nodecuts that do not contain any statements from outside the cfor, \nso that these nodecuts can be executed concurrently. The .ve nodecuts computed by our algorithm for the \nstreet\u00adparking example in Figure 1 are shown in Figure 4. Nodecut 2 is induced due to the use of isfree@n \nin line 11 of Figure 1, since n is de.ned in line 8. The transitions from nodecut 2 to 3 and nodecut \n3 to 4 are induced to keep the cfor body separate from statements outside the loop, as mentioned above. \nFurther, an extra nodecut is induced within the cfor body (nodecut 5) to maximize read concurrency. The \nheuristic attempts to separate read and written variables into different nodecuts so that the acquisition \nof write locks, which is done before a nodecut starts execution, can be delayed until the write locks \nare actually required. In the current implementation we assume that a Pleiades pro\u00adgram does not create \naliases among node variables. Such aliasing has not been necessary in any of our experiments with the \nPleiades language so far. It is straightforward to augment our algorithm for generating nodecuts to handle \nnode aliasing by consulting a static may-alias analysis. Control Flow Migration. The Pleiades runtime \nsystem is respon\u00adsible for sequentially (ignoring cfor for the moment) executing each nodecut produced \nby the compiler across the sensor network. Figure 4. Nodecuts generated for the street-parking example. \nWhen execution of a nodecut C completes at some node n,that node s runtime system determines an appropriate \nnode n' at which to run the subsequent nodecut C' and migrates the thread of con\u00adtrol to n'. All of the \nPleiades program s central variables migrate along with the thread of control, thereby making them available \nto C'. Because of the special property of nodecuts, the runtime system knows exactly which node-local \nvariables are required by C',so these variables are also concurrently fetched to n' before execution \nof C' is begun. To determine where the next nodecut should be executed, the runtime uses the overall \nmigration cost as the metric. The runtime knows the number of node-local variables needed from each node \nfor executing the next nodecut as well as the distances (the number of radio hops) of these nodes relative \nto each other according to the current topology. The runtime chooses the node that minimizes the cost \nof transfers from within this set. For example, nodecut 2 in Figure 1 accesses the node-local variable \nisfree@n, as well as two central variables reserved and reservedNode. The cost of running this nodecut \nat the node executing nodecut 1 is the cost of fetching the value of isfree from n at the beginning of \nnodecut 2 and writing back isfree if necessary. This cost is two reliable messages across multiple radio \nhops. By contrast, if the runtime at nodecut 1 hands off nodecut 2 to node n, the cost is that of transferring \nthe thread of control along with the central variables. This is only one reliable message across the \nsame number of hops. So, Pleiades executes nodecut 2 at n. Since the nodecuts along with the set of node-local \nvariables accessed in each nodecut are statically supplied by the compiler, our migration approach thus \nexploits a novel combination of static and dynamic information in order to optimize energy ef.ciency. \nWe note that this approach does not require every node to keep a fully consistent topological map, but \nonly the relative distances of the nodes involved in the nodecut. In our current implementa\u00adtion, nodes \nuse a statically con.gured topological map in order to make the migration decision; we will explore lightweight, \ndynamic approaches to determine approximate topological maps as part of future work. 3.2 Serializable \nExecution of cfors To execute a cfor loop, the Pleiades runtime system forks a sepa\u00adrate thread for each \niteration of the loop. We call the forking thread the cfor coordinator. Program execution following the \ncfor only continues once all the forked threads have joined. Each forked thread is initially placed at \nthe node representing the value of the variable the cfor iterates over, and any subsequent nodecuts in \nthe thread are placed using the migration algorithm for nodecuts de\u00adscribed above. A forked thread may \nitself execute a cfor state\u00adment, in which case that thread becomes the coordinator for the inner cfor, \nforking threads and awaiting their join. To provide reliability in the face of concurrency, Pleiades \nen\u00adsures serializability of cfor loops. This allows programmers to cor\u00adrectly understand their Pleiades \nprograms in terms of a sequential execution semantics. The Pleiades compiler and runtime ensure serializability \nby transparently locking variables accessed in each cfor body. The use of locking has the potential to \ncause deadlocks, so we also provide a novel distributed deadlock detection and re\u00adcovery algorithm. Distributed \nLocking. To ensure serializability, the Pleiades im\u00adplementation protects each node-local and central \nvariable accessed within a cfor iteration with its own lock. We employ a pessimistic locking approach, \nsince this consumes less memory than optimistic approaches such as versioning. To ensure serializability, \na lock must be held until the end of the outermost cfor iteration being executed; thus, the implementation \nuses strict two-phase locking. However, locks are acquired on demand rather than at the begin\u00adning of \nthe cfor iteration, thereby achieving greater concurrency. To further increase concurrency, our algorithm \ndistinguishes be\u00adtween read and write locks. Readers can be concurrent with one another, while a writer \nrequires exclusive access. The implementa\u00adtion acquires locks at the granularity of a nodecut. This allows \nthe locks to be fetched along with the associated variables before the nodecut s execution, decreasing \nmessaging costs. Our algorithm acquires locks in a hierarchical manner. Each cfor coordinator keeps track \nof which locks it holds, the type of each lock (read or write), which of its spawned threads are currently \nusing each lock, and which of its threads are currently blocked waiting for each lock. When a nodecut \nrequires a particular lock, it asks the coordinator of its innermost enclosing cfor for the lock. If \nthe coordinator has the lock, it either provides the lock or blocks the thread, depending on the lock \ns current status, and updates the lock information it maintains appropriately. If the coordinator does \nnot have the lock, it recursively requests the lock from its cfor coordinator, thereby handling arbitrarily \nnested cfors. Once the top-level cfor coordinator has been reached, it acquires the lock from the variable \ns owner and grants the lock to the requesting thread (who will then grant the lock to its requesting \nthread, and so on down to the original requester). Once a thread has obtained the lock on a variable, \nit fetches the actual value of the variable directly from the owner. When a spawned thread joins, it \nreturns its locks to its cfor coordinator, who may therefore be able to unblock threads waiting for these \nlocks. Also, if any of the locks owned by the joining thread were write locks, before releasing the locks \nit writes back the current value of the variable at the owner. It is possible to argue that this locking \nscheme always results in a serializable execution of a cfor, but we omit the details due to space constraints. \nLet us revisit the street parking example in Figure 1. For each cfor iteration, the Pleiades runtime \nat the coordinator sends a message containing the fork command to each of the remote nodes selected for \nexecution. Each node initially acquires a read and write lock respectively on its own versions of the \nnode-local variables isfree and neighbors. isfree uses a read lock instead of a write lock even though \nit can potentially be modi.ed in line 26, because using a read lock .rst and then upgrading it to a write \nlock if the conditional in line 23 succeeds signi.cantly enhances concurrency. On receiving these locks, \nthe threads fetch the variable values from the owners and begin concurrent execution of the initial nodecut \nof the cfor (nodecut 3 in Figure 4). Threads that run on nodes with an occupied parking space fail the \nif condition in line 23, release their locks, and join with the cfor coordinator. Threads on nodes that \nhave a free space contend for a write lock on central variables reserved and reservedNode and have to \nexecute the second nodecut of the cfor sequentially. The .rst thread to do so is selected as the winner, \nand other nodes do not change their isfree status. Distributed Deadlock Detection and Recovery. While \nthe lock\u00ading algorithm ensures serializability of cfors, it can give rise to deadlocks. One possibility \nwould be to statically ensure the absence of deadlocks, for example via a static or dynamic global ordering \non the locks. However, such an approach would be very conser\u00advative in the face of cfors containing multiple \nnodecuts, nested and conditional cfors, or cfors that contain updates to node vari\u00adables, thereby overly \nrestricting the amount of concurrency pos\u00adsible. Further, we expect deadlocks to be relatively infrequent. \nTherefore Pleiades instead implements a dynamic scheme for dis\u00adtributed deadlock detection and recovery. \nWhile such schemes can be heavyweight and tricky in general [4], we exploit the fork-join structure of \na cfor to arrive at a simple and ef.cient state-based deadlock detection algorithm. Our algorithm requires \nonly two bits of state per thread, does not rely on timeouts, and .nds deadlocks as soon as it is safe \nto determine the condition. Furthermore, this algorithm is implemented by the compiler and runtime, without \nany programmer intervention. We require every thread to record its state during execution, which is either \nexecuting, blocked,or joined.Wede.ne a cfor coordinator to be executing if at least one of the coordinator \ns spawned threads is executing, blocked if at least one of the coordinator s threads is blocked and none \nare executing,and joined if all of the coordinator s threads are joined. A thread can easily update its \nstate appropriately as its locks are requested and released during the locking algorithm described above, \nin the process also causing the thread to recursively update the state of its cfor coordinator. The program \nis deadlocked if and only if the top-level cfor coordinator ever has its state set to blocked. Once a \ndeadlock has been detected, we use a simple recovery algorithm. Starting from the top-level cfor coordinator, \nwe walk down the unique path to the highest thread in the tree of cfor co\u00adordinators that has at least \ntwo blocked child threads. We then re\u00adlease all locks held by these blocked threads and re-execute them \nin some sequential order. This simple approach guarantees that we will not encounter another deadlock \nafter restart. To support re\u00adexecution, each thread records the initial values of all variables to which \nit writes, so that the variables previously updated at their owners can be rolled back appropriately \nduring deadlock recov\u00adery. We assume that the iterations are idempotent, so there are no harmful side-effects \nof re-execution. This is true in many sensor networks programs, which primarily involve sensing and actuation \nas side effects.  4. Evaluation We have implemented the Pleiades compiler and runtime described in \nSection 3. In this section, we describe an evaluation of this implementation for various applications, \nwith Pleiades running on TelosB Tmote Sky motes. We .rst discuss the performance of a 0.5 90 0.45 0.4 \n80 70 60 0.35 0.3 50 Correct execution (SP) Correct execution with induced deadlocks (SPID) Incorrect \nexecution without locking (SP-NL) Incorrect execution under deadlocks (SPID-NR) 0 12 3 4 5 6 7 8 910 \n11  Fraction of reports Latency (seconds) 0.25 0.2 0.15 0.1 40 30 20 10 0.05 0 0 Request ID Figure \n5. PEG application error. Figure 6. Street parking latency. Pleiades application relative to a nesC implementation \nof that same application. Then, we quantify the performance of Pleiades support for serializability and \nnodecut migration. Pleiades and nesC Comparison. We compare a Pleiades imple\u00ad mentation of a Pursuit-Evasion \nGame (PEG) against a hand-coded node-level nesC implementation of the same application written by others \n[7] on a 40 node mote testbed. PEGs [26] have been ex\u00ad plored extensively in robotics research. In a \nPEG, multiple robots (the pursuers) collectively determine the location of one or more evaders using \nthe sensor network, and try to corral them. The mote implementation of this game consists of three compo\u00adnents: \na leader election module performs data fusion to determine the centroid of all sensors that detect an \nevader; a landmark rout\u00ading module routes leader reports to a landmark node; in turn, the landmark routes \nreports to pursuers. The Pleiades version of PEG implements the leader election component of PEG, and \nleverages the routing provided by the Pleiades runtime to route the leader reports directly to the pursuer. \nIt is less than a tenth of the nesC im\u00adplementation in terms of lines of code (63 lines as opposed to \n780). An important feature of this application is that it requires no serial\u00adizability semantics for \nthe core leader election module; in fact, the data we present below were obtained using a version of \nPleiades that did not support serializability. We also implemented PEG on Pleiades with full serializability \nsupport for leader election, and found that it does not incur additional overhead due to locking, be\u00adcause \nleader election needs only read locks, which are acquired once at the beginning, and retained until the \nend. Figure 5 depicts the main application-perceived measure of per\u00ad formance, the error in position \nestimate on a topological (reduced) map of the environment [16]. This .gure is highly encouraging; the \nPleiades program exhibits comparable error to a hand-crafted nesC program. The frequency of 2-and 3-hop \nerrors is slightly higher for Pleiades-PEG than for mote-PEG. On the other hand, Pleiades-PEG does not \nincur instances of 5-hop error that mote-PEG does. We also measured the latency between when a mote detects \nan evader and when the corresponding leader report reaches the pursuer. Mote-PEG has noticeably lower \nlatency than Pleiades-PEG, but for most nodes (about 80%), this latency difference is within a factor \nof two. This is because our implementation of Pleiades is unoptimized for handling cfor forks and joins, \nand because our nodecut placement implementation relies on relatively static hop count information. There \nis scope for improving both signi.cantly. The average network overhead for mote-PEG is 193 messages per \nminute, while for Pleiades-PEG is 243. The minimum and maximum network overhead is 137 and 253 for mote-PEG \nand 146 and 341 for Pleiades-PEG. While these results merit further study, they suggest that Pleiades \nperformance can be comparable to that of node-level programming. 0 1 2 3 4 5 6 7 8 910 11 Request ID \nFigure 7. Street parking message cost. Serializability Evaluation. We ran the street-parking application \nof Figure 1 on a 10-node chain mote topology. This topology is an extreme con.guration, and thus stresses \nour serializability imple\u00admentation, because the ef.ciency of packet delivery in a chain of wireless \nnodes drops dramatically with the length of the chain. In our experiments, 10 requests for free spots \narrive sequentially at the node in the center of the chain. To illustrate the power of Pleiades s serializability \nguarantees, and to understand its performance, we ran four different versions of the application: SP-NL, \nin which we con.gured the Pleiades compiler and runtime to disable locking; SP, which uses the complete \nPleiades compiler and runtime for locking, deadlock detection and recovery; SPID-NR, in which we induced \na deadlock into the application and con.gured the Pleiades runtime to disable deadlock recovery; and \nSPID, which uses the complete Pleiades implementation with the deadlock-induced ap\u00adplication. To improve \nperformance, we implemented message ag\u00adgregation for lock requests and forwarded locks across consecutive \nnodecuts. As expected, SP and SPID execute correctly, assigning exactly one spot to each request. SPID-NR \nfails to allocate a spot to all but the .rst request; in the absence of recovery code, the program deadlocks \nafter the .rst request. Finally, SP-NL violates the cor\u00adrectness requirements of the application, correctly \nsatisfying the .rst request, but assigning two free spots in each direction of the center node for the \nnext four requests; consequently, it also fails to satisfy the last four requests. Figure 6 plots the \ntime taken to assign a spot to the request, and Figure 7 plots the total number of bytes transmitted \nover the net\u00ad work for each request. The same qualitative observations may be drawn from both graphs. \nSP and SPID message cost and latency increase since successive requests have to search farther out into \nthe network to .nd a free spot. However, for the initial requests, the overhead of SP is comparable to \nthat of SP-NL. Moreover, SPID message cost and latency are only moderately higher than SP. The difference \nis attributable to the sequential execution of the cfor threads during deadlock recovery, with rollback \noverhead be\u00ading negligible. The periodic spikes in both plots arise because, for even-numbered requests, \nthere are two free spots at the same dis\u00adtance away from the requester that contend to satisfy the request. \nThese two free spots also cause a deadlock in the case of SPID. Finally, the latency and overhead of \nSP-NL .atten out for later re\u00adquests because they each incur the same cost: they search the en\u00adtire network \nfor a free spot and fail, because spots were incorrectly over-allocated during earlier requests. Thus, \nour Pleiades implementation correctly ensures serializ\u00adability and incurs moderate overhead for deadlock \ndetection and recovery. The absolute overhead numbers imply that even for the request which encounters \nthe highest overhead, the average band\u00adwidth of a node used by Pleiades is around 250bps, with the max\u00adimum \nbeing 1kbps at the node where the requests come in. This is quite reasonable, considering that the maximum \ndata rate for the TelosB motes is 250kbps. The absolute latency seems modestly high compared to the expected \nresponse time for human interactivity. For example, the last request takes almost a minute and a half \nto satisfy. This is an artifact of the end-to-end reliable transport layer that Pleiades currently uses, \nwhich waits for 2 seconds, before trying to resend a packet that has not been acknowledged as received. \nWe believe that the overall latency can be signi.cantly reduced by optimizing the transport layer. The \nBene.ts of Migration. Finally, we brie.y report on a small ex\u00adperiment on a 5-node chain that quanti.es \nthe bene.t of Pleiades s control .ow migration. In this application, a node accesses node\u00adlocal nodesets \nfrom other nodes more than a hop away, so that application-level network information can be gathered. \nWithout mi\u00adgration, the total message cost is 780 bytes, while, with migration, it is 120 bytes. Thus, \nwe see that, even for small topologies, control .ow migration can provide signi.cant bene.ts. 5. Related \nWork Pleiades is related to many programming concepts developed in parallel and distributed computing. \nWe classify related work into three broad categories. They are embedded and sensor systems lan\u00adguages, \nconcurrent and distributed systems languages, and parallel programming languages. Embedded and Sensor \nNetworks Languages. Several researchers have explored programming languages for expressing the global \nbehavior of applications running on a network of less-constrained 32-bit embedded devices (e.g., iPAQs). \nPleiades s programming model borrows from our earlier work on Kairos [8], an extension to Python that \nalso provides support for iterating over nodes and accessing node-local state. However, Kairos does not \nsupport au\u00adtomatic code migration or serializability. Kairos provides support for application-speci.c \nrecovery mechanisms [9], which Pleiades lacks. SpatialViews [25] is an extension to Java that supports \nan expressive abstraction for de.ning and iterating over a virtual net\u00adwork. In SpatialViews, control \n.ow migrates to nodes that meet the application requirements. To avoid concurrency errors, Spa\u00adtialViews \nrestricts the programming model within iterators. Regiment [24] is a functional programming language \nfor cen\u00ad trally programming sensor networks that models all sensor data generated within a programmer-speci.ed \nregion as a data stream. Regiment is a purely functional language, so the compiler can po\u00adtentially optimize \nprogram execution extensively according to the network topology. On the other hand, since the language \nis side\u00ad effect-free, it does not support the ability to update node-local state. For example, the car \nparking application would be much harder to write in Regiment. TinyDB [19] provides a declarative interface \nfor centrally ma\u00ad nipulating the data in a sensor network. This interface makes cer\u00adtain applications \nreliable and ef.cient but it is not Turing-complete. Because TinyDB lacks support for arbitrary computation \nat nodes, it cannot be easily used to implement the kinds of applications we support, like car parking. \nResearch on Abstract Regions [29] pro\u00ad vides local-neighborhood abstractions for simplifying node-level \nprogramming. This work is focused on programmability and ef.\u00adciency and does not provide support for \nconsistency or reliability. Concurrent and Distributed Systems. Argus [17] is a distributed programming \nlanguage for constructing reliable distributed pro\u00adgrams. Argus allows the programmer to de.ne concurrent \nobjects and guarantees their atomicity and recovery through a nested trans\u00adactions facility, but makes \nthe programmer responsible for en\u00adsuring serializability across atomic objects and for handling any application-level \ndeadlocks. Recently, composable Software Trans\u00adactional Memory (STM) [10] has been proposed as an abstrac\u00ad \ntion for reliable and ef.cient concurrent programming. Also, Ato\u00admos [2] is a new programming language \nwith support for implicit transactions and strong atomicity features. Our cfor construct, with its serializability \nsemantics and nest\u00ading ability, is designed in a similar spirit a concurrency prim\u00aditive with simplicity, \nef.ciency, reliability, and composability as goals. Unlike these systems, however, Pleiades derives concur\u00adrency \nfrom a set of loosely coupled and distributed, resource con\u00adstrained nodes. Therefore, the Pleiades implementation \nof cfor emphasizes message and memory ef.ciency over throughput or la\u00adtency. For the same reason, it \nuses a simple distributed locking al\u00adgorithm for serializability and a novel low-state algorithm for \ndis\u00adtributed deadlock detection and recovery. Pleiades cforsare also similar to atomic sections in Autolocker \n[20] in that both imple\u00ad mentations use strict two-phase locking. But Autolocker guaran\u00adtees the absence \nof deadlocks through pessimistic locking, while Pleiades uses an optimistic locking model in which locks \nare ac\u00adquired or upgraded as needed, and any deadlocks are detected and recoveredbythe runtime. Approaches \nto automatic generation of distributed programs have also been explored. For example, Coign [12] is a \nsystem for automatically partitioning coarse-grained components. Magne\u00adtOS [18] also has support for \npartitioning a program written to a sin\u00ad gle system image abstraction. A program transformation approach \nfor generating multi-tier applications from sequential programs is described in [22]. All these systems \nare primarily meant for par\u00ad titioning and distribution of programs into coarse-grained compo\u00adnents, \nthat can then be run concurrently on multiple nodes. Pleiades differs from these systems in generating \nnesC programs with .ne\u00adgrained nodecuts and supporting lightweight control .ow migration across such \nnodecuts. Parallel Processing Languages. Pleiades differs from prior paral\u00adlel and concurrent programming \nlanguages such as Linda [6] and Split-C [3] by obviating the need for explicit locking and synchro\u00ad nization \ncode. Pleiades also differs from automatic parallelization languages such as High Performance Fortran \n[14] by equipping the compiler and runtime with serializability facilities. This is be\u00adcause parallel \nprogramming languages focus on data parallelism on mostly symmetric processors, leaving to the programmer \nthe responsibility of ensuring deadlock and livelock freedom at the ap\u00adplication level. On the other \nhand, Pleiades offers task-level par\u00adallelism, where data sharing among sensor nodes is common, and where \nit is desirable to of.oad the correct implementation of con\u00adcurrency to the compiler and runtime. 6. \nConclusions and Future Work Pleiades enables a sensor network programmer to implement an application \nas a central program that has access to the entire net\u00adwork. This critical change of perspective simpli.es \nthe task of pro\u00adgramming sensor network applications on motes and can still pro\u00advide application performance \ncomparable to hand-coded versions. Pleiades employs a novel program analysis for partitioning cen\u00adtral \nprograms into node-level programs and for migrating control .ow across the nodes. Pleiades also provides \na simple construct that allows a programmer to express concurrency. This construct uses distributed locking \nalong with simple deadlock detection and recovery to ensure serializability. Together, these features \nensure that Pleiades programs are understandable, ef.cient, and reliable. Our implementation of these \nfeatures runs realistic applications on memory-limited motes. While our current Pleiades implementation \nis robust to one as\u00adpect of network dynamics (packet loss), the failure of a cfor co\u00adordinator can cause \nan application to fail. We are currently imple\u00admenting support for handling node dynamics such as crashes \nand additions through a simple retry-based mechanism that extends the reliable routing and transport \nmechanisms already present in the runtime. The basic idea is that node failures trigger an undo mech\u00adanism \nsimilar to that already used for deadlock recovery, which allows the initiator of the computation to \nretry. This approach natu\u00adrally .ts the semantics of the cfor construct and complements our programmability, \nef.ciency and reliability contributions. In future work, we intend to optimize the message and latency \ncosts of our implementation by exploring more ef.cient message batching alternatives. We also plan to \nsupport various relaxed con\u00adsistency models as alternatives to serializability. In addition, we would \nlike to allow the programmer to be able to easily trade off quality of results for time of distributed \nexecution. Finally, we plan to examine approaches to specifying sophisticated power manage\u00adment policies \nin Pleiades. Acknowledgments We thank Ki-Young Jang and Marcos Veiera for help with mote-PEG, Jeongyeup \nPaek and Omprakash Gnawali for help with the transport software, Avinash Sridharan for help with mote \nhardware, and Ben Greenstein and the anonymous reviewers for their com\u00adments that helped improve the \npaper. References [1] G. Calinescu, C. G. Fernandes, and B. Reed. Multicuts in unweighted graphs with \nbounded degree and bounded tree-width. LNCS 1998. [2] B. D. Carlstrom, A. McDonald, H. Cha., J. Chung, \nC. C. Minh, C. Kozyrakis, and K. Olukotun. The Atomos transactional program\u00adming language. In PLDI 2006. \n[3] D. E. Culler, A. C. Arpaci-Dusseau, S. C. Goldstein, A. Krishna\u00admurthy, S. Lumetta, T. von Eicken, \nand K. A. Yelick. Parallel pro\u00adgramming in Split-C. In Supercomputing, 1993. [4] A. K. Elmagarmid. A \nsurvey of distributed deadlock detection algo\u00adrithms. SIGMOD Rec., 1986. [5] D. Gay, P. Levis, R. von \nBehren, M. Welsh, E. Brewer, and D. Culler. The nesC language: A holistic approach to networked embedded \nsys\u00adtems. In PLDI 2003. [6] D. Gelernter and N. Carriero. Coordination languages and their sig\u00adni.cance. \nCommun. ACM, 1992. [7] O. Gnawali, B. Greenstein, K.-Y. Jang, A. Joki, J. Paek, M. Vieira, D. Estrin, \nR. Govindan, and E. Kohler. The TENET architecture for tiered sensor networks. In SenSys 2006. [8] R. \nGummadi, O. Gnawali, and R. Govindan. Macro-programming wireless sensor networks using Kairos. In DCOSS \n2005. [9] R. Gummadi, N. Kothari, T. Millstein, and R. Govindan. Declarative failure recovery for sensor \nnetworks. In AOSD 2007. [10] T. Harris, S. Marlow, S. Peyton-Jones, and M. Herlihy. Composable memory \ntransactions. In PPoPP 2005. [11] J. Hill, R. Szewczyk, A. Woo, S. Hollar, D. Culler, and K. Pister. \nSystem architecture directions for networked sensors. SIGOPS Oper. Syst. Rev., 2000. [12] G. C. Hunt \nand M. L. Scott. The Coign automatic distributed parti\u00adtioning system. In OSDI 1999. [13] P. Keleher, \nA. L. Cox, and W. Zwaenepoel. Lazy release consistency for software distributed shared memory. In ISCA \n1992, 1992. [14] C. Koelbel. An overview of High Performance Fortran. SIGPLAN Fortran Forum, 11(4), 1992. \n[15] L. Krishnamurthy, R. Adler, P. Buonadonna, J. Chhabra, M. Flanigan, N. Kushalnagar, L. Nachman, \nand M. Yarvis. Design and deployment of industrial sensor networks: Experiences from a semiconductor \nplant and the north sea. In SenSys 2005. [16] B. J. Kuipers and Y.-T. Byun. A Robust Qualitative Method \nfor Spatial Learning in Unknown Environments. In AAAI 1988. [17] B. Liskov. Distributed programming in \nArgus. Commun. ACM, 31(3), 1988. [18] H. Liu, T. Roeder, K. Walsh, R. Barr, and E. G. Sirer. Design and \nimplementation of a single system image operating system for ad hoc networks. In MobiSys 2005. [19] S. \nMadden, M. J. Franklin, J. M. Hellerstein, and W. Hong. The design of an acquisitional query processor \nfor sensor networks. In SIGMOD 2003. [20] B. McCloskey, F. Zhou, D. Gay, and E. Brewer. Autolocker: Synchro\u00adnization \ninference for atomic sections. In POPL 2006. [21] G. C. Necula, S. McPeak, S. Rahul, and W. Weimer. CIL: \nIntermediate language and tools for analysis and transformation of C programs. In Conference on Compilier \nConstruction, 2002. [22] M. Neubauer and P. Thiemann. From sequential programs to multi-tier applications \nby program transformation. In POPL 2005. [23] R. Newton, Arvind, and M. Welsh. Building up to macroprogram\u00adming: \nAn intermediate language for sensor networks. In IPSN 2005. [24] R. Newton and M. Welsh. Region Streams: \nFunctional macroprogram\u00adming for sensor networks. In DMSN 2004. [25] Y. Ni, U. Kremer, A. Stere, and \nL. Iftode. Programming ad-hoc networks of mobile and resource-constrained devices. In PLDI 2005. [26] \nC. Sharp, S. Schaffert, A. Woo, N. Sastry, C. Karlof, S. Sastry, and D. Culler. Design and implementation \nof a sensor network system for vehicle tracking and autonomous interception. In EWSN 2005. [27] D. Shoup. \nNew York Times Op-Ed: Gone Parkin , http://www. nytimes.com/2007/03/29/opinion/29shoup.html. [28] G. \nTolle, J. Polastre, R. Szewczyk, D. Culler, N. Turner, K. Tu, S. Burgess, T. Dawson, P. Buonadonna, D. \nGay, and W. Hong. A macroscope in the Redwoods. In SenSys 2005. [29] M. Welsh and G. Mainland. Programming \nsensor networks using Abstract Regions. In NSDI 2004.  \n\t\t\t", "proc_id": "1250734", "abstract": "<p>It is currently difficult to build practical and reliable programming systems out of distributed and resource-constrained sensor devices. The state of the art in today's sensornet programming is centered around a component-based language called nesC. nesC is a <i>node-level</i> language-a program is written for an individual node in the network-and nesC programs use the services of an operating system called TinyOS. We are pursuing an approach to programming sensor networks that significantly raises the level of abstraction over this practice. The critical change is one of perspective: rather than writing programs from the point of view of an individual node, programmers implement a <i>central</i> program that conceptually has access to the entire network. This approach pushes to the compiler the task of producing node-level programs that implement the desired behavio.</p> <p>We present the Pleiades programming language, its compiler, and its runtime. The Pleiades language extends the C language with constructs that allow programmers to name and access node-local state within the network and to specify simple forms of concurrent execution. The compiler and runtime system cooperate to implement Pleiades programs efficiently and reliably. First, the compiler employs a novel program analysis to translate Pleiades programs into message-efficient units of work implemented in nesC. The Pleiades runtime system orchestrates execution of these units, using TinyOS services, across a network of sensor nodes. Second, the compiler and runtime system employ novel locking, deadlock detection, and deadlock recovery algorithms that guarantee serializability in the face of concurrent execution. We illustrate the readability, reliability and efficiency benefits of the Pleiades language through detailed experiments, and demonstrate that the Pleiades implementation of a realistic application performs similar to a hand-coded nesC version that contains more than ten times as much code.</p>", "authors": [{"name": "Nupur Kothari", "author_profile_id": "81323492304", "affiliation": "USC, Los Angeles, CA", "person_id": "P757712", "email_address": "", "orcid_id": ""}, {"name": "Ramakrishna Gummadi", "author_profile_id": "81100334079", "affiliation": "USC, Los Angeles, CA", "person_id": "P335071", "email_address": "", "orcid_id": ""}, {"name": "Todd Millstein", "author_profile_id": "81100018064", "affiliation": "UCLA, Los Angeles, CA", "person_id": "P283207", "email_address": "", "orcid_id": ""}, {"name": "Ramesh Govindan", "author_profile_id": "81327488641", "affiliation": "USC, Los Angeles, CA", "person_id": "PP40032778", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1250734.1250757", "year": "2007", "article_id": "1250757", "conference": "PLDI", "title": "Reliable and efficient programming abstractions for wireless sensor networks", "url": "http://dl.acm.org/citation.cfm?id=1250757"}