{"article_publication_date": "06-10-2007", "fulltext": "\n Automatic Inversion Generates Divide-and-Conquer Parallel Programs Kazutaka Morita Akimasa Morihata \nKiminori Matsuzaki Zhenjiang Hu Masato Takeichi Graduate School of Information Science and Technology, \nUniversity of Tokyo {kazutaka,morihata,kmatsu}@ipl.t.u-tokyo.ac.jp {hu,takeichi}@mist.i.u-tokyo.ac.jp \nAbstract Divide-and-conquer algorithms are suitable for modern parallel machines, tending to have large \namounts of inherent parallelism and working well with caches and deep memory hierarchies. Among others, \nlist homomorphisms are a class of recursive func\u00adtions on lists, which match very well with the divide-and-conquer \nparadigm. However, direct programming with list homomorphisms is a challenge for many programmers. In \nthis paper, we propose and implement a novel system that can automatically derive cost\u00adoptimal list homomorphisms \nfrom a pair of sequential programs, based on the third homomorphism theorem. Our idea is to reduce extraction \nof list homomorphisms to derivation of weak right in\u00adverses. We show that a weak right inverse always \nexists and can be automatically generated from a wide class of sequential pro\u00adgrams. We demonstrate our \nsystem with several nontrivial exam\u00adples, including the maximum pre.x sum problem, the pre.x sum computation, \nthe maximum segment sum problem, and the line-of\u00adsight problem. The experimental results show practical \nef.ciency of our automatic parallelization algorithm and good speedups of the generated parallel programs. \nCategories and Subject Descriptors D.1.2 [Programming Tech\u00adniques]: Automatic Programming; D.1.3 [Programming \nTech\u00adniques]: Concurrent Programming Parallel programming General Terms Algorithms, Design, Languages \nKeywords Divide-and-conquer parallelism, Inversion, Program transformation, Third homomorphism theorem \n1. Introduction Divide-and-conquer algorithms solve problems by breaking them up into smaller subproblems, \nrecursively solving the subproblems, and then combining the results to generate a solution to the original \nproblem. They match very well for modern parallel machines, tending to have large amounts of inherent \nparallelism and working well with caches and deep memory hierarchies [28]. Among others, list homomorphisms \nare a class of recursive functions on lists, which match very well with the divide-and-conquer paradigm \n[9, 11, 24, 27, 30]. Function h is said to be a list homomorphism, if Permission to make digital or hard \ncopies of all or part of this work for personal or classroom use is granted without fee provided that \ncopies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. PLDI 07 June 11 13, 2007, San Diego, California, \nUSA. Copyright c &#38;#169; 2007 ACM 978-1-59593-633-2/07/0006. . . $5.00 there is an associated operator \n8 such that for any list x and list y h (x ++y)=h(x)8 h(y) where ++is the list concatenation. When function \nh is de.ned as the equation above, the computation of h on a longer list, which is a concatenation of \ntwo shorter ones, can be carried out by computing h on each piece in parallel and then combining the \nresults. For instance, the function that sums up the elements in a list can be described as a list homomorphism \nsum (x ++y)=sum(x)+sum(y). List homomorphisms are attractive in parallel programming for several reasons. \nFirst, being a class of natural recursive functions on lists, they enjoy many nice algebraic properties, \namong which, the three well-known homomorphism lemmas form the basis of the for\u00admal development of parallel \nprograms [9,19,21,22]. Secondly, and very importantly, they are useful to solve really practical problems. \nFor example, many algorithms executed on Google s clusters are on MapReduce [24], and most of them, such \nas distributed grep, count of URL access frequency, and inverting index, are certainly noth\u00ading but list \nhomomorphisms. Moreover, homomorphisms (catamor\u00adphisms) are suitable for developing robust parallel programs, \nand are considered to be a primitive parallel loop structure in the de\u00adsign of the new parallel programming \nlanguage Fortress in Sun Mi\u00adcrosystems [30]. Despite these appealing advantages of list homomorphisms \nin parallel programming, a challenge remains for a programmer to use them to solve their problems, particularly \nwhen the problems are a bit complicated. Consider the maximum pre.x sum problem [6], which is to compute \nthe maximum sum of all the pre.x sublists. For instance, supposing mps is the function that solves the \nproblem, we have mps [1, -1, 2] = 0. 1. (1+(-1)). (1+(-1)+2) =2 where . is an in.x operator returning \nthe bigger of two numbers. It is not straightforward to obtain a parallel program by .nding an operator \n8 such that mps (x ++y)=mps(x)8 mps(y). It is, however, easy to obtain two sequential programs. We may \ncompute the maximum pre.x sum either by scanning the list from left to right as mps (x ++[b])=mps(x). \n(sum(x)+b) or by scanning the list from right to left as mps ([a]++y)=0. (a +mps(y)). These two sequential \nprograms are specialized ones of list homo\u00admorphisms: In the former program y is specialized to a list \nwith a single element b, and in the latter program xis specialized to a list with a single element a, \nThis ease of sequential programming suggests us to look into possibilities of obtaining list homomor\u00adphisms \nfrom sequential programs. Noticing that not every sequen\u00adtial program can be parallelized, that is, not \nall functions can be described as list homomorphisms (in fact mps cannot be a list ho\u00admomorphism), it \nis important to clarify under what condition list homomorphisms exist and can be automatically derived. \nInterestingly, in the context of list homomorphisms, there is a famous theorem, called the third homomorphism \ntheorem,which says that if two sequential programs in some speci.c form exist in solving a problem, then \nthere must exist a list homomor\u00ad phism that solves the problem too. This theorem suggests a new parallel \nprogramming paradigm, that is, developing a parallel program with a list homomorphism from a pair of \nsequential ones. Although this theorem gives a necessary and suf.cient condition for the existence of \nlist homomorphisms, it mentions nothing of how to construct them. In fact, it remains open whether there \nis a general and automatic way to extract an ef.cient list homomorphism from two sequential programs \n[15]. In this paper, we propose a novel approach to automatic deriva\u00adtion of cost-optimal list homomorphisms \nfrom a wide class of se\u00adquential programs. Our idea is to reduce automatic extraction of list homomorphisms \nto automatic derivation of a weak right in\u00adverse of a function. We show that a weak right inverse always \nexists and can be automatically generated for a wide class of sequential functions. As will be seen later, \nthis new approach is applicable to many nontrivial examples, including the maximum pre.x sum problem \n[6, 14, 15], the pre.x sum computation [6], the maximum segment sum problem [3], and the line-of-sight \nproblem [6]. Our main contribution can be summarized as follows. We design a new automatic parallelization \nalgorithm based on the third homomorphism theorem, by reformalizing the third homomorphism theorem with \nthe weak right inverse and gen\u00aderating parallel programs by deriving weak right inverses. The optimization \nprocedure in the algorithm plays an important role in making parallelized programs be ef.cient.  We \nde.ne a language in which users can describe sequential programs for solving various kinds of problems \non lists. It is guaranteed that under a reasonable condition an ef.cient parallel program can be automatically \nderived from a pair of sequential programs in the language.  We have implemented the new automatic parallelization \nal\u00adgorithm, and tested the generated parallel programs using the SkeTo parallel programming environment, \nwhich directly sup\u00adports parallel programming with map and reduce (two special cases of list homomorphisms.) \nThe experimental results show practical ef.ciency of our automatic parallelization algorithm and good \nspeedups of the generated parallel programs. This in\u00addicates the promise of our new approach.  The rest \nof this paper is organized as follows. In Section 2, we brie.y explain the base theory used in our parallelization \nframe\u00adwork, our parallel computation patterns, and the third homomor\u00adphism theorem. In Section 3, we \nde.ne the new concept of the weak right inverse. In Section 4, we describe our automatic paralleliza\u00adtion \nalgorithm: we explain the source language we deal with, the algorithm to derive a weak right inverse, \nwhich is the core part of our parallelization algorithm, and the optimization algorithm for the derived \nprograms. We discuss the implementation issues and show experimental results, together with several application \nexam\u00adples in Section 5, and discuss the extensions of our technique and related work in Section 6. We \nconclude the paper in Section 7. 2. Basic Theory of List Homomorphisms In this section, we brie.y explain \nthe base theory of our paralleliza\u00adtion framework, our parallel computation pattern, and the third ho\u00admomorphism \ntheorem that gives a necessary and suf.cient condi\u00adtion for the existence of list homomorphisms. 2.1 \nNotations on Functions and Lists Our notations are basically based on the functional programming language \nHaskell [5]. Functional application is denoted by a space and an argument may be written without brackets. \nThus fameans f(a). Functions are curried, i.e. functions take one argument and return a function or a \nvalue, and the function application associates to the left. Thus fabmeans (fa)b. In.x binary operators \nwill often be denoted by ., ., 8. Functional application binds stronger than any other operators, so \nfa. bmeans (fa). b, but not f(a. b). A functional composition is denoted by a centered circle .. By de.nition, \n(f. g)x=f(gx). A functional composition is an associative operator. The identity function is denoted \nby id.The operator . is for tupling two functions, de.ned by (f. g)a=(fa,ga). The operator . expresses \nthe operation that computes the maxi\u00admum, and is de.ned as x. y=if (x= y)then xelse y. Lists are (nonempty) \n.nite sequences of values of the same type. A list is either a singleton or a concatenation of two lists. \nWe denote [a]for a singleton list with element a,and x++yfor a concatenation of two lists xand y. The \nconcatenation operator is associative. Lists are destructed by pattern matching. 2.2 Leftwards, Rightwards, \nand List Homomorphisms One point of our parallelization method lies in good capture of the structure \nof recursive computation. We classify most of the list manipulation computation into three classes, namely \nrightwards functions, leftwards functions, and list homomorphisms [4]. De.nition 2.1 (Leftwards function). \nFunction his leftwards if it is de.ned in the following form with function fand operator .. h[a]= fa \nh([a]++x)= a. hx That is, a leftwards function iterates the computation by sequen\u00adtially scanning the \nlist from right to left. De.nition 2.2 (Rightwards function). Function his rightwards if it is de.ned \nin the following form with function fand operator .. h[a]= fa h(x++[a]) = hx. a That is, a rightwards \nfunction iterates the computation by sequen\u00adtially scanning the list from left to right. De.nition 2.3 \n(List homomorphism [4]). Function his a list ho\u00admomorphism ([f, 8]),where 8 is an associative operator, \nif it is de.ned in the following divide-and-conquer form. h[a]= fa h(x++y)= hx8 hy Leftwards and rightwards \nfunctions have good correspondence to usual sequential programs, while list homomorphisms can be seen \nas a general form for ef.cient divide-and-conquer parallel computation on list: If a function is a list \nhomomorphism then its result does not depend on the place where the input list is split, because the \noperator 8 is associative. 2.3 Parallel Skeletons List homomorphisms play a central role in skeletal \nparallel pro\u00adgramming [8, 27], in which users are provided with a .xed set of parallel computation patterns \n(skeletons) to write parallel pro\u00adgrams. The following four parallel skeletons, namely map, reduce, and \ntwo scans, are considered to be the most fundamental to de\u00adscribe parallel computation on lists. Map \nis the operator that applies a function to every element in a list. Informally, we have map f [x1,x2,...,xn]=[fx1,f \nx2,...,f xn]. If f uses O(1) computation time, then map f can be implemented using O(1) parallel time. \nReduce is the operator that collapses a list into a single value by repeated application of an associative \nbinary operator. Informally, for associative binary operator 8,we have reduce (8)[x1,x2,...,xn]= x1 8x2 \n8\u00b7\u00b7\u00b78xn. If 8uses O(1) computation time, then reduce (8) can be imple\u00admented using O(log n) parallel \ntime. In fact, we can express every instance of list homomorphisms by map with reduce. ([f, 8]) = reduce \n(8) .map f Therefore, list homomorphisms can be ef.ciently implemented by map and reduce. Scanl is the \noperator that accumulates all intermediate results for computation of reduce from left to right. Informally, \nfor asso\u00adciative binary operator 8,we have scanl (8)[x1,x2,...,xn]=[x1,x18x2,...,x18x28\u00b7\u00b7\u00b78xn]. The dual \noperator scanr is to scan a list from right to left. If 8 uses O(1) computation time, both scanl (8) \nand scanr (8) can be implemented using O(log n) parallel time. Scans have a strong relationship with \nlist homomorphisms. Consider the functions inits and tails, for computing all the pre.x sublists and \nthe post.x sublists of a list, respectively. inits [x1,x2,...,xn]= [[x1],[x1,x2],...,[x1,x2,...,xn]] \ntails [x1,x2,...,xn]=[[x1,x2,...,xn],...,[xn-1,xn],[xn]] Then, scans can be de.ned in terms of map, reduce, \ninits,and tails. scanl (8)= map (reduce (8)) .inits scanr (8)= map (reduce (8)) .tails This implies that \ncomputation patterns of map (reduce (8)).inits and map (reduce (8)) . tails can be ef.ciently computed \nin parallel. This fact will be useful to parallelize sequential programs that use accumulation parameters. \nOur objective is to derive programs written by parallel skeletons from usual sequential programs. 2.4 \nThird Homomorphism Theorem When writing a program in terms of list homomorphisms or parallel skeletons, \nthe most dif.cult step is to .nd the associative binary operator. The third homomorphism theorem [16] \nis the theorem that gives a necessary and suf.cient condition for the existence of the associative binary \noperator. Theorem 2.1 (Third homomorphism theorem [16]). Function h can be described as a list homomorphism, \nif and only if it can be de.ned by both leftwards and rightwards functions. That is, there exists associative \noperator 8and function f such that h =([f,8]) if and only if there exist f, .,and .such that h[a]= fa \nh([a]++ x)= a.hx h(x++[a]) = hx.a. Corollary 2.2 ([16]). If function hcan be de.ned as both leftwards \nand rightwards functions, and if there is function g satisfying h. g.h = h, then there exists an associative \nbinary operator 8such that the following equation holds. h(x ++ y)= hx 8hy where a8b= h(ga++ gb) It is \nworth noting that the third homomorphism theorem only shows the existence of a list homomorphism. We \nneed an automatic method to derive such list homomorphisms, which is the main topic of this paper. 3. \nWeak Right Inverse In this section, we introduce the basic concept of weak right in\u00adverses, which play \na very important role in construction of our par\u00adallelization algorithm in Section 4. De.nition 3.1 (Weak \nright inverse). Function g is a weak right inverse of function f,if gsatis.es .b.range(f),gb = a.fa = \nb where range(f) denotes the range of the function f. Compared to the standard right inverse, the above \ng is weak in the sense that the domain of g can be larger than the range of f. In other words, g is a \nright inverse of f only if the domain of g is within the range of the original function f. Unlike inverses, \nany function has one or many weak right inverses. Consider the weak right inverse of sum. Each function \nthat returns a list whose sum is equal to the input value is a weak right inverse of sum. For instance, \neach of the the following functions is a weak right inverse of sum. g1 a =[a] g2 a =[a-1, 1] g3 a =[1, \n2,a-3] g4 a =[a/2,a/2]. While a function may have many weak right inverses, there exists at least one \nweak right inverse for any function. Lemma 3.1. At least one weak right inverse exists for any func\u00adtion. \nProof. Let f be a function. We de.ne function g by returning one of those xthat satis.es fx = yfor an \ninput y.This gis obviously a weak right inverse from the de.nition. Therefore, a weak right inverse exists \nfor any functions. For notational convenience, we write f. to denote a weak right inverse of function \nf. Below we give more examples, where sort is a function for sorting the elements of a list. sum . a=[a] \n. sortx= x (map f). x= map f. x Weak right inverses is useful for parallelization, because of the following \nlemma. Lemma 3.2. Let f be a function. The following property holds for a weak right inverse f. . f .f. \n.f = f Proof. By the de.nition of the weak right inverse, we know that for any b in the range of f,if \nf. b = a then fa = b.So (f .f.) b = f (f. b)= fa = b, and thus f .f. .f = f holds. Recall Corollary 2.2, \nwhich says that an associative operator for parallel computation can be derived if function g satisfying \nf . g . f = f exists. Now, by Corollary 2.2 and Lemma 3.2, we have the following parallelization theorem. \nTheorem 3.3 (Parallelization with weak right inverse). If function h is both leftwards and rightwards, \nthen h =([f, 8]) where fa =h [a] a 8 b =h (h. a ++h. b) holds. Theorem 3.3 shows that, when the function \nh is leftwards and rightwards, we can obtain the de.nition of the list homomorphism of h, provided that \nwe have a weak right inverse of h. Example 3.1. Let us see how Theorem 3.3 works. Function sum is leftwards \nand rightwards: sum [a]= a sum ([a]++x)= a +sum x sum (x ++[a]) = sum x +a so sum can be de.ned in the \nform of the list homomorphism ac\u00adcording to the third homomorphism theorem. Unlike the third ho\u00admomorphism \ntheorem that helps little in deriving the list homomor\u00adphism for sum, Theorem 3.3 shows us a way to go. \nAs seen before, we have obtained sum . a =[a], so we can derive the following list homomorphism: sum \n=([f, 8]) where fa =a a 8 b =a +b by two calculations, sum [a]=a and sum (sum . a++sum . b)= sum ([a]++[b])=a \n+b. Example 3.2. As a more involved example, consider the maximum pre.x sum problem, which we introduced \nin the introduction. We start with a quick but incorrect derivation of a list homomor\u00adphism for mps. \nNoticing that mps . a =[a], one may calculate as follows: a 8 b = mps (mps . a ++mps . b) = mps ([a]++[b]) \n=0. a . (a +b) and conclude (see Theorem 3.3) that mps (x ++y)= mps x 8 mps y. This derived homomorphism \nis actually incorrect, because mps [1, -2, 2, 1]should be 2,but mps [1, -2]8 mps [2, 1]gives 3. The problem \nin this derivation is in its wrong application of Theorem 3.3; it did not check whether the function \ncan be written by both leftwards and rightwards functions, which is required by the theorem. In fact, \nmps is leftwards: mps ([a]++x)=0. (a +mps x) but it is not rightwards in the sense that there does not \nexist . such that mps (x ++[a]) = mps x . a. However, it can be de.ned rightwards if the auxiliary function \nsum is allowed to be used: mps (x ++[a])=mps x . (sum x +a) This suggest us to consider tupling of the \ntwo functions. In fact, the tupled function (mps . sum)is both leftwards and rightwards: (mps . sum)[a] \n=(a . 0,a) (mps . sum)([a]++x)= a . (mps . sum)x (mps . sum)(x ++[a]) = (mps . sum)x . a where a . (p, \ns)=(0. (a +p),a +s) (p, s). a =(p . (s +a),s +a) To derive the list homomorphic form of (mps . sum)by \nThe\u00adorem 3.3, we need to .nd a weak right inverse of (mps . sum), sequential programs leftwards function \n&#38; rightwards function parallel skeletons inverter 1. unfolding functions 2. solving constraint \nequations 3. optimization 4. veri.cation of domain weak right inverse   parallel programs Figure \n1. Parallelization framework. namely (mps . sum)., which should take a pair of two values (p, s), and \noutput a list whose maximum pre.x sum must be p and whose sum must be s.That is, (mps . sum). (p, s)=x \nsuch that mps x =p . sum x =s. Derivation of (mps . sum). is not obvious at all, which will be studied \nin the next section. 4. Automatic Parallelization Algorithm Theorem 3.3 indicates that in order to obtain \na list homomorphism, it is suf.cient to derive its weak right inverse. In this section, we propose a \nnovel parallelization algorithm based on this observa\u00adtion. With our parallelization algorithm, users \ncan obtain ef.cient parallel programs for free, by focusing only on the development of a pair of sequential \nprograms.  4.1 Overview Figure 1 shows the framework of our parallelization algorithm. It accepts a \nsequential program as input, which may contain a list of functions that are both leftwards and rightwards, \nderives weak right inverses of the functions in the input, and generates a parallel program composed \nby parallel skeletons. Input: Sequential Programs The input to our parallelization algorithm is a sequential \nprogram that describes computation on lists. Different from the other paral\u00adlelizing tools, our algorithm \nrequires each function in the input to be implemented by a pair of sequential functions that perform \ncom\u00adputation by scanning lists leftwards and rightwards respectively. This requirement is to guarantee \nthe existence of parallel programs. To see concretely what the input sequential programs look like, consider \nagain the maximum pre.x sum problem. If the input is a list with only a single element, easily we obtain \nmps [a]=0. a. Otherwise, we hope to de.ne mps by scanning the lists leftwards and rightwards, that is, \nto .nd t1 and t2 such that mps ([a]++x)= t1 mps (x ++[a]) = t2. The basic restriction on the terms t1 \nand t2 is that x must appear in the form of fx where f is a function that is de.ned both leftwards and \nrightwards. This restriction ensures the order of visiting the elements of the list. As seen in Example \n3.2, such t1 and t2 exist with an auxiliary function sum that can be de.ned both leftwards and rightwards. \nTo parallelize more general programs, we consider another ex\u00adample. It is known that accumulation parameters \nare important in designing ef.cient programs. Ef.cient programs often appear in the following extended \nleftwards and rightwards forms: f ([a]++ x) c = t1 f (x++[a]) c = t2 where an accumulation parameter \nc is used. They seem out of the scope discussed above, but it has been shown [1] that this kind of programs \ncan be decomposed into a combination of skeletons ' g1 .map f1'.inits ' g2 .map f2'.tails '' where f1', \nf2', g1,and g2 are sequential functions de.ned without accumulation parameters. Our algorithm can deal \nwith the latter '' style of sequential programs, whenever f1', f2', g1,and g2 are de.ned leftwards and \nrightwards. As an example, consider the pre.x sum problem [7, 14, 15]: given a list, compute the sums \nof all the pre.x sublists. It can be sequentially described by ' psum x = psumx0 psum' [a] c =[a+ c] \n '' psum([a]++ x) c =[a+ c]++ psumx(a+ c) which can be equivalently described by psum = map sum .inits \nwhere sum can be sequentially implemented by both leftwards and rightwards functions, and our algorithm \ncan generate a parallel program for psum. The syntax of the source language is summarized in Figure 2. \nThe sequential program, namely the input to our algorithm, is of the following three forms: lrseqs: \nleftwards/rightwards programs;  map lrseqs .inits: leftwards accumulative programs;  map lrseqs .tails: \nrightwards accumulative programs.  The main functions are speci.ed by the special identi.er main.The \nlanguage to de.ne leftwards/rightwards programs will be discussed in Section 4.2. Parallelization Engine: \nInverter and Code Generator Our parallelization engine consists of two layers: The .rst is the inverter \nand the second is the code generator. The .rst layer of our algorithm, namely the inverter, derives a \nweak right inverse from lrseqs.Let f1,f2,..., fn be all the functions de.ned in the lrseqs. Then the \ninverter derives a weak right inverse of the tupled function, that is (f1 L f2 L \u00b7\u00b7\u00b7L fn). . This layer \nis the main part of our parallelization engine, because weak right inverses derive associative operators \nthat are necessary to construct list homomorphisms. We will show the automatic weak right inversion algorithm \nin Section 4.3. The second layer, namely the code generator, does three things. Firstly, it derives list \nhomomorphism for lrseqs based on Theo\u00adrem 3.3, from the weak right inverse generated by the inverter. \nSec\u00adondly, it transforms the sequential program de.ned in one of the three basic forms into a skeletal \nparallel program by the following rules. ([f,8]) = reduce (8) .map f map ([f,8]) .inits = scanl (8) .map \nf map ([f,8]) .tails = scanr (8) .map f Finally, it generates executable parallel code for the skeletal \nparallel program. Our parallelization never fails whenever the derivation of weak right inverses succeeds. \nOutput: Skeletal Parallel Programs Our algorithm automatically generates parallel programs that are de.ned \nwith parallel skeletons, which can be executed ef.ciently in parallel [27]. For instance, our algorithm \ngenerates the following skeletal parallel program for mps: mpspara = fst .reduce (8) .map f where fst \n(a,b)= a fa =(a.0,a) (px,sx) 8(py,sy) =(mps L sum)([px,sx -px]++[py,sy -py]) This program is an O(log \nn) parallel program for mps where nis the length of the input list. 4.2 Language to Specify Leftwards/Rightwards \nPrograms We provide users with a language to write leftwards and rightwards sequential programs, which \nare the input of the inverter. The lan\u00adguage captures a wide class of functions that accepts a list as \ninput and computes a numeric value. Leftwards and rightwards sequen\u00adtial programs are speci.ed by the \nnonterminal lrseqs in Figure 2. The nonterminal lrseqs consists of one or more function de.nitions (def \n), which provide leftwards de.nitions and rightwards de.ni\u00adtions. The body of each de.nition is a linear \nterm lterm,which is constructed by additions of two linear terms, multiplications by constants, applications \nof a function to the rest of the list x,values of the element of the list a, constants, or conditional \nexpressions. Any functionusedin lterm must be de.ned in the program, and the list element a should appear \nin the de.nition body at least once. As an example, recall the maximum pre.x sum problem. We can use \nthe language to describe a sequential program as follows. In the following program, the . operator is \nunfolded with the conditional expression. main = mps; mps[a] =if (a <= 0) then0elsea; mps ([a]++x) = \nif (0 <= a + mps(x)) then a + mps(x) else 0; mps (x++[a]) = if (mps(x) <= sum(x) + a) then sum(x) + a \nelse mps(x); sum[a] =a; sum ([a]++x) = a + sum(x); sum (x++[a]) = sum(x) + a; It is worth noting that \nwe need to write the leftwards and rightwards program of sum, because sum is necessary to write the rightwards \nprogram of mps. From this program, our inverter derives a weak right inverse of (mps L sum). For successful \nand automatic derivation of a weak right inverse, we impose some restriction on our language. This restriction \nex\u00adcludes some functions that are necessary for parallel computation. We can relax this restriction and \ndeal with wider classes of func\u00adtions in our framework. We will discuss these extensions in Sec\u00adtion \n6. 4.3 Automatic Weak Right Inversion for Parallelization Now we consider how to derive a weak right \ninverse. If lrseqs consists of only one function, it is easy to derive its weak right inverse, as seen \nin Example 3.1. But as seen in the Example 3.2, it is hard to derive a weak right inverse of a tupled \nfunction. Since many prog ::= main; lrseqs (program) main ::= main = fun (main function) | main = map(fun). \n(inits | tails) lrseqs ::= def + def ::= fun [a] = lterm; (function de.nition) fun ([a]++x) = lterm; \nfun (x++[a]) = lterm; lterm ::= lterm (+ | -) lterm (addition) | lterm (* | /) Integer (multiplication) \n| fun(x) (function application) | a (element of the list) | Integer (constant number) | if(cond ) then \nlterm else lterm (condition) cond ::= lterm (== | != | <= | >= | < | >) lterm | cond (|| | &#38;&#38;) \ncond Figure 2. The source language of our parallelization algorithm functions are de.ned in terms of \nboth leftwards and rightwards by using other auxiliary functions, we will focus on the derivation of \na weak right inverse of such a function. As shown in Figure 3, our derivation of a weak right inverse \nis carried out by the following procedure. Firstly, we get the constraint equations by unfolding the \nsequential functions for an input list of .xed length. Secondly, we solve the constraint equations and \nget a weak right inverse that is correct but may be inef.cient. Next, we optimize the weak right inverse \nby eliminating redundant conditional branches. Finally, we verify whether the domain of the weak right \ninverse is correct with respect to our preconditions. Figure 3 summarizes the four steps of the procedure. \nWe explain these four steps from Section 4.3.1 to Section 4.3.4, and after that, we demonstrate how they \nwork with a concrete example in Section 4.3.5. The inverter takes only one program, either leftwards \nor right\u00adwards, to derive a weak right inverse. Note that the inverter with a single program does not \nmake up the correct parallelization algo\u00adrithm without the other one, since both sequential programs \nof the pair are required to guarantee the existence of a parallel program as stated in Theorem 3.3. 4.3.1 \nUnfolding Functions and Getting Constraint Equations As the .rst step to obtain a weak right inverse, \nwe unfold the origi\u00adnal functions to get relational expressions (constraints) that explic\u00aditly describe \nthe relation between their input list and their output values. Since the input list of the original functions \ncorresponds to the output of the objective weak right inverse, our objective is to derive the computation \nthat computes the input list from the output values. Taking the maximum pre.x sum problem (Example 3.2) \nas an example, we may assume that the weak right inverse (mpsLsum). takes (p, s) . range(mps L sum) and \nreturns a singleton list [a] as (mps L sum). (p, s)=[a]. Then we get the following equation according \nto the de.nition of the weak right inverse as (p, s)=(mps L sum)[a] that is, p= mps [a],s = sum [a]. \nBy unfolding the functions mps and sum,we get p= if (a= 0) then 0 else a, s= a // function // Inversion \n// input // funcSet : all functions included in the program // output // a weak right inverse of the \ntupled function function Inversion (funcSet) return Optimize(Solve(Unfold(funcSet))) function Unfold \n(funcSet) constraintEquation = funcSet.Unfold(funcSet) return constraintEquation.EnumCond() function \nSolve (constraintEquationSet) foreach ((cond, eq) in constraintEquationSet) exprs = eq.solve() cond.Subst(exprs) \nweakRightInverse.Add(cond, exprs) return weakRightInverse function Optimize (weakRightInverse) foreach \n((cond, exprs) in weakRightInverse) if (!cond || weakRightInverse.Or()) then weakRightInverse.Remove(cond,exprs) \nreturn weakRightInverse // function // Veri.cation // input // preCond : the precondition that the functions \nsatisfy // weakRightInverse : a weak right inverse // output // whether the weak right inverse is valid \nfunction Veri.cation (preCond, weakRightInverse) return (!preCond || weakRightInverse.Or()) Figure 3. \nAlgorithm to derive a weak right inverse which can be expressed by the constraint equations with conditions \nin the form of C . E1,E2,...,En,where C is a constraint and each Ei is an equation: {a= 0}. p=0,s = a \n{a>0}. p= a, s= a. These two constraint equations imply that the weak right inverse (mps L sum). (p,s) \nreturns the singleton list [s] when (1) pis 0 and sis 0 or less, or (2) sis positive and equal to p. \nThe result is not satisfactory: The result is a partial de.nition of a weak right inverse and we know \nnothing about the case where the weak right inverse should return a longer list. It is obviously impossible \nto run the same algorithm for all the possible lengths of the output lists, though it would derive a \nfull de.nition of a weak right inverse. To resolve this problem, we assume that a weak right inverse \nreturns a list whose length is the same as the number of de.ned functions. For example, we assume that \n(mps L sum). returns a list of two elements. This assumption is problematic because the assumption may \nnarrow the domain of the derived weak right inverse. We will discuss the correctness of the derived weak \nright inverse in Section 4.3.4. 4.3.2 Solving Constraint Equations After unfolding the functions, we \nget the simultaneous equations that express the relation between the input variables and the output list \nof the objective weak right inverse. We then solve these equa\u00adtions using the constraint solver. If there \nare some variables we can\u00adnot decide their values uniquely, we substitute an arbitrary value for one \nof them. After that, we substitute the result of the simultane\u00adous equations for the variables of the \nconditional expressions, and derive a weak right inverse. 4.3.3 Optimization In general, the weak right \ninverse derived by the above-mentioned process is inef.cient: If there are nfunctions with mconditional \nbranches, the number of constraints can be 2m(n+1) in the worst case. Since there usually exist many \nunnecessary branches, we can improve the ef.ciency of the weak right inverse by eliminating unnecessary \nbranches. Let Ci be the ith conditional branch. If _ Ci .Ck (1) i. =k holds, then the domain of the weak \nright inverse does not change even if Ci is removed. Moreover, all the branches return a cor\u00adrect list \nas a weak right inverse if the input value is in the domain. Therefore, the expression corresponding \nto Ci is redundant and can be removed. The expression (1) is in the form of Presburger arith\u00admetic [25], \nand we can compute it by quanti.er elimination [10]. 4.3.4 Veri.cation As mentioned in Section 4.3.1, \nthe derived weak right inverse may W be a partial function. That is, i Ci =true may not hold, where Ci \ndenotes the ith condition. However, it is suf.cient for a weak right inverse to return the value in the \nrange of the original function. In other words, if _ P .Ci (2) i holds, where P corresponds to the range \nof the original function, then the weak right inverse meets the requirement of the de.nition. Our algorithm \nworks as follows. At .rst, the algorithm checks W whether i Ci = true holds. If not, the algorithm requires \nthe precondition that corresponds to the range of the original function, and tries checking the condition \n(2). These checks can be also done by quanti.er elimination. If the veri.cation fails, we fail to derive \na weak right inverse. 4.3.5 Example: Inversion of Maximum Pre.x Sum Now let us demonstrate the whole \nprocess of inversion by deriving a weak right inverse of (mps L sum). Because the number of de.ned function \nis two, we assume that a weak right inverse of (mps L sum)returns a list of two elements: (mps L sum). \n(p, s)=[a, b] where (p,s).range(mps L sum). This equation is equivalent to the following equation: (p, \ns)=(mps L sum)[a, b] By unfolding the de.nitions of L, sum,and mps,we get the following equations: p \n= if (b=0) then if (0=a)then aelse 0 else if (0=a+b)then a+belse 0 s = a+b Let us solve these simultaneous \nequations for aand bto get a weak right inverse. To solve these simultaneous equations, we enumerate \nall conditional branches and get a set of constraint equations with conditions as follows: {b=0.0=a}.p=a, \ns=a+b {b=0.0>a}.p=0,s=a+b {b>0.0=a+b}.p=a+b, s=a+b {b>0.0>a+b}.p=0,s=a+b Solving the equations gives \nthe following result. Two things are worth noting. Firstly, we cannot uniquely decide the values of a \nand bin the second, third and fourth equations, so let abe 0here. Secondly, there is a dependency between \npand sin the .rst and second equations, and we add it to the conditional part: {b=0.0=a}.a=p, b=s-p {b=0.0>0.p=0}.a=0,b=s \n{b>0.0=b.p=s}.a=0,b=s {b>0.0>b.p=0}.a=0,b=s Removing clearly unreachable branches and replacing aand \nbin the conditions by their solution yield the following equations: {s=p.0=p}.a=p, b=s-p {s>0.p=s}.a=0,b=s \nTherefore we get the following program of (mps L sum)., because (mps L sum). (p, s)=[a, b]: (mps L sum).(p, \ns) = if (s=p.0=p)then [p, s-p] else if (s>0.p=s)then [0,s] Next, we optimize this weak right inverse. \nSince the condition {s=p.0=p}includes {s>0.p=s}, our optimizer removes the branch and yields the following \nresult: (mps L sum).(p, s) = if (s=p.0=p)then [p, s-p] Finally, we verify the weak right inverse. While \nthe weak right inverse is not a total function, it is correct, because the condition (2) holds; s=p.0=pholds \nfor all list x,where (mps L sum)x= (p, s), because the return value of the function mps is larger than \nor equal to that of sum. To verify the correctness, users specify s =p.0 =pwhen our algorithm requires \nthe precondition of (mps L sum). Now that we have got a weak right inverse of (mps L sum), Theorem 3.3 \nsoon gives a parallel program seen in Section 4.1.  4.4 Properties Two remarks are worth making on the \nproperties of our paralleliza\u00adtion algorithm. First, our inversion procedure always terminates, and moreover, \nthe derived weak right inverse is always correct provided that the veri.cation step succeeds. Second, \nour derived parallel programs are guaranteed to be ef.cient in the sense that they use O(logn)parallel \ntime, where n is the length of the input list. This is because the weak right inverse returns a list \nof constant length, and thus the computation of 8in Theorem 3.3 uses constant time. 5. Implementation \nand Experiments We have implemented an automatic parallel code generator on the parallelization algorithm \nin C++. Two major issues in our imple\u00admentation are the speedup of the optimization step and the genera\u00adtion \nof architecture-independent parallel programs. In the optimization step (Section 4.3.3), we need computation \non a large number of constraint equations, which occupies most of the time in our parallelization algorithm. \nTo resolve this problem, we note that all the expressions in the optimization step are in the form of \nPresburger arithmetic, and we used the Omega library that solves truth judgments in the Presburger arithmetic \nfast based on the omega test method [26]. The use of the Omega library enables our parallel code generator \nto work in practical time. The generation of architecture-independent executable parallel programs is \nanother implementation issue. We need to take into account the architecture of parallel computers to \ngenerate ef.cient parallel programs, but there are so many parallel architectures from shared-memory \nones to distributed-memory ones that architecture\u00adspeci.c implementation is almost impossible. Our approach \nis to generate parallel programs that can be combined with the SkeTo li\u00adbrary [24], which provides not \nonly map and reduce but also scanl and scanr as parallel primitives based on the MPI environments. Our \nparallelization system generates C++ code of the main routine and the function objects used with the \nparallel primitives. In the rest of this section, we demonstrate the ability of our par\u00adallelization \ntools with two more examples, and give experimental results on the ef.ciency of our system. 5.1 Maximum \nSegment Sum Problem To show the power of our system, we consider the maximum segment sum problem, which \ncomputes the maximum of the sums for all the segments (contiguous sublists) of a list. It is an instance \nof the maximum weight-sum problems [29] that capture many dynamic-programming problems. Our system can \nautomatically parallelize all the problems on lists in [29]. As an example, we show automatic parallelization \nof the maximum segment sum problem. The input sequential programs for the maximum segment sum is given \nas follows: main = mss mss [a] = max(a, 0); mss ([a]++x) = max(a + mps(x), mss(x)); mss (x++[a]) = max(mss(x), \nmts(x) + a); mps [a] = max(a, 0); mps ([a]++x) = max(0, a + mps(x)); mps (x++[a]) = max(mps(x), sum(x) \n+ a); mts [a] = max(a, 0); mts ([a]++x) = max(mts(x), a + sum(x)); mts (x++[a]) = max(mts(x) + a, 0); \n sum[a] =a; sum ([a]++x) = a + sum(x); sum (x++[a]) = sum(x) + a; where max is a macro and de.ned as \nfollows: max(x, y) . if (x>=y) then x else y and mts is a function to compute the maximum sum of all \nthe post.x sublists. From this input source code, our parallelization system generates the following \nweak right inverse for the function (mss L mps L mts L sum). weakinv(mss &#38; mps &#38; mts &#38; sum) \n(m, p, t, s) = if(0 <= p <= m&#38;&#38; 0 <= t<= m &#38;&#38; s+m <=t+p) then [p, -p-t+s, m, -m+t] And \nbased on the Theorem 3.3 with this weak right inverse, our code generator generates the parallel program \nfor the SkeTo library as shown in Figure 4. 5.2 Line-of-Sight Problem Given an observation point and \na list of buildings along a line (see Figure 5), the line-of-sight problem is to .nd which buildings \nare visible originating at the observation point [7]. Let each building be represented by the vertical \nangle from the observation point to the top of the building. Then, a building on the line is visible \nif and struct mss_tuple_t { int m, p, t, s; mss_tuple_t(int m_, int p_, int t_, int s_) { m= m_; p =p_; \nt = t_; s= s_; } mss_tuple_t(){} }; struct func_t : public skeleton::unary_function <int, mss_tuple_t> \n{ mss_tuple_t operator()(int a) const { return mss_tuple_t(max(a, 0), max(a, 0), max(a, 0), a); } } func; \ninline mss_tuple_t fwd(int *ar, int n) { if (n == 1) { return mss_tuple_t(max(*ar, 0), max(*ar, 0), max(*ar, \n0), *ar); } else { mss_tuple_t x = fwd(ar + 1, n -1); return mss_tuple_t(max(*ar + x.p, x.m), max(0, \n*ar + x.p),max(x.t, *ar + x.s), *ar + x.s); } } inline void bwd(const mss_tuple_t &#38;x, int* ar) { \nar[0]= x.p; ar[ 1] =-x.p -x.t +x.s; ar[2]= x.m; ar[ 3] =-x.m +x.t; } struct odot_t : public skeleton::binary_function \n<mss_tuple_t, mss_tuple_t, mss_tuple_t> { mss_tuple_t operator()(const mss_tuple_t &#38;x, const mss_tuple_t \n&#38;y) const { int ar[4*2]; bwd(x, ar); bwd(y, ar+4); return fwd(ar, 4*2); } } odot; ... /* main routine \n*/ dist_list<mss_tuple_t> *list1 = list_skeletons::map(func, data); mss_tuple_t result = list_skeletons::reduce(odot, \nlist1); return result.m; Figure 4. The generated parallel program for the maximum seg\u00adment sum problem. \nThe bwd function is the implementation of the weak right inverse and func and odot are function objects \nthat are used in calling skeletal primitives. only if no other building between it and the observation \npoint has a greater vertical angle. We start by solving this problem sequentially. Since we have to compute \non each point, it is natural to use the form of map f .inits as discussed in Section 4.1. main = map(visible) \n. inits; visible [a] = 1; visible ([a]++x) = if (a <= amax(x) &#38;&#38; visible(x) == 1) then 1 else \n0; visible (x++[a]) = if (amax(x) <= a) then 1 else 0; amax [a] = a; amax ([a]++x) = max(a, amax(x)); \namax (x++[a]) = max(amax(x), a); From this source code the system automatically generates a weak right \ninverse for the function (visible L amax). weakinv(visible &#38; amax) (v,m) = if(v == 1 &#38;&#38; 0<= \nm) then [0, m] else if(v == 0 &#38;&#38; 0 <= m) then [m, 0] Finally, based on this weak right inverse, \nthe system generates parallel code using skeletal primitives, map and scanl. Note that 12345678 Figure \n5. Line-of-sight problem. This .gure shows the sequence of buildings. There is no obstacle to see the \nbldg-7, so the bldg-7 is visible. In this case, the answer is [1, 1, 0, 0, 1, 0, 1, 0]. Table 1. Execution \ntime (in seconds) for inversion. Branches Execution time Max. pre.x sum 4 0.021 Max. segment sum 70 0.209 \nLine of sight 2 0.021 Table 2. Execution time (in seconds) for generated parallel pro\u00adgrams. Processors \n1 4 16 32 Max. pre.x sum Max. segment sum Line of sight 3.30 6.25 6.53 0.845 1.58 1.70 0.212 0.401 0.445 \n0.126 0.236 0.250 the following equation holds for the main program: los = map visible . inits = map \n(fst . (visible L amax)) . inits = map fst . map (visible L amax ) . inits = map fst . scanl (8) where \nfst is a function that returns the .rst value of the pair and 8 is the associative operator derived from \nthe weak right inverse of (visible L amax). 5.3 Experimental Results To verify the ef.ciency of our \nparallelization algorithm and the gen\u00aderated parallel programs, we have made the following experiments. \nFirst, we give the experimental result of the parallelization algo\u00adrithm. It is worth remarking that \nfor mss, nine if-statements implied in the function max cause 70 branches in total before optimization. \nBut due to the use of the Omega library, it takes only 0.209 s in deriving the weak right inverse. Table \n1 shows the execution times of computing weak right inverses for three examples. Next we show the experimental \nresult of the generated parallel programs. We used our PC-clusters that consists of uniform PCs with \nPentium 4 Xeon 2.4 GHz CPU and 2 GB of memory con\u00adnected Gigabit Ethernet. The OS is Linux 2.4 and the \nC++ com\u00adpiler and the base MPI library used in SkeTo library are gcc 4.1.1 and mpich 1.2.7 respectively. \nWe executed three parallel programs for the maximum pre.x sum problem, the maximum segment sum problem, \nand the line-of-sight problem using an array of 64M (al\u00admost 67 million) elements using up to 32 processors. \nThe execution times are given in Table 2 and the relative speedups against each programs executed on \none processor are given in Figure 6. With these results, we can con.rm the ef.ciency of the parallel \nprograms generated by our system. 6. Discussion and Related Work In this section, we discuss most related \nwork in addition to that in the introduction, and highlight limitations and extensions of our parallelization \nsystem. Speedups 32 28 24 20 16 12 8 4 1 2 4 8 12 16 24 32Number of Processors Figure 6. Speedups for \nthe three programs 6.1 Derivation of List Homomorphism The research on parallelization via derivation \nof list homomor\u00adphisms has gained great interest, and there have been many approaches, such as the third \nhomomorphism theorem based method [15, 19], function composition based method [14], matrix multiplication \nbased method [31] and recurrence equation based method [2]. Our approach is unique in its use of weak \nright in\u00adverse in derivation of parallel programs. One of the advantages of our approach is that our \nparallelization framework works well for any function, if we have a way to obtain a weak right inverse. \nWe used the Omega library to solve truth judgments in the Presburger arithmetic. We would derive the \nmore parallel programs from se\u00adquential programs if we use the more powerful constraint solvers. In addition, \nIt might be possible to extend our approach to other data structures such as trees, but this could be \ndif.cult with the other approaches. 6.2 Inversion and Parallelization We have reduced parallelization \nprocess to derivation of a weak right inverse, and thus our approach is related to researches on au\u00adtomatic \ninversion [12, 13, 17, 18, 20, 23]. Different from the inverse that does not always exist for a given \nfunction, our weak right in\u00adverse always exists for any function. To guarantee the ef.ciency of the weak \nright inverse, as in Section 4, we impose some restric\u00adtion on our objective functions and obtain a weak \nright inverse by solving linear equations. Though it may look restrictive, our framework is powerful \nenough to solve many practical examples including most recurrence equations, and to generate ef.cient \npar\u00adallel programs within the framework of Presburger arithmetics. It will be interesting to see if we \ncan make better use of techniques on inversion to parallelize more functions. 6.3 Generalization We \nhave an assumption (as in Section 4.3.1) that the output list of a weak right inverse is of constant \nlength. This assumption excludes many functions that are necessary for parallel computation. One example \nis the function length, which computes the length of a list. Our algorithm cannot derive a weak right \ninverse of length though length is both leftwards and rightwards. length [a] =1 length ([a]++ x)=1+ length \nx length (x ++[a]) = length x +1 This is because variable a does not appear in the de.nition body, which \nis required by our system. This problem may be solved by generalization: generalizing constant 1 to variable \na. This general\u00adization would split length into two functions, length ' and map,as follows: length =length \n' . map one length ' [a]= a length ' ([a]++x)= a +length ' x length ' (x ++[a]) = length ' x +a where \none is the function which always returns 1 for any input. Now we can derive a parallel program of length \nbecause our inversion algorithm can deal with length '. (In fact, length ' is sum.) 7. Conclusion In \nthis paper, we have introduced a new concept called weak right inverse, proposed a novel parallelization \nframework based on the third homomorphism theorem and derivation of a weak right in\u00adverse, and implemented \na parallelization system that can automat\u00adically generate ef.cient parallel programs suitable for the \ndivide\u00adand-conquer paradigm. The experimental results show promise of the approach. We are now looking \ninto how to formalize and automate the generalization step discussed in Section 6. In addition, we are \nconsidering using more powerful constraint solvers in our system so that more involved constraint equations \ncan be solved and more sequential programs can be parallelized. Extending our approach to other data \nstructures such as trees is interesting future work. Acknowledgments The authors would like to thank \nIsao Sasano and Shin-Cheng Mu for valuable discussions with them, and the anonymous referees for their \nvariable advice. This work was partially supported by Japan Society for the Promotion of Science, Grant-in-Aid \nfor Scienti.c Research (B) 17300005, and the Ministry of Education, Culture, Sports, Science and Technology, \nGrant-in-Aid for Young Scientists (B) 18700021. References [1] J. Ahn and T. Han. An analytical method \nfor parallelization of recursive functions. Parallel Processing Letters, 10(1):87 98, 2000. [2] Y. Ben-Asher \nand G. Haber. Parallel solutions of simple indexed recurrence equations. IEEE Transactions on Parallel \nand Distributed Systems, 12(1):22 40, 2001. [3] J. Bentley. Algorithm design techniques. In Programming \nPearls, Column 7, pages 69 80. Addison-Wesley, 1986. [4] R. S. Bird. An introduction to the theory of \nlists. In Logic of Programming and Calculi of Discrete Design, NATO ASI Series F 36, pages 5 42. 1987. \n[5] R. S. Bird. Introduction to Functional Programming using Haskell. Prentice Hall, 1998. [6] G. E. \nBlelloch. Scans as primitive operations. IEEE Transactions on Computers, 38(11):1526 1538, 1989. [7] \nG. E. Blelloch. Pre.x sums and their applications. Technical Report CMU-CS-90-190, School of Computer \nScience, Carnegie Mellon University, 1990. [8] M. Cole. Algorithmic skeletons : A structured approach \nto the management of parallel computation. Research Monographs in Parallel and Distributed Computing, \n1989. [9] M. Cole. Parallel programming with list homomorphisms. Parallel Processing Letters, 5(2):191 \n203, 1995. [10] D. C. Cooper. Theorem proving in arithmetic without multiplication. Machine Intelligence, \n7:91 99, 1972. [11] J. Dean and S. Ghemawat. MapReduce: Simpli.ed data processing on large clusters. \nIn 6th Symposium on Operating System Design and Implementation (OSDI 2004), pages 137 150, 2004. [12] \nE. W. Dijkstra. Program inversion. In Program Construction, LNCS 69, pages 54 57. 1978. [13] D. Eppstein. \nA heuristic approach to program inversion. In Proceedings of the 9th International Joint Conferences \non Arti.cial Intelligence, pages 219 221, 1985. [14] A. L. Fisher and A. M. Ghuloum. Parallelizing complex \nscans and reductions. In Proceedings of the ACM SIGPLAN 94 Conference on Programming Language Design \nand Implementation (PLDI 94), pages 135 146, 1994. [15] A. Geser and S. Gorlatch. Parallelizing functional \nprograms by generalization. In Algebraic and Logic Programming (ALP 97), LNCS 1298, pages 46 60. 1997. \n[16] J. Gibbons. The third homomorphism theorem. Journal of Functional Programming, 6(4):657 665, 1996. \n[17] R. Gl\u00a8uck and M. Kawabe. A program inverter for a functional language with equality and constructors. \nIn Programming Languages and Systems. Proceedings, LNCS 2895, pages 246 264. 2003. [18] R. Gl\u00a8uck and \nM. Kawabe. Derivation of deterministic inverse pro\u00adgrams based on LR parsing. In Functional and Logic \nProgramming, 7th International Symposium (FLOPS 2004), Proceedings, LNCS 2998, pages 291 306. 2004. [19] \nS. Gorlatch. Systematic extraction and implementation of divide-and\u00adconquer parallelism. In Programming \nlanguages: Implementation, Logics and Programs. PLILP 96, LNCS 1140, pages 274 288. 1996. [20] D. Gries. \nInverting programs. In The Science of Programming, chapter 21, pages 265 274. 1981. [21] Z. Hu, H. Iwasaki, \nand M. Takeichi. Formal derivation of ef.cient parallel programs by construction of list homomorphisms. \nACM Transactions on Programming Languages and Systems, 19(3):444 461, 1997. [22] Z. Hu, M. Takeichi, \nand W. N. Chin. Parallelization in calculational forms. In 25th ACM Symposium on Principles of Programming \nLanguages (POPL 98), pages 316 328, 1998. [23] R. E. Korf. Inversion of applicative programs. In Proceedings \nof the 7th International Conferences on Arti.cial Intelligence (IC-AI 81), pages 1007 1009, 1981. [24] \nK. Matsuzaki, K. Emoto, H. Iwasaki, and Z. Hu. A library of constructive skeletons for sequential style \nof parallel programming (invited paper). In 1st International Conference on Scalable Information Systems \n(InfoScale 2006), 2006. [25] M. Presburger. Uber die vollstandigkeit eines gewissen systems der arithmetik \nganzer zahlen, in welchem die addition als einzige operation hervorstritt. Sprawozdanie z I Kongresu \nMatematikow Krajow Slowcanskich Warszawa, pages 92 101, 1929. [26] W. Pugh. The omega test: a fast and \npractical integer programming algorithm for dependence analysis. In Proceedings of the 1991 ACM/IEEE \nconference on Supercomputing, pages 4 13, 1991. [27] F. Rabhi and S. Gorlatch. Patterns and Skeletons \nfor Parallel and Distributed Computing. 2002. [28] R. Rugina and M. C. Rinard. Automatic parallelization \nof divide and conquer algorithms. In Proceedings of the 7th ACM Symposium on Principles Practice of Parallel \nProgramming (PPoPP 99), pages 72 83, 1999. [29] I. Sasano, Z. Hu, M. Takeichi, and M. Ogawa. Make it \npractical: A generic linear time algorithm for solving maximum-weightsum problems. In Proceedings of \nthe 5th ACM SIGPLAN International Conference on Functional Programming (ICFP 00), pages 137 149. 2000. \n[30] G. Steele. Parallel programming and parallel abstractions in fortress. In Functional and Logic Programming, \n8th International Symposium (FLOPS 2006), Proceedings, LNCS 3945, page 1. 2006. [31] D. N. Xu, S. C. \nKhoo, and Z. Hu. PType system: A featherweight parallelizability detector. In Proceedings of 2nd Asian \nSymposium on Programming Languages and Systems (APLAS 2004), LNCS 3302, pages 197 212. 2004.   \n\t\t\t", "proc_id": "1250734", "abstract": "<p>Divide-and-conquer algorithms are suitable for modern parallel machines, tending to have large amounts of inherent parallelism and working well with caches and deep memory hierarchies. Among others, list homomorphisms are a class of recursive functions on lists, which match very well with the divide-and-conquer paradigm. However, direct programming with list homomorphisms is a challenge for many programmers. In this paper, we propose and implement a novel systemthat can automatically derive cost-optimal list homomorphisms from a pair of sequential programs, based on the third homomorphism theorem. Our idea is to reduce extraction of list homomorphisms to derivation of weak right inverses. We show that a <i>weak right inverse</i> always exists and can be automatically generated from a wide class of sequential programs. We demonstrate our system with several nontrivial examples, including the maximum prefix sum problem, the prefix sum computation, the maximum segment sum problem, and the line-of-sight problem. The experimental results show practical efficiency of our automatic parallelization algorithm and good speedups of the generated parallel programs.</p>", "authors": [{"name": "Kazutaka Morita", "author_profile_id": "81331499971", "affiliation": "University of Tokyo, Tokyo, Japan", "person_id": "P871680", "email_address": "", "orcid_id": ""}, {"name": "Akimasa Morihata", "author_profile_id": "81372591868", "affiliation": "University of Tokyo, Tokyo, Japan", "person_id": "P871664", "email_address": "", "orcid_id": ""}, {"name": "Kiminori Matsuzaki", "author_profile_id": "81316489698", "affiliation": "University of Tokyo, Tokyo, Japan", "person_id": "P795982", "email_address": "", "orcid_id": ""}, {"name": "Zhenjiang Hu", "author_profile_id": "81100253989", "affiliation": "University of Tokyo, Tokyo, Japan", "person_id": "PP15027466", "email_address": "", "orcid_id": ""}, {"name": "Masato Takeichi", "author_profile_id": "81100466948", "affiliation": "University of Tokyo, Tokyo, Japan", "person_id": "PP15032927", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1250734.1250752", "year": "2007", "article_id": "1250752", "conference": "PLDI", "title": "Automatic inversion generates divide-and-conquer parallel programs", "url": "http://dl.acm.org/citation.cfm?id=1250752"}