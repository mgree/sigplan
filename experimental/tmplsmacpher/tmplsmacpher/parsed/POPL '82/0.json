{"article_publication_date": "01-25-1982", "fulltext": "\n TERMINATION OF PROBABILISTIC CONCURRENT PROGRAMS (Extended Abstract) Sergiu Hart, Micha Sharir Tel Aviv \nUniversity Amir Pnueli Weizmann Institute of Science Abstract. The asynchronous execution behavior of \nseveral concurrent processes, which may use randomization, is studied. Viewing each process as a discrete \nMarkov chain over the set of common execution states, we give necessary and sufficient conditions for \nthe processes to converge almost surely to a given set of goal states, under any fair, but otherwise \narbitrary schedule, provided that the state space is finite. (These conditions can be checked mechanically. \n) An interesting feature of the proof method is that it depends only on the topology of the transitions \nand not on the actual values of the probabilities. We also show that in our model synchronization protocols \nthat use randomization are in certain cases no more power\u00ad ful than deterministic protocols. This is \ndemon\u00adstrated by (a) Proving lower bounds on the size of a shared variable necessary to ensure mutual \nexlusion and lockout-free behavior of the protocol; and (b) Showinq that no fully symmetric randomized \n protocol can ensure mutual exclusion and freedom from lockout. o. Introduction. Randomization has \nproven to be a very useful tool in the construction of certain algorithms for parallel processes, which \nbehave better than their deterministic counterparts, by using less shared memory, having relatively simpler \nstructure, and in certain cases accomplishing goals that deterministic algorithms provably can not accom \nplish. The price that one usually has to pay in using such a probabilistic algorithm is that certain \nproperties that are required from the algorithm will happen with probability 1, but not necessarily with \ncertainty. Recently published algorithms of this sort include synchronization protocols [Ral] , choice \ncoordination [Ra2] , a symmetric distributed solution to the dining philosophers problem [LR] , Permksion \nto copy whhout fee all or part of thk material k granted provided that the copies are not made or dktributed \nfor dkcct commercial advantage, the ACM copyright notice and the title of the publication and its date \nappear, and notice k given that copying k by permission of the Association for Computing Machinery. To \ncopy otherwise, or to republkh, requires afeeand/or specific permksion. @ 1982 ACM0-89791-065-6/82/001 \n/oOOl $00.75 1 a probabilistic implementation of a common resource allocation scheme [FR] , and various \nalgorithms in symmetric distributed systems [IRI. The problem that one encounters in designing such an \nalgorithm is in showing that the algorithm is correct in a probabilistic sense. This is quite a tedious \ntask, since one must consider all possible sequences of execution steps taken by the processes and show \nthat none of them can prevent a certain desired event from happening with probability 1. TO quote Lehmann \nand Rabin in [LRI , The realm of proofs of correctness for con\u00adcurrent processes is not well known. As \nthe reader will realize, proofs of correctness for probabilistic distributed systems are extremely slippery. \nOur aim in this paper is to shed some light on this unexplored realm. In particular, assuming that each \nprocess has only finitely many states, we will give necessary and sufficient conditions for certain probabilistic \nproperties of a parallel algorithm to hold with probability 1. Moreover, these conditions can be tested \nalgorithmically, given the atomic structure of each process. That is, we present a fully mechanical decision \nprocedure of certain probabilistic properties, including, e.g. , freedom from deadlock and from lockout. \nWe will also be concerned with comparing the capabilities of probabilistic synchronization protocols \nwith those of deterministic protocols. We will provide in Section 2 twc instances in which the probabilistic \napproach is provably not more powerful than the deterministic approach. These instances are: (a) Achieving \nmutual exclusion and freedom from individual lockout by using a shared variable v on which indivisible \ntest and-set operations are allowed (here the size of v rm.lst be Q(N) in both cases, where N is the \nnumber of processes) . (b) Achieving those properties by a fully symmetric protocol which uses a shared \nvariable on which only indivisible read and write operations are allowed. (This is impossible to achieve \nin either approach. ) It turns out that a major conceptual problem in the analysis of a probabilistic \nparallel program is the choice of the right model for the possible execution sequences of the processes. \nIt is gene\u00adrally accepted paradicyn that the processes can be viewed as executing in sequence (with no \ntwo processes ever executing simultaneously). At each step some process is chosen to perform the next \natomic action. The decisions which process will be chosen to operate next are thought of as being taken \nby some imaginary scheduler, and the main problem is: what rules can the scheduler use in its decisions. \nFor example, the admissible schedules considered by Rabin in [Ral] are more restricted than those considered \nby Lehmann and Rabin in [LRI . We will show, as a consequence of our general theorems, that Rabin s \nalgorithm for synchronization given in [Ral] does not have the required properties if one allows also \nschedules of the sort considered in [LRI . 1. Characterization of Probabilistic Concurrent Termination. \nThe model that we use for representation of a probabilistic parallel program is the following: States \nin the program are identified by the location in each process and by the values of shared vari\u00ad ables \nand local variables. With each process k we associate a transition probability matrix Pk describing the \npossible state transitions that can occur as a result of a single atomic action of k , i. e., P! is \nthe probability of reaching 11 state j from state i under one step of  process k . Let I denote the \nset of all possible states. Let P1, . . ..PS be the transition matrices of the processes 1, . . . s \nparticipating in such an asynchronous parallel program. An execution tree, also referred to as a schedule \nu , is a tree constructed as follows: Nodes in the tree are pairs (i, k) where i C I is a program \nstate and k C {l,... ,s} is the process scheduled The root is labeled by (iO,k) where i. is the initial \nstate. The tO operate on that node. edges going out of a node labeled by (i,k) correspond to nonzero \nentries in the i-th row of pk , and designate transitions having positive to other states underprobabilities \nfrom state i Thus an edge fromexecution of process k . (il,kl) to (i2, k2) means that if process kl is \nscheduled to operate on the state 11 , there is a positive probability to reach state i2 . Th&#38; appearance \nof these nodes in the schedule o implies that kl is indeed scheduled at il and k at if the execution \nactually gets 2 2 there Note that the scheduler s decisions may depend on the whole path in the tree \nleading to a node, and so different processes may be scheduled on the same state in different nodes of \nthe tree. To ensure proper functioning of the parallel program, we will require that u possesses certain \nfairness properties. Definition. A schedule 5 is called strictly fair if each path II in the associated \ntransition tree is a fair path, i.e. each process k E K labels infinitely many nodes of H . That is, \nin each execution of the program each process is scheduled infinitely often. This is a standard requirement \n(see [LR] for example, where this is called proper ) in analysis of parallel programs. We will also consider \na weaker notion of fair ness. For this, note that a schedule o induces ~=~ on the sequencea probability \nmeasure space whose elements are paths in the transition tree associated with o . This measure is defined \non the Borel field generated by all the finite cylinders (i.e. sets of paths having the same initial \nsegment) such that the measure of such a cylinder i~ the product of probabilities of the edges of the \ncommon initial path. Definition. A schedule u is called fair if po-almost every path in the associated \ntransition tree is fair. As will be shown below, fair schedules are in some sense limit points of strictly \nfair schedules, so that with no loss of generality, we could deal with either class of schedules, and \nobtain essen\u00adtially the same results. The main problem that we shall be concerned is the probability \nof reachability of certain states from other states. For example, if i is a state in which a process \nk is trying to enter its critical section, and X is the set of all states in which k is at its critical \nsection, then the probability of reaching X from i is the probability that k will not be locked out of \nthe set X at state i . Similarly, if is the 1 set of states in which some process is at its critical \nsection, then the probability of reaching from i is the probability that the system 1 will not be deadlocked \nat state i . Let therefore X c I be a given set of goal i C I be an initial state. states, and let Our \naim is to compute f; x (u) s probability of ever reaching a state in xfrom i under u , where u is some \nfair schedule. Note that this can also be expressed as where f(n) = x(u) = probability of reaching a \nstate in x from i for the first time after exactly n steps under 5 . Since we will be interested in \nshowing that f: ~,x(u) = 1 for every fair schedule u , we define h* = in f{f~ =,x(o) : 0 a fair schedule} \n1,X and we seek conditions under which h; x =1. J Let us first explain our reasoning intuitively. each \ni~IPk >0. m l,J The way in which an adversary scheduler can prevent m-1 the system from ever entering \nx is to iterate (Condition (4a) says that if process k can forever through a set of states E disjoint \nfrom transfer the system from a state in I to a state x, scheduling at each iEE some process k outs \nide Im , then some k-transitions ~with non\u00adwhich transfers i Only to StdteS in E , and zero probabilities) \nmove the system down the chain using all processes in this manner. We will show I towards the goal . \nCondition (4. b) ensures r 10 that the nonexistence of such a set E is a neces\u00adthe existence of at \nleast one process that would do sary and sufficient condition for h: to be I1,X this for all states in \nI .) m We begin by a precise definition of the The proof is technical and involved, and is above notion: \ntherefore omitted in this version. Part of the proof, namely that (3) is equivalent to (4) , is Definition. \nA set EcI is called K-ergodic if implied by the algorithm below. the following holds: For each iCE~ \nRemarks. (1) We can regard the partition of I into &#38;hisis the set of processes that cannot lead from \nas the assignment of an element of the :l.;; ;;;ded set {l i to a state outside E) . Then we require \nthat ,..., n} to each state i~I. That is, each i~Im, l$m Sn is assigned the (i) U KE(i) = K , ,i. e. \nfor every process k E K value m . Denote this assignment by P:I + {1, . . ..n}. iEE there is a state \ni~E such that all k transitions out of Condition (4a) then states that for every i i are still in E . \nand k , if there is no k transition leading to i such that P(i) > p(i ) , then all k transi\u00ad (ii) For \nevery i#j6E there exists a chain tions must lead to i s such that p(i) = p(i ) . irir...,i ~~EwithiO=i, \ni= j,and 01 r Condition (4.b) states that for every m , a chain kl, . . ..kl-l ~ K of processes ~$m$n \no there exists a k = k(m) such that for such that for each S=o , 1,. ... r-l every i~I , there is a \nk transition from i m to i with p(i ) <p(i) =m. ks E KE(iS) This view shows our method to be an extension \n and of the proof method given in [LPS] for the fair k termination of deterministic concurrent programs. \nP,s, >0 11 Indeed, if we restrict ourselves to deterministic s S+l processes (i. e., assume that for \nany i E I , k ~ K (a set E satisfying (ii) will be called P:j = 1 for exactly one j C I ) , then our \ncondi\u00adcommunicating) . ,. tion 4 reduces to the proof method given in [LPS]. Let i ~ I be the initial \nstate, qnd X c I m open question is whether our results can be be the goal set with ; ~ X . Define I \nas the extended to processes with infinite state sets. set of all states that can be reached (with positive \nprobability) from $ before a state in (2) .ln immediate corollary of the preceding X is reache: using \nany finite sequence of theorem is that the convergence is independent of processes. I includes ~ and \nis disjoint from the particular values of nonzero transition proba\u00ad x. Our main result is bilities. .A \nTheorem 1: L~t i , X , I be as above, and (3) It is also easy to show that, under the assume that I is \nfinite. Then the followinq assumptions of the preceding theorem, min ~ h?1,X conditions are equivalent: \nis either O or 1 . We also show tha#E1the (1) h;.. ==l preceding theorem remains true if one admits only \n1,X ,.  strictly fair schedules. (2) h~,x=1, for each i~I . Next we present an algorithm that, given \nthe  (3) I does not contain any K-ergodic set set I of intermediate states, the set X of ,. goal states, \nand the transition probability  (4) There exists a decomposition of I matrices of the pro~esses, either \nconstructs a into disjoint sets 11, . . ..In , with K ergodic set EcIr thereby showing that  h: <1, \nor else builds up the decomposition {Im} Xm such that if we put I,x 10 = as in t&#38;s statement of \nthe theorem, thereby showing Jm= U Ir, m = 0,1, .. ..n , then for each that h. =1. r=o 1,X m=l,2, . . \n..n we have The algorithm proceeds as follows: (a) For each iCI m,kEK, P? (a) COnStrUCt a transition \ngraph G , which is a ;irected labeled graph whose nodes are elements of .O*P: =1 l,J 1,1 I, with one \nadditional ~ode designa~ing X ; for m-1 m each k ~ K , each iCI andj61 (orj=X) ~k>o, (b) There exists \nk = k(m) C K such that for such that we draw an edge e from i l,j to j and label a byk. (b) We begin \nto construct a decomposition {Im} by putting 10=X. (c) Suppose that have already been 10  lr constructed. \nLelete from G all nodes belonging to the union J = ; Ik of these sets; also for r k=O each i~I~J delete \nfrom G all edges e such that either e leads from i to some j~J,  or there exists another edge e leading \nfro; i to some node in J , and e and e are labeled by the same proce~s k C K . (In other words, we ignore \nstates that have already been assigned a rank, and transitions from a state i by processes that can, \nin a possibly alternative transition from state 1 t move the system into J . ) Let G rr denote the resulting \ngraph. (d) Partition Gr into strongly connected compo nents, and let E be such a component having no \nsuccessors (terminal component) . If each process k C K labels some internal edge of E , then E is a \nK ergodic set, in which case the algorithm halts, having found such a set; otherwise, put I =E, define \nk(r+l) to be a process k which d~~~ not label any internal edge of E , and rePeat steps (c) and (d) . \n(e) If finally G empties out, then we have found the desired decomposition of I . An example. Consider \nthe following two-process synchronization, where a shared variable c is used (we assume indivisible test \nand set opera\u00adtions on c) . c has 3 values: O -designating a neutral state, 1 a state in which process \n1 will enter its critical section, and 2 -where k2 will enter. The code for (resp. k2) 1 is as follows: \nTry: If c = 1 [resp. 2] then go to Ex else if ~=o then c:=Random (1,2) fi; /* draw I or z with equal \nprOb~ilities */ go to Try fi Ex: critical region \u00adc ;= o go to TKY Here we have 5 states, each represented \nas (C,Q1,!2) , where c is the value of the shared variable, and where (resp. t2) is the % location in \nprocess kl (resp. k2) , which can assume tke values T (for being at the trying section) and X (fo~ being \nin the critical section). We assume that both processes remain live indefinitely. The states are = \n(O, T,T) o = (I,T,T) 1 = (2,T,T)12 = (2,T,X) 13 X=i = (l, X,T)4 and the transition matrices are  r \n,.,., 10 1 2 3 x 10 1 2 13x 11 1 10TT T 1 11 1 1 -2 1 13 We will show that I = {io, i ~,i2,i3} does \nnot conta in a K-ergodic set by applying the algorithm described above. Begin by putting 10 = X = [i4} \n. The other sets are found as follows: We first construct G :  k, k, k2 kl kl (each edge is labeled \nby the process that induces it). To find , delete the k edge from to x 11 111 to obtain A terminal strongly \nconnected o component in is {il} , and only k2 labels o internal edges of this component. Hennce, we \nput = {ill , k(1) = kl . TO find , delete from 1 2 the node and all the edges going out of o 1 10 while \nthe other 11 . edges are siblinqs of such edges, zn the sense defined above. ) thus has the following \nform 1 (some of these edges lead to k,  k2 k2 from which one easily obtains 12 = {iO} , k(2) = = {i3}, \nk(3) = k2 and 14 = {i4} , k(4) = k2 we emphasize again that this construction and its consequences do \nnot depend on the particular values of nonzero transition probabilities used in the randomization statements \nin the processes involved. 2. Probabilistic vs. Deterministic Protocols. In this section we compare the \ncapabilities of probabilistic protocols with those of deterministic protocols. h example motivating this \nstudy is an algorithm, given in [Ral] , for synchronization of N processes, which uses only O(log N)-valued \nshared variable on which indivisible test-and-set operations are allowed. On the other hand, it is shown \nin [BFJLP] that in any deterministic synchronization algorithm for N processes using test-and-set operations \non a shared variable, and providing both mutual exclusion and freedom from lockout, the shared variable \nmust have about % values, and this lower bound increases to about N/2 values if certain natural constraints \nare imposed on the structure of the processes involved. The algorithm given in [Ral] is shown to provide \nmutual exclusion and to be lockout free. However, the schedules under which this algorithm is assumed \nto operate form a restricted subset of the general fair schedules that we consider. It is helpful in \nthis regard to ignore the demonic nature of a schedule, and simply interpret it as an integral part of \nthe execution tree. Thus the schedule does not really use the past history of an execution to determine \nthe next process to be scheduled, but simply records at each execution step which process is next ready \nto perform an atomic action. Other families of schedules considered in the literature impose certain \nconstraints on the scheduling, e.g. bounded speed ratios between processes [RsI, Or, as is the case in \n[Ral] , allowing the schedule to base its decisions only upon partial information concerning past execution \nhistory, so that it must make the same scheduling choice for all sequences having the same partial structure. \nSuch a restriction on the behavior of the schedule can improve the performance of a synchronization algorithm, \nbut may make it less robust, in the sense that it may fail to meet some of its requirements if more general \n(fair) schedules are allowed. We show in this section that Rabin s algorithm is not lockout-free if \ngeneral fair schedules are allowed. As it turns out, this fact does not depend on the particular fOrm \nOf the algorithm, but follows from a general lower bound on the size of a shared variable necessary to \nensure both mutual exclusion and freedom from lockout. Specifically, we show that the lower bounds given \nin [BFJLP] are also required for probabilistic synchronization algorithms of the same kind. This will \nfollow from a generalization of the proofs given in [BFJLP] to the probabilistic case. Thus any algorithm \nattempting to use fewer values for such a shared variable is doomed to faikure against general fair schedules, \nalthough it may still be successful against weaker schedules, as is indeed the case in [Ral]. These results \ncan be summarized as follows: kl, Proposition 1: The algorithm in [Ral] is almost sure\u00ad. ly lockout-free \nagainst the restricted family of (fair) schedules introduced there, but fails to have this property if \ngeneral fair schedules are allowed. Indeed, an adversary scheduler may base its decisions on values obtained \nby random assignments. Thus a program based on the assumption that if a variable in a process is repeatedly \nassigned a ran\u00ad dom value, and a certain value of this choice will ensure admittance into a critical \nsection, if sche\u00adduled immediately then eventually the process will be admitted -such a program may fail \nunder a generally adversary scheduler. This is so since the sche\u00adduler may observe the randomly chosen \nvalue and sche\u00addule the process only when such scheduling will not enable the process to access its critical \nsection. Such scheduling is still fair with probability 1. This is not the case with Rabin s scheduler \nwhose decisions may not depend on randomly assigned values. Consequently a program correct under Rabin \ns scheduler may still fail under generally fair scheduler as defined here. Theorem 2: Suppose that N \nprocesses participate in a synchronization protocol using a shared variable v on which indivisible test-and-set \noperations are Then at least fi values of v areallowed. required to ensure both mutual exclusion and \nfree\u00addom from lockout. Furthermore, if each process has only a single state while being in its idle section \n(i.e. if it cannot remmeber any past experience) , then at least (N+l)/2 values for v are required. Proof: \nA generalization of the proofs given in [BFJLPI. The results stated so far in this section raise the \ngeneral question: Under what circumstances does randomization really help us to obtain better algorithms? \nThe above negative result shows that we cannot hope to save space by incorporating randomization into \nsynchronization protocols of the kind discussed in [B FJLp] and [Ral] (if we admit general fair schedules). \nWe will next give another negative results. Consider a synchronization process involving N ? 2 identical \nprocesses, all of which use a common variable v which they may read from, or write into, in one indivisible \nstep (all processes access v in exactly the same manner) . It is well known that no deterministic solution \nto this problem, that provides mutual exclusion, can exist. Surprisingly enough, an analogous proof shows \nthat a similar result holds in the probabilitisticcase as well: Theorem 3: There does not exist a probabilistic \nfully symmetric protocol that solves the above synchronization problem, and ensures both mutual exclusion \nand freedom from lockout with probability 1. Proof: Suppose that there exists such a protocol. We assume \nthat no lockout can occur with positive probability. Hence, if we start from a symmetric configuration \ni in which all processes are idle, and we let only one process (say kl) execute by itself and leave the \nother processes in their idle must eventually enter its critical tat: 1 section with probability 1. \nThis implies that 5 there exists at least one finite execution path II REFERENCES in which executes alone, \nand all other processes 1 remain idle, such that all transitions on II have [BFJLP] J. E. Burns, M. \nJ. Fischer, P. Jacksonr positive probabilities and such that k N. A. Lynch and G. L. Peterson, Shared \n II from its idle section to its critics i Sfl::o:a Data Requirements for Implementation of Let the product \nof the transition probabilities Mutual Exclusion Using a Test-and-set along II be p>0. Primitive , Proc. \n1978 International Conference on Parallel Processing, 79-87. Let us now execute the protocol as follows. \n Start at the symmetric configuration i , and [FR] N. Francez and FL ROdeh, A Distributed assume that \nall processes become active (i.e. get Data Type Implemented by a Probabilistic out of the idle section) \ntogether. Schedule pro-Communication Scheme , Proc. 21st Symposium cesses in a round robin fashion, \nletting each of on the Foundations of Computer Science them perform one atomic action at its turn. We \n(1980), 373-379. will construct a path (having positive probability) [LPS] D. Lehmann, A. Pnueli and \nJ. Stavi, such that at the end of each rou.ud,the configuration Impartiality, Justice, Fairness : The \nis again symmetric. Assume that this were the case Ethics of Concurrent Termination , Proc. at the beginning \nof some round r . Then each 8th International Colloquium on Automata, process is at the same internal \nstate and so Languages and Programming, Haifa, Israel performs the same action in its turn. If this (1981) \n. action does not involve randomization, then either all the processes in this round manipulate their \n[LR] D. Lehmann and M. O. Rabin, On the internal variables in the same way, or they all Advantages of \nFree Choice: A Symmetric and read the (same value of the) shared variable, or Fully Distributed Solution \nto the Dining they all write the same value into the shared Philosophers Problem , Proc. 8th Symposium \nvariable. Hence, in either case, their final con-on the Principles of Programming Languages, figuration \nis again symmetric. Note that in this Williamsburg (1981), 133-138. mode of scheduling, each process \nis not aware that [Ral] M. O. Rabinr N Process Synchronization byother processes are also executing, \nso that each a 4 log N -Valued Shared Variable , Proc. process will follow a sequence of states identical \n21st Symposium on the Foundation of Computex to those that kl passed by itself along the Science (1980) \n, 407-410. sequence II (as long as there is not randomization) Now suppose that the common action of \nthe processes [Ra2 ] M. O. Rabin, The Choice Coordination Problem , in round r did involve randomization. \nThen with Mere. No. UCB/ERL M80/38, Electronics Research some positive probability Pr > 0 , kl will make \nLab. University of California at Berkeley, the choice that will keep it in the sequence II , August 1981. \nand with the same probability each other process [Rsl J. Reif and P. Spirakis, Distributed will make \nan identical choice. Consequentlyr Algorithms for Synchronizing Interprocesswith probability PN (where \nN is the number of Communication within Real Time , Proc. processes) all pr~cesses would make this common \n13th ACM Symposium on Theory of Computing choice, and will again reach a symmetric configura\u00ad(1981), \n133-145. tion; moreover, in this new configuration each process is unaware of the existence of the other \n[IR] A. Itai and M. Rodeh, The Lord of the Ring, processes, and its state is the symmetric image of or \nProbabilistic Methods for Breaking the r-th state of kl on II . Symmetry in Distributive Networks , Tech. \nRep. RJ 3110, IBM, San Jose 1981. continuing in this manner, with probability PN>O all processes would \nenter their critical sections together after II steps, since kl does so and the behavior of the other \nprocesses remains symmetric to that of under this 1 particular execution sequence. This contradicts our \nassumptions, and so proves the theorem. Q.E.D.  \n\t\t\t", "proc_id": "582153", "abstract": "The asynchronous execution behavior of several concurrent processes, which may use randomization, is studied. Viewing each process as a discrete Markov chain over the set of common execution states, we give necessary and sufficient conditions for the processes to converge almost surely to a given set of goal states, under any fair, but otherwise arbitrary schedule, provided that the state space is finite. (These conditions can be checked mechanically.) An interesting feature of the proof method is that it depends only on the topology of the transitions and not on the actual values of the probabilities. We also show that in our model synchronization protocols that use randomization are in certain cases no more powerful than deterministic protocols. This is demonstrated by (a) Proving lower bounds on the size of a shared variable necessary to ensure mutual exlusion and lockout-free behavior of the protocol; and (b) Showing that no fully symmetric 'randomized' protocol can ensure mutual exclusion and freedom from lockout.", "authors": [{"name": "Sergiu Hart", "author_profile_id": "81100122916", "affiliation": "Tel Aviv University", "person_id": "P262168", "email_address": "", "orcid_id": ""}, {"name": "Micha Sharir", "author_profile_id": "81410596192", "affiliation": "Tel Aviv University", "person_id": "PP14149843", "email_address": "", "orcid_id": ""}, {"name": "Amir Pnueli", "author_profile_id": "81100648459", "affiliation": "Weizmann Institute of Science", "person_id": "PP15038449", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/582153.582154", "year": "1982", "article_id": "582154", "conference": "POPL", "title": "Termination of probabilistic concurrent programs: (extended abstract)", "url": "http://dl.acm.org/citation.cfm?id=582154"}