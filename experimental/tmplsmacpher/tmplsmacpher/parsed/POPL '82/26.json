{"article_publication_date": "01-25-1982", "fulltext": "\n COMPLETION SEMANTICS AND INTERPRETER GENERATION by Martin C. Henson Raymond Turner University of Essex \nContinuation Denotational Semantics is now a standard method of formally specifying a programming language \n(even ADA the language of the 80 s has almost succumbed). Completion semantics is the natural operational \nanalogue of continuation semantics. A continuation specifies what function the rest of the program computes; \na completion is a representation of this function. Continuation semantics contains higher order functions \nand is denotational whereas completion semantics contains only first order domains and is operational \n(i.e. referentially opaque). One of the important features of completion semantics concerns the structure \nof ita domain equations. They are, as we have said, first order in that the domain equations only employ \n+ and x (and * ) but no J-) . Despite this there ia a close correspondence between the domain equationa \nof the continuation semantics and those of the completion or first-order semantics. There is almost a \none-one correspondence between the domain equations. This close similarity makes the congruence proof \nquite straightforward. This is one reason why we advocate completion semantics so strongly: given a continuation \nsemantics it is a relatively straightforward matter to write down a completion semantics and prove it \ncongruent to the continuation semantica. Once this much has been achieved we can automatically generate \na correct and efficient interpreter for the source language. It is these two aspects of completion semantics \nwhich together provide a strong argument for viewing completion semantics as the standard operational \nsemantics. Our main interest is in semantics directed language implementation. We believe that the right \napproach to this topic concerns the development of general methods for systematically transforming denotational \nspecifications into operational models which are directly implementable. Completion semantics is, conceptually, \nmid way between continuation semantics on the one hand and useable interpreters/compilers on the other. \nMost of the burden of this paper is to offer evidence for this claim. Completion semantica is to serve \nas the intermediate stage in the formal development of usable interpreters. This paper is in five parts. \nThe firat deals with the denotational continuation semantics of a programming language (DEVIL) which, \nalthough quite small, contains moat of the features which force a wedge between denotational and definitiona. \nThe second part introduces the completion semantics for the OperatiOnal language and so provides an introduction \nto the techniques of completion semantics. The third part contains the proof of equivalence for the two \nlanguages. The fourth section deals with formal interpreter generation from such completion semantics \nand the last contains a compariaion with other work in the area of cOmpiler/interpreter generation. 1. \nDENOTATIONAL SEMANTICS To begin with we present the denotational continuation semantics of DEVIL. This \nis based on the expository language used by Strachey and Wadsworth in their paper which introduced continuation \nsemantics [13]. We first give the syntax definitions and then proceed to the semantics. This section \ncontains very little by way of explanation and presupposes that the reader has some knowledge of denotational \nsemantics cf. Stoy [12], Milne &#38; Strachey [7]. Permission tocopywithout feeallorpartof this material \nkgranted provided that the copies are not made ordktributed fordkect commercial advantage, the ACM copyright \nnotke and the title of the prrblicadonand itsdate appear, andnotice kgkenthat copying is by permission \nof the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or \nspecific permksion. @ 1982ACM0-89791-065-6/82/001/0242 $00.75 SYNTACTIC DOMAINS : IDE Identifiers G \n:cND -Commsnds E :EXP-Expressions SYNTACTIC PRODUCTIONS G ::= dummy G O;G1 lE := call E W.E!ZE resultis \nE E -> Go, Gl while Edo G . =KIoJ*o*~Im;Go;Im+l Gl; E ::= I true false o -> 1 Ez valof G ~G DENOTATIONAL \nSENANTICS SEMANTIC DOMAINS t:T l:L e :E =T+F+ C d: D=L+C+K v: V=T+F+C u: S=L->(VXT) :U=(IDE->D)XKP e: \nC=s->s k :K= E->C f :F= C->c ;Imti:Gn@ -skip -sequencing -assignment -call -jump -escape -conditional \n-iterator -block -identifier -true -false -conditional -block -procedure -truth values -locations -expressible \n-denotables -storables -stores -environments -command continuations -expression continuation -function \nclosures SEMANTIC FUNCTIONS c: CMD->U->C->C -commanda E :EXP->U->K->C -expressions CLAUSES FOR THE SEMANTIC \nFUNCTIONS cl. C[dummy] ~p@.@ C2. CIGO;GI] ~P~.C[GO]P{C[Gl]P8} C3. CII := E] = ~p8.E[E]p{update(p[I] )9} \nC4. C[call E] = ~p@.EIE]p{call E)} C5 . C[~E] =~p@.E[E]p{jump} C6. C[resultis E] = ~p@.EIE]p{result p} \n C7 . C[E -> GO,GI] =~pt3.EIE]p{cond(CIGO]pE),CIGl ]pe)} C8 . C[while E do G] =Ap@.fix(A&#38;J .E[E]p{cond(C[G]p@l \n,9)) . C9 . C[begin ~ 10, . . ..Im.GO;Im+l:Gl; . . .;Imti:Gn~] = )@hY.eocro* @O = CIGO]PO~l @l = C[Gl]Po@2 \n. en = c[Gn]poe where PO = p[<@l,...,en>l <lm+l,... , Imti>l [news m Ul<Io,...,Im>[l and = cr[news m \nmuse m] o El. EII] = ~pkm.k(deref(p[I] )u)d E2. E[true] = ~pk.k(true) E3. E[false] = ~pk.k(false) E4. \nEIEO -> El, E21 Apk.E[Eo]P{cond(E[Ellpk,E[E21pk)} E5. E[valof G] =~pk.CIG]p[klres]{fail} E6. E[proc G] \n=Apk.k(C[G]p) AUXILIARY FUNCTIONS map = Acr.~1.ul~l area =~<.Al.cTl$2 update = AdCI.Aecr.dEL -> El~dlL~lV,tru#,As \ncal 1 =~L3.Ae.eEF-> (eIF)@, ~ jump =~e.eEC-> elC,{fail} result = ~p.~e.(p[res]lK)e cond ~eO,@l.Ae.eET-> \n(eIT-> 80,e1), .Lc use =Av.v = O -> ,Lvl}use(v-l) news m u is a list of m distinct locations which are \nunueed in 6. p[d*lI*] = d?% = -> p, p[[hd*lhI*] td*ltI*] p[di I] =hI .(I= I ) ->d, p[I ] ~l*lv*] SV* \n= tl* I tv*] -> CT, u[hl*lhv*] cr[llv] =A1 .(l=l ) ->V, U[l ] NOTATION p[I] when I:IDE for p+ 1 I] p[res] \nfor p+2 h for }X.XJ.1 t for Ax.xtl p[klres]  {fail}:C -the failure continuation. Two aspects of the \ndenotational semantics are worth stressing -especially in relation to the completion semantics. To begin \nwith the domains of the continuation semantics employ higher order functions. Indeed, in some cases the \ndomains contain their own function spaces. For example, the domain F satisfies the following equations: \nF=C->C C=s->s S=L->(VXT) V=T+F+C.  Secondly, the semantic functions are referentially transparent in \nthat the denotation of a whole program is a function of the denotations of its parts. These are the two \nfeatures which force a wedge between denotational semantics and its first-order counterpart. 2. COMPLETION \nSEMANTICS Here we introduce the FIRST-ORDER semantica of DEVIL. In this definition of DEVIL all the domain \nequations involve only the domain constructors + , x (and *) but not -> . One particular problem for \nany such first-order semantics concerns the ability of DEVIL to jump at inconvenient moments and in particular \nto jump out from the evaluation of an expression. We handle this by the use of completions . These are \nthe data items which directly represent continuations (which are of course higher-order functions). Our \nfirst\u00adorder semantics is termed Completion Semantics to emphasise the analogy with continuations. To \nbegin with we present the semantic domains of our completion semantics. SEMANTIC DOMAINS t:T -truth values \n1 :L -locations e : E = T+F+ C -expressible values  d: D=L+C+K -denotable values v : V = T+F+ C -storable \nvalues. cr:S=(Lx V)* -stores P :U=(Idex D)*x K -environments 0 :C=Jx C+ ExK+ {fail}+ {final} -command \ncompletions k :K=({update} xDxC)+ ({call} x C) + { jump} + ({result} x U) + ({cond} x C x C) -expression \ncompletions  f :F=CMDXU -command closures :G=EXPXU -expression closures j :J=F+G -closures Before we \npresent the actual semantic functions a brief comparison with the domains of the continuation semantics \nwill be instructive. Notice firstly that the domains of expressible, denotable and storable values have \nthe same structure as the corresponding domains of the continuation semantics. This is characteristic \nof completion semantics. The domain F of procedure values is a higher-order domain in the continuation \nsemantics; in the completion semantics it is represented as a iclosure ! -a data item which consists \nof text to be evaluated together with an environment to evaluate it in. The domains of command completions \nand expression completions correspond to the domains of command continuations and expression continuations \nrespectively. Elements of C have the following forms: TEXTXUXC &#38; ExK  In other words, with the first \nform , we have some text to evaluate, an environment to evaluate it in, and a completion to evaluate \nnext. Such packages are usually referred to as program closures. The second form is best understood by \ninspecting the definition of the domain of expression completions: These consist of comma nd completions \nwhich expect a value before yielding a command continuation. The second form thus mirrors the fact that \nfor k:K and e:E, ke:C. The fact that more than one completion may occur in an expression completion allows \nfor the possibility that there may be several possible ways of completing the computation -the selection \ndepending upon the supplied value. As we shall see shortly the close relationship between the semantic \ndomains of the continuation and completion semantics has two obvious consequences. On the one hand the \nsemantic functions themselves are also going to be closely related, and secondly, the proof of equivalence \nis going to be straightforward, if a little tedious. We now turn our attention to the definition of the \nsemantic functions themselves. SEMANTIC FUNCTION FOR COMNANDS g c : CMD-> [U-> [C-> [S-> S]]] cl. C [dummy] \n=Ap9U.run(13,.@ C2. CIGO; GII ~Pe.C[GO]P{Gl,P,8} C3. CII := E] ~pe.EIE]p{update,p[I] ,41} C4. C[call \nE] =~p~.EIE]p{call,O} C5 . C[~ E] =~p(3.E[E]p{jump}  246 C6 . C[resultis E] =Ap13.EIE]p{result,p} c1 \n. C[E -> GO,GI] =~pe.EIE]p{cond,{Go,p,@},{Gl,p,O} } C8 . C[while E do G] =Ap9&#38;.run(Y[~(3 .{E,p,{cond,{G,p,(3 \n} ,13}}],er) C9 . C[begin var 10, . . ..lm.GO;Im+l:Gl; . ..;Im%:Gn@] = ~p13U.run(eo,uO) where 80 = {Go,PO,eI} \n1 = {Gl,Po,e2} . . en= {Gn>Po>@l g Po = P[<@o, . . ..@n>l<Im+ly...JIm~ >][new m &#38;I<Io,...,Im>] and \n~. = &#38;[new m Uluse m] where new m cft and %aetv etc have the same meaning as in the denotational \nsemantics. SENANTIC FUNCTION FOR EXPRESSIONS E: EXP-> [U-> [K-> [S-> s111 El. EII] =~pk&#38;.run(send(k,deref(p[I])&#38;)6) \nE2. E[true] =Apke.run(send(k,true) ,&#38;) E3. _=~pkcr.run(send E[falae] (k, false),cr) E4. EIEO = Jpk.EIEolp{cond, \n{El,p,k}, {E2,p,k}} -> 1 E21 E5. EIValof G] =~pk.CIG]p[kl~]{fail} E6. _G] =~pkcr.run(aend(k~,p)) ,IY)E[proc \nlD> lookup ~,up = -> hhp = I -> thp, Lookup(tp,I) deref =Adu. d:L -> contents(dlL)m,d where contents \n=Jlcr.a= -> Av, hhcr= 1 -> thu, contents(t~,l); ~ {fail} is the failure completion. {final} is the initial \ncompletion. NOTATION p[I] when I:IDE for lookup(I,p~l) p[~] for p$2 p[kl~] for~l,k> AUXILIARY FUNCTIONS \nsend :KxE->C send =Ak,ek,k> run :Cxs->s run(e, u) = r if tl={final } C[G]p$3 if @={G,p,@ } E[E]pk if \nEl={E,p,k} run((l , ~d/e]) if k={update, d,e } C[G]P@ if k={call,13 } &#38; if e=<G, p> run(tJ , &#38;) \nif k={jump} &#38; if ~{e, k} if e=e~ run(send(p [res], e), cr) if k={result, p} I run(e , cr) if e=true \nI run(e ,~) if e=false J if e:T J otherwisek Once again the close similarity between the continuation \nand completion semantic functions is quite stunning. Let us consider a couple of clauses. The clause \n(C2), for example, is identical to the clause for the continuation semantics, except that the continuation \nC[Gz]p(j is replaced by the cOmPletiOn {G2,p,0}. The clause (C8), for the while-loop, is also worthy \nof special attention. Notice how the fixed\u00adpolnt is now over the domain of completions rather than continuations \n-otherwise the clauses are very similar. Indeed, since C[E -> GO,Gl]plb= run({E,p,{cond,{Go,p,@},{Gl \n,p,e}}},cr) the continuity of run gives immediately the expected equivalence: rr while Edo G7= E -> \nG; while E do G, =1 . Completion semantics is just the first-order counterpart of denotational semantics. \nAs a consequence of being first-order completion semantics is operational in that the semantic function \nis no longer referentially transparent. The naturalness of completion semantics suggests that we view \ncompletion semantics as the standard operational semantics -the standard against which actual interpreters \nand compilers are to be meaeured. We shall have more to say about this in the next two sections. THE \nEQUIVALENCE PROOF The continuation semantics of DEVIL is denotational and employs reflexive domains which \ninvolve -> . Completion semantics, on the other hand, is operational and first-order. The proof of equivalence \nis, as a consequence, in two parts. In part one we prove that the completion semantics is an approximation \nto the continuation, and in the second part we prove that the continuation semantics is an approximation \nto the completion semantics. The proof offered here is only a sketch which, however, should be sufficient \nto enable the reader to carry out the details. The structure of the proof for the congruence between \nany continuation and completion semantics will have an identical structure to the one presented here. \nThere may be more or leas clauses involved in the inductions but that is the only real difference. The \nstructure of the proof offered will serve as a paradigm for any such proof relating continuation and \ncompletion semantica. NOTATION We adopt the Diacritical Convention of Milne [7]: Acute and grave accents \nare used to distingui~h, elemen~s~and d~m$i?s of the continuation semantics and completion eemantics \nrespectively. For ~:fi and w:W we write w:W for<w,t.kW x ~. To achieve the first part of the proof we \nneed to introduce functions which map the domains of the completion semantics to those of the continuation \nsemantics. EMBEDDING FUNCTIONS (1) Tval(~[;) in,;, Fun(c?l?~ in E,, Ccon(~[C) in E, (2) Loc(~l~j in \n(,,  Ccon(~l:) in D, Kcon(dlK) in 6, (3) Tval~;J;) in,;, Fun(vlFj in V,, CcOn(~lC) in V, (4) En(p) \n=(~1.Dval(Lookup(I,~)),Kcon(pJ2)>  \\ (5) St : s >{ St(a) =~1.Vval(cOntents(~)~)  (6) Fun : >->; Fun(G,p) \n=~~.C[G]En(p)f)  (7) Ccon :; -> f Ccon[fina:] = ~;.; Ccon[G,~,8] = ~[G]En(~)Ccon(~), Ccon[E,~,~] = E[E]En(3)Kcon(l?), \n * (8) Kcon : ~--> ~-.  .. Kcon[updat~,l, (3] = upzd~te(l)Ccon(9), Kcon[ca~l, e] = ca:l(Ccon(@)), \nKcon[j~mp] = jum], \u00adKcon[re~ult,~ ] = resu-lt(~n(p)),  Kcon[ c~nd,{Gl,~,~},{G2,~,~}] = Cond[C~G ]En(~)(Con(~),~~G \n]En(j)Con(~)/], Kcon[ c~nd,{El,~,k},{E2,fi,l?}] = C%d[E[~l]En(~)Kcon(~),E[~2]En(~)Kcon(k) ], 1, AK Our \nfirst result can now be stated using these functions. Theorem 3.1 VG,E,$,@,?,i? (1) Sto(~[G]~;) ~~~G]En(~)Ccon(~)Sto(;) \n (2) Sto(E[)G@~~) ~EIE]~n(~)K&#38;on(~)Sto(~)  (3) Sto(run(@,;)) ~Ccon(9)Sto(&#38;) The proof of this \nresult is by (simultaneous) fixed-point induction on the definitions of C, E, and run -these are defined \nby simultaneous recursion and are therefore the fixed-points of certain functional. In fact the only \ntwo minor complications require a sub induction (fixed-point) to show that Ccon and En behave over the \ncompletion and environment of clauses C8 and C9 respectively. The details are straightforward but tedious. \nWe cannot obtain this result by structural induction because the semantic functions C, E etc are not \nreferentially transparent. Nor can we obtain the converse result of theorem 3.1 by fixed point or structural \ninduction. Essentially the problem is caused by the fact that the semantic functions of the denotational \nsemantics are to be found in the definition of the embedding functions. This causes .s circularity in \nthe argument if one attempts the converse of Theorem 3.1. (see Stov---[101 for a fuller discussion of \n~hese issues).- The converse result is a little more tricky. This time instead of functions we must be \nsatisfied with INCLUSIVE predicates which relate our two sets of domains. PFU!DICATES A A A.\\ (1) T[~] \n= ::: &#38; ict (2) L[l] = l:L &#38; ~~? (3) E[~] = T[~] V F~~] V C[$] (4) D[t] = L[$] V C[dl V K[~l \n (5) V[$] = T[$] V F[$] V C[?] (6) U[$l = ~:fi &#38; /\\{D[$[Ill I I:Dom~} &#38;K[b2]6 (7) C[8] = $3:; \n&#38; /\\{S[6$,~$] I S($)} vBottom(El) (8) K[l?l = t:K &#38; /\\{C[fi6,~$] I E[$]} vBottorn@ (9) S[f] \n= ~:$ &#38; /\\{V[6~,~~] I ar~a($,~) &#38; L[l~} v Bottom($) (10) F[f] = ?:? &#38; /\\{C[f~&#38;~~] I \nC[@]} v Bottom(f), where Bottom(x) =~=~ vk=~ => x=L&#38;>=~  Table A Notice the inequality in the predicate \nT . We cannot (using known techniques) establish the existence of stronger predicates (i.e. ones with \nequality). If we could the first half of the congruence proof would not be , necessary. Notice that for \nclarity.~e have omi$t~d m-~y project:o?s in T~ble A. We have also adopted a systematic abuse of notation \nin writing ke for send(k,e), W for run(e,u) and p42 for <p4.2,pJ2>. ,, On, Che assumption that the predicates \nof table A exist and are inclusive we can prove the following. The proof is by structural induction (simultaneous) \non E and G. Theorem 3.2 AAAA VG,E,p,9,k,r A h h If U[~~ &#38; C 8] &#38; K~lcl &#38; S[cr] then (1) S[~[G]f \nb $,t[G]p&#38;T] (2) SIEIE]fil%,~[E]~;~ There is a rather large snag with the predicates defined in \nTable A. These are not monotonic and so their existence cannot be established by a direct appeal to Tarski \ns theorem. Milne [7] provides us with a proposition which shows that certain directed sets of non-monotonic \nfunctions on directed sets of structured retracts admit appropriate least-upper-bounds. Moreover, he \ndescribes a mapping from non-monotonic functions to non-monotonic functions which becomes a PREDICTOR \nwhen it preserves certain constraints ( which in fact form the definition of the ordering which directs \nthe sets mentioned above). A predictor for some functor, based on some primitive inclusive function> \ngenerates a directed set under the appropriate ordering. The existence of predictors therefore implies \nthe existence of the required limits. He also describes a variety of constructions f 0 r sums, products, \nlists, and function spaces of non-monotonic functions which provide predictors when the domains of interest \nare similar in shape (Milne [7] 2.5.5 P.362). Unfortunately the constraints are very severe and often \npredictors will not exist by appealing to theee constructions. In our case the predicates fall way outside \nMilne s constraints. Indeed , we have noted that we cannot, using known techniques, justify the stronger \npredicate at all. Generalisation of Milne s techniques are, as far as we know, not yet forthcoming. We \nhope that Ccmplecion Semantics will be an appropriate tool in developing these generalisations; combining \npresisely the concepts (completion, closure) which cause the problems without, as far as possible, disturbing \nthe shape of the domains. The style of proof we use here , however, is due to Stoy [12]. The strategy \nis to define approximations to these predicates, which are monotonic, and then define predicates which \nsatisfy the conditions laid down in Table A as the limits of these approximations. The approximations \nare defined as follows: DOMAINS AND KELATIONS F, =T+F+C Dn =L+Cn+Kn v; =T+F;+Cn n s =L->(V XT) s = {~} \nn+l = c s -> Sn c: ={1} n+l K En -> Cn KO= {1} n+l = F Cn-> c: F. ={J.} *+1= n A Co[$l = true; Ko[k] \n= true; SOICT] = true. h E [~] <=> T[# v F [!] V C[~] Dn[~ <+ L[dl V Cn[$l v Kn[$l V:[$] ~> T[$] V F~[$] \nV C~[?] c *+1[$] 4= :n+l: t &#38; /\\{sn[;;,;:l I S*(2)] Our required predicates are then given by ;he \nfollowing definitions: (i) E[2] ~ef (Vn)(En[en]) (ii) D[!] ~~ef (~)(Dn[$n])  (iii) V[$] +~ef (f%)(Vn[tnl) \n(iv) C[$] <=~ef (Vn)(Cn[$nl) (v) K[$] ~~ef (Vn)(Un[;n]) (vi) S[:] ~ef (Vn)(Sn[$n])  Table C Theorem \n3.3 The predicates E, D, V, C, K and S defined by Table C satisfy the conditions stipulated in Table \nA. In order to prove this result we need to establish the following results. In 3.4, 3.5 and 3.6 let \nP be n any of the predicates En, D , Vn, Cn or K and x be in either E, D, V, C or K as appropriate. The \nfollowing resul!s are establish ~ by induction (simultaneous for each P) on n. Lemma 3.4 For each ~ in \nthe appropriate domain (i) Pn[!l => Pn+l[tnl (ii) Pn+l[~] => Pn[tnl Corollary 3.5 For each % in the \nappropriateAdomains, if there exists an n such that Pn[$$] then P[xn]. Lemma 3.6 Each of the predicates \nPn is inclusive. As a direct consequence of this we see that each of the predicates in Table C is inclusive. \nThese results enable us to prove theorem 3.3 rather easily. We consider each of the predicates in Table \nC one at a time and show they satisfy the constraints imposed in Table A -one direction requires the \nuse of 3.5 and 3.6 (left to right) the other direction requires the use of 3.5 only. So far so good but \nhow can we put these results 3.1 and 3.2 together to yield a congruence proof? The following definition \nseems most natural: ~ A A For eac~ 13:C, ~:K, ~:S e:E, d:D, ~:V agd ~:U define (i) $ =c g ~> C[t] &#38; \nCcOn(~) ~g (ii) k =~ ~ ~> K[fi] &#38; Kcon(k) =k  (iii) ~ ES u-e> S[$] &#38; StO(&#38;) =6 (iv) ~ e \n4=> E[~] &#38; Eval(~) E: (v) ~ =: ~ -> D[~] &#38; Dval(d)sd (vi) % = v 4=9 V[~] &#38; Vval(;) Z: \n (vii) ~ =; ~ <=> u[p] &#38; En(fi) =$ Table D These notions of equivalence are quite satisfactory from \na computational point of view, and really all we Should expect. To illustrate consider the congruence \nrelation for continuations/completions and the congruence relation for states -these follow directly \nfrom the definitions. As a direct consequence of 3.1 and 3.2 we obtain the following congruence result \nfor our semantic specifications. Theorem 3.7 A ForeachE, G,~,~,$ ,CT satisfying the appropriate congruences \nin Table D we have (i) ~[E]~f; = ~[E]~~~ (ii) C[G]fif% =s C[G]@~. The proof has been rather longsand \nlaborious. The reason for the complications are intuitively not hard to find. We are trying to relate \ninfinitary abstract objects associated with the denotational definition with their First-order or finitary \nrepresentations. Stoy [12] and Milne [7] contain a full discussion of the issues. Our purpose here is \nsimply to argue that a standardisation of the operational semantics along the lines sugges ted here affects \na corresponding standardisation of the congruence proof. Indeed, the great benefit of the completion \nsemantics arises because the domains of the two semantic specifications are closely related . One consequence \nof this concerns the structure of the congruence proof. It is relatively easy to define the embedding \nfunctions and predicates; where the domains are vastly dissimilar this becomes a truth which stretches \nhuman ingenuity to the full. 4. FORMAL INTERPRETER GENERATION It is possible to view the completion \nsemantics as an implementation of the language in a weak sense. Apart from the obvious inefficiencies \nassociated with copying environments, and so forth, we have still retained the use of a fix-point operator \nand consequently the completion semantics manipulates infinite structures, albeit of a rather harmless \nnature. Most crucially it is the fact that the completion semantics is referentially opaque, and therefore \noperational, that allows us to view the completion semantics as an interpreter. Proofs of congruence \nbetween operational semantic specifications are established by fix point induction and utilise continuous \nfunctions to relate the domains. Others [12] [7] seem to believe that such proofs are, generally, more \ndifficult than those like theorem 3.7 which relate a denotational to an operational specification. Whilst \nwe do not wish to argue against this in general, since the very free hand one has in operational semantics \nmakes it impossible to classify all styles of specification, we do claim that for a class of styles the \nproofs are very much easier. Furthermore, these styles are sufficient to express all that we wish to \nsay about interpreters and their implementation. Indeed one can abstract the computational structures \nof the styles and show that congruence follows when the definitions obey simple constraints [2]. This \nopens up two fruitful avenues: Firstly, given an interpreter, we can utilise these constraints to induce \na congruent completion semantics. From here we can prove the interpreter correct by proving the completion \nsemantics congruent to the denotational semantics for the language concerned. In other words, we reduce \nthe conceptual distance between the interpreter and the refernce semantica to make the proof easier. \nIn fact we have an interpreter for DEVIL which we have proven correct in this way (Henson [3]). Secondly, \ngiven a completion semantics we can, systematically and correctly, induce an interpreter which is efficient. \nThis is more difficult than proceeding in the other direction because we have to add information when \nmoving from the abstract to the concrete. It would seem sensible to proceed in small stages; introducing \ncomputationally plausible extentions until we reach our goal. For the language DEVIL we might proceed \nas follows: Firstly, we should separate the state changes brought about by the definition from the control \n issues. In other words our new specification would involve two parts; a state to state function, and \nan iterator which applies the former reiteratively until some termination condition holds. From this \nmoment we can ignore the iterator. Secondly, we seperate out the partial results of expressions from \nthe completion by introducing a stack, or workspace. Thirdly, we observe that the environment changes \nin a very systematic way at block entry and exit. This allows us to seperate a local environment from \nthe completion. 252 Next, we observe that the previous two transformations have put distinct layers into \nthe completion corresponding to local and various global computational contexts. We seperate the local \ncomputation from the completion. The specification now resembles the SECD-machine [13] with the completion \nacting as dump or global context. Then we adopt a particular target machine and transfom, finally, to \nthis. In the process we remove the infinite objects and replace them by circular data-structures. We \nbeleive that the normalisation of the fix-point structures is best left to this stage as it involves \nthe concept of pointer (and therefore location ) and it not until this last stage that we have a coherent \nuse of locations. Also at this stage the primitive valuations of earlier specifications are transformed \ninto short sequences of machine ins truct ions. We have made the penultimate stage an abstract version \nof the virtual machine we wish to simulate on the target machine. This buffers the transformations, up \nto this stage from details of the target machine and issues of storage allocation. Moreover, because \nwe are effectively showing how to correctly and efficiently implement that cluster of concepts (continuation, \nstore,environment, etc.) which we know to be sufficient to describe sequential programming languages, \nwe have transformations which are stable with respect to choice of source language. 5. COMPARISON WITH \nOTHER WORK There are now several compiler/interpreter generation systems discuaaed in the literature. \nThe system known as S1S produces implementations automatically by simply implementing the semantic metalanguage \n(LAMC), that is to say, by treating semantic specifactiona as programs for a simulated lambda machine. \nThis approach is simple and general, but is far too inefficient to be practical. A second approach originally \nannounced in Raskovsky and Turner [9] and continued in [1OI is much more realistic in that it produces \nan efficient compiler directly from the denotational specification. It seems less general than the Mosses \napproach but it is not clear at this point exactly its limitations are as regarda generality. A more \ntelling point for the the system in [9], [10] concerns the question of correctness for this compiler \ngenerator. The best we can do, it seems, is to relate the generated compiler (viewed as an operational \ndefinition) to the original denotational definition. But the proof of congruence is very hard since the \ndomains of the two definitions will, in general be very dissimilar. For each language one has to relate \nan operational and denotational definition which are very different as regards their domain structure. \nThe strength of our system concerns the ease of these proofs; the domains are so closely related that \nwe can almost mechanise them. The reader should again compare the domains of the continuation and completion \nsemantics. A third approach worthy of mention is Jones &#38; Schmidt [5]. These authors describe a technique \nfor generating provably correct compilers from denotational definitions. The system (initially) produces \ncompilers into STM code where an STM or state transition machine is a flow-chart-like program, low-level \nenough to be t rana la ted into efficient code on conventional computers. The object programs produced \nare inefficient and so the second phase effects an optimization. This work is quite close to oura in \nthat the STM -compiling a theme a involve a process of defunctionalisation. However, the correctness \nproofs offered by the authors only show that computation by a translated LANC expression faithfully simulatee \nbeta-reduction. They are not concerned to establish that the compiler viewed as an operational specification \nis congruent to the original denotational definition. This appears to establish less than our congruence \nproofs. Indeed, they seem to be arguing (if implicitly) that the meaning of the source language (for \npurposes of the proof of correctness of the generated compiler) is given in two phases, viz. a translation \ninto the language LANC plus beta\u00adreduction. In other words, they are proving correctness with respect \nto an operational definition of the source language. 6. BIBLIOGRAPHY [1] Gaudel. M. C. of as Abstract \nData .\u00ad Specification Compilers Type Representationa ii [3], 1980. [2] Henson, M. C. Some Observations \nConcerning Congruence Proofs for the Operational Semantics Description of Programming Languages , CSM-36, \nUniversity of Essex, 1981. [31 Henaon, M. C. Proofs for Interpreters by Transformations of Semantic \nSpecifications , CSM-37, University of Essex, 1981. [4] Jones, N. (Ed.) Proc. of Aarhus Workshop on \nSemantics-directed Compiler Generation, Lect. Notes in Computer Science 94, 1980. [51 Jones, N. &#38; \nSchmidt. Compiler Generation from Denotational Semantics , in [3]. [6] Landin, P. The Mechanical Evaluation \nof Expressions , pp.308-20 Computer Journal, 6. 1964. [7] Milne, R. E. &#38; Strachey, C. A Theory of \nProgramming Language Semantics , Chapman &#38; Hall, 1976. [8] Mosses, P. D. Compiler Generation using \nDenotational Semantics , Proc. Symp. on Math Fnds of Computer Science, Gdansk, Lect. Notes Comp. Science \n45, 1976. [9] Rakovsky, M. 6 Turner, R. Compiler Generation and Denotational Semantics , Fundamentals \nof Computation Theory, 1979. [101 Raskovsky, M. &#38; Collier, P. From Standard to Implementational \nDenotational Semantics , Lecture Notes in Computer Science 94, Springer Verlag, 1980. [11] Reynolds, \nJ. Definitional Interpreters for Higher-order Programming Languages , Proc. 25th ACM Nat. Conf., 1976. \n[12] Stoy. Denotational Semantics , MIT Press, 1977. [13] Strachey, C. &#38; Wadsworth, C. P. Continuations, \na Mathematical Semantics for Handling Full Jumps , PRG-11 Proj. Res. Group, Oxford, 1974. [14] Turner, \nR. Continuation Semantics, Completion Semantics and Congruence Proofs , CSM-40, University of Essex, \n1981.  \n\t\t\t", "proc_id": "582153", "abstract": "", "authors": [{"name": "Martin C. Henson", "author_profile_id": "81100301162", "affiliation": "University of Essex", "person_id": "P192255", "email_address": "", "orcid_id": ""}, {"name": "Raymond Turner", "author_profile_id": "81100229462", "affiliation": "University of Essex", "person_id": "PP36025720", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/582153.582180", "year": "1982", "article_id": "582180", "conference": "POPL", "title": "Completion semantics and interpreter generation", "url": "http://dl.acm.org/citation.cfm?id=582180"}