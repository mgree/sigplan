{"article_publication_date": "01-25-1982", "fulltext": "\n Eliminating Redundant Object Codel Jack W. Davidson$ Christopher W. Fraser Department of Computer Science \nUniversity of Arizona Tucson, AZ 85721 Abstract Compilers usually eliminate common subexpres\u00adsions in \nintermediate code, not object code. This reduces machine-dependence but misses the machine\u00addependent \ncommon subexpressions introduced by the last phases of code expansion. This paper describes a machine-independent \nprocedure for eliminating machine-specific common subexpressions. It also identifies dead variables, \ndefines windows for a com\u00adpanion peephole optimizer, and forms the basis of a retargetable register allocator. \nIts techniques for han\u00addling machine-specific data should generalize to other optimizations as well. \n1. Introduction Most implementations of common subexpression elimination ( CSE ) accept intermediate \ncode such as triples or quadruples, not object code. This simplifies implementation but may sacrifice \ncode quality because the eventual expansion of the intermediate code may introduce new common subexpressions. \nFor example, expanding address calculations often creates redun\u00addant machine-dependent subexpressions \nthat cannot be eliminated in the intermediate code because they do not appear until after the final stages \nof code genera\u00adtion. This problem may be attacked by carefully tailor\u00ading the intermediate code to the \nmachines at hand, but this complicates the design of the intermediate code and jeopardizes its machine-independence. \nTThis work was supported in part by the National Science Foundation under Grant MCS-7802545. $Present \naddress: Jack W. Davidson, Dept. of Applied Mathematics and Computer Science, University of Virginia, \nCharlottesville, VA 22901 Permission to copy without fee all or part of this material is granted provided \nthat the copies are not made or distributed for direct commercial advantage, the ACM copyright notice \nand the title of the publication and its date appear, and notice is given that copying is by permission \nof the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or \nspecific permission. @ 1982 ACM 0-89791-065-6/82/001/0128 $00.75 This paper describes two programs called \ncacher and assigner that implement CSE and register allo\u00adcation on object code for an optimizing Y compiler \n[Hanson]. They treat machine-specific instructions, but they do so in a machine-independent fashion. \nBy generalizing an existing CSE algorithm [Freiburg\u00adhouse, Sites], cacher attacks the problems described \nabove, identifies dead variables, and defines windows for a companion peephole optimizer [Davidson]. \nWith assigner, it implements register allocation. The current implementation operates locally, but its \ntech\u00adniques for handling machine-specific data should gen\u00aderalize to global optimizations as well. 2. \nRegister Transfers Like many recent retargetable code generators and optimizers [Cattell, Davidson, Glanville], \ncacher represents instructions using ISP-like register transfers [Bell]. For example, the instruction \nsequence r[l] = r[l] + m[a] m[b] = r[l] PC = if cc s Othen loop else pc adds memory location a to register \n1, stores the result in memory location b, and jumps to loop if register cc exceeds zero. The code generator \nemits these instead of equivalent assembly or machine code, Because cacher and assigner assume responsibility \nfor register allocation, the code generator may assume an infinite supply of registers. assigner maps \nreferences to non\u00adexistent pseudo-registers (like r[l 000]) onto real regis\u00adters. This is a common technique \nfor simplifying code generation [Chaitin, Gries], 3. cacher cacher symbolically simulates register transfers \nand records the values that they store, When it encounters one that recomputes an existing value, it \nedits the instruction stream to reuse the value stored earlier. To cacher, a block is a section of code \nwith exactly one entry. Two symbolic expressions (or s-exprs ) a 128 and b are said to trrarch at a given \npoint in a block if and only if the block has set variables so that a and b have the same value at that, \npoint. For example, after the sequence r[l] = m[a] r[l] = r[l] + m[b] r[l ] matches m[a]+m[b]. One s-expr \nis said to be older than another at a given point in a block if and only if it was computed earlier in \nthe block. For example, after the instructions above, m[a] is older than rn[a]+m[b], which is older than \nr[l ]. cacher reads a block at a time, processes it, and emits the simplified code. As it advances through \nthe block, it maintains a cache of the s-exprs that are com\u00adputed. It partitions this cache into equivalence \nclasses induced by the equivalence relation tnatch, and it sorts the s-exprs in each equivalence class \nby age, oldest first. The cache associates each active s-expr with the list of s-exprs it now matches, \nmaking it easy to identify incoming s-exprs that have been computed already. The procedure follows: 1. \nSplit each register transfer into two parts. Call its left-hand side d.~[ and its right-hand side src. \n 2. Find the oldest expression that matches src. That is, for each register that appears in src, replace \nthe register in src with the oldest value in the register s equivalence class. Call the resulting string \ncsrc. 3. Find the oldest expression that matches dsf s address. First, strip off the leading name and \nouter\u00admost brackets, if any, exposing the address calcula\u00adtion. Call this string addr. Replace each register \nin addr with the oldest value in the register s equivalence class. Call the result caddr. Restore the \noriginal name and outermost brackets to caddr, and call the result cdst. 4. Find the cheapest replacement \nfor src and addr. If src has been computed before then some previous instruction has had the same csrc \nand has entered it in the cache (see Step 5). Thus the cheapest expres\u00adsion for this src is the cheapest \ns-expr in csrc s equivalence class. If a cheaper expression for src is found, it replaces src in the \noriginal register transfer. Otherwise, src is not disturbed. A similar, procedure replaces addr with \nits cheapest equivalent. Cheap\u00adness is determined by a machine-dependent function described below. 5. \nUpdate the cache to reflect the current instruction s change to dst. Deletions are needed because this \ninstruction changes cdst and thus invalidates s-exprs that use it. Insertions are needed because this \ninstruction forms a new equivalence, between csrc and cd.~f. First, find m-c s equivalence class. If \nno equivalence class contains csrc, create one. Next, delete from the cache every s-expr with which cdst \ninterferes. Finally, add cdst to the equivalence class  found for csrc above. cdst s new home is identified \nbefore the deletions because it cannot be found later if csrc is among the deletions. Interference is \ndeter\u00admined by a machine-dependent function described below. cacher also creates and maintains use lists, \nwhich link the instructions that use each particular s-expr. When an instruction is first encountered, \nit is added to the use lists of each s-expr it uses. When an instruction is changed to use a cheaper \nreference, it is removed from the use lists of the s-exprs it had used, and it is added to the use lists \nof the new s-exprs that it now uses. When a register s use list becomes empty, the instruction that loaded \nthat register is deleted and removed from the use lists of the s-exprs it had used. This may trigger \nfurther deletions and removals, recur\u00adsively. This prompt removal from use lists assumes that the code \ngenerator uses each temporary just once. This simplifies management of temporaries for all par\u00adties yet \nsacrifices nothing because the code generator may use as many registers as it needs. Finally, cacher \nmay be asked to keep some values out of registers. For example, the PDP-11 has so few allocatable registers \nthat it may be better to save them for values harder to access than, say, constants. Code generators \nimplement this by marking instructions that load such values. cacher treats marked instructions like \nall others, except that it will not reuse their cdsts in step 4 above. A marked instruction may, however, \nbe deleted if it provides input to a larger redundant com\u00adputation. Though simple, cacher does several \noptimizations at once. Among these are redundant load elimination, common subexpression elimination, \ndead-variable identification, and peephole definition. 3.1 Redundant Load Elimination Since register \nreferences are cheaper than memory references, the algorithm above will remove loads. For example, a=c \nb=c+l might compile into code that begins r[l] = m[cl m[al = r[l] r[2] = m[cl .. The first two instructions \ncache three s-exprs r[l 1, m [a], and m [cl which form one equivalence class. As the third instruction \nis processed, dst and cdst become r[21, and src and csrc become m [cl. There is an equivalence class \nthat contains m[cl, and the cheapest member of this set is r[l 1, so the third instruction is changed \nto 129 r[2] = r[l] eliminating a redundant load of memory location c. This example exposes one of cacher \ns assumptions. In theory, a machine might have instructions for load\u00ading from memory but not for inter-register \ntransfers like the one above. Thus, in theory, cacher should use the peephole optimizer s instruction \nchecker [David\u00adson] to verify the legality of its changes. ln practice, most machines allow register \nreferences to replace memory references, so instruction checking has not yet proven necessary. 3.2 Common \nSubexpression Elimination Deleting unused instructions eliminates common subexpressions. For example. \nthe code a=c+3 b=c+s might compile into 1. r[l] = m[c] 2. r[2] =3 3. r[l] = r[l] + r[2] 4. m[a] = \nr[l] 5. r[3] = m[c] 6. r[4] =3 7. r[3] = r[3] + r[4] 8. m[b] = r[3]  When cacher processes instruction \n6, it recognizes that r[2] already holds 3, so it changes the instruction to use the cheaper r[2] and \nadds it to r[2] s use list. It then processes instruction 7 and discovers that the sum m[c]+3 is already \navailable in r[l ]. It changes the instruction to use the cheaper reference, adds the instruction to \nr[l ] s use list, and removes it from the use lists of r[3] and r[4]. This empties these lists, so instruc\u00adtions \n5 and 6 are deleted. It then turns to instruction 8, replaces r[3] with the cheaper r[l ] (older s-exprs \nare the cheaper of equals), deletes the now-unused instruction 7, and yields the program: 1. r[l] = m[c] \n 2. r[2] =3 3. r[l] = r[l] + r[2] 4. m[a] = r[l]  8. m[e] = r[l] Common subexpressions are usually \neliminated at a higher level, but machine-level CSE can do more because all values are exposed at this \nlevel. For exam\u00adple, address calculations often require code to multiply indices, shift offsets, or add \nframe pointers. Expanding similar address calculations may create redundant mul\u00ad f(a[i]) ... i = i*2 \n might generate the machine-independent postfix code push i push a index call f ... push i Push 2 mul \npop i There are no obvious common subexpressions in this code, but expanding the index for byte-addressed \nmachines may form the same product (or shift result) computed by the mul. It is hard for conventional \ncode generators to catch such common subexpressions without introducing machine-dependencies into their \nCSE code. Also, some instructions have side effects (e.g., divi\u00adsions often yield a remainder as well \nas a quotient) that cannot be used in higher-level CSE because they do not appear at a higher level. \nSimilarly, expanding com\u00adparisons requires, on some machines, a subtraction fol\u00adlowed by a comparison \nwith zero. The difference may be redundant, but it cannot be recognized as such at a higher level because \nit does not appear until after the machine-dependent stages of code expansion. The situations that create \nmachine-specific common subex\u00adpressions are ad hoc and hard to enumerate, but they occur nonetheless. \n 3.3 Window Definition cacher also defines windows for a peephole optim\u00adizer. Most peephole optimizers \nuse a fixed window and thus consider many pairs that cannot combine and miss many valid combinations \nmerely because the instructions are not adjacent. Scanning for more dis\u00adtant candidates [Wulf] may consider \nmany instructions that are unlikely to combine. cacher exploits the observation that many peephole optimizations \ncombine an instruction that sets some cell with the next instruction that uses itl . cacher employs its \nuse lists to link such instructions for its companion peephole optimizer, which tries combining only \nlinked instructions. This improvement on the fixed window typically makes the peephole optimizer run \n30~0 faster and yield code that is 2070 shorter. Code inspection uncovers few missed peephole optimi\u00adzation, \ntiplications, shifts, or additions that cannot be elim- TWhen the cell is the program counter, it K next \nused by the instruc\u00adinated earlier because they do not appear earlier. For tion after the branch and \nby tbe instruction targeted by the branch. example, the source program This definition of program counter \nwe, aIIOVA p~eph~lcto ~pt,rn,~e~~ collapse branch chains and eliminate unreachable code [Davidson]. 130 \ntogether to allow, for example, a naive code generator 3.4 Dead-variable Identification cacher records \nwhere cells are last used, so it passes this information to the peephole optimizer and the register assigner, \nwhich can better combine instructions and assign registers if they know where cells die. Com\u00adpilers usually \nidentify dead variables earlier, but just as machine-level CSE permits a few new optimizations, so does \nmachine-level dead-variable analysis. For exam\u00adple. many calling sequences return function values in \na fixed register. After the function return, the calling sequence often moves the value to another register, \nin case the special register is needed again. If it is not needed again, this move will prove unnecessary. \nCon\u00adventional compilers identify such avoidable moves deep in a machine-dependent code generator. cacher \nidentifies such moves with a far more general opera\u00adtion. 4. Register Assignment assigner maps the pseudo-registers \nonto the real registers. It assigns a real register to each pseudo\u00adregister, and it replaces each use \nof the pseudo-register with the associated real one. It frees the real register when the pseudo-register \ndies. When the demand for hardware registers exceeds the supply, assigner allocates a temporary, saves \nthe contents of the least-recently-used hardware register, and frees it for use. The LRU replacement \npolicy is sub-optimal [Freiburghouse], and the new loads and stores could introduce inefficiencies, but \nthese shortcomings have not yet earned attention: the 3500\u00adline Y compiler is compiled using only 42 \nregister spills for the PDP-11 (with three allocated registers) and none for the DE Csystem-10 (with \ntwelve). The register assigner also translates the register transfers to assembly code. It uses a machine\u00ad \nindependent algorithm that is driven by a machine description [Davidson]. 5. Implementation cacher and \nassigner are written in C [Kernighan] and run on a PDP-I 1/70 under UNIX. cacher is 1100 lines of code \nand processes about 100 instructions per second. assigner is 310 lines of code and processes about 140 \ninstructions per second. Neither has been much optimized, so these rates can probably be improved. Because \ninformation is not maintained across labels. neither program requires much memory. cacher and assigner \nmake typical object programs $10% smaller and 10-15~0 faster. In addition, cacher s window definition \nmakes the compiler s peephole ,, Optlmlzer run about 3070 faster and Yield code that is 20~0 shorter \nthan the version of the peephole optimizer that uses a fixed window. The three programs work for the \nPDP-I 1 to yield code that is typically at least as good as that produced by UNIX S machine-dependent \ncompiler. Similar results have been observed for implementations for the DECsystem-10, the CDC Cyber \n175, and the Intel 8080. cacher is retargeted by replacing the patterns that identify register names \nand by revising the functions that determine cost and interference. The cost function accepts two s-exprs \nand returns the cheaper. Usually, s-exprs matching a register pattern are preferred to those matching \nsimple memory references, which are preferred to those matching indirect memory refer\u00adences. This function \nmust be revised for each machine, but the change typically effects fewer than ten lines of code. The \ninterference function reports a conflict when assignment to cdt invalidates a cache entry src. This happens \nwhen cdst appears in sr-c. when At indexes an array used in src, when cdst indexes a global array and \nsrc uses parameter array, and when cdst indexes a parameter array and src uses global array. These rules \ninvolve fewer than ten lines of machine-dependent code, though languages with more opportunities for \naliasing [Aho] than Y (e. g., pointers) might require a few more, assigner is retargeted by replacing \nthe patterns that identify the names and numbers of the machine regis\u00adters and by giving code templates \nfor loading and stor\u00ading such registers. These changes are typically simpler than those made to cacher. \n6. Discussion Conventional code generators optimize as early as possible. This often simplifies the requisite \nanalysis and avoids machine-dependence. but it may sacrifice some code quality. Whenever an intermediate \ncode is expanded, it is possible for the expansion to introduce optimizable patterns that will be missed \nby early optimizers. Experience with cacher shows that at least one optimization traditionally applied \nto machine\u00adindependent triples or quadruples can be applied at reasonable cost to equivalent register \ntransfers. It is now natural to seek other optimizations that can be usefully applied to object code. \nFor example, address expansion often produces code that can be moved out of loops and that needs global \nregister allocation. Work in progress is adapting such existing optimiza\u00ad tion to the machine level, \nbut other optimizations may merit similar treatment. Acknowledgment Dave Hanson provided useful advice \nand parts of the Y compiler. 131 References A. V. Aho and J. D. Unman. Principles of Compiler Design, \nAddison-Wesley, 1977. C. G. Bell and A. Newell. Computer Structures: Readings and Examples. McGraw-Hill, \n1971. R, G. G. Cattell. Automatic derivation of code genera\u00adtors from machine descriptions. ACM Transactions \non Programming Languages and Sj stems 2(2): 173-190, April 1980. G. J. Chaitin, M. A. Auslander, A. \nK. Chandra, J. Cocke, M. E. Hopkins, and P. W. Markstein. Register allocation via coloring. Compuler \nLanguages 6(1):47\u00ad57, January 1981. J. W. Davidson and C. W. Fraser. The design and application of a \nretargetable peephole optimizer. A CJ4 Transactions on Programming Languages and Systems 2(2): 191-202, \nApril 1980. R. A. Freiburghouse. Register allocation via usage counts. Communications of the ACM 17( \n11]:638-642, November 1974. R. S. Glanville and S. L. Graham. A new method for compiler code generation. \nConference Record of the F@h Annual A CM Symposium on Principles of Pro\u00adgramming Languages:23 1-240, \nJanuary 1978. D. Gries. Compiler Construction for Digital Comput\u00aders. Wiley, 1971. D. R. Hanson. The \nY programming language. SZG-PLAN Notices 16(2):59-68, February 1981. B. W. Kernighan and D. M. Ritchie. \nThe C Program\u00adming Language. Prentice-Hall, 1978. R. L. Sites. Machine-independent register allocation. \nSIGPLAN Notices 14(8):221-225, August 1979. W. Wulf, R. K. Johnsson, C, B, Weinstock, S. O. Hobbs, and \nC. M. Geschke. The Design of an Optin~\u00adizing Compiler. American Elsevier, 1975. 132 \n\t\t\t", "proc_id": "582153", "abstract": "Compilers usually eliminate common subexpressions in intermediate code, not object code. This reduces machine-dependence but misses the machine-dependent common subexpressions introduced by the last phases of code expansion. This paper describes a machine-independent procedure for eliminating machine-specific common subexpressions. It also identifies dead variables, defines windows for a companion peephole optimizer, and forms the basis of a retargetable register allocator. Its techniques for handling machine-specific data should generalize to other optimizations as well.", "authors": [{"name": "Jack W. Davidson", "author_profile_id": "81100099215", "affiliation": "University of Arizona, Tucson, AZ", "person_id": "PP14044617", "email_address": "", "orcid_id": ""}, {"name": "Christopher W. Fraser", "author_profile_id": "81100364566", "affiliation": "University of Arizona, Tucson, AZ", "person_id": "P47620", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/582153.582167", "year": "1982", "article_id": "582167", "conference": "POPL", "title": "Eliminating redundant object code", "url": "http://dl.acm.org/citation.cfm?id=582167"}