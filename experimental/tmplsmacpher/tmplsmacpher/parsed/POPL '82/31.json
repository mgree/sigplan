{"article_publication_date": "01-25-1982", "fulltext": "\n ALGORITHMIC PROGRAM DIAGNOSIS Ehud Y. Shapiro Department of Computer Science Yale University New Haven, \nCT 06520 Abstract. The notion of program correctness with respect to an irrterpreta~ion is defined for \na class of programming languages. Under this definition, 1~ a program !erminates with an incorrect output \nthen it contains an incorrect procedure. Algorithms for detecting incorrect procedures are developed. \nThese algori~hmsformalize what experienced rogrammers may kno w already. A Iogicprogram implementation \nof these algorithms is described. Itsperformance suggests that thea[gorithms can be ~he backbone of debugging \naids that go far beyond what is environments. offered by current programming Applications of algorithmic \nprogram conso-uc~ion are debugging explored. to automatic 1. Introduction Program debugging is composed \nof program diagnosis, the process of finding a bug, and program correction, the process of fixing the \nbug. In this paper we develop algorithms that mechanize program diagnosis. They apply in the following \nsetting. A programmer is given a program P that returned on input x an incorrect output y. The goal of \nthe programmer is to locate erroneous procedures in P and fix them. We assume the programmer knows what \nthe program P is supposed to compute, either because she wrote it or because she is familiar with its \ndocumentation, and that she can effectively use this knowledge to correctly answer questions like is \ny a correct output for procedure p on input x? and what is a correct output for procedure p on input \nx? , for every procedure p in P. The algorithms we develop can, by asking such questions, detect erroneous \nprocedures in the program P, and provide the programmer with useful information as to how to correct \nthem. lThe author acknowledges the support of the National Science Foundation, grant No. MCS8002447. \nPermission to copy without fee all or part of this material is granted provided that the copies are not \nmade or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication \nand ha date appear, and notice is given that copying is W permission of the Association for Computing \nMachinery. To copy otherwise, or to returblkh, reauirm a fee and) or stsecific permission. We give an \ninformal characterization of the programming languages to which these algorithms apply. We assume that \na program is a static collection of procedures. Each procedure has input variables and output variables \n(a function can be viewed as a procedure with one output variable), which take values from some domain \nD not containing the symbol 1. A procedure with n input variables and m output variables computes a (possibly \npartial) function from D to finite subsets of Dm. We assume some procedure call mechanism, and that a \nprocedure call can fail to return a defined output either by not terminating or by terminating with its \noutput undefined. In the latter case its output is denoted by J... We require the output of a procedure \ncall to be undefined if any of its inputs is undefined or any subordinate procedure call returned an \nundefined output. The programming language can be nondeterministic (with bounded nondeterminism), and \nin such a case a procedure can terminate with its output undefined only if all its possible computations \nterminate, and none of them returns a defined output. We also assume that the control structure within \nprocedures is rather simple, by requiring that a computation can fail to terminate only by having an \ninfinite chain of procedure calls, but not by looping indefinitely inside one procedure. Purified versions \nof many existing and proposed programming languages satisfy the above description. For example. pure \nLisp. pure Prolog, loop-free Algol-like languages with no side-effects and a provision for enforcing \nmonotonicity, procedural APL with no goto s, Backus applicative languages, HareI s And/Or programs, etc., \nWe define semantics for programs. This definition is, in a sense, an abstraction of the semantics of \nlogic programs as defined by Van Emden and Kowalski [6]. An interpretation of a procedure with a name \nP, n input variables and m output variables is a set of triples <p,x,y>, where p k a procedure in P, \nx is inD and y is in Dm. We assume that for anyp and x there are only finitely many y s such that <p,x,y> \nis in M. An interpretation of a program is the union of the interpretations of its procedures. We say \nthat y is a correct output of a procedure call <p,x> in M if <p,x,y> is in M, or, in case .Fl, if for \nno y , <p,x,y > is in M. Otherwise, Y is said to be an incorrect outpur in M. We define two types of \noracles for M, which correspond to o 1982 ACM 0-89791-065-6/82/001/0299 $00.75 the questions we have \nassumed the programmer is capable of answering about a program. A ground oracle for M is a device that, \non input <P,x,y>, outputs yes if <P,.v,y> is in Mand no otherwise. A functional oracle for M is a device \nthat on input <p,x> nondeterministically outputs a y which is a correct output for <p,x> in M. Definition \n1.1: Let p be a procedure and M an interpretation. An oracle simulation of p on x with M is a computation \nofp on input x where every procedure call immediately subordinate to <p,x> k simulated by a call to a \nfunctional oracle for M. An oracle simulation of a procedure call is simply a computation with one-level, \ncorrectness-guaranteed procedure invocation. The goal of an oracle simulation is to isolate the execution \nof a procedure call from possible errors in subordinate procedure calls. Note that an oracle simulation \nis a safeguard even from errors in recursive calls to the procedure being simulated. We say that a procedure \np k correct in M if, for any x, the output of any oracle simulation of p on x with M is correct in M, \nand that p k incorrect in M otherwise. If a procedure p is incorrect in M, then there is some x such \nthat p has an oracle simulation on x that returns an incorrect output y. Such a simulation is called \na counzerexarnple to tbe correctness of p in M. Naturally, a program is said to be correct in M if all \nits procedures are correct in M. Our notion of correctness is stronger that the standard notion of partial \ncorrectness [13]. A correct program is also partially correct, but the opposite is not always true. For \nexample, any everywhere nonterminating program is partially correct, but the nonterminating program ,f(x) \n-if2x k m integer lhen f(2x) is incorrect in the interpretation M = {<f,.-t, 1> I .Y is an integer). \nThe oracle simulation off on 0.5 with M returns 1, which is an incorrect output in M. Another important \ndifference between the two definitions is that partial correctness of a program can not, in general, \nbe reduced to the partial correctness of its procedures, For example, a self-contained procedure p in \na partially correct program can be modified in a way that preserves p s partial correctness but not the \npartial correctness of the program as a whole. In contrast to this property of partial correctness, program \ncorrectness can be effectively reduced to procedure correctness, given a ground oracle for M, as Algorithm \nI below shows. We say that a program P is complete for M if for any procedure p in P and any <p,x.y> \nin M there is a computation of p with input x that outputs .v, and that P is incomplete for M otherwise. \nA program that is both partially correct and complete is sometimes called totally correct. Reducing the \nnotion of program completeness to some P=+=ty of ~ts p,-cdu,-:S not as easy as for program correctness. \nWe describe a partial solution to this problem. We say that a procedure p k sufficient for <x,y> with \nrespect [O M if there is an oracle simulation of p on x that outputs y. If no such simulation exists \nwe say that p k insufjcientfor <x,y>, A procedure p is sufficient for M if for any <p,x,y> in M, p is \nsufficient for <x,y> with respect to M. A program is sufficient for M if all its procedures are sufficient \nfor ~. The notion of sufficiency is a weak form of completeness. Clearly, if a program is complete for \nM then it is also sufficient for M. The opposite is not always true. For example, the program f(x) ifx-1 \nis an integer then f(x-1) else O is sufficient for the interpretation {<f,x,y> I x is an integer and \ny=l or .Yis not an integer and y= O}, but is incomplete for it, Similar to program correctness, program \nsufficiency can be effectively reduced to procedure sufficiency, although here a functional oracle is \nneeded. Algorithm 2 below demonstrates this. Note that if we restrict ourselves to deterministic programming \nlanguages, the theoretical framework developed can be simplified. For example, -1 can be incorporated \nin the domain D, the notion of program sufficiency is then subsumed by program correctness, and Algorithms \nI and 2 below can be unified into one. We do not restrict ourselves to deterministic languages. however, \nsince our intended application is the debugging of logic programs, which are nondeterministic. We comment \non the effectiveness of our definitions. An interpretation M is said to be computable if the mapping \nfrom <P,x> to the finite set {y I <p,x,.v> k in ~ is computable. Clearly, if M is computable then ground \nqueries and functional queries for M are computable. However, in such a case even a stronger claim holds. \nLemma 1.2: Let M be a computable interpretation. Then the mapping from <p,x> to the finite set {j I there \nis an oracle simulation ofp on .Y with M that returns v}, is computable for any procedure and input x. \nTo compute the above set one cycles through all possible oracle simulations of p on x and collects their \nresults. By the assumption made above, any computation within a procedure terminates, and for any pair \n<p ,x> there are only finitely many .v such that <p ,x:j > is in M. Hence there are only finitely many \npossible oracle simulations, and each of them terminates. Correctness and sufficiency of programs are \nsemi-decidable for computable interpretations, using the following procedure. Given a program P and a \ncomputable interpretation M, enumerate in some standard way all triples <p,x,y>, where p is a procedure \nin P, x and y are vectors over D with the appropriate arities. For any such triple <p,x,y>, check whether \nthere is an oracle simulation of p on x that returns y. If such a simulation exists and the triple is \nnot in M, then P is incorrect in M, If such a simulation does not exist and the triple is in M, then \nP is insufficient for M, In both cases output an appropriate message and terminate. We relate the notions \ndefined to fixpoint semantics, We associate with any procedure p a transformation i-P from interpretations \nto interpretations. Let M bc an interpretation and p a procedure. Then 7P(M) is defined to be the set \n{<p,x,.v> y is an output of an oracle simulation of p on Y with M). The transformation TP associated \nwith a program P is the union of the transformations associated with Ps procedures. The transformations \ndefined are monotonic, by the assumptions made above concerning undefined values in procedure calls. \nIt is easy to see that a program P is correct in M iff m@f) c M, and that P is sufficient for M iff MC \nT:(M). It 300 follows that a program P is both correct and sufficient with respect to M iff M is a fixpoint \nof TP. Using an argument similar to the one in [6], it can be shown that P is both partially correct \nand complete with respect to Miff M is the least fixpoint of -rp. With these notions we can draw the \nlimits of our approach: the debugging algorithms described below are applicable only when the intended \ninterpretation M of a program P is nor a fixpoint of the transformation rp. 2. A Igorithmsfor Program \nDebugging VVe describe two algorithms. Algorithm I is applicable when a program returns a defined but \nincorrect output. It can then detect an incorrect procedure in the program, using a simple debugging \ntechnique: trace the execution of the program; the first procedure to return a wrong output is wrong. \nAlgorithm 1: Tracing an incorrect procedure Input: A procedure p in P and an input x such that p on x \nreturns an output y#l incorrect in M. Output: A counterexample to the correctness in M of some procedure \np in P. Algor-ithrn: The algorithm simulates an execution of ~,x> that returns y. As a procedure call \n<p ,x> returns with output v: the algorithm calls the ground oracle with <p ,x ,-V>. If the oracle answers \nyes , the algorithm proceeds with the simulation. If the oracle answers no , then P is an incorrect procedure. \nThe algorithm returns the counterexample to the correctness of p implied by x and y and terminates. We \nargue that Algorithm 1 is correct. Consider the ordered tree of procedure calls associated with the computation \nof <p,x> that returns -v. The nodes of the tree are triples <p,x,y> for every invocation of the procedure \np on input x that returned output y in the computation. The parent relation in the tree reflects the \nprocedure invocation relation in the computations, and sons in the tree are ordered according to the \norder in which the procedures are invoked. The order in which Algorithm 1 queries the oracle corresponds \nto the post-order traversal of the tree thus defined. Consider the first node <p ,x ,y> of the tree in \npost-order, for which the ground oracle answers no . By the definition of Algorithm 1, all sons of this \nnode were already tested and found correct., hence the procedure p k incorrect in M, and the subtree \nof depth 1 rooted at the node <p ,x ,y > corresponds to a counterexample to the correctness of p in M. \nThis establishes the following theorem. Theorem 2.1: Let P be a program and M an interpretation. If a \nprocedure in P has a computation on input x that returns an output y#l incorrect in WI, then Algorithm \n1 applied to p and x returns a counterexample to the correctness in M of some procedure p in P. Algorithm \n2 below is applicable when a program terminates with its output incorrectly undefined. It can then detect \nan insufficient procedure in the program. We explain the procedure ip using the notion of tree of procedure \ncalls associated with a computation, defined above. Algorithm 2: Tracing an insufficient procedure Input: \nA procedure p in P and an input x such that p on x incorrectly returns 1. Ourput: A triple <p ,x ,y> \nin M such that p is a procedure in P which is insufficient for <x\\y > with respect to M. Algorithm The \nalgorithm first calls a functional oracle for M with <p,x>. Since L is an incorrect output, the oracle \nreturns some y#l. It then calls a recursive procedure ip with input <p, X,.V>. The procedure ip uses \na functional oracle for M. It operates as follows. On input <p,x,y> it first nondeterministically tries \nto construct an oracle simulation of the call <p,x> that returns y. If it fails, then by definition is \ninsufficient for<x,y>, and ip returns <p,x.y>. If the oracle simulation succeeds, ip iterates through \nall the oracle calls performed during the simulation. For any oracle call <p ,x> with output y performed \nduring the simulation, ip calls itself recursively with <p ,xty>. If any of ip s recursive calls returns \nwith output <g,u,v>, then ip returns immediately with output <q,wv>. If either no omcle calls were done \n(the oracle simulation did not perform oracle calls), or all the recursive calls to ip returned ok , \nthen ip returns with output ok . Whenever ip is called with <p,x,y> ittries to construct such a tree, \nrooted at <p,x,y>. When ip succeeds it returns ok. Since ip uses a functional oracle for M to construct \nthe tree, once it succeeds, the correctness in M of the simulated computation is guaranteed. Lemma 2.2 \nsummarizes this. Lemma 2.2: Assume that the procedure ip in Algorithm 2 is called with input <p,x,y> \nin M, Then ip returns ok iff there is a computation of p on x that returns y in which all procedure calls \nreturn an output correct in M. Algorithm 2 calls ip with a triple <p,x,y> that is in M. It is easy to \nsee that triples in subsequent recursive calls of ip are also in M. Also, if at some point the procedure \nip fails to construct an oracle simulation of p on x that returns y, then ip returns <P,x,.y>, and, by \nthe definition of sufficiency, p is insufficient for <x,-v>. This establishes Lemma 2.3. Lemma 2.3: Assume \nthat the procedure ip in Algorithm 2 is called with input <p,x,y>, which is in WI. If ip returns a triple \n<p ,x ,y>, then <p ,x ,y> is in M and p is insufficient for <x:-v>. To establish the correctness of Algorithm \n2, as stated in Theorem 2.4. note that ip is called initially with a triple <p,x,v> in M. and that ip \ncannot succeed in simulating a computation of p on x that returns y, since such a computation does not \nexist by the assumption that p on x originally returned 1. Hence, by Lemma 2.2, ip cannot return ok. \nBy Lemma 2.3, if ip returns a triple <p ,xt.v>then this triple is in M, and p is insufficient for <x:+. \nTheorem 2.4: Let P be a program and M an interpretation. If a procedure p in P on input x terminates \nand incorrectly returns J-, then Algorithm 2 applied to <p,x> returns a triple <p:x:y> in M such that \np is insufficient for <.r ,y> with respect to M. 3. A n Implementationjor Logic Programs We recall what \nlogic programs are [ 10], relate the program semantics defined above to the standard semantics of logic \nprograms [6], and develop logic programs that implement Algorithms 1 and 2. A logic program is a collection \nof definite clauses, which are universally quantified logical sentences of the form A -B,, Bz,.,. ,Bk \nk>O where the A and the B s are atoms. Such a sentence is read ,4 is implied by the conjunction of the \nB s , and is interpreted procedurally to satisfy goal A, satisfy goals BI,Bz,..., . . .4 is sometimes \ncalled the procedure name and the Bs the procedure body. If the Bs are missing, the sentence reads ,4 \nis true or goal A is satisfied , A sentence B,, Bz,... ,Bk, k>l, is called a negarive clause, and is \nread the Bs are false , or satisfy tbe Bs . Given a negative clause, a collection of definite clauses \ncan be executed as a program, using this procedural interpretation. As an example, a logic program that \nimplements the quicksort algorithm is shown in Figure 1. Figure 1: A logic program for quicksort qsort([X/ \nL], LO) \u00ad partition(L, X, LI, L2), qsorr(Ll, L3), qsort(L2, L4), append(L3,[X/ L4], LO). qsort([],[]). \npartition([X/ L], Y, Ll,[X/ L2]) -X> Y, partition(L, Y, Ll, L2). partition([X/ L], Y,[X/ LIJ, L2) -X \n< Y, pariiiion(L, Y, Ll, L2). partiiion([], X,[],[j). append(/X/ Ll], L2,[X/ L3]) -append(Ll, L2, L3). \nappend([], L. L). We use upper-case strings as variable symbols, and lower-case strings for all other \nsymbols. The term /] denotes the empty list, and the term /X/ Y) stands for a list whose head (car) is \nX and tail (cdr) is Y. The results of unifying the term [A/X/ with the list /1,2,3,4) is A=l, X=/2,3,4/, \nand unifying [X/Y] with [a] results in X=a, Y=[]. The procedure qsort(X, Y) computes the relation Y \nis the result of sorting the list X . Its first clause is read, procedurally: to sort the list whose \nhead is X and tail is L into a list LO, partition the list L according to ~ into lists LI and L2, recysively \nsort LI into L3 and L2 into L4, and append L3 to the list whose head is X and tail is L4 to get LO. The \nprocedure partirion(L, X, Ll, L2) computes the relation L1 contains the elements of L which are less \nthan X and L2 contains the elements of L which arc greater than or equal to X . The procedure aPpendfLl, \nL24 L3) computes the relation L3 is the result of appending the list LI to the List L2 . We demonstrate \nhow this program works on the Prolog-10 [ 15] with some examples. User input is in italics, Below are \nseveral possible ways of using append, I?-append([l,2,3],[4 SJ,X). X = [1 ,2,3,4,5] yes ~?-append(X,[4,5],[l,2,3,4,5]). \n X = [1,2,3] yes \\?-append([1,2,3],[4,5],[l,5]). no an example of using partition, I 7-Parlirion([4,1,5,8,2],3, \nLI,L2). LI =[1,2], L2 = [4,5,8] yes and a traced execution of qsorf, where subordinate procedure calls \nare skipped. Tbe first number associated with a procedure invocation is its sequential number, the second \nis its depth. Numbers preceded by _ are names of internal Prolog variables. ~?-trace, qsort([2,1,3],X). \n(1) O Call: qsort([2,1,3],_55) ? (2) 1 Call: partition([l,3],2, 137, 138) ?s > (2) 1 Exit: partition([ \n1,3],2,[ 1],[3])  (8) 1 Call: qsort([l], 139) ?s > (8) I Exit: qsort([l],[l])  (13) 1 Call: qsort([3], \n140) ?s > ( 13) 1 Exit: qsort([3],[3])  (18) 1 Call: append([l],[2,3],J5) ?s > (18) 1 Exit: append([l],[2,3],[ \nl,2,3])  (1) O Exit: qsort([2,1,3],[l ,2,3])  X = [1,2,3] yes We describe an interpreter for logic \nprograms, written as a logic program. We assume that the conjunction BI, Bz,..., Bn is represented as \n(BI,(Bz,(. ...BJ.. .)) , that the clause P- is represented as P-true, and that a program P is represented \nas set of clauses clause(C, P) -for any clause C in P. The semantics of execute(A, P) is A is provable \nfrom P Figure 2: A logic program interpreter execute(true, P). execute((A, B), P) -e.xecu~e(A, P), e.xecute(B, \nP). execute (A, P) -clause((A-B), P), execute (B, P). The reader should not be misled by the simplicity \nof this interpreter, This is executable code, and, modtdo implementation-dependent syntactic conventions, \nit can be loaded and run as it is by a standard Prolog interpreter. e.g. the Prolog-IO. To understand \nhow it works, it might help to realize that the real computation is done in the call clause((A-B), P), \nin which the clause to be invoked is chosen, its head is unified with A (by the interpreter that executes \nthis interpreter) and its body is instantiated with the unifying substitution, giving B. The logic programs \nbelow that implement Algorithms 1 and 2 are extensions to this interpreter. We define the semantics of \nlogic programs. Since in logic programs there is no explicit distinction between input variables and \noutput variables, we rephrase the definition of an interpretation, and define a new kind of oracle. An \nirrterpreiafion ofa /ogic program is a set of ground (variable-free) atoms. An exis/entia/ oracle for \nan interpretation M is a device that, on input B1, Bz,..,, B., n> 1, nondeterministically returns a substitution \nO such that B,Ois in M for all 1<i<n, if such a 8 exists, and answers L otherwise. Substituting existential \noracle for functional oracle in the definitions above, correctness of a logic program in M is simply \nits truth in M as a logical sentence, and a counterexample to procedure correctness in M is a ground \ninstance of a clause which is false in M. Sufficiency also has a natural model\u00adtheoretic definition: \na program is sufficient for a ground atom ,4 if it has a clause A -BI, B2,..., B., nzO, for which there \nis a substitution 19that unifies,A and ,4 , and BiO is inM, for all I<i<n. Program completeness is its \ncompleteness as a logical theory with respect to M, and the transformation associated with a program \nis identical to the transformation defined in [6]. It should be mentioned that procedure termination \nwith undefined output, as defined for general programs, corresponds to finite .fai/ure of goals in logic \nprograms [1]. We say that a goal -A I, Az,...,Am, n> 1, irr?nwdia/e/], fai/.r in P if there is no clause \nA -B in P such that A unifies with A,. A goal -A I, Az,.,.,A., nz 1,finire!}~faik in a program P if all \ncomputations of Pon -A I, Az,...,A. are finite, and each computation contains at least one goal that \nimmediately fails. The following logic program implements Algorithm 1. It contains one procedures, ,fi](A. \nP, CE) (read false procedure A, P, CE ), which computesthc relation A improvable using only correct ground \ninstances of clauses in P and CE0,4, or A is provable using some fa]se ground instance A -Bofaclausein \nP, and CE=A -B . The program is best understood by viewing its first two arguments as inputs andthc third \nasoutput, although this is not implied by its definition. Theprogram also containsa procedure caIlfesf(A, \nP, V)toaground oracle for&#38;f. Theway,fp is defined the atom A in the call /esf(A, P,V) is not necessarily \nground. We assume that in such a case the ground oracle instantiates it arbitrarily to some member of \nthe Herbrand base of P before testing it. Figure 3: A logic program for tracing a false procedure fp(true, \nP,ok). fp((A,B).p,(A -BY) .ti(A.p.(A -B?). fP((ArB)tp,X) fp(At p,ok)r.ti(B, p.x). fp(A.p,(A -B]) -clause((A-B), \nP), fp(B,P,(A -B)). fp(A, p,ok) -clause((A-B). P), fp(B. P,ok), test(A, P, true ) fp(A,pt(A-B)) -clause((A-B), \nP), fp(B, P,ok), test(A,P, ~alse ) The procedure fp contains six clauses, The first one is the base case \nand needs no explanation. The next two clauses deal with conjunctive goals, and correspond to the second \nclause of the interpreter described above. They return (A -ll) if the recursive call on any conjunct \nreturns (A -B>, and return ok otherwise. The next three clauses deal with procedure invocation, and they \ncorrespond to the third clause of the interpreter. They return (A -B) if the recursive call on the body \nof the invoked clause returned (A -B). Otherwise they return ok if the instantiated procedure head is \ntested and found true, and return the (ground instance) of the clause invoked if the result of the test \nis false. The procedure fp as defined is not very efficient, and contains many unnecessarily repeated \ncomputations. Well known transformations (which may take the program outside the pure part of Prolog) \ncan easily eliminate the problem, probably at the cost of the program s readability. Appendix 1 containsa \nmore efficient implementation. The following implementation of Algorithm 2 contains one procedure, ip(A, \nP,B) (read insufficient procedure A, P, B ), which computes the relation A is true and provable from \nP and &#38;ok, or A is true but not provable from P and B is a ground atom in Al such that P is insufficient \nfor B . The program also contains a procedure call salisfiab/e(A) to an existential oracle for M, that \nsucceeds by instantiating A to a conjunction of ground atoms true in M, and fails if no such instance \nexists. As should be expected according to the semantics of ip, its definition incorporates a notion \nof unprovability. The metaiogical term not(X) is defined to be true just in case X is not provable. It \nis a built in procedure in any standard Prolog implementation, and a precise semantics for it is given \nin [ 1, 4]. The term atom(X) is introduced as a hack so we can detect easily whether the recursive call \nto ip returns an atom for which the program P is insufficient, or the constant ok . Figure 4: A logic \nprogram for tracing insufficiency ip(true, P, ok). ip((A, B), P,atom(A ?) -ip(A, P,atom(A ~) ip((A. B), \np, X) -ip(A, P, ok), ip(B, P, X). ip(A, p. X) -clause((A -B), P), satisfiable(B), ip(B, P, X). ip(A, \nP,atom(A)) -not((dause((A -B), P), satisfiable(B))), The first three clauses in the program ip are similar \nto the ones in the program ,fp. The fourth clause is doing an oracle simulation of the procedure call \nA. If the simulation succeeds, it calls itself recursively on the body of the procedure invoked, and \nreturns the result of this call. The last clause applies when the oracle simulation fails. Then the program \nP is insufficient for A, and atom(A) is returned. It is possible that a more sophisticated implementation \nof the procedure ip, using techniques for selective backtracking in logic programs [14], may decrease \nthe number of existential queries performed during its execution. 4. Suggested Applications It seems \nthat a straightforward implementation of Algorithms 1 and 2, in which the programmer is the acting oracle, \nwoutd not constitute a significant improvement over existing debugging packages such as [3, 22]. Rather, \nwe see the prospects of our approach in mechanizing the oracle queries as much as possible. This can \nbe done in a large variety of ways. One way to partly mechanize the oracle queries is to accumulate a \ndatabase of answers to previous queries. A new query is first posed to the database, and only if the \ndatabase fails to answer it the programmer is asked, and his answer is then added to the database. As \nthe debugging of a program progresses, the database contains more data about the program, and the debugging \nprocess becomes more automatic. Experience with such an implementation shows that if the same test-data \nis used consistently, the number of queries left unanswered by the database decreases rapidly. Another \napproach for reducing the number of queries the programmer has to answer is by making correctness assumptions \nabout certain procedures. This is similar to what programmers do when they restrict their attention to \nsuspicious procedures by setting break points on them, and temporarily assuming that other procedures \nare correct. The result of a wrong correctness assumption is not fatal. since when the algorithm detects \nan incorrect procedure it provides a counterexample to its correctness, which contains all results of \nimmediately subordinate procedure calls. These results can be examined carefully, and if one of them \nis found wrong it can then by traced after reversing some correctness assumptions. The same holds for \nsufficiency assumptions. Another setting in which mechanizing the oracle queries can yield considerable \ngains can be termed programming by stepwise optimization. This well known idea is to let the programmer \nwrite a simple, lucid, but maybe an inefficient program, and then transform it to a more efficient, but \nmaybe less comprehensible code. Mechanizing the transformation process is a major research goal in the \nfield of automatic programming [2. 9.11. 12]. Viewing it as a manual process, it is a reminiscent of \nWirth s programming by stepwise refinement [23], but with one important difference: a program in an intermediate \nstage of refinement can not be executed and debugged, while an unoptimized program can, This is one major \nobstacle for creating an environment that effectively supports programming by refinement, as reported \nin [17, 21]. Algorithmic program debugging can aid programming by stepwise optimization in the following \nway: the later, more efficient version of the program can be debugged with the debugging algorithms. \nusing an earlier version of the program as an oracle. The information in the database from debugging \nthe earlier programs can be used as a source for test-data. Clearly, the greater the overlap between \nthe procedural structure of the programs, the more queries the earlier program can answer while debugging \nthe later one. Algorithmic debugging can also contribute to mechanizing the process of program optimization. \nOne reason for the practical weakness of current program transformation techniques may be their insistence \non using correctness-preserving transformations only. This requirement can be relaxed. Even if the result \nof a transformation is not always correct, the new program can be debugged with respect to the old one, \nusing the debugging algorithms. The idea can be summarized with a slogan: Don t be compulsive about correct \nrealization of your dreams . We speculate that such a relaxation of requirements may enable the development \nof more powerful, but not always correct transformations. This approach to programming seems most easy \nto implement for logic programs, since a logic program specification is just a (very slow) logic program, \nand hence the process of program derivation from specifications and program optimization are the same \n[9]. Since even specifications may be wrong [7], there is a need to debug them. If the specifications \nare executable, as in logic programs, then the debugging algorithms can be applied. Another possible \napplication of algorithmic debugging is the debugging of the rule-base of an expert system, a task confronted \nso far only with heuristic approaches [5]. The two components in a standard architecture of an expert \nsystem are a rule-base and an inference mechanism. RuIes are supplied by the expert, and their intent \nis to realize the expert s knowledge of her domain of expertise. The system uses its inference mechanism \nin an attempt to reach the same conclusions the expert would. Algorithmic debugging allows the expert \nto debug the rules she proposed without fully understanding the inference procedure of the system, as \nthe debugging algorithms simulate the inference procedure, and query the expert only for declarative \ninformation. Expert systems sometimes incorporate a certainty specification and evaluation mechanism, \nwhich enables the expert to qualify the rules she purposes, and the system to qualify its conclusions \nfrom these rules. Such a mechanism can be implemented in logic programs with uncertainties [18], and \nsince logic programs with uncertainties have semantics, algorithmic debugging is applicable. Most of \nour experience with the debugging algorithms was obtained, however, not using them as a stand-alone program, \nbut as a component of the Model Inference system. The Model Inference system is a Prolog program that \ninductively infers logic programs from examples and non-exam~les of their input-output behavior. The \nsystem implements the general inductive inference algorithm described in [19, 20], specialized to infer \nlogic programs, and a detailed account of its performance is provided there. The system starts with an \nempty program, and debugs its way to a correct and sufficient program for the given interpretation. Algorithms \n1 and 2 are used by the system to detect incorrect and insufficient procedures in the currently conjectured \nprogram. Once such a procedure is detected, it is modified by the system, using the additional information \nsupplied by the algorithms (a counterexample to procedure correctness; an input-output relation for which \nthe procedure is insufficient). The system can identify in the limit [8] various syntactic classes of \nlogic programs; the particular class depends on a parameter that can be tuned. Algorithm 1 specialized \nto logic programs is an instance of the Contradiction Backtracking algorithm [ 19, 20], which is an essential \npart of the general inductive inference algorithm. Algorithm 2 is not part of the general inference algorithm, \nbut incorporating it in the Model Inference system greatly improves its performance. 5. An Example: Debugging \na Quicksort Program The implementation of Algorithms 1 and 2 which we demonstrate is based on the programs~, \nand ip described above, and a complete listing of it is provided in appendix 11. We demonstrate its performance \nin debugging a faulty quicksort program. We start by typing into the Prolog interpreter a (hopelessly?) \nbuggy program for quicksort. A reader with a keen eye can find six differences between this program and \nthe correct program in Figure 1. I ?-[user]. I qsort([X/L],LO) \u00ad j partition(L,X,LI, L2), \\ qsort(LI,L3), \nqsort(L2,L4), append([X/ L3], L4, LO). I ~partition~X/L], Y, Ll,[X/L2J) \u00ad ~ partition(L, Y, L1,L2). ~partiliorr([X/L], \nY,[XjLI],L2) - Y< X, partition(L, Y, Ll, L2). \\ parlition([],X,[],[]). ~append~X/LlJ,L2,[X/L3J) -append(LI,L2,L2). \nI appeird([J,L,[j). I user consulted 152 words 1.12sec. We try the program, I?-qsorlf12, I], X). no and \nit finitely fails, although it should have succeeded with X= /1,2/. Recall that in such a case the program \nip can be applied, detecting an insufficient procedure in the program being debugged. We invoke ip on \nqsort@2,1], X). In the course of its execution, ip queries the user for satisfiability of atoms in the \nmodel, If the user answers true , she is required to supply the satisfying instance. The result of the \nqueries are accumulated in Prolog s internal database, and are used to answer future instances of the \nsame queries. ~?-ip(qsort([2,1], X), A). Query: qsort([2, l],_47)? 1: true. Value of_47? 1:/1,2]. Query: \npartition([ 1],2,-770,-77 1)? I: true. Value of_770? 1:[Ij. Value of 77 1?  I: [1.  Query: qsort([l],-772)? \nI: true. Value of 772? \\: [1]. Query: qsort([],-773)? I: true. Value of .-773? 1:/1. Query: append([2, \n1],[],[1,2])? Ifalse. A = atom(qsort([2, 1],[ 1,2])), X=[I,2] yes We examine why the clause for qsorr \nis insufficient for the atom qsorr([2,1],[1,2fl, and find that this is because the atom append([2, 1],1],11,2]) \nis false. The problem is that we added the number X partitioned upon to the head of the L3, the list \nof the smaller numbers, instead of between L3 and L4, the list of the larger numbers. So we fix that, \nI qsort~X/LJLO) - I partition(L,X,Ll, L2), I qsort(Ll,L3), qsort(L2,L4), I append(L3,[X/L4], LO). and \ntry again, Note the top-down debugging style of the procedure ip: among the six bugs in the program, \nthe one in the top-level procedure is discovered first. ~?-qsort~2, I],X) no [ ?-Mwri[jt 11,X)*A). Query: \nappend([l],[2],[l ,2])? I: true. Query: s(2, I )?  I: false.  A = atom(partition([ 1],2,[ 1],[])), \nX=[1,2] yes We find that the atom parrition(il],2, [l],[fi has no sufficient procedure. The second clause \nofpar/i/ion should have taken care of it. We examine it more closely, and discover that the arguments \nof the < test are switched. We correct this I partition([~ L], Y,[~ LI],L2) -I X< Y, partition(L, Y, \nLI,L2). try again, ~?-qsort([2,1],X). no ~?-ip(qsort([2, I], X), A). Query: <( 1,2)? I: true. Query: \npartition([].2,[],[]) ?  I: true.  Query: partition([], 1.-1441 .-1 442)? I: true. Value of 1441? 1:/]. \nValue of 1442? [: [], Query: append([],[ 1],[ 1])? I: true. A = atom(qsort([],[] )). X=[1,2] yes and \nfind that the base case qsorr(j ],[]) is missing. We add it, and try again. \\?-qsort([2, l], X). X=[] \nyes This time the call succeeds, but with a wrong answer. So we apply the procedure ~p. that will detect \na false clause in the program. I ?-fp(qsort([2,1],X), CE). Query: partition([ 1],2,[],[ l])? Ifalse. \n x = _47, CE = (partition([ 1],2,[],[ I ])-partition([],2, [],[])) yes A counterexample to the first \nclause of parti[ion was found. The > test is missing. We fix this bug, ~partition([X/L], Y, LI,[X/L2]) \n-I X> Y, partition(L, Y, LI,L2). and try qsort again. I?-qsort([2, I], X). X=[] yes ~?-fp(qsort([2,1], \nX),CE). Query: append([],[ 1],[])? I: false. x=_47, CE = (append([].[ l],[])-true) yes And the fact \nthat the base clause for append is wrong is detected. Note that only one query was needed. We fix the \nbase clause to be append([], L, L), and try again. I?-qsort([2,1],X). X = [II-258] yes The answer we \ngot is too general. It has the correct answer, X= /1,2] as an instance, but is also has wrong answers \nas an instance. This means that the program still contains a false clause. We choose the false instance \nX = [1] of the answer and call fp with it. I?-fP(qs0rt([2,1],[l]), CE). Query: append([],[2],[2])? \\: \ntrue. Query: append([ 1],[2],[ l])? I: false. CE = (append([ 1],[2],[1])-append([], [2],[2])) yes And \na counterexample to the main clause of append is discovered. Note the bottom-up debugging style of the \nprocedure fp. The base clause for append was found false before the main clause. Also, note that by now \nthe database has almost all the information needed to monitor the execution of qsort([2,1], X) without \nquerying the user. For example, in the course of this execution offp, the database answered eight out \nof the ten queries performed. We fix the bug in append, I append([X/LI], L2,[XjL3]) -append(Ll,L2,L3). \ntry again. ~?-qsort([2,1],X). X=[1,2]; no I?-qsort([2,1,3],X). X=[I,2,3]; no ~?-qsort([2, 1.3,55,3,1414,43,6S,6,44],X). \nX = [1,2,3,3,6,43,44,55,65,1414] ; no and it works. We summarize the main points exemplified in this \nsession: 1. Any logic program that returns an incorrect result can be debugged, no matter how buggy it \nis. 2. There is no need to finish debugging low level procedures before one can debug higher level procedures, \nor vice-versa. A program can be debugged as a whole. The procedure ip detects bugs in a top-down order, \nandfp in a bottom-up order. 3. The number of queries the programmer needs to answer is small, and it \ndecreases as the debugging progresses. For example, to detect the six bugs in the quicksort program, \nthe progmmmcr had to answer 15 queries, with distribution per bug of 5-2-4-1-1-2. The total number of \nqueries performed during the debugging was 40, with distribution of 5-6-1 I-3-6-IO. 4. Counterexamples \nto procedure correctness and examples to procedure insufficiency are useful clues as to how to fix the \nwrong procedure,  6, Conclusions We have developed a theoretical framework for algorithmic program debugging. \nand shown that a practical implementation of it is within the reach of current programming technology. \nWe suggested that the debugging algorithms developed can improve current debugging facilities. support \nautomatic program derivation from specifications, and be the basis for program inference from input-output \nbehavior. The major theoretical limitation of the debugging algorithms is that they are applicable only \nwhen the incorrect computation terminates. We do not yet know how to treat non\u00adterminating computations. \nThe major practical limitation of the algorithms is that they apply only to programs with no side-effects. \nThis is prohibitive considering current programming practices. One may argue, however, that developing \ndebugging algorithms for programs with side-effects is not a promising approach. A better one may be \nto develop enough computerized support for pure programming, so that one who refrains from impure programming \ntechniques not only will be saved on Judgement Day, but also be more productive as a programmer. We would \nlike to reflect on the choice of logic programs both as the target language and the implementation language \nof the debugging algorithms. It is easy to specialize the general algorithms for logic programs because \ntheir semantics is simple. It is easy to implement them in a logic programz because it is simple towrite \nalogicprogram interpreter in Prolog. We believe that these two properties, in addition to the simple \nsyntax of logic programs, make Prolog an ideal language for developing programming aids and for automatic \nprogramming. We can apply these arguments to any programming language L. If the syntax of L is simple \nthen it is easy for programs to manipulate L-programs. If the semantics of L is simple, it iseasyfor \nprograms to reason about L-programs. For many obvious reasons, it is desirable that the implementation \nlanguage for such algorithms be L itself. This can be accomplished only if L can interpret L-programs \nin a natural way, as suggested by Sandewall [ 16]. Having this in mind, it is clear that extensively \nhard-wiring arbitrary features into a programming language is the wrong direction to pursue. Acknowledgments \n1 am thankful to Dana Angluin for comments and suggestions that greatly improved the correctness of this \nwork, and to Gregory Sullivan for pointing to me that Algorithms I and 2 can be unified for deterministic \nprograms. Luis Pereira suggested the applicability of selective backtracking to reduce the number of \nexistential queries in the logic program implementation of Algorithm 2. For a reader not familiar with \nthe relationship between logic programming and Prolog, one may say that logic programming is to Prolog \nwhat the lambda mlculus 1S to LISP. Appendices I. The Debugging System The following is a complete listing \nof the debugging system demonstrated in section S It is written for the Edinburgh Prolog-10 [15]. It \nis different from the logic programs described in section 3 in the following aspects: 1. There is no \nexplicit mentioning of the program being debugged. The whole internal database of clauses is considered \nas the program. 2. The procedures ~p and @ are optimized, to prevent unnecessarily repeated computations. \n 3. The procedures ~p and ip include provision to allow the program being debugged to contain compiled \nor built-in procedures. The execution of such procedures is not being simulated. This has the effect \nto making correctness-assumptions about complied procedures, as discussed in section 4. 4. The procedure \ntest asks the user to instantiate the atom being tested, and assumes that it has only one true instance. \n(This is close to assuming the functionality of the program, but not quite the same).  fp(true,ok) -!. \nfP((A,B),X) -!, fp(A,XI), (xI=(_-_), X=xl ; Xl=ok, fp(B,X)). fp(A,X) \u00adclause (A, B), fP(B,XI), ( X1=(---), \nX=X1; Xl=ok, (test(A, V), ( V=lrue, X=ok; V=false, X=(A-B)) )). fp(A,ok) -not(clause(A, B)), A. ip(P, \nA) -test(P,true), ipl(P,A). ipl (true, ok) -!. ipl ((A, B), X) -!, ipl(A, Xl), (Xl=atom(-), X=X1; Xl=ok, \nipl(B,X)). ipI(A, X) -ciause(A, B), satisfiable(B), ipl(B, X), !. ipl(A,ok) -not(clause(A, B)), A, !. \nipl (A ,atom(A)). test(P, V) \u00adrecorded fact, (P, true). ), !, V=true. test(P, V) \u00adnumbervars(P, O,_), \nq. instantiates P with Skolem constan recorded fact, (P,false),-), !, V=false. test(P, V) - write( Query: \n~, ask-for(P, VI), ~F l=rrue, ash#or_var<F); VI~Tahe), recordzfact, (P, VI), ), !, v= V1. [lo] Robert \nA. Kowalski, Predicate Logic as a Programming Language. In Proceedings of the IFIP Congress, pages 569-574. \nlFIP, Amsterdam, 1974. [11] Zohar Manna and Richard Waldinger. [1] [2] [3] [4] [5] [6] [7] [8] [9] satisj7able(true) \n-!. satisjiable((P, Q)) \u00ad !, ~e~l(p, true), !, ~ali~fiab/e(Q) satisfiable(P) -test(P, true). ask_jor_var(X) \n\u00advar(X), !, write( VaIue of ~, ask-for(X, X), !. askJor_var(P) -P=. .[F/AJ, askJor-varI(A). ask~or_varl \n~X/ LJ) \u00adask_for_var(X), ask_for_varl(L) askJor_varI([J). ask-for(Request, Answer) \u00ad repeal, display \n(Request), display(? ], nl, read(A nswer). References K. R. Apt and M. H. van Emden. Contruburions lo \nthe Theory of Logic Programming. Technical Report CS-80-I 2, Department of Computer Science, University \nof Waterloo, February, 1980. R. M. Burstall and J. Darlington. A Transformation System for Developing \nRecursive Programs. JA CM 24( 1):44-67, January, 1977. Lawrence Byrd. Prolog Debugging Facilities. Technical \nnote, Department of Artificial Intelligence, Edinburgh University. 1980. Keith L. Clark. Negation as \nFailure. In H. Gallaire and J. Minker, editor, Logic and Data Bases, Plenum, 1978. Randall Davis. Applications \nof h4eta Lzvel Knowledge io the Construction, Maintenance and Use of Large Knowledge Bases. Technical \nReport STAN-CS-76-552, Computer Science Department, Stanford University, July, 1976. M. H. van Emden \nand R. A. Kowalski. The Semantics of Predicate Logic as a Programming Language. JA CM 23:733-742, October, \n1976. Susan L. Ckrhart and Lawrence Yelowitz. Observations of Fallibility in Applications of Modern Programming \nMethodologies. IEEE Transactions on Software Engendering SE-2: 195-207, September, 1976. E. M. Gold. \nLanguage identification in the limit. Information and Control 10:447-474, 1967. C, J. Hogger. Derivation \nof Logic Programs. JA CM27:372-392, April, 1981 [12] [13] [14] [15] [16] [17] [18] [19] [20] [21] [22] \n[23] Artlj_icial Intelligence Series. : Sludies in Automalic Programming Logic. North-Holland, 1977, \nZohar Manna and Richard Waldinger. Synthesis: Dreams -Programs, IEEE Transactions on Sofr ware Engendering \n SE-5:294-328, Ju]y, 1979. Zohar Manna. Mathematical Theory of Computation. McGraw-Hill, 1974. L. M. \nPereira and A. Porto. Selective Backtracking for Logic Programs. In W, Bibel and R. Kowalski, editor, \nl+~th Conference on Automated Deduciion, pages 306-317. Springer, 1980. L. Pereira, F. Pereira and D. \nWarren. User h Guide to DECsystem-10 PROLOG. Technical Report 03/ 13/ 5570, Labortorio National De Engenharia \nCivil, Lisbon, September, 1978. Provisional version. Erik Sandewall. Programming in an Interactive Environment: \nThe LISP Experience. Computing Surveys, March, 1978. E. Shapiro, G. Collins, L. Johnson, J. Ruttenberg, \nPASES: A Programming Environment for Pascal. SIGPLA N Norices, August, 1981, Ehud Y, Shapiro, Execution \nand Debugging of Logic Programs With Uncertainties, In preparation. 1981. Ehud Y, Shapiro. Inductive \nfnfererrce of Theories from Facts. Technical Report 192, Yale University, Department of Computer Science, \nFebruary, 1981. Ehud Y. Shapiro. An Algorithm that Infers Theories from Facts, In IJCAI-81, No. 7. IJCAI, \nAugust, 1981. T. Teitelbaum and T. Reps. The Cornell Sjmthesizer: a Syntax-Directed Programming Environment. \nTechnical Report TR79-381, Department of Computer Science, Cornell University, July, 1979, Warren Teitelman. \nINTER LISP Reference Manual. Technical Report, Xerox Palo Alto Research Center, September, 1978. Niklaus \nProgram CACM Algorithm Algorithm Figure Figure Figure Figure Wirth. Development by Stepwise 14:221-227, \nApril, 1971. Refinement List of A lgorithms 1: Tracing 2: Tracing an incorrect procedure an insufficient \nprocedure 5 5 List 1: A logic program 2: A [ogicprogram 3: A Iogicprogram 4: A logic program of F&#38;ures \nfor quicksort interpreter for tracing a false procedure for tracing insufficiency : 9 10 308\n\t\t\t", "proc_id": "582153", "abstract": "The notion of program correctness with respect to an interpretation is defined for a class of programming languages. Under this definition, if a program terminates with an incorrect output then it contains an incorrect procedure. Algorithms for detecting incorrect procedures are developed. These algorithms formalize what experienced programmers may know already.A logic program implementation of these algorithms is described. Its performance suggests that the algorithms can be the backbone of debugging aids that go far beyond what is offered by current programming environments.Applications of algorithmic debugging to automatic program construction are explored.", "authors": [{"name": "Ehud Y. Shapiro", "author_profile_id": "81100431580", "affiliation": "Yale University, New Haven, CT", "person_id": "PP40027199", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/582153.582185", "year": "1982", "article_id": "582185", "conference": "POPL", "title": "Algorithmic program diagnosis", "url": "http://dl.acm.org/citation.cfm?id=582185"}