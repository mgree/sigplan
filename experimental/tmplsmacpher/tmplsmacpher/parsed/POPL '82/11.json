{"article_publication_date": "01-25-1982", "fulltext": "\n Description -Driv~ Code Generation using .Attribu~e Grammars Mah~devan Ganapathi and Charles N. Fischer \nComputer Sciences Department University of Wisconsin -Madison Abstract The instruction-set of a target \narchitecture is r~?present.ed as a set of attribute-grammar prOdlJc\u00adtions. A code generator is obtained \nautomatically for any compiler using attributed parsing tech\u00adniques. A compiler blJilt on this model \ncan au\u00adtomatically perform most popular lmachine-dependent opt,imizaLions, including peephole optimization, \nThe code generator is also easily retargetable to different machine architectures. ~. Introduction Code \ngeneration is the process of mapping some in\u00adtermediate representation of the source program into assembly \nor binary machine-code. It is a choice-dominated field: a craft requiring pain\u00adstaking effort. as well \nas significant creativity. It is perhaps the most irritating ~ noire ~f compiler writers. The pasl. decade \nhas seen a number of attempts at alJtomaling the process of building code generators For compilers. Interest \nin this area is motivated bv the followino factors: (i) Advances il~ hardware technology (microprogram\u00ad \nming and VLSI) have led to the design and Imanufacture of a larae number of diVQrSQ cnm\u00ad .-.. puter architectlJres-(Intel-8086, \nZ-8000, MC\u00ad68000, TMS-9!300). (2) Advances in pro~ramming langoage design have led to the design of a \nlarge oumber OF sophis\u00ad ticated programming languages (Pascal, Ada, Bliss, C, Fortran 77}. (3) Portable \ncompilers p~oducing high quality code for a variety of machine architectures are needed [Graham 80, \nWulf 80]. (4) Portable compilers must rest on a forrnciliza\u00adtion of machine-dependent aspects of compila\u00adtion \nincluding, (a) storage and tempordry allocation, (b) code generation, (c) machine-dependent optimization. \n  F/osearch supported il) part by NSF grant MCS78~\u00ad02570. Permission to copy without fee all or part \nof this material is granted provided that the copies are notmade ordistributed for direct commercial \nadvantage, the ACM copyright notice and the title of the publication and its date appear, and notice \nis given that copying is by permission of the Association for Computing Machinery. TO copy otherwise, \nor to republish, requires a fee and/or specific permission. @ 1982ACMO-89791-065-6/82/OOl/O108 $00.75 \n Previous research in code genr?ration can be broad\u00adlY classified into three categories: formal.. &#38;9W \ngll?llei , interpretive approaches and descri~tivo ~~ Formal treatments thus far have concen\u00adtrated on \nonly arithmetic expressions. Interpre\u00adtive approaches are improvements over ad-hoc code generation (because \nonly P-EM translators are need\u00aded to implement P languages on M architectures). But in such schemes machine \ndescriptions are in\u00adtermixed with the code generation algorithm. Re\u00adtargeting thus requires changing \nthe code genera\u00adtor for every new machine. Descriptive approaches separate the machine description from \nthe code generation algorithm, providing a higher degree of portabil ity. In such schemes, pattern matching \nis used to replace interpretation. For a classifica\u00adtion of automatic code generation techniques and \na survey of the work on these techniques the reader may refer to [Fraser 77, Glanville 78, Catt.ell 80, \nGanapathi 81a]. p~o~c~e~. ~. Code Generation Goals Our goals are: (1) structure the code generation process \nso that target machine dependency does not taint other phases of a compiler. Interfacing the corJe\u00adgenerator \npackage with other phases of a com-\u00adpiler should therefore become considerably easier. (2) devise a simple \nand clean model of code gen\u00ad eration and machine-dependent optimization Ljs\u00ad ing attribute grammars [Knuth \n68, Ilaiha 80]; ideally, one that is simpler and cleaner than a means-end-analysis model [Newell 69] \nas stipulated in [Cattell 80]. (3) retain the speed and efficiency of Glanville s [78] approach by using \na fundamentally single-pass code generation scheme. (4) include machine-clependent optimization that \nIhave not been incl(Jded in other portable code-generators. These optimization include choosing between \nthree-address and two-address instructions.: subsuming (via auto-increment) additions widely separated \nfrom the current instruction (in effect, floating an addition across many instructions), subsuming \nsubtrac\u00ad tions Via aLlto-decrement, removing redundant loads and stores, replacing memory references \nby register references, delaying operand move\u00ad ment i nto costlier storage locations and span-dependent \nOpt.ionizations [Szymanski 78, 80]. such optimizations are hard to incor\u00ad porate as a separate pass of \npeephole optimi\u00ad zation [Fraser 79, 80] since the instruction COUICi have effects outside the window \n(e.g. condition-code setting and register contents). 108 (5) demonstrate the practical_j~y of code-generator \nretargetabi lity. A compilefi-development ef\u00adfort should not suffer innumerable delays due to code generation \n(the authors share the com-value they can hold and their contribution to piler writer s disgust with \nthe code genera-instruction size, tion ordeal!). ThlJs, a retargetable code gen-(5) addressing modes \navailable to access and re\u00aderation scheme should decrease software expen-trieve operands, diture in building \nnew compi Iers for new (6) side effects of instructions , such as machines. condition-code setting and \neven-odd register pair use, ~. Attribute Grammars (7) assembler or binary fOl ma t.S of instructions \nand addressing modes. Attribute grammars were proposed by Knuth [68] as a means of formally specifying \nsemantics within The hardware abstractions essential to code gen\u00ad the context free grammar of a language \n(CFG). eration are data types, addressing Inodes and in\u00ada formal definition of attribute grammars or \naffix structions. Data types are groups of bits that For qrammars, see [Koster 74, Lewis 76, Milton \n77, can participate as operands to instructions. Some tiatt 77, Raiha 60]. Intuitively, each grammar \nexamples are: symbol in a CFG is allowed to have a fixed number of associated values, termed attributes, \nwhose vAx-11/780: byte, word. long. quad. float. double. domains may be finite or infinite. As an Tnp(Jt \nis Z-8000: bit., byte, word, long, quad, BCD, parsed, attributes are evaluated. The resulting Intel-8088: \nbyte, word, 8CD, syntax tree augmented with attributes represents MC-66000: bit, byte. word, long. BCD. \nthe semantics of the input. The attributes asso\u00adciated with a given symbol may be synthetic or in-The \ninterpretation of these bits by the ceotral heri ted: Synthetic attributes (which we denote processing \nunit depends on their representation by prefixing them with a T ) are used to pass in-(e.g. signed magnitude. \ntwo s complement). formation up a parse tree. Inherited attributf!s (prefixed with a f ) are used to \npass information Addressing Inodes are access paths to retrieve down a parse tree. Each context-free \nproduction operands residing in storage locations such as has an associated set of attribute evaluation \nmemory, stack or register. The time taken to ac\u00adrules. All rules can be packaged into predicate cess \nan operand depends on the access path and the symbols (which check for attribute correctness) or storage \nlocation in which the operand resides. action symbols (which compute new attribute For example, it is \nfaster to retrieve an operand values). To evaluate an action symbol, its inher-from a register than from \nmemory or the stack. ited attributes are first made available,, The ac-The timings may also be dependent \non tho Presence attri\u00ad tion symbol is then applied; its synthetic of a cache, a floating-point accelerator. \npipel in\u00ad butes are computed as a result. This model makes ing or other configuration details. The size \nof a it very easy to implement action symbols as sub-machine instruction is also affected by the ad\u00adroutine \ncalls. , dressing mode. Addressinrl-mode UQg@W will be used to describe patterns for address formation \neffective Attribute grammars have been used as an in the target architecture. tool to structure the translation \nphase of com\u00adpilers [Watt 74, Lewis 76, Milton 79]. Our use of The operations of machines can broadly \nbe classi\u00ad attribute grammars in code synthesis (storage al-fied, with respect to mapping IR operators \nto location, code generation and machine-dependent machine op codes, under the following categories: \noptimization) should complement their traditional use. (1) Data-transfer instructions are used to imple\u00adment \nsource-language assignments to variables. ~. ~itecturg Primitives Assignments can sometimes be subsumed \nas part of other operations (e.g. A := A + B can be Code generation requires descriptions of the fol-implemented \nas add B, A). Assignments of ag\u00adlowing components of a machine architecture: gregates, may not be irnplemeotable \nin a single data-transfer instruction; Often a series of (1) addressable units for storinq source-langoaoa \nmoves or a loop is required to imPlement them. values (e.g. memory, regi~ters and har~w~~~ (2) Arithmetic \ninstructions are used to implement stack), arithmetic-expression evaluation and address (2) a run-time \ndisplay mechanism such as calculations.  display pointer, activation pointer (which may be a (3) Boolean \ninstructions are used to imPlement register or a memory location)  -evaltJation under two con\u00ad and direction \nBoolean-exDresslon of frame growth (either up or down), texts: (3) the set of instructions available \nfor imple-(a) as values to be manipulated or as\u00ad menting IR operations: signed and and size, (b) as predicates \nto conLrol conditior]al their execution time (4) primitive data types (data objects having execution. \ndirect hardware realization) that can partici-(4) Control instructions (comparisons and pate as operands \nto instructions, the maximum branches) =~to implement relational operators, somet imes including an implicit \ncomparison with zero. Oft~n the result of a comparison is a condition-code setting that is subsequently \ntested to decide the control Flow of the user-program. (5) Subroutine call and return instructions are \nnecessary to implement procedures. . (6) Special ~uctions are used to optimize ob\u00adiect code. Examoles \ninclude:  ~a) single instructions that effectively per\u00adform combinations of arithmetic and con\u00adtrol \noperations (e.g. subtract one and branch on the PDP-11/70, add one and branch if less than o n the VAX-11/7&#38;T0, \nloop and repeat on the Intel-8086), and (b) shift instructions on integers (often used to replace integer-multiplication \nand, in some cases, division by a power of two). The orthooonalitv of an instruction set is the re\u00adgularity \nwith ~hich any op-code can be used wi~h any machine-primitive data type and addressing mode [Wulf 81b]. \nEvery architecture designed and marketed so far possesses some amount of non orthogonality (for a comparison \nof contemporary 16-bit microprocessor architectures from the point of view of COMpilE!r code generation, \nthe reader may refer [Ganapathi 81c]). For example, o n the Z-8000 and Intel-8086, no memory-to-memory \narith\u00admetic is possible. On the PDP-11/70 and IBM-370 no memory-to-memory multiplication or division \nis possible, but memory-to-memory addition and sub\u00adtraction are allowed. Such irregularities force the \ncode generator to produce extra code for relo\u00adcating operands. To implement C := B * C on the PDP-11/70, \nwhere both B and C are integers in memory locations, C has to be relocated Lo an even register of an \neven-odd pair. Consequently the corresponding odd register may need to be relocat\u00aded before the multiplication \nso that its contents are not destroyed as a side-effect. The code gen\u00aderator automatically takes care \nof such special cases when parsing through the attribute-grammar productions. ~. AttribuQ-Grammar Machine \n.~~.scription For purposes of pattern matching and instruction selection, the instruction set of the \ntarget ar\u00adchitecture is represented as a set of attribute\u00adgrammar productions. These productions form \nthe input to a program that generates a code generator for the target machine. All productions are of \nthe form LHS -> RtlS , where LRS stands for left\u00adhand side, RHS for righthand side. The LHS is a sin~le \nnon-terminal usually appearing with syn\u00adthetic attributes. The RHS contains: (1) terminals with synthetic \nattributes (prefixed by a ~ ), (z) non-terminals with synthetic attributes, (3) disambiguating predicates \n[Milton 77] (under\u00adlined) with inherited attributes (prefixed with a ~ ) and  (4) action symbols (capitalized) \nwith synthetic and inherited attributes. Attribute occurrences may be constants or vari\u00adables. Constant \nattributes (with the exception of self-defining constants) are enclosed within quotes. The kinds of productions \nneeded for an entire code generator can be broadly classified into addressing-m productions and instruction\u00ad \n. selection ~oductions. flddressing-mode productions: Each production has an RHS specifying the pattern \nof an IR addressing mode. The production creates the proper machine address (in an action symbol). For \nexample, the following production is used to specify an index addressing mode on the PDP-11/70 and the \nIntel-8086 or a displacement addressing mode on the VAX-11/780 ( , denotes indexing in the IR): Address~a \n-> , Disptb Basetc ADDR (~b~c~a) Disp represents a local variable with attributes specifying the machine \ndata type and offset from a frame pointer. These attributes are determined when IR variables are bound \nto locations in the target machine. The attribute variable c speci\u00adfies the base (or display) register \nof the IR variable. The action symbol ADDR synthesizes an address for a datum on the target machine. \nThe attribute a represents this address. Instruction-selection ~ductions: Each production has an RHS \nspecifying a pattern in the IR and the corresponding code sequence to be emitted on a match. The LHS \nmay be an explicit result location (a register or a memory location), in which case it specifies the \ndata type of the result, or a condition code location, or simply a non-terminal place-holder. Consider \naddition on the VAX-11/780. There are two-address and three\u00adaddress add op-codes. Furthermore, the increment \ninstruction can be used for adding one. For a byte datum, these three forms of addition are ex\u00adp~-essed \nas follows: Bytetr -> + Byteta Byte -> + Bytetr Byte -> + Byte~a Byte -> + 8yie~a !3yte~b GETTEMP (~ \nbyte TP) EMIT(f addb3 fa~b~r) The first and second productions specify the addi\u00ad 110 tion of 1 to r \n. Both productions are needed to represent the commutativity of addition. I n case either production \nis selected, the op-code inch (increment byte) is emitted. The non-terminal on the LHS (Byte) and its \nattribute (r) specify the data type and address of the result respectively. The third and fourth productions \nspecify two\u00adaddress addition of a and r using op-code addb2. Similarly, the last production soecifies \nthree-address addition of a and code addb3. In this case, the S~l is j~~~ecjo~~ r that is obtained from \naction symbol GETTEMP, The location r may be a free register or the LHS of an assignment statement whose \nprevious contents need not be preserved. An addition of two IR data in byte format will match the RHS \nof one of these productions. The choice of the RHS is determined by attribute values and the disambiguating \npredicates. If an operand is 1 then an inch instruction is selected. Productions three through five handle \naddition of a constant other than 1 as well as variables. In an assignment context, a global attribute \nkeeps track of the target address of the assignment statement. The disambiguating predicate TwoOp evaluates \nto true if either operand is the target of assignment or its value nee d not be preserved after addition \n(i.e. is not bUSy). Consequently, a two-address addb2 is selected. If TwoOp evalu\u00adates to false, then \na three-address addb3 is selected. For complete attribute-grammar target machine descriptions, the reader \nmay refer to [Ganapathi 80]. ~. Linear Intermedia~ Representation In a classical monolithic compiler \n(i.e., a com\u00ad piler written for a single programming language to run on a single target machine), an \nintermediate form of program code is used primarily to allow optimization. Examples of popular intermediate \nforms are quadruples, triples, tree representation of programs and directed acyclic graphs (DAGs) [Aho \n77]. Unfortunately, these conventional in\u00ad termediate forms are inadequate to serve the needs of compiler \nportability. The design of an Inter\u00ad mediate Representation (IR) is critical to com\u00ad piler portability \nand code generation efficiency. This efficiency issue includes both the efficiency of the code generation \nalgorithm and that of the object code produced by the code generation algo\u00ad rithm, In this paper, an \nIR is visualized as a demarcation or boundary-line between machine\u00ad independent and machine-dependent \nparts of a com\u00ad piler, A compiler front-end would translate a programming language to this IR. All language\u00ad \ndependent issues are to be processed by this front-end. For example, in Ada [Ichbiah 79] an operator \ncan have several valid meanings within the same scope. This operator overloading must be disambiguated \nby the front-end before it generates the IR. Similarly, re-ordering of procedure parameters in Ada must \nalso be done by the front\u00adend. A code generating back-end would translate the IR to target machine code. \nAll machine\u00addependent issues are to be processed by this back-end. As a consequence of this approach, \nstorage binding is treated as a code generation issue to be addressed by the back-end. However, compiler \nfront-ends may influence this binding de\u00adcision via the IR. Furthermore, no assumptions are made regarding \nthe basic run-time model of the target architecture (e.g. whether the amhit.ecture is a stack machine \nor not). Under this conlpila\u00adtion model, a compiler can be retargeted to a new machine simply by changing \nthe IR to object code translation phase. For the rationale behind the choice of an attri\u00ad buted prefix \nIR, its relative standing with respect to other IR proposals i n the 1 iterature and the detailed design \nof the IR, the reader may refer to [Ganapathi fllb]. The IR for the Pascal statement A := B -1 is: :=A \n-B1 1 0 tf allSlate this IR to target-machine code, a two-phase single-pass code generation algOI ithfII \niS employed. The implementation is modularly divided into: (a) storage !J&#38;@ig p~a~ and (b) instructiol~ \nufl.e~fl.m,  At the IR level , variables are represented by their names, which are converted to machine \nad\u00addresses before instructions are selected. After the storage binding phase, lR names are transformed \n10 machine addresses, with nlachine\u00ad independent. addressing modes that allow multiple levels of indirection \nand indexing. The Imappiflg of these machine-independent addressing modes to the actual modes present \nin the target arcbitec\u00adL.ure is done when parsing through the addressing mode productions of the target \nmachine. issues and the eXpaflSiOfl ~S&#38;Z. Storacie-binding __ This section discusses some of the \n;torage\u00adassignment options that an automatic code genera\u00adtor must handle. A compiler front-end selects \nop\u00adtions t)y setting synthetic attributes of tokens. At the IR level , variables are represented by their \nnames, which are then bound to machine ad\u00addresses before instruction selection. The deci\u00adsion of bow \nto address locals and globals is not made at the IR level; it is treated as a code\u00ad generation issue. \nSome storage allocation and reclamation is done at well-defined times during execution (e.g. allocating \nspace at block entry and releasing it at block exit). Other storage management is done at arbitrary moments \n(e.g. ac- Storage as\u00ad quiring and releasing heap space). , signment, that is. bil~din9 constonts, slmP1e \n!ar7\u00adables and aggregates to machine storage-locatlons, may be based on a pre pl anneal strategy, such \nas global f-low analysis, or done during code genera\u00adtion. In the former case, Iregister preferences \nFor example,aPP~ar as attributes in the IR. :X~R denotes that X is a variab le that should prefer\u00ad \nably be placed in a register. Whether the code generator is able to satisfy requests for register assignment \ndepends on the number of general\u00adpurpose registers in the target machine and the peculiarities of the \ninstruction set (e.g. even\u00adodd register-pair use and ordering of operands in instructions). To support \nblock structure, references to non\u00ad global variables are usually implemented through one of the following \nmechanisms: (1) descending a static chain of linked frames; elements in the chain are at a fixed offset \nfrom the frame pointer (FP) [Aho 77], (2) displacement from the relevant display [Dijks\u00adtra 60]; the \ndisplay is stored in a fixed lo\u00adcation that is indexed off a display pointer (DP), usually a register, \n (3) displacement from a display created at every procedure or block entry and placed on the stack [Gries \n71]. In this case, the chain is descended only once per block (or procedure)  entry instead of once \nper non-local reference. The targeL architecture usually provides the FP (or a base register), LIP and \na hardware stack\u00ad pointer as registers. If they are not registers, then memo ry locations must be used \nto simulate them. Non-global variables whose space require\u00adments are determinable at compile-time (e.g. \nin\u00adtegers. reals, characters and Booleans) can be bound to general-purpose registers, addresses with \na Fixed offset from the FP, addresses accessed in\u00ad directly through the DP r by explicit code se\u00ad quences \nthat descend a static chain that links frames. The current code-generator implementation supports all \nof these accessing methods. Tt.e displacement from the frame may be positive or negative depending on \nthe direction of frame growth; this information is provided as part of the mact]ine description to the \ncocle generator. Variables that ale botb oon-loca l and norl-global are accessed through a display or \nthrough the static-ch~in mechanism outlined above. In these cases, another attribute (in the for,rr \nof ?number) specifies either the index of the relevant display or the number of levels of indirection \nfrom the FP (via the static link). This attribute is used to obtain the base register used to address \nthe vari\u00ad able. The machine data-type of an IR-variable is deter\u00ad mined by searching a machine-description \ntable, which provides information on data-types, their alignment restrictions and the maximum values \nstorable in them. If the frame is part of the stack (as is usual in most architectures), the stack pointer \nmay also have to be aligned, A good quality code generator must be able to support all these varieties. \nIll variables are thus converted into addresses be\u00ad fore instructions are selected. In order to pro\u00ad \nvide flexibility to allow any of the above block\u00ad structure mechanisms, the code generator accepts multiple \nlevels of both indirection and indexing (even indexing through a memory lo~ation). These machine-independent \naddressing modes are auton}ati\u00ad cally mapped to the addressing modes of the machine b.y productions. \nThe usefulness of multi\u00ad ple levels of indexing is apparent on machines such as the Burroughs B-5500, \nwhich needs no in\u00ad teger multiplication for array-element referenc\u00ad ing. The subscript values are pushed \non the stack, and the hardware uses a base descriptor and a subscript to obtain a descriptor for the \ncorrect row. Other subscripts are used to index the desired element. In our scheme, one could provide \nproductions to capture this array indexing mode. Addresses for dynamic arrays, strings and pointers are \ncalculated and assigned at run time. Usually, for dynamic arrays, the dimensions of the array are known \nat block entry. Since space for arrays will be released at block exit, arrays can be as\u00adsigned areas \non the stack and accessed through a dope vector [Gries 71]. Dynamic strings and heap objects, however, \ncannot be stored on the stack. They need a heap and associated routines for heap management and garbage \ncollection. Almost all machine architectures provide primitives for stack manage tnent Comparable heap-management \nprimitives are usually not available. Such allocation primi\u00adtives are therefore normally realized as \nsubro[l\u00adtine calls or as in-line code. Procedure calls and returns require code (as pro\u00ad cedure prologue \nand epilogue) to adjust the frame and stack pointers, save and restore displays and registers and pass \narguments. Some architectures, such as the VAX-11/780 and the MC-68000, allow a variable numbe r of registers \nto be saved and re\u00adstored in a single instruction, thus lowering the overhead for a context-switch during \na procedure call . In such a case, code has to be generated t.o specify which registers to save. Address \nassign\u00adment for procedure parameters can be relative to either the FP or a special argument pointer (e.g. \nAP on the VAX-11/780). The front end can indicate its choice of offset, whether positive or nega\u00adtive, \nfrom the FP or AP. This choice may be over\u00adruled by the code generator if the machine archi\u00adtecture does \nnot provide a facility for an imple\u00admentation. For example, negative offsets from base registers are \nimpossible on the IBM-370 and Univac-1100 series machines. Anot!~er option that must be supported by \nautomatic 112 description of that machine is given to the CGG. code generators is specification of the \norder of Transition tables for the machine are then au\u00adargument pushed last or ~irst), Param(!ter-pushing \ntomatically obtained and the same driver is used, pushing procedure parameters on the stack (first The \nCGG constructs a col,text-serls itive parserorder affects the ability of target programs to [Watt 74, \n77]. Bottom-up parsing is preferred tointerface with other system routines. Other top-down parsing (for \na rationale, the startup routine (which is reader is referred to [Ganapathi 80]). In order implementation-dependent \nissues are the run-time interested mostly language\u00addepondent) and implementation of 1/0 calls (which \ndisambiguat\u00ad to resolve conflicts during parsing, is L!sually operating-system dependent). ing predicates \nare used to examine the parser\u00adstack and control parsing. The disambiguating In short, IR variables are \nassigned storage before predicates of each production are usually written as the productions are designed. \nserve as code selection. IR names are transformed to They a guide to when the production is applicable. \nmachine addresses, with machine-independent ad- The attributed bottom-up parser with predicates employs \nthe standard LR(k) parsing loop dressing modes that allow multiple levels of in- disambiguating direction \nand indexing. Consider the statement: with added code to manipulate attributes. Since the set of attributes \nis relatively small (the prototype code generator uses ten attriblltes where A is a local variable and \nB is both non\u00ad A:=B-1 all, COVering many architectures), the parser do~~ not need to be able to handle \nfully general attri\u00adbute sets. local and non-global. The corresponding IR is: : A ~Local ~integer tl \n: begins a declaration],, B TChain ?2 B is accessed through the static In our notation, RHS symbols with \nconstant attri\u00adbute values differ significantly from RtlS symbols chain and is two levels outside :=A \n-B1 with symbolic (variable) attributes. Symbols with constant attribute values can only match corresponding \nvalues in productions. Datumt2 will only match a datum with an attribute value of 2, After storage binding \n(the symbol , implies in\u00ad dexing and 1? represents indirection): whereas llatum~a can match a datum with \nany attri\u00ad bute value. In case the same attribute variableA becomes Disp?a Base?Fp B becomes Disp?s Base?FP \nappears several times within one production, at\u00adtribute values need !~s!~bt.h~ ~f~set of the static \nnot be coPied; the relative chain from the frame pointer) offset (from the stack top) of the defining \nin\u00ad stance of the attribute variable is carried for\u00adward. On a shift operation, attribute values of a \nsymbol are copied onto the stack, 1 becomes Datumtl. The assignment statement becomes: On a reduce operation, \naction symbols are processed and syn\u00ad .= thetic attributes are returned to the LHS symbol of the production. \nFor each ,, Disp?a BasetFP Disp?b @ @ , Disp?s BaserFP Datum?l. action symbol, in turn, its inherited \nattributes are first evaluat\u00ad ed, and then the corresponding Function The mapping of these addressing \nmodes to actual (the ac\u00adtion symbol) is c?lled to evaluate its synthetic modes provided by the target \narchitecture is im\u00adplemented by parsing through the addressing-mode attributes. The algorithm is detailed \nbelow: productions. After parsing, the above statement PROGRAM CodeGenerator; becomes: State := 0: Action \n:= Shift;:= Addressta -Address?b Addresstc SWITCH (Action) OF where the attributes la!! and ,,b,, represent \nthe ad_ dresses of A and B, and the attribute c represents the constant 1. CASE Shift: Push(State): \n( stick th~ token s synthetic attributes*) Push(TToken); ~, ~o~-Generator Generator Q@ ~ =e~n ~lgorithm \n( determine next action *) Actset:=Nextaction(State ,Token,The attribute grammar for the target machine \nis input to a code-generator generator (CGG) whose ~Token,LaToken); IF Actset is single-valuedoutput \nis a specific code generator for the target machine. The code generator consists of a set of THEN Action \n:= Actset ELSE Action:=Disambiguate(State ,Actset); tranSitiOn tables and a d[.iver for these tables, \n(* determine next state *} This driver serves as a push-down automaton that State := Nextstate(State, \nToken): parses the IR form. Instructions (machine opera-END; (* case shift *) tions) are selected during \nparsing. To transport COillpil@rS to a new machine, the at,t-ribut,e-gral,]mar 113 CASE Reduce: SWITCH \n(LhsProd) OF CASE actionsymbol: (* Func(Inh, Syn) *) Func(Stack[l] ,. .,Stack[Inh], Stack[Syn] ,.. ,Stack[l]); \nEND; (* case action symbol *) CASE nonterrninal: Pop(RhsProd); Pop(?RhsProd); (* state = top of stack \n*) State := Stack[l]; Push(?LhsProd); END; (* case nonterminal *) END; (* switch *) Actset:=Nextaction(State \n,LhsProd, ?LhsProd,LaToken); IF Actset is single-valued THEN Action := Actset ELSE Action:=Disambiguate(State \n,Actset): State := Nextstate(State, LhsProd); END; (* case reduce ) CASE Accept: halt, accepting; END; \n( case accept *) CASE Error: halt, rejecting; (*the front-end generated an invalid IR sequence*) END; \n( case error *) END; (* switch *) END. (* end program codegenerator *) Q. Examples of Instructio&#38; \nSelection: Attributed Parsing In this section we illustrate examples of using attributed parsing to generate \nVAX-11/780 code. Consider the translation of the statement: A :=B-l on typical architectures with several \nlengths of integers (e.g. byte, word, long), The IR after storage-binding is: := Address~a -Address~b \nAddress~c where the attribute variable a includes long and address information for A, b has word and \nother in~ormation for B, and c has byte and 1 as the actual value. We now trace the parsing process. \n(1) The following production is recognized: Longtx -> Address~x IsLon~ (ix) This production matciles \nany Address with attri\u00adbutes that declare that its type is long. Because x appears twice in this production, \nit is impli\u00adcitly copied from Address to Long. Thus, the at\u00adtributes of A are carried forward. We now \nhave: := Longta -Address~b Addresstc (2) Next, production: Wordtx -> Addresstx IsWord (*x) matches Addressfb, \nbecause it is a word. The lo\u00ad cal attribute variable x is instantiated as b . We reduce the IR further \nto: := Long~a -Wordtb Addresstc (3) Now, the following production is matched: Longtx->Wordty ConvToLo~ \n(~y) GETTEMP (~ long tx) EMIT (j cvtwl ~y~x) We convert from word to long format by first allo\u00adcating \na temporary (say register rl) through the action symbol GETTEMP, then issuing a convert word to long \ninstruction through the action sym\u00adbol EMIT. We now have reduced the IR to: := Longta - Longtrl Address~c \n(4) Next, the constant 1 is reduced to a Long by the production: Longtx -> Addresstx IsLong (*x) We have \nreduced the IR to: := Longta -Longtrl Longtc (5) The following production is now matched: Longtx -> \n-Longtx Longty -(~y) NotBus~ (*X) EMIT (~ decl *x) This production describes a special-purpose decre\u00adment \ninstruction, applicable only if the second operand is the constant 1. We have reduced the IR to: := Longta \nLongtrl (6) Finally, the following production is matched (if x and y are distinct): Instruction -> := \nLongtx Longty NotEquate (~x~y) DELAY (~ movl ~y $x) NotEqua~ evaluates to false if x and Y are equivalent \nlocations. Consequently, the following production (which generates no code) is matched: Instruction -> \n:= Longtx Longty (no code emitted) DELAY is a variant of EMIT that can delay genera\u00adtion of an instruction \npending future instruc\u00ad tions. In this case, the move of rl to A is de\u00ad 114 layed so that future references \nto A can be re\u00adplaced by rl. Also, the move may be completely suppressed if, for example, another assignment \nto A is encountered before it is referenced. The use of attribute values to control parsing the IR allows \nus to significantly improve the quality of generated code with little effort. For exam\u00ad ple, in step \n(5), above, if the left operand had been busy, we would have generated two instruc\u00adtions (a move , then \nthe decrement). A better code sequence would be to use the VAX s three ad\u00address format to generate, for \nexample, sub13 1, B, rl . If the x s attributes show it is busy, NotBusy evaluates to false, and recognition \nof this pro\u00adduction is blocked. Instead, an equivalent (but longer) instruction is generated by this \nalternate production: Longtz -> -Longtx Longty GETTEMP (~ long tz) EMIT (~ sub13 ~y *X $2) ~. Machine--Dependent \nOptimization Optimizing compilers attempt to produce a more ef\u00adficient represent.ation of user programs, \naiming both for compact object code size (an operational constraint on computers with 1 imited address \nspace) and execution speed. A large number of op\u00adtimization are wholly architecture-dependent: (1) using \nspecial machine instructions (e.g. in\u00adcrement, subtract-one-and-branch , add-one\u00adand-branch-less-than-or-equal \n) and available addressing modes (e.g. indexing) to avoid ex\u00adplicit addition; also, subsuming addition \nor subtraction (e.g. using auto\u00adincreme nt/decrement) , (2) avoiding redundant register loads and stores \n(or redundant pushes and pops), (3) optimizing branches (e.g. branch chaining, cross-jumping [Wulf 75], \nspan-dependent in\u00adstructions [Robertson 77, Szymanski 78, 80]). Code goneration for control constructs \nhas not been given much attention in recent approaches to formalization and automatic derivation of code \ngenerators. Ripken [77] uses attribute predicates to describe different forms of branch instructions, \nincluding forms oLher than merely long and short. (The VAX-11/780 has three types of branch instructions: \nbranch-byte (2 bytes), branch-word (3 bytes) and jump (5 bytes)). He proposes rearranging basic blocks \nof code in order to generate ap\u00adpropriate branch instructions in a later pass. Finding such an optimal \nrearrangement (to minimize program length) has been proved to be NP-complete by Robertson [77] and Szymanski \n[78].  (4) peephole optimization [McKeeman 65]. A separate peephole-optimization pass over as\u00adsembler \ncode is used by the Unix C compiler [Ritchie 78]. Bliss FINAL [Wulf 75] has demonstrated that a considerable \nreduction (15-40%) in code size can be achieved by such a pass, FINAL directs almost all its efforts \nin improving code for control constructs. Re\u00adcently, Fraser [79, 80] has implemente~ha; machine-independent \npeephole optimizer reads machine descriptions and attempts to op\u00adtimize adjacent pairs of assembler instruc\u00adtions. \nFor a window of more than two instruc\u00adtions, the speed of the optimizer is degraded. Optimization of \ninstructions not physically adjacent requires more context information. Attributes are a good means of \nmaintaining contextual information. Our implementation attemDts to exDress optimiza\u00adtion <n a non-procedu \nral form, replacing the hand-coding of machine-dependent optimization by the use of attribute grammars. \nOur intention is not to expand on the vast store of optimization techniques, but to cleanly organize \ntricky machine-dependent optimization (especially those optimization that are both popular and effec\u00adtive). \nSome of these optimization, such as re\u00admoving redundant loads/stores and using arithmetic shifts instead \nof multiplications, are commonly used in compilers with the help of specially hand-coded routines, Others, \nsuch as the use of sob (subtract-one-and-branch) on the PDP-11/70 and auto-increment, are not commonly \nused. Our implementation formalizes machine-dependent optim\u00adization within the attributed parsing framework \nunder the following categories by incrementally spelling out plans to improve target code quality: (1)addition \nof attribute grammar productions to incorporate special instructions, (2) delaying generation of code \ntill the end of a basic block, (3) code subsumpt,ion within addressing modes, (4) deletion of redundant \ncode and (5) code alteration (back-patching) using informa\u00adtion gathered after instruction selection. \n Consider the addition of another disambiguating predicate DontTrySob before NotBusy in the Decre\u00ad m~nt \nproduction to obtain Longta -> -Long?a Long?b -(~b) DontTryS@ (j a) NotBusy ( #a) EMIT (~ decl #a) In \nthis case, both DontTrySob and Not8usy must evaluate to true in order that the production be applicable. \nThe context in which an evaluation takes place is determined by interrogating the left context of the \nevaluation (i.e. looking at symbols below the RHS on the stack). In the as\u00adsignment context := A -B l \n, the target address A is known after step (1) in the previous section. If a here is not the same as \nA (a globa l attri\u00adbute keeps track of A) or if the following opera\u00adtor (which is determined by examining \nthe 1 ook\u00adahead already provided by the parser) is not a greater-than or greater-than-or-equal com\u00adparison \nwith zero, then DontTrySob evaluates to true (this evaluation does not include any less\u00adthan comparisons \nbecause the VAX-11/780 does not provide a corresponding special instruction for subtract-one-and-branch \n). If DontTrySob evalu\u00ad ates to false, recognition of this production is blocked in hopes of matching \none of the following longer productions so that a special instruction may be selected. If this hope fails \n(determined by the predicate SobOk), then an alternate longer production is matched and a sequence of \nequivalent instructions is emitted. In the following exam\u00adple, O< is a unary operator that tests if its \noperand is greater than zero. Similarly, o=< tests i f its operand is greater than or equal to zero. \nOrel stands for other tests with zero as the second opevand: Instruction -> := Long?d -Long?a Long*b \n-> := Long+d -Long?a Long*b O=< Long+c Labeltn w (~b) m (~dj ajc~n) EMIT (~ sobgeq ~d ~n) -> := Long?d \n-Long?a Long?b Orel?branch Lonq?c [.abel*n m (~b) - EMIT (~ decl ~d) EMIT (~ tstl ~C) EMIT (~branch \n~n) The disambiguating predicate = evaluates to true if c, d and a are the same variable, and the distance \nof n from the current position is not greater than the short-branch distance. Forward references are \ntaken as farther than a short\u00adbranch distance. If SobOk evaluates to false, we generate the sequence \nof instructions that would have been generated if DontTrySob evall~ated to true. This sequence consists \nof first decrement\u00ading d, then setting condition codes by testing c and finally branching on the condition \nof the ap\u00adpropriate condition-code bit(s). If the subtraction in step (5) of the previous section was \nperformed within the context of ad\u00ad dress calculation, the IR representation is: := A@ -81 Then, a longer \nproduction is matched: Addr*a -> @ -Long?b Longtc IsCon$ (~c) w (*b) ADDR (~-~b *C ?a) If the attribute \nvariable c s an integer con\u00adstant and b is a register, hen the subtraction is implicitly performed by \nus the index ad\u00ad n9 dressing mode supported by the architecture. ADDR composes an address attribute \na with negative displacement c and base regi ter b . Q. Experience with Cq Implementations of our code \ngenerator (Cg) exist on both the PDP-11/70 and the VAX-11/780. The generator occupies 86K bytes on the \nPDP-11/70 and 115K bytes on the VAX-11/780. The 11/70 implemen\u00adtation generates about 30 lines of assembler \ncode per second (real time), while the VAX version gen\u00aderates about 50 lines per second. I n contrast, \nthe C compiler produces about 40 lines per second on the PDP-11/70 and about 60 lines per second on the \nVAX-11/780. YACC [Johnson 75], running on the pDp-11/70 and the VAX-11/780, was used to generate tables \nfor these implementations. Its driver was modified to accommodate disambiguating predicates and create \nparsers For attribute grammars. It required about four minutes to process each of the (highly optim\u00adizing) \ncode-generation grammars. We have been able to accommodate both target\u00admachine independence and a good \ndeal of machine\u00addependent optimization with equal felicity. The reader may feel that there has to be \na catch some\u00ad where! However, experiments show that advantages do accrue in several areas: -- in human \ntime: Typically, one man month is needed to retarget the code generator to a new architecture. This amount \nof time seems remarkable when compared to writing a code generator from scratch. Much of the sav\u00adings \ncomes from the orthogonality of the attribute-grammar specification. For example, all addresses are handled \nby a single set of produc\u00ad tions. When constructing other parts of the specification, this information \nneed not be con\u00adtinually bol ne in mind. Important target-machine restrictions on the programming model \nsuch as: 1, register restrictions for indexing, multiplica\u00adtion and division, 2. data-type restrictions \nfor operands to instruc\u00adtions, 3. addressing-mode use and access restrictions, 4. irregular and inflexible \nregister architecture (e.g. Intel-8086), and 5. short displacements for branch instructions  are conveniently \nhandled by attributes and predi\u00ad cates that serve to band-aid such architectural restrictions. Thus, \nan architecture s weakness is made transparent to the compiler writer. --in size of generated target \ncode: The code-optimization results are very encourag\u00ad ing. Cg produces code comparable to hand-written \nassembler code for user programs, It produces code far superior to the unoptimizing C compiler on both \nthe PDP-11/70 and the VAX-11/780. In most cases, Cg produces code that uses 35-50% less space than the \ncode produced by the C compilers ,,-~,, prior to optimization. Even with the peephole optimization performed \nby the C compiler, the code produced by Cg is usually 5-10% smaller. In contrast, the Bliss-n code for \nthe PDP-11 demonstrates the advantage of good register allo\u00adcation [Wulf 81a]. It is slightly better \nthan Cg mainly because it uses registers for the user s locals even when these locals are not explicitly \ndeclared to be register variables. In our scheme, an adherence to tile single-pass code generation syndrome \nruled out iterative op\u00adtimization. We are caught in a, nasty cleft between the resulting optimization \ngained on the one hand and speed and size of the implementation on the other. Our intention has not been \nto be the frog in the fable that tried to swell Up to the size of a bull! --in size of the compiler back-end: \nProducing a small code generator was not among the primary goals. Nevertheless , the size turned out \nto be very reasonable, The code generator can be compiled and used on a mini-computer such as the PDP-11/70. \nAn amazingly wide variety of code-generation op\u00adtimization can be realized in a highly modular manner. \nAlmost all optimizations can be real ized by addition of new attribute grammar productions. Furthermore, \nwhen the code generator is retargeted to a new machine, most of the basic (non\u00adspecialized) productions \ncan be retained. In par\u00adticular, a simple (but un-optimized) code genera\u00adtor can be implemented for a \nmachine easily and rapidly. As time permits, and the need arises, improvements can be included by adding \nnew rules to the machine description arid automatically re\u00adgenerating the code generator. The chief differ\u00adence \nbetween an optimized and an unoptimized code generator is how carefully and thoroughly the pro\u00ad duction \nrules reflect the details and complexities of the target machine. Code generation driven by attribute-grammar \nmachine descriptions should be viewed as one more step in the advancement towards automated compiler \nconstruction where such com\u00adpilers can be as competent as today s hand-written compilers! ~. Future Research \nMore experiments with attribute grammar specifica\u00adtions are needed for othor machines, including special \npurpose computers such as the Cray-1, data-flow architectures and object-oriented archi\u00adtectures (Intel-iAPX \n432). The reader familiar with optimization literature will notice the ab\u00adsence of of iterative optimization. \nTo give a complete treatment of this challenging and mul\u00adtifac{?tecl problem of optimization, time-varying \nattributes [Skedzeleski 78] can be used to specify iterative algorithms in a non-procedural manner and \nefficiently implement them. Future research in iterative optimization can therefore proceed in two directions: \n(a) hiding iterations within attribute action\u00adsymbols, or (b) introducing an attribute-evaluator iterator. \n Such an iterative evaluation must be performed by traversing parse trees, but the order of evalua\u00adtion \nof attributes can calculated at evaluator-generation (code-ge~~rator generation) time rather than at \ncode-generation time. The drawback of such an iterative approach may very well be the amount of time \ntaken to achieve the optimization. More research is necessary to determine if the time taken justifies \nthe extra optimization gained. Acknowledqemen ts We gratefully acknowledge the assistance of Bruce Carneal, \nEd Desautels, Ray Finkel, Jack Fishburn, Sue Graham, Johannes Heigert, John Hennessy, Will Leland, Don \nNeuhengen, Steve Scalpone, Keith Thompson and Bill Wulf. Bibliography  [Aho 77] A,V. Aho and J,D. Unman, \nPrinci\u00adples of Compiler Design , Addison-Wesley publishing co., 1977. [Cattell 80] R.G.G. Cattell, Automatic \nDeriva\u00adtion of Code Generators from Machine Descriptions , ACM Trans. Programming Languages and Systems, \nVol . 2 No. 2 pp. 173-190, April 1980. e [Dijkstra 60] E.W. Dijkstra, Algol GO Transla\u00adtion , Supplement, \nAlgol 60 6ul\u00adletin 10, 1960, [Fraser 77] C.W. Fraser, Automatic Generation of Code Generators , PhD thesis, \nComputer Science Dept. , Yale University, New Haven , Corm. , 1977. [Fraser 79] C.W. Fraser, A Compact \nMachine Independent Peephole Optimizer , Principles of Programming Languages, 1979, [Fraser 80j C.W. \nFraser and J.W. Davidson, The! Design and Application of a Retargetable Peephole Optimizer , ACN Transactions \non Programming LanguageS and Systems, Vol. 2 No. 2, 1980. [Ganapathi 80] M. Ganapathi, Retargetable Code \nGeneration and Optimization using Attribute Grammars , Phil disserta\u00adtion, Technical Report #406, Com\u00adputer \nSciences Department, Univer\u00adsity of Wisconsin -Madison, 1980. [Ganapathi 81a] M. Ganapathi and C.N. Fischer, \nA Review of Automatic Code Genera\u00adtion Techniques , Technical Report #407, Computer Sciences Depart\u00adment, \nUniversity of Wisconsin -Madison, 1981. [Ganapathi 81b] M. Ganapathi, C.N. Fischer, S.J. Scalpone and \nK.C. Thompson, Linear Intermediate Representa\u00adtion for Portable Code Genera\u00adtion , Technical Report #435, \nCom\u00adputer Sciences Department, Univer\u00adsity of Wisconsin -Madison, 1981. [Ganapatbi 81c] M. Ganapathi \nand J.R. Goodman, Compiler and Operating System re\u00adquirements for 16-bit Microcomput\u00ader Architectures: \nIntel 8086, Zi\u00adlog Z8000 and Motorola MC68000 , Technical Report #452 , Computer Sciences Department, \nUniversity of Wisconsin -Madison, 1981. [Glanville 78] R.S. Glanville and S.L. Graham, A New Method for \nCompiler Code Gen\u00aderation , Conf. Record Fifth ACM Symp . Principles or Programming Languages, Jan. 1978. \n[Graham 80] S.L. Graham, Table-Driven Code Generation , IEEE Computer, Vol. 13 No. 8 pp. 25-34, Aug. \n1980. [Gries 71] D. Gries, Compiler Construction for Digital Computers , John Wiley &#38; sons, 1971. \n[Ichbi~h 79] J. Ichbiab et al., Preliminary Ada Reference Manual , SIGPLAN No\u00adtices, Vol. 14 No. 6, June \n1979. [Johnson 75] S.C. Johnson YACC -Yet Another Compiler Compiler , C.S. Tech Re\u00ad port #32, Bell Telephone \nLabora\u00ad tories, Murray Hill, New Jersey, 1975. [Knuth 68] D.E. Knuth, Semantics of Context-free Languages \n, Math. Systems rheory, vol. 2 No. 2 pp. 127-145, June 1968. [Koster 74] C.H.A. Koster, Using the CDL \nCompiler-Compiler , in Compiler Construction: An Advanced Course, F.L. Bauer and J. Eickel, eds., Springer-Verlag, \npp. 366-426, Ber\u00adlin 1974. [Lewis 76] P.M. Lewis, II, D.J. Rosenkrantz and R.E. Stearns, Compiler Design \nTheory, Addison-Wesley, Reading, Mass., 1976. [McKeeman 65] W.M, McKeeman, Peephole Optinliza\u00adtion , \nCACM, VO1 8. No. 7, 1965, [Milton 77] D,R, Milton, Syntactic Specifica\u00adtion and Analysis with Attribute \nGrammars , PhD thesis, University of Wisconsin-Madison, 1977. [Milton 79] D.R. Milton et al., An ALL(1) \nCompiler Generator , ACM Sigplan Symp. Compiler Construction, Boulder, Colo., Aug. 1979. [Newell 69] \nA. Newell and G.W, Ernst, GPS: A Case Study in Generality and Prob\u00adlem Solving , Academic Press, 1969. \n[Raiha 80] K.J. Raiha, Bibliography on At\u00adtribute Grammars , ACM Sigplan No\u00adtices, Vol. 15 No. 3 pp. \n35-44, , Mar 1980. [Ripken 77] K. Ripken, Formale Beschreibun von Maschinen, Implementierungen und Optimierender \nMaschinen\u00adcodeerzeugung aus Attributierten Programmgraphe , Technische Univer. Munchen, Munich, Germany, \nJuly 1977. [Ritchie 78] D.M. Ritchie and B.W. Kernighan, The C Programming Language , Prentice-Hall , \nEnglewood Cliffs, New Jersey, 1978. [Robertson 77] E.I.. Robertson, Code Generation for Short/Long Address \nMachines , Tech. Report, Computer Sciences Dept., University of Wisconsin-Madison, 1977. [Szymanski 78] \nT.G. Szymanski, Assembling Code for Machines with Span-Dependent Instructions , CACM, Vol. 21 No. 4 pp. \n300-308, April 1978. [Szymanski 80] T.G. Szymanski and B. Leverett, Chaining Span-Dependent Jump In\u00adstructions \n, ACM Transactions on Programming Languages and Systems, Vol , 2 No. 3, 19S30. [Watt 74] D.A. Watt, L.R. \nParsing of Affix Grammars , PhD thesis, University of Glasgow, Report #7, 1974, [Watt 77] D.A. Watt, \nThe Parsing Problem for Affix Grammars , Acts Informa\u00adtica, Springer Verlag, 1977, [Wulf 75] W. Wulf \net al. The Design of an Optimizing Compiler , American El\u00adsevier Publishing Co., 1975. [Wulf 80] W. Wulf \net al., An Overview OF the Production-Quality Compiler-Compiler Project , ~..7p~~Zy  - Vol. 13 No. \n[Wulf 81aSW. Wulf, personal communication. / T: [Wulf 81b] w. Wulf, Compilers and Computer Architecture \n, IEEE Computer, July 1981. \n\t\t\t", "proc_id": "582153", "abstract": "The instruction-set of a target architecture is represented as a set of attribute-grammar productions. A code generator is obtained automatically for any compiler using attributed parsing techniques. A compiler built on this model can automatically perform most popular machine-dependent optimizations, including peephole optimizations. The code generator is also easily retargetable to different machine architectures.", "authors": [{"name": "Mahadevan Ganapathi", "author_profile_id": "81100121014", "affiliation": "University of Wisconsin --- Madison", "person_id": "PP39028395", "email_address": "", "orcid_id": ""}, {"name": "Charles N. Fischer", "author_profile_id": "81100312451", "affiliation": "University of Wisconsin --- Madison", "person_id": "P43394", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/582153.582165", "year": "1982", "article_id": "582165", "conference": "POPL", "title": "Description-driven code generation using attribute grammars", "url": "http://dl.acm.org/citation.cfm?id=582165"}