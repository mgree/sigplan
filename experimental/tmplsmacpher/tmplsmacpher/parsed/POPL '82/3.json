{"article_publication_date": "01-25-1982", "fulltext": "\n MESSAGES AS ACTIVE AGENTS>+ David W. Wall Computer Science Department The Pennsylvania State University \nUniversity Park, PA 16802 Abstract 2. Background Network algorithms are usually stated from the viewpoint \nof the network nodes, but they can often be stated more clearly from the viewpoint of an active message, \na process that intentionally moves from node to node. This paper gives some examples of this notion, \nand then discusses a means of im\u00ad plementing it. This implementation applied in both directions also \ndemonstrates the logical equiva lence of the two viewpoints. 1. Introduction The difference between \na clear algorithm and an obscure one is often no more than a matter of finding the right viewpoint from \nwhich to describe it. For instance, an algorithm that makes active and confusing use of a growing and \nshrinking stack may well have a concise, elegant formulation in terms of recursion. Algorithms for use \nin a distributed network are often confusing. A process at one node de tides that something must be done, \nso it sends a message to another node to do something about it. No one node has a complete picture of \nwhat is hap\u00adpening, and nodes throw messages back and forth in a manner reminiscent of circus jugglers. \nThe pro\u00adgram at each node is likely to be written as a catalog of responses to the various message stimu \nli that can arrive, obscuring the actual algorithm. My claim is that there is a simple reason for this \nconfusion: In many cases, it is misleading to think of the processor node as the active agent in the \ncomputation. Instead, the active agent is the message, moving around the network performing different \nacts at each node. An algorithm stated from the viewpoint of the messages will often be simpler and clearer \nthan if stated from the view\u00adpoint of the processor nodes. * This material is based upon work supported \nby the National Science Foundation under Crant MCS-8102278. Permission to copy without fee all or part \nof thm material IS granted provided that the copies are not made or distributed for direct commercial \nadvantage, the ACM copyright notice and the title of the publication and its date appear, and notice \nis given that copying is by Permission of the Association for Computing Machinery. To copy otherwne, \nor to republish, requires a fee and/or specific permission. @ 1982 ACM 0-89791-065-6/82/001/0034 $00.75 \nResearchers have considered many ways of de\u00adscribing parallel and distributed algorithms. Ear\u00adly work \nin parallel algorithms emphasized communi\u00adcation by means of shared variables, using critical regions \nor monitors to arbitrate the sharing [4, 5, 11]. Growing interest in distributed systems has led to the \nexploration of message-based schemes. These include synchronous unbuffered approaches like Hoare s communicating \nsequential processes [10], Brinch Hansen s distributed processes [3], and Ada s notion of a rendezvous \n[12]. The idea is that a process trying to send a message or receive a message is suspended until there \nis a matching process trying to communicate with it in the other direction. We can contrast this with \nthe asynchronous buffered approach exemplified by PLITS [8], Gypsy J9], actors 12], mailboxes in SUE/360 \n[18], and UNIX pipes [17]. In this case a process may wait to receive a message, but it can send a message \nwithout waiting; the message is delivered to the destination process and buffered until the desti\u00adnation \ndecides to receive it, and the sending pro\u00adcess can meanwhile go on with whatever it was doing. Recent \npapers by Lauer and Needham 113] and Andrews [1] are notable attempts to unify various of the approaches. \nThe concept of active mes sages can be taken as yet another approach to the problem of writing distributed \nalgorithms, but it also represents another attempt to unify aspects of the different existing approaches. \nIf we view the world through the eyes of a message, it is no longer of critical importance whether that \nmessage was passed synchronously or asynchronously. More over, since active messages interact with each \nother by changing the states of the nodes, much of the work on ccmmrunication via shared variables may \nprove more useful in distributed networks than might otherwise have been expected. 3. A Simple Example \nA very simple distributed algorithm is the ARPANET algorithm for routing a message from a source to a \ndestination 114, 15] . Each node in the network maintains a table called NEXT, which tells which of its \nneighbors is the first node on the shortest path to each possible destination. If a node is trying to \nforward a message to some desti\u00adnation D, it looks in its table and forwards the message to node NEXT[D], \nwhich looks in its own looks to see if the message is addressed to this node ( me ), and reads it if \nso; otherwise the node looks in its forwarding table to decide which neighbor it should pass the message \non to. table algorithm and Since forwards it another a message can come would be stated as step, in at \nfollows. and any so on. time, this do forever wait for a message; D := destination; if D = me then process \nelse message fi N := send NEXT[D]; message to N od Each arrive. node loops forever waiting Each time \na message for arrives, messages to the node This is etraightforward, But this statement of the algorithm \nhas a strange property: It is an endless loop ! As a result, it hides the fact that there is something \nterminating, because after all any message does eventually reach its destination. If we state the algorithm \nfrom the point of view of the message, we get a somewhat different pro gram. D : = destination; while \nD z here do N := NEXT[D]; goto N od; process message The ~ in this program is not a transfer of control, \nbut instead represents a movement of this active message from one node to another, This formulation is \nslightly simpler, in the sense that it is shorter and not as deeply nested. But more important, it makes \nit clearer that the goal of the algorithm is to get the message to the desti nation! A message repeatedly \nlooks in a table for the right next node, and goes there, where it finds a new table, until it finally \nmanages to make D=he re. I have intentionally been a little sloppy about the variables in this example. \nIn an active message program, some of the variables are local to the nodes, like here , which identifies \nthe node at which the message currently is active, and the forwarding table NEXT. Others are local to \nthe active message, like D ; these correspond to the content of the message. We can distinguish between \nthese two kinds of variables in their declarations; in the examples that follow, vari\u00adables local to \nthe message will be declared as such, and all others are assumed to be local to the nodes. 4. Broadcast \nand Reverse Broadcast in a Tree Another distributed algorithm that benefits from a change of viewpoint \nis the forwarding of broadcast messages on a tree. A node can initiate a broadcast by sending a copy \nto each of its neighbors in the tree, and a node that receives such a broadcast will forward it by sending \na copy to each neighbor except the one on which it arrived. From the node s point of view it looks something \nlike this. do forever wait for a broadcast message; if it was initiated locally then LIST := all neighbors; \nsend msg to LIST else N := neighbor that sent it; LIST := all neighbors except N; send message to LIST; \nprocess message fi od Once again we have an infinite loop simply because a message can be initiated \nby any user at any time. Moreover, this version obecures the fact that we carry out a broadcast by having \nan initial node send a copy to all of its neighbors and subsequent nodes send it to all but one. The \nsequentiality of those two cases does not appear here, since from the node s viewpoint it can freely \nintermix the forwarding of foreign broadcasts and the ini\u00adtiating of its own broadcasts. If we state \nthis in terms of active messages, this becomes clearer. {P is local to the message} P := here; LIST := \nall neighbors; go to LIST; pass message to local user; while number of neighbors > 1 do LIST := all \nneighbors except P; P := here; go to LIST; pass message to local user od Note the implicit fanning out \nof the message pro\u00adduced by going to a set of nodes. A copy goes to each node in the destination set, \nand each copy continues executing irmnediately after the go to. We can run a broadcast in reverse to \ncollect values from the nodes and select the smallest one. Each leaf sends its value to its neighbor; \neach internal node waits until it has received values from all but one of its neighbors, and then sends \nthe smallest value it has seen, possibly its own, to the remaining neighbor. Event~ally these incoming \nsignals cross at some edge in the middle of the tree, and the endpoints of that edge can decide what \nto do with the answer. Because a node does not know whether it is an endpoint of the edge at which the \nmessages will happen to cross, the node-oriented program must force each node to wait for a crossing \nmessage to arrive, as we see in the following. VAL := value; LIST := neighbors; while size(LIST) > 1 \ndo wait for message; MSGVAL := value in message; LIST := LIST -{sender}; if MSGVAL < VAL then VAL : \n= MSGVAL fi od ; send VAL to only member of LIST; wait for message; MSGVAL i = value in message; if \nMSGVAL < VAL then VAL := MSGVAL fi; reverse broadcast by a forward broadcast whose only purpose is to \nassure the waiting nodes that all is well and they can stop waiting. Moreover, the code for comparing \nold and new VALS appears twice, which seems odd when we see what the active message version looks like. \nNow VAl is minimum; negotiate with sender of last message for next action. This has several ugly properties. \nFirst of all, most nodes never finish; they get hung up at the last wait, which might force us to follow \nthis {P, MSGVAI. are local to message] VAL := value; LIST := neighbors; while size(LIST) = 1 do MSGVAL \n:= VAL; P := here; go to only member of LIST; LIST := LIST -{P}; if MSGVAL < VAL then VAL := MSGVAL \nfi od; if size(LIST) = O then VAL is minimum; negotiate with sender of last message for action fi Now \nwe see that any message tries to minimize VAL whenever it finds itself in a new location. More\u00adover, \nwe don t have cases when somebody gets hung up early; messages that come into a node earlier than others \nsimply update the VAL in that node if appropriate and then quietly terminate. The last message that arrives \nsees that it is last and takes its minimum VAL on to the next node inward. When two messages cross in \nthe middle, those two are the only ones active, and they don t need to worry about anybody else. I am \na little disappointed that we can t make messages fan in implicitly as we made them fan out by going \nto a list in the example of forward broadcasting. To do this we would need something like a fork/join \nconstruct for fanning out and back in. This does not seem as elegant as simply going to a set of destinations; \nmoreover it is hard to imagine what we do with all of those VALS at the join. Even without implicit fanning \nin, the algorithm as stated from the viewpoint of active messages seems more attractive and easier to \nunderstand than the active node program. The reverse broadcast example has another interesting property. \nI stated it in a form that required a message to start at each node, but mes\u00adsages that started at internal \nnodes of the tree did nothing except initialize VAL and LIST. We may be able to accomplish this initialization \nin some othe~ way, for instance by making it the last step of a broadcast that initiates the reverse \nbroadcast. If so, messages need to be created only at the leaves of the tree, and the algorithm for those \nmessages is exactly the same. We can easily imagine a broadcast performed as described previously, with \nthe addition that a message reaching a leaf does not terminate, but instead goes on to perform a reverse \nbroadcast. Combining the two active message algorithms is a simple matter of concatenation. Combining \nthe two node\u00adoriented algorithms is messier; how do we make the node follow a broadcast by a reverse \nbroadcast and still allow unrelated broadcasts to take place? 5. Construction of a Minimum S~anning Tree \nThe reader might reasonably argue that the previous examples are somewhat contrived. After all, the ARPANET \nalgorithm and the broadcasting algorithm (and to a smaller extent the reverse broadcast) are in fact \nmessage routing algorithms; should we be surprised that we can state them more conveniently from the \nviewpoint of the messages being routed? To demonstrate that the notion of active mes\u00adsages is more generally \napplicable, let us con\u00adsider an algorithm developed by Dalal [7] and sim\u00adplified by Wall [19] by which \na network can con\u00adstruct its own minimum spanning tree. Although an important reason for constructing \nsuch a tree is to use it for routing messages, the algorithm is not itself a routing algorithm, and the \nconstruc\u00adtion of the tree is typical of distributed graph algorithms. Each node begins with a Local Image \nof the network, which tells it the cost of each incident edge, and finishes with a Local Image of the \nminimum spanning tree, which tells it which incident edges are branches of the MST. The algorithm is \nbased on Prim s principle [16], which states that if we consider any frag\u00adment of a minimum spanning \ntree, even one consist\u00ading only of a single node, then the cheapest edge connecting the fragment to a \nnode outside the fragment is a branch of the MST. We can describe the algorithm in terms of obligations. \nEach node starts out with an obligation to mark a branch, which it first tries to fulfill by marking \nthe cheapest incident edge. If both endpoints of an edge mark it as a branch, one of them must resume \nits obligation by examining the larger fragment that was created by the branch in question and trying \nto mark a new branch connecting this frag\u00adment to some other. If this results in another doubly marked \nbranch, the conflict is resolved the same way. Eventually a node will find that it cannot fulfill its \nobligation because its fragment contains all the nodes, and at this point the algorithm is finished. \nAs it performs the algorithm, each node main tains a description of its perception of the fragment to \nwhich it belongs. The actual fragment may be bigger than this, since branches may have been created about \nwhich this node has not yet received word. This fragment description includes a list of the edges that \nlead to nodes outside this node s perception of the fragment. The node transmits this fragment description \nwith each mes\u00adsage it sends, and the destination node merges it with its own fragment description, giving \nthat node a possibly more complete picture of the actual fragment to which both nodes belong. When cwo \nendpoints of the edge mark it as a branch, one of them must resume its obligation. We assume a subsidiary \nroutine called DECIDE that each endpoint can call to decide if it should resume its obligation. DECIDE \ncan make this deci\u00adsion in any manner whatsoever, as long as it will decide consistently from one node \nto another. Dalal s algorithm consists of a main program and a subroutine that we can call FULFILL-OBLIGATION, \nwhich is performed by a node trying to add a branch to the tree. FULFILL-OBLIGATION can be stated as \nfollows. routine FULFILL-OBLIGATION: examine the fragment description and let the edge {NEAR, FAR] be \nthe smallest edge leaving the fragment, where NEAR is the endpoint in the fragment; if there is no such \nedge then TREE-DONE : = true elseif NEAR = me then if edge is already marked then send obligation to \nFAR else mark the edge as a branch; send mark this branch to FAR fi else send obligation to NEAR fi \n Each node starts by initializing its fragment description to contain only itself, and the edges leading \naway from the fragment to be exactly those edges incident to that node. The main program can then be \nstated as follows., TREE-DONE : = false; FULFILL-OBLIGATION; while not TREE-DONE do wait for a message \nand merge its fragment state; if message is obligation then FULFILL-OBLIGATION else {message ia mark \nthis branch } if edge not already marked then mark it as a branch else {doubly-marked} if DECIDE selects \nme over the message source then FULFILL OBLIGATION fi fi fi od A correctness proof of this algorithm \n[19, 20] argues that partial correctness follows from Prim s principle, and that the algorithm ter\u00adminates \nbecause a given obligation cannot be passed around very far without acquiring a frag\u00adment description \nthat is bigger than it used to be. This suggests that the algorithm might be easier to understand if \ndescribed in terms of active messages, as follows. {SUCCESS, TRRE-DONE, the two endpoints, and a fragment \ndescription are local to the message. Each node also has a fragment description. } SUCCESS : = false; \nTREE-DONE : = false; repeat select smallest edge {NEAR,FAR} while ( there is such an edge) and (NEAR \nz here) do go to NEAR and merge fragment; select smallest edge {NEAR,FAR} od ; if there is no such edge \nthen TREE-DONE : = true elseif edge is already marked then go to FAR and merge fragment else mark it \nas a branch; go to FAR and merge fragment; if edge not already marked then mark it as a branch; SUCCESS \n:= true elseif DECIDE picks FAR then SUCCESS := true fi fi until SUCCESS or TREE-DONE We no longer need \nFULFILL-OBLIGATION to be a sub rout ine, since as soon as an active message is sure that it has fulfilled \nits obligation it simply terminates. The sequence that marks a branch has a certain elegance to it; to \nmark a branch, a message simply records it at one endpoint, goes to the oth er endpoint, and records \nit there. Whether or not it has fulfilled its obligation rests on whether or not some other message has \nalready marked this branch at the other endpoint, a fact that is also made somewhat clearer in this version. \nMost striking of all, perhaps, is the fact that the active message algorithm exhibits the nested loop \nstructure of the process. A message proceeds by looking for an edge to mark as a branch, but it can only \ndo this if the edge it finds is incident to the node at which the message currently resides. Otherwise, \nit must travel to the node in question and verify that this node knows of no better edge. This leads \nto the inner loop at the beginning. When the message finally reaches a node at which its obligation can \nbe ful filled, it marks the branch as we discussed a moment ago. If it finds that the branch is doubly \nmarked, it may have to resume its obliga\u00adtion and start over, which leads to the outer loop, Of course, \nall of this is true of the node\u00adoriented version as well, but it appears only in the proof and not in \nthe program. 6. Implementation The initial reaction to this method of describing a distributed algorithm \nis often dismay at the thought of passing programs around in the messages. This is one possible implementation, \nof course, but a better solution exists. To execute an active node program, we must get copies of it \ninto the nodes. We can do the same thing with an active message program. Each node keeps a set of active \nmessage programs; each message identifies the program it is executing and its position in that program, \nand includes values for its local variables. Each node cycles in an endless loop waiting for such messages. \nWhen such a message arrives, the .no de branches to the app ro\u00adpriate point in the appropriate active \nmessage pro\u00adgram and executes it until it either terminates or reaches a ~. In the latter case, the node \nsends a message with the new state of the active message to the set of nodes in the ~; in either case, \nit then resumes waiting. In some sense, therefore, all we are doing is giving a more elegant means of \ndescribing the re\u00adsponses that a node can have to the various mes\u00adsage stimuli that can arrive. It is \ntempting to compare an active message to a coroutine; but a node resumes this coroutine at a location \nthat depends on where some other node suspended it. An interesting point to note about all this is that \nit really doesntt matter whether we do the message passing synchronously without buffers or asynchronously \nwith buff ers. The loop that repeatedly waits for a message and responds to it is now invisible, and \nwe can write it either way depending on the distributed environment in which the algorithm is being performed. \nIf we use syn\u00ad chronous message passing, a node must be able to wait for a message without specifying \nthe node from which it is expected; moreover, the correct\u00ad ness of an algorithm may still depend on which \nunderlying model we are using. Nevertheless the notion of structuring the computation from the viewpoint \nof the messages does not seem to require the assumption that messages are passed asynchro\u00ad nous ly. 7. \nEquivalence of Active Messages and Active Nodes The previous section sketched a means of implementing \nactive messages without passing pro grams around, essentially by simulating active messages by means \nof active nodes. We can use the same idea to simulate active nodes by means of active messages. To simulate \nan active message program in an active node notation, we kept the program at the nodes and let the message \ncontain only the loca\u00adtion counter. Conversely, we can keep an active node program in the messages (at \nleast logically) and let the node contain only the location counter of the wait statement it is currently \nstuck at. When a message is received, execution is resumed and continues until another wait is encountered. \nBecause a node can send a message and then go on doing something else before it goes back to wait\u00ading, \nthis equivalence does depend on our active message notation including some kind of forking facility beyond \nmerely going to a set of destina tions, but this should not distress us too much. The point is that it \ndoesn t matter whether we suppose that the node was spurred to action by receiving the message, or that \nthe message arrived at the node and then looked around to see what needed to be done. The two are entirely \nequivalent, and our choice between them should be based upon the relative clarity and elegance of the \nresulting programs. 8. Conclusions and Future Work We have shown a simple way of describing dis\u00adtributed \nalgorithms from the point of view of the messages rather than the nodes, and have given a simple means \nof implementing this idea practi. tally. We can use this implementation in both directions to demonstrate \nthe logical equivalence of $he two methods of description, which leaves us free to choose between them \naccording to con\u00adsiderations of readability and clarity. It is entirely possible that the two approaches \nof active nodes and active messages represent special cases of a general approach in which we can consider \nboth nodes and messages m be equal partners. For instance, while the ARPANET routing algorithm was clearer \nfrom the message s viewpoint, the associated algorithm for maintaining the NEXT routing tables, which \nurrtil recently was done by periodically exchanging these tables with each imnediate neighbor [ 14], \nwould probably be difficult to state as an active mes sage program. Development of a technique in which \nboth nodes and messagea are considered to be active agents presents an interesting challenge, A related \nobservation is that in this paper I assumed that only one active message existed at any given node at \nany given time, because the traditional active node approach usually assumed that only one message was \nreceived at a time, oth ers being buffered or delayed until the node was ready to receive them. If we \nview our messages as active processes, and if a network node is capable of multiprocessing or multiprogramming, \nthen there is no reason we can t have several active messages at the same node. This raises some timing \nprob lems with regard to accessing the variables local to the node, but the general problem of synchron \nizing use of shared variables is old and well explored [4, 5, 11] . Even if it turns out that these two \ngener\u00adalizations are of little use, we need to be able to design an environment in which active node \npro grams and active message programs can be inter mixed. Organization of the control program at the \nprocessor nodes to admit this kind of structure may be a practical problem of some interest. Postscript \nI have an interesting additional argument for the appropriateness of active messages. After writing this \npaper, I gave it to Cherry s fiendish STYLE program [6], which among other things asked me to reword \nplaces where I used the passive voice. A surprising proportion of these occurred in the passages about \nnode-oriented algorithms: As I was writing, phrases like the message is passed or the obligation ia transferred \nfelt like the moat comfortable phrases to use, even though the node was theoretically the hero of my \ntale. The conclusion I draw is only half in jest. Just aa we shouldn t torture our writing style by inappropriately \nresorting to the passive voice, we shouldn t torture our programming style by re stricting our attention \nto the passive elements of our program. Acknowledgements I would like to thank Susan Owicki, Brent Hailpern, \nJohn Gilbert, Robert Roos, and V. Ashok for discussions about network algorithms that led directly or \nindirectly to the idea of active mes\u00adsages. References [1] Gregory R. Andrews. Synchronizing resources. \nTransactions on Programming Languages and Systems ~, 4 (October 1981), pages 405-430. [2] Russell Atkinson \nand Carl Hewitt. Synchron\u00adization in actor systems. Proceedings of the Four th Symp osium on the Principles \nof Programming Languages (January 1977), pages 267-280. [3] Per Brinch Hansen. Distributed processes: \nA concurrent programming concept. Communications of the ACM 21, 11 (November 1978), pages 934 941. 14] \nPer Brinch Hansen. Operating System Princi-I&#38;SE.. Prentice-Hall, 1973. 15] Per Brinch Hanaen. The \nprogramming language Concurrent Pascal. IEEE Transactions on Software Engineering SE-1, 2 (June 1975) \n, pages 199-207. [6] Lorinda Cherry. Computer aids for writers. SIGPLAN Notices 16, 6 (June 1981), pages \n61-67. [7] Yogen Kantilal Dalal. Broadcast protocols in Packet Switched Computer Networks, PhD thesis, \nStanford University, April 1977. (Computer Systems Lab Technical Report 128. ) [8] Jerome A. Feldman. \nHigh level programming for distributed computing. Communications of the ACM 22, 6 (June 1979), pages \n353-368, [9] Donald I. Good, Richard M, Cohen, and James Kee ton-Williams. Principles of proving concurrent \nprograms in Gypsy, Proceedings of the Sixth Sym ~osium on the Principles of Programming Languages (January \n1979) , pages 42-52. [10] C.A. R. Hoare. Communicating sequential pro\u00adcesses, Communications of the ACM \n21, 8 (August 1978), pages 666-677. [11] C,A. R, Hoare. Monitors: An operating system structuring concept. \nCommunications of the ACM 17, 10 (October 1974), pages 549-557. [12 ] J. D. Ichbiah, et al. Preliminary \nAha refer\u00adence manual. SIGPLAN Notices 14, 6 (June 1979) , part A. [13 ] Hugh C. Lauer and Roger M. Needham. \nOn the duality of operating system structures. Proceed\u00adings of the Second International Symposium on \nOper sting Systems, IRIA, 1978. Also appeared in Opera\u00adting Systems Reivew 13, 2 (April 1979), pages \n3-19. [14 ] John M. McQuillan. Adaptive Routing Algo\u00adrithms for Distributed Networks. PhD thesis, Har\u00advard \nUniversity, May 1974 (BBN Report 2831), [15 ] John M. McQuillan, Ira Richer, and Eric C. Rosen. An overview \nof the new routing algorithm for the ARPANET. Sixth Data Communications Sympo\u00ad~, November 1979, pages \n63-68. [16] R,C. Prim. Shortest connection networks and some generalizations. Bell System Technical Jour\u00adnal \n36 6 (November 1957), pages 1389-1401. , [17] Dennis M. Ritchie and Ken Thompson. The UNIX time-sharing \nsystem. Communications of the ACM 17, 7 (July 1974), pages 365-375. [18] K.C. Sevcik, J.W. Atwood, M.S. \nGrushcow, R.C. HoIt, J.J. Homing, and D. Tsichritzis. Project SUE as a learning experience. Proceedings \nof the Fall Joint Computer Conference 1972, Volume 41, pages 331-338. AFIPS Press. [19] David Wayne \nWall. Mechanisms fer Broadcast and Selective Broadcast. PhD thesis, Stanford University, June 1980 (Computer \nSystems Lab Tech\u00adnical Report 190). [20] David W. Wall and Susan Owicki. The correct\u00adness of two network \nalgorithms. In preparation. \n\t\t\t", "proc_id": "582153", "abstract": "Network algorithms are usually stated from the viewpoint of the network nodes, but they can often be stated more clearly from the viewpoint of an active message, a process that intentionally moves from node to node. This paper gives some examples of this notion, and then discusses a means of implementing it. This implementation applied in both directions also demonstrates the logical equivalence of the two viewpoints.", "authors": [{"name": "David W. Wall", "author_profile_id": "81100439103", "affiliation": "The Pennsylvania State University, University Park, PA", "person_id": "PP39042865", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/582153.582157", "year": "1982", "article_id": "582157", "conference": "POPL", "title": "Messages as active agents", "url": "http://dl.acm.org/citation.cfm?id=582157"}