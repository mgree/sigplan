{"article_publication_date": "01-25-1982", "fulltext": "\n A Semantics-Directed Compiler Generator Lawrence Paulson Department of Computer Science Stanford University \nCurrent address: Computer Laboratory, University of Cambridge, CambridgeCB23QG, U. K, Work supported \nin pat by Advanced ReseazchProjects Agency Corstr@ MDA 903-76-C-02306and Joint ServicesElectronics Progrsru \nCmrtmt DAAG 29\u00ad79-C4047. 1. Introduction Language designers must compromise between their goals and resources, \nand reconcile conflicting features into a harmonious whole. They cannot try out their ideas on real programs, \nbecause of the cost and time required to write compilers. This paper describes research [8] that makes \nit easier to design, document, and implement programming languages. There is no widely accepted notation \nfor describing programming languages, so the designets generally use a mixture of Backus-Naur Form (BNF) \nand English. The resulting document is often confusing, ambiguous, and tedious. A bad document compounds \nthe burden on the compiler writers. Before they can begin to implement the language, they must ur,derstand \nthe document and resolve its ambiguities. No wonder compilers are so often incompatible with one another, \nand that programs written in high-level languages are not transportable. This paper introduces a formal \nnotition, the semanfic grammar, for defining programming languages. Semantic grammars combine denotational \nsemantics and attribute grammars. They describe syntax and wmantics together, without separate tists \nof formulas or rules that need to be put into correspondence. They handle both static and dynamic semantics, \nboth compile-and run-time actions. Ihey describe languages at a high level of abstraction. I have implemented \na compiler generator that converts semantic grammars into compilers. It has generated compilers for Pascal, \nFortran, and other languages. Using the Pascal grammar, it has executed an intricate seven-page program: \nan LR(0) parser constructor. The compiler generator consists of a grammar analyzer, universal translator, \nand srack machine. The gmmmar analyzer converts a semantic grammar into a Ianguagc description file. \nThe universal translator reads the tile and compiles programs into stack machine instructions, reporting \nsemantic errors. The stack machine reads the program s inptz~ execukx the instructions, and prints the \noutput- Ttte compiler generator is the starting point for many systems that translate programs into another \nformalism. For program vcritication, it can translate programs into vcriftcation conditions. Permission \nto copy without fee all or part of this material is granted provided that the copies are not made or \ndistributed for direct commercial advantage, the Acfli copyright notice and the title of the publication \nand its date appear, and notice is given that copying is by permission of the Association for Computing \nhiachinery. To copy otherwise, or to republish, requiras a fee andlor specific permission. @ 1982 ACM0-89791-065-6/82/001 \n/02M $00.75 For efficient compilation, it can translate programs into intermediate code, which a separate \nprogram could usc to generate optimized code. l hecompile rgeneratorcan provide compatible compilers \non different machines it is transportable, since it is written in standard l%scaL 2. Compiler Generators \nA compiler generator is a program that converts a formaf description of a programming Ianguagc into a \ncompiler for that language. The description may take many forms, but usually contains a large amount \nof program code. Several recent compiler generators accept descriptions in terms of attribute grammars \nor denotational semantics, NEATS isa compiler writing system that acccpts extended attribute grammars \n[12]. It provides domains to represent environments, parameters, types, and other language concepts. \nDuring compilation, it translates the source program into an output stream, calling a user procedure \nevery time an output symbol is generated. fhe NEATS attribute evaluator, which I have adopted, is fast \nand general. Raiha s [9] Helsinki Language Processor (IILP) has genenrted compilers, assemblers, and \npreprocessors fora dozen languages. Itconstructs a parse tree andevaluates attributes in alternating \npasses. Attributes are Algol procedures. Raiha is studying optimi zations, bccausc 111.1>compiles Pascal \nprograms at thirteen tokens pcr second, and consumes 90,000 words when compiling a one-page Euclid program. \nS1S, by Peter Mosses, is the first compiler generator that does notneed user-coded semantic routines \n[7], Instead itusesforrnal descriptions of the syntax and denotational semantics of the language to be \ncompiled. It constructs the parse tzee of a progratn, applies semantic fiurctions to it, and interprets \nthe result. Asix-lineprogratn requires several mintrtes ofcompttter time for both compilation and execution. \nDespite ttds inefficiency, S1S proves that compilers can be generated automatically from high-level language \ndescriptions. Ravi Sethi[ll] is experimenting with semantics-directed compilation. IIis simplifier applies \nfunctions to arguments and Iooksup identifiers in environments. Itcanresolvc references to labels in \ngoto-prograrns, eliminating the environment It produces acircular expression that matches the control \nflow of the program. It has processed many of Mosses s example languages, but does not execute programs. \nMartin Raskovsky s eompiler generator [lO] has produceda compiler forasirnpl elanguage. Inaseries ofsteps, \nittranslatcsa standard dcnotational definition into an low-level definition, then into the programming \nlanguage BCPL. The compiler generates instructiotts for the PDP-10 computer. My compiler generator is \nunique in that it accepts a semantic grammar, a readable notation for denotational semantics. Although \nits semantic notation is entirely nonprocedural, it is efficient enough to produce compilers for real \nLanguages such as Pascal, Iteancxecute programs wveralpages long, inseconds. 3. Semantic Grammars A semantic \ngrammar is an attribute grammar that uses the domains and formulas of denotational semantics. An attribute \ngrammar [5] is a context-free grammar augmented with attributes and attribute equations, which propagate \nsemantic information along the edges of the parse tree. Inherited attributes, prefixed by .$, move information \nfrom a node down to its children. Synthesized attributes, prefixed by t, move information from the children \nup to the parent. Consider the assignment command (statement), with the syntax command = variable := \nexpression. An attribute grammar can describe its syntax and static semantics determining the types of \nthe variable and expression, and checking that they are compatible. The type of a variabIe depends on \nthe current environment of declarations: a compiler s symbol table. Below, the nonterminal variable inherits \nan environment and synthesizes a type. The rule includes a constraint that the types of the variable \nand expression are equal; if the constraint does not hold, then the program has a semantic error. command< \n$env > = vanablti $env ttypel > .!._,! expression< $env Nype2 > constraint typel = type2 There are \ndifferent styles of writing attribute grammars. Watt and Madsen s ex[ended attribute grammars [12] express \nthe constraint typel = type2 implicitly by using the same attribute type with bolh the variable and the \nexpression. Such conventions shorten rules command< $.env > = variable< $env ttype > $,. ,, expression< \n$env ttype > Denotational semantics [3] uses a powerful theory of recursively defined data structures, \nlambda-expressions, and fixedpoints of tlmctions. A dcnotational definition expresses the semantics of \na construct in tcrrns of the semantics of its syntactic constituents. A traditional presentation includes \na context-free grammar, and introduces a semantic function for every nonterminal symbol in the grammar, \ndefined by caseson the rules rewriting that nonterminal. In contrast, a semantic grammar embeds the semantic \nfunctions in the context-free grammar. TO illustrate how to embed dynamic semantics in a semantic grammar \nrule, I will use a simple dcnotational description of assignment. It uscs a function var representing \nvariables and a function exp representing expressions, The assignment command evaluates the expression \nin the current states and passesthe result to var, which stores it in the state K As. var(exp S)S A traditional \ndenotational definition separates the semantics from the syntax, re-establishing the context by explicitly \nproviding var and exp with a syntactic construct ancl environment to operate on. 1Ierc com is the semantic \nfunction for commands: var[variabIe]env (cxp[expression]env s)s Fmbedding this function in the attribute \ngrammar rule yields the semantic grammar rule for the assignment command, The variable and expression \nsynthesize their semantic functions var and exp; the rule combines these to produce the semantics of \nthe command. command< $env t As. var(exp S)S>= variablti $ env ttype tvar> !,.-,, . expression< $env \nttype texp> A semantic grammar consists of domain definitions, expression definitions, attribute declarations, \nsemantic rules, and a resolution part. The domains and expressions are those of denotational semantics. \nThe symbol end terminates the grammar. Comments may appear anywhere; they begin with a number sign (#) \nand continue to the end of the line. 3.1. Domain Definitions Domains represent semantic data types, such \nas mappings, tuples, and tree structures. The domains INT, BOOL, and NAME (representing identifiers in \nsource programs) arc built-in. The user can define product, Junction, and union domains. A special case \nof union domain resembles an enumerated type of Pascal. If D , . . .. D are domains, and di denotes any \nvalue of Di, then the /oIlowingnare afso domains Domain Values DIX,,. XD n-tuples (du. . ., dn) D1-+D \nn functions from Dl to D2 f + ... + narneJDn]] narnei[di~ ~ yt~ ~ n[rramel Dl] [namel + . . . + narnen] \nname., 1 The domain definitions fist all the domains used to describe the semantics; there are no syntactic \ndomains. Definitions may be recursive, such as LIST, VAL, and TYPE below. domain LIST = [nil + cons[INT \nX LISTl]; #lists of integers VAL = [intVIINTl + array VIINT ~ VAL] ]; #data values ENV = NAME ~ TYPE \n#environments TYPE = [intTy + arrayTy~YPE] ]; #types S = NAME -VAIJ #states EXP=S*VA~ #expressions COM=S-+S: \n#commands VAR = VAL ~ COM; #variables The domain LIST deserves special mention, for it illustrates how \nto define list domains in terms of union domains. (The compiler generator does not provide lists as a \nprimilivc.) A list is eilhcr nil, or has the form consfirrt,lisl]; a list of n intcgem is cons[intl, \n..., cons~ntn,nil], ..] Domain names are written in UPPER CASE. The variables of a domain have the same \nname in lower case, possibly followed by digits. For example, the variables list, listO, and 1ist435 \nbelong to the domain LIST. com[variable: =expression]envs = 225 3.2. Expression Definitions Expressions \nmay contain integer and boolean constants, the bottom element 1, and variables. If d, e, f, e , ., e \nare expressions, and V, v,, . . .. Vmare variables, then th~ followin are .. also expressions: 1 the \nbottom element of any domain 0,1,2,,.. integer constants false,true boolean constants a, x98 ,. . . \nname constants d+e, d-e,... integer operators dande, dore,.. boolean operators deqe, dnee,... relational \noperators if d then e else f fi conditional expression (e ,.. ., en) n-tuple leteI extract left element \nof a tuple Av.e lambda-abstraction of e over v A(v ,.. ., vn).e abstraction over a tuple of variables \nfix i v.e fixedpoint of the fiurction Av.e fe application of function f to arg e Ietv=dine local definition \nof v to be d [dae]f function f updated at d If namel, . . .. name are the tags of a union domain, then \nthe following are expressions: namei[e~ injection creating a union vahte e I namei projection of e onto \ndomain D. 1 e is namei test that the tag of e is narnei Every expression belongs to a unique domain, \nand the grammar analyzer checks that operators are only applied to operands of the proper domain, detecting \nmany user errors. The expression definitions list the expressions used to describe the semantics; most \ngrammars define functions to check types or cmmbine declarations. Definitions maybe recursive. If a name \nis referenced before its definition, it must appear in the forward declarations, along with its domain. \nThe function append is an example of list manipulation and the case expression. forward append: (LIST \nX LIST) ~ LIST: define append = A(listl,list2). case listl of nil. list2, Consfint,list]. cons[int, append(list,list2)] \nesac; abort = As.J-; 3.3. Attribute Declarations The attribute declarations list every nonterminal symbol \nin the grammar, afong with the domains of its attributes. A dot separates inherited from synthesized \nattributes. In the following example, the symbol idenrfjler has an inherited attribute of domain ENV, \nand synthesized attributes of domains NAME and TYPE: attribute identifier< ENV.NAME,TYPE>; expression< \nENV.TYPE,EXP>; vanable<ENV.TYPE,VAR>; command< ENV.COM>; Four symbols are built in, for use only on the \nright side of rules number<.INT> representa an integer number, a string of digits name< .NAME> represents \nan identifier, an alphanumeric string beginning with a letter wherKBOOL.> represents the empty string; \nadds a constraint that the boolean condition is true uniqueName<.NAME> represents the empty string: each \ninstance in the parse tree generates a distinct name; useful for generating arbitrary labels 3.4. Rules \nTIM rules describe the syntax and semantics of a programming language. The tule part begins by naming \nthe start symbol of the syntax: rule start-symbol Terminal symbols, either alphanumeric reserved words \nor combinations of special characters, are enclosed in quotes: begin + := Many of the example rules \nin this paper use arrows t and $ to indicate whether an attribute is synthesized or inherited, but the \ncompiler generator expects rules in which commas separate the attributes, (The attribute declarations \nspecify the types of attributes.) The assignment rule becomes: command<env, As. var(exp S)S>= variabl~env, \ntype, var> ,..-J, .\u00ad expression<env, type, exp>; Any inherited attribute on the left side of a rule sees \na value from above in the parse tree. Likewise, any synthesized attribute on the right side sees a value \nfrom below. These are dejined attributes. If the defined attribute is an expression, then the value it \nsees must have the same form, or the program contains an error [12]. This pattern-matching is implemented \nas a list of constraints on the rule. Any synthesized attribute on the left side of a rule sends a value \nup into the parse tree. Likewise, any inherited attribute on the right side sends a value down. These \nare applied attributes. An applied attribute may contain any expression, as long as all of its free variables \nare defined elsewhere in the same rule. A rule may contain clauses of the form with x= y. This defines \nx to denote y in the rule. Strictly speaking, x is a defined attribute that sees the value y. an applied \naltribute. Using a with clause to extract the embedded expression in the rule for the assignment command \nyields an equivalent rule: command<env, corn> = variable<env, type, var> ,,._!! .\u00ad expression<env, type, \ncxp> with com = As. var(exp S)S There is no way to specify the lexical conventions of a language; the \nimplementation assumes the following: . the braces { and } enclose comments in programs  spaces, newlines, \nand comments separate numbers and identifiers  keywords are reserved  there are no string constants \n  3.5. Resolution Part The resolution part assigns binding powers and associativities to terminal symbols, \nfor eliminating syntactic ambiguities [1]. It can resolve the dangling-else problem and specify operator \nprecedence. Operators can be left-, right-, or non-associative; each left, right, or nonassoc declaration \ndefines a group of operators with the same binding power. 4. Example Grammar Here is a semantic grammar \nfor a tiny language lacking both side-effects and jump commands. Integer expressions may contain arithmetic \noperators. The condition of an if or while command may contain boolean connective and integer relational \noperators. There are integer variables and unbounded integer arrays. Input and output arc each a single \ninteger, transmitted via the pre-declared variables inpu[ and OULLW. The compiler generator has executed \na prime number program using this grammar. Although this grammar is trivial compared to Pascal s, it \nillustrates many of the same concepts. The grammar defines srafic semantics: the compiler functions of \ntype-checking and symbol table management. It defines a domain TYPE to hold the two types, integer and \narray, and a domain ENV to hold the types of variables in the program. In the Pascal grammar, TYPE is \na recursive tree stmcture, and ENV hoIds the meanings of constants, types, and procedures, as well as \nvariables. Watt [13] illustrates the basic techniques, The grammar also defines dynamic semantics: the \ncompiler function of code generation. Dcnotational semantics provides two frameworks for control flow: \na direct semantics can describe structured commands; a continuation semantics can describe any flowchart. \nThis grammar uses direct semantics an expression only computes a value, and a command only changes the \nvalues of variables. The PaseaI grammar also uses direct semantics, but the Fortran grammar requires \ncontinuations because of I:ortran s GO TO statements. Gordon [3] explains how to represent procedures, \nparameter passing, expressions, and other dynamic concepts. A grammar can also define axiomatic or operational \nsemantics, as Madsen [6] discusses. If the grammar detines axiomatic semantics, then the compiler generator \ntranslates a program into a fist of ver2ficaIion conditions assertions that, if proven, certifi that \nthe program is correct domain VAL = fintVIINT] + array V[lNT -+ INTl ]: #values: integers and arrays \nENV = NAME ~ TYPE; #environmentlypes of variables TYPE = fintTy + arrayTy]; #types S = NAME -VAL; #states: \nvalues of variables EXP=S~INT; #integer expressions COND = S ~ BOO~ #boolean conditions COM=S~S: #commands \n(statements) INTFILE = INT -INT; #integer mappings for IiO DATA = IN(FILE X INT; #1/O interface for stack \nmachine define #Input/output interface functions beginProg = A(intFile,int). ~ input ~ intV[intFile(l)] \n] -L; endProg = As. ( [1 ~ s( output ) IintVl 1, 1); attribute identifieKENV.NAME,TYPFD; expression< \nENV.EXP>; condition< ENV.COND>; command< ENV.COM>; declaration< .ENV~ program< .DATA~ DATA>; rule program \n #Look up the name in the environment; return the type identifier<cnv, name,env(name)> = name&#38;rneX \n# # ##Expressions expression<env,exp> = ( exprcssion<env,exp> ) ; #Integer constants expression<env, \nAs.inO = number<inO; #Integer variables expression<env, As.s(name)lintV> = identifier<env, narne,intTyX \n #Subscripted variables expression<env, As.s(name)larrayV (exp s)> = identitier<env, narne,arrayTy> [ \nexpression<env, exp> ] ; #Arithmetic operators expression<cnv, As.expl(s) + cxp2(s)> = expression< env,expl> \n+ expression<env,exp2>; expression<env, As.cxpl(s) exp2(s)> = expression<env, expl>  expression< env,exp2>; \nexpression<env, As.expl(s) * exp2(s)> = exprcssion<env,expl> * cxpression<env, exp2>; expression<env, \nAs.cxpl(s) div exp2(s)> = expression< env,expl> / expression< env,exp2>; # # # #Conditions for if and \nwhile commands condition<env,conLU = ( condltion<env,con~ ) ; #Comparisons of integers condition<env, \nAs.expl(s) It exp2(s)> = expression< env,expl> < exprcssion<env,exp2>; condition<env, As.expl(s) gt \nexp2(s)> = expression< env,expl> > expression<env, exp~; condition<env, As.cxpl(s) cqexp2(s)> = expression< \nenv,expl> = expression<env,exp2>; #Boolean connective condition<env, M. not cxmd(s)> = not condition<cnv,con~; \ncondition<env, As.condl(s) and cond2(s)> = condition<cnv,condl> and condition<env,cond2>; condition<env, \nAs.condl(s) or cond2(s)> = Condition<env,condl> or condition<env,cond2>; # # # # Cbmmands #Assignment \nto integer variable command<env, As.[name~intV[exp(s)] ]s> = identifier<env, name,intTy> := expression<env,exp>; \n#Assignment to array element command<env, As.[name~arrayV[[expl(s) -+exp2(s)]s namelarrayV]]s> = identifier<env, \nname,arrayTy> [ expression<env, expl> ]  expression< env,exp2>; . #Compound commands command<env,As. \ncom2 (coml s)> = command<env,cornl> ; command<env,com2>; #If commands command<env,M.if cond(s) then corn(s) \nelses fi> = i~ condition<env,cond> then commaird<env,coti fi ; #While commands command<env, tixkom.As.if \ncond(s) then com(comls) elses fi> = while condition<env,concD do command<env,coml> nod : # # # # Declarations \n #Empty declaration declaration< [ input ~intTy] ~ output ~int ry] 1> = ; #Integer variable declarations \ndeclaration< [name~ intTy]env> = int name< name> ; declaration<env>; #Array variable declarations declaration< \n[name~arrayTy]env> = array name< name> ; declaration<env>; program< Adata.endProg (corn (beginProg data))> \n= begin declaration<env> command<env,coti end ; resolution nonassoc not ; #most binding left * / and \n; left +  or ; nonassoc < > = : left ; ; #least binding end 5. Grammar Analyzer The grammar analyzer \nreads a semantic grammar and converts it into a language description file. The analyzer is organized \nlike a recursive descent compiler, and performs the following tasks: . Read a semantic grammar, parsing \nthe domain definitions, expression definitions, and rules. Check that the information is consistent. \n. Compute LALR(l) parse tables for the syntax part of the grammar [1]. output tie languagedescription \nfile, which contains the semantics of each rule and the parse tables. The language description file contains \nall the information needed by the universal translator. For each semantic rule, it gives the applied \nattribute expressions and attribute constraints. It also contains the pseudo attributes, which are generated \nby with clauses and uniqueName. All expressions are represented in postfix. The description file contains \ninformation that the translator needs to print out formulas and error messages. This includes the names \nof the domains and union tags, but not the definitions of the domains. Every attribute expression is \nfollowed by its location in the rule, for pinpointing semantic errors. When parsing a rule, the analyzer \nrecords all the free variables of applied attributes. These are the affribure variables that must be \ngiven values by appearing as defined attributes. In a recursive scan of the defining attribute expressions, \nthe analyzer accumulates constraints and defines the attribute variables. The attribute grammar should \nnot be circular, but there are no other restrictions on how attributes can depend upon each other. fhe \ngrammar analyzer does not check for circularity. The analyzer contains an LALR(l) parser generator that \nprocesses the syntactic part of the grammar, It checks that the grammar contains no unreachable or useless \nsymbols, computes its I,R(O) set of states, and adds LALR(l) lookahead. It resolves shift-reduce conflicts \naccording to the user s resolution part It generates parse tables, compressed by merging rows whenever \npossible. 6. Universal Translator Ihe universal translator can compile a program written in any language, \ngiven the proper language description file. It perfolms several steps: . Read a language description \nfile, reconstructing the tables and expressions. . Read a source program.  Print a listing of the program \ns semantic errors.  Print the semantic functirm describing the program.  Generate stack machine instructions \nfor the program.  6.1. Parsing The translator s shitt-reduce parser builds a directed acyclic graph \n(DAG) of attribute dependencies during parsing. (A DAG is a tree in which several parent nodes may share \nthe same child node.) Inherited attributes complicate the process. If there were only synthesized attributes, \nit would be possible to evaluate all of them bottom-up, like constructing a parse tree. This is because \nthe synthesized attributes on the right-hand side of a rule are all dctined when the parser reduces by \nthat rule. Inherited attributes, which represent the context of the reduction, may not yet be available. \nSo the parser substitutes dummy nodes for them, and patches the correct value in as soon as it appears. \n The following description is adapted from Madsen [6]. A shift-reduce parser uscs a stack to record the \ngrammar symbols parsed at a given point. To handle semantics, the stack is augmented with the synthesized \nand inherited attributes of every symbol. It represents each synthesized attribute as a pointer to a \nDAG, A symbol s synthesized attributes may depend upon its inherited attributes, which the DAG represents \nby dummy nodes. The stack represents an inherited attribute as a fixup-list locating its dummy nodes. \nThe parser redtrees by a rule _ yly2 yn by popping the right-side symbols and attributes, Yl. . . Yn, \nand pushing the left-side, X. The fixup-lists representing left-side inherited attributes are initially \nempty. They accumulate the locations of dummy nodes during evaluation of the rule s applied attributes: \nleft-side synthesized and right-side inhelited. After evahrating an inherited attribute, its fixup-list \nis scanned to replace its dummy nodes with the correct vahre. Each applied attribute is a function K$! \n~...Im, Sr ....Sn) of the rule s defined attributes: Icft-side inherited and right-side synthesized. \nEvaluation creates a DAG node labelled f, with pointers to the DAGs representing the synthesized attributes, \nand pointers to dummy nodes representing the inherited attributes. If the applied attribute is simply \na copy of a defined attribute, there is usually no need to create a new node. The rule s constraints, \nproduced by the grammar analyzer, are evahrated like applied attributes and accumulated, producing a \nlist of all the constraints in the program. Rules may contain pseudo attributes, which are created by \nwith clauses and tbc uniqueName symbol. Pseudo attributes are defined and applied in the same role. Since \nother applied attributes may depend on pSWdO attributes, the pseudos are evaluated first. Every use of \na pseudo attribute refers to the same DAG. This assures that with clauses arc only evaluated once, and \nthat every use of a uniqueName attribute gets the same generated name. The DAG consumes a Iot of storage, \nafthough no parse tree is constructed. The largest program compiled is a twenty-one page LALR(I) parser \ngenerator. Its DAG contains over 15,000 nodes, and swells to over 26,000 during simplification. 6.2. \nSimplification At first, each DAG node is labelled with a pointer to an attribute function, and its sons \nare arguments. The simplifier traverses the DAG depth-first, expanding function definitions and applying \nthem to arguments. The expanded timctiorr is linked back into the DAG so that shared nodes arc only expanded \nonce. Expanded parts of the DAG represent semantic formulas each node is labclled with an operator, and \nits sons are the operands. The DAG contains both semantics and attribute constraints. During expansion, \nthe simplifier checks that the constraints hold and executes all of the compile-time actions in the DAG. \nThe simplified DAG is ready for translation into machine instructions. Example simplifications: iftruethenaelsebfi \na Before After 3 5 2 left (a,b) a tag[a] I tag a ([a~b]f)a b An essential but difficult simplification \nis bekr-reductiorz: applying a Iambda-expression to its arguments by substituting the arguments for the \nbound variables. Substitution is slow, because it requires copying list structures, The simplifier avoids \none copy operation by simplifying during substitution, rather than after substitution in a separate traversal. \nWhenever possible, the simplifier substitutes for several variables at once to avoid repeated copying. \nWhen simplifying if x then y else z fi, the simplifier first simplifies x, to see whether it reduces \nto a constant (ttie or false). If so, it need simplify only one of y or z. The case expression uses a \nsimilar technique. Taken together, these improvements cause simplification to resemble symbolic execution \nof the expression, rather than a sequence of costly macro-expansions. 6.3. Representation of Bound Variables \nIf bound variables arc represented by identifiers, then substituting an argument for a variable may compute \nan incorrect result: a free variable of the argument may become bound because of a name conflict. The \ntranslator does not use variable names: it numbers bound variables by their depth in the nest of lambda-expressions \n[2]. For instance, the expression Ax.f x (Ay.g X y) has depth numbers Ax.f x. (Ay.g xl J d When inserting \nor removing lambdas in front of an expression, the translator must adjust the numbers of the expression \ns free variables. Every expression node contains a free vartable index indicating ils deepest variable \nreference. Indexes arc put in incrementally as an expression is buil~ the index of a node depends only \non the indexes of its children. In most casesit is the maximum of the indexes of the children; however, \nthe index of a lambda node is one less than that of its body, because lambda binds the top level free \nvariable. A closed expression is one with no free variables, The translator can easily identify closed \nexpressions, for they have a free variable index of zero. Many procedures that traverse expressions, \nsuch as substitution, perform operations only on the free variables. When they encounter a closed expression, \nthey return immediately. This saves the simplifier a tremendous amount of work. If a bound variable occurs \nmore than once, then beta\u00adrcduction replicates its argument. The simplifier performs no beta-reductions \nthat would replicate expressions requiring evaluation at run-time, which would make the object program \nless efficient. The simplifier only replicates simple arguments. The key queslion: what is simple? The \nsafest answer is that only atomic expressions are simple  variables, numbers, etc. But this does not \nhandle structured bound variables: (A(intl,int2). intl + int2) (3,8) The bound variable, (intl,int2), \nis referenced twice; the argumen~ (3,8), is not atomic. Beta-reduction is prohibited even though no component \nof the bound variable is used more than once. One solution is making the simplifier transform the above \nexpression into: ((Aintl int2. intl + int2) 3) 8 This allows beta-reductions, but the complete process \ncopies the firnction several times. Instead of relying on an expensive transformation, I generalize simple \nto include any closed expression; these can bc detected using the free variable index. Experience shows \nthat this version of simple aflows efficient simplification without exponential blow-up, afthough it \nis not obvious why. 6.4. Error Reporting The translator only recovers from semantic errors. If it encounters \na syntax error, it prints a list of expected symbols and halts. Automatic syntax error recovery is a \nseparate research problem. The simplifier evaluates the DAG depth-first and records all the semantic \nerrors: attributes that equal 1 and constraints that are not true. The error handler sorts the errors \nby line number in the source program, reads the program again, and prints the erroneous lines. It names \nthe relevant nontcrminal and attribute domain, and composes a message appropriate for the failed constraint. \nTo prevent one error from triggering many others, it patches the DAG with a dummy value. Sample listing \nfrom the translator VAR V,V: integer; {v is declared twice] ? Semantic error: Should be UNDEFINED x: \narray[l. . n] of integer; {nisnotdeclared} f Semantic error: IDENTIFIER Undefined attribute MODE i[8] \n:= O; {i is not an array} S~mantic error: COMPONENT Should be ARRAYTY 6.5. Code Generation Since the \nstack machine is oriented towards execution of lambda-calculus formulas, code generation is straightforward, \nusing a depth-first traversal of the simplified DAG. first the DAG is split into a forest of trees, to \nprevent a shared tree from being compiled more than oncc. Ashared tree iscompiled intoa parameter-less \nsubroutine that each of its parents calls. Burge [2] presents two methods for computing fixedpoints, \nin aclassic trade-off between generality and efficiency. fhegencral method performs a tortuous simulation \nof the fixedpoint eombinaton fix =Af.(Ag.f@g))(Ag. f7t3t3)) The efticicnt method only works for functions, \ncompiling them Iikeordinary recursive functions. Flxcdpoints arcmainly used to represent the semantics \nof while and goto statements; these ordy define functions. fherefor eluscth eefticien tmethod,andhave \nnot felt limited by its lack of generality. The fixedpoint s body must be a firnction or tuple of functions. \nThe code generator creates an entry point for each function, and compiles each use of the fixedpoint \ns bound variable into a call of the corresponding function. The code generator performs a few optimizations, \nbesidea those lhrrgerccommcnds. Forinstancc, itcmits codctodclctca bound variable after its last use, \nlocated during the DAG traversal. Given the expression (Ax.A B)y where A does not use x, it generates \ncode for A((Ax.B)Y) It also optimizes cases that cannot be illustrated by expression transformations. \nDeleting dead variables eliminates obsolete references to arrays and permits more efficient array compacting, \nas described below. The DAG may contain nanies from several sources: identifiers inthe source program, \nname constants in the semantic grammmar, and the name-generating nonterminal uniqueName. The code generator \nreplaces every distinct name with a distinct integer, so that no names appear in the object code. If \nthe grammar defines the state to be indexed by names, STAfX=NAMl?~ VALUE, itwillbe as efficicntasifthe \nstate were indexed by integer locations. 6.6. Garbage Collection Thesimplifier crcatcs alotofgarbagc. \nfhetranslator colleets afl of it using reference counting: it keeps track of how many pointers reference \neach node and periodically scans the list of aflocated nodes, deleting those no longer referenced. While \ncompiling the parser generator mentioned above, the garbage collector reclaims 133,000 nodes. References \nfrom local, temporary variables are not counted, This frees most of thecodc of the translator from any \ngarbage collection instructions, and reduces the overhead needed to maintain reference counts. A drawback \nis that the garbage collector can only be called between simplifier calls. Garbage collection consumes \nabout twenty percent of simplification time. 7, Stack Machine The stack machine executes the object code \nproduced by the universal translator. Ithasthe SECDarchitecture [2]: asmckof pending operands, an environment \nof bound variables, a control of instructions, and a dump of return addresses and environments. Control \ninstructions: halt stop program; print top of stack return return from function: restore state jump pc \njump to location pc falseJump pc jump to PCif stack top is false Instructions that push some value onto \nthe stack loatlConst value the given value loadPos int the value of the variable at depth int loadC1osure \npc the tirnction compiled at pc Instructions that pop several values f, x, y, . . . from the stack and \npush some result computed from them: plus thcsumx+y alter the updated function [x~y]f apply the result \nof the call ~x) pair the pair (x,y) left the component left x inject tag the injection tag[x] project \ntag the projection xltag is tag the inspection x is tag  7.1. Input/Output is another fiznction call, \nthe machine does not save the current Input and output are lists of integers. The machine reads a list \nkl,..., kn from the user s input, and pushes ([l-+kl] . . . [n~k~ ~, n) onto the stack, The machine \nexpects to find a similar data structure on the stack after executing the object program, and prints \nthe list it denotes. 7.2. Run Time Errors The value L ( bottom ) represents run-time errors. For instance, \na subscript out of bounds may set the state to J-, which will propagate to the end of the program, producing \na final state 1. The machine should halt immediately upon seeing -L, for prompt error detection, but \nnot every L indicates an erro~ 1 is sfso used for initialization. If -k is the operand of an instruction, \nthe machine usually halts, but some instructions return J-as the result, or treat 1 as an ordinary value, \nThe machine prints its current state upon halting. To aid debugging of user programs, every value of \n1 is flagged with the program location where it was generated. 7.3. Closures The lomlcfosure instruction \nbinds an entry point to the current environment, creating a functional value that may be stored like \nany other vahze. The function may be invoked later using the apply instruction. These values, called \nfunction closures, are an important difference between the S13CD machine and ordinary computers. Closures \nfree environments from the stack discipline (where they would be like static links) and allow them to \npersist indefinitely. Reference counting deletes environments that are no longer used. Any fambda-abstraction \nin the final DAG can be represented by a closure at nrntime. Optimization eliminates many closures that \nwould be invoked immediately after creation. 7.4. Array Compacting A denotational definition considers \narrays to be functions mapping subscripts to elements. The subscripkd assignment A[i]: = v is represented \nby the tirnction update fi+v]A, a mapping which takes i to v but otherwise is the same as A. After the \nloop fori:=l to5do A[O]:=i*i the value of A is represented [0-+25] [0-+-16] [0-+9] [0-+4] [0s1] -L \nThese association lists, or history sequences, waste storage and runtime. States, which are also mappings, \nsuffer the same problcm. Efficient execution is impossible UUICSSthe machine compacts association lists \ninto arrays. All data in the machine are referenced by pointers and may be shared. An association list \nmay be referenced by many pointers, some of them no longer needed but still persisting in the environment \nor dump. The machine must compact lists into arrays without disturbing the value seen by any of the pointers \nIn effect, the pointers divide a list into segments that must be compacted separately. l%e machine converts \na list of segments into a list of indexable arrays. The machine tries to eliminate unnecessary references \ninto lists, in order to allow the most compacting. 1 hc main source of obsolete references is tail-recursion, \nwhere a function calls another function and then returns. When a iimction s last action environment on \nthe dump; it will never be needed. lle function call is treated tike a jump. This optimization is essential \nbecause any loop or goto command causes tail-recursion. The most common type of tail recursion is the \ncode sequence apply;rcturn, which is easily recognized. Other forms of tail recursion are apply :jump \nx; ...; x:rcturn apply; useless-instruction; return The compiler generator uses peephole optimization \nand careful code generation to convert these to apply ;return, During execution of a program, the machine \ncompacts the same list many times. The machine updates an existing array, instead of allocating a new \none, as long as the new list elements fit within its bounds. When it allocates a new array, it allows \nroom for expansion above and below. The machine merges two segments into one if the reference separating \nthem disappears. The array compacting algorithm is complex and slow, but satisfactory. The stack machine \nhas executed prime number and Eight Queens programs that use arrays extensively. 7.5. Union Tags The \nmachine has instructions inject, project, and is for manipulating objects of union domains: inserting, \nremoving, and inspecting tags of union domains. In languages like Pascal, where types are known at compile \ntime, tags provide no useful information at run time. The universal translator has an option to suppress \ninject and project instructions, resulting in smaller, faster code. This is allowed only if the code \ncontains no is instructions, which require tags at run time. 8. Conclusions I treat well-known fanguages, \nas faithtlrlly a.. possible, to prove that my work applies to real problems. Pascal embodies the major \nlanguage concepts and has several formal definitions. My Pascal grammar covers afl static and dynamic \nsemantics except goto statements, real numbers, strings, aliasing, function side effects, procedures \npassed as parameters, etc. Most of the deficiencies stem from my attempt to make the semantica as high- \nIevel as possible; it avoids both continuations and machine locations, The grammar includes afl types \nand statements, recursive procedures, and block structure. I have checked most of it, by running test \nprograms on the compiler generator. It is only nineteen pages long (1400 lines), including comments: \none page of domains. five of functions, and thirteen of rules. Fortran, with its low-level state and \ncontrol structure, and non-recursive subroutines, contrasts WC1l with Pascal its grammar uses continuations \nand locations. Fortran s grammar is less complete than Pascatk, but still covers Iabclled COMMON blocks, \nEQUIVALENCE statements, DO sLllcmcnts with extended range. assigned and computed GO TO, unformatted input/output \nwith implied DO, subroutines, and functions.  8.1. Errors and Debugging Writing a grammar, like writing \na program, requires revising and debugging. The compiler generator recovers WCI1 from errors, even those \ndefined by a grammar for a source language, It prints the erroneous tine, points to the error, prints \na descriptive message, and usually continues processing. On the rare occasions that it aborts, the usual \ncause is subscript error: defining too many domains, terminal symbols, rules. Debugging user programs \non the stack machine is dit%cult. A user program aborts by producing the value -L; the only information \nreported is the current program counter and machine state, which is often incomprehensible. To locale \nthe error in the source program, you must study the program s machine code and semantic formula. 8.2. \nEfficiency The compiler generator is efficient enough to run experimental programs, but it is impractical \nfor production runs. Consider the following approximate statistics: The grammar analyzer is 4400 lines \nof Pascal. It processes the Pascal grammar in twenty-three seconds, producing a language description \nfile of 14,000 thirty-six-bit words. Half of this is parse tables, the rest attributes and definitions. \nThe universal translator is 3900 lines of Pascal. Using the Pascal grammar, it compiles programs at eight \nseconds per page, which is twenty-five times slower than the regular Pascal compiler. Storage limitations \nprevent it from compiling programs longer than twenty pages. The stack machine is 1300 lines of Pascal. \nIts speed varies considerably, depending on the grammar and user program, averaging 4000 instructions \nper second. It runs Pascal programs a thousand times slower than the regular Pascal system. I have run \ndozens of programs in several languages. The longest is an I.R(O) parser generator, a seven-page program \nthat uses nearly every feature of Pascal. I have also run prime number generators, Eight Queens solvers, \nand binary search tree sorters. The compiler generator is simple and compact, considering its capabilities. \nTogether with the Pascal grammar, it is an implementation of Pascal that is smaller than the standard \nPascal compiler at Stanford. My dissertation [8] suggests several ideas to improve the efficiency. . \nSeparate the semantic language into independent notations for static and dynamic semantics. Replace the \nsimplifier with two specialized, more efficient routines: an evaluator for static semantics, and an optimizer \nfor dynamic semantics. Compile programs one procedure at a time, so that programs of any size can be \ncompiled. Allow a grammar to designate certain nonterminal symbols, such as procedure, to be compilation \nunits. . Eliminate the stack machine. A real machine, with a run\u00adtime support package, can pcrforrn afl \nof its functions much more efficiently. 8.3. Implications for Language Design A programming language \nshould be formally defined even while it is being dcvclopcd, bccausc a formal definition highlights the \nIanguagc s inconsistcncics, Un forttrnatcly, most language designers find definitions too diftlcult to \nwrite. The compiler generator allows anyone to debug a formal definition, written as a semantic grammar. \nAs an extra incentive, it offers a free compiler for every definition. Compiling and executing test progrruns \non the compiler generator provides further insights into a language. Pascal s grammar reveals some trouble \nspots. Set expressions require special handling because they do not completely determine the set type; \nlikewise, the constant nil can have any pointer type, Enumcraled types declare constant identifiers as \na side-effect, complicating every rule that refers to types. Using a funclion s name to designate its \nreturn variable requires extra bookkeeping. A Fortran program can specify a variable s type, dimensions, \nCOMMON block. and stora~e ecruivalence in anv order. or not at al These opti(sns cause me&#38;iness through&#38;rt \nthe Fortran grammar, even though it imposes an order on declarations. Other Fortran constructs are so \ntroublesome that the grammar does not handle them at all. A DATA statement affects the initial state, \nbut may appear anywhere in a program. A statement function creates a local environment, but may implicitly \ndeclare global variables. A subscripted array variable is syntactically identical to a function call. \nI would not condemn a language construct simply because it was difficult to define. The fault might lie \nin the formalism: for instance, denotational semantics can not represent tasking. Still, semantic grammars, \nafong with the compiler generator, cart contribute to the design of consistent, clean, and simple programming \nlanguages. Acknowledgment. I would fike to thank my advisor, John Hennessy, for supervising and supporting \nthis research. References [1] Alfred V. Aho, Jeffrey D. Unman. Principles of Compi[er Design Addison-Wesley, \n1978. [2] W. H. Mrrge. Recursive Programming Addison-Wesley, 1976. Techniques [3] Michael Gordon. The \nDenotational Description oJProgramming Languages: An Introductiofi Springer-Verlag, 1979. [4] Neil \nD. Jones (editor). Semantic*Directed Compiler Generation. Spnnger-Verlag, 1980. [5] D. E. Knuth. Semantics \nof Context-Free Languages. Mathematical Systems Theoty 2:127 145, February, 1968. [6] Ole L, Madsen. \nOn DeJbring Semantics by Means of Extended Attribute Grammars. Technical Report DAIMI PB-109, Computer \nScience Departmen~ Pages 259 299 of Jones [4]. [7] Peter D. Mosses. MathematicalSemantics and Compiler \nGeneration. PhD thesis, Oxford University, 1975. [8] Lawrence Paulson. A Compiler Generator for Semantic \nGrammars. PhD thesis, Stanford University, 1982. Forthcoming. [9] Kari-Jouko Raiha. Experiences with \nthe Compiler Writing System HLP. Pages 350 362 of Jones [4]. [10] Martin Raskovsky. Step by Step Generation \nof a Compiler for Flow Diagram Lunguage Aarhus University, Denmark, January, 1980, with Jumps. Technical \nReport CSM-42, Department of Computer Science, University of Fxsex, June, 1981. [11] Ravi Sethi. Circular \nExpressions: Elimination of Static Environments, In S. Even, O. Kariv (editors), Eighth International \nColloquium Verlag, 1981. on Automat@ Lunguages and Programming, pages 378-392 Springer\u00ad [12] David A. \nWatt, O1e L, Madsen. Extended Attribute Grammars. ~cchnical Rcporl DAIM1 PB-105, Computer Science Department, \nAarhus University, Denmark, November, 1979. [13] David A. Watt. An Extended Attribute Grammar for Pascal. \nSIG1 LAN Notices 14:60 74, February, 1979.   \n\t\t\t", "proc_id": "582153", "abstract": "", "authors": [{"name": "Lawrence Paulson", "author_profile_id": "81100146628", "affiliation": "Stanford University and Computer Laboratory, University of Cambridge, U. K.", "person_id": "P169761", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/582153.582178", "year": "1982", "article_id": "582178", "conference": "POPL", "title": "A semantics-directed compiler generator", "url": "http://dl.acm.org/citation.cfm?id=582178"}