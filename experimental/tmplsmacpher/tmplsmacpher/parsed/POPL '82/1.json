{"article_publication_date": "01-25-1982", "fulltext": "\n Guardians and Actions: Linguistic Support for Robust, Distributed Programs Barbara Liskov Robert Scheifler \n Laboratory for Computer Science Massachusetts Institute of Technology Cambridge, MA 02139 Abstract \nThis paper presents an overview of an integrated programming language and ~lstem designed to support \nthe construction and maintenance of distributed programs: programs in which modules reside and execute \nat communicating, but geographically distinct, nodes. The language is intended to support a class of \napplications in which the manipulation and preservation of Iong.lived, on-line, distributed data is important. \nThe language addresses the writing of robust programs that survive hardware failures without loss of \ndistributed information and that provide highly concurrent access to that information while preserving \nits consistency. Several new linguistic constructs are provided; among them are atomic actions, and modules \ncalled guardians that survive node failures. :. Introduction Technological advances have r,lade it cost-effective \nto construct large systems from collections of computers connected via networks. To support such systems, \nthere is a growing need for effective ways t? organize and maintain distributed programs: programs in \nwhich modules reside and execute at communicating, but geographically distinct, locations. In this paper \nwe present an overview of an integrated programming language and system designed for this purpose. Distributed \nprograms run on nodes connected (only) via a cjommunicafions network. A node consists of one or more \nprocessors, one or more levels of memory, and any number of This research was supported in part by the \nAdvanced Research Projects Agency of the Department of Defense, monitored by the Office of Naval Research \nunder contl-act NOOO14.75,C.0661, and ill part by the National Science Foundation under 9rant NKX79-23769. \nPermission to copy whhout fee all or part of this material k granted provided that the copies are not \nmade or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication \nand its date appear, and notice k given that copying k by permission of the Association for Computing \nMachinery. To copY otherwise, or to republish, requires a fee and/or sPecific Permission. @ 1982 ACM \n0-89791-065-6/82/001/0007 $00.75 external devices. Different nodes may contain different kinds of processors \nand devices. The network may be Ionghaul or shorthaul, or any combination connected by gateways. Neither \nthe network nor any nodes need be reliable. However, we do assume that all failures can be detected as \nexplained in [14]. We also assume that message delay is long relative to the time needed to access local \nmemory, and therefore access to nonlocal data is significantly more expensive than access to local data. \nThe applications that can make effective use of a distributed organization differ in their requirements. \nWe have concentrated on a class of applications in which the manipulation and preservation of long-lived, \non-line data is important. Examples of such applications are banwng Systems, airline reservation systems, \noffice automation systems, data base systems, and various components of operating systems. In these systems, \nrealtime constraints are not severe, but reliable, available, distributed data is of primary importance. \nThe systems may serve a geographically distributed organization. Our language is intended to support \nthe implementation and execution of such systems. The application domain, together with our hardware \nassumptions, imposes a number of requirements Service, A major concern is to provide continuous service \nof the system as a whole in the face of node and network failures, One principle that applies here is \nthat local problems should be localized. For example, a program should be able to perfornl its task as \n10n9 as the Particular nodes it needs to communicate with are functioning and reachable. In addition, \nit should be possible for an application program to use replication of both data and processing as a \nmeans for increasing the availability of a service and for providing graceful degradation. Extensibility. \nAn important reason for wantin9 a distributed implementation is to make it easy to add and reconfigure \nhardware in order to increase processing power, decrease response time, or increase the availability \nof data. Similarly, it should be easy to remove hardware. In the same way that the physical system can \nbe extended or reconfigured, it must be possible to implement logical systems that can be expanded and \nreconfigured. For example, a banking system might need to grow or shrink to accommodate changing numbers \nof tellers. TO maintain continuity of service, it must be possible to make both logical and physical \nchanges dyrwnica//y, while the system continues to operate. Autonomy. We assume that nodes are owned \nby individuals or organizations, and that with ownership comes the desire to control how the node is \nused. For example, the owner may want control over what runs at the node, or, if the node provides a \nservice to programs running at other nodes, the owner may want control over when that service is available. \nFurther, the node might contain data that must remain resident at that node; for example, a multinational \norganization must abide by laws governing information flow among countries. The important point here \nis that the need for distribution arises . not only from efficiency considerations, but from political \nand sociological considerations as well. Distribution. The distribution of data and processing can have \na major impact on overall efficiency, both in terms of responsiveness, and cost. effective use of hardware. \nDistribution also affects availability, To create efficient, available systems while retaining autonomy, \nthe programmer needs explicit control over the placement of modules in the system. However, to support \na reasonable degree of modularity, changes in location of modules should have limited, localized effects \non the actual code. Concurrency. Another major reason for choosing a distributed implementation is to \ntake advantage of the potential concurrency in an application, thereby increasing efficiency and decreasing \nresponse time. For example, the clerks in an airline reservation system should be able to process requests \nconcurrently. Consistency. In almost any system where on-line data is being read and modified by oll.going \nactivities, there are important consistency constraints that must be maintained. For example, in an airline \nreservation system a flight must not be overbooked by more than a certain number of seats. Such constraints \napply not only to individual pieces of data, but to distributed sets of data as well. For example, when \nfunds are transferred from one account to another in a banking system, the net gain over the two accounts \nmust be zero. Also, data that is replicated to increase availability mu$t be kept consistent, In the \nremainder of this paper we discuss a programming language and system, called Argus, that satisfies these \nrequirements, To avoid rethinking issues that arise in sequential languages, we have chosen to base Argus \non an existing sequential language. CLU [16, 19] was chosen because it supports the construction of well. \nstructured programs through abstraction mechanisms, and because it is an object. oriented language, in \nwhich programs are naturally thought of as operating on (potentially) Iong.lived objects. Of the above \nrequirements, we found consistency the most difficult to meet. The main issues here are the coordination \nof concurrent activities (permitting concurrency but avoiding interference), and the masking of hardware \nfailures. Thus, to support consistency we had to devise methods for building a reliable system on unreliable \nhardware. Reliability is an area that has been almost completely ignored in programming languages (with \nthe exception of [21]). Yet our study of applications convinced us that consistency is a crucial requirement: \nan adequate language must provide a modular, reasonably automatic method for achieving consistency. Our \napproach is to-provide atornicify as fundamental concepts in the language, The concept of atomicity is \nnot original with our work, having been used extensively in data base applications [3, 4]. However, we \nbelieve the integration into a programming language of a general mechanism for achieving atomicity is \nquite novel. Atomicity is discussed in the next section. Section 3 presents an overview of Argus The \nmain features are guardfarm, the logical unit of distribution in our system, and atomic actions. Section \n4 illustrates most of the important features of the language with a simple mail system. The final section \nconcludes with a discussion of what has been accomplished. 2. Atomicity Our solution to the problem of \nmaintaining a consistent distributed state in the face of concurrent, potentially interfering activities, \nand in the face of system failures such as node crashes and network disruptions, is to make activities \natomic. The distributed state is a collection of data objects that reside at various locations in the \nnetwork. Some of these objects are stable: they are stored on Nabk storage cfevfces, for which the probability \nof loss of information due to hardware failures is extremely small (see [14]). Other objects are stored \nin volatile r7emory. Since the probability of Io;s of volatile objects is relatively high, these objects \nmust contain only redundant information if the system as a whole is to avoid loss of information. Such \nredundant information is useful for improving efficiency, e.g., an index for fast access into a data \nbase. An activity can be thought of as a process that attempts to examine and transform some objects \nin the distributed state from their current (initial) state to some new (final) state, with any number \nof intermediate state changes. Two properties distinguish an activity as being atomic: indivisibility \nand recoverability. By indivisibility, we mean that the execution of one activity never appears to overlap \n(or contain) the execution of any other activity, [f the objects being modified by one activity are observed \nover time by another activity, the latter activity will either always observe the initial states or always \nobserve the final states, but it will never observe intermediate states. By recoverability, we mean that \nthe overall effect of the activity is all.or. nothing: either all of the objects remain in their initial \nstate, or all change to their final state. If a failure occurs while an activity is running, either it \nmust be possible to complete the activity, or to restore all objects to their initial states. Supporting \nthese requirements as part of the semantics of a programming language imposes substantial implementation \ndifficulties. However, we believe atomic activities are necessary and are a fairly natural model for \na large class of applications. If the language/system does not provide actions, the user will be compelled \nto implement them, perhaps unwittingly reimplementing them with each new application, and may implement \nthem incorrectly. Therefore, atomicity must be an integral concept of the language. 1, We need merely \nassume that stable storage is accessible to every node in the system; it is not necessary that every \nnode have its own local stable storage devices, 2.1 Actions We call an atomic activity an action. An \naction may complete either by cornrrriffing or aborting. When an action aborts, the effect is as if the \naction had never begun: all modified objects are restored to their previous state. When an action commits, \nall modified objects take on their new states; only at this point do changes to stable objects become \npermanent. One simple way to implement the indivisibility property is to force actions to run sequentially. \nHowever, one of our goals is to provide a system that supports a high degree of concurrency. The usual \nmethod of providing indivisibility in the presence of concurrency, and the one we have adopted, is to \nguarantee serializability [5], namely, actions are scheduled in such a way that their overall effect \nis as if they had been run sequentially in some order. To prevent one action from observing or interfering \nwith the intermediate states of another action, we need to synchronize access to shared objects. In addition, \nto implement the recoverability property, we need to be able to undo the changes made to objects by aborted \nactions. Since synchronization and recovery are likely to be somewhat expensive to implement, we do not \nprovide these properties for all objects For example, objects that are purely local to a single action \ndo not require these properties. The objects that do provide these properties are called atomic objects, \nand we restrict our notion of atomicity to cover only access to atomic objects. That is, atomicity is \nguaranteed only when the objects shared by actions are atomic objects. Atomic objects are encapsulated \nwithin atomic abstract data types. An abstract data type consists of a set of objects and a set of primitive \noperations; the primitive operations are the only means of accessing and manipulating the objects [15]. \nAtomic types have operations just like normal data types, except that operation calls provide indivisibility \nand recoverability for the calling actions. Some atomic types are built. in while others are user-defined. \nArgus provides, as built-in types, atomic arrays, records, and variants, with operations nearly identical \nto the normal arrays, records, and variants provided in CLU. In addition, objects of built-in scalar \ntypes, such as characters and integers, are atomic, as are structured objects of built. in immutable \ntypes, such as strings, whose components cannot change overtime. Our implementation of built-in atomic \nobjects is based on a fairly simple locking model. There are two kinds of locks: read locks and write \nlocks. Before an action uses an object, it must acquire a lock in the appropriate mode. The usual locking \nrules apply: multiple readers are allowed, but readers exclude writers and a writer excludes readers \nand all other writers. When a write lock is obtained, a version of the object is made, and the action \noperates on this version, If, ultimately, the action commits, this version will be retained, and the \nold version discarded. If the action aborts, this version will be discarded, and the old version retained. \nFor example, atomic records have the usual component selection and update operations, but the selection \noperations obtain a read lock on the record (not the component), and the update operations obtain a write \nlock and create a version of the record the first time the action modifies the record. .%rce changes \nbecome permanent only at the granularity of entire aciions, acquired locks and versions can be kept in \nvolatile storage while an action executes, All locks acquired by an action are held until the completion \nof that action, a simplification of standard two-phase locking [7]. This rule avoids the problem of cascading \naborts: if a lock on an object could be released early, and the action later aborted, any action that \nhad observed the new state of that object would also have to be aborted. Within the framework of actions, \nthere is a straightforward way to deal with hardware failures at a node: they simply force the node to \ncrash, which in turn forces actions to abort. The volatile information (e.g., versions) kept for an action \nthat has not yet finished will be lost if the node crashes. If this happens the action must be forced \nto abort. To ensure that the action will abort, a two. phase commit protocol [8] is used, In the first \nphase, an attempt is made to verify that all locks are still held, and to record the new state of each \nmodified stable object on stable storage. If the first phase is successful, then in the second phase \nthe locks are released, the recorded states become the current states, and the previous states are forgotten. \nIf the first phase fails, the recorded states are forgotten and the action is forced to abort, restoring \nthe objects to their previous states. Turning hardware failures into aborts has the merit of freeing \nthe programmer from low-level hardware considerations. On the surface it also appears to reduce the probability \nthat actions will commit. However, this is a problem only when the time to complete an action approaches \nthe mean time between failures of the nodes. We believe that most actions are quite short compared to \nrealistic MTBF for hardware available today. It has been argued that indivisibility is too strong a property \nfor certain applications, because it limits the amount of potential concurrency [13]. We believe that \nindivisibility is the desired property for most applications, if it is required only at the appropriate \nlevels of abstraction. In particular, we intend to provide a mechanism for user-defined atomic data types. \nThe important property of these types is that they are free to violate ii~divisibility internally, but \nthey present an external interface that does not violate indivisibility. We will not present such a mechanism \nhere; the exact linguistic constructs are still a subject of current research. 2.2 Nested Actions Thus \nfar, we have presented actions as monolithic entities, In fact, it is useful to break down such entities \ninto pieces; to this end we provide hierarchically structured, nesfed actions. Nested actions, or subactions, \nare a mechanism for coping with failures, as well as for introducing concurrency within an activity. \nAn action may contain any number ot subactions, some of which may tJc performed sequentially, some concurrently. \nThis structure cannot be observed from outside; i.t,, the overall action still satisfies the atomicity \nproperties. Subactions appear as atomic activities with respect to other subactions of the same parent. \nSubactions can commit and abort independently, and a subaction can abort without forcing its parent action \nto abort. However, the commit of a subaction is conditional: even if all subactions commit, aborting \nthe parent action will abort all of the subactions. Further, changes to stable objects become permanent \nonly when top-level actions commit. Nested actions aid in composing (and decomposing) activities in a \nmodular fashion. For example, a collection of existing actions can easily be combined into a single, \nhigher-level action, and can be run concurrently within that action with no need for additional synchronization. \nTo extend this exampfe, the concurrent actions might be reads or writes to the sites of a replicated \ndata base. If only a majority of the reads or writes must be successful for the overall action to succeed, \nthis is easily accomplished by committing the overall action once a majority of the subactions commit, \neven though some of the other subactions aborted. Nested actions have been proposed by others [4, 22]; \nour model is similar to that presented in [20]. To keep the locking rules simple, we do not allow a parent \naction to run concurrently with its children. The rule for read locks is extended so that an action may \nobtain a read lock on an object provided every action holding a write lock on thahobject is an ancestor. \nAn action may obtain a write lock on an object provided every action holding a (read or write) lock on \nthat object is an ancestor. When a subaction commits, its locks are inherited by its parent; when a subaction \naborts, its locks are discarded. Note that the locking rules permit multiple writers, which implies that \nmultiple versions of objects are now needed, However, since writers must form a linear chain when ordered \nby ancestry, and actions cannot execute concurrently with their subactions, only one writer can ever \nactually be executing at one time. Hence, it suffices to use a stack of versions (rather than a tree) \nfor each atomic object. On commit, the top version becomes the new version for the parent; on abort the \ntop version is simply discarded. A detailed description of locking and version management in a system \nsupporting nested actions is presented in [20], Since changes become permanent only when top-level actions \ncommit, the two-phase commit protocol IS used only for top-level actions. Nested actions do not guarantee \nany additional reliability in the face of node crashes, Various checkpoint mechanisms have been proposed \nto increase this kind of reliability [9]. Although we are considering way~ of including similar mechanisms, \nit appears they alter fundamel)taily the semantic view presented to the user, and thus cannot be considered \nmerely as optional features of a system, 2.3 Remote Procedure Call Perhaps the single most important \napplication of nested actions is in masking communication failures. Logical nodes (described in the next \nsection) in our system communicate via messages, We believe that the most desirable form of communication \nis the paired send and reply for every message sent, a reply message is expected. in fact, we believe \nthe form of communication that is needed is remote procedure ca//, with at-most-once semantics, namely, \nthat (effectively) either the message is delivered and acted on exactly once, with exactly one reply \nreceived, or the message is never delivered and the sender is so informed. The rationale for the high-level, \nat-most-once semantics of remote procedure call is presented in [18]. Briefly, we believe the system \nshould mask from the user Iow.level issues, such as packetization and retransmission, and that the system \nshould make a reasonable attempt to deliver messages. However, we believe the possibility of long delays \nand of ultimate failure in sending a message cannot and should not be masked. The sender should be allowed \nto cope with communication failure according to the demands of the particular application, and must be \nable to terminate communication if the delays become excessive. If communication is terminated, then \nthe remote procedure call should have no effect. The all.or-nothing nature of remote procedure call is \nsimilar to the recoverability property of actions, and the ability to cope with communication delays \nand failures is similar to the ability of an action to cope with the failures of subactions. Therefore, \nit seems natural to implement a remote procedure call as a subaction: communication failures wiil force \nthe subaction to abort, and the sender has the ability to abort the subaction on demand, However, as \nmentioned above, aborting the subaction does not force the parent action to abort. The sender is free \nto find some other means of accomplishing its task, such as communicating with some other node. 2.4 \nRemarks In our model, there are two kinds of actions: nested actions and top-level actions, We believe \nthese correspond in a natural way to activities in the application system, Top-level actions correspond \nto activities that interact with the external environment. For example. in an airline reservation system, \na top.level action might correspond to an interaction with a clerk who is entering a related sequence \nof reservations, Nested actions, on the other hand, correspond to internal activities that are intended \nto be carried out as part ot an external interaction; a reservation on a single flight is an example.2 \nAtomic types provide two services to the user of the language: they guarantee indivisibility and recoverability \nfor using actions, The user -of our language does not need to write any code to undo or compensate for \nthe effects of aborted actions. On the other handl the commit of a top-level action is irrevocable. If \nthat action is later found to be in error, actions that compensate for the effects of the erroneous action, \nand all later actions that depended on it (read its results), must be defined and executed by the user, \nNote that in general there is no way that such compensation could be done automatically by the system, \nsince extra+ystem activity is needed (e.g., canceling already issued checks). Given our use of a locking \nscheme to implement atomic objects, it is certainly possible for two (or more) actions to dead/ock, each \nattempting to acquire a lock held by the other. Although in many cases deadlock can be avoided with careful \nprogramming, certain deadlock situations are unavoidable. Our method of breaking deadlocks is to abort \nactions, rather than refuse locks Although distributed deadlock detection algorithms that detect a farge \nclass of deadlocks are possible (see [20]), the Argus system is not guaranteed to detect deadlocks; in \ngeneral, deadiocks must be broken by timing out and aborting actions. 3. Linguistic Constructs In this \nsection we describe the main features of a new language designed to support the requirements discussed \nin Section 1. The most novel features of this language are the constructs for implementing guardians, \nthe logical nodes of the system, and for implementing actions, as described in the previous section, \nAs stated in the introduction, we have chosen to use the sequential language CLU as a basis for the design. \nAs in CLU, all typechecking in Argus is done at compile time. 2. Nested top-level actions are also available, \nThey are useful for accomplishing benevolent side effects, e.g., updating a cache or performing garbage \ncollection or collecting stati~tics, that need ncit De undone if the parent aborts,  3.1 Overview 3.2 \nGuardian Structure In Argus, a distributed program is composed of a group of guardians A guardian encapsulates \none or more resources, and provides controlled access to those resources. The external interface of a \nguardian consists of a set of operations called handlers, which may be invoked by other guardians using \nthe at-most-once, remote procedure call semantics discussed previously. The guardian executes the calls \non these handlers, synchronizing them as needed. Furthermore, it may refuse to perform an access desired \nby a caller if the caller does not have proper authorization. Internally, a guardian contains data objects \nand processes. Some of the data objects comprise the global state of the guardian; these objects, such \nas the actual resources, are shared by the processes. Other objects are local to the individual processes. \nGuardians exist entirely at a single physical node: all of a guardian s processes run at that node, and \n(the volatile state of) the guardian s objects are stored at that node. However, as explained below, \na guardian survives crashes of the node at which it resides. A guardian s global state is a portion of \nthe distributed state and as such may consist of both stable and volatile objects, After a crash of the \nguardian s node, the language support system re-cx eates the guardian with the stable objects as they \nwere when last written to stable storage, i.e., as of the last commit of a top-level action that modified \nsome of the guardian s stable objects. A process is started in the guardian to re-create the volatile \nobjects. Once the volatile objects have been restored, the guardian can resume background tasks, and \ncan respond to new requests. Although the processes inside a guardian can share objects directly, direct \nsharing of local objects between processes in different guardians is not permitted. The only method of \ninter-guardian communication is by calling handlers, and the arguments to handlers are Passed by value: \nit is impossible to pass a reference to a local object in a message. This rule ensures that objects local \nto a guardian remain local, and thus ensures that a guardian retains control of its own objects. It also \nprovides the programmer with a concept of what is expensive: local objects are close by and inexpensive \nto use, while non-local oblects are more expensive to use; this is underhned by the different access \nmethods (procedure call versus handler call). A method for passing data values between heterogeneous \nnodes using different internal representations is presented in [10]. Guardians and handlers are an abstraction \nof the underlying hardware of a distributed system. A guardian is a logical node of the system, and inter-guardian \ncommunication via handlers is an abstraction of the physical network. While the implementation of a guardian \nts guaranteed never to be split across physical nodes, for convenience several guardians may remie at \nthe same physical node. Such guardians communicate via handler CalIS, fIOWW?r: all inter-guardian communication \nis location-independent. The most important difference between the logical system and the physical system \nis reliability: the stable state of a guardian is never lost (to a very high probability), and the at-most.once \nsemantics of handler calls ensures that handlers either succeed completely or have no effect. The syntax \nof a guardian definition is shown in Figure 1.3A guardian definition implements a special kind of abstract \ndata type whose operations are handlers. The name of this type, and the names of the handlers, are listed \nin the guardian header. In addition, the type provides one or more creation operations, called creators, \nthat can be invoked to create new guardians of the tYPe; the names of the creators are also listed in \nthe header. Guardians may be pararneterized, providing the ability to define a class of related abstractions \nby means of a single module. Parameterized types are discussed in[16. 19]. Fig, 1 Guardian structure. \nrrarne = guardian [ [pararneter-dec,.s] ] is creator-names handles harrd/er-narnes { [ stable] variab/e-dec/.s-arrd-inifs \n} recover body end [1 [ background body end] creator-and-handler-definitions {} % /oca/ procedures and \niterators may a/so be defined end name The first internal part of a guardidr, is a list of variable \ndeclarations, with optional initializations c dining the guardian state, Some of these variables can \nbe declared as stable variables; the others are volatile variables. The stable state of a guardian consists \nof all objects reachab/e from the stable variables; these objects, called stable objects, have their \nnew versions written to stable storage by the system when top-level actions commit. Argus, like CLU, \nhas an object oriented semantics. Variables name (or refer to) objects residing in a free storage area. \nObjects themselves may refer to other objects, permitting recursive and cyclic data structures without \nthe use of explicit pointers. Thesetof objects reachable from a variable consists of the object that \nvariable refers to, any objects referred to bythatobject, and soon. (Ina language with explicit pointers, \nthe concept of reachability would still be needed to accommodate the use of pointers in stable objects.) \nWe require that all stable objects also be atomic objects, as discussed in Section 2. This requirement \nis enforced by compile.time type-checking: the type of each stable variable must be atomic. One reason \nfor this requirement is that the system knows how to syncfwonize with activity in the guardian to ensure \nthat atomic objects are written to stable storage in internally consistent states. In addition, the system \nknows how to write atomic objects in an incremental manner and still preserve the sharing among these \nobjects. These same properties d.o not hold 3. In the syntax, optional clauses are enclosed with [], \nzero or more repetitions are indicated with { }, and alternatives are separated by 1. The A sign starts \na comment. for non-atomic objects. As mentioned in Section 2, the language provides a number of built-in \natomic types, arid users may define new abstract atomic types. in fact, guardians are themselves one \nclass of user. definable atomic types. Guardian instances are created dynamically by invoking creator \noperations of the guardian type. For example, suppose we have a guardian definition with header: g = \nguardian is create handles hl, h2, h3 and the create operation has header: create = creator (n: int) \nreturns (g) When a process executes x: g : = g$create(3) the guardian object x is created at the same \nphysical node where the process is executing. The handlers provided by the guardian are referred to as \nX,IJ 7, x. /?2and x.f13. When a creator is invoked. a new guardian instance is created, and any inttializ.ohons \nattached to the variable declarations of the guardian state are executed The body of the creator is then \nexecuted; typically, tl!is code will finish initializing the guardian state and then return the guardian \nobject. (Within the guardian, the expt-ession self refers to the guardian object.) All three of these \nsteps are performed within a single subaction of the catler: the guardian will be destroyed if the body \nof the creator aborts this subaction. Aside from creating new guardian instances and executing state \nvariable initializations, creators have essentially the same semantics as handlers, as described further \nbelow, The recover section runs after a crash. Before creating a process to run the recover section, \nthe system restores the guardian s stable objects from stable storage and executes any initializations \nattached to declarations of volatile variables of the guardian state. Since updates to stable storage \nare made only when toplevel actions commit, the stable state has the value it had at the latest commit \nof a top-level action before the guardian crashed The effects of actions that had executed at the guardian \nprior to the crash, but had not yet committed to the top level, are lost and the actions are aborted. \nThe job of the recover section is to re-create a volatile state that is consistent with the stable state. \nThis may be trivial, e.g., creating an empty cache, or it might be a lengthy process, e.g., creating \na data base index. The recover section is not run as an action, although it may create top-level actions, \nas explained in Section 3.4.4 After the successful completion of a creator (when the guardian is first \ncreated) or of the recover section (after a crash), two things happen inside the guardian: a process \nis created to run the background section, and handler invocations may be executed, The background section \nprovides a means of performing periodic (or continuous) tasks within the guardian; an SXatMDle lS Presented \nifi Section4. Like the recover secticm, the background section is not run as an action.  3.3 Handlers \nHandlers (and creators), Iike proceduresin CLU, are based on the termination model of exception handling \n[17]. A handler can terminate in one of a number of conditions: one of these is considered to be the \nnormal condition, while others are exceptional, and are given user-defined names. Results can be returned \nboth in the normal and exceptional cases; the. number and types of results candiffer among conditions. \nTheheader ofa handler definition lists the names of all exceptional conditions and defines thenurnber \nandtypes ofresults in all cases. Forexample, file_date= handler (fn:file_name) returns (date) signals \n(not-.possible(string)) is the header of ahandler whose calls either terminate normally, returning a \nresult of type date, or exceptionally in condition not_possib/e with a string result. In addition to \nthe named conditions, any handler can terminate in the fai/ure condition, returning a string result; \nfailure termination may be caused explicitly by the user code, or implicitly by the system when something \nunusual happens, as explained further below. Handler calls differ from ordinary procedure calls in several \nimportant ways: 1. Procedures always runinside theguardian in which they are called. Handlers usually \nbelong tosome other guardian (although a call to a handler of your own guardian is permitted), and that \nguardian is likely to reside on some other node. Thus, the system will construct a message containing \nthe arguments and send it to the appropriate node. When the handler call terminates, the system constructs \nanother message containing the termination condition and results, and sends it back to the calling guardian.5 \n2. Procedure arguments and results arepassed by sharing (see [19]); i.e., the argument and result objects \nare shared between the calling and called pt-ocedure. As mentioned above, handler arguments and results \nare always passed by value. 3. To achieve the at. most-once semantics discussed previously, handlers \nare executed as subacfions of the calling action. Procedures simply execute within the calling action. \n Since ahandler executes as an action, it must, in addition to returning or signaling, either commit \nor abort. We expect committmg to bethemost comrnoncase, and therefore execution of a return or signaf \nstatement indicates commitment. To cause an abort, the return or signal is prefixed with abort. Let us \nexamine a step-bystep description of what the system does when a handler is invoked: 4. Aprocess that \nisnotrunning asanaction is severely restricted 5. If the calling and called guardians reside on the same \nnode, inwhatitcando, Forexample, itcannot call operations on atomic the system may be able to avcid this \nmessage passing. objects without first creating a to~>-level action. 1. Anewsubaction iscreated. 2. \nA message containing the arguments is constructed. Since part of building this message involves executing \nuser-defined code (see [10]), message construction may fail, Ifso, thesubaction aborts andthecall terminates \nwith a faihre exception. 3. Thesystem suspends thecalling process and sends the message to the target \nguardian. If the handler s guardian no longer exists, the subaction aborts and the call  ,termi nates \nwith a failure exception. 4, The system makes a reasonable attempt to deliver the message, butsuccess \nis not guaranteed. Thereason isthat it may not be sensible to guarantee success under certain conditions, \nsuch as a crash of the target node. In such cases, the subaction aborts and the call terminates with \na fai/ure exception. The meaning of such a failure is that there is very low probability of the call \nsucceeding if it is repeated immediately. 5. Thesystem creates aprocess atthe receiving guardian toexecute \nthe handler. Note that multiple instances of the same handler may execute simultaneously. The system \ntakes care of locks and versions of atomic objects used by the handler in the proper manner, according \nto whether the handler commits or aborts. When the handler terminates, the system destroys the process. \n 6. Thesystem creates theresponse message and sendsit tothe calling guardian. lfthisis impossible (as \nin(2) or(4) above), the subaction aborts and the call terminates witha fai/ure exception. 7. Thecalling \nprocess continues execution. Itscontrol flow is affected by the termination condition as explained in \n[17], For example, for a call of file_date above we might have  d:date:= file_date(frl) normal YOreturn \nexcept when not_possible, failure (why string): Exceptional return end As was mentioned above (in step \n4), the system does not guarantee to deliver messages; it merely guarantees that if message delivery \nfails there is a very low probability of the call succeeding if it is repeated immediately. Hence, there \nis no reason for user code to repeatedly retry handler calls. Rather, user programs should guarantee \nprogress by retrying top-level actions, which may fail because of node crashes even if all handier calls \nsucceed. 3.4 lnline Actions The preceding section explained handler calls only in terms of subactions. \nTop-level actions are created by means of the action statement: enter topaction body end Thiscauses \nthe bodytoexecute asanewtop-level action. When the bodycompletes, itdoesso either bycommitiing oraborting. \nIt is also possible to have an in line subaction: enter action body end This causes the body to run \nas a subaction of the action that executes the enter. When an inline action terminates, it must indicate \nwhether it is committing or aborting. Since committing is assumed to be most common, it isthedefault; \nthe qualifier abort can be prefixed toanytermination statement tooverride this default. Forexample, an \ninline action can execute leave to commit and cause execution to continue with the statement following \nthe enter statement; to abort and have the same effect on control, it executes abort leave Falling off \nthe end of the &#38;rdy causes the action to commit. Examples of inline actions are given in Section \n4.  3.5 Concurrency The language as defined so far allows concurrency between actions, but not within \na single action. Toallow subactionsto run concurrently, we provide the following statement form: Coenter \n{ Co m } nd where coarm ::= arm(ag foreach dec/-/ist [ in ifer-irwocaliorr body armtag ::= action Itopaction \n The process executing the coenter, and the action (if any) on whose behalf it is executing, are suspended; \nthey resume eXeCUtiOTI after the coenter is finished. A foreach clause indicates that multiple instances \not tne coarm will be activated, one for each item (a collection of objects) yielded by the given iterator \ninvocation,6 Each such coarm will k,ave local instances of the variables declared in the dec/-/ist, and \nthe objects constituting the yielded item will be assigned to them. Execution of the coenter star!s by \nrunning each of the iterators to completion, sequentially, in textual order. Then all coarms are started \nsimultaneously as concurrent siblings. Each coarm instance runs in a separate process, and each process \nexecutes within a new top-level action or subaction, as specified. A simple example making use of foreach \nis coenter action foreach i: int in int$from_to (1, 5) p(i) end which creates five processes, each with \na local variable i, having the value 1 in the first process, 2 in the second process, and so on. Each \nprocess runs in a newly created subaction. A coarm may terminate without terminating the entire coenter \neither by falling off the end of its body, or by executing a leave statement, As before, leave may be \nprefixed by abort to cause the completing action to abort; otherwise the action commits. A coarm also \nmay terminate by transferring control outside the coenter statement. Before such a transfer can occur, \nall 6. An iterator is a limited kind of coroutine that provides results to its caller one at a time [16, \n19]. other active coarms of the coenter must be terminated. To accomplish this, the system forces all \ncoarms that are not yet completed to abort, To abort a coarm, the system waits for its process to leave \nany critical regions (see next section); it then destroys the process and aborts the action. A simple \nexample where such early termination is useful is in timing out a handler call: coenter action x.h(...); \nexit done action slee~(units); exit timed_out end Whichever of these two actions completes first, itcommits \nitself and aborts the other. In either case; the abort takes place immediately (since there are no critical \nregions). In particular, it is not necessary for the handler call, x.h(,,, ), to finish before the calling \naction can be aborted. This last fact isirnportant,sinc ethe reason for timing out a call may be to avoid \nwaiting a long time due to crashes, loops, or deadlocks elsewhere in the system, (Such timeouts can result \nin orphan processes tha( continue to run at the called guardian and elsewhere. Wehavedeveioped algorithrnsfor \ndealing with orphans, but they are beyond the scope of this paper.) There is another form of coente r \nfor use outside of actions, as in the recover and background sections of a guardian. In this form the \narmfag is process. The semantics is as above, except that no actions are created.  3.6 Synchronization \nBy now we have lots of potential concurrency. The background code of a guardian may have many concurrent \ntasks in progress. Multiple handler calls may be active inside the guardian, and some of these may have \ncreated concurrent subactions. Howisall this concurrent activity synchronized? We expect that most concurrent \nactivity within the action system will be synchronized automatically through the use of atomic objects \nFor example, consider a guardian that guardsa single atomic array and provides a number of handlers, \nsome of which read thearray while others modify it, These handlers will be synchronized by their use \nof the atomic array. There might be several handler calls concurrently reading the array, while handler \ncalls wanting to modify the array will wait for the system to release the locks held by the actions on \nwhose behalf reading is taking place. However, not all execution takes place within the action system. \nThe background and recover sections are not actions, and may use the coenter statement to perform tasks \nwith concurrent processes, Some mechanism is need to synchronize such processes. In addition, a Process \nsynchronization me~hanism is needed when implementing highly concurrent user. defined atomic types, and \nalso occasionally to schedule handler calis. Tosupport both needs, weprovide aformofcrifica/ region by \nmeans of abuilt-in type called mutex, No two processes may execute simultaneously in critical regions \ncontrolled by the same mutex object, Each mutex object has a data object associated with it, Weguarantee \nthat, while aprocess executesin a critical region controlled by the rnutex object, the system is prevented \nfrom writing the associated daia object to stable storage, This latter guarantee can be used to ensure \nthat only consistent states of the associated data object are written to stable storage. The main interaction \nof critical regions with the material presented in previous sections is that when a coarm transfers control \noutside its coenter statement, the other processes inside the coenter must be terminated and their associated \nactions, if any, aborted. This termination happens immediately unless a process isina critical region; \ninthlscase, thesystem allows the process to continue until it exits all critical regions. The assumption \nis that processes communicate only via atomic data, or via data protected by critical regions. The above \nrule ensures that a process will be terminated as quickly as possible, but not while the data it shares \nwith other processes is in an inconsistent state.  3.7 Remarks The language sketched above has two main \nconcepts: guardians and actions. Guardians maintain complete local control over their Iocal data. Thedata \ninside a guardian is truly local; no other guardian has the ability to access or manipulate the data \ndirectly. The guardian provides access to the data via handler calls, but the actual access is performed \ninside the guardian. It is the guardian s job to guard its data in three ways: by synchronizing concurrent \naccess to the data, by requiring that the caller of a handler have the authorization needed to do the \naccess, and by making enough of the data stable so that the guardian as a whole can survive crashes without \nloss of information. While guardians are the unit of modularity, actions are the means by which distributed \ncomputation takes place, Atop-level action will start at some guardian. This action can perform a distributed \ncomputation by making handler calls to other guardians; those handler calls can make calls to still more \nguardians, and soon. Since the entire computation is an atomic action, it is guaranteed that the computation \nis based on a consistent distributed state, and that when the computation finishes, thestate is still \nconsistent, assuming in both cases that user programs are correct. To provide this guarantee, the system \nmust do a lot of work. It keeps track of the history of actions: which guardians are visited, which objects \nare read, and which are modified. As subactions commit and abort, this history is modified appropriately. \nFinally, when a top.level action commits, this history is used to ensure that none of the guardians involved7 \nhave crashed since they were used. If this condition ismet, the system updates stable storage appropriately, \nreleases locks, and discards old versions If the condition is not met, the system forces the action to \nabort, releases all locks, and restores old versions. 4, ASimple Mail System In this section we present \na very simple mail system. We have designed the system somewhat along the lines of Grapevine [1]. Although \nwe have chosen inefficient implementations for some features. and have omitted many necessary and desirable \nfeatures of a real mail system, we hope to give some idea of how a real system could be implemented in \nArgus. fhe interface to the mail system is quite simple. Every user has a unique name (user_id) and a \nmailbox. However, mailbox 7, The guardians involved are those visited by handler calls performed as subactions \nof the top4evel action, wf~ere the subaction and all of its ancestors have committed. locations are \ncompletely hidden from the user. Mail can be sent to a user by presenting the mail system with the user \ns user_id and a message; the message will be appended to the user s mailbox, Mail can be read by presenting \nthe mail system with a user s user_id; all messages are removed from the user s mailbox and are returned \nto the caller. For simplicity, there is no protection on this operation: any user may read another user \ns mail. Finally, there is an operation for adding new users to the system, and some operations for dynamically \nextending the mail system. All operations are performed within the action system. For example, a message \nis not really added to a mailbox unless the sending action commits, messages are not really deleted unless\u00adthe \nreading action commits, and a user is not really added unless the requesting action commits. The mail \nsystem is implemented out of three kinds of guardians: mailers, maildrops and registries. Mailers act \nes the front end of the mail system: all use of the system occurs through calls of mailer handlers. To \nachieve high availability, many mailers will ,exist, e.g., one at each physical node. A maildrop contains \nthe mailboxes for some subset of users. Individual mailboxes are not replicated, but multiple, distributed \nmaildrops are used to reduce contention and to increase availability, in that the crash of one physical \nnode will not make all mailboxes unavailable. The mapping from user_id to maildrop is provided by the \nregistries. Replicated registries are used to increase availability, in that at most one registry need \nbe accessible to send or read mail. Each registry contains the complete mapping for all users. In addition \nregistries keep track of all other registries. Figure 2 defines a number of abbreviations for atomic \ntypes used in implementing the mail system, For simplicity, we use only types obtained from the built. \nin atomic type generators sfrucf and afomic_array, together with the abstract types user_id and message, \nwhose implementations we omit, Structs are immutab/e records: new components cannot be stored in e stl-uct \nobject once it is built. Since structs are immutable, they are atomic. Atomic arrays are one.dimensional, \nand can grow and shrink dynamically. Of the array operations used in the ln:~il system, new creates an \nempty array, addh adds an element to the high end, frim removes elemen!s, e/cmer7ts iterates over the \nelements from low to high, and copy copies an array, Read locks on the entire iirray are obtained by \nnew, e/ements, ttnd copy, and write locks are obtained by addh and trim. The mailer guardian definition \nis presented in Figure 3. A mailer must be given a registry when created; this registry is the mailer \ns stable handle on the entire mail system. The mailer also keeps a volatil~ handle: the registry representing \nthe best access path into the system. The background code is used to periodically choose a new registry \nto play this role; the closest responding registry would be an appropriate choice. A mailer performs \na request to send or read mail by first using the best registry to determine the maildrop of the specified \nFig. 2. Abbreviations mailbox = struct[mail: messagelist, %messages for usec user-id] h this user messagelist \n= atomic-arrsy[messsge] mailboxlist = atomic_array[mailbox] registrylist = atomic-array[registry] steeringlist \n= atOmic_array[steering] steering = struct[users: userlist, % users with mailboxes drop maildrop] h at \nthis maildrop userlist = atomic-array[user-id] Fig. 3. Mailer Guardian rnsiler = gusrdian is create handles \nsend_msil, read_mail, add_user,add_maildrop, add_registry stable some: registry % stable handle best: \nregistry 04volatile handle recover best: = some % reassign after crash end background while true do enter \ntopsction best: = ...% choose closest responding registry end sleep(...) end end  create = creator (reg: \nregistry) returns (mailer) some:= reg best: = rag return(self) end create send-mail = handler (usec user-id, \nmsg: meseage) signals (no-such-user) best.lookup(user).send-mail(user, msg) res ignal no-such-user end \nsend-mail read_mail = handler (user user_id) returna (messsgelist) sign als (no_such-user) retu rn(best.lookup(user).read_mail(user)) \nresignal no_auch_user end read_msil add_user = handler (user: user_id) signals (user-exists) drop: maildrop: \n= best.chooseo all: registvlist: = best.all_registrieso coenter action drop.edd-user(user) action foreach \nreg: registry in registrylist$elements(all) reg.add-user(user, drop) abort resignal user-exists end end \nadd-user  a&#38;Lmaildrop = handler () all: registrylist: = best.all_registrieso drop: maildrop: = maildrop$createo \ncoenter action foreach reg: registry in registrylist$elements(all) reg.add-maildrop(drop) end end add-msildrop. \n add-registry = handler () all: regiatrylist : = best.all-registrieso new registry: = registry$create(all, \nbest.all_steeringso) coenter action foreach reg: registry in registrylist$elements(all) reg.add-registry(new) \nend end add-registry  end mailer user, and then forwarding the request to that maildrop. A mailer adds \na new user by first using the best registry to choose a maildrop, and then concurrently asking that maildrop \nto create a mailbox and informing all registries of thenewuser/maildrop pair, Note that if the user is \ndiscovered to exist at any registry, the overall action aborte. A new registry is added by extracting \nthe entire user. to-maildrop mapping andthelist of all registries from the best registry, and using them \nto create a new registry. The other registries are then informed of thenewregistry sothey may add it \ntotheir registry lists. Finally, anew maildrop isadded by creating one and informing all registries of \nits existence. Figure 4 shows an implementation of the registry guardian. The state of a registry consists \nof an array of registries, together with a steering /ist associating an array of users with each maildrop. \nWhen a registry is created, it is given an array of all other registries, to which it adds itself, and \nthe current steering list, The add_user handler checks to make sure the user is not already present, \nand adds the user to the user array for the given maildrop. The add_rnai/drop and add_registry handlers \nperform no error-checking because correctness is guaranteed by the mailer guardian, An implementation \nof the maildrop guardian is given in Figure 5. The state of a maildrop consists of an array of mailboxes; \na mailbox is represented by a struct containing a user_id and an array of messages. A maildrop is created \nwith no mailboxes. The add_user handler can be invoked to add a mailbox. Note that this handler does \nnot check to see if the user already exists; this checking is performed by the registries, The send_mai/ \nand read_mai/ handlers use linear search to find the correct mailbox, When themailbox is found, serrd_rnai/ \nappendsa message to the end of the message array; read_mai/ first copies thearray, then deletes all messages, \nand finally returns the copy. Both handlers assume the user exists; this is guaranteed by the registries, \nFinally, in Figure 6, we show a simple use of the mail system, namely, sending a message to a list of \nusers, with the desire that the message be delivered only if all of the users exist, and otherwise to \nget back a list of all nonexistent users. The message is sent to all of the users simultaneously, and \nthe non-existent users are collected in an array. Although a non-atomic array is used, its addh operation \nis defined to be indivisible, so no explicit synchronization is needed here, After all sends are completed, \nif the array is non.empty, the overall action is aborted, thus ensuring that none of the users are sent \nmail. 4.1 Remarks Close examination of the mail system will reveal many places where the particular \nchoice of data representation leads to far less concurrency than might be expected. For example, in the \nmaildrop guardian, since both send_mai/ and read_mai/ modify the message array in a mailbox, either operation \nwill lock out all other operations on the same mailbox wrtil the executing action commits to the top \nlevel. Even worse, since both send_rnai/ and read_mai/ read the mailbox array, and add_user modifies \nthat array, an add_user operation will lock out all operations on all mailboxes at that maildrop, In \nthe registry guardian, an add_user operation will lock out lookup operations on all users with mailboxes \nat the given maildrop, and an add_mai/drop operation locks out all lookup operations In a real system, \nthis lack of concurrency would probably be unacceptable. What is needed are data types that allow more \nconcurrency than simple atomic arrays. For example, an Fig. 4. Registry Guardian registry = guardian \nis create handles lookup, choose, all_registries, all-steerings, add-user, add-maildrop, add-registry \nstable registries:registrylist %all registries stable steerirrgs:steeringlist %all users and mai/drops \ncreate = creator (rest: registrylist, eteers steeringlist) returns (registry) registrylist$addh(rest, \nself)- Aadd se/f to /ist registries: = rest steerings: = eteers return(self) end create lookup = handler \n(usec user_id) returns (maildrop) signals (no_such_user) for steer:steering in ateeringlist$elements(steerings) \ndo for USEuser_id in usedist$elements(steer.users) do if usr = user then return(steer.drop) end end end \nsignal no-such_user end lookup choose = handler () returns (maildrop) eignals (none) if steeringlist$empty(steerings) \nthen signat none end drop: maildrop: = % choose, e.g., mai/drop with /east users return(drop) end choose \nall-registries = handler () returne (regiatryisi) ret urn(registries) end all-registries all_steerings \n= handler () returns (steeringlist) return(steerings) end all-steerings add_user = handler (user: user_id, \ndrop: maildrop) signals (user-exists) for steer steering in steeringlist$elements(steeririgs) do for \nusc user_id in userlist$elements(steer.users) do if usr = user then signal user_exists end end if steer.drop \n= drop then userlist$addh(steer.users, user) end % append user end end add_user add_maildrop = handler \n(drop: maildrop) steeringlist$addh(steeringa,steering${ueere: userlist$newo, drop: drop}) end add_maildrop \nadd_registry = handler (reg: regiat~) regist~list$addh(registries, reg) end add_registry and registry \nFig. 5. Maildrop Guardian maildrop = guardian is create handles send_mail, read-mail, add_uaer stable \nboxes: mailboxlist: = mailboxlist$newo create = creator () returna (maildrop) return(self) end create \nsend-mail = handler (user: user-id, msg: message) for box: mailbox in mailboxlist$elements(boxes) do \nif box.user = user then messagelist$addh( box.mail, mag) % append message return end end end send-mail \nread_mail = handler (user user_id) returns (rrressagelist) for box: madbox in mailboxlist$elements(boxes) \ndo if box.user = user then mail: messagelist: = messageliat$copy(box.mail) messagelist$trim(box.mail,1,O) \nAdelete mes$ages return(mail) end end end read_mail add-user = handler (user user-id) mailboxlist$addh(boxes, \nmailbox${mail: messagelist$newo, user: user}) end add-user end maildrop Fig. 6. Distributing Mail distribute_mail \n= proc (m: mailer, users: id list, msg: message) signals (no-such_users( idlist)) idlist = array[user..id] \nenter action bad: idlist: = idlist$newo coenter action foreach user: user-id in idlist$elements( uaars) \nm,send_mail(user, msg) except when no-such_usec idlist$addh(bad, user) % indivisible end end if -idlist$empty(bad) \nthen abort signal no_such_users(bad)end end end distribute-mail associative memory that allowed concurrent \ninsertions and lookups could replace the mailbox array in maildrops and the steering list in registries; \na queue with a first-commit first-out semantics, rather than a first-in first.out semantics, could replace \nthemessage arrays in maildrops. Such types can be built as user-defined atomic types, although we will \nnot present implementations here. The concurrency that is built in to the mail system leads to a number \nof potential deadlock situations. For example, in the registry guardian, two instances of add_user could \nsimultaneously read the same user array, and then simultaneously r&#38;mpt to modify that array, neither \nsucceeding because theotller still holds a read lock, In the mailer guardian, deadlock is possible if \ntwo different add_user, acfcf_rnai/drop, or add_registry requests modify registries in opposite orders. \nSome of these deadlock situations would go away if data representations allowing more concurrency were \nused. For example, the use of a higflly concurrent associative memory for the steering list would alfow \nacfd_mai/drop requests to run concurrently. In other cases, the algoritflms must be modified, For example, \nto avoid a deadlock between two different requests to add the same user, the mailer add_user operation \ncould pick a distinguished registry, such as the first one in the list of all registries, and perform \nthe registry add_user operation there sequentially before performing alf of the rest concurrently. To \navoid deadlock between concurrent add_mai/drop and add_registry requests, the mailer add_regis/ry operation \ncould first get a write lock on the registry list of a distinguished registry, and add_mai/drop could \nbe forced to obtain its registry list from that same registry. It may be argued that the strict serialization \nof actions enforced by the particular implementation we have shown is not important in a real mail sy$tem. \nThis does not mean that actions are inappropriate in a mail system, just that the particular granularity \nof actions we have chosen may not be the best. For exampfe, if an action discovers that a user does (or \ndoes not) exist, it may not be important that the user continues to exist (or not exist) for the remainder \nof the overall action. It is possible to build such loopholes through appropriately defined abstract \ntypes. As another example, it might not be important for alf registries to have the most up-to-date information, \nprovided they receive aff updates eventually. fn particular, when adding a new user, it may suffice to \nguarantee that all registries eventually will be informed of that user. This could be accomplished by \nkeeping appropriate information in the stable state of a mailer guardian, and having the background process \nof that mailer be responsible for eventually informing all registries. 5. Summary and Conclusions In \nthis paper we have presented a fairly high.level overview of a new language for writing distributed programs. \nAlthough a great many details have been omitted, we believe enough of the language has been described \nto indicate how the requirements stated in the introduction have been met Consistency, Actions and atomic \nobjects provide a powerful, easy to use mechanism for ensuring consistency of distributed information. \nGuarrJlans ensure that the effects of completed activities are not lost in node crashes, Service. Service \nis a basic aspect of the fanguage, in that each activity uses just those guardians of interest. Of course, \nthe underlying system c~mponents managing those guardians must be highly available, but these are local \nto the physical nodes executing those guardians. Replication of both data and processing at the application \nprogram is achieved thr&#38;rgh the use of multiple guardians, as seen in the simple mail system presented \nabove. Distribution. Guardians give the application program control of distribution. Tightly coupled \nprocessing and data can be grouped together within a singfe guardian, which alfows for fast local processing, \nFurthermore, the application program specifies where guardians reside. Concurrency. A great deal of concurrency \nis possible between actions, as well as within actions, through the use of concurrent aubactiorm. However, \nuser-defined atomic types may be required to achieve acceptable degrees of concurrency, as illustrated \nin the mail system. 17 Extensibility. Guardians can be created dynamicall~ they can also be destroyed, \nand moved from one physical node to another, although we have not discussed these latter capabilities. \nThus an application program can be reconfigured as needed. Reconfiguration has a minimal impact on guardians \nthat use the application because communication between guardians is location-independent. The syntax \nof haridler calls is Iocation.independent, ensuring that distinct guardians that are instances of the \nsame guardian definition can be used interchangeably. In addition, the names of handlers are Location. \nindependent, permitting a guardian to be moved from one node to another without affecting the users of \nthat guardian. Examples of expanding an existing service dynamically can be seen in the mail system. \nAutonomy. Guardians have complete control of their local data and resources, and the application program \ncontrols where guardians reside; the system never moves guardians on its own initiative. Guardians can \nbe freely created at the same node as the creating guardian, but to create a guardian at a foreign node \n(or to move a guardian to a foreign node) an intermediary guardian must be used; this guardian can check \nfor proper authorization. However, the owner of a node may wish to allow a particular guardian to be \ncreated at that node but disallow that guardian from creating other guardians at the node. ThLIS our \nprotection model is inadequate, and we are investigating better models. Argus is quite different from \nother languages that address concurrent or distributed programs (e g., [2, 6, 11, 12]). These languages \ntend to provide modules with a superficial resemblance to guardians, and some form of communication between \nmodules based on message passing. For the most part, however, the modules have no internal concurrency \nand contain no provision for Iong.term storage that survives crashes. Indeed, many such languages completely \nignore the problem of node crashes. In the area of communication, either a Iow.level, unreliable mechanism \nis provided, or reliability is ignored, implying that the mechanism is completely reliable, with no way \nof actually achieving such reliability. We have completed a preliminary, centralized, partially simulated \nimplementation of the language, ignoring hard problems such as flow control, deadlock detection, and \norphan detection. We expect to begin work on a real, distributed implementation in 1982. At this point \nit is unclear how efficient such an implementation can be. As we have argued, actions are necessary for \nmany applications, so they must be implemented; we expect the implementation will be more efficient at \nsystem level than at the application level. For other applications, actions may simply be a convenient \ntool, not a strictly necessary one. We conjecture that actions can be implemented efficiently enough \nthat they will be used in many applications even when they are not strictly necessary. using our initial \nimplementation as a test bed, we have worked out several distributed programs, some abstracted from the \napplications of interest, others from within the system itself. We have been pleased with the language \nso far, For example, actions are a useful tool in thinking about the interface to an application. However, \nwe expect to get a much more realistic idea of the strengths and weaknesses of the language when the \ndistributed implementation is complete and we can run real applications. We expect to go through another \nlanguage/system design cycle after we have gained some experience with such applications. References \n1. Birrell, A,, Levin, R., and Schroeder, M., Grapevine , Xerox PARC, Palo Alto, CA, April 1981, To appear \nin Communications ACM. 2. Brinch Hansen, P,, Distributed processes: a concurrent programming concept \n, Communications ACM 27, 11, November 1978,934-941. 3. Davies, CT., Recovery semantics for a DB/DC system \n, Proceedings of th; 1973 ACM Nationa/ Conference, 1973, 136-141. 4. Davies, CT,, Data processing spheres \nof control , IBM Syslerns Journal 77,2, 1978, 179-198. 5. Eswaren, K.P, Gray, J.N, Lorie, R.A,, and \nTraiger, I.L,, The notion of consistency and predicate locks in a database system , Communications ACM \n19, 11, November 1976,624-633. 6. Feldman, J.A., High Level Programming for Distributed Computing , \nCorn,mu,~icat\\orIs ACM 22, 6, June 1979, 7. Gray, J,N,, Lorie, R.A., Putzolu, G.F., and Traiger, i.L, \nGranularity of locks and degrees of consistency in a shared data base , Modeling in Data Base Managemerrf \nSystems, G.M. Nijssen editor, North Holland, 1976. 8. Gray, J. N,, Notes on data base operating systems \n, Lecture Notes in Computer Science 60, Goos and Hartmanis editors, Springer-Verlag, Berlin, 1978,393-481. \n 9. Gray, J.N,, et al. The recovery manager of a data management system , IBM Research Report rW2623, \nAugust 1979. 10. Herlihy, M, and Liskov, B., A value transmission method for abstract data types , Computation \nStructures Group Memo 200-1, MIT Laboratory for Computer Science, Cambridge, MA, July 1981, submitted \nto ACM TOPLAS.  353.368. 11, Hoare, C.A.R,, Communicating sequential processes , Communications ACM \n21, 8, August 1978, 666-677. 12. Ichbiah, J.D, et al., Preliminary ADA reference manual , S/GPLAN Notices \n74, 6, June 1979, 13. Lamport, L., Towards a theory of correctness for multi-user data base systems \n, Report CA-7670-0712, Massachusetts Computer Associates, Wakefield, MA, October 1976,  14, Lampson, \nB. and Sturgis, H. Crash recovery in a distributed data storage system , Xerox PARC, Palo Alto, CA, April \n1979, 15. Liskov, B. and Zilles, S. N., Programming with abstract data types , Proceedings ACM SIGPLAN \nConference on Very High Level Languages, S/GPLAN Notices 9, 4, April 1974,50-59. 16. Liskov, B., Snyder, \nA,, Atkinson, R.R., and Schaffert, J.C, Abstraction mechanisms in CLU , Communications ACM 20,8, August \n1977,564-576. 17. Liskov, B. and Snyder, A., Exception handling in CLU , IEEE Transactions on Software \nEngineering 5, 6, November 1979,546-558. 18. I_Lskov, B., On linguistic support for distributed programs \n, Proceedings, IEEE Symposium on Reliability in Distributed Software and Database Systems, Pittsburgh, \nPA, Juiy 1981,53-60. 19. Liskov, B. et al., CLU reference manual , Lecture Notes in Computer Science \n114, Goos and Hartmanis editors, Springer. Verlag, Berlin, 1981. 20. Moss, J.E.B., Nested transactions: \nan approach to reliable distributed computing , Ph,D thesis, Technical Report MIT/LCS/Tf?-26r3, MIT Laboratory \nfor Compute r Science, Cambridge, MA, 1981. 21. Randell, B. System structure for software fault tolerance \n, IEEE TransactIons on Soffware Engineering 7, 2, June 1975, 220-232, 22. Reed, D.P., Naming and synchronization \nin a decentralized computer system , Ph.D thesis, Technical Report MIT/LCS/7 R-205, MIT Laboratory for \nComputer Science, Cambridge, MA, 1978.   \n\t\t\t", "proc_id": "582153", "abstract": "This paper presents an overview of an integrated programming language and system designed to support the construction and maintenance of distributed programs: programs in which modules reside and execute at communicating, but geographically distinct, nodes. The language is intended to support a class of applications in which the manipulation and preservation of long-lived, on-line, distributed data is important. The language addresses the writing of robust programs that survive hardware failures without loss of distributed information and that provide highly concurrent access to that information while preserving its consistency. Several new linguistic constructs are provided; among them are atomic actions, and modules called guardians that survive node failures.", "authors": [{"name": "Barbara Liskov", "author_profile_id": "81100323833", "affiliation": "Massachusetts Institute of Technology, Cambridge, MA", "person_id": "PP39037707", "email_address": "", "orcid_id": ""}, {"name": "Robert Scheifler", "author_profile_id": "81100077964", "affiliation": "Massachusetts Institute of Technology, Cambridge, MA", "person_id": "P246348", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/582153.582155", "year": "1982", "article_id": "582155", "conference": "POPL", "title": "Guardians and actions: linguistic support for robust, distributed programs", "url": "http://dl.acm.org/citation.cfm?id=582155"}