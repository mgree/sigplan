{"article_publication_date": "01-25-1982", "fulltext": "\n UNBOUNDED SPEED VARIABILITY IN DISTRIBUTED COMWIUNICATIQN SYSTEMS* John Reif and Paul Spirakis Aiken \nComputation Laboratory Division of Applied Sciences Harvard Universityr Cambridge, Massachusetts 02138 \n ABSTRACT This paper concerns the fundamental problem of synchronizing communication between distributed \nprocesses whose speeds (steps per real time unit) vary dynamically. Communication must be established \nin matching pairs, which are mutually willing to communicate. We show how to implement a distributed \nlocal scheduler to find these pairs. The only means of synchronization are boolean flag variables, \neach of which can be written by only one process and read by at most one other process. No global bounds \nin the speeds of processes are assumed. Processes with speed zero are con\u00adsidered dead. Howeverr when \ntheir speed is nonzero then they execute their programs correctly. Dead processes do not harm our algorithms \nperformance with respect to pairs of other running processes. When the rate of change of the ratio of \nspeeds of neighbour processes (i.e., relative acceleration) is bounded, then any two of these processes \nwill establish communication within a constant number of steps of the slowest process with high likeli\u00adhood. \nThus our implementation has the property of achieving relative real time response. We can use our techniques \nto solve other problems such as resource allocation and implementation of parallel languages such as \nCSP and ADA. Note that we do not have any probability assumptions about the system behavior, although \nour algorithms use the technique of probabilistic choice. 1. INTRODUCTION Recently, [Reif, Spirakis, \n1981] showed how to achieve interprocess communication with real time response using ~robabilistic synchronization \ntechniques, assuming that the speeds of all processes were bounded between fixed nonzero b~unds . This \nlead to (see Appendices 1 and 11 ~f [Reif, Spirakis, 19811) real time resource Permission to copy without \nfee all or part of this material is granted provided that the copies are not made ordistributed for direct \ncommercial advantage, the ACM copyright notice and the title of the publication and its date appear, \nand notice is given that copying is by permission of the Association for Computing Machi:e:y. To copy \notherwise, or to republish, requires a fee and/or speclflc permission. @ 1982 ACM O-89791-065-6/82/OOl/O046 \n$00.75 allocation algorithms and real time implementation of message passing in CSP. In this paper we \nassume no global bounds on the proeesso?s speeds. They can vary dynamically from zero to an upper bound \nwhich may be different for each processor, and not known by the other processors. We allow a possibly \ninfinite number of processes, so that there may not be a global uPPer bound on the speeds. Processes \nmay die (have zero speed) but when they have nonzero speed then we assume they execute their programs \ncorrectly. we are interested in direet inter\u00ad process communication (ratheK than packet switch\u00ading) which \nis of the form of handshake (rather than buffered) , as in Hoare s CSP, [Hoare, 1978]. The essential \ntechnique that we utilize is that of probabilistic choice. This technique, introduced to synchronization \nproblems by [Rabin, 1980] , [Lehmann and Rabin, 1981] and [Francez, Rodeh, 1980] , was also utilized \nin our previous work. The use of probabilistic choice in the algo\u00adrithms leads to considerable improvements \nin the space and time efficiency [Rabin, 1980] , [Reif, Spirakis , 1981]; we feel that this may be because \nOf the locality of the decisions and because complex sequences of processes steps prohibiting communication \nhave very low probability of occurrence. Of course, we assume that each process makes probabilistic choices \nindependent of other processes. We also introduce new adaptive techniques: The processes estimate the \nspeeds of neighbour processes and select them to communicate with probabilities depending on the speeds, \npenalizing the slowest processes. These adaptive techniques do not seem to have ever been utilized in \nthe previous synchronization literature. This paper proposes a new high level synchro\u00adnization construct \nassociated with interprocess communication. The construct is implemented very efficiently by the use \nof boolean flag variables. We do not use any standard high level synchroniza\u00adtion construct (such as \nshared varitiles with a mutual exclusion mechanism) since these have no known efficient implementations. \n(There is not even any known bounded time implementation of a mutual exclusion mechanism when processes \nrun on different processors) . * This work was supported in part by the National Science Foundation Grant \nNSF-MCS79 21024 and the Office of Naval Research Contract NOO014-80-C-0674. If processes are bounded \nin speed then it is natural to define real time response to be a response to a communication request \nthat uses no more than constant number of units of real time. This measure is inapplicable in our case \nin which there is no global upper bound and no nonzero lower bound on speeds. Thus we introduce the notion \nof Tei!ative real time r%?sponsa which is establishment of communication between any paiY of neighboring \nprocesses within constant number of local rounds. (A ~oea~ round of neighbour processes, i,j is the minimum \ntime interval which contains at least one step of each process and exactly one step of the slowest of \nWe achieve this by our probabi\u00ad iJj)\u00adlistic algorithms with some probability of error which can be made \narbitrarily low. We conjecture that it is not possible to achieve relative real time response without \nuse of randomization. The best determ<nistie symmetric algorithms which attempt to form matchings in \ndistributed systems have a relative res~onse depending linearly on the network s diameter. ~ The paper \nis organized as follows: In the next section we define our model for communication. In Section 3 we \ndiscuss applications of this model. In Section 4 we give a relative real time imple\u00ad mentation of communication \nin this model. In Section 5 we give correctness properties of our proposed implementation and time analysis. \n2. THE MODEL VS-DCS (Variable Speed Distributed Communication System) 2.1 The Model We develop here \na theoretical model related to, but more general than, the DCS (Distributed Commu\u00ad nication System) \nof [Reif, Spirakis, 19811 . A detailed description of the fundamental issues can be found in [Reif, Spirakis, \n19811. We assume a possibly infinite collection of processes Tr={l,2, . ..}. Events of the system are \ntotally ordered on the real time line [O, m) . Each process consists of a fixed set of synchronous parallel \nsubprocesses (i.e., with same speeds). The processes wish at various times to communicate with other \nprocesses but have no means of communi cation except via the communication system. This is implemented \nby many poller subprocesses (five for each target process) which are synchronous with themselves. We \nassume a fixed connections graph H which is undirected and has the set T as its vertex set. An edge {i,j} \nindicates that process i is physically able to communicate with process j (but not necessarily willing \nto). H is assumed to have finite valence. We also assume for each time t the willingness d<graplz Gt \nA T (Alsor [Arjomandir Fischer, Lynch, 1981] have actually shown that some synchronization problems which \nare global (in contrast to our problem) can\u00ad not be done in real time and require time propor\u00ad tional \nto the logarithm of the total number of processors in the network, A typical situation where this could \noccur is the problem of detecting connected components of processes whose speeds are within given bounds, \ne-9-, with nonzero values.) which indicates the willingness of a qiven process i at a given time t. \n(We indicate this by the edge i~j and say i is a willing neig?abour of i--7 j j). Note that OnlY1f {i,j} \n@H. Let t l~jif i~j j~i. and The edges of the graph are stored distributedly so that the t edges departing \nfrom a given process are only known to that process. We assume that the out\u00ad degree of each vertex of \nG is upper bounded by t a fixed constant v. For each t>o the (possibly infinite) di\u00ad graph Mt with \nvertices T and directed edges i -j denotes which processes open communica\u00ad ki071 to which other processes \nat time t. We denote i~j if both i~+j and j+&#38;i. Thus i.@&#38;j denotes i,j achieve mutual communication \nat time t. M is the di\u00adt qraph that implementations of distributed synchro\u00ad nization achieve. We assume \nthat (Al) Two way communication between any two processes i,j E Tr requires only one step of i and j. \n(Thus, processes communicate in short bursts ) . (A2) If i-j and not i -j, t >t tl t2 21 then i~j for \nsome t such that tI < t < t2; i.e. processes can withdraw willing\u00ad ness to communicate only after communication \nhas been established. We wish implementations to be proper in the sense that (a) i~j only if i~j (neighbors \ntry to speak only if they are mutually willing to) (b) ~ must be a partial matching: If i~j then not \nj ~>i for any ., 3 in n-{j}. (No process is allowed to achieve communication with more than one neighbour \nat the same time.) We assume that processes can suddenly die (i.e., have zero speeds) but when they \nare awake they execute their programs correctly. We further more assume that each process has a fixed \nupper bound on its speed which may be different from the other processes and not known to them. We define \nthe relative acceleration bound a of processes i and j to be the worst case absolute rate of change of \nthe ratio of steps of the two processes per step of any of the two processes. The correctness of our \nsynchronization algorithms does not depend on whether processes are acceleration bounded, however, we \nassume fixed acceleration bound a in our time complexity ana\u00adlysis (i.e., the relative acceleration of \none neighbour with respect to another is bounded by a constant a or can be -CO if the process dies) . \nWe assume an adverse oracle &#38; which at time O whooses the speeds of all the processes for all times. \n~ is also able to dynamically change the willingness relation (subject to assumption T (A2)) so as to \nachieve the worst case performance of the implementation of VS DCS. Note that, in practice, we can assume \neach process has a director subprocess which dynamically changes the willingness relation ~ and at time \nO determines the speed of that process for all times. Thus , in this case, the oracle &#38; is defined \ndist~ibutedly by the director subprocess. It should be noted that the oracle W is useful to us because \nit may be ex\u00ad plicitly used to define the worst case performance of the system, when communication requests \nhappen at times most difficult for our implementation and speeds vary in the most difficult way. Thus \n, if we prove that the system has a certain performance for a worst case oracle, then we have upper bounds \non the performance of any set of director subprocesses. The following communication primitives can be \nimplemented by the poller subsystem of each process: (In practice, the director may not get an immediate \nanswer but may proceed to some other instruction and later a time slot for communication will be arranged \nby the poller subsystem). ATTEMPT-COMi (j) : indicates that the director of i wishes to communicate with \nthe director of process j. CANCEL-COMi(j ): indicates that the director of i wishes no longer to communicate \nwith j. The precise semantics of ATTEMPT-COM and CANCEL-COM are given by the relation t+~nx?l (the willing\u00adness \ndigraph, defined previously). Note that assumption (A2) implies that the oracle &#38; can withdraw willingness \nto communicate only after communication has been established. Thus, if ATTEMPT-COMi (j) is called at \ntime t~ and CANCEL-COMi(j) is called at time tz (tz>tl) then communication was established for some t \non [tl, tz) . (In fact, our implementations do not really require this assumption but only require that \nthe willingness to communicate will not be cancelled before some constant number of steps. However, the \nassumption A2 given here, considerably simplifies our analysis.)  2.2 Complexity of VS-DCS We assume \nhere a global constant a. We say &#38; is tame for i, j on time interval ~ if the pairs {(i,j)}U{(i,k)lk \nis a neighbour of i} U {(j,k)lk is a neighbour of j} are each relative acceleration bounded by a on the \ntime interval A. For every &#38; on (0,1) let the E-error response s(E) be the minimum integer >0 such \nthat for every pair of neighbors i,j and each time interval A and for every oracle ~ which is tame for \nirj on Arif i~j and the number of steps of the slowest of i,j w~thin A is >s(~) then there exists a subinterval \nA cA con\u00adtaining at least one step of the slowest of i,j such that i~j with probability >1 -E. Intuitively \n1-E gives a lower bound in the prob\u00adability of establishing communication in the case process i issues \nan ATTEMPT-COM(j) at the beginning of A, and after S(c) steps it calls CANCEL-COM(j ) . (Note that we \npresume here that i and j and their neighbors have relative acceleration bound a, only during the interval \nA; at other times this acceleration bound may be violated, and furthermore the acceleration bound a need \nnot hold for other processes even during the interval A.) We consider an implementation to be relative \nreal time if for all constants E on (0,1), the relative E-error response s(&#38;) is independent of any \nglobal measure of the willingness digraph Gt (such as Inl orany function of it) and only depends on the \nconstant maximum valence v of the vertices of Gt, and on the bound a on the re\u00adlative acceleration). \nNote that relative real time response does ?-zot imp2y that communication is guaranteed within any time \ninterval but instead it is guaranteed within a bounded number of steps of the processes with high likelihood \n(this is because processes can slow down arbitrarily). In this paper we show how to implement the VS-DCS \nso that relative real time response is achieved. 3. APPLICATIONS The primitives ATTEMPT-COM, CANCEL COM \nare powerful enough to supply real time implementations of synchronization constructs of high level \nparallel languages such as CSP and ADA. The following proposition will be useful in the applications: \n PROPOSITION 3.1. If the oracle d is tame forproeesses i,j on an inte~val A ulziek in\u00adcludes at least \n60 x+aX2/2 steps of eithe~ i or j, (where is the speed ratio of processes !0 irj in the beginning of \nA), then A includes at least x local Founds. Proof. Consider the number of local rounds to be the time \n during which a fictitious moving object with initial speed 60 and acceleration a moves a distance equal \nto the maximum number of steps done by either i or j on A. 0 3.1 Real Time Resource Granting Systems \nwith Process Failure Previously, in [Reif, Spirakis, 1981] we utilized the more restricted DCS system \n(which does not allow process failures) to implement a real time resource granting system. In this paper, \nwe can cope with sudden process failures (zero speeds). In this case, the process governing a resource \nwill first get an estimate on the speed of a process granted the resource (say, 60) and then it will \nattempt to communicate for doS(&#38;) +aS2(E)/2 of its steps with the process granted the resource. (Note \nthat 60 will be 1 if the process governing the resource is the fastest). By Proposition 3.1 the above \ninterval should be enough for the process which has been granted the resource to respond. If that process \ndoes not respond, the resource govern\u00ading process may reclaim the resource. If a re\u00adsource allocator \ndies, then other processes can play its role (for details see [Reif, Spirakis, 1981b]). 3.2 Relative \nReal Time Implementation of CSP and ADA s Synchronization Constructs In a typical stage during execution, \nthe processes comprising a CSP program may be divided into two classes: those busy with local computa\u00adtions \nand those waiting for a partner to communicate with. A distributed guard scheduler can be imple\u00admented \nby using the poller subprocesses of the relative real time VS-DCS system. The CSP (Communicating Sequential \nProcesses) was defined by [Hoare, 1978] for concurrent pro\u00ad gramming. The language has elegant synchroniza\u00adtion \nconstructs: (1) An output Command of the form i!u where i is a process and u is a value which i receives. \n (2) An input command of the form i?x where i is a process which sends a value which is assigned to variable \nx.  CSP also allows a~ternative statements which consist of a sequence of guarded commands of the form \nG+c where the guard G is a list of booleans followed by at most one input command and C is a command \nlist. We assume here the extension of CSP given in [Bernstein, 1981], which allows G to be a list of \nbooleans followed by at most one input or output command. An alternative statement is executed by nondeterminately \nchoosing a guard which is satisfied (by executing its elements from left to right) and then executing \nthe corresponding command list. If no guard is satisfied, the alter\u00adnative statement fa{ls. CSP also \nallows a repe\u00adtitive statement allowing repeated execution of an alternative statement until it fails. \nThus , the essential problem in implementing CSP is to synchronize execution of input and out\u00adput commands. \nA CSP implementation is relative real time if there exists a positive integer k (which is inde\u00adpendent \nof the number of processes n) such that if in some alternative or repetitive statement S some guard G \nis satisfied continuously for a time interval containing at least L steps of both processes associated \nwith the guard and if the worst case oracle &#38; is tame with respect to those processes during that \ninterval, then the command list associated with some satisfied guard is immediately executed with probability \n>1-E and otherwise a failure exit is always made, after at most k steps. Hencer we allow an e?ror probabili~ \nE for a failure exit even though some guard may be satisfied. However, E may be fixed to an arbitrarily \nsmall constant on the interval (0,1). For a process i to execute an output command j!u, process i must \nexecute the communication command ATTEMPT-COMi(j). Also, to execute an input command i?x, process j must \nexecute the communi\u00adcation command ATTEMPT-COMj (i). If successful communication is established between \ni and j, then during that time process j transmits value u to variable x in process i. Processes i,j \nthen execute CANCEL-COMi(j) and CANCEL-COMj (i) , respectively. (Note that if processes i or j happen \nto die at this point, before canceling communication, then successful communication cannot be made with \nthem during the time their speed is O, so it is not essential for the communication ,request to be canceled.) \ni++j Let S be an alternative statement with guarded input and output commands, say Gl, . . ..Gs with \nS<v. To execute the statement S, process i first executes the booleans appearing in each guard. Let R \nbe the set of processes appearing in those guards of S all of whose booleans evaluate to true. Process \ni must then execute ATTE~T-COMi (j) for each process jCRr for k steps of i, where f,= Ct32(E)/2. At the \nfirst time a communication is established between i and some willing process j~R, Pi must immediately \nexecute CANCEL-COMi(j ) for each j CR and then execute the command list associated with the now satisfied \nguard in the statement S. Otherwise, after aS2(c)/2 steps of i, process i makes a failure exit from statement \nS. By Proposition 3.1 and the definition of S(E) , the probability of an incorrect failure exit is < \nE. Also , in ADA, two-way communication between pairs of tasks is allowed in synchronized time instances \ncalled rendezvous. An accept statement of the form aeeept f(-) appearing in task T1 indicates that T1 \nis willing to rendezvous at f with any task of similar argument type. The task T2 may execute a call \nstatement of the form f(-) indicating that TZ is willing to rende~vous with T~ at the accept statement \ncontaining f. ADA also allows for selective accept statements con\u00adtaining multiple accept statements, \none of which must be nondeterministically chosen to execute. (This is similar to the se~eet statement \nof CSP.) ADA s tasks may be implemented by processes whose speeds vary dynamically. (Processes may even \nfail for various time intervals.) The key imple\u00admentation problem is to synchronize task rendezvous within \nrelative real time, in spite of the dynamic speed variations. These processes may be connected within \na distributed network whose transmission channels may also have variable speeds or fail. Unreliable transmission \nchannels can be viewed as processes which are connected with the processes of the network via reliable \ncommunication channels. We assume that it is possible to analyze (perhaps by data flow analysis) an \nADA program to determine an undirected (possibly infinite) connections graph whose nodes are all the \ntasks possibly created by the ADA program and edges are the possible task communication pairs. Since \nan actual implementation will have in its hands at any time only a finite set of processes we assume \nthat only the currently active tasks have an associated implementing process and that a call to ADA s \ninitiate statement devotes a currently free process to a given newly created task. An abOPti statement \ngarbage collects the implementing process from the deleted task and places it back to the free list of \nprocesses. These implementation techniques were developed by [Denis and Misunas, 1974] fOr real time \nimplementation of data flow machines. The synchronization facilities of our VS-DCS system provide (by \nuse of the ATTEMPT-COM and CANCEL-COM primitives) a real time implementation of the accept and Call \nstatements. A version of the Oc?tiVe statement can be implemented so that deleted tasks and tasks implemented \nby nontame processes can be detected by their neighbors in real time with some (arbitrarily small) error \nprobability. This can be done in our VS-DCS system by repeatedly attempting communication with neighboring \nprocesses. Finally, the symmetry and ~oea~ity of the VSLDCS implementation (due to its probabilistic \nnature) may help in eliminating the tradeoff between generality of expression and ease of implementation \nin ADA. The probabilistic fairness guaranteed by the algorithms of the pollers eliminates the danger \nof bottlenecks which could be created if conventional techniques were used (a new task which centralizes \nrequests and keeps track of busy server tasks is one of the conventional proposed solutions) . Most of \nthe problems which VS-DCS could cure are dis\u00adcussed in [Mahjoti, 1981], [Francez, Rodehr 1980]. A probabilistic \nsolution to some of the discussed problems was given also in [Francez, Rodeh, 19801 but no discussion \nabout real-time properties was done and neither the problem of speed variations and dying processes was \naddressed. 4. RELATIVE REAL TIME IMPLEMENTATION OF VSDCS 4.1 Intuitive Description of the Algorithm \nWe utilize 7V+1 synchronized parallel processes to implement the poller subprocess of each Process i. \nThese are the communicators  ii i CP1,CP2J. ..,CP the speed estimators eP~, ..., 2V ., i ep~ and the \njudge subprocesses jp~,jp~, . . . ,jp4v of process i. Each pair of the communicators ii Cpk, ,Cpk,, (with \nk modv = k modv = k) is devoted to communication with a specific neigbbour (the k-th neighbor). Each \nestimator is used to continuously update an estimation of the speed of The collection ofa particular \nneighbour process. judges has the task to select under certain conditions one communicator and to give \nto him the right to open the communication channel of process i to its corresponding neighbour. we frequently \nuse the technique of handshake by which we mean that each subprocess modifies a flag variable observed \nby the corresponding neighbour subprocess. Process contention between syr+chronized subprocesses is easy \nto implement (we can allow each to take a separate step in a small round) . Our algorithm for the k th \ncommunicator sub\u00adprocess of the poller of process ~p~ (l<k<2v) i proceeds as follows: t>o,Let k =k modv. \nAt every time ,. E1(l),. ..,E1(D~) is the list of targets of edges of departing from iETr, ,and D~ is \nthe % current number of targets Those variables (D~< ) . are dynamically set by the oracle &#38; and \nthey are the neighbors to which process i is willing to 1 open communication at time t. The subprocess \nCpk deals with the E=(k ) neighbour. If k<v, then c P; is an asker subprocess, else it is a responder \nprocess. cp~ must first handshake with the corresponding subprocess of process E= (k ) to which node \ni wishes to communicate. We need two handshake subprocesses (ask, respond respectively) per neighbour \nbecause of a cetain asymmetry in the handshake (some has first to modify a flag) . In particular the \nasker procedure initiates the hand\u00ad shake and the responder answers to it. Next we wish to find a time \nslot in which the two neighbors may communicate, Because there may be contention among other processes \nj which also wish to communicate with i (and consequently, other askers or responders of node i also \nwill handshake) we must resolve the contention by a, fair judge. To do this, we add tie process CP; to \na Q and the collection of judge synchronous suhprocesses of poller i takes a random process from this \nqueue and allocates time slots for communication attempts. To ensure that slower neighbors do not utilize \nany more total time on the average than faster neighbors during communi\u00ad cation attempts, we weigh the \nprobabilities of sub\u00ad processes to be chosen from the queue by the factor l/Aik u(Ai) where Aij (j=l, \n. . ..v) is the current estimation of the steps of process i per step of process jr supplied by the estimator \nand u(~i) = ep~, Zj l/A. ,. 13 The judge subprocesses are organized in a tree (balanced binary tree) \nof height log(2v) +1. Any time a random process is to be selected from the queue, the judge jp~ (the \nsupreme judge sub\u00adp~oeess) enables the tree of the rest of the judges to conduct a tournament between \nthe waiting processes in the queue and to select a winner with the above stated probability. In that \nway, the total number of steps needed for a winner to be selected is 0(log(2v)). (Note that less efficient \nways of using a random number generator to choose one waiting process from the queue could take o (v) \nsteps of process i, because of the form of the weight factor in the probabilities.) The fact that a subprocess \nis chosen from the queue with the above stated probability, has the effect that each subprocess in the \nqueue attemptsto communicate on the average l/(2v+v2/u) of the tOtal time. (See the analysis for a proof \nof that. ) If a process is chosen by the judges but the communi\u00adcation is not established, the algorithm \nrequires that subprocess to initiate another handshake with its par cner (to check if they are still \nmutually willing to communicate and to synchronize steps) . Then, it is again added to the queue to be \ngiven another chance to establish communication. This process proceeds until either the director of i \nwithdraws its willingness to communicate with Ei (k ) or until it establishes communication. Note that \nthe time slots for communication attempts, allocated by the supreme judge jpl to each selected communicator, \ntake into accoun ~ the current speed ratio of the process i and its neighbour corresponding to that \ncommunicator, ad\u00adjusted by a factor related to the worst-case acceleration and the log2 2V delay in the \nprocess of choosing a winner, to give the opportunity of at least one step overlap in time of process \ni and its neighbour, if their corresponding channels are both open. We introduce random waits which help \nsub\u00adprocess cp~ to eliminate the possibility of schedules set-up by the adverse oracle .x2 to have always \na particular subprocess arrive first in the queue and win the contest. This possibility is eliminated \nsince we have assumed that the oracle sets the speeds at time O and cannot affect the random choices \ndone by the processes. Alwo, we assume that the random number generator RANDOM(O,l) of each snbprocess \nyields truly random numbers, uniform on the interval [0,11, and independent of the random numbers generated \nby any other subprocess. Note that we trade computation effort (parall\u00adelism) in a node to achieve reliable \ncommunication. This parallelism is limited because of the bounded valence v of the graph Gt. We can alway \nsimulate these synchronous techniques. Thie will reduce the effective speed of each subprocess by only \na factor of 7V+ 1. 4.2 The Algorithms of the Poller Subprocesses In each process i~n, we assume synchronous \n subprocesses ii i askers : CP1, CP2, . . ..CP v ii i responders: cPv+lr cPv+2#. . . ,CP2V ii estimators: \neplr. ..tepv  ,. judges: jp~, jp~, ...rjp~v. The askers and the responders are the communic\u00adators. \n In the following algorithm, exe~uted by each of the communicator subprocesses Cp~r l<k<2v, w? implement \nthe queue of process i by an array Q= (k) , l<k<2v. Qi(k) =1 holds juet if cp~ waits in the queu~. Another \narray of binary values, marriage, l<k<2v, is used to indicate which communicator subprocess currently \nholds process s i channel and attempts communi\u00adcation. When the predicate marriage =1 is true, then CP; \nattempts communication at that time. The algorithms,have designed so that at most one of marriage , l<j<2vr \nis set at any time. We now present the algorithm for the communicator subprocesses: process cp~ WHILE \ntrue DO ~ + true LO: IF D1>k THEN BEGIN Ll: W+C1.RANDOM(O,l) ~ lwJ noops IF k<v THEN ASK(E1(k))ELSE \n RESPOND(E1(k) ) COMMENT: Add k to queue Q1(k) +1 WHILE marriage~ (k) = O DO noop x+EsTABLISH-COM (Ei(k) \n,c2Aik) marriage=(k) +0 IF X THEN GOTO L1 ELSE GOTO LO . END OD  The constants and are as follows: \nc1 C2 = 2(2V+1)C2 c1 =4(ufl+l)C2 ~ = 6 log(2v) . The speed estimators execute the following algorithm, \nwhich continuously does a handshake in order to estimate the speed ratio of the process to which the \nhandshake is attempted and the process of which the speed estimator is a subprocess: process ep~ DO FOREVER \n set 1?. to1 Ik LOOP UNTIL F I ki Is to o; s+clJRsTfJp ~k LOOP UNTIL Fki is O A: set F. ~ CURSTEP-S B: \nA ik 2 OD  Note that I?ik is a flag set by i, read by k. The special register CURSTEP gives the current \nstep of process i. We assume that a step consists of.an elementary statement of the algorithms; ep~ s \nexecution assures that Aik, is (within a factor of 2) the actual speed rat~o of processes i and k, since \nfrom step A to step B the fastest of the partners does CURS TEP-S steps and the slowest does 2 steps. \n4.3 The Algorithms of the Judge Subprocesses The algorithm of the supreme judge Process jp~ WHILE true \nDO , IF queue Q 1 not empty THEN BEGIN Use the tree of judges to select a random element k of the queue \nQi with probability l,/Aik u (Ai) ELSE COMMENT : delete k from Q1 BEGIN Q=(k) +O r+~N~M(O,l) ml + \nLCHIJ J (~) marriage (k) +1 mo+RcHIJ,,D (m) WHILE marriage (k) = 1 DO noop 4. END sum=(m) +suml(ml) \n+,sum1(m2) OD ~ r<Suml(ml)/suml(m) Intuitively, the supreme judge triggers the THEN choice=(m) +choiceL(ml) \noperation of the tree of judges. In each level, ELSE choicel(m) +choice1(m2) the winners of the previous \nlevel are paired up and half of them are selected. The judge sub-END processes of each level work synchronously \nin END . parallel. Finally, the jp~ accepts the choice of the root of the tree of Judge subprocesses \nto be the communicator which is going to attempt Note that each judge subprocess which is not communication. \nThe supreme judge r$moves this a leaf uses a uniform random number generator w<nner subprocess from the \nqueue Q1 (by setting RANDOM(O,l) to return a random number between O Q1(k) tO O) and allows it to attempt \ncommunication and 1 and uses it to select one of the choices Of (i. e., t? use process s i channel) by \nsetting its, children with conditional probability marriage to 1. Note that we can test if Q1 suml(m)/(suml(ml) \n+sum1(m2)) where ml, m2 are the is empty by keeping a counter of the number of children of m. elements \nin Q1. LEMMA 4.1. It takes 6=6 log(2v) steps for We now give the algorithms of the judges: the tree,of \njudges to select a winner from the queue Q . Furthermore, the probability that the process jp~ winner \nis the eormnunieator cp; (g<ven that WHILE true DO Q1(k)=l and A.. l<j<2v, is as given at the . beginning \nof thel%nkest) is IF Q1 is not empty THEN BEGIN I/L!ik FOR level =lr. ..,log(2v) + 1 DO Zq  BEGIN -=. \nThe judges of each level work synchro- L1+ level; do 6 noops nously in parallel and each does at most \n6 steps, per iteration of their loop. Sihce the tree has END ~ a height of log(2v), the total number \nof steps k+chOi~e (4 ) required is 6=6 log(2v). The probability that will be selected is the product \nof the Q;+O % cond~tional probabilities that cpi will be marriage +1 selected in each node of the path \n+ rom k to 4V (= root). A simple inductive argument on the level WHILE marriage =1 DO noop of nodes in \nthe tree then proves the lemma. 0 END OD 4.4 Low Level Synchronization Procedures The rest of the judges \nare organized in a The following are the low level synchroniza\u00adfull binary tree pf 4V podes. The leaves \nare tion procedures used by the poller algorithms: the processes jp~, . . ..jp~v. Each internal node \n mE {2v+1 4v} has two children LCH~LD(m) , procedure aski (target) RCHILD(m) . The root is the process \njp~v. Each BEGIN jp~ ,..., has its level stored in MYLEVEL (m) . + 1; ~,target process jp~ Q. WHILE \nA =O DO noop target,i IF MYLEVEL(m) =L1 THEN Qi, target + 0; BEGIN WHILE A =1 DO noop target, i IF \nLi=l THEN END  BEGIN Note: The set of the flag Q, means that choicel(m) +m 3.,target asks the target. \nIf the target detects marriage +0 ;. =1 then it answers positively by setting L,target IF Q1(m) =0 THEN \nsum=(m) +0 A =1. Both partners reset these flags to target,iELSE sumi (m) +1/l! im O at the end of procedures \nask and respond. END procedurere,epondi (asker) _ . BEGIN LOOP UIJTIL Q =1 asker, i BEGI~J A, +1; L, \nasker WHILE Q =1 DO noop . asker,i Ai,a~ker + O; END END We finally present the code for the procedure \nESTABLISH_COMi (target,s) . During its execution process i opens its channel to process target. A simple \nprotocol (symmetric handshake) is then attempted to see if the neighbour responded to that communication \nattempt. If the protocol succeeds then i is sure that process target also opened its channel and communication \ntook place. Else, process i knows that the attempt failed. procedure ESTABLISH-COMi (target,s) BEGIN \nOPEN CHANNEL, -colt. +1 L, target L,target + CURSTEP o b+l WHILE (COM =0) or (b=l) DO target,i IF CURSTEP-SO>S \nTHEN b+O OD IF (COM =1) AND (b=l) . targetri THEN success + 1 ELSE success + O b+o COM +0 I,target \n CLOSE CEANNCL, I,target return(success) END Note: COM. is a flag of node i and I,target COM is a flag \nof node target. OPEN target,i CHANNEL results in the appearance of I,target i ~ target at the time of \nits execution, and sets i ~-target. Also, I,target CLOSE CHANNEL. s is the maximum number of steps we \nare allowed to keep the channel open before we fail. 5. CORRECTNESS PROPERTIES OF OUR PROPOSED IMPLEMEt?TATICIJ \nAND TIME ANALYSIS LIXMA 5.1. A matching (with respeet to the Ye7,ation 4&#38;%) is guaranteed by the \nimplementation. Proof. In any tine instant, only one of the subprocesses of any poller can have the marriage \nvariable set and its channel open. So, the relation ~ is one-one which means that ~-cannot 0 be more \nthan matching. DEFINITION. A subproeess cp~ gets the e? knznel when it executes ESTAELISH-CONi (k). LEl@lA \n5.2. Death ofa process does not affect the communication of other processes. =. Death of process target \nat any time will only cause blocking of only one subprocess (Cp;arqet ) per neighbour i of target. This \ndoes not disrupt the other subprocesses of the neighbors. 0 LEMMA 5.3. Suppose that i,j start to be \nmutually ui?,l-ing to eomunicate at some time and continue to be willing fo~ 5 lo~al rounds. Then, 1 \nall fozw subproeesses and cp~l, cpi2 Cpjl Cpjz (with jl modv= j2 modv= j and il modv= modv= i) w-ill \narrive in the queues of i and 12 j in a constant number (5) local rounds. Proof. Note that at each time \nthe slower of it] will do only one step in the busy waits of procedures ask or respond. The result follows \nsimply by counting the steps to be executed in each of the procedures. 0 Let A. be the current estimation \n(within a factor of &#38;@) of the ratio of steps of i per step of j (estimated by i). DEFINITION. Let \nPij be the ratio l/(u(Ai)Aij) . In the following we assume that the oracle &#38; is tame with respect \nto processes i,j in the time interval T they attempt communication. DEFINITION: Let si be the ave?age \nnumber of steps that cpj does be $ ore it is selected to attempt communication, measured from the time \nit enters the queue. LEMMA 5.4. Sii < 2VC A, ,, wlzere Aij is the 2 1] most current estimation (i.e., \ntile Aij used in the last competition in which cp~ was the uinner), and 4v(aB + 1), as d~fined previously.C2 \n= =. l~ote that the probability to be chosen is bounded above by a geometric density gij (x) = (l-Pij)x \nP. where x is the number of l] selections done before Cp 1 is selected. The mean 1 value of this number \nis X gij (x).x < I/p, ,. ~=o 11 Each tirce is not chosen, it waits in the queue CP ; Qi for an average \nnumber of steps bounded above by the worst case value of 2V C2 ~ c2Aikpik<~ k=l  so, 2V C2 sij<~. 1 \n . 2VC A, . m(Ai) 2 1, l] DEFIIJITION. A process cp~ h in the queue -if Q~(k) = 1. DEFINITION. Let i \n= 2V 1+:. ()  THEO?U2M 5.1. Each subprocess expects to get the channel > I/i of the time. Proof. Let \nS be the average number of Q,k i steps cp attempts to communicate by executingk ESTABLISE-COM. Then, \nby Lemma 4.5, l/Aik c A. = C2/U (Ai) :,k = u(Ai) 2 ~k H where Aik is the estimation used in the last \ni competition in which the judges selected cPk to be the winning process. Let T1 be the subinterval \nof a time inter\u00adQ,k val T in which CP ~ gets the channel. Let ( length of :<k u mean length of T ) \nIn the worst case, where process i is the fastest and all neighbors slowidown with the same worst case \nacceleration a, S is the same for all subprocesses intheque8&#38;~ Theworst case conten cion happens \nwhen all 2V subprocesses are in the queue. Thus , in the worst case s= Q,k p> 2v ~ (s;rk+B) -2( %) k=l \n(since the length of T is the sum of time in channel for each process plus the time of competi\u00adtion for \neach process) . But 2V LL)(Ai)   ,~1 /Aik gets its maximum of 2V when Aik = 1 for k=l 2V. so, ,..-, \nRLd(Ai) 2VB <~ <24 . 4(a!3+l) 2a C2 C2 so, 1 0 a Note that the above theorem justifies the use of \nthe estimate (l/Aik)/LO(Ai\\ as the probability to select the subprocess cp~ from the queue. In the following \nwe assume l<i, k <v and i k is the k=k +V. Thus cPk, is the asker and CP i responder. LEMNA 5.5. The \nrelative position of the time interva2s during ukiek the channels of tuo neigh\u00adhour processes a~e open, \nis a uniform random position and is not affected by the oracle ~. Proof. The oracle sets the speeds of \nprocesses at time O and cannot affect their probabilistic choices. Also, each subprocess cP~ of a process \ni suffers a random. wait before each return to the queue of process i. Their randov. waits are uni\u00ad formly \ndistributed in the interval whose length is the mean number of local rounds to atten.pt communication \n(as we shall see in Theorem 5.3). 0 LEMMA 5.6. The probability of instantaneous overlap of open channels \nof subproeesses .kand CD1 k Wi is > (1/A)2. Proof. By Theorem 5.1 and Lemma 5.5. 0 DEFINITION. Let \nan overlap between processes irk be the interval in which channels of i and k are simultaneously open. \nDEFINITION. Let success in communication between i and k be an overlap of open channels of i and k for \nat least one step of both processes i,k . DEFINITION. A phase of subproeess cp~ is a random wait, a handshake \nwith cp~, a wait zn queue and a communication attempt. DEFINITION. Let Ym,in= 1/2(1/A) 2 . THEOREM 5.2. \nTke probability of success in communication in a phase of subproeess cp~ is 2 ymin. K Proof . When the \nsubprocess opens its CP; channel, the number of steps done from the time Of the estimation of Aik used \nin the selection  process of the judges, is B = 6 log(2v) and hence, since Aik > 1, the new speed ratio \ncan be Aik + C@ < (aB+l)Aik in the worst case. The worst case is when process i is the fastest and process \nk slows down continuously with the maximum acceleration, so that process i does more and more steps \nper step of k . In this case, a communication attempt of time slots where c2Aik k, C2 = 4(a(3+l) guarantees \nthat pi, will do at least 2 steps during the time has its % channel open. Note the random relatiVe \nposition of these steps with respect to p~ s steps (due to random waits in the poller subpro cesses algorithms.) \nThus, given that there is an overlap, the probability is at least 1/2 that the length of the overlap \nis at least 1 step. Hence, by Lemma 5.6, the probability that there i. an overlap and its length is \n> 1 step of both processes is Note that the above theorem justifies the selection of the constant C2 \n= 4(a(3+l) in the communicators algorithm.  54 Let % be the class of oracles &#38; for and the s-error \nresponse of the presented imple\u00adwhich the out-valence of each node of mentation of VS-DCS is %-is v for \nall t. This class of oracles creates the maximum contention and gives the worst relative s(E) <c response \ntime. 1 max(&#38;) DEFINITION. Let qlk(h/~) be the probabiZit.y ov%log vlog : that it takes exaetZy \nh, phases for subprocess CP+ =( )to communicate with Cpk . 1 for Zarge v. DEFINITION. Let y =2 . max \n(2V) 2 Proof. By previous remark and the fact that LEMMA 5.7. For any oraeZe M , log h-1 qik(h/#) ~ \n(l-Ymin) . h +2, + ,2109(= (1 +;)) max(c) = log l-~ A For oracles &#38;c% , qik(h/~) ~ (l-Ymin) Ymax \n. h-1 . ~v.<~l.+,(l +%)) Proof. It suffices to observe that the process for large v. 0 k of Cp ~ be \nanswered by is a geometric cPi stochastic process with success probability bounded 6. CONCLUSION by \n[Ymin,ll. For oracles in class W, it is Since we have assumed global parameters a bounded within [Ymin, \nYmaxl due to the contention and v to be constant, by Theorem 5.3 our VS-DCS of all 2V processes in any \ncommunication attempt. system has relative real time respor,se. Our restrictions on processors rates \nare much less By using the above lemma and known expressions than in our previous paper [Reif, Spirakis, \n1981] . for the mean and the tail of a geometric we get Furthermore, our algorithms seem much more modular \nand simple in design, although we have LEMMA 5.8. For oraezes in % utilized new adaptive techniques to \ndeal with arbitrary speed variability. Y max mean(h) < (Ymin) 2 REFERENCES Angluin, D., Local and Global \nProperties in Net- LEMMA 5.9. For oracles in %? works of Processors, 12th Annual Sympos$um on Theory \nof Computing, Los Angeles, California, April 1980, PP. 82-93. VE, O<E<l, Prob{h > h max(E)} < E Arjomandi, \nE., M. Fischer, and N. Lynch, A where Difference in Efficiency between Synchronous log (y min E) -log \nymax and Asynchronous Systems, 13th AnnuaZ h Symposium on Tlzeory of Computing, April 1981. Inax(c) = \n(l-ymin) log Bernstein, A.J. , Output Guards and Nondeterminism Note that, by Lemma 5.1 and Theorem \nin Communicating sequential Processes, ACM 5.1 in the worst case relation of speeds of TPans. on Prog. \nLang. and Systems, vol. 2, processes i,k, the total length of a phase of No. 2, April 1980, pp. 234-238. \nsubprocess is the number of local rounds in P; the random wait plus the number of local rounds Dennis, \nJ.B. and D.P. Misunas, A Preliminary up to the end of the communication attempt. This Architecture for \na Basic Data Flow Pro\u00adsum is = 2(2 V+1)C2. c1 cessor, PPoe. of the 2nd AnnuaZ Symposiwn o% Computer Architecture, \nACM, IEEE, 1974, This justifies the use of the constant cl pp. 126-132. in our Algorithms for the poller \nsubprocesses. Francez, N. and Rodeh, A Distributed Data Type THEOREM 5.3. For oracles in class % (and \nImplemented by a Probabilistic Communication hence for the worst ease of any adverse oraeZe Scheme, \n21st Annual Symposiw?? on Foundations d), the mean number of ZoeaZ rounds to achieve of Computer science, \nSyracuse, New York, eomumozieation is Octi. 1980, pp. 373 379. Y max Hoare, C.A.R., rCommunicating \nSequential Processes; <c. = 4c1a2 1+$ 1 Corn. ofACM, Vol. 21, No. 8, Aug. 1978, () Y:in pp. 666-677. \n= O(v% log v) Lehmann, D. and M. Rabinr On the Advantages of Free Choice: A Symmetric and Fully Distributed \nSolution to the Dining Philo\u00adsophers Problem, to appear in 8th ACM Symposiwn on Principles of Program \nLanguages, Jan. 1981. Lipton, R. and F.G. Sayward, Response Time of Parallel Programs, Research Report \n#108r Dept. of COmpUter Science, Yale Univ. , June 1977. Lynch, N.A., Fast Allocation of Nearby Resources \nin a Distributed System, 12th Annual Sympo-S k iI in Theory of Computing, Los Angeles, Californiar April \n1980, pp. 70-81. ,7Some ~oments on ADA as a Real-time Mahjoub, A., Programming Language, to appear. Rabin, \nM., N-Process Synchronization by a 4 log2N\u00advalued Shared Variable, 21st Annual Symposium on Foundations \nof Computer Science, Syracuse, New York, Oct. 1980, pp. 407-410. Rabin, M., The Choice Coordination Problemr \nMere. No. UCB/ERL M80/38r Electronics Research Lab., Univ. of California, Berkeley, Aug. 1980. Reif, \nJ.H. and Spirakis, P., Distributed Algorithms for Synchronizing Interprocess Communication Within Real \nTime, 13the Annua2 ACM Sqmposium on Theory of Computation, wisconsinr ~9~1, pp. 133-145. Reif, J.H. and \nSpirakis, P. , A I.eal Time Resource Granting System, to appear. [Also appearing in preliminary form \nas Appendix II of Distributed Algorithms. .. referenced above. Schwartz, il., Distributed Synchronization \nof Communicating Sequential Processes, DAI Research Report No. 56, Univ. of Edinburgh, 1980. Tonag, S. \n, Deadlock and Livelock-Free Packet Switching Networksr 12th Annual Symposium on Theory of Computing, \nLos Angeles, California, April 1980, pp. 82-93. Valiant, L.G., A Scheme for Fast Parallel Communi\u00adcation, \n Technical Report, Computer Science Dept.r Edinburghr Scotland, July 1980. \n\t\t\t", "proc_id": "582153", "abstract": "This paper concerns the fundamental problem of synchronizing communication between distributed processes whose speeds (steps per real time unit) vary dynamically. Communication must be established in matching pairs, which are mutually willing to communicate. We show how to implement a distributed local scheduler to find these pairs. The only means of synchronization are boolean \"flag\" variables, each of which can be written by only one process and read by at most one other process.No global bounds in the speeds of processes are assumed. Processes with speed zero are considered dead. However, when their speed is nonzero then they execute their programs correctly. Dead processes do not harm our algorithms' performance with respect to pairs of other running processes. When the rate of change of the ratio of speeds of neighbour processes (i.e., relative acceleration) is bounded, then any two of these processes will establish communication within a constant number of steps of the slowest process with high likelihood. Thus our implementation has the property of achieving relative real time response. We can use our techniques to solve other problems such as resource allocation and implementation of parallel languages such as CSP and ADA. Note that we do not have any probability assumptions about the system behavior, although our algorithms use the technique of probabilistic choice.", "authors": [{"name": "John Reif", "author_profile_id": "81100567232", "affiliation": "Harvard University, Cambridge, Massachusetts", "person_id": "P145670", "email_address": "", "orcid_id": ""}, {"name": "Paul Spirakis", "author_profile_id": "81100455984", "affiliation": "Harvard University, Cambridge, Massachusetts", "person_id": "PP15032733", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/582153.582159", "year": "1982", "article_id": "582159", "conference": "POPL", "title": "Unbounded speed variability in distributed communication systems", "url": "http://dl.acm.org/citation.cfm?id=582159"}