{"article_publication_date": "01-25-1982", "fulltext": "\n AUTOMATIC GENERATION OF t4ACHINE SPECIFIC CODE OPTIMIZERS Robert Gie.gerich Institut fOr In formatik \nTechnische Universit8t. MUnchen Arcisstr.?l 8 Mt!nchen ? Nest Germany 1. Introduction 1.1. Motivation \nWhile compiler generating systems are gradually moving towards production quality, the compiler subtask \nof machine code optimization has so far withstood automation (except for [Fra791, see below). The necessity \nfor such a phase comes from the fact that code generators produce code in a piecemeal fashion, in order \nto limit the otherwise combinatorially growing amount of case analysis to be done. The emitted code sequences \nmay be optimal when considered by themselves, but allow (if not require) optimization when placed in \nthe context of others ([PQC80], [Fra79]). This effect is especially severe where, for reasons of retargetability, \ncode for various processors is generated from a common low-level intermediate form by some kind of macro \nexpansion. This work was supporter by Sonderforschungsbereich 49 -Pro.grammiertechnik Permission to copy \nwithout fee all or part of this material is granted provided that the copies are not made or distributed \nfor direct commercial advantage, the ACM copyright notice and the title of the publication and its date \nappear, and notice is given that copying is by permission of the Association for Computing Machinery. \nTo copy otherwise, or to republish, requires a fee and/or specific permission. @ 1982 ACM 0-89791-065-6/82/001/0075 \n$00.75 1.2. Problems with machine code optimization (MCO) Mechine code optimization ( peephole optimization \nin [McK65])is the least understood of all compiler phases because of its total machine depend~nce. Inconsistent \naddressin~ mode constraints, inhomogenous register sets, overlapping of registers or memory cells, side-effects \nof instructions and addressing and the like introduce an annoying kind of complexity, making it diffioult \nto produce a correct NCO, even when writing it by hand [PQc801. To the knowled~e of this author, there \nhas been no (published) attempt to cast instruction sets and the optimization based on them into a formal. \nframework which allows the automatic generation of correct machine code optimizers. Research in this \ndirection has been announced by the PQCC-project [PQCNI]. 1.3. Previous work Fraser s retargetabl. \ne peephole optimizer (PO) [Fra791, fDaF801 takes a conceptually very elegant approach, Instruction sets \nare described by ISP-[TieN72] patterns. During optimization, adjacent pairs of instructions are taken \nfrom the program, and their descriptions are combined. Via symbolic manipulation an attempt is made to \nsimplify the combined ISP-patterns into a pattern describing a single instruction. If successful, the \ninstruction pair is replaced by this instruction. Although the results are quite Rood, there is a twofold \nlimitation inherent to an interpretive approach like this, as already addressed in [DaF80]. one is the \ntwo (or three) instruction window: Accidentally intervening Fig. 1 Diagram of a generative approach \nto machine code optimization ; standard rules ~ <--- -additional rule bo . .............. ., Suggestions \n! machine I GENERATOR i description -- - -> ! I < - implementation 1 produces I decisions t, by abstract. \nymbolic interpretation evaluation GENERATION TIME -----------.-- ___________ _________ --------------------------TRANSLATION \nTIME ~ ana----> !trans-1 machine code --- - > llyser I <---lforrnerl - --> optimized code ,,I8 I machine \nspecific optimizer instructions prevent the ronbinat.ion of instructions which could otherwise be optimized. \nThis cannot in principle be avoided by using a larger window. The second point. is thst. doinfi all the \nsymbol ic manipulation at optimization time makes PO very S1OW -clearly, much string processing is repeated \nfor each attempt to combine an instruction pair, be it successful or not. Most of this effort. need not \nbe done again and again at. translation time. It can be done once and for all, if we can cleanly separate \nmachine and program dependent aspects. Trying to improve on PO in these two respects leads to a radically \ndifferent, generative approach which was developed in [Gie811 and is outlined below. 1.0. Principal ideas \n The overall structure of our spproach may be visualized as in figure 1. We give a formal model of instruction \nset semantics. By means of this model, we can describe the information (Ilcontext!r) about the program \nrelevant. for optimization as an approximate semantics [COC791. A machine-specific program a alyser which \nderives this context, information using data flow analysis (DFA) methods can thus be fienerat.ed from \na given machine description. We define a formal notion of instruction equivalence relative to a given \ncontext. This notion is turned into a generator which takes an arbitrary pair of machine instructions \nB,B and produces a criterion, under which the replacement, of B by R! is legal. From this criterion, \nall but the t,ruly program dependent clauses are removed at generation time. This leads to a flexible \nconcept for the automatic generation of efficient machine code optimizers. 2. Formal Instruction Set \nSemantics A formal instruction set semantics is the key to t,he generative approach. It precisely captures \nthe effects of overlapping cells, side effects, etc.. Due to space limitations, we are here faced with \nthe paradox of presenting the formal model informally. Generally, machine descriptions are written in \na sort of ISP notation, which is given a semantics in t<he style of an denotational language definition, \naugmented with a number of concepts specific to the machine code level..  2.1. Operative StoraRe Level \n Processor registsrs, flaxs and memory are described as operative cell classes in the sense of [Rip771. \nThey may be thought of as arrays, indexed from a set of addresses. Let C be the set of cells, i.e. the \nelements of these arrays. A relation overlap: CXC-->FJOOL indicates overlapping of cells. Example (from \nMC68000 description): A[O:71<31: O>; --32-hit registers Y8Kf):&#38;2)<7:O>; --byte memory Z<o:o> --zero \nflag So far, overlap(cl,c2)= (cl=c2). From cell classes already defined, further ones can be built by \nmeans of a set of cell class constructors, such as pairing, subcell selection, etc. : A16[il: =.A~il<15:O> \n--low order half register Y16:=Mfl ~ 2 --word memory M32:.N16 by 2 not aliKned - double word memory \nThe semantics of the cell class constructors are such that a complete overlapping relation can be derived \nautomatically, e.g. overlap(M8[al,M32[hl) = (b<a<b+?), -. overlap (M32[a], Y?2[b]) = (b 2<a<b+2). .\u00ad \n 2. ?. Instruction Execution Level Instruction effects are modeled <as St, ate transformations, where \nstates are associations of values t,o cells. fElementaryfq effects affect (except for overlapping) exactly \none cell ; instructions are built from elementary ~ffects hy parallel (,) and sequential (;) combination, \ne.fi. AIOl:=A[ll, A[l]:=AIOI --exchange tlS[17]:=A[O]; z:=(M[17]=O) --move with condition code s-tting. \nCells are the operands of instructions as they appear in the program. Therefore they are the main objects \nof interest to flow analysis. They are represent.ed using static addresses, e.q. .A(OI, or dynamic addresses, \ne.fl. ~s[.irnll, v32r.hrll+1203. The latter are model led formally as maps from states into addresses. \n2.3. Tnst. ruct, ion Set Level. This level embodies all the concepts ,which structure the addressing \nand operation capabilities of the qiven machine. Address templates generate the available dynamic addresses; \naddressing modes declare which templates may be used for which cell classes, plus cost and side effects \nassociated with a particular access, A[j], A[iJ+d - address templates infl(j): f8rArj11 ~ 0.3 --indirect \nmode riisp(i,d): M32[A[i]+d] cost 2.6 --base+displacement addressing mode posting: M32[A[$j]]; A[j]:=A[j]+4 \ncost 0.6 --postincrement mode Instructions, as they are described, are parametr?rized by operand classes, \ni.e. sets of admissible addressing modes. 3. Context Information We use a pair of (left/ right,) context \nwhich constitutes the essential information for most optimization. Our context may be viewed as a derivate \nof the semantic contexts used in [Ger751, obtained by applying the methods of abstract interpretation \n[COC791 to our instruction set semantics. RiRht. context is a framework called f faint variables , a \nslight strengthening of dead variable information. Similar, bl>t. more restricted information is considered \nin [PQC801 and used in rFra7~l. Tt is essential for elimination of redundant stores and the like, but \nalso for handling introduction and elimination of side effects that come along with other optimization. \nLeft. context is a combined copy and constant propagation framework. If a left context 1 contains the \npair (M32[A[i]+12U]],A[Ol), this !ne~ns that both cells hold the same value. This information is mainly \nresponsible for exploiting special Casas of instructions, redundant transfers, constant folding and \noperand substitution. The latter two are particularly beneficial with microprocessor addressing. The \ndomains of left and ri~ht context will be denoted by L and R, while (I, r) will stand for a particular \ncontext associated with some point in a program. Note that both kinds of context provide static information \nabout dynamically addressed cells. Information about M8[A[111 is affected when the cell itself, A[ll, \nor any cell possibly overlapping one of those is used/modified by some instruction. In our model, the \ncorrect, machine-specific context propagation functions can he generated automatically. Any compiler \nwriter would find it very hard to do this correctly by hand for real machines.  4. Generation of a \nMachine Specific if i.j then d<O<d+3 else true. Program Analyzer Clearly, this reduc~s the optimization \ntime effort to a mimimum. One example roust suffice here to demonstrate how the formal model serves to \nisolate data, program and machine dependent aspects. Let Z cell classes, A addresses, 5. Feedback Between \nContext Information V values, and Optimizations &#38;~{Z[aIlZ6~, a&#38;A] cells,  SCC - > V states, \n ._ ~)A+-->(S >~) address templates, When optimizing programs usin~ transformations ~sZ Xl) addressing \nmodes, based on DFA-information, we want to use the Cd ~{ZIDa]l(Z,D)~fl, a~domain(l))} dynamically calculated \nsolution for a whole series of addressed cells. transformations. As these moriify the program, they (We \nuse parenthesis-free notntion for function may render the previous flow information application. Hence, \nDas stands for (D(a))(s) and suboptimal or even invalid. This fact has been denotes an effective address.) \nwidely neglected in the DFA literature. For deriving flow information abo,ut. dynamically Example: elim(l, \nr) addressed cells, we must consider their Let RI be the rule X:.Y >skip, with overlappin~. elir(l,r)= \n(x,y)~lvxer. d overlap:Cd XCd XS-->BOOL. is defined as In the programm d overlap(ZIDa]~Z [D a 1,s) := \nX:=y: X:=y overlap(ZIDasl,Z [D a s1). either statement may be eliminated by RI, but This is useful only \nas a starting point, since it obviously not both. Speaking flow analytically, is not evaluable at optimization \ntime. We need application of RI to the first assignment some approximation which indicates potential \n(since xEr) makes the left context of the overlapping only, hut. is evaluable at optimization second \n((x,Y)61) invalid, and vice versa. time. Hence we define two abstractions: The fact that the mutual interference \nof various a) total abstraction from program behaviour: optimization techniques with the properties of \nt overlap: Cd XCd-->BOOL with programs they rely on has not been studied in a t overIap(ZIDaI,Z [1) a \n1) := formal framework is probably responsible for much ~s S: d_overlap(ZIDa],Z rD a ],s). of the unreliability \nof current optimizing b) partial abst. ract. ion relative to con ~xt. compilers. information: We state \na few results which help to deal with isssume we hsve some predicate 16LS(~-->RnOL), this situation not \nonly in machine code restricting the set of possible states. (Left. optimization. l~le can define notions \nof tsafety! context provides such predicates. ) Then we and !!invariance ! of a DFA framework with respect \nobtain a more precise relation by to a set T of tranformation rules ,with the p overlap:C.d XCd XL-->BOOL \nwith following pr~perties (cf. [Gie811, [GMWR1l): p_overlap(ZIDa],Z [D a ],1) := a) If a DFA is safe, \nflow information remains ~s~: ls~d_overlap(ZIDa],Z 1[D!a?],s ). admissible over any series of transformations \nRoth a) and b) are evaluable at optimization time. from ~, Rut we can do much better, Consirlerinfi that. \nV is b) If it is invariant, optimal fixpoint solutions known at fleneration time and smsll compared to \nCd are preserved. (or its subset used in a program), we transform c) In the case of invariance, a single \ne.g. t_overlap into optimization pass with rules from T is ~_over]sp!~X~--> (~+ X A+ __>BolJL) with exhaustive. \ng overlap(Z[l)l,Z [Dyl) := cl) Our frameworks of left./right context are ~a,a .t_0verlnp(Z[Dal,Z [D a \nl). invariant with respect to transformation rules g_overlap is evaluable for each pair of addressing \nusing only left/right context. (This does not modes at generation time.. Thus we have a generator hold \nfor a right context dead variables .) yielding an overlapping condition dependin~ only e) If a I)FA is \ninvariant wrt,. ~, then any on the actual a,a! as they occur at any given approximation [CoC791 of it \nis safe wrt. ~. point in the program. For example, (This frees us to do an almost arbitrarily g_overlap(ind(j),disp( \ni.,d)) yields simplified flow analysis instead of iterative solving of DFfl-equations. ) 6. Generation \nof Machine Specific Replacement Rules Let ~ be the set of instructions. From our formal model, we obtain \na relation of instruction equivalence relative to a given context, S:BXBXLXR --> FYIOL. . This notion, \nof course, is the heart of our approach. Still, its precise definition cannot. be given here. ~, as it \nstands above, is evaluable at optimization time. We turn it into a generator by converting to GEN: ~XF&#38;->(LXR \n--> BOOL). Pt-acticall.y this is done by interpreting the definition of : for fixed pairs of instructions, \nbut symbolic values for 1 and r. Example (from 8080 description): For API): A:= A+op R, Cy:=-(A+op8); \nZ:=(A=O), INC: m reg:=m reg+l; Z:=(m reg=(l)  where op8 = {register, implied, immediate], m_reg = {reRister, \nimplied} operands, we obtain GEN(ADD, INC) = (op8,1)el A Cy6r AAm_reg. The symbolic manipulation itself \nis comparable to what Fraseris PO does at optimization time. The point is, however, that we have moved \nthe essential part of the computation from optimization to generation time. During, optimization, we \nmust only check some typically very simple conditions regarding context and operand bindings. Equipped \nwith the capability to generate the applicability criterion crit(l,r) for an arbitrary ~ crit(l,r) > \n~,, we are left optimization rule with the question which pairs B,Bf to consider, and in what fashion \nto apply the generated rules. 7. Overall Organization of the Generated Machine Code Optimizer Figure \n2 gives a list of optimization considered in our investigation. They include (and generalize) the examples \nin [DaF801 and most of rwu~751, plus some addressing optimization particularly relevant for some microprocessors \nwhere addresses are bulkier then most data. Each phase consists of a set of generated rules, which are \napplied as the programm is traversed in forward or backward direction. While all other phases decrease \na space over time or time over space cost function with each rule application, ALC combines CAM and CRA \nwith a simple strategy which may introduce address loads if they have a payoff later. CRJ may be extended \nover [WU175], since we can generalize the (accidental; ordering of a code sequence to a partial ordering \nby means of an automatical ly generated relation of instruction interchangeability. Fig. 2 Phases of \nmachine code optimization nam? phase ELE elementary replacements ERT elimination of redundant instructions \nCAM change of addressing mode CRA cross addressing ALC addressing linear code sequences ASE use addressing \nside effects SEC special effect combinations EJC elimination of jump chains CRJ cross jumping SI)J span \ndependent jumps example MUL Al, #2 --> SLA A MOV A, B --> skip if A=B or A faint ..M[al. .-->. .MIR].. \nif R ..M[a]. .-->. .R.. if R=M[al ..M[al. . --> INC R; ..M[R], see below Fi:=R+l --> autoincr. with exploit \nexchange, redundant increment-and-jump, BEQ m;.. .m:BGE n --> BEQ n:... (m:BGE n) but cf. [WU1751 and \nbelow is CAM PIUS strategy from =a , an MIRI test, etc. see below [szY731. Fig. 3 Machine specific rules \ngenerated from a standard rule ADDlx,y --> ADDI X,2 : x #{predec,postino}A (y,z) &#38;l ADDlx, y --> \nADDQ X,Z : x ~{predf?c,postinc] A(y, Z) &#38;l AZ ~[1:81 AI)DA X,y --> ADDI X,2 : false ADDA X,y --> \nADDQ X,2 : x${predec,postinc}~ (Y,z)el AZ G[l:RIA{CY ,X,V,Z,N) &#38;!r ADD1x,postinc --> ADDI X,Z: (y, \nz) GIAA[j]&#38;r Knowledge about the various phases is incorporated restrictions: Profirams must not \nmodify in the generator via i few dozen machine themselves, cells which may be accessed from independent \n!~standardf! rules. \\fhen producing the outside the executing program (because they are MCO, the generator \nselects specific instructions io-ports, or via interrupt) must be known, and the from the machine description \nwhich instantiate flow graph of the program must be construct.able. these rules. Additionally, the compiler \nFor the latter two, the relevant. information is implementor may suggest pairs of instructions and readily \nprovided by a code generator or have a machine specific rule generated. programmer. ?{o special assumptions \nare made about Example: the method of code generation. In principle, we For the standard rule t x +y \n--> x+zq?, from a can optimize hand coded assembly programs as well. NC68000 description the generator \nsel~cts the instructions AI)DI, ADI)A, ADDQ, and t,he two variants of ADD: ADD, = add to register, and \nADD2 = add to memory. Some of the generated 8.2. Adaptability of the Generated Optimizer machine specific \nrules are shown in figure 3. There are positive and negative phese ordering problems [WU1751 abound in \nmachine code When generating a machine specific optimizer, it optimization. Their presence or absence \nis easily cen be tailored to the known strengths or shown by considering the context. information which \nweaknesses of the code generator. This is not only the various phases depend on, or modify. done in describing \nthe overall strategy. DurinR Which optimizations are to be used at nll and to generation, the implementor \nmay interactively what extend they are to be performed is an restrict, Renerat,ed optimization rules \naccorrlin R implementation decision, which must be provided his judgement of the expected cost/effect \nratio. by the implementor of the compiler. Therefore the Defining context information using flow analysis \noverall. strategy may be described by a (restricted methods enables us to provide a consistent form of) \nregular expressions. For example, hierarchy from sophisticated to very efficient,, although our phases \nare not strictly comparable to but more restricted methods of context those named in [WU1751, the FINAL \nphase of the propagation. For example, the version we are BLISS/n compiler would be described by currently \nimplementing considers only basic blocks (ELE:EJC ;ASE; CAMIERI; CRJ)* ELE SDJ, plus their immediate \nenvironment. Some optimizations need particular information other than our left/right. context. For example, \non most machines flags may be set, individually. Thus, a BEQ does not always imply a BGE. Jump chain \nelimination must make sure that Zero-and 8, Conclusion Negative-flag were in fact set consistently. This \nis easily handled by a trivial flow analysis. so, the need for another kind of information does 9.4. \nScope and Limitation. not lead out of our ovcrsll framework. Only if s new subtrategy were desired for \nsome phase, the optimizer skeleton would have to be augmented. The investigation in [Gie811 was limited \nto The ultimate question of optimal balance of microprocessors (ranging from F8 to MC86000 and optimization \neffort vs. code quality for a given 8086). Most of the optimizations are relevant for set of implementation \ndecisions can probably only bigger machines, too . Methods like exploiting be answered experimentally. \nOur approach makes CPU-internal parallelism were not studied. such experimental studies feasible. The \ncode to be optimized must obey three Acknowledgement Technical University Munich, 19/31 [GMW811 R. Giegerich, \nU.!13ncke, R. Wilhelm: Thanks go to H.Ganzinger and W.Willmertinger, who Invariance of approximative \nsemantics with wrote the text forms tter with which this paper was respect to program transformations, \nprepared -just. in time. proceedings llth annual GI conference, Munich 1981, Springer, In formatik Fac!hbericht.e \n50 [hlcK651 W.M.McKeeman: References Peephole optimization, CACM 8,7, 1965 [PQc801 B,W.Leverett, R.G.G,Cattell, \nS.O.Hobbs, [13eN711 C.G.Bell, A.Newell: Computer structures: J.M. Newcomer, A.H.Reiner, B.R.Schatz, Readings \nand examples, McGraw-Hill, 1971 W.A.Wulf: [COC79] P.Cousot, R.Cousot: An overview of the Production Quality \nSystematic design of program analysis Compiler-Compiler project,, IEEE computer, frameworks, POPL 6, \n1079 Aug. 1980 fDaF80J J.W.Davidson, C.W.Fraser: [Rip771 K. Ripken: The design and application of a Formale \nBeschreibung von Maschinen, retarget.able peephole optimizer, TOPLAS Imple!nentierungen und optimierender \n?,2, 1980 Maschinencodeerzcugung aus attributier\u00ad rFra791 C. W.Frzser: ten Programmgraphen, TUM-INFO-7731, \nA compact,, nachine-independent peephole Tnstitut fil r Tnformatik, Technical optimizer, POPL 6, 1979 \nUniversity Munich, 1977 [Ger751 S.GPrhart: [3zY78] T.G.Szymanski: Correctness preservinfl program Assembling \ncode for machines with transformations, POPL 2, 1975 span-dependent instructions, CACM 21,4, [Gie811 \nR,Giegerich: 1978 Automatische ErzeuKung von Maschinen\u00ad [WU1751 W.A.Wulf, R. K. Johnsson, C. B. Weinstock, \ncode -Opt. imierern, Dissertation, S. O.Hobbs, C. M. Geschke: TIJM-T8112, Instit,ut far Informatik, The \ndesign of an optimizing compiler, American l?lsevier, New York 1975  \n\t\t\t", "proc_id": "582153", "abstract": "", "authors": [{"name": "Robert Giegerich", "author_profile_id": "81100110280", "affiliation": "Institut f&#252;r Informatik Technische Universit&#228;t M&#252;nchen West Germany", "person_id": "PP14048764", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/582153.582162", "year": "1982", "article_id": "582162", "conference": "POPL", "title": "Automatic generation of machine specific code optimizers", "url": "http://dl.acm.org/citation.cfm?id=582162"}