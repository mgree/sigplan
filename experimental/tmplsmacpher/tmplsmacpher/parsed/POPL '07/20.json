{"article_publication_date": "01-17-2007", "fulltext": "\n Towards a Mechanized Metatheory of Standard ML * Daniel K. Lee Karl Crary Robert Harper Carnegie Mellon \nUniversity {dklee,crary,rwh}@cs.cmu.edu Abstract We present an internal language with equivalent expressive \npower to Standard ML, and discuss its formalization in LF and the machine-checked veri.cation of its \ntype safety in Twelf. The in\u00adternal language is intended to serve as the target of elaboration in an \nelaborative semantics for Standard ML in the style of Harper and Stone. Therefore, it includes all the \nprogramming mechanisms nec\u00adessary to implement Standard ML, including translucent modules, abstraction, \npolymorphism, higher kinds, references, exceptions, recursive types, and recursive functions. Our successful \nformaliza\u00adtion of the proof involved a careful interplay between the precise formulations of the various \nmechanisms, and required the invention of new representation and proof techniques of general interest. \nCategories and Subject Descriptors D.3.1 [Programming Lan\u00adguages]: Formal De.nitions and Theory Semantics; \nSyntax; F.3.1 [Logics and Meanings of Programs]: Specifying and Veri\u00adfying and Reasoning about Programs \nMechanical veri.cation General Terms Languages, Veri.cation Keywords Standard ML, language de.nitions, \ntype safety, mech\u00adanized metatheory, logical frameworks, Twelf 1. Introduction A formal de.nition of \na programming language provides a rigor\u00adous, implementation-independent description of the semantics \nof well-formed programs. By giving a precise meaning to programs a formal de.nition provides the foundation \nfor building a commu\u00adnity of users, for ensuring compatibility of implementations, and for proving properties \nof the language and programs written in it. But a formal de.nition does not stand on its own, but must \nbe sup\u00adported by a body of metatheory that establishes both its internal consistency and coherence with \nexternal expectations. The formal de.nition of a full-scale programming language can easily run into \nhundreds of pages, as exempli.ed by The De.nition of Standard ML [21]. Verifying the metatheory of such \na language taxes, or even exceeds, human capabilities. Absent complete ver\u00adi.cation, the best alternative \nis to employ well-established meth\u00adods, such as type systems and operational semantics, supported by \n* This work is supported by the National Science Foundation under grant ITR/SY+SI 0121633 and a Graduate \nResearch Fellowship, and by a grant from the Alfred P. Sloan foundation. Permission to make digital or \nhard copies of all or part of this work for personal or classroom use is granted without fee provided \nthat copies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. POPL 07 January 17 19, 2007, Nice, France. \nCopyright c . 2007 ACM 1-59593-575-4/07/0001. . . $5.00. small case-studies that expose pitfalls. But \neven using these best practices, errors and inconsistencies arise that are not easily dis\u00adcovered. Moreover, \nas languages evolve, so must the metatheory that supports it, introducing further opportunities for error. \nA promising approach to reducing error is to use mechanized veri.cation tools to ease the burden of proving \nproperties of language de.nitions. Ideally, a language de.nition would come equipped with a body of metatheory \nthat is mechanically checked against the de.nition and that can be extended as need and interest demands. \nWith the development of powerful tools such as mechani\u00adcal theorem provers and logical frameworks, it \nis becoming feasible to put this idea into practice. For example, Klein and Nipkow [17] have recently \nused the Isabelle theorem prover [23] to formalize a large part of the Java programming language and \nto prove type safety for it. In this paper we report on the use of the Twelf implementa\u00adtion [27] of \nthe LF logical framework [12] to verify the type safety of the full Standard ML programming language. \nTo our knowledge this is the .rst mechanical veri.cation of safety for a language of this scale. The \n.rst mechanical formalizations of signi.cant subsets of The De.nition of Standard ML were performed independently \nby Syme [36] and VanInwegen and Gunter [39] using HOL [10] for the purpose of establishing determinicity \nof evaluation. An at\u00adtempt by VanInwegen [38] to prove type safety was partially suc\u00adcessful, but ran \ninto dif.culties with the formalism of The Def\u00adinition of Standard ML, the immaturity of veri.cation \ntools and methodology at that time, and the unsoundness of the language it\u00adself. Our approach draws on \nintervening experience with logical frameworks [12, 27] and with formalizing language de.nitions us\u00ading \ntype-theoretic techniques [16]. Perhaps the most signi.cant les\u00adson to be drawn from VanInwegen s and \nour experience is that lan\u00adguage de.nitions must be formulated with mechanical veri.cation of metatheory \nin mind. The formulation of the de.nition provides the framework for veri.cation, but the demands of \nveri.cation must also be permitted to in.uence the de.nition. Just as programs ought to be written in \nconjunction with proofs of their key properties, so too must language de.nitions be developed hand-in-hand \nwith their veri.cation. 2. Overview Our approach is based on the type-theoretic de.nition of Standard \nML given by Harper and Stone [16]. The Harper-Stone semantics divides the de.nition of the language into \ntwo aspects: 1. Elaboration, which translates the external language,the ab\u00adstract syntax of Standard \nML, into the internal language, a well\u00adbehaved type theory based on the translucent sums formalism of \nHarper and Lillibridge [14]. Elaboration performs type re\u00adconstruction, overloading resolution, equality \ncompilation, pat\u00adtern compilation, and coercive signature matching, resulting in a well-typed term of \nthe internal language. 2. Typing and evaluation, which enforces type constraints and imposes an operational \ninterpretation on programs. The inter\u00adnal language is a well-behaved, explicitly typed .-calculus equipped \nwith a transition-based operational semantics on states of an abstract machine. We will refer to the \ngeneral structure of de.nition-by-elaboration as elaborative semantics, and to the Harper-Stone semantics \nfor Standard ML in particular as H-S. Note that the utility of elab\u00adorative semantics is not limited \nto ML; it has also been used by Drossopoulou, et al. [8, 9] in rigorous accounts of the semantics of \nJava. The use of elaborative semantics follows in the spirit of Reynolds [32] which explored the semantics \nof an idealized lan\u00adguage that embodied the essence of Algol. Elaborative semantics goes further, in \nthat the relationship between a language and its idealized counterpart is made explicit by a formal translation. \nIn the context of Standard ML, the H-S semantics has been used successfully as the foundation for the \nTILT compiler for Standard ML [37, 25, 34, 24], and as a basis for language extensions such as recursive \nmodules [5] and modular type classes [7]. For the present purposes, as well as that of the cited work, \nthe utility of elaborative semantics accrues from the isolation of an internal language to which operational \nmeaning is assigned using standard techniques; the external language is given meaning only via its elaboration \ninto internal form. This permits one to formalize type safety for the internal lan\u00adguage as the conjunction \nof preservation (well-formed states tran\u00adsition only to well-formed states) and progress (well-formed \nstates are either .nal or can make a transition). Type safety for the ex\u00adternal language then follows \nfrom the fact that elaboration yields a well-typed program in the internal language. In the present pa\u00adper \nwe outline the proof and mechanization of the type safety of the Standard ML internal language, leaving \nthe formalization of elaboration and the proof of well-typing of elaboration to future work. The internal \nlanguage used is similar enough to the H-S in\u00adternal language that we believe the elaboration relation \ngiven in H-S can be adapted for our IL without serious dif.culties. In fact, the safety proof of the \ninternal language represents the bulk of the effort, since the internal language has the same expressive \npower as Standard ML, lacking only conveniences such as type inference and pattern matching, which can \nbe handled separately. However, we will discuss elaboration, where appropriate, to motivate the in\u00adternal \nlanguage s design. It is natural to ask whether one could carry out a similar mecha\u00adnization based on \nThe De.nition of Standard ML. Our results have no direct bearing on this question, but it may be informative \nto ex\u00adplain why we did not choose this approach. First, we had in mind VanInwegen s earlier attempt, \nwhich was not entirely successful due in part to errors in The De.nition itself (the language she stud\u00adied \nwas not type safe), and in part to complications in handling the machinery used in The De.nition (a considerable \narray of ad hoc semantic objects such as .nite maps, generative stamps, realiza\u00adtions, and so forth). \nSecond, we drew on our previous experience with the H-S semantics in the implementation of the TILT compiler \nand in studies of language extensions that supported our belief in the utility of type-theoretic methods \nin language design. Third, we intended to make use of the Twelf implementation of the LF logi\u00adcal framework \nfor our work, which provides strong support for for\u00admalizing and proving properties of type-theoretic \nlanguages with purely syntactic notions of binding and scope. The machinery of The De.nition is not amenable \nto formalization in Twelf more precisely, using such machinery would obviate the advantages af\u00adforded \nby Twelf. Fourth, The De.nition is based on an evaluation semantics, which does not support a direct \nproof of type safety. In\u00adstead, one must extend the semantics with spurious wrong tran\u00adsitions, only \nto prove that they are, indeed, spurious. Finally, The De.nition relies on a number of informal conventions, \nsuch as the handling of overloading and the dynamic signi.cance of signature matching, whose formalization \nwould, we suspect, require a layer of elaboration if fully developed. In short, the structure of The \nDef\u00adinition poses problems in general for proving safety, and in partic\u00adular for proving safety in mechanized \nform. Present Work In our present work, we do not utilize the H-S semantics per se, but rather a variation \nof it that was informed by subsequent work on the type theory of modularity and by our .rst, failed attempt \nat veri.cation using H-S verbatim. The inter\u00adnal language of H-S is an extension of Harper and Lillibridge \ns translucent sums formalism [14] with support for recursive types, mutable references, extensible datatypes, \nand exceptions. That lan\u00adguage is endowed with an operational semantics given as a transi\u00adtion system \nfor an abstract machine with an explicit control stack, as well as a store for reference cells and generated \ntags. Our .rst attempts at veri.cation, conducted by the second and third authors with Michael Philip \nAshley-Rollman, uncovered numerous minor .aws in the H-S internal language, as would be expected in a \nfor\u00admal veri.cation effort, but also ran into apparently insurmountable technical dif.culties with proving \ntype safety for the corrected lan\u00ad 1 guage. Our effort is based on Dreyer s variant [5] of the H-S seman\u00adtics. \nDreyer s internal language (and ours) is based on Stone and Harper s singleton kind formalism [35], rather \nthan translucent sums, and employs phase separation to propagate type information properly while respecting \nthe phase distinction [15] in a language with dependently typed modules. (In addition it treats data \nabstrac\u00adtion as a computational effect [6], which would permit supporting applicative functors [20], \nwhich we do not consider here.) More\u00adover, we formulate the operational semantics as a transition system \nusing Plotkin s method of structured operational semantics [31]. This supports a direct statement of \ntype safety, and avoids the com\u00adplications that arose in our initial attempt using the H-S semantics. \nThe veri.cation of safety for the internal language presents a number of challenges. These may be divided \ninto two broad cat\u00adegories: (1) mathematical challenges, and (2) formalization chal\u00adlenges. The mathematical \nchallenges are those posed by the proof, independently of any mechanization effort. These include delicate \nmatters of staging the key technical lemmas, such as functional\u00adity and validity, so that they may be \nproved in logical progres\u00adsion. These challenges are generally typical of those arising in the metatheories \nof module calculi. More novel are the challenges arising from the demands of for\u00admalization of our language \nin LF and of verifying its metatheory using the Twelf theorem prover. Here our previous experience with \nlogical frameworks was important. Our internal language is care\u00adfully formulated to ensure straightforward \nrepresentation in LF us\u00ading higher-order abstract syntax and judgements-as-types [12], to full advantage. \nUsing these methods ensures that we avoid the com\u00adplications encountered by VanInwegen in the treatment \nof binding operators, since the machinery of binding and scope is provided for free by LF. For example, \nas dicussed in Section 3, Dreyer correlated module variables with underlying type constructor vari\u00adables \nusing a naming convention that is untenable in higher-order abstract syntax. In our work we maintained \nthe correlation using a hypothetical judgement. Not only does this resolve the issue, but it also results \nin a more elegant language. 1 Space limitations prevent detailing these dif.culties here, but they can \nbe traced to a con.ict between the de.nition of well-formed machine states and side conditions on some \ntyping rules restricting certain terms to be values. In many cases, such as the issue of variable correlation, \nLF for\u00admalization pushed us to a cleaner development. However, we were not always so fortunate. The formalized \nproof of functionality independent of its staging with other theorems deals with the con\u00adtext in a manner \ndif.cult to represent in LF. Consequently, proving functionality required the development of a new proof \ndevice for Twelf that adds a detour to the proof. This issue is discussed in Section 6. One issue .ts \ninto both categories. In both progress and preser\u00advation, it is essential to establish inversion properties \nsuch as that no function type is equal to any non-function type, and that equal func\u00adtion types must \nhave equal domains and equal codomains. Such obvious facts are very far from trivial to prove in languages, \nsuch as ours, with a rich notion of type equality! The Standard ML type system relies on a formulation \nof type equality that, among other things, propagates type sharing information across module bound\u00adaries \n[14, 19, 35]. The declarative formulation of type equality in the presence of singleton kinds captures \nthese properties naturally, but at the expense of making it very dif.cult to verify inversion properties. \nOn the other hand, an algorithmic formulation, as is used in an implementation, makes it easy to establish \ninversion properties, but very dif.cult to establish other key properties (in\u00adcluding ones as apparently \nsimple as transitivity). Thus, a crucial ingredient in the canonical forms lemma is a proof of equivalence \nof the declarative and algorithmic formula\u00adtions of type equality. This result has already been established \nby Stone and Harper [35], but their proof cannot currently be ex\u00adpressed in Twelf due to the limits of \nTwelf s relational metatheory. Consequently, it was necessary to develop a new, syntactic proof based \non Watkins s technique of hereditary substitutions [40]. This issue is discussed in Section 4.3. We begin \nour discussion with an informal presentation of the internal language (IL) in Section 3. For the purposes \nof this paper, we mean by informal that the presentation is given in English and mathematical notation, \nnot in machine-checkable form. We follow with an informal (in the same sense) account of the IL s metatheory \nin Section 4. We discuss the IL s formalization in LF in Section 5, and the safety proof s formalization \nin Twelf in Section 6. We as\u00adsume no familiarity with LF or Twelf until Section 5. In Sections 5 and \n6 we assume familiarity with the methodology of LF encod\u00ading [12, 26, 13], and, in the latter, some elementary \nfamiliarity with the Twelf meta-logic will also be helpful. The full Twelf code of our formalization \nis available at: www.cs.cmu.edu/~crary/papers/2006/tslf.tgz  3. The Internal Language The internal language \n(IL) of our semantics is an explicitly-typed .-calculus based on Dreyer s modules formalism [5], enriched \nwith a variety of features to encompass the full expressive power of Standard ML. One simpli.cation compared \nto Dreyer s language is that we eliminate support for applicative functors, only because they are not \nnecessary for modeling Standard ML.2 The IL itself is not a research contribution of this paper; nearly \nevery construct in it appears in some form in prior work [15, 14, 19, 16, 35, 6, 5]. Consequently, our \ndiscussion here is a summary, not a thorough discussion. Our purpose is to convey the scope of the language, \nand to set the scene for discussion of its formaliza\u00adtion. Full technical details are included in the \ncompanion technical report [18]. The IL is structured into two levels: (1) the core level, which consists \nof constructors classi.ed by kinds, and terms classi.ed 2 It would not present serious dif.culties to \naccommodate them to model other languages, such as Objective Caml. constructors C ::= a kinds K |.. unit \nconstructor |.C1, C2. pairs |p1C left projection |p2C right projection |.a:K.C abstraction |C1 C2 application \n|Unit unit type |C1 \u00d7C2 products |C1 .C2 functions |C1 +C2 sums |Ref C1 references |Tag C1 generative \ntags |Tagged tagged expressions |\u00b5a:T.C recursive types ::= 1 unit kind |T |S(C) |.a:K.C |Sa:K.C types \nsingleton kind dependent functions dependent pairs Figure 1. Constructor and Kind Syntax by types (constructors \nof kind T) and (2) the module level, which consists of modules classi.ed by signatures. The layers are \nlinked by projections that extract the type and term components of a module. The language is designed \nto enforce the phase distinction, which ensures that type equality is independent of term equality by \narranging that an admissible type projection can be immediately determined to be a core language type. \nType de.nitions and type sharing relationships are managed using singleton kinds [35], which are separable \nfrom modules, in contrast to the translucent sums formalism used in the Harper-Stone semantics. This \nformalism isolates the issues of type equality from the other aspects of the language, and is of independent \ninterest from its application here. In particular, the complications related to type equality mentioned \nin the overview are cleanly isolated from the rest of the metatheory. Type abstraction is managed using \nthe effects-based techniques of Dreyer, Crary, and Harper [6] in which the imposition of ab\u00adstraction \nis regarded as akin to a computational effect. However, in contrast to the DCH formalism, we need only \nthe basic distinction between pure and impure modules, rather than the more sophisti\u00adcated classi.cation \nconsidered there. The reason is simply that the additional sophistication is not required for the semantics \nof Stan\u00addard ML. 3.1 Core Language 3.1.1 Constructors and Kinds The grammar for constructors and kinds3 \nis given in Figure 1. The kind T classi.es types, which are themselves used to clas\u00adsify terms.4 Most \nof the types and constructors are familiar. The Tag and Tagged types are used to implement Standard ML \ns exn type [16]. The singleton kind [35], S(C), classi.es the constructors that are de.nitionally equivalent \nto the constructor C.It isusedtomodel 3 In addition to these constructs, our formalization supports some \northogo\u00adnal constructs not used by Standard ML in anticipation of future develop\u00adment. 4 In the terminology \nof the ML type system [3] these are the monotypes;the polytypes arise in our formalism as a special case \nof functor signatures [16]. def S1(C) =1 def ST(C) = S(C) def SS(D)(C) = S(C) def SSa:K1.K2 (C) = SK1 \n(p1C) \u00d7S[p1C/a]K2 (p2C) def S.a:K1.K2 (C) = .a:K1.SK2 (C a) G .p1C:K1 G .p2C:K2 G .C:K1 \u00d7K2 G .C:.a:K1.L \nG,a:K1 .C a :K2 a/.Dom(G) G .C:.a:K1.K2 Figure 2. Higher-Order Singletons and Extensionality type de.nitions \nand type sharing speci.cations. Singletons create dependencies of kinds on constructors, so function \nand product kinds take dependent form, .a:K1.K2 and Sa:K1.K2, respec\u00adtively. The following judgement \nforms govern constructors and kinds: K is a well-formed kind: G .K.  C has kind K: G .C:K.  K1 is \na subkind of K2: G .K1 =K2.  C1 and C2 are equivalent at kind K: G .C1 =C2 :K.  K1 and K2 are equivalent \nkinds: G .K1 =K2.  The grammar of typing contexts is given in Figure 8. Constructor equivalence is induced \nby \u00df. rules for application and projection, together with rules for introducing and eliminating singleton \nkinds. The introduction rule for singletons, called self\u00adi.cation, assigns to each constructor C of kind \nT the singleton S(C); this is evidently the most precise kind for C. The elimina\u00adtion rule for singletons \npermits deduction of G . C1 = C2 : T from G . C1 : S(C2). Consequently, constructor equivalence is context-sensitive. \nFor example, we have a:S(C) . a = C: T because of the kinding assumption on a. Because of dependencies, \nconstructor equivalence induces a non-trivial kind equivalence. In addition there is a subkinding re\u00adlation \nthat is used to forget type equivalences due to single\u00adtons. Subkinding contains constructor equality, \nand is closed under the axiom S(C) = T, together with the usual variance rules for product and sum kinds. \nConstructor formation and equivalence are closed under kind subsumption. Consequently, constructor equiva\u00adlence \nis kind-sensitive. For example, the identity operator and the constantly C operator are equivalent at \nkind S(C) .T, but not at the kind T .T. Although singleton kinds can be formed only over constructors \nof kind T; we can lift them to higher kinds as well. In Figure 2 we give the de.nition of higher-order \nsingletons, together with two extensionality rules that are key to introducing them. For example, if \nC: T \u00d7T, then we can use the .rst extensionality rule to show that C: S(p1C) \u00d7S(p2C) = ST\u00d7T(C).  3.1.2 \nTerms The syntax for the term language is given in Figure 3. The lan\u00adguage of terms includes tuples, \nvariants, isomorphisms for recur\u00adsive types, generative tags, and tagged values. It also includes prim\u00aditives \nfor raising and handling exceptions, tag checking, and projec\u00adtion of the dynamic component from a module. \nTerms also include locations . ::= \u00b7\u00b7\u00b7 terms e ::= x variables |.. |.e1,e2. | p1e | p2e | fun x(y:C1):C2.e \n| e1 e2 | inlCe | inrCe | case(e1,x.e2,y.e3) | loc . | ref e | ! e | e1 := e2 | tagloc . | newtagC | \ntag(e1,e2) | iftagof (e1,e2,x.e3,e4) unit term pairs left projection right projection recursive function \napplication sum intro sum intro case locations new reference dereference assignment tag literals new \ntag tag injection tag check ||||| raise e try(e1,x.e2) rollCe unroll e snd(M) raise exception try/handle \nrecursive type intro recursive type elim module projection Figure 3. Term Syntax signatures K ::= 1 \nunit signature | [[C] constructor signature | [[K] kind signature | .a:s1.s2 dependent functions | Sa:s1.s2 \ndependent pairs Figure 4. Signature Syntax locations (reference literals) and tag literals, which arise \nduring ex\u00adecution but not in user programs. The typing judgment for terms is written as G; F .e:C,where \nG is a context and F is a store typing that assigns types to locations and tags (Figure 8).  3.2 Module \nLevel The chief technical novelty of the IL s module level (due to Dreyer [5]) are two projection operations, \nFst and snd.The for\u00admer is a judgement that extracts the constructor portion of a (pure) module. The \nlatter is a dynamic operation that projects the term portion of an (appropriately-typed) module. It is \nimportant to note that while snd is an ordinary operation, Fst is used only by the se\u00admantics, and cannot \nappear in the syntax of programs. In addition, the static semantics has a meta-operation on signatures \n(mirror\u00ading Fst, and also written Fst) that extracts the kind portion of a signature. This too cannot \nappear in the syntax of programs. 3.2.1 Signatures The syntax of signatures is given in Figure 4. There \nare three basic forms of signature, 1 for the trivial signature, [[K] for the signature of modules containing \na single constructor of kind K, and [[C] for the signature of modules containing a single term of type \nC. Signatures are closed under formation of dependent func\u00ad Fst(1) def = 1 Fst([[C] ) def = 1 Fst([[K] \n) def = K Fst(.a:s1.s2) def = 1 Fst(Sa:s1.s2) def = Sa:Fst(s1).Fst(s2) Figure 5. Static Part of a Signature \n(Fst) tions, .a:s1.s2, and dependent products, Sa:s1.s2.These are used to represent functors and sub-structures \nin Standard ML, re\u00adspectively. Consequently, functor signatures are partial (generative) in that applications \nare regarded as impure. (More on this below.) The astute reader will have noticed that the function and \nproduct signatures bind constructor variables, rather than module variables. This is a re.ection of the \nphase distinction in Standard ML, which precludes constructors that depend on terms, even indirectly \nvia modules. In general modules consist of a static part,which is a type constructor, and a dynamic part, \nwhich is a term. The syntax expresses the fact that the result signature of a functor can depend only \non the static part of its arguments, and similarly that a module in the scope of a sub-module can only \ndepend on the static part of the sub-module. Only certain forms of module the projectible modules have \na static part that can be determined during type checking. (The class of projectible modules will be \ndiscussed in more detail shortly.) When the static part exists, it is a constructor whose kind is deter\u00admined \nby the module s signature. The meta-level operation Fst(s) de.ned in Figure 5 determines the kind of \nthe static part of a mod\u00adule of signature s. Hence the rule: G .s1 G,a:Fst(s1) .s2 G ..a:s1.s2 Note that \nthe static part of a functor signature is trivial. This is a re.ection of the fact that Standard ML functors \nare generative, and hence do not have a statically computable static part. (Were we to support pure, \nor applicative, functors, their static parts would have functional kinds.) The judgement forms governing \nsignatures are as follows: s is a well-formed signature: G .s.  s1 and s2 are equivalent signatures: \nG .s1 =s2.  s1 is a subsignature of s2: G .s1 =s2.  Signature equivalence is induced by constructor \nand kind equiva\u00adlence. The subsignature relation is induced by the subkind relation in the usual fashion, \nexcept that our functor signatures are invariant, rather than contra-and covariant on the left and right. \nThis re.ects from the fact that our intended elaborator [16] employs explicit co\u00adercions at functor applications,5 \nso the IL does not require subsig\u00adnatures at those points. It would not be dif.cult to use the standard \nrule instead, were the IL to be used with a different elaborator.  3.2.2 Modules The syntax of modules, \ngiven in Figure 6, consists of introductory and eliminatory forms for each form of signature, plus two \naddi\u00adtional constructs that we shall describe shortly. The introductory 5 The reason for this, in turn, \nis because much more can happen in a Standard ML signature coercion than just dropping of type information. \nFor example, dynamic .elds may be dropped and polymorphic functions specialized. modules M ::= s |.. \nunit module |[e] term module |[C] constructor module |.M1,M2. pairs |p1M left projection |p2M right projection \n|.(s/as:s1):>s2.M functor |M1 M2 application |M :>s sealing |let s/as = M1 in(M2 :>s) let binding Figure \n6. Module Syntax forms for atomic signatures are [C] for [[K] ,and [e] for [[C] .In\u00adstead of an elimination \nform for [[C] , the static semantics use the aforementioned Fst. Atomic term modules [[C] are eliminated \nus\u00ading snd. Modules of product signature are introduced by pairing, and eliminated by projection, and \nmodules of function signature are in\u00adtroduced by .-abstraction, and eliminated by application. The syn\u00adtax \nof .-abstraction is unusual in that it introduces two variables, s and as, standing for the argument \nmodule itself, and its static part. The variables s and as are twinned in the sense that there is an \nimplicit correlation between them, even in the face of alpha\u00adconversion. (As we shall see in Section \n5, this is represented in LF by an explicit judgement form.) Typing contexts are extended ac\u00adcordingly \nwith a declaration of the form s/as : s. There are two additional constructs, sealing a module with a \nsignature to impose abstraction, written M :>s, and a form of let-binding for modules whose syntax is \nreminiscent of that of .-abstractions. (The explicit signature information is necessary to circumvent \nthe avoidance problem [6].) Abstraction is enforced using an effects system that classi.es modules into \ntwo categories, pure (or projectible), and impure (or non-projectible). Sealed modules are impure, and, \nsince functors are generative, so are all functor applications. On the other hand module variables are \npure, as are pairs of pure modules.6 As the name suggests, projectible modules permit projection of type \ncomponents. Moreover, if a module expression, M,of signature s is projectible, then Fst(M) is a constructor \nof kind Fst(s) representing the static part of M. Just as for signatures, Fst(M) is a meta-level operation, \nrather than a constructor-forming construct of the language. Its de.nition is given in Figure 7; in the \ncase of a module variable, s,we de.ne Fst(s) to be the correlated constructor variable, as. Note that \nwhile snd may be used only with modules having signature [[C] , Fst may be used with any projectible \nmodule. The meta-variable . stands for the purity (projectibility) class of a module, either P or I, \naccording to whether the module is pure or impure. The typing judgment for modules is G; F .M :. s,where \n. is a projectibility class. Projectibility classes are ordered by P .I, so that every pure module may \nbe regarded as (vacuously) impure. 6 Lambda abstractions may be considered pure, but this turns out to \nbe of no importance: since Standard ML functors are .rst-order and generative, there is no way to use \nthem without incurring an effect. s/as:s .G G .Fst(s) .as G .Fst(..) ... G .Fst([e]) ... G .Fst([C]) \n.C G .Fst(M1) .C1 G .Fst(M2) .C2 G .Fst(.M1,M2.) ..C1, C2. G .Fst(M) .CG .Fst(M) .C G .Fst(p1M) .p1CG \n.Fst(p2M) .p2C G .Fst(.(s/as:s1):>s2.M) ... Figure 7. Static Part of a Projectible Module (Fst)  3.3 \nStates and Stores As Standard ML is an imperative language, terms must be consid\u00adered along with a store \nthat assigns meanings to locations. Loca\u00adtions are used in two ways in our IL: to identify reference \ncells and to identify tags (we use the latter to implement exn constructors in Standard ML). When used \nto identify a reference cell, a loca\u00adtion is mapped to the value contained by that cell, and when used \nto identify a tag, a location is mapped to the type expected by that tag. Thus, stores have two components, \na heap serving the former purpose, and a tag typing serving the latter. A state in the IL s ab\u00adstract \nmachine consists of a term or module being evaluated, and an accompanying store. (The syntax for states \nand stores is given in Figures 8 and 9.) Heaps are classi.ed by heap types and stores by store types \n(Figure 8). Tag typings do not require a classi.er; we merely re\u00adquire that they be well-formed, which \nis the case when each tag type in it is well-formed with kind T. Thus, the judgement forms governing \nstates and stores are as follows: T is a well-formed tag typing: .T.  H has type .: F .H :..  S has \ntype F: F .S :F.  . is a well-formed state: ...  A state (e, S) is well-formed if there exists some \nstore type F classifying S, and some type C, such that \u00b7;F .e :C: F .S :F \u00b7;F .e : C .(e, S) A similar \nrule exists for typing states in which a module is being evaluated.  3.4 Dynamic Semantics The dynamic \nsemantics of the core level involves three main judge\u00adment forms: Expression e1 is a value: e1 val. \n Expression e1 raises the exception e2: e1.e2.  Transition between states: (e1,S1) ..(e2,S2).  These \nare de.ned simultaneously with three analogous judgements at the module level. Our progress theorem proves \nthat for any well typed state (e1,S1), one of these three judgements will hold. Note that val\u00adues are \nde.ned using an explicit predicate val. Although it is contexts G ::= \u00b7 | G,a:K constructor binding | \nG,x:C value binding | G,s/as:s module binding heap types . ::= \u00b7 | .,.:C reference type tag typings T \n::= \u00b7 | T,.:C tag type store types F ::= (., T) Figure 8. Contexts and Store Typings heaps H ::= \u00b7 stores \nS |::= H, ..e (H, T) states . ::= (e, S) | (M, S) Figure 9. States and Stores somewhat more common \nto de.ne values using a sub-syntactic grammar, to do so requires subtyping on syntactic classes, which \nis not supported by LF.  4. Informal Metatheory We prove the type safety theorem for our IL in the usual \nmanner. In that proof, complications arise due to dependent typing of con\u00adstructors, due to stores, due \nto the interaction of evaluation with type abstraction in modules, and most signi.cantly due to single\u00adton \nkinds. We outline here the most interesting lemmas on the path to proving type safety. 4.1 Preliminaries \nOne baseline property that is used throughout the metatheory is validity which states that the static \nsemantics deals only with well\u00adformed entities. For example, if C1 is equal to C2 at the kind K, then \nC1 and C2 individually have the kind K,and K itself is well\u00adformed. A direct proof of validity is stymied \nby the asymmetry in some de.nitional equality rules. For example: G .C1 =C. 1 :.a:K1.K2 G .C2 =C2 . :K1 \nG .C1C2 =C. 1C. 2 :[C2/a]K2 That C1C2 has kind [C2/a]K2 follows immediately by induction and the appropriate \nformation rule, but the same is not true for C. 1C. 2. For the latter, we need the additional fact that \n[C2. /a]K2 is equal to [C2/a]K2. Induction and inversion on kind formation gives us that G,a:K1 . K2, \nso the desired fact appears to follow from a functionality prin\u00adciple. However, to use this principle \nat this stage, we must carefully state it so that it can be proven before validity. Lemma 4.1 (Functionality). \n1. If G,a:K .K. , and G .C1 =C2 :K, and G .C1 :K, and G .C2 :K, then G .[C1/a]K. =[C2/a]K. .  2. If \nG,a:K .C. :K. , and G .C1 =C2 :K,  and G .C1 :K, and G .C2 :K, then G .[C1/a]C. =[C2/a]C. :[C1/a]K. \n. The third and fourth premises (G .C1, C2 :K) can be dropped, but only after validity has been established: \nLemma 4.2 (Validity for Constructors and Kinds). 1. If G .C:K,then G .K. 2. If G .C1 =C2 :K, then G \n.C1 :K, and G .C2 :K, and G .K.  3. If G .K1 =K2,then G .K1 and G .K2. 4. If G .K1 =K2,then G .K1 and \nG .K2.  In our preservation theorem, it is necessary to know that typ\u00ading of terms and modules is not \ndisturbed by the allocation of new references or tags. Thus, we require a lemma that expresses weak\u00adening \nwith respect to heap types: Lemma 4.3 (Weakening with Respect to Heap Types). 1. If G; (., T) .e :C and \n./.Dom(.), then G; ((.,.:C.), T) .e :C.  2. If G; (., T) .M :. s and ./.Dom(.), then G; ((.,.:C.), T) \n.M :. s.  A similar lemma expresses weakening with respect to tag typ\u00adings. As an aside, although weakening \nwith respect to the context comes for free with the formalization in LF, weakening with re\u00adspect to these \nstore type constituents requires explicit proof. This is because, as we will see, our formalization treats \nthe store type not as a context handled implicitly by LF, but as an explicit argument to the term and \nmodule formation judgement.  4.2 Modules As usual, the preservation theorem requires substitution lemmas \nfor term and module values. Term substitution is standard, but the statement of module substitution must \naccount for Fst: Lemma 4.4 (Module Substitution). 1. If G,s/as:s;F .e . :C. , and G; F .M :P s, and G \n.Fst(M) .C, then G; F .[C/as][M/s]e . :[C/as]C. .  2. If G,s/as:s;F .M. :. s. , and G; F .M :P s, and \nG .Fst(M) .C, then G; F .[C/as][M/s]M. :. [C/as]s. .  Note that module substitution requires that the \nsubstitutend be pure, so its constructor component can be extracted. Since the let\u00adbinding construct \npermits the module being bound to be impure,7 we require a lemma saying that module values are necessarily \npure. In other words, by the time we are ready to substitute a module, it is permissible to do so, because \nall of its effects have been resolved. Lemma 4.5 (Module Values are Pure). If M val and \u00b7;F .M :. s,then \n\u00b7;F .M :P s. Finally, since the module language is dependently typed, an issue typical to dependently \ntyped systems arises in the cases of the preservation theorem pertaining to application or to projection \nfrom a pair. Since the signature of FM depends on Fst of M (and similarly for p2M), when M steps to M., \nwe need to show that their constructor portions are equal. 7 This is a vital feature; without it, abstract \ntypes (the type components of impure modules) can never be used. Lemma 4.6 (Evaluation Preserves Fst). \nIf (M, S) . .(M.,S.), and \u00b7;F .M :P s, and \u00b7.Fst(M) .C, then \u00b7.Fst(M.) .C. and \u00b7.C =C. : Fst(s). 4.3 \nInversion As is the case for any language supporting a non-trivial notion of de.nitional type equality, \nour safety proof requires a number of inversion properties such as that no function type is equal to \nany non-function type, and that equal function types have equal domains and equal codomains. For example, \nthe canonical forms lemma (the key lemma for progress), states that any value e with type C1 .C2 must \nhave the form fun x(y:C1. ):C. 2.e .. This is not dif.cult to prove, provided one can rule out the case \nthat e is actually (say) a pair, which is placed into the function type by virtue of C1 \u00d7C2 =C1 .C2. \nSimilarly, in preservation, when considering the case of the beta-reduction of a function application, \nwe need to know that if fun x(y:C1):C2.e has type C1 . . C2. ,then C1 = C1 . and C2 = C2. . Again, this \nis not dif.cult to show, provided one may employ inversion on the equality of function types. Quite a \nnumber of such inversion results are required (quadratic in the number of type primitives, which is eight). \nA few typical instances are as follows: Lemma 4.7. Inversion: Contradiction 1. It is not the case that \nG .Unit =C1 \u00d7C2 : T. 2. It is not the case that G .Unit =C1 .C2 : T. 3. It is not the case that G .C1 \n\u00d7C2 =C. 1 .C. 2 : T. 4. Andsoforth.  Lemma 4.8. Inversion: Injectivity 1. If G .C1 \u00d7C2 =C. 1 \u00d7C. 2 \n: T, then G .C1 =C. 1 : T and G .C2 =C. 2 : T.  2. If G .C1 .C2 =C. 1 .C2 . : T, then G .C1 =C. 1 : \nT and G .C2 =C. 2 : T.  3. Andsoforth.  Since our IL includes a transitivity rule over constructors, \nwe cannot obtain any of these results by direct inductive proof. We re\u00adquire some strategy for taming \nthe complexity of de.nitional equal\u00adity. The solution is to employ an equivalent but syntax-directed \npre\u00adsentation of type equality. We will refer to this alternative presen\u00adtation as algorithmic equality. \nIt is not at all dif.cult to show that functional types are never algorithmically equal to non-functional \ntypes, and that two function types are algorithmically equal ex\u00adactly when they have equal domains and \nequal codomains. What remains is to show that algorithmic equality coincides with de.ni\u00adtional equality. \nThus far, the story is typical of languages with non-trivial no\u00adtions of type equality. For our IL, de.nitional \nequality is resolutely context sensitive (recall Section 3.1.1) making it far from clear how to use a \ntypical reduction-based account [1, 29]. Nevertheless, a satisfactory algorithm was devised and proven \nequivalent to de.ni\u00adtional equality by Stone and Harper [35]. Unfortunately, while Stone and Harper s \nalgorithm would be satisfactory for our purposes, their proof is not, because it can\u00adnot currently be \nformalized in Twelf. (We discuss why not in Sec\u00adtion 6.4.) Consequently, we found it necessary to develop \nan en\u00adtirely new proof, based on a somewhat different algorithm. Space considerations preclude a complete \ndiscussion of the new proof here. Instead, we summarize the salient points. The proof is based on Watkins \ns technique of hereditary sub\u00adstitutions [40]. It in we formulate a canonical presentation of the singleton \nkind calculus. (By the singleton kinds calculus, we mean the kind and constructor portions of the IL. \nFor the purposes of this proof, the remainder of the IL can be neglected.8) The canonical presentation \nrequires that constructors must be written in canonical form.9 The key property of the canonical presentation \nis that a con\u00adstructor can be written in only one way, up to alpha-equivalence. In other words, de.nitional \nequality is identity. Substitution cannot be de.ned in the usual manner in the canon\u00adical presentation, \nsince it could produce non-canonical construc\u00adtors. Thus, a notion of hereditary substitution is de.ned \nthat simul\u00adtaneously substitutes and re-canonizes. It is not at all obvious that any of the usual properties \nof substitution hold for hereditary substi\u00adtution, so those properties must be established. Measuring \ncoarsely by lines of Twelf code, this is about half the work. Once the properties of substitution are \nestablished, we may show, by induction on derivations, that equivalence classes of con\u00adstructors in the \nsingleton kind calculus can be mapped to a unique canonical form. This immediately provides an equivalence \nalgo\u00adrithm and its completeness proof. For soundness, it remains to show that a constructor is equal \nto its canonical form, which can be proven by induction over the structure of the canonical form.  4.4 \nType Safety Once inversion is established, we can prove the canonical forms lemma: Lemma 4.9 (Canonical \nForms). 1. If e val and \u00b7;F. e :C1 \u00d7 C2,then e has the form .e1,e2.. 2. If e val and \u00b7;F . e :C1 . C2,then \ne has the form funf(x:C.):C.. .e . . 3. If e val and \u00b7;F. e :Ref C,then e has the form loc .. 4. Andsoforth. \n We now have everything we need to prove progress and preser\u00advation. Theorem 4.10 (Progress). 1. If \n. (e,S)then either: e val,or  e.e .,for some e .,or  (e,S)..,S.),for some e . , S.  . (e . 2. If \n. (M, S)then either: M val,or  M.e,for some e,or  (M, S).,S., S. .  . (M.),for some M. Theorem 4.11 \n(Preservation). 1. If (e,S)..,S.), . (e and \u00b7;F. e :C, and F. S :F, then there exists some extension \nF. of F such that \u00b7;F. . e . :Cand F. . S. :F. . 2. If (M, S).,S. . (M.), and \u00b7;F. M :. s, and F. S \n:F, then there exists some extension F. of F such that \u00b7;F. . M. :. s and F. . S. :F. . This concludes \nthe informal overview of the safety proof. 8 The fact that Fst is a meta-operation and not a syntactic \nconstruct is key here. 9 Canonical forms are those that are beta-normal and eta-long, and do not include \nany subterms whose natural kind (in the sense of Stone and Harper [35]) is a singleton.  5. Formalization \nin LF We encode the IL in LF using the usual LF methodology [12]. That is, syntax is represented using \nhigher-order abstract syntax, and semantics using judgements-as-types. We will not reprise the basic \nLF methodology here; unfamiliar readers are referred to summaries by Harper, et al. [12], Pfenning [26], \nand Harper and Licata [13]. The examples use Twelf s [27] concrete syntax for LF. To establish notation, \nwe give the declarations of the type fam\u00adilies (that is, the syntactic classes and judgements) in Figure \n10, and the declarations of context blocks in Figure 11. Most of the formalization goes through smoothly, \nleaving little worth remark\u00ading on. However, two points do raise interesting issues, and each of them \nshed some light on the on-paper presentation as well. These points are (1) the use of the store type \nin the static semantics of terms and modules, and (2) the matter of twinned variables, arising in the \nformalization of the Fst operation on modules. 5.1 Store Types in Term and Module Typing In the informal \nsystem, we follow standard practice and write the store type to the left of the turnstile in the term \nand module typing judgements. However, the store typing is not truly a context, in the sense of a collection \nof alpha-convertible assumptions.10 Instead, the store typing indicates a world of discourse, relative \nto which the term or module is to be considered. Accordingly, while the IL context is identi.ed with \nthe LF context, as usual, the store type is included explicitly in the typing judgements for terms and \nmodules. (Moreover, for technical convenience, the heap type and tag typing are actually supplied as \nseparate parameters.) However, an explicit store type is not given for term and mod\u00adule variable assumptions. \nSince, as is typical, our IL s store evolves monotonically, an assumption x : Ref C, provides a usable \nrefer\u00adence value not just for the current store, but for all future stores. Put more concretely, if variable \nassumptions were good only for speci.c store types, Lemma 4.3 would not hold. In the terminology of modal \nlogic, variable assumptions are necessary. Consequently, the judgement used for terms and modules on \nthe left (that is, for variable assumptions) is different than that used on the right. On the left we \nuse an assumption judgement with no store type parameter: assm/tm : tm -> cn -> type. %% x :C assm/md \n: md -> sg -> type. %% s :s The assumption judgements are tied to the typing judgements by hypothesis \nrules: oftp/var : oftp HT TT E C <-assm/tm E C. ofsg/var : ofsg HT TT pty/p M S <-assm/md M S.  Note \nthat the store type in these rules are unconstrained. This is sound because it is an invariant of the \ntype system that whenever a value is substituted for a variable, that value is well-typed relative to \nthe current store. This invariant is typical of type systems for ML-like languages [11, 30], but rarely \nis it considered explicitly. 5.2 Twinned Variables A key technical point in the design of the IL is \nthat the constructor and kind language may be considered independently. In particular, it makes no reference \nto the syntactic class of modules. However, it is clearly necessary for constructors to be able to refer \nto module assumptions. Harper et al. [15] resolved the apparent contradiction 10 The account of Morrisett \net al., [22] suggests that perhaps it could be, but we have not attempted to adapt their approach to \nour work. kd : type. %% kinds (K) cn : type. %% constructors (C) sg : type. %% signatures (s) tm : type. \n%% terms (e) md : type. %% modules (M) ht : type. %% heap type (.) tt : type. %% tag typing (T) st : \ntype. %% heap (H) pty : type. %% purity level (. = P or I) kd-wf : kd -> type. %% . K ofkd : cn -> kd \n-> type. %% . C:K kd-deq : kd -> kd -> type. %% . K = K. kd-sub : kd -> kd -> type. %% . K = K. cn-deq \n: cn -> cn -> kd -> type. %% . C = C. :K def fst-sg : sg -> kd -> type. %% Fst(s)=K fst-md : md -> cn \n-> type. %% . Fst(M) . C sg-wf : sg -> type. %% . s sg-deq : sg -> sg -> type. %% . s = s. sg-sub : sg \n-> sg -> type. %% . s = s. oftp :ht -> tt -> tm -> cn -> type. %% (., T) . e :C ofsg :ht -> tt -> pty \n-> md -> sg -> type. %% (., T) . M :. s oflt :st -> ht -> tt -> type %% (., T) . (H, T) : (., T) val/tm \n: tm -> type. %% e val val/md : md -> type. %% M val raises/tm : tm -> tm -> type. %% e.e . raises/md \n: md -> tm -> type. %% M.e step/tm : tm -> st -> tt -> tm-> st ->tt -> type. %% (e, (H, T)) .. (e . , \n(H. , T.)) step/md : md -> st -> tt -> md-> st ->tt -> type. %% (M, (H, T)) ..(M. , (H. , T.)) In oflt, \nthe heap type and tag typing are given once each, not repeated two and three times as in the informal \nsystem. Figure 10. Encoding in LF (Type Families) %block ofkd-block %% a :K : some {K:kd} block {a:cn} \n{da:ofkd a K}. %block oftp-block %% x :C : some {C:cn} block {x:tm} {dx:assm/tm x C}. %block ofsg-block \n%% s/as : s : some {S:sg} {K:kd} block {s:md} {ds:assm/md s S} {a:cn} {da:ofkd a K} {dfst:fst-md s a}. \nFigure 11. Encoding in LF (Context Blocks) by allowing constructors to refer directly to the module variables. \nWhen used within a constructor, a module variable s is written s c , and refers to the constructor portion \nof the module represented by s. This device was borrowed by Dreyer [5] in his type theory on which we \nbased our IL. This device poses a problem for our formalization, since we have no license in LF to take \na variable with type md and decree that it has type cn. On the other hand, to make -c into an explicit \nconstruct by adding mdToCn : md -> cn. would introduce exactly the dependence of constructors on mod\u00adules \nthat we wish to avoid.11 Another alternative is to introduce a new type for module vari\u00adables, mdVar, \nwith one injection into modules and one into con\u00adstructors. mdVarToMd : mdVar -> md. mdVarToCn : mdVar \n-> cn. We can avoid introducing a dependence of constructors on mod\u00adules by ensuring that the only objects \nof type mdVar are variables. However, such an approach (sometimes called weak higher-order abstract syntax \n[4]) sacri.ces a key advantage of LF: the ability to de.ne substitution simply as LF application. Rather, \nan explicit def\u00adinition of substitution (and requisite theorems) would be required. Instead, we decided \nthat the binding of a module variable ac\u00adtually introduces two variables: the module variable itself \nand a constructor variable representing its Fst part. We do not maintain the connection between the two \nvariables using a convention of spelling (as do Harper et al. and Dreyer), since such a spelling con\u00advention \nwould do violence to alpha-convertibility, and is therefore incompatible with LF. Instead, as discussed \nearlier in Section 3.2.2, we explicitly use two binding occurrences to bind two distinct vari\u00adables, \neach one of which may be freely alpha-varied. (Signatures, which care only about a module s constructor \nportion, actually bind only the constructor variable.) Although this design seems obvious in retrospect, \nit took the pressure of formalization in LF to lead us to it. For example, functors (module lambdas) \nare encoded using: md/lam : sg -> (cn -> sig) -> (md -> cn-> md) -> md.  The .rst argument is the functor \ns domain, the second its codomain, which can depend on the constructor portion of the ar\u00adgument, and \nthe third its body, which can depend on the argument and its constructor portion. Once introduced, we \ncan cleanly maintain the pairing between module and constructor variables using a hypothetical judgement. \nThe type fst-md M C (recall Figure 10) represents the judgement . Fst(M) . C, expressing the relationship \nbetween a module and its constructor portion. When we introduce twinned variables, we simply introduce \na fst-md hypothesis at the same time. For example, the typing rule for functors is: 11 It might be possible \nto use a typing rule to ensure that mdToCn was used only with module variables, but since Twelf s subordination \nrelation would still be affected, it is unclear how much advantage we would still receive from the separation. \nIn any case, we had a better idea. ofsg/md/lam : ofsg HT TT pty/p (md/lam S1 S2 M) (sg/pi S1 S2) <-fst-sg \nS1 K1 <-sg-wf S1 <-({s:md} assm/md s S1 {a:cn} ofkd a K1 -> fst-md s a -> ofsg HT TT Y(M sa) (S2 a)). \nWhen typing the body of the functor, in addition to introducing the twinned variables s and a, with signature \nS1 and kind K1,we also introduce an assumption of type fst-md s a, indicating that a is the constructor \nportion of s. The resulting block of assumptions added to the context while typing the body is ofsg-block,given \nin Figure 11.  6. Veri.cation with Twelf We veri.ed the IL s metatheory using the Twelf meta-logical \nframe\u00adwork [27]. We will not reprise the Twelf methodology here; unfa\u00admiliar readers are referred to \nsummaries by Crary and Sarkar [2], Harper and Licata [13], and Pfenning [26] (in increasing order of \ndetail). Once we devised the appropriate formalization of the IL in LF, the process of verifying the \nmetatheory within Twelf went smoothly for the most part. Many of the proofs are very similar to how the \nmetatheory would be developed on paper. However, several interesting issues did arise, which we discuss \nhere. 6.1 Functionality and Explicit Contexts The functionality lemma (Lemma 4.1) expressed in Twelf \ntakes the form: funct/kd-wf : ({a:cn} ofkd a K -> kd-wf (K a)) -> cn-deq C1 C2 K -> ofkd C1 K -> ofkd \nC2 K %% outputs begin here -> (kd-deq (K C1) (K C2)) -> type. %mode funct/kd-wf +D1 +D2 +D3 +D4 -D5. \n12 The dif.culty appears with the very .rst argument. This argu\u00adment refers to a derivation of kd-wf \n(K a) that is permitted to depend on some a:cn such that ofkd aK. Furthermore, it can re\u00adfer to other \nvariables residing in the context. However, note that no variables in the context can refer to the variable \na; its entire scope is the .rst argument. In other words, implicit in the theorem statement is the fact \nthat a appears last in the context. This is satisfactory for all our uses of the lemma, but it is not \na strong enough induction hypothesis for the proof to go through. Any rule that adds a new assumption \nto the context will break the invariant, and such new assumptions cannot in general be moved to before \na because our constructors are dependently kinded. Consequently, it is necessary to strengthen the induction \nhy\u00adpothesis so that that variable of interest need not appear last. To expressing the strengthened hypothesis \nin LF requires a novel de\u00advice we call explicit contexts. The idea is to formulate an alternative formalization \nof the IL in which contexts are made explicit, rather than identi.ed with the LF context as usual. More \nprecisely, we reformulate the static seman\u00adtics of constructors, kinds, and signatures; terms and modules \nhave 12 The %mode declaration speci.es the input/output behav\u00adior of the metatheorem. A + identi.es an \ninput position, while - iden\u00adti.es output positions. no bearing on the former and require no functionality \nlemma in their own right. Importantly, the alternative formalization changes only the static semantics; \nthe syntax remains unchanged. Thus, the explicit system is talking about the same objects as the original \nsys\u00adtem. For example, constructor formation is formalized in the explicit system by the judgement: eofkd \n: cxt -> cn -> kd -> type. %% G . C:K where contexts are formalized simply as lists of assumptions: \ncxt : type. cxt/nil : cxt. %% \u00b7 cxt/cons : cxt -> cn -> kd -> cxt. %% G,a:K In the explicit system one \ncan prove functionality using a direct proof by induction, but this is not enough. We must also prove \nthat the explicit system is equivalent to the standard (implicit context) system. (The explicit system \nis clunkier to work with, so we wish to use it as little as possible. We certainly do not wish to use \nit for our entire veri.cation.) Consequently, for each judgement, we prove an implication and explication \ntheorem. For example: implicate-closed/ofkd : eofkd cxt/nil C K -> ofkd C K -> type. %mode implicate-closed/ofkd \n+D1 -D2. explicate-closed/ofkd : ofkd C K -> eofkd cxt/nil C K -> type. %mode explicate-closed/ofkd \n+D1 -D2.  Note that the explication theorem always produces a derivation using an empty explicit context. \n(This must be so, since we do not have access to the LF context.) However, we do need to use functionality \nin non-empty contexts. Therefore, the explicit context system is formulated so that it can refer to the \nimplicit (LF) context, as well as the explicit one. In practice, then, the explicit context is used for \nbindings accumulated during the proof, while the implicit context is used for pre-existing bindings. \n 6.2 Evaluation Contexts In the course of proving preservation for modules, we need to establish a lemma \nregarding beta reduction of module pairs: Lemma 6.1. 1. If G; F . p1.M1,M2. :. s1 then G; F . M1 :. s1. \n 2. If G; F . p2.M1,M2. :. s2 then G; F . M2 :. s2.  The proof of this lemma is complicated by an extensionality \nrule for modules (recall from Section 3.1.1 and Figure 2 a similar rule for constructors): G; F . p1M \n:P s1 G; F . p2M :P s2 G; F . M :P s1 \u00d7 s2 This rule makes it possible for the module in question to \ngrow by the addition of projections as we consider smaller derivations. Consequently, it is necessary \nto strengthen the induction hypothe\u00adsis: Lemma 6.2. Let evaluation contexts be de.ned by: . ::= [] | \np1. | p2. Then: 1. If G; F . .[p1.M1,M2.]:. s,then G; F . .[M1]:. s. 2. If G; F . .[p2.M1,M2.]:. s,then \nG; F . .[M2]:. s.  3. If G; F ..M1,M2.:. s, then there exists s1 and s2 such that G; F .C1 :. s1, and \nG; F .C2 :. s2, and G .s1 \u00d7s2 =s. Our represention of evaluation contexts in LF is based on the observation \nthat an evaluation context is simply an LF function of type md ->md. We then used a judgement to restrict \nattention to the members of that type that actually represent evaluation con\u00adtexts: psi-md : (md -> md) \n-> type. psi-md/eps : psi-md ([s] s). psi-md/pj1 : psi-md ([s] md/pj1 (F s)) <- psi-md F. psi-md/pj2 \n: psi-md ([s] md/pj2 (F s)) <- psi-md F. With this de.nition in hand, we may state the induction hypoth\u00adesis. \nFor example, the .rst clause is: module-beta/pj1 : psi-md F -> ofsg HT TTY (F (md/pj1 (md/pair M1 M2))) \nS %% outputs begin here -> ofsg HT TTY (F M1) S -> type. %mode module-beta/pj1 +D1 +D2 -D3.  6.3 Subderivations \nIn various places in the proof we utilize subderivation lemmas in which a judgement is asserted to be \nderivable by a subderivation of an input derivation. For example, the following lemma arises when proving \ncanonical forms for modules: Lemma 6.3. If G; F . p1M :. s is derivable, then there exist Sa:s1.s2 and \n.. . . such that G; F . M :.. Sa:s1.s2 is derivable by a subderivation. This arises when, given a module \nvalue M whose signature is a sum, we wish to show that M must be a pair. We cannot prove that directly \nby induction, because the extensionality rule mentioned above makes it possible to ascribe such a signature \nto M by virtue of ascribing signatures to p1M and p2M. Using the lemma, we can show that there exists \na subderivation that gives M another sum signature, and then we can proceed by induction. Subderivation \nrequirements are expressed in Twelf using a %reduces directive [28]: subder/md/pj1 : ofsg HT TT Y (md/pj1 \nM) S %% outputs begin here -> ofsg HT TT Y M (sg/sgm S1 S2) -> pty-sub Y Y -> type. %mode subder/md/pj1 \n+D1 -D2 -D3. ... proof ... %reduces D2 < D1 (subder/md/pj1 D1 D2 ). The %reduces directive causes Twelf \nto check that the output ofsg derivation is always a subderivation of the input one. There\u00adafter, that \ninformation is used automatically by Twelf s theorem checker when it checks that inductions are valid. \n 6.4 Inversion As we discussed in Section 4.3, a crucial component of the metathe\u00adory is a collection \nof inversion lemmas for constructors that are proved using an algorithmic presentation of de.nitional \nequality. The core of the inversion arguments is the proof that the algorithm is sound and complete for \nde.nitional equality. Prior to this work, Stone and Harper [35] gave an algorithm for the singleton \nkind calculus and proved it sound and complete. Unfortunately, it was not possible to utilize that proof \nin this work, because it relied crucially on a logical relation. In Twelf, every theorem takes the form \nof a logic program ac\u00adcompanied by a mode declaration indicating which parameters are inputs and which \nare outputs. Thus, every Twelf theorem is a sim\u00adple implication from a universally quanti.ed collection \nof inputs to an existentially quanti.ed collection of outputs. In particular, it is not possible to nest \nimplications, either on the left or on the right. Since the method of logical relations requires arbitrary \nnesting of implication on both the left and the right, there is no way to express a logical relations \nargument in the Twelf meta-logic.13 Consequently, it was necessary to develop a syntactic proof of correctness \nof an algorithm for the singleton kind calculus. The proof was summarized previously in Section 4.3. \nIn the proof s formalization, we made heavy use of the explicit context technique from Section 6.1 while \nestablishing the properties of hereditary substitution [40], for reasons similar to those that motivated \nit for functionality.  7. Conclusion This work establishes a new high-water mark in the veri.cation \nof the safety of programming languages. Although many safety proofs exist for various core calculi, none \nbefore have existed for any full-scale programming language, due to the daunting complexity of such languages. \nFor the languages we actually use, we have always settled for much less: at most, a general agreement \nthat the language s core aspects have been studied carefully, and that any errors that might exist in \nthe language as a whole must be minor. Thus it is unsurprising that numerous minor errors have been uncovered \nin supposedly type-safe languages. Our aim is to place Standard ML on as solid a footing as any core \ncalculus, using the techniques of elaborative semantics and mechanical veri.cation in Twelf to deal with \nthe complexities of a full-scale programming language. In this work we mechanize the proof of type safety \nof an internal language with equivalent expressive power to Standard ML. It remains, in our ongoing work, \nto de.ne elaboration of Standard ML into this internal language, along the lines suggested by Harper \nand Stone [16].  Acknowledgments Thanks to Michael Ashley-Rollman and Susmit Sarkar for their help with \nmechanization of the Harper-Stone IL, to Derek Dreyer for advice on the design of the internal language, \nand to Kevin Watkins for suggesting applying hereditary substitutions to the metatheory of singleton \nkinds. References [1] H.P.Barendregt. The Lambda Calculus: Its Syntax and Semantics. Elsevier, 1984. \n[2] Karl Crary and Susmit Sarkar. Foundational certi.ed code in a metalogical framework. In Nineteenth \nInternational Conference on Automated Deduction, Miami, Florida, 2003. Extended version published as \nCMU technical report CMU-CS-03-108. 13Sarnat and Sch\u00a8urmann [33] have suggested that some logical relations \nargument can be formulated by Twelf by re.ecting the logical relation into the object language, but their \ntechnique does not appear to generalize to Stone and Harper s proof. [3] Luis Damas and Robin Milner. \nPrincipal type-schemes for functional programs. In Ninth ACM Symposium on Principles of Programming Languages, \npages 207 212, Albuquerque, New Mexico, January 1982. [4] Jo\u00a8elle Despeyroux, Amy Felty, and Andr\u00b4e Hirschowitz. \nHigher-order syntax in Coq. In 1995 International Conference on Typed Lambda Calculi and Applications, \nvolume 905 of Lecture Notes in Computer Science, Edinburgh, April 1995. Springer-Verlag. [5] Derek Dreyer. \nUnderstanding and Evolving the ML Module System. PhD thesis, Carnegie Mellon University, School of Computer \nScience, Pittsburgh, Pennsylvania, May 2005. [6] Derek Dreyer, Karl Crary, and Robert Harper. A type \nsystem for higher-order modules. In Thirtieth ACM Symposium on Principles of Programming Languages, pages \n236 249, New Orleans, Louisiana, January 2003. [7] Derek Dreyer, Robert Harper, Manuel M.T. Chakravarty, \nand Gabriele Keller. Modular type classes. Technical Report TR-2006-03, University of Chicago, April \n2006. [8] Sophia Drossopoulou and Susan Eisenbach. Towards an operational semantics and proof of type \nsoundness for Java. In Formal Syntax and Semantics of Java. Springer-Verlag, March 1998. [9] Sophia Drossopoulou, \nTanya Valkevych, and Susan Eisenbach. Java type soundness revisited, September 2000. Technical report, \nImperial College London. [10] Michael J. C. Gordon and Tom F. Melham. Introduction to HOL: A Theorem \nProving Environment for Higher-Order Logic. Cambridge University Press, 1993. [11] Robert Harper. A simpli.ed \naccount of polymorphic references. Information Processing Letters, 51(4):201 206, 1994. Follow-up note \nin Information Processing Letters, 57(1), 1996. [12] Robert Harper, Furio Honsell, and Gordon Plotkin. \nA framework for de.ning logics. Journal of the ACM, 40(1):143 184, January 1993. [13] Robert Harper and \nDaniel R. Licata. Mechanizing language de.nitions. Submitted for publication, April 2006. [14] Robert \nHarper and Mark Lillibridge. A type-theoretic approach to higher-order modules with sharing. In Twenty-First \nACM Symposium on Principles of Programming Languages, pages 123 137, Portland, Oregon, January 1994. \n[15] Robert Harper, John C. Mitchell, and Eugenio Moggi. Higher-order modules and the phase distinction. \nIn Seventeenth ACM Symposium on Principles of Programming Languages, pages 341 354, San Francisco, January \n1990. [16] Robert Harper and Chris Stone. A type-theoretic interpretation of Standard ML. In Proof, Language \nand Interaction: Essays in Honour of Robin Milner. The MIT Press, 2000. Extended version published as \nCMU technical report CMU-CS-97-147. [17] Gerwin Klein and Tobias Nipkow. A machine-checked model for \na Java-like language, virtual machine and compiler. Technical Report 0400001T.1, National ICT Australia, \nSydney, March 2004. [18] Daniel K. Lee, Karl Crary, and Robert Harper. Mechanizing the metatheory of \nStandard ML. Technical Report CMU-CS-06-138, Carnegie Mellon University, School of Computer Science, \n2006. [19] Xavier Leroy. Manifest types, modules and separate compilation. In Twenty-First ACM Symposium \non Principles of Programming Languages, pages 109 122, Portland, Oregon, January 1994. [20] Xavier Leroy. \nApplicative functors and fully transparent higher\u00adorder modules. In Twenty-Second ACM Symposium on Principles \nof Programming Languages, San Francisco, January 1995. [21] Robin Milner, Mads Tofte, Robert Harper, \nand David MacQueen. The De.nition of Standard ML (Revised). The MIT Press, Cambridge, Massachusetts, \n1997. [22] Greg Morrisett, Matthias Felleisen, and Robert Harper. Abstract models of memory management. \nIn Conference on Functional Programming Languages and Computer Architecture, pages 66 77, La Jolla, California, \nJune 1995. [23] Lawrence C. Paulson. The foundation of a generic theorem prover. Journal of Automated \nReasoning, 5:363 397, 1989. [24] Leaf Petersen. Certfying Compilation for Standard ML in a Type Analysis \nFramework. PhD thesis, Carnegie Mellon University, School of Computer Science, Pittsburgh, Pennsylvania, \n2005. [25] Leaf Petersen, Perry Cheng, Robert Harper, and Chris Stone. Implementing the TILT internal \nlanguage. Technical Report CMU-CS-00-180, Carnegie Mellon University, School of Computer Science, December \n2000. [26] Frank Pfenning. Computation and deduction. Lecture notes, available electronically at http://www.cs.cmu.edu/~twelf. \n[27] Frank Pfenning and Carsten Sch\u00a8urmann. Twelf User s Guide, Version 1.4, 2002. Available electronically \nat http://www.cs.cmu.edu/~twelf. [28] Brigitte Pientka and Frank Pfenning. Termination and reduction \nchecking in the logical framework. In Workshop of Automation of Proofs by Mathematical Induction, June \n2000. [29] Benjamin Pierce and Martin Steffen. Higher-order subtyping. Theoretical Computer Science, \n176(1 2):235 282, 1997. [30] Benjamin C. Pierce. Types and Programming Languages, chapter 13. The MIT \nPress, 2002. [31] Gordon D. Plotkin. A structural approach to operational semantics. Technical Report \nDAIMI FN-19, University of Aarhus, 1981. [32] John C. Reynolds. The essence of Algol. In J. W. de Bakker \nand J. C. van Vliet, editors, Proceedings of the International Symposium on Algorithmic Languages, pages \n345 372, Amsterdam, October 1981. North-Holland. [33] Jeffrey Sarnat and Carsten Sch\u00a8urmann. A proof-theoretic \naccount of logical relations. Submitted for publication, 2006. [34] Christopher A. Stone. Singleton Kinds \nand Singleton Types.PhD thesis, Carnegie Mellon University, School of Computer Science, Pittsburgh, Pennsylvania, \nAugust 2000. [35] Christopher A. Stone and Robert Harper. Extensional equivalence and singleton types. \nACM Transactions on Computational Logic, 2006? To appear. An earlier version appeared in the 2000 Symposium \non Principles of Programming Languages. [36] Donald Syme. Reasoning with the formal de.nition of Standard \nML in HOL. In Sixth International Workshop on Higher Order Logic Theorem Proving and its Applications, \npages 43 60, Vancouver, August 1993. [37] D. Tarditi, G. Morrisett, P. Cheng, C. Stone, R. Harper, and \nP. Lee. TIL: A type-directed optimizing compiler for ML. In 1996 SIGPLAN Conference on Programming Language \nDesign and Implementation, pages 181 192, May 1996. [38] Myra VanInwegen. The Machine-Assisted Proof \nof Programming Language Properties. PhD thesis, University of Pennsylvania, Philadelphia, Pennsylvania, \nMay 1996. [39] Myra VanInwegen and Elsa Gunter. Hol-ml. In Sixth International Workshop on Higher Order \nLogic Theorem Proving and its Applica\u00adtions, pages 61 74, Vancouver, August 1993. [40] Kevin Watkins, \nIliano Cervesato, Frank Pfenning, and David Walker. A concurrent logical framework: The propositional \nfragment. In S. Berardi, M. Coppo, and F. Damiani, editors, Types for Proofs and Programs, volume 3085 \nof Lecture Notes in Computer Science, pages 355 377. Springer-Verlag, 2004. Papers from the Third International \nWorkshop on Types for Proofs and Programs, April 2003, Torino, Italy.  \n\t\t\t", "proc_id": "1190216", "abstract": "We present an internal language with equivalent expressive power to Standard ML, and discuss its formalization in LF and the machine-checked verification of its type safety in Twelf. The internal language is intended to serve as the target of elaboration in an elaborative semantics for Standard ML in the style of Harper and Stone. Therefore, it includes all the programming mechanisms necessary to implement Standard ML, including translucent modules, abstraction, polymorphism, higher kinds, references, exceptions, recursive types, and recursive functions. Our successful formalization of the proof involved a careful interplay between the precise formulations of the various mechanisms, and required the invention of new representation and proof techniques of general interest.", "authors": [{"name": "Daniel K. Lee", "author_profile_id": "81541415356", "affiliation": "Carnegie Mellon University", "person_id": "PP24044555", "email_address": "", "orcid_id": ""}, {"name": "Karl Crary", "author_profile_id": "81100253026", "affiliation": "Carnegie Mellon University", "person_id": "P157139", "email_address": "", "orcid_id": ""}, {"name": "Robert Harper", "author_profile_id": "81100140064", "affiliation": "Carnegie Mellon University", "person_id": "PP39029368", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1190216.1190245", "year": "2007", "article_id": "1190245", "conference": "POPL", "title": "Towards a mechanized metatheory of standard ML", "url": "http://dl.acm.org/citation.cfm?id=1190245"}