{"article_publication_date": "01-17-2007", "fulltext": "\n Variance Analyses from Invariance Analyses Josh Berdine Aziem Chawdhary Byron Cook Microsoft Research \nQueen Mary, University of London Microsoft Research jjb@microsoft.com aziem@dcs.qmul.ac.uk bycook@microsoft.com \n Dino Distefano Queen Mary, University of London ddino@dcs.qmul.ac.uk Abstract An invariance assertion \nfor a program location e is a statement that always holds at e during execution of the program. Program \ninvari\u00adance analyses infer invariance assertions that can be useful when trying to prove safety properties. \nWe use the term variance asser\u00adtion to mean a statement that holds between any state at e and any previous \nstate that was also at e. This paper is concerned with the development of analyses for variance assertions \nand their applica\u00adtion to proving termination and liveness properties. We describe a method of constructing \nprogram variance analyses from invari\u00adance analyses. If we change the underlying invariance analysis, \nwe get a different variance analysis. We describe several applications of the method, including variance \nanalyses using linear arithmetic and shape analysis. Using experimental results we demonstrate that these \nvariance analyses give rise to a new breed of termination provers which are competitive with and sometimes \nbetter than to\u00adday s state-of-the-art termination provers. Categories and Subject Descriptors D.2.4 [Software \nEngineer\u00ading]: Software/Program Veri.cation; F.3.1 [Logics and Meanings of Programs]: Specifying and \nVerifying and Reasoning about Pro\u00adgrams General Terms Veri.cation, Reliability, Languages Keywords Formal \nVeri.cation, Software Model Checking, Pro\u00adgram Analysis, Liveness, Termination 1. Introduction An invariance \nanalysis takes in a program as input and infers a set of possibly disjunctive invariance assertions (a.k.a., \ninvariants) that is indexed by program locations. Each location e in the program has an invariant that \nalways holds during any execution at e.These invariants can serve many purposes. They might be used directly \nto prove safety properties of programs. Or they might be used in\u00addirectly, for example, to aid the construction \nof abstract transition relations during symbolic software model checking [29]. If a de\u00adsired safety property \nis not directly provable from a given invariant, Permission to make digital or hard copies of all or \npart of this work for personal or classroom use is granted without fee provided that copies are not made \nor distributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. POPL 07 January 17 19, 2007, Nice, France. Copyright c &#38;#169; \n2007 ACM 1-59593-575-4/07/0001. . . $5.00. Peter O Hearn Queen Mary, University of London ohearn@dcs.qmul.ac.uk \nthe user (or algorithm calling the invariance analysis) might try to re.ne the abstraction. For example, \nif the tool is based on abstract interpretation they may choose to improve the abstraction by delay\u00ading \nthe widening operation [28], using dynamic partitioning [33], employing a different abstract domain, \netc. The aim of this paper is to develop an analogous set of tools for program termination and liveness: \nwe introduce a class of tools called variance analyses which infer assertions, called variance assertions, \nthat hold between any state at a location e and any previous state that was also at location e. Note \nthat a single variance assertion may itself be a disjunction. We present a generic method of constructing \nvariance analyses from invariance analyses. For each invariance analysis, we can construct what we call \nits induced variance analysis. This paper also introduces a condition on variance assertions called the \nlocal termination predicate. In this work, we show how the variance assertions inferred during our analysis \ncan be used to establish local termination predicates. If this predicate can be es\u00adtablished for each \nvariance assertion inferred for a program, whole program termination has been proved; the correctness \nof this step relies on a result from [37] on disjunctively well-founded over\u00adapproximations. Analogously \nto invariance analysis, even if the in\u00adduced variance analysis fails to prove whole program termination, \nit can still produce useful information. If the predicate can be estab\u00adlished only for some subset of \nthe variance assertions, this induces a different liveness property that holds of the program. Moreover, \nthe information inferred can be used by other termination provers basedondisjunctivewell-foundedness,suchas \nTERMINATOR [14]. If the underlying invariance analysis is based on abstract interpre\u00adtation, the user \nor algorithm could use the same abstraction re.ne\u00adment techniques that are available for invariance analyses. \nIn this paper we illustrate the utility of our approach with three induced variance analyses. We construct \na variance analysis for arithmetic programs based on the Octagon abstract domain [34]. The invariance \nanalysis used as input to our algorithm is composed of a standard analysis based on Octagon, and a post-analysis \nphase that recovers some disjunctive information. This gives rise to a fast and yet surprisingly accurate \ntermination prover. We similarly con\u00adstruct an induced variance analysis based on the domain of Polyhe\u00addra \n[23]. Finally, we show that an induced variance analysis based on the separation domain [24] is an improvement \non a termination prover that was recently described in the literature [3]. These three abstract domains \nwere chosen because of their relative position on the spectrum of domains: Octagon is designed to be \nextremely fast, at the expense of accuracy, whereas Polyhedra and the separation domain are more powerful \nat the cost of speed. 01 VARIANCEANALYSIS(P, L, I.){ 02 IAs := INVARIANCEANALYSIS(P, I.) 03 foreach i \n.L { 04 LTPreds[i]:= true 05 O := ISOLATE(P, L, i) 06 foreach q .IAs such that pc(q)=i { 07 VAs := INVARIANCEANALYSIS(O, \nSTEP(O, {SEED(q)})) 08 foreach r .VAs { 09 if pc(r)=i .\u00acWELLFOUNDED(r){ 10 LTPreds[i]:= false 11 } 12 \n} 13 } 14 } 15 return LTPreds 16 } Figure 1. Parameterized variance analysis algorithm. P is the program \nto be analyzed, the set of program locations L is a set cutpoints, and I. is the set of initial states. \nTo instanti\u00adate the variance analysis one must .x the implementations of INVARIANCEANALYSIS,STEP,SEED \nand WELLFOUNDED. In their own right each of these induced variance analyses is on the leading edge in \nthe area of automatic termination proving. For example, in some cases the Octagon-based tool is the fastest \nknown termination prover. But the more important point is that these variance analyses are not specially-designed: \ntheir power is determined almost exclusively by the power of the underlying invariance analysis.  2. \nInducing invariance analyses In this section we informally introduce the basic ideas behind our method. \nLater, in Sections 3 and 4, we will formally de.ne the components in the algorithm, and prove its soundness. \nFig. 1 contains our analysis algorithm. To instantiate the analy\u00adsis to a particular domain, we must \nprovide implementations for the following components: INVARIANCEANALYSIS: The underlying invariance \nanalysis.  STEP: A single-step function over INVARIANCEANALYSIS s abstract domain.  SEED: An additional \noperation on elements of the abstract do\u00admain (De.nition 15 in Section 4).  WELLFOUNDED: An additional \noperation on elements of the abstract domain (De.nition 13 in Section 4).  The implementations of INVARIANCEANALYSIS \nand STEP are given by the underlying invariance analysis, whereas the imple\u00admentations of SEED and WELLFOUNDED \nmust usually be de.ned (though they are not dif.cult to do so in practice). When instantiated with the \nimplementations of SEED,WELL-FOUNDED, etc. this algorithm performs the following steps: 1. It .rst runs \nthe invariance analysis, computing a set of invari\u00adance assertions, IAs. 2. Each element q (from IAs) \nis converted into a binary relation via the SEED operation. 3. The algorithm then re-runs the invariance \nanalysis from the seeded state after a single step of execution to compute a .xed point over variance \nassertions, VAs . That is, during this step the invariance analysis computes an approximation (represented \nas a binary relation on states) of the behavior of the loop.  4. The analysis then takes each element \nof VAs and uses the WELLFOUNDED operation in order to establish the validity of a set of local termination \npredicates, stored in an array LTPreds . A location e s local termination predicate holds if LTPreds \n[e]= true. The reason we take a single step before re-running the invariance calculation is that we are \ngoing to leverage the result of [37] on disjunctive well-foundedness, which connects well-foundedness \nof a relation to over-approximation of its non-re.exive transitive clo\u00adsure. Without STEP we would get \nthe re.exive transitive closure instead. In general, VAs , IAs and I. in this algorithm might be (.nite) \nsets of abstract elements, rather than singletons. We regard these sets as disjunctions and, in particular, \nif a variance assertion at e is the disjunction of multiple elements of VAs ,then e s local termination \nlemma holds only in the case that WELLFOUNDED returns true for each disjunct. Although we regard each \nset as a disjunction, we are not insist\u00ading that our abstract domains are closed under disjunctive comple\u00adtion \n[19]. INVARIANCEANALYSIS might even return just a single abstract element, or it might return several \nwithout computing the entire disjunctive completion; we might employ techniques such as in [33, 41] to \nef.ciently approximate disjunctive completion. But, the decision of how much disjunction is present is \nrepresented in the inputs STEP and INVARIANCEANALYSIS, and is not part of the VARIANCEANALYSIS algorithm. \nFor our experiments with numerical domains, we .tted them with a post-analysis to extract disjunctive \ninformation from oth\u00aderwise conjunctive domains. That is, the invariance analyses used by the VARIANCEANALYSIS \nalgorithm are composed of the stan\u00addard numerical domain analysis together with a method of disjunc\u00adtion \nextraction. On the other hand, for our shape analysis instantia\u00adtion no pre-.tting is required because \nthe abstract domain explicitly uses disjunction (Section 6). 2.1 Illustrative example Consider the small \nprogram fragment in Fig. 2, where nondet() represents non-deterministic choice. In this section we will \nuse this program while stepping through the VARIANCEANALYSIS algo\u00adrithm. We will assume that our underlying \ninvariance analysis is based on the Octagon domain, which can express conjunctions of inequalities of \nthe form \u00b1x + \u00b1y = c for variables x and y and constant c. Note that during this example we will associate \ninvariance as\u00adsertions and variance assertions with line numbers. We will say that an assertion holds \nat line e if and only if it is always valid at the be\u00adginning of the line, before executing the code \ncontained at that line. Furthermore, we will choose a set of program location cutpoints to be the .rst \nbasic block of a loop s body: L = {82, 83, 85}. Location 82 is the cutpoint for the loop contained in \nlines 81 91, location 83 is the cutpoint for the loop contained in lines 82 90, and 85 is the cutpoint \nfor the loop within lines 84 86. Given L, our parameterized variance analysis attempts to estab\u00adlish \nthe validity of a local termination predicate for each location e . L, when the program P is run from \nstarting states satisfying input condition I.. Note that while the outermost loop in Fig. 2 does not \nguarantee termination, so long as execution remains within the loop starting at location 82, it is not \npossible for the loop in lines 82 90 to visit location 83 in.nitely often. In this example we will show \nhow VARIANCEANALYSIS is able to prove a more local property at location 83: 81 while (nondet()) { 82 \nwhile (x>a &#38;&#38; y>b) { 83 if (nondet()) { 84 do { 85 x= x\u00ad1; 86 } while (x>10); 87 } else { 88 \ny= y\u00ad1; 89 } 90 } 91 } Figure 2. Example program fragment. LT (P, L, 83,I ): Line 83is visited in.nitely \noften only in the case that the program s execution exits the loop contained in lines 82to 90in.nitely \noften. The formal de.nition of LT (P, L, e, I ), the local termination predicate at e, will be given \nlater (De.nition 8 in Section 3). Although we will not do so in this example, VARIANCEANALY-SIS would \nalso attempt to establish local termination predicates for the remaining cutpoints: LT (P, L, 82,I ): \nLine 82is visited in.nitely often only in the case that the program s execution exits the loop contained \nin lines 81to 91in.nitely often. LT (P, L, 85,I ): Line 85is visited in.nitely often only in the case \nthat the program s execution exits the loop contained in lines 84to 86in.nitely often. Because the outer \nloop is not terminating, VARIANCEANALYSIS would fail to prove LT (P, L, 82,I ).As for 85, it would succeed \nto prove LT (P, L, 85,I ). We are using a program with nested loops here to illustrate the modularity \nafforded by our local termination predicates: even if the inner loops and outer context are diverging, \nthis will not stop us from proving the local termination predicate at location 83.Thatis to say: the \ntermination of the innermost loop beginning at line 84 does not affect our predicate. We could replace \nline 86 86 } while (x>10); with 86 } while (nondet()); and still establish LT (P, L, 83,I ).However, \nLT (P, L, 85,I ) would not hold in this case. Invariance analysis (Line 2 of Fig. 1). We start by running \nan invariance analysis using the Octagon domain (possibly with a disjunction-recovering post-analysis). \nIn this example, if we had the text of the entire program, we could start with an initial state of I \n=(pc =0). Note that we will assume that the program counter is represented with an additional equality \npc = c in each abstract program state where c is a numerical constant. Instead of starting at location \n0, assume that at location 81we have I =(pc =81.x = a +1. y = b +1). From this starting state the invariance \nanalysis could compute an invariant IA83 . IAs: IA83 . pc =83. x = a +1. y = b +1 An abstract state, \nof course, denotes a set of concrete states. IA83, for example, represents the set of states: {s | s(pc)=83. \ns(x)= s(a)+1. s(y)= s(b)+1} Isolation (Line 5 of Fig. 1). The next thing we do, for location 83, is isolate \nthe smallest strongly-connected subgraph of P s control-.ow graph containing location 83, subject to \nsome con\u00additions involving the set of locations L = {82, 83, 85},de.ned formally in Section 3. Concretely, \nfrom the overall program P we construct a new program O, which is the same as P with the ex\u00adception that \nthe statement at line 90is now: 90 }; assume(false); Because of this assume statement, executions that \nexit the loop are not considered. Furthermore, pc in the isolated program s initial state will be 83. \nTogether, these two changes restrict execution to stay within the loop. This isolation step gives us \nmodularity for analyzing inner loops. It allows us to establish a local termination predicate for O even \nwhen it is nested within another loop P that diverges. Con\u00adcretely, isolation will eliminate executions \nwhich exit or enter the loop. Inferring variance assertions (Lines 6 and 7 of Fig. 1). From this point \non we will use our invariance analysis to reason about the isolated subprogram rather than the original \nloop. Let -.O denote the transition relation for the isolated subprogram O.We thentake all of the disjuncts \nin the invariance assertion at location 83(in this case there is only one, IA83) and convert them into \nbinary relations from states to states: SEED(IA83)=(pc =83. pcs =83. x = a +1. y = b +1 . xs =x . ys \n=y . as =a . bs =b) SEED(IA83)is, of course, just a state that references variables not used in the program \nthese variables can be thought of as logical constants. However, in another sense, SEED(IA83)can be thought \nof as a binary relation on program states: {(s, t) | s(pc)=t(pc)=83 . s(x)=t(x) . s(y)=t(y) . s(a)=t(a) \n. s(b)=t(b) . t(x)= t(a)+1 . t(y)= t(b)+1 } Notice that we re using xs to represent the value of x in \ns,and x to represent the value of x in t.That is, s gives values to the variables {pcs,xs,ys,as,bs} while \nt gives values to {pc,x,y,a, b}. We call this operation seeding because it plants a starting (di\u00adagonal) \nrelation in the abstract state. Later in the algorithm this relation will grow into one which indicates \nhow progress is made. We then step the program once from SEED(IA83) with STEP, approximating one step \nof the program s semantics, giving us: pcs =83. pc =84. x = a +1. y = b +1 . xs =x . ys =y . as =a . \nbs =b Finally, we run INVARIANCEANALYSIS again with this new state as the starting state, and the isolated \nsubprogram O as the program, which gives us a set of invariants at locations 82, 83,etc. that corresponds \nto the set VAs in the VARIANCEANALYSIS algorithm of Fig. 1. VAA 83 . (pcs =83.pc =83.x =a +1.y =b +1. \nxs =x +1.ys =y .as =a .bs =b) VAB 83 . (pcs =83.pc =83.x =a +1.y =b +1. xs =x .ys =y +1.as =a .bs =b) \nVAC 83 . (pcs =83.pc =83.x =a -1.y =b +1. xs =x +1.ys =y +1.as =a .bs =b) ABC {VA83, VA83, VA83}.VAs \nThe union of these three relations ABC VA83 .VA83 .VA83 forms the variance assertion for line 83in P \n, which is to say a su\u00adperset of the possible transitions from states at 83to states also at line 83 \nreachable in 1 or more steps of the program s execution. (Note that in this case INVARIANCEANALYSIS is \nextracting dis\u00adjunctive information implicit in the .xed point computed.) The dis\u00adjunction VAA 83 .VAC \n83 .VAB 83 is a superset of the transitive closure of the program s transition relation restricted to \npairs of reachable states both at location 83. One important aspect of this technique is that the analysis \nis not aware of our intended meaning of variables like xs and ys:it simply treats them as symbolic constants. \nIt does not know that the states are representing relations. (See De.nition 12 and the further remarks \non relational analyses at the end of Section 4.) However, as it was for SEED(IA83), it is appropriate \nfor us to interpret the meaning of VAA 83 as a relation on pairs of states. The variance assertion VAA \n83 .VAC 83 .VAB 83 shows us different ways in which the subprogram can make progress. Because VAA 83 \n. VAB 83 is a variance assertion, this measure of progress holds 83 .VAC between any two states s and \nt at location 83where s is reachable and t is reachable in 1 or more steps from s. Notice that VAA 83 \ncontains an inequality between x and xs, whereas SEED(IA83) contained an equality. This means that, in \nthe .rst of the three disjuncts in the variance assertion at line 83, x is at least 1 less than xs: In \nits relational meaning, because it is a variance assertion the formula says in the current state, x is \nless than it was before . Finally, when we run the analysis again on subprogram O the inner loop containing \nlocation 85must be analyzed. Literally, then, to determine the local termination property for locations \n83 and 85involves some repetition of work. However, if we analyzed an inner loop .rst an optimization \nwould be to construct a summary, which we could reuse when analyzing an outer loop. The exact form of \nthese summaries is delicate, and we won t consider them explicitly in this paper. But, we remark that \nthe summary would not have to show the inner loop terminating: When an inner loop fails to terminate \nthis does not stop the local termination predicate from holding for the outer loop, as the example in \nthis section demonstrates. Proving local termination predicates (Lines 8 11 of Fig. 1). We now attempt \nto use the variance assertion at line 83in O to establish the local termination predicate at line 83in \nP . Consider the relation Tr 83 = {(s, t)| s |=IA83 . s -.+ O t . t(pc)=83} Showing that Tr 83 is well-founded \nallows us to conclude the local termination predicate: Location 83is not visited in.nitely often in executions \nof the isolated subprogram O. The reason is due to the over-approximation computed at line 2 in Fig. \n1. The abstract state VA83 over-approximates all of the states that can be reached at line 83, even as \nparts of ultimately divergent executions, so we now do not need to consider other states to understand \nthe behavior of this subprogram. We stress that the not visited in.nitely often property here does not \nimply in general that the isolated subprogram O termi\u00adnates. In the example of this section the inner \nloop does terminate, but a trivial example otherwise is 1 while (x>a) { 2 x=x-1; 3 while (nondet()) {} \n4 } Here we can show that location 2is not visited in.nitely often when the program is started in a \nstate where x>a. Continuing with the running example, due to the result of [37], a relation Rel is well \nfounded if and only if its transitive closure Rel+ is a subset of a .nite union T1 . \u00b7\u00b7\u00b7 . Tn and each \nrela\u00adtion Ti is well-founded. Notice that we have computed such a .nite set: VAA 83,and VAC 83, VAB 83. \nWe know that the union of these three relations over-approximates the transitive closure of the transition \nrelation of the program P limited to states at location 83.Further\u00admore, each of the relations are, in \nfact, well-founded. Thus, we can reason that the program will not visit location 83 in.nitely often unless \nit exits the subprogram in.nitely often. We will make this connection formal in Section 3. The last step \nto automate is proving that each of the relations VAA 83,and VAC 83, VAB 83 are well-founded. Because \nthese relations are represented as a conjunction of linear inequalities, they can be automatically proved \nwell-founded by rank function synthesis engines such as RANKFINDER [36] or POLYRANK [6, 7]. Bene.ts \nof the approach The technique above is fast. Using the Octagon-based program invariance analysis packaged \nwith [34] together with RANK-FINDER, this example is proved terminating in 0.07 seconds. TERMINATOR,incontrast, \nrequires 8.3seconds to establish the same local termination predicate.  Like TERMINATOR [14] or POLYRANK \n[6, 7], the technique is completely automatic. No ranking functions need to be given by the user. Simply \nchecking termination arguments is easy, and has been done automatically since the 1970s. In contrast, \nboth automatically .nding and checking termination arguments for programs is a much more recent step. \nThis will be discussed further in Section 7.  As in TERMINATOR, the technique that we have described \nmakes many little well-foundedness checks instead of one big one. If we can .nd the right decomposition, \nthis makes for a strong termination analysis. In the proposed technique, we let the invariance generator \nchoose a decomposition for us (e.g.  VAA 83, VAC 83, VAB 83). Furthermore, we let the invariance engine \napproximate all of the choices that a program could make dur\u00ading execution with a .nite set of relations. \nAs is true in TERMINATOR, because this analysis uses a dis\u00adjunctive termination argument rather than \na single ranking func\u00adtion, our termination argument can be expressed in a simpler domain. In our setting \nthis allows us to use domains such as Oc\u00adtagon [34] which is one of the most ef.cient and well-behaved \nnumerical abstract domains. For example, consider a traditional ranking function for the loop contained \nin lines 82 90: f(s)=s(x)+s(y) Checking termination in the traditional way requires support for four-variable \ninequalities in the termination prover, as we must prove R . Tf ,where R is the loop s transition relation \nand Tf = {(s, t) | f(s) = f(t) - 1 . f(t) = 0} i.e. Tf = {(s, t) | s(x)+s(y) = t(x)+t(y)-1. t(x)+t(y) \n= 0} Notice the four-variable inequality (where s((x) and t((x) will be treated with different arithmetic \nvariables): s(x)+ s(y) = t(x)+ t(y) - 1 Thus, we cannot use the Octagon domain in this setting. We can \nin our setting because VAA 83,and VAC 83, VAB 83 are simpler than Tf : they are all conjunctions of two-variable \ninequalities, such as x = xs but not x + y>xs. Although tools like RANKFINDER synthesize ranking func\u00adtions, \nwe do not need them we simply need a Boolean result. Thisisincontrastto TERMINATOR,whichusesthesynthesized \nranking functions to create new abstractions from counterex\u00adamples. As a consequence, any sound tool \nthat proves well\u00adfoundedness will suf.ce for our purposes.  Our technique is robust with respect to \narbitrarily nested loops, as we re simply using the standard program analysis techniques to prove relationships \nbetween visits to location 83.Even if the innermost loop did not terminate, we would still be able to \nes\u00adtablish the local termination predicate at location 83. For this reason our new analysis .ts in well \nwith termination decompo\u00adsition techniques based on cutpoints [25].  If the termination proof does not \nsucceed due to the discovery of a non-well-founded disjunct, the remaining well-founded disjuncts are \nnow in a form that can be passed to a tool like TERMINATOR TERMINATOR can then use this as a better initial \ntermination argument than its default one from which it will re.ne based on false counterexamples as \ndescribed in [12].  In contrast to TERMINATOR,VARIANCEANALYSIS seeds in a dynamic fashion. This means \nthat abstract states are seeded after some disjunction has been introduced by the invariance analysis, \nwhich can improve precision and allows us to dynam\u00adically choose which variables to include in the seeding. \nIn fact, an alternative method of approximating our core idea would be to .rst use the source-to-source \ntransformation described in [13] on the input program and then apply an invariance analysis on the resulting \nprogram. We have found, though, that taking this approach results in a loss of precision.  We do not \nneed to check that the disjunction of the variance assertions forms a transition invariant it simply \nholds by con\u00adstruction. In TERMINATOR this inclusion check is the perfor\u00admance bottleneck.   3. Concrete \nsemantics and variance assertions In this section we give a precise account of the local termination \npredicates, their relation to well-foundedness for isolated programs, and the relation to variance assertions. \nThese properties can be formulated exclusively in terms of concrete semantics. 3.1 Programs and loops \nDEFINITION 1 (Locations). We assume a .xed .nite set L of pro\u00adgram locations. DEFINITION 2 (Programs). \nA program P . P is a rooted, edge\u00adlabeled, directed graph over vertex set L. Programs are thought of \nas a form of control-.ow graphs where the edges are labeled with commands which denote rela\u00ad  81  \n 92 91 assume(\u00ac(x>a . y>b))  @ 82  @   86 83 89   y := y - 1 87 @  85 84 88 Figure 3. Graph \nrepresentation of the program from Fig. 2, where we have circled a set of cutpoints. Note that assumptions \ninvolving nondet have been elided. tions 2(DC \u00d7DC ) on program states DC .1 This formulation rep\u00adresents \nprograms in a form where all control .ow is achieved by nondeterministic jumps, and branch guards are \nrepresented with assumptions. For example, Fig. 3 shows a representation of the program from Fig. 2 in \nthis form. We use the following notation: We write P (s) to indicate that there is a directed path in \ngraph P through the ordered sequence of vertices s. We write \u00b7 for sequence concatenation. The control-.ow \ngraph structure of programs is used to de.ne the notion of a set of cutpoints [25] in the usual way. \nDEFINITION 3 (Cutpoints). For a program P ,aset L of cutpoints is a subset of L such that every (directed) \ncycle in P contains at least one element of L. 3.2 Isolation In order to formally describe the ISOLATE \nprocedure from Fig. 1, we .rst must de.ne several constructs over program control-.ow graphs. DEFINITION \n4(SCSG). For a program P and set of cutpoints L, we de.ne a set SCSG(P, L) of strongly-connected subgraphs \nof P : SCSG(P, L) .e . L mscsgd(e) where O . mscsgd(e) iff 1. O is a non-empty, strongly-connected subgraph \nof P ; 2. all vertices in O are dominated by e, where for vertices m and n, n is dominated by m iff \nP (r \u00b7 s \u00b7 n) implies m . s where r is the root vertex; 3. every cycle in P (that is, a cycle in the \ncontrol-.ow graph, not in the executions of the program) is either contained in O or contains a cutpoint \nin L that is not in O; and 4. there does not exist a strict supergraph of O that satis.es these conditions. \n 1 The invariance analysis algorithm relies (via the ISOLATE operation) on being able to identify loops \nin programs. This led us to be explicit about control-.ow graphs, rather than use the usual, syntax-free, \nformulation in terms of functions over concrete or abstract domain elements. For a well-structured program, \nand the set of cutpoints consisting of all locations just inside of loop bodies and recursive function \ncall-sites, De.nition 4 identi.es the innermost natural loop contain\u00ading e. This also handles non-well-structured \nbut reducible loops, but does not allow isolation of non-reducible subgraphs (such as loops formed by \ngotos from one branch of a conditional and back). The subgraphs of P identi.ed by SCSG(P, L) are the \nstrongly\u00adconnected components of P , plus some which are not maximal. Condition 2 limits the admitted \nnon-maximal subgraphs to only those that, intuitively, are inner loops of a strongly-connected com\u00adponent. \nCondition 3 ensures that the allowed subgraphs are not at odds with the given set of cutpoints, which \nmay force merging mul\u00adtiple loops together into one subgraph. Condition 4 ensures that the subgraph for \na loop includes its inner loops. These sorts of issues are familiar from compilation [1, 2]. Note that \nthe elements of SCSG(P, L), being a superset of the strongly-connected components of P , cover every \ncycle in (the control-.ow graph of) P . Another point to note is that two elements of SCSG(P, L) are \neither disjoint or one is a subset of the other. DEFINITION 5(LP). For a program P , set of cutpoints \nL, and location e, LP(P, L, e) is the set of vertices of the smallest element of SCSG(P, L) which contains \ne. As an example, if P is the program in Fig. 3, and L = {82, 83, 85},SCSG(P, L)= {{84..86}, {82..90}, \n{81..91}}, and we have: LP(P, L, 82) = {81..91}LP(P, L, 83) = {82..90}LP(P, L, 85) = {84..86} DEFINITION \n6(ISOLATE). For program P , set of cutpoints L, and program location e, ISOLATE(P, L, e) is the induced \nsub\u00adgraph based on LP(P, L, e). That is, the subgraph of P contain\u00ading only the edges between elements \nof LP(P, L, e). The root of ISOLATE(P, L, e) is e. Informally, ISOLATE(P, L, e) constructs a subprogram \nof P such that execution always remains within LP(P, L, e). Note that we have given mathematical speci.cations \nof, but not algorithms for computing, sets of cutpoints, SCSG,LP,etc. In practice ef.cient algorithms \nare available. 3.3 Local termination predicates We now develop the de.nition of a local termination \npredicate. To do so we must also develop notation for several fundamental concepts, such as concrete \nsemantics. DEFINITION 7 (Concrete semantics). The concrete semantics of a program is given by: aset \nDC of program states, and  a function -.(\u00b7): P . 2(DC \u00d7DC ) from programs to transition relations. \n We use a presentation where program states include program loca\u00adtions, which we express with a function \npcP : DC . L from program states to values of the program counter. The transition relations are constrained \nto only relate pairs of states for which there is a corresponding edge in the program, that is, s -.P \nt implies P (pcP (s) \u00b7 pcP (t)). When we associate a program P with a set IP . DC of initial states we \nwill require that pc(s) is the root of the control-.ow graph for each s . IP . Recall from Section 2 \nthat the local termination predicate at line 82 was informally stated as Line 83 is visited in.nitely \noften only in the case that the program s execution exits the loop contained in lines 82 to 90 in.nitely \noften. That is, the local termination predicate is a liveness property about location 83, which could \nbe expressed in linear temporal logic [35] as: () DD.pc =83 =. .pc . LP(P, L, 83) Next we formally de.ne \nthe notion of local termination predicate. DEFINITION 8 (Local termination predicate (LT )). For program \nP , cutpoint set L, program location e, and set of initial states IP , LT (P, L, e, IP ) holds if and \nonly if for any in.nite execution sequence s0,s1, ..., si, ... with s0 . IP and .i. si -.P si+1 for all \nj = 0 if pc(sk)= e for in.nitely many k>j then pc(sk' ) ./LP(P, L, e) for some k' >j. We now de.ne a \nvariant of well-foundedness (of the concrete semantics) in which the domain and range of the relation \nis spe\u00adcialized to a given program location e. DEFINITION 9(WF). For program O, program location e, and \nset of initial states IO, we say that WF(O, e, IO) holds iff for any in.nite execution sequence s0,s1, \n..., si, ... with s0 . IO and .i. si -.P si+1 there are only .nitely many j> 0 such that pc(sj)= e. The \nkey lemma is the following, which links well-foundedness for an isolated loop to the LT (P, L, e, IP \n) property. PROPOSITION 1 (Isolation). Let O = ISOLATE(P, L, e) and sup\u00adpose IP is a set of initial \nstates for program P , and  IO = {t |.s . IP .s -. * t . pc(t)= e}.  P If WF(O, e, IO) holds, then \nLT (P, L, e, IP ) holds. Proof: Removing a .nite pre.x ending just before a state at e from a counterexample \nto LT (P, L, e, IP ) yields a counterexam\u00adple to WF(O, e, IO). That is: Suppose by way of contradiction \nthat WF(O, e, IO) and that \u00acLT (P, L, e, IP ), that is, there exists an in.nite execution sequence s0,s1, \n..., si, ... with s0 . IP and .i. si -.P si+1 where there exists a j = 0 such that pc(sk)= e for in.nitely \nmany k>j and pc(sk' ) . LP(P, L, e) for all k' >j. Consider the suf.x sj' ,sj'+1, ..., sj'+i, ... of \nthe in.nite execution sequence for some j' = j such that pc(sj' )= e.Since pc(sj'+i) . LP(P, L, e) for \nall i = 0,and O = ISOLATE(P, L, e),we have an execution sequence in O that visits e in.nitely often. \nThat is, sj'+0,sj'+1, ..., sj'+i, ... with sj' . IO and .i. si -.O sj'+i+1 such that pc(sj'+k)= e for \nin.nitely many k>j. This contradicts WF(O, e, IO). Finally, if an analysis can establish the validity \nof a complete set of local termination predicates, then this is suf.cient to prove whole program termination. \nPROPOSITION 2. Let L be a set of cutpoints for P and IP be a set of initial states. If, for each e . \nL, LT (P, L, e, IP ), then there is no in.nite execution sequence starting from any state in IP . Proof: \nSuppose for each e . L, LT ( P, L, e, IP ) . Suppose by way of contradiction that there is an in.nite \nexecution sequence: s0,s1, ..., si, ... with s0 . IP and . i. si -. P si+1. Therefore at least one location \nis visited in.nitely often. Each of the in.nitely\u00adoften visited locations e has an associated LP( P, \nL, e) .Let e ' be an in.nitely-often visited location whose set of locations LP( P, L, e ' ) has cardinality \nnot smaller than that of LP( P, L, e ''' ) for any other in.nitely-often visited location e ''' . A consequence \nof the de.ni\u00adtion of LT ( P, L, e ' ,IP ) is that execution must leave, and return to, the set (of control-.ow \ngraph locations) LP( P, L, e ' ) in.nitely often. Therefore there is a cycle C . L in P which is not \ncon\u00adtained in LP( P, L, e ' ) and, by De.nition 4, contains an in.nitely\u00adoften visited cutpoint e '' \nnot in LP( P, L, e ' ) . [De.nition 4 does not directly guarantee that e '' is visited in.nitely often, \nbut since exe\u00adcution leaves and returns to LP( P, L, e ' ) in.nitely often, by a pi\u00adgeonhole argument, \nat least one of the choices of cycle C must include an in.nitely-often visited cutpoint. Without loss \nof gener\u00adality we choose e '' .] Therefore, since the elements of SCSG( P, L) cover every cycle of P \n,there must exist LP( P, L, e '' ) that con\u00adtains C.Since C is not disjoint from LP( P, L, e ' ) and \ncontains e '' ./LP( P, L, e ' ) ,LP( P, L, e ' ) . LP( P, L, e '' ) . In particular, LP( P, L, e '' ) \nis larger than LP( P, L, e ' ) . Now since it contains an in.nitely-often visited cutpoint not in LP( \nP, L, e ' ) , this contradicts the proof s assumption that LP( P, L, e ' ) is maximal. 0  4. From Invariance \nAbstraction to Termination In this section we use abstract interpretation to formally de.ne the items \nin the VARIANCEANALYSIS algorithm. We then link local termination predicates and well-foundedness for \nisolated programs to abstraction to prove soundness of VARIANCEANALYSIS. 4.1 Abstract interpretations \nWe will assume that an abstract interpretation [18, 19] of a pro\u00adgram is given by two pieces of information. \nThe .rst is an over\u00adapproximation of the individual transitions in programs, such as 2 D. 2 D. by a function \nSTEP : P .. that works on abstract states D .STEP( P, X) typically propagates each state in X for\u00adward \none step, in a way that over-approximates the concrete tran\u00adsitions of program P . The second is the \nnet effect of what one gets from the overall analysis results, which may be a function INVARIANCEANALYSIS \n: P . 2 D. . 2 D. that for program P , over-approximates the re.exive, transitive closure -. P * of the \ncon\u00adcrete transition relation of P .INVARIANCEANALYSIS is typically de.ned in terms of STEP. However \nthe details as to how they are connected is not important in this context. Widening or other meth\u00adods \nof accelerating .xed-point calculations might be used [18]. In this paper we are only concerned with \nthe net effect, rather than the way that INVARIANCEANALYSIS is obtained, and our formu\u00adlation of over-approximation \nbelow re.ects this. We do, however, presumethatSTEPand INVARIANCEANALYSIS arefunctionsfrom programs to \nabstractions. This assumption allows the local variance analysis using ISOLATE. If R is a binary relation \nthen we use IMAGE( R, X) to denote its post-image { y |. x . X. xRy} . DEFINITION 10 (Over-Approximation). \nAn over-approximation A of a concrete semantics -. (\u00b7) over concrete states DC is aset D of abstract \nstates  a function [[ \u00b7 ]] : D . 2 DC  a function pc : D . L  a function STEP(\u00b7) : P . 2 D. . 2 D. \n a function INVARIANCEANALYSIS(\u00b7) : P . 2 D. . 2 D. such that for all X . D. IMAGE( -. P , [[ X]]) . \n[[ STEP( P, X)]] and * IMAGE( -. P , [[ X]]) . [[ INVARIANCEANALYSIS( P, X)]] if s . [[ a]] then pcP \n( s)= pc ( a) . where we use the point-wise lifting of [[ \u00b7 ]] to sets of abstract states: [[ \u00b7 ]] :2 \nD. . 2 DC . We use a powerset domain due to the fact that most successful termination proofs must be \npath sensitive and thus we would like to have explicit access to disjunctive information.2 Sincewehave \nlifted the meaning function [[ \u00b7 ]] pointwise, it is distributive (preserv\u00ad ing unions) as a map from \n2 D. to 2 DC . But, we are not requiring the analysis to be a (full) disjunctive completion. In particular, \nnote that we do not require distributivity, or even monotonicity, of INVARIANCEANALYSIS or STEP; thus, \nwe can allow for acceleration methods that violate these properties [8, 20, 22]. Furthermore, we do not \nrequire that union be used at join\u00adpoints in the analysis, such as at the end of if statements; our de.nition \nis robust enough to allow an over-approximation of union to be used. We have used the powerset representation \nsimply so the result VAs in the VARIANCEANALYSIS algorithm gives us a collection of well-foundedness \nqueries, allowing us to apply the result of [37]. If the invariance analysis is not disjunctive, then \nthe VAs result set will be a singleton. In this case the variance analysis will still be sound, but will \ngive us imprecise results. Notice that this de.nition does not presume any relation be\u00adtween STEP and \nINVARIANCEANALYSIS, even though the latter is usually de.ned in terms of the former; the de.nition just \npresumes that the former over-approximates -. P , and the latter -. P * .We have just put in minimal \nconditions that are needed for the sound\u00adness of our variance analysis.3 We do not assume that INVARIANCEANALYSIS( \nP, X) always returns a .nite set, even when X is .nite. However, if the re\u00adturned set is not .nite when \nVARIANCEANALYSIS( P, L, I ) calls INVARIANCEANALYSIS( P, X) , our variance analysis algorithm will itself \nnot terminate. 4.2 Seeding, well-foundedness, and ghost state We now specify seeding (SEED) and the \nwell-foundedness check (WELLFOUNDED) usedinthe VARIANCEANALYSIS algorithm from Fig. 1. These comprise \nthe additions one must make to an existing abstract interpretation in order to get a variance analysis \nby our method. Often, these are already implicitly present in, or easily added to, an invariance analysis. \nThroughout this section we presume that we have a concrete semantics together with an over-approximation \nas de.ned above. Seeding is a commonly used technique for symbolically record\u00ading computational history \nin states. In our setting, the SEED opera\u00adtion in Fig. 1 is speci.c to the abstract domain. Therefore, \ninstead of providing a concrete de.nition, we specify properties that each instance must satisfy. As \na result, this gives signi.cant freedom to the developer of the SEED/WELLFOUNDED pair, as we will see \nin Section 6 where we de.ne the SEED/WELLFOUNDED pair for a shape analysis domain. 2 It might be possible \nto formulate a generalization of our theory without explicit powersets, using projections of certain \ndisjunctive information out of an abstract domain; we opted for the powerset representation for its simplicity. \n3 In fact, the variance analysis could be formulated more brie.y using a sin\u00adgle over-approximation of \nthe transitive closure -. +, but we have found P that separating INVARIANCEANALYSIS and STEP makes the \nconnection to standard program analysis easier to see. After seeding has been performed, the VARIANCEANALYSIS \nalgorithm proceeds to use the INVARIANCEANALYSIS to compute variance assertions over pairs of states. \nIn the following develop\u00adment we formalize the encoding and interpretation between rela\u00adtions on pairs \nof states and predicates on single states. First, we require a way to identify ghost state in the concrete \nsemantics. [In the program logic literature (e.g, see Reynolds [39]), ghost variables are speci.cation-only \nvariables that are not changed by a program. We are formulating our de.nitions at a level where we do \nnot have a description of the state in terms of variables, so we refer to ghost state, by analogy with \nghost variable.] DEFINITION 11 (Ghostly Decomposition). A ghostly decomposi\u00adtion of the concrete semantics \nis a set SC with a bijection (\u00b7, \u00b7) : SC \u00d7 SC . DC such that (g,p)-.P (g ' ,p ') implies g = g ' . \n (g1,p)-.P (g1,p ') implies (g2,p)-.P (g2,p ')  pcP (g1,p) = pcP (g2,p)  In SC \u00d7 SC we think of the \nsecond component as the real program state and the .rst as the ghost state. The .rst two conditions say \nthat ghost state does not change, and that it does not impact transitions on program state. Given a transition \nsystem it is easy to make one with a ghostly decomposition just by copying the set of states. We do not \ninsist, though, that the bijection in the de.nition be an equality because transition systems are often \ngiven in such a way that a decomposi\u00adtion is possible without explicitly using a product. Typically, \nstates are represented as functions Var . Val from variables to values, and if we can partition variables \ninto isomorphic sets of program variables and copies of them, then the basic bijection ~ (A . V ) \u00d7 (B \n. V )=(A + B . V ) can be used to obtain a ghostly decomposition. In fact, we will use this idea in all \nof the example analyses de.ned later. Given a ghostly decomposition, we obtain a relational mean\u00ading \n((a)), which is just [[\u00b7]] adjusted using the isomorphism of the decomposition. Formally, DEFINITION \n12 (Relational Semantics). For any a . D ,the re\u00adlation ((a)) . SC \u00d7 SC is ((a)) {(g, p) |(g, p). [[a]]} \nWe are using the notation (g, p) here for an element of DC cor\u00adresponding to applying the bijection of \na ghostly decomposition, reserving the notation (g, p) for the tuple in SC \u00d7 SC . Using this notion we \ncan formally de.ne the requirements for the well-foundedness check in the algorithm of Fig. 1. DEFINITION \n13 (Well-Foundedness Check). Suppose that A is an over-approximation of a program P with ghostly decomposition. \nThen a well-foundedness check is a map WELLFOUNDED : D .{true, false} such that if WELLFOUNDED(a) then \n((a)) is a well-founded rela\u00adtion. Recall that a relation R is well founded iff there does not exist \nan in.nite sequence p such that .i . N. (pi,pi+1) . R. Then a well\u00adfoundedness check must soundly ensure \nthat the relation ((a)) on program states is well founded. For our variance analysis to work properly \nit is essential that the abstract semantics work in a way that is independent of the ghost state. DEFINITION \n14 (Ghost Independence). Suppose the concrete se\u00admantics has a ghostly decomposition. We say that a . \nD is ghost independent if (g,p) .((a)) =..g ' . (g ' ,p) .((a)) i.e., if the predicate [[a]] is independent \nof the ghost state. Also, X . D is ghost independent if each element of X is. An over-approximation preserves \nghost independence if INVARIANCEANALYSIS(P, X) is ghost independent whenever X . D is ghost independent. \nThe idea here is just that the abstract semantics will ignore the ghost state and not introduce spurious \nfacts about it. Curiously, our results do not require that STEP preserves ghost independence, even though \nit typically will. Preservation of ghost independence is needed, technically, only in the statement IAs \n:= INVARIANCEANALYSIS(P, I ) in the VARIANCEANALYSIS algo\u00adrithm; for seeding to work properly we need \nthat all the elements of Q are ghost independent if all the initial abstract states in I are. The formal \nrequirement on the SEED operation, which takes independence into account, is: DEFINITION 15 (Seeding). \nA seeding function is a map SEED : D . D such that if a is ghost independent and (g,p) .((a)) then (p,p) \n. ((SEED(a))). SEED(a) can be thought of as an over-approximation of the di\u00adagonal relation on program \nvariables in a. That is, we do not re\u00adquire that SEED exactly copy the state, which would correspond \nto SEED(a)= {(p, p)} instead of (p, p) . SEED(a) in the de.nition. 4.3 Soundness To establish the soundness \nresult, we .x: a concrete semantics with ghostly decomposition;  an over-approximation that preserves \nghost independence, with a seeding map and sound well-foundedness check;  a program P and set of initial \nstates IP . DC ;  a .nite set I . D of initial abstract states, each of which is ghost independent, \nand where IP . [[I ]].  THEOREM 1 (Soundness). If VARIANCEANALYSIS(P, L, I ) of Fig. 1 terminates, for \nL a .nite set of program locations; then upon termination, LTPreds will be such that for each e . L and \ns . IP , LT (P, L, e, IP ) if LTPreds [e]= true . As an immediate consequence of Proposition 2 we obtain \nCOROLLARY 1. Suppose L is a set of cutpoints. Assume that LTPreds = VARIANCEANALYSIS(P, L, I ). In this \ncase P ter\u00adminates if .e . L. LTPreds [e]= true. Now we give the proof of the theorem. Proof: [Theorem \n1] Consider e . L and suppose LTPreds [e]= true on termination of VARIANCEANALYSIS.Let O = ISOLATE(P, \nL, e) and = {t |.s . IP .s -. * t . pc(t)= e} . IOP We aim to show that WF(O, e, IO) holds. The theorem \nfollows at once from this and Proposition 1. LetTr e { (b, c) |.(g,b). IO (g, b)-.+ (g,c). pc((g,c))= \ne} O Assume that the algorithm in Fig. 1 has terminated and that LTPreds [e]= true. First, wehavealemma: \nIf Tr e is well founded then WF(O, e, IO) holds This lemma is immediate from the transition conditions \nin De.ni\u00adtion 11. So, by the lemma we will be done if we can establish that Tr e is well founded. For \nconvenience, we de.ne: LOCS(e1,e2) {(s, t) | pc(s)= e1 . pc(t)= e2} We need to show two things: 1. Wanted: \n{((r)) n LOCS(e, e) | r . VAs } is a .nite set of well\u00adfounded relations. VAs is clearly .nite, as otherwise \nthe algorithm would not terminate. Therefore we know that there exists a .nite dis\u00adjunctively well-founded \ndecomposition of ( ((r))) n r.VAs LOCS(e, e) where, for each r . VAs ,WELLFOUNDED(r)= true. This is due \nto De.nition 13, which tells us that, for each r . VAs , ((r)) is well-founded. . 2. Wanted: Tr e . ((r)) \nn LOCS(e, e). r.VAs Assume that there exist (b, c) . Tr e. That is: we have some (g, b). IO with (g, \nb)-.+ (g, c) and pc((g, c))= e.By O over-approximation, (g, b). [[IAs]]. Thus, there exists a q . IAs \nsuch that (g, b). [[q]]. Since the start states in I are ghost independent, and INVARIANCEANALYSIS preserves \nghost independence, we obtain that q is ghost independent. We obtain from De.nition 15 that (b, b) .((SEED(q))).By \nghostly decomposition, (b, b)-.+ (b, c), and so by over- O approximation for STEP followed by INVARIANCEANALYSIS, \nthere exists r . VAs where (b, c). [[r]]. By the de.n\u00adition of ((\u00b7)) this means that (b, c) .((r)). Thus, \nbecause pc(q)= pc(r)= e, Tr e . ((r)) n LOCS(e, e). . r.VAs We can now prove that Tr e is well founded \nas follows. The two facts just shown establish that Tr e . T1 .\u00b7\u00b7\u00b7 . Tn for a .nite collection of well-founded \nrelations given by VAs (note that the union need not itself be well founded). Further, by the de.nition \nof Tr e it follows that Tr e = Tr + e . So we know Tr + e . T1 .\u00b7\u00b7\u00b7. Tn for a .nite collection of well-founded \nrelations. By the result of [37] it follows that Tr e is well founded. 0 We close this section with two \nremarks on the level of generality of our formal treatment. Remark: On Relational Abstract Domains. A \nrelational ab\u00adstract domain is one in which relationships between variables (like x<y or x = y) can be \nexpressed. Polyhedra and Octagon are classic examples of relational domains, while the Sign and Interval \ndomains are considered non-relational. The distinguishing feature of Sign and Interval is that they are \ncartesian, in the sense that the abstract domain tracks the cartesian product of properties of pro\u00adgram \nvariables ([15], p.10), independently. It has been suggested that our variance analyses might necessarily \nrely on having a rela\u00adtional (or non-cartesian) abstract domain, because in the examples above we use \nequalities to record initial values of variables. But, consider the Sign domain. For each program variable \nx the Sign domain can record whether x s value is positive, negative, or zero. If the value cannot be \nput into one of these categories, it is T. We can de.ne a seeding function, where SEED(F ) assigns to \neach seed variable xs the same abstract value as x. For example, if F is positive(x) . negative(y) then \nSEED(F ) is positive(x) . negative(y) . positive(xs) . negative(ys) This seeding function satis.es the \nrequirements of De.nition 15. The Sign domain is an almost useless termination analysis; it can prove \nwhile(x<0 &#38;&#38;k>0) x =x *x +k; but not a typical loop that increments or decrements a counter. \nWe have mentioned it only to illustrate the technical point that our de.nition of seeding does not rely \non being able to specify the equality between normal and ghost state. In this sense, our formal treatment \nis perhaps more general than might at .rst be expected. A more signi.cant illustration of this point \nwill be given in Section 6. Remark: On Relational Analyses. A relational analysis is one where an abstract \nelement over-approximates a relation between states (the transition relation) rather than a set of individual \nstates. This notion is often used in interprocedural analysis, for example in the Symbolic Relational \nSeparate Analysis of [21]. (This sense of relational is not to be confused with that in relational abstract \ndomain ; the same word is used for distinct purposes in the pro\u00adgram analysis literature.) It has been \nsuggested [17] that our use of ghost state above is a way to construct a relational analysis from a standard \none (where states are over-approximated). Indeed, it would be interesting to rework our theory using \na formulation of relational analyses on a level of generality comparable to standard abstract interpretation \n2DC \u00d7DC where, say, the meaning map had type [[\u00b7]] : D . rather than [[\u00b7]] : D . 2DC . In this sense, \nour formal treatment here is probably not as general as possible; we plan to investigate this generalization \nin the future. Among other things, such a for\u00admulation should allow us to use cleverer representations \nof (over\u00adapproximations of) relations than enabled by our use of ghost state; see [21], Section 9, for \npointers to several such representations.  5. Variance analyses for numerical domains The pieces come \ntogether in this section. By instantiating VARI-ANCEANALYSIS with several numerical abstract domains, \nwe ob\u00adtain induced variance analyses and compare them to existing termi\u00adnation proof tools on benchmarks. \nAs the results in Fig. 4 show, the induced variance analyses yield state-of-the-art termination provers. \nThe two domains used, Octagon [34] and Polyhedra [23], where chosen because they represent two disparate \npoints in the cost/precision spectrum of abstract arithmetic domains. Instances of SEED and WELLFOUNDED \nfor numerical domains. Before we begin, we must be clear about the domain of states. We presume that \nwe are given a .xed .nite set Var of variables with pc . Var.4 Concrete states are de.ned to be mappings \nfrom variables to values DC Var .V that (for simplicity of the presentation) are limited to a set of \nnumerical values V (where V could be the integers, the real numbers, etc.). The abstract states D are \nde.ned to be conjunctions of linear inequalities over V.We assume that each abstract state includes a \nunique equality pc = c for a .xed program location constant c. This gives us the way to de.ne the projection \npc required by an over-approximation (De.nition 10). Next, in order to de.ne seeding we presume that \nthe variables in Var are of two kinds, program variables and ghost variables. The former may be referenced \nin a program, while the latter can be referenced in abstract states but not in programs themselves. The \nprograms are just goto programs with assignment and conditional branching based on expressions with Boolean \ntype (represented in .ow-graph form as in De.nition 2). 4 For simplicity we are ignoring the issue of \nvariables in F that are not in\u00adscope at certain program locations. This can be handled, but at the expense \nof considerable complexity in the formulation. We assume a disjoint partitioning GVar . PVar of the set \nof variables Var where pc . PVar. We presume a bijective mapping . :PVar . GVar that associates ghost \nvariables to program variables. This furnishes the isomorphisms (GVar .V) \u00d7 (PVar .V) ~~ = (PVar .V) \n\u00d7 (PVar .V) = Var .V from which we obtain the bijection (\u00b7, \u00b7) : SC \u00d7 SC . DC required by De.nition 11 \n(where SC =PVar .V and DC = Var .V). At this point we have everything that is needed to de.ne the SEED \nand WELLFOUNDED functions: . SEED(F) F .{v = .(v)} v.PVar WELLFOUNDED(F) WFCHECK(.(PVar), PVar,F) SEED \nuses . to add equalities between ghost and program vari\u00adables. The well-foundedness check calls either \nRANKFINDER [36] or POLYRANK [6, 7], which take an input formula and then report whether or not the formula \ndenotes a well-founded relation. We will not give the explicit de.nitions of the semantics of concrete \nprograms, or of the corresponding de.nitions of INVARIANCEANALYSIS and STEP on the particular abstract \ndo\u00admains, referring instead to [23, 34]. The concrete dynamic seman\u00adtics -.P satis.es the required conditions \nof Ghostly Decompo\u00adsition (De.nition 11) because ghost variables do not appear in programs. Because these \nvariables are never modi.ed by the pro\u00adgram the functions STEP and INVARIANCEANALYSIS will pre\u00adserve \nghost independence of abstract states. Furthermore, the seeding function in this section satis.es De.n\u00adition \n15.Also, WELLFOUNDED satis.esDe.nition13asaresultof soundness of the well-foundedness checker (RANKFINDER \n[36] or POLYRANK [6, 7]). Thus we have given the de.nitions needed to obtain a speci.c variance analysis \nas an instance of the framework in the previous section. Example. Let PVar = {x, y, pc}, GVar = {xs,ys,pcs},and \n. = {(x, xs), (pc, pcs), (y, ys)} is a bijective mapping. Let s be the Octagon state x<y . pc =10. Thus, \nSEED(s)= x<y . pc =10 . x = xs . pc = pcs . y = ys If we execute the command sequence x := x +1; assume(x<y); \ngoto 10 from s, the abstract transfer function should produce a state q: q x<y . pc =10 . x = xs +1 . \npcs = pc . ys = y ((q)) is a well-founded relation because x is increasing while being less than y,and \ny is unchanging; x cannot increase forever and yet remain less than an unchanging y. Indeed, when RANKFINDER \nis passed this formula with the ghost and program variables as the from and to variables, it reports \nthat it can .nd a ranking function con.rming that ((q)) is a well-founded relation. Experiments. In order \nto evaluate the utility of our approach for arithmetic domains we have instantiated it using analyses \nbased on the Octagon and Polyhedra domains and then compared these analyses to several known termination \ntools. The tools used in the experiments are as follows: O) OCTATERM isthevarianceanalysisinducedby OCTANAL \n[34] composed with a post-analysis phase (see below). OCTANAL is included in the Octagon domain library \ndistribution. During these experiments OCTATERM was con.gured to return Ter\u00adminating in the case that \neach of the variance assertions in\u00adferred entailed their corresponding local termination predicate. The \nWFCHECK operation was based on RANKFINDER. P) POLYTERM is the variance analysis similarly induced from \nan invariance analysis POLY based on the New Polka Polyhedra library [30].5 PR) A script suggested in \n[5] that calls the tools described in the POLYRANK distribution [6, 7] with increasingly expensive command-line \noptions. T) TERMINATOR [14]. These tools, except for TERMINATOR, were all run on a 2GHz AMD64processorusingLinux2.6.16. \nTERMINATOR wasexecuted on a 3GHz Pentium 4 using Windows XP SP2. Using different ma\u00adchines is unfortunate \nbut somewhat unavoidable due to constraints on software library dependencies, etc. Note, however, that \nTER-MINATOR running on the faster machine was still slower overall, so the qualitative results are meaningful. \nIn any case, the running times are somewhat incomparable since on failed proofs TERMI-NATOR produces \na counterexample path, OCTATERM and POLY-TERM give a suspect pair of states, while POLYRANK gives no \ninformation. Also, note that the script used to call POLYRANK will never terminate for a divergent input \nprogram; the tool may quickly fail for a given set of command-line options, but the script will sim\u00adply \ntry increasingly expensive options forever. Fig. 4 contains the results from the experiments performed \nwith these provers. For example, Fig. 4(a) shows the outcome of the provers on example programs included \nin the OCTANAL distrib\u00adution. Example 3 is an abstracted version of heapsort, and Example 4 of bubblesort. \nIn this case OCTATERM is the clear winner of the tools. POLYRANK performs poorly on these cases due to \nthe fact that any fully-general translation scheme from programs with full\u00ad.edged control-.ow graphs \nto POLYRANK s input format will at times confuse the domain-speci.c rank-function search heuristics used \nin POLYRANK. Fig. 4(b) contains the results from experiments with the 4 tools on examples from the POLYRANK \ndistribution.6 The examples can be characterized as small but famously dif.cult (e.g. McCarthy s 91 function). \nWecanseethat,inthesecases,neither TERMINATOR nor theinducedproverscanbeat POLYRANK shand-craftedheuristics. \nPOLYRANK is designed to support very hard but also carefully expressed examples. In this case each of \nthese examples from the POLYRANK distribution are written such that POLYRANK s heuristics .nd a termination \nargument. Fig. 4(c) contains the results of experiments on fragments of Windows device drivers. These \nexamples are small because we cur\u00adrently must hand-translate them before applying all of the tools but \nTERMINATOR. In this case OCTATERM again beats the competi\u00adtion. However, we should keep in mind that \nthe examples from this suite that were passed to TERMINATOR contained pointer aliasing, whereas aliasing \nwas removed by hand in the translations used with POLYRANK,OCTATERM and POLYTERM. From these experiments \nwe can see that the technique of in\u00adducing variance analyses with VARIANCEANALYSIS is promising. For \nprograms of medium dif.culty (i.e. Fig. 4(a) and Fig. 4(c)), OCTATERM is many orders of magnitude faster \nthan the existing program termination tools for imperative programs.  Remark: On Octagon versus Polyhedra \nfor variance analysis. Example 1 from Fig. 4(b) demonstrates that, by moving to a more precise abstract \ndomain (i.e. moving from Octagon to Polyhedra), we get a more powerful induced variance analysis. For \nanother 5 POLY uses the same code base as OCTANAL but calls an OCaml module for interfacing with New \nPolka, provided with the OCTANAL distribution. 6 Note also that there is no benchmark number 5 in the \noriginal distribution. We have used the same numbering scheme as in the distribution so as to avoid confusion. \n O 0.11 . 0.08 . 6.03 . 1.02 . 0.16 . 0.76 . P 1.40 . 1.30 . 10.90 . 2.12 . 1.80 . 1.89 . PR 0.02 . 0.01 \n. T/O - T/O - T/O - T/O - T 6.31 . 4.93 . T/O - T/O - 33.24 . 3.98 . 1 2 3 4 5 6 (a) Results from experiments \nwith termination tools on arithmetic examples from the Octagon Library distribution. (b) Results from \nexperiments with termination tools on arithmetic examples from the POLYRANK distribution. (c) Results \nfrom experiments with termination tools on small arithmetic examples taken from Win\u00addows device drivers. \nNote that the examples are small as they must currently be hand-translated for the three tools that do \nnot accept C syntax.  1 2 3 4 6 7 8 9 10 11 12 O 0.30  0.05  0.11  0.50  0.10  0.17  0.16  0.12 \n 0.35  0.86  0.12 P 1.42 . 0.82 . 1.06  2.29  2.61  1.28  0.24  1.36 . 1.69  1.56  1.05 PR \n0.21 . 0.13 . 0.44 . 1.62 . 3.88 . 0.11 . 2.02 . 1.33 . 13.34 . 174.55 . 0.15 . T 435.23 . 61.15 . T/O \n- T/O - 75.33 . T/O - T/O - T/O - T/O - T/O - 10.31 1 2 O 1.42 . 1.67 P 4.66 . 6.35 PR T/O - T/O T 10.22 \n. 31.51  3 4 5 6 7 8 9 10 0 0.47 0 0.18 . 0.06 . 0.53 . 0.50 . 0.32 . 0.14 0 0.17 . 0 1.48 0 1.10 . \n1.30 . 1.60 . 2.65 . 1.89 . 2.42 0 1.27 . - T/O - T/O - 0.10 . T/O - T/O - T/O - T/O - 0.31 . 0 20.65 \n0 4.05 . 12.63 . 67.11 . 298.45 . 444.78 . T/O - 55.28 . Figure 4. Experiments with 4 termination provers/analyses. \nO is used to represent OCTATERM, an Octagon-based variance analysis. P is POLYTERM, a Polyhedra-based \nvariance analysis. The PR rows represent the results of POLYRANK [5]. T represents TERMINATOR [14]. Times \nare measured in seconds. The timeout threshold was set to 500s. .= a proof was found . = false counterexample \nreturned . T/O = timeout . 0= termination bug found . Note that pointers and aliasing from the device \ndriver examples were removed by a careful hand translation when passed to the tools O, P,and PR. example \nof how the Polyhedra-based variance analysis is more precise, consider the following program fragment: \nwhile(x+y>z) { if(nondet()) { x=x-1; } else { if(nondet()) { y=y-1; } else { z=z+1; } } } POLYTERM can \nprove that this program is terminating when ex\u00adecution starts in a state where both x and y are larger \nthan z,but OCTATERM reports a false bug because the Octagon domain only tracks two-variable inequalities. \nRemark: On Disjunction. As mentioned above, if the underly\u00ading abstract domain of an induced termination \nanalyzer does not support some level of disjunction, then the termination analysis re\u00adsults are likely \nto be quite imprecise. Because disjunctive comple\u00adtion is expensive (exponential) and there is no canonical \nsolution, abstract orders and widening operations must be tailored for the ap\u00adplication. For our present \nempirical evaluation we use an extraction method after the .xed-point analysis has been performed in \norder to .nd disjunctive invariance/variance assertions. The precise degree of dependence that termination \nproofs have on disjunctive comple\u00adtion, or an approximation thereof, is an important direction for fu\u00adture \nwork that we hope the existence of the VARIANCEANALYSIS algorithm will catalyze.  6. Variance analyses \nfrom shape analyses SONAR [3] is an invariance analysis tool which tracks the sizes of summarized or \nabstracted heap structures. SONAR was .rst used in the MUTANT termination prover, which implements an \nalgorithm from which that in Fig. 1 is generalized. MUTANT has been used to prove the termination of \nWindows OS device driver dispatch rou\u00adtines whose termination depends on arguments about the changing \nshape of the heap during the dispatch routine s execution. Due to isolation, SONAR s induced variance \nanalysis (i.e., the analysis re\u00adsulting from SONAR and Fig. 1) is more powerful than the original MUTANT. \nAs an example consider the following loop where we as\u00adsume, before entering into the loop, that x is \na pointer to a circular list: 1 z=x; 2 do { 3 z = z->next; 4 y=z; 5 while (y != x) { 6 y = y->next; 7} \n8 } while (z != x) SONARTERM can prove this example terminating, while MUTANT cannot. SONARTERM is an \ninteresting case of an induced variance analysis, as it demonstrates how SEED does not need to be the \nmost precise approximation of the diagonal relation, and it is also an ex\u00adampleofhow WELLFOUNDED candoadditionalabstractiononan \nalready abstract state before attempting to prove it well-founded. Elementsof SONAR sabstractdomain D \nare of the form ..S, where S is a spatial formula represented as a *-conjoined set of possibly inductive \npredicates expressed in separation logic [40], and . is a conjunction of arithmetic inequalities over \nvariables DVar that describe the number of inductive unwindings (depth) of the inductive predicates in \nS.SONAR is path-sensitive in a way that can be expressed as a control .ow based trace partitioning [33] \nwhere the analyzer dynamically computes a partition by merging partitions when the reachable states can \nstill be precisely repre\u00adsented. Alternately, this trace partitioning can be seen as a dynami\u00adcally computed \ncontrol-.ow graph elaboration ala [41]. As for the numerical domains, in order to de.ne the projection \npc required by an over-approximation (De.nition 10), we assume that the . part of each abstract state \nincludes a unique equality pc = c for a .xed program location constant c. Before presenting the details \nof the SONAR instantiation we begin with a small example. Consider an abstract state s such that ' s \nlsk(x, y) *lsk ' (y, x) .k>0 .k>0 This is an invariant of the loop at line 5 of the example above and, \ninformally speaking, s states that x is a pointer to a linked list segment such that following the trail \nof pointers in the next .elds for k steps (for some k) will lead to a node at address y. Note that, if \nwe follow the next .elds from y (for k ' steps), we will get back to the original node at x. Additionally, \ndue to the *, we know that there is no aliasing between the .rst and second lists: they occupy disjoint \nmemory. In this case SEED(s) equals: lsk(x, y) *lsk ' (y, x) .k>0 .k ' >0 .k=ks .k ' =ks ' Note that \nwe are only copying arithmetic variables, not pointers. If we symbolically execute this new state through \nthe instruction sequence y = y->next; assume(y!=x); then this could lead to the symbolic state s ' (amongst \nothers): ' ''' s lsk(x, y) *lsk ' (y, x) .ks>0 .k>0 .k=ks+1 .k =ks-1 WELLFOUNDED(s ' ) will project a \nrelation between states (ks,ks' ) and (k, k ' ) such that ' '' ks>0 .k>0 .k=ks+1 .k =ks-1 This relation \ncan be proved well-founded by both RANKFINDER and POLYRANK. Instance of SEED and WELLFOUNDED. For SONARTERM,we \nassume a partitioning GVar . PVar of the set of variables Var, and assume a set of depth variables DVar \n. PVar. We assume that pc . PVar \" DVar, and that the program neither reads from nor writes to the ghost \nvariables GVar. The set of concrete program states DC is then de.ned: GStack GVar .Val PStack PVar .Val \nStack Var .Val Heap Loc -.n Val GState GStack \u00d7Heap PState PStack \u00d7Heap DC Stack \u00d7Heap We assume a bijective \nmapping . :PVar .GVar, thus giving us an isomorphism ~~ GStack \u00d7PStack = PStack \u00d7PStack = Stack which \nthen yields isomorphisms ~~ GState \u00d7PState = PState \u00d7PState = DC to obtain (\u00b7, \u00b7) : SP \u00d7 SP . DC (where \nSP =PState), the bijection required by a Ghostly Decomposition (De.nition 11). The following four equations \nde.ne an instantiation for the operations required to induce a variance analysis from SONAR: . SEED(. \n.S) (. .S .{v = .(v)}) v.fDV(..S) SEED(T) T WELLFOUNDED(. .S) WFCHECK(.(DVar), DVar, .) WELLFOUNDED(T) \nfalse where fDV(. . S) denotes the set of depth variables appearing in . . S,and WFCHECK could be tools \nsuch as POLYRANK or RANKFINDER. The domain element T is used in SONAR to represent the case where memory-safety \ncould not be established by the abstract interpretation. Notice that these de.nitions ignore the spatial \npart S, and treat only the depth variables. In particular, WELLFOUNDED isconstantinthespatialpart,and \nSEEDplantsno information about the spatial part. In this way, SEED is not the best approximation of the \ndiagonal relation, and so is an example that exercises the looseness of De.nition 15. The bijection for \nghostly decomposition and WELLFOUNDED/ SEED operations just de.ned are the necessary additions to the \nSONAR invariance analysis to obtain the SONARTERM variance analysis. We refer to [3] for the remaining \ndetails of the SONAR analysis. 7. Related work A number of termination proof methods and tools have \nbeen re\u00adported in the literature. Examples include the size-change prin\u00adciple for purely functional programs \n(e.g. [31]), the dependency pairs approach for term-rewrite systems (e.g. [26]), rank-function synthesis \nfor imperative programs with linear arithmetic assign\u00adments (e.g. [6, 9, 10, 42, 38]) and even non-linear \nimperative pro\u00adgrams [16]. Many of these tools and techniques use speci.c and .xed abstractions. The \nsize-change principle, for example, builds an analysis around the program s call-graph. Rank-function \nsyn\u00adthesis techniques, for example, support limited control-.ow graph structures (e.g. single unnested \nwhile loops, perhaps without con\u00additionals), meaning that support for general purpose programs re\u00adquires \nan abstraction before rank synthesis can be performed. The work presented here is not tied to a .xed \nabstraction. This paper generalizes the previous work on MUTANT in [3]. Thus, given that it is a generalization, \nsome overlap in contri\u00adbutions is to be expected. SONAR s induced variance analysis, SONARTERM, is unique \nto this paper and is also more accurate than MUTANT. There, we described a speci.c termination check\u00ading \ntool, concentrating on a particular domain (shape analysis) and focusing much of our attention to the \nunderlying invariance analy\u00adsis used (SONAR). Here, we introduce the notion of variance analy\u00adsis, describe \na general method of implementing the analysis with invariance analyses, show the general algorithm at \nwork in several contexts, and give a proof of the soundness of parameterized vari\u00adance analyses. These \ncontributions are all unique to the new work. The work in this paper builds on the fundamental result \nof [37] on disjunctively well-founded relations, which shows that a relation Rel is well-founded if and \nonly if its transitive closure Rel+ is a subset of a .nite union T1 .\u00b7\u00b7\u00b7 . Tn of well-founded relations \n(called a transition invariant). The result in [37] led to a method of constructing termination arguments \nusing counterexample-guided re.nement [12]. The idea is that when an inclusion check R+ . T1 .\u00b7\u00b7\u00b7.Ti \nfails (so that we do not have an over-approximation), a counterexample can be used to produce a new well-founded \nrelation Ti+1. Then if the inclusion R+ . T1 .\u00b7\u00b7\u00b7 . Ti . Ti+1 holds, well-foundedness of R has been established; \nif not the abstraction can be re.ned again using a counterexample. TERMINATOR is a symbolic model checker \nfor termination and liveness properties that is based on this idea, as described in [11, 13, 14]. Thedif.cultyin \nTERMINATOR isthatcheckingthevalidity of the termination argument (i.e. checking the invariance property \nthat proves the validity of the termination argument) is extremely expensive. This was shown in Section \n5, which presents the .rst known experimental evaluation of tools like TERMINATOR and POLYRANK. In contrast, \nhere we directly compute an over-approximation, T1 .\u00b7\u00b7\u00b7.Tn, where the inclusion R+ . T1 .\u00b7\u00b7\u00b7.Tn holds \nby construction. Unlike in TERMINATOR,the Ti are not guaranteed to be well-founded: we have to check \nthat. As we have seen, this has some advantages as regards speed and the ability to tune precision. The \nwork here takes the opposite perspective from TERMINA-TOR. We show how an over-approximating T1 . \u00b7\u00b7\u00b7 \n. Tn can quickly be computed using an off-the-shelf abstract interpretation. We do not need to check \nthe inclusion, as TERMINATOR must, because it holds by construction. But we must still check that each \nTi is well-founded. So, although justi.ed by the same re\u00adsult on disjunctively well-founded relations, \nwe use of this result in a fundamentally different way than does TERMINATOR:we get over-approximation \nby construction, but have to check for well\u00adfoundedness, while TERMINATOR s candidate termination argu\u00adments \nT1 .\u00b7\u00b7\u00b7.Ti are disjunctively well-founded by construction, but it may not be an over-approximation. Our \ntechnique is not de\u00adpendent on counterexample-driven re.nement or on predicate ab\u00adstraction. The work \nin [37] uses the result on disjunctively well-founded relations to justify several inference rules for \ntransition invariants; a transition invariant is an over-approximation of the transition relation of \na program, when restricted to reachable states. It would be straightforward to modify the VARIANCEANALYSIS \nalgorithm to compute transition invariants. As it stands, given a program with transition relation R, \nour variance assertions can be seen as transition invariants for the transitive closure R+, restricted \nto where the start and end states of the transition relation are at the same program location. They are \nwhat we need to reason about whether a location is visited in.nitely often, and to formulate the more \nre.ned local termination predicates which give added modularity to our analysis. The notions of seeding \nand ghost variables are basic and have been used many times [3, 4, 16, 32, 39]. 8. Conclusion We have \nintroduced the notion of a variance assertion together with a class of tools called variance analyses. \nFurthermore, we have developed a generic method of inducing variance analyses from invariance analyses, \nand shown how several analyses developed through the method lead to termination provers that are competitive \nwith known termination provers. The proposed approach has several unique advantages over known existing \napproaches. For example: The induced variance analyses reuse the machinery of the underlying invariance \nanalysis to quickly and automatically .nd a (disjunctive) candidate termina\u00adtion argument. The argument \nitself can often be expressed using a less precise, and thus more ef.cient, domain e.g. Octagon versus \nPolyhedra as in Section 2. Furthermore, the induced variance analyses are truly analyses. That is, like \ninvariance analyses infer invariance assertions, the induced variance analyses infer variance assertions. \nAs invariance assertions can be used to prove safety properties (amongst other uses), variance assertions \nare useful when proving termination and probably other liveness properties. For example, if termination \ncannot be proved directly by prov\u00ading well-foundedness of each disjunct in the VARIANCEANALYSIS algorithm, \nthe well-founded subset of the variance assertion could be passed to a tool like TERMINATOR which could \ndirectly use it during further attempts to prove termination or other liveness prop\u00aderties. This would \nundoubtedly lead to faster termination provers, with at least as much precision as is now possible. A \nmore inter\u00adesting question is: Can the combination of approaches prove new programs terminating that \ncannot be proved terminating with the approaches separately? This arises from the fact that TERMINA-TOR \nsometimes suffers in cases where the loop variables are ini\u00adtialized to constant values (e.g. for(i=0;i<N;i++);). \nIn this case TERMINATOR might (as it is based on predicate abstraction [27]) produce an in.nite number \nof predicates (i==0, i==1,etc). TER-MINATOR has heuristics thatattempt to mitigatethis issue, but these \ntechniques often fail. OCTATERM and POLYTERM, for example, do not suffer from this problem. Thus, we \nmight be able to use the information from failed runs of OCTATERM to help TERMINATOR overcome its limitations \nin contexts like this. Finally, liveness properties (including termination) for .nite\u00adstate systems amount \nto invariance properties. All known model checkers that support liveness properties for .nite-state systems \nmake use of this fact. Biere et al. [4] go so far as to simply convert the liveness checking problem \nfor .nite-state systems into safety. Thus, liveness and termination only become distinct from safety \nin the context of in.nite-state systems. It is known, though, that liveness properties for in.nite state \nsystems can be reduced to fair termination (see [11, 43]). It therefore seems likely that the ideas in \nthis paper could be used for liveness analyses other than termination. The variance analyses we have \ngiven as instances of this paper s proposed algorithm are built from invariance analyses that are con\u00adcerned \nwith established ingredients of termination provers: linear arithmetic and size of data structures. However, \nthere exist many invariance analyses built on a broad spectrum of abstractions. It will be exciting to \nsee what kinds of variance analyzers will come of combining these abstract domains with VARIANCEANALYSIS. \nAcknowledgments. We are grateful to Domagoj Babic, Nick Benton, Aaron Bradley, Andreas Blass, Patrick \nCousot, Georges Gonthier, Alexey Gotsman, Arie Gur.nkel, Antoine Min\u00b4e, An\u00addreas Podelski, Helmut Veith \nand Hongseok Yang for discussions and comments that helped to improve the paper. Chawdhary was supported \nby a Microsoft PhD studentship. Distefano and O Hearn acknowledge the support of the EPSRC.  References \n[1] A. V. Aho, R. Sethi, and J. D. Ullman. Compilers: Priciples, Techniques, and Tools. 1986. [2] A.W.Appel. \nModern Compiler Implementation in ML. 1998. [3] J. Berdine, B. Cook, D. Distefano, and P. O Hearn. Automatic \ntermination proofs for programs with shape-shifting heaps. In CAV 06: International Conference on Computer \nAided Veri.cation, 2006. [4] A. Biere, C. Artho, and V. Schuppan. Liveness checking as safety checking. \nIn FMICS 02: Formal Methods for Industrial Critical Systems, 2002. [5] A. Bradley. Personal communication. \nAaron Bradley s suggested script that iteratively applies the tools described in [7] and [6] with increasingly \nexpensive options, June 2006. [6] A. Bradley, Z. Manna, and H. Sipma. Termination of polynomial programs. \nIn VMCAI 05: Veri.cation, Model Checking, and Abstract Interpretation, 2005. [7] A. R. Bradley, Z. Manna, \nand H. B. Sipma. The polyranking principle. In ICALP 05: International Colloquium on Automata, Languages \nand Programming, 2005. [8] C. Calcagno, D. Distefano, P. O Hearn, and H. Yang. Beyond reachability: Shape \nabstraction in the presence of pointer arithmetic. In SAS 06: Static Analysis Symposium, 2006. [9] M. \nCol\u00b4on and H. Sipma. Synthesis of linear ranking functions. In TACAS 01: Tools and Algorithms for the \nConstruction and Analysis of Systems, 2001. [10] M. Col\u00b4on and H. Sipma. Practical methods for proving \nprogram termination. In CAV 02: International Conference on Computer Aided Veri.cation, 2002. [11] B. \nCook, A. Gotsman, A. Podelski, A. Rybalchenko, and M. Vardi. Proving that programs eventually do something \ngood. In POPL 06: Principles of Programming Languages, 2006. [12] B. Cook, A. Podelski, and A. Rybalchenko. \nAbstraction re.nement for termination. In SAS 05: Static Analysis Symposium, 2005. [13] B. Cook, A. Podelski, \nand A. Rybalchenko. Termination proofs for systems code. In PLDI 06: Programming Language Design and \nImplementation, 2006. [14] B. Cook, A. Podelski, and A. Rybalchenko. Terminator: Beyond safety. In CAV \n06: International Conference on Computer Aided Veri.cation, 2006. [15] P. Cousot. The calculational design \nof a generic abstract interpreter. In M. Broy and R. Steinbr\u00a8uggen, editors, Calculational System Design. \n1999. [16] P. Cousot. Proving program invariance and termination by parametric abstraction, Lagrangian \nrelaxation and semide.nite programming. In VMCAI 05: Veri.cation, Model Checking, and Abstract Interpreta\u00adtion, \n2005. [17] P. Cousot. Personal communication, 2006. [18] P. Cousot and R. Cousot. Abstract interpretation: \nA uni.ed lattice model for static analysis of programs by construction or approximation of .xpoints. \nIn POPL 77: Principles of Programming Languages, 1977. [19] P. Cousot and R. Cousot. Systematic design \nof program analysis frameworks. In POPL 79: Principles of Programming Languages, 1979. [20] P. Cousot \nand R. Cousot. Abstract interpretation frameworks. J. Log. Comput. 2(4), pp511-547, 1992. [21] P. Cousot \nand R. Cousot. Modular static program analysis. In CC 02: Conference of Compiler Construction, 2002. \n[22] P. Cousot, R. Cousot, J. Feret, L. Mauborgne, A. Min\u00b4e, D. Monniaux, and X. Rival. The ASTR EE analyzer. \nIn ESOP 05: European \u00b4 Symposium on Programming, 2005. [23] P. Cousot and N. Halbwachs. Automatic discovery \nof linear restraints among variables of a program. In POPL 78: Principles of Programming Languages, 1978. \n[24] D. Distefano, P. W. O Hearn, and H. Yang. A local shape analysis based on separation logic. In TACAS \n06: Tools and Algorithms for the Construction and Analysis of Systems, 2006. [25] R. W. Floyd. Assigning \nmeanings to programs. In Mathematical Aspects of Computer Science, 1967. [26] J. Giesl, R. Thiemann, \nP. Schneider-Kamp, and S. Falke. Automated termination proofs with AProVE. In RTA 04: Rewriting Techniques \nand Applications, 2004. [27] S. Graf and H. Sa\u00a8idi. Construction of abstract state graphs with PVS. In \nCAV 97: International Conference on Computer Aided Veri.cation, 1997. [28] B. S. Gulavanii and S. K. \nRajamani. Counterexample driven re.nement for abstract interpretation. In TACAS 06: Tools and Algorithms \nfor the Construction and Analysis of Systems, 2006. [29] H. Jain, F. Ivancic, A. Gupta, I. Shlyakhter, \nand C. Wang. Using statically computed invariants inside the predicate abstraction and re.nement loop. \nIn CAV 06: International Conference on Computer Aided Veri.cation, 2006. [30] B. Jeannet. NewPolka polyhedra \nlibrary. http://pop-art.inrialpes.fr/ people/bjeannet/newpolka/index.html. [31] C. S. Lee, N. D. Jones, \nand A. M. Ben-Amram. The size-change principle for program termination. In POPL 01: Principles of Programming \nLanguages, 2001. [32] Z. Manna and A. Pnueli. Axiomatic approach to total correctness of programs. Acta \nInformatica, 1974. [33] L. Mauborgne and X. Rival. Trace partitioning in abstract interpre\u00adtation based \nstatic analyzers. In ESOP 05: European Symposium on Programming, 2005. [34] A. Min\u00b4e. The Octagon abstract \ndomain. Higher-Order and Symbolic Computation. (to appear). [35] A. Pnueli. The temporal logic of programs. \nIn 18th IEEE Symposium on Foundations of Computer Science, 1977. [36] A. Podelski and A. Rybalchenko. \nA complete method for the synthesis of linear ranking functions. In VMCAI 04: Veri.cation, Model Checking, \nand Abstract Interpretation, 2004. [37] A. Podelski and A. Rybalchenko. Transition invariants. In LICS \n04: Logic in Computer Science, 2004. [38] A. Podelski and A. Rybalchenko. Transition predicate abstraction \nand fair termination. In POPL 05: Principles of Programming Languages, 2005. [39] J. C. Reynolds. The \nCraft of Programming. London, 1981. [40] J. C. Reynolds. Separation logic: A logic for shared mutable \ndata structures. In LICS 02: Symposium on Logic in Computer Science, 2002. [41] S. Sankaranarayanan, \nF. Ivancic, I. Shlyakhter, and A. Gupta. Static analysis in disjunctive numerical domains. In SAS 06: \nStatic Analysis Symposium, 2006. [42] A. Tiwari. Termination of linear programs. In CAV 04: International \nConference on Computer Aided Veri.cation, 2004. [43] M. Y. Vardi. Veri.cation of concurrent programs: \nThe automata\u00adtheoretic framework. Ann. Pure Appl. Logic, 51(1-2):79 98, 1991. \n\t\t\t", "proc_id": "1190216", "abstract": "An invariance assertion for a program location <i>l</i> is a statement that always holds at <i>l</i> during execution of the program. Program invariance analyses infer invariance assertions that can be useful when trying to prove safety properties. We use the term <i>variance assertion</i> to mean a statement that holds between any state at <i>l</i> and any previous state that was also at <i>l</i>. This paper is concerned with the development of analyses for variance assertions and their application to proving termination and liveness properties. We describe a method of constructing program variance analyses from invariance analyses. If we change the underlying invariance analysis, we get a different variance analysis. We describe several applications of the method, including variance analyses using linear arithmetic and shape analysis. Using experimental results we demonstrate that these variance analyses give rise to a new breed of termination provers which are competitive with and sometimes better than today's state-of-the-art termination provers.", "authors": [{"name": "Josh Berdine", "author_profile_id": "81100298282", "affiliation": "Microsoft Research", "person_id": "PP33025071", "email_address": "", "orcid_id": ""}, {"name": "Aziem Chawdhary", "author_profile_id": "81322490900", "affiliation": "Queen Mary, University of London", "person_id": "P831276", "email_address": "", "orcid_id": ""}, {"name": "Byron Cook", "author_profile_id": "81323489213", "affiliation": "Microsoft Research", "person_id": "PP40031078", "email_address": "", "orcid_id": ""}, {"name": "Dino Distefano", "author_profile_id": "81100325271", "affiliation": "Queen Mary, University of London", "person_id": "PP24041729", "email_address": "", "orcid_id": ""}, {"name": "Peter O'Hearn", "author_profile_id": "81332519314", "affiliation": "Queen Mary, University of London", "person_id": "PP45027416", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1190216.1190249", "year": "2007", "article_id": "1190249", "conference": "POPL", "title": "Variance analyses from invariance analyses", "url": "http://dl.acm.org/citation.cfm?id=1190249"}