{"article_publication_date": "01-17-2007", "fulltext": "\n Locality Approximation Using Time Xipeng Shen Jonathan Shaw Brian Meeker Chen Ding Computer Science \nDepartment Shaw Technologies Computer Science Department The College of William and Mary jshaw@cs.rochester.edu \nUniversity of Rochester xshen@cs.wm.edu {bmeeker,cding}@cs.rocheester.edu Abstract 25 Reuse distance \n(i.e. LRU stack distance) precisely characterizes program locality and has been a basic tool for memory \nsystem 20 research since the 1970s. However, the high cost of measuring has restricted its practical \nuses in performance debugging, locality analysis and optimizations of long-running applications. In this \nwork, we improve the ef.ciency by exploring the connec\u00ad tion between time and locality. We propose a \nstatistical model that converts cheaply obtained time distance to the more costly reuse distance. Compared \nto the state-of-the-art technique, this approach reduces measuring time by a factor of 17, and approximates \ncache line reuses with over 99% accuracy and the cache miss rate with less than 0.4% average error for \n12 SPEC 2000 integer and .oating\u00adpoint benchmarks. By exploiting the strong correlations between time \nand locality, this work makes precise locality as easy to obtain as data access frequency, and opens \nnew opportunities for program optimizations. Categories and Subject Descriptors D.3.4 [Programming Lan\u00adguages]: \nProcessors optimization, compilers General Terms Algorithms, Measurement, Performance Keywords Time Distance, \nProgram Locality, Reuse Distance, Reference Af.nity, Trace Generator, Performance Prediction 1. Introduction \nAs the memory hierarchy becomes deeper and shared by more pro\u00adcessors, cache performance increasingly \ndetermines system speed, cost and energy usage. The effect of caching depends on program locality or \nthe pattern of data reuses. Initially proposed as LRU stack distance by Mattson et al. [18] in 1970, \nreuse distance is the number of distinct data elements ac\u00adcessed between the current and the previous \naccess to the same data element [11]. As an example, in the data reference trace a b c b d d a , the \nreuse distance of the second access to data element a is 3 since b , c and d are the distinct data elements \nbetween the two accesses to a . Reuse distance provides an architecture\u00adindependent locality metric, \nprecisely capturing program temporal locality and re.ecting memory reference af.nity [26]. A Reuse dis\u00adtance \nhistogram, illustrated in Figure 1, summarizes the distribu\u00adtion of the reuse distances in an execution. \nIn the graph, the sev- Permission to make digital or hard copies of all or part of this work for personal \nor classroom use is granted without fee provided that copies are not made or distributed for pro.t or \ncommercial advantage and that copies bear this notice and the full citation on the .rst page. To copy \notherwise, to republish, to post on servers or to redistribute to lists, requires prior speci.c permission \nand/or a fee. POPL 07 January 17 19, 2007, Nice, France. Copyright c . 2007 ACM 1-59593-575-4/07/0001...$5.00 \n5 0 1 2 4 8 16 32 64 128256512 Reuse distance Reference% 15 10 Figure 1. A reuse distance histogram \non log scale. enth bar, for instance, shows that 25% of total memory accesses have reuse distance in \nrange [32, 64). Researchers have used reuse distance (mostly its histogram) for many purposes: to study \nthe limit of register [15] and cache reuse [10, 13], to evaluate program transformations [1, 4, 25], \nto predict performance [17], to insert cache hints [5], to identify critical instructions [12], to model \nreference af.nity [26], to detect locality phases [22], to manage superpages [7], and to model cache \nsharing between parallel processes [8]. Because of the importance, the last decades have seen a steady \nstream of research on accelerating reuse distance measurement. In 1970, Mattson et al. published the \n.rst measurement algorithm [18] using a list-based stack. Later studies e.g. Bennett and Kruskal in 1975 \n[3], Olken in 1981 [19], Kim et al. in 1991 [14], Sugumar and Abraham in 1993 [24], Almasi et al. in \n2002 [1], Ding and Zhong in 2003 [11] have reduced the cost through various data structures and algorithms. \nDespite those efforts, the state-of-the-art measurement tech\u00adnique still slows down a program s execution \nup to hundreds of times: The measurement of a 1-minute execution takes more than 4 hours. The high cost \nimpedes the practical uses in performance debugging, locality analysis, and optimizations of long-running \nap\u00adplications. All previous algorithms have essentially implemented the de.\u00adnition of reuse distance \ncounting the number of distinct data ac\u00adcessed for each reuse. In this work, we address the problem from \na different aspect: Can we use some easily obtained program behav\u00adior to statistically approximate reuse \ndistance? The behavior we choose is time distance, which is de.ned as the number of data el\u00adements accessed \nbetween the current and the previous access to the same data element. (The time distance is 6 for the \nexample given in the second paragraph.) The difference from reuse distance is not having the distinct \nrequirement, which makes its measurement as light as just recording the last access time of each data \nelement a small portion of the cost of reuse distance measurement. As what people commonly conceived, \ntime distance itself can\u00adnot serve as an accurate locality model. In access trace abbbb a , for example, \nthe time distance of the second access to variable a is 5, which could correspond to 5 different reuse \ndistances, from 0to4,ifnoother informationisgiven.However,ifweknowthe time distance histogram among four \nreuses, one has time distance of 5 and three have time distance of 1, we can easily determine the trace \ngiven the number of variables and thus obtain the reuse dis\u00adtance. Although it s not always possible \nto derive a unique trace from time distances, this work discovers that a time distance his\u00adtogram contains \nenough information to accurately approximate the reuse distance histogram. We describe a novel statistical \nmodel that takes a time distance histogram to estimate the reuse distance histogram in three steps by \ncalculating the following probabilities: the probability for a time point to fall into a reuse interval \n(an interval with accesses to the same data at both ends and without accesses to that data element in \nbetween) of any given length, the probability for a data element to appear in a time interval of any \ngiven length, and the binomial distribution of the number of distinct data in a time interval. The new \nmodel has two important advantages over previous pre\u00adcise methods. First, the model predicts the reuse \ndistance histogram for bars of any width, which previous methods cannot do unless they store a histogram \nas large as the size of program data. The second is a drastic reduction of measuring costs with little \nloss of accuracy. Our current implementation produces over 99% accuracy for reuse distance histogram \napproximation, providing a factor of 17 speedup on average compared to the fastest precise method.  \n2. Approximation of Locality The inputs to our model are the number of distinct data accessed in an execution \nand the time distance histogram of the execution; the output of the model is the reuse distance histogram \nwhich characterizes the locality of that execution. To ease the explanation, we assume that the size \nof bars in both histograms is 1 and the histograms are of data element reuses. Section 2.4 describes \nthe extensions for histograms of any size bars and for any size cache blocks. We use the following notations: \nB(x) : a binary function, returning 1 when x is true and 0 when x is false. M(v) : the total number of \naccesses to data element v. N : the total number of distinct data elements in an execution. T : the \nlength of an execution. Without explicitly saying so, we use logical time, i.e. the number of data accesses. \nEach point of time corresponds to a data access.  Tn(v) : the time of the n thaccesstodata v. T>t(v), \nT<t(v) : the time of v s .rst access after time t and the time of its last access before t respectively. \nvt : the data element accessed at time t. The algorithm includes three steps, outlined below in reverse \norder for the purpose of clarity. Please see our technical report [20, 21] for more details. 2.1 Step \n3: Estimate Reuse Distance The last step is based on the following observation: OBSERVATION 2.1. Given \na program s execution, we can approx\u00adimate the reuse distance histogram if we know the time distance \nhistogram of the execution and the probability for any given data to appear in any given time interval \nof length ., represented by P3(.)1 . Note that the probability P3(.) is a function of . only and is independent \nto the identity of the data. The approximation of reuse distance through P3(.) is based on the model \nof Bernoulli processes. A Bernoulli process is a discrete\u00adtime stochastic process consisting of a sequence \nof independent random variables taking values over the set {0,1}. A typical ex\u00adample is coin tossing: \nthere is a coin with probability p showing heads when being tossed. The probability for k heads in n \ntosses is a binomial distribution, denoted by f(k; n, p) \u00ab n f(k; n, p)= kp k(1 -p)n-k . Assuming the \nprobability of any data to be accessed in an in\u00adterval is independent from other data, data accesses \ncan be consid\u00adered as a Bernoulli process. Each distinct data is like one toss of an experimental coin \nwith probability of P3(.) showing heads. The number of tosses is equal to the total number of distinct \ndata ele\u00adments in the program. The number of times showing heads is the number of distinct data elements \nbeing accessed in an interval of length .. We use P (k, .) to represent the probability of having k distinct \ndata in a . long interval, which is equivalent to the probability of having k heads in N tosses of a \ncoin with P3(.) probability of showing heads each time. The percentage of references having reuse distance \nof k can therefore be calculated as follows: X PR(k)= P (k, .) \u00b7 PT (.) (1) . where, \u00ab N kN-k P(k, .) \n= P3(.)(1 -P3(.)) k PR(k) and PT (.) respectively denote the Y-axis value of a bar in reuse distance \nhistograms at position of k and time distance histograms at position of ..  2.2 Step 2: Calculate P3(.) \nThis step is to calculate P3(.), the probability of any given data element to appear in a time interval \nof length .. Given any time point t, the interval [t -., t) includes . time points, represented by t-t,where \nt = ., ..., 2, 1. A given data being accessed in time range [t -., t) means that its last access time \nbefore t is at time t -1,or t -2, or, ...., or t -..Let P2(t) be the probability for the access to happen \nat time t -t. We calculate P3(.) as follows: . X P3(.) = P2(t) (2) t=1 In a real execution, the value \nof P2(t) may differ at different times and for different data. But for statistical inference, we want \nthe average probability, which is calculated as follows. 1 The formal de.nition of P3(.) is as follows: \nGiven a random time point t,ifwepickadata v at random from those that are not accessed at time t, P3(.) \nis the probability that v s last access prior to t is after t - . - 1. X 1 P2(t)= B(t -T<t(v)=t) T \u00b7 \n(N -1) t,v =vt MX(v) Tn+1(v)-1 XX 1 = B(t -Tn(v)=t) T \u00b7 (N -1) vn=1 t=Tn(v)+1 MX(v) X 1 = B(Tn+1(v)-Tn(v)>t) \nT \u00b7 (N -1) vn=1 Mathematical inferences [21] produce the following result: T X 1 P2(t)= \u00b7 (d -1) d=t+1 \nX 1 B(T>t(v)-T<t(v)=d). T \u00b7 (N -1) v,t(vt =v) X Let P1(d)= 1 \u00b7 B(T>t(v)-T<t(v)=d),the T \u00b7(N-1) v,t(vt=v) \nequation becomes T X P1(d) P2(t)= (3) d -1 d=t+1 The problem is now reduced to that of calculating P1(d). \n 2.3 Step 1: Calculate P1(d) P1(d)is the probability for a randomly chosen time t to fall into a d\u00adlong \nreuse interval of a data element that is randomly chosen from the data elements that are not accessed \nat t. It has the following relation with time distance histogram PT (d)[21]: d -1 P1(d)= PT (d) (4) N \n-1 This concludes the basic model for the approximation of a reuse distance histogram. Putting equations \n(2, 3, 4) together produces the following formula for P3(.): .T XX 1 P3(.)= PT (d) N -1 t=1 d=t+1 Although \nit seems simple, the formula is hard to interprete intuitively. When the bars are wider than 1, the corresponding \nformula becomes very complex [20]. The three steps above give not only an intuitive explanation of the \nmodel, but also a clear guide for implementation. 2.4 Extensions and Implementation In the above description, \nwe assumed that each bar s width in both time and reuse distance histograms is 1. With some extensions, \nour model is general enough to allow bars of any width, and hence histograms on any scale (e.g. linear, \nlog or random scale.) For lack of space, please see [20] for the extensions. In our implementation, we \nremove the redundant calculations by reordering some computations. We use a look-up table generated of.ine \nto minimize the computation of binomial probabilities. A boundary case is the treatment of the .rst access \n(i.e. the cold access) to a variable. We use the variable s circular time distance, which is the sum \nof the time distance from the starting of the program to its .rst access and the distance from its last \naccess to the end of the execution. We tried other options such as ignoring the .rst accesses and assigning \na large distance value to them. The circular time distance scheme shows the best effect.  3. Evaluation \nThis section presents experimental results on both generated traces from a trace generator [20] and 12 \nreal programs in SPEC CPU2000 suite (Table 2). We use the generated traces to test the ap\u00adproximation \nmodel on histograms of different distributions. We use the SPEC programs to measure the ef.ciency and \naccuracy in real uses. All experiments were run on Intel(R) Xeon(TM) 2.00GHz Processors with 2 GB of \nmemory running Fedora Core 3 Linux. We use PIN 3.4 [16] for instrumentation with GCC version 3.4.4 as \nour compiler (with the -O3 .ag). We use Ding and Zhong s technique to measure the real reuse distance \nhistograms [11]. It is asymptotically the fastest tool for measuring reuse distance at a guaranteed precision2. \n 3.1 Results on Generated Traces Using the trace generator, we generate traces of different reuse distributions. \nFigure 2(a)(b) show the reuse distance histograms of pulse-like and exponential distributions. In the \nexperiment, we .rst measure the time distance histogram (TDH) of the generated trace. We then apply the \nstatistical model to TDH to approximate the reuse distance histogram (R RDH) of the trace. After measuring \nthe real reuse distance histogram (RDH) of the generated trace, we calculate the accuracy of the approximation \nas follows: P c |Bi -Bi| accuracy =1- i (5) 2 Where, Bi is the Y-axis value of the i th bar in RDH and \nc Bi is the one in R RDH. The division by 2 is to normalize the accuracy to [0,1]. In the experiments \non generated traces, we make the bars of both histograms as wide as 1 so that we may observe errors that \nwould otherwise be hidden by a larger bar size. The three graphs on the left column of Figure 2 are for \nthe trace whose reuse distance histogram is in a pulse-like shape. The esti\u00admated histogram matches the \npulses with a smoother shape, which causes some deviations at the pulse boundaries. However, because \nthe estimated curve .ts the .ow of the real curve well, the local deviations tend to cancel each other \nin a bar graph. Figure 2 (e) shows the log-scale reuse distance histograms with approximation accuracy \nof 98.4%. Figure 2(c) is the time distance curve, which has 5 smoothly .uctuating waves decreasing gradually \nto 0. What the graph does not show for lack of space is a thin peak at the end of the curve, distance \n50000, with 0.001% references, which is due to the limited number of counters and the time distance of \ncold ac\u00adcesses (Section 2.4). The right column of Figure 2 shows the results for a trace whose reuse \ndistance histogram exhibits an exponential distribution. The estimated curve matches the real curve so \nwell that they cannot be distinguished in Figure 2(b). Figure 2(d) shows the time distance curve, which \nis a long tail curve. Figure 2(f) gives the log-scale reuse distance histograms with accuracy 98.7%. \nWe see similar results on random distribution histograms [21]. Note, in the generated traces, the data \nelement to be accessed at a time point depends on all the prior accesses (including the accesses to other \ndata elements.) Therefore, different data elements can have different reuse distributions. In our model, \nwe give all data elements the same probability the average probability, which may hurt the prediction \nof the reuses of a data element, but is effective for the reuses of a whole program. 2 We set the tool \nto guarantee 99.9% accuracy.  0 100 200 300 400 500 0 100 200 300 400 500 Reuse distance Reuse distance \n(a) Reuse distance histogram of a pulse distribution (b) Reuse distance histogram of an exponential distribution \n2 0.4 0.35 1.5  0.3 0.25 0.2 0.15 Reference% Reference% Reference% 1 0.5 0.1 0.05 0 0 0 10000 20000 \n30000 40000 50000 Time distance Time distance (c) Time distance histogram of a pulse distribution (d) \nTime distance histogram of an exponential distribution 50  25 40 20 Reference% 30 20 15 10 10 5 0 \n0 Reuse distance Reuse distance (e) Log-scale reuse distance histogram of a pulse distribution (f) Log-scale \nreuse distance hist. of an exp. distribution Figure 2. Comparisons of the actual reuse distance histograms \nand the histograms estimated from the time distance histograms. Different distributions and bar sizes \nare shown. 3.2 Results on SPEC CPU2000 We apply the statistical model to 12 SPEC CPU2000 benchmarks. \nIn all the experiments, the approximation uses 1K-wide bars in both the measured time distance histograms \nand the approximated reuse distance histograms. The log-scale histograms are derived from the linear-scale \nones. 3.2.1 Comparison of Time Cost Table 1 shows the time cost of the statistical model compared with \nthe previous fastest method [11] for 12 SPEC CPU2000 bench\u00admarks with the train inputs. The .rst column \nshows the benchmark names. The second column gives the basic overhead of the instru\u00admentation, Tinst. \nIt is the running time of the benchmarks after beinginsertedaninvocationofanemptyfunctionateachmemory \naccess. Much of that overhead could be saved with a more ef.\u00adcient instrumentor, e.g. a source-code level \ninstrumentor through a compiler. The left half of the rest of the table shows the result of element reuses, \nand the right half gives that of cache line reuses (cache line is 128B wide.) Within each part, the .rst \ncolumn is the time of Ding and Zhong s technique, TRD; the sum of the next two columns is the time of \nour technique, including the time to measure time distance, TTD, and the time to convert time distance \nhistograms to reuse distance histograms, Tconv. The next column shows the speedup of our technique. In \norder to avoid the effects of different instrumentors, we subtract the instrumentation overhead from \nboth kinds of measured time as follows: Speedup =(TRD -Tinst)/(TTD +Tconv -Tinst) The table demonstrates \n19.4 times speedup for data element reuses. The smallest speedup is 8 times on program mcf, which has \nthe largest number of data elements, 10.1 million, among integer benchmarks. The program has a large \nrange of reuse distances and time distances, which make its conversion time much longer than others. \nFor cache line reuses, the speedup is from 10 to 21 times, with the average as 17.2. Program mcf shows \n19.2 times speedup. The signi.cantly greater speedup compared to data element reuse is due to the decrease \nof the number of counting units and the range of distances. Figure 3 shows the reuse distance histograms \nof element reuse and cache line reuse. The distance range shrinks from 16M to 512K. 3.2.2 Approximation \nAccuracy Table 2 shows the accuracy of the reuse distance approximation on both element and cache line \nlevel for the test and train runs of the SPEC CPU2000 benchmarks. A bar in linear-scale histogram covers \nthe reuse distance of 1K; the bars in a log-scale histogram have the range as [0 1K), [1K, 2K), [2K, \n4K), [4K, 8K), ....The accuracy calculation is through Equation 5. The approximation accuracy for cache \nline reuses is 99.3% and 99.4% for linear and log scale respectively. The lowest accuracy is 96.5% and \n96.6% on benchmark ammp. Nine out of the 12 benchmarks have over 99% accuracy. The accuracy for element \nreuses is 91.8% and 94.1% on aver\u00adage. Benchmark mcf and equake give low accuracy. Figure 3 shows the \nhistograms of mcf reuses. The largest error of element reuse ap\u00adproximation happens in bars of [128K, \n256K) and [256K, 512K). The estimated bar in the range [256K, 512K) matches well with the real bar in \nthe range [128K, 256K). A possible reason for that mis\u00admatch is the independence assumption of the statistical \nmodel: we assume that the probability for a variable to appear in an interval is independent of the other \nvariables. However, a larger granular\u00adity removes the error almost completely, as shown in the result \nof cache line reuses in Figure 3 (b). Program equake has the similar phenomenon. A common use of reuse \ndistance is to study cache behavior through cache line reuse histograms, for which the occa\u00adsionally \nlow accuracy of element reuses does not matter. Overall, the statistical model shows 17 times speedup, \nover 99% accuracy for cache line reuses, and 94% for element reuses. Uses for Cache Miss Rate Prediction \nPrevious work has shown the uses of reuse distance histograms in cache miss rate predic\u00adtion [25]. In \nthis experiment, we compare the predicted cache miss rates from the actual and the approximated reuse \ndistance his\u00adtograms to test the effectiveness of the statistical model in real uses. Among all 12 benchmarks, \nthe largest errors are 2.5% for twolf and 1.4% for equake, which is consistent with Table 2, where, the \ntwo benchmarks have the lowest accuracy of cache line reuse estimation. Note although it has the worst \nelement reuse estimation, benchmark mcf has only 0.34% miss rate estimation error. That is because of \nits excellent cache line reuse estimation, the basis for cache performance prediction. On average for \nall benchmarks, the miss rates predicted from the actual and the approximated histograms have less than \n0.42% differences [21].  4. Related Work This section discusses some related work that are not mentioned \nin Section 1. In 1976, Smith gave a probability method for predicting the miss rate of set-associative \ncache [23]. To predict the effect of cache sharing, Chandra et al. proposed models to predict inter\u00adthread \ncache contention from isolated cache stack distance [8]. Our method complements these techniques by combining \nthem one can predict the (shared) performance of set-associative cache solely based on time distance \nhistograms. An orthogonal way of reducing the measuring cost is using a sampled trace rather than the \nfull trace [2, 6, 9]. The combination of sampling and this work has the potential to make reuse distance \napplicable to run-time optimizations. 5. Conclusions In this work, we demonstrate the strong connection \nbetween time and locality with a novel statistical model to approximate program locality from easily-obtained \ntime distance histograms. The experi\u00adments show 17 times speedup over the state-of-the-art locality mea\u00adsurement. \nThe approximation accuracy is over 99% for cache block reuse and over 94% for element reuse. The model \nis general enough to allow reuse distance histograms of any scale and data reuse of different granularity \nto be approximated. The new levels of ef.\u00adciency and generality open up opportunities for performance \npre\u00addiction, debugging, and optimizations. 6. Acknowledgments We thank Hans Boehm for his insightful \ncomments and valuable suggestions on this paper. Yutao Zhong, Kristof Beyls, and the anonymous referees \nalso helped improve the presentation. This research is supported in part by grants from The College of \nWilliam and Mary and NSF grants CNS-0509270 and CCR-0238176. Any opinion, .ndings, and conclusions contained \nin this document are those of the authors and do not re.ect the views of these agencies.  References \n[1] G. Almasi, C. Cascaval, and D. Padua. Calculating stack distances ef.ciently. In Proceedings of the \n.rst ACM SIGPLAN Workshop on Memory System Performance, Berlin, Germany, June 2002. [2] M. Arnold and \nB. G. Ryder. A framework for reducing the cost of instrumented code. In Proceedings of ACM SIGPLAN Conference \non Programming Language Design and Implementation,Snowbird, Utah, June 2001. Benchmark Instrument overhead \n(sec.) Data element level Cache line level Reuse dist. measure (sec.) Time dist. measure (sec.) Time \nto approximate (sec.) Speedup times Reuse dist. measure (sec.) Time dist. measure (sec.) Time to approximate \n(sec.) Speedup times CINT crafty 423 15654 1162 5 20.5 X 9946 1030 2 15.6 X gcc 135 1333 223 7 12.6 X \n926 212 1 10.1 X gzip 12 254 28 1 14.2 X 159 23 1 12.3 X mcf 131 3856 534 61 8.0 X 2585 257 2 19.2 X \ntwolf 138 4262 403 2 15.4 X 2859 292 2 17.4 X vortex 236 8142 601 6 21.3 X 5142 548 2 15.6 X CFP ammp \n354 20175 1333 4 20.2 X 12923 997 2 19.5 X applu 170 9806 534 9 25.8 X 5718 447 2 19.9 X equake 127 13182 \n766 3 20.3 X 7773 489 1 21.1 X mesa 1363 45316 3131 5 24.8 X 31191 2955 2 18.7 X mgrid 206 17336 823 \n2 27.7 X 10358 677 2 21.5 X wupwise 406 21145 1374 1 21.4 X 10876 1075 2 15.6 X Average 19.4 X 17.2 X \n Table 1. Comparison of the time of reuse distance approximation and measurement Benchmark Lang. Description \nInput Number of data elements Number of mem. accesses Accuracy (%) Element Cache line linear log linear \nlog CINT crafty C Game playing: chess test 484K 2.8B 93.2 93.2 100 100 train 484K 17.8B 93.4 93.4 100 \n100 gcc C GNU C programming language compiler test 916K 675M 99.3 99.4 99.9 99.9 train 3.46M 1.5B 99.2 \n99.4 99.9 99.9 gzip C GNU compression using Lempel-Ziv coding test 69.8K 94.1M 98.9 99.8 99.5 99.5 train \n79.6K 273M 98.5 99.3 99.6 99.6 mcf C Combinatorial optimization for vehicle scheduling test 349K 53.5M \n82.7 89.0 99.0 99.1 train 10.1M 3.4B 67.9 72.4 97.7 98.4 twolf C Place and route simulator test 29.1K \n109M 99.0 99.0 100 100 train 435K 4.8B 89.5 90.2 97.5 97.5 vortex C Object-oriented Database test 2.92M \n5.3B 93.0 93.1 100 100 train 2.76M 9.6B 93.2 93.2 100 100 CFP ammp C Computational chemistry: Modeling \nmolecule system test 3.16M 2.5B 89.2 96.5 96.5 96.6 train 3.16M 22.8B 95.9 96.2 99.2 99.5 applu F77 Parabolic/Elliptic \nPartial Differential Equations test 212K 207M 94.0 99.5 99.8 99.9 train 2.00M 10.5B 92.5 99.9 99.3 99.4 \nequake C Seismic wave propagation simulation test 2.30M 478M 91.9 92.8 99.5 99.5 train 2.30M 13.0B 75.7 \n77.9 98.5 98.5 mesa C 3-D graphics library test 1.63K 57.6B 95.4 95.4 99.9 99.9 train 1.63K 62.1B 96.8 \n98.6 99.9 100 mgrid F77 Multi-grid solver: 3-D potential .eld test 10.0M 14.0B 89.8 97.1 99.6 100 train \n1.39M 17.9B 90.5 97.6 99.7 99.8 wupwise F77 Physics/Quantum Chromodynamics test 38.4M 5.09B 92.8 93.2 \n99.7 99.7 train 38.4M 24.9B 91.2 91.6 99.6 99.6 Average 91.8 94.1 99.3 99.4 Table 2. Approximation accuracy \nof data element and cache line reuse distance histograms (a) mcf element reuse histogram (b) mcf cache \nline reuse histogram Figure 3. The actual and estimated reuse distance histograms of mcf, the benchmark \nwith the largest approximation error. The X-axes are on log scale. [3] B. T. Bennett and V. J. Kruskal. \nLRU stack processing. IBM Journal of Research and Development, pages 353 357, 1975. [4] K. Beyls and \nE. D Hollander. Reuse distance as a metric for cache behavior. In Proceedings of the IASTED Conference \non Parallel and Distributed Computing and Systems, August 2001. [5] K. Beyls and E. D Hollander. Generating \ncache hints for improved program ef.ciency. Journal of Systems Architecture, 51(4):223 250, 2005. [6] \nK. Beyls and E. D Hollander. Discovery of locality-improving refactoring by reuse path analysis. In Proceedings \nof HPCC. Springer. Lecture Notes in Computer Science Vol. 4208, pages 220 229, 2006. [7] C. Cascaval, \nE. Duesterwald, P. F. Sweeney, and R. W. Wisniewski. Multiple page size modeling and optimization. In \nProceedings of International Conference on Parallel Architectures and Compilation Techniques, St. Louis, \nMO, 2005. [8] D. Chandra, F. Guo, S. Kim, and Y. Solihin. Predicting inter\u00adthread cache contention on \na chip multi-processor architecture. In Proceedings of the International Symposium on High Performance \nComputer Architecture (HPCA), 2005. [9] T. M. Chilimbi and M. Hirzel. Dynamic hot data stream prefetching \nfor general-purpose programs. In Proceedings of ACM SIGPLAN Conference on Programming Language Design \nand Implementation, Berlin, Germany, June 2002. [10] C. Ding and M. Orlovich. The potential of computation \nregrouping for improving locality. In Proceedings of SC2004 High Performance Computing, Networking, and \nStorage Conference, Pittsburgh, PA, November 2004. [11] C. Ding and Y. Zhong. Predicting whole-program \nlocality with reuse distance analysis. In Proceedings of ACM SIGPLAN Conference on Programming Language \nDesign and Implementation, San Diego, CA, June 2003. [12] C. Fang, S. Carr, S. Onder, and Z. Wang. Instruction \nbased memory distance analysis and its application to optimization. In Proceedings of International Conference \non Parallel Architectures and Compilation Techniques, St. Louis, MO, 2005. [13] S. A. Huang and J. P. \nShen. The intrinsic bandwidth requirements of ordinary programs. In Proceedings of the 7th International \nConferences on Architectural Support for Programming Languages and Operating Systems, Cambridge, MA, \nOctober 1996. [14] Y. H. Kim, M. D. Hill, and D. A. Wood. Implementing stack simu\u00adlation for highly-associative \nmemories. In Proc. ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, pages 212 \n213, May 1991. [15] Z. Li, J. Gu, and G. Lee. An evaluation of the potential bene.ts of register allocation \nfor array references. In Workshop on Interaction between Compilers and Computer Architectures in conjunction \nwith the HPCA-2, San Jose, California, February 1996. [16] C.-K. Luk et al. Pin: Building customized \nprogram analysis tools with dynamic instrumentation. In Proceedings of ACM SIGPLAN Conference on Programming \nLanguage Design and Implementation, Chicago, Illinois, June 2005. [17] G. Marin and J. Mellor-Crummey. \nCross architecture performance predictions for scienti.c applications using parameterized models. In \nProceedings of Joint International Conference on Measurement and Modeling of Computer Systems, New York \nCity, NY, June 2004. [18] R. L. Mattson, J. Gecsei, D. Slutz, and I. L. Traiger. Evaluation techniques \nfor storage hierarchies. IBM System Journal, 9(2):78 117, 1970. [19] F. Olken. Ef.cient methods for calculating \nthe success function of .xed space replacement policies. Technical Report LBL-12370, Lawrence Berkeley \nLaboratory, 1981. [20] X. Shen, J. Shaw, and B. Meeker. Accurate approximation of locality from time \ndistance histograms. Technical Report TR902, Computer Science Department, University of Rochester, 2006. \n[21] X. Shen, J. Shaw, B. Meeker, and C. Ding. Locality approximation using time. Technical Report TR901, \nComputer Science Department, University of Rochester, 2006. [22] X. Shen, Y. Zhong, and C. Ding. Locality \nphase prediction. In Proceedings of the Eleventh International Conference on Architect ural Support for \nProgramming Languages and Operating Systems (ASPLOS XI), Boston, MA, 2004. [23] A. J. Smith. On the effectiveness \nof set associative page mapping and its applications in main memory management. In Proceedings of the \n2nd International Conference on Software Engineering, 1976. [24] R. A. Sugumar and S. G. Abraham. Multi-con.guration \nsimulation algorithms for the evaluation of computer architecture designs. Technical report, University \nof Michigan, 1993. [25] Y. Zhong, S. G. Dropsho, X. Shen, A. Studer, and C. Ding. Miss rate prediction \nacross program inputs and cache con.gurations. IEEE Transactions on Computers, to appear. [26] Y. Zhong, \nM. Orlovich, X. Shen, and C. Ding. Array regrouping and structure splitting using whole-program reference \naf.nity. In Proceedings of ACM SIGPLAN Conference on Programming Language Design and Implementation, \nJune 2004.  \n\t\t\t", "proc_id": "1190216", "abstract": "Reuse distance (i.e. LRU stack distance) precisely characterizes program locality and has been a basic tool for memory system research since the 1970s. However, the high cost of measuring has restricted its practical uses in performance debugging, locality analysis and optimizations of long-running applications.In this work, we improve the efficiency by exploring the connection between time and locality. We propose a statistical model that converts cheaply obtained time distance to the more costly reuse distance. Compared to the state-of-the-art technique, this approach reduces measuring time by a factor of 17, and approximates cache line reuses with over 99% accuracy and the cache miss rate with less than 0.4% average error for 12 SPEC 2000 integer and floating-point benchmarks. By exploiting the strong correlations between time and locality, this work makes precise locality as easy to obtain as data access frequency, and opens new opportunities for program optimizations.", "authors": [{"name": "Xipeng Shen", "author_profile_id": "81452603368", "affiliation": "The College of William and Mary", "person_id": "PP39057352", "email_address": "", "orcid_id": ""}, {"name": "Jonathan Shaw", "author_profile_id": "81100396657", "affiliation": "Shaw Technologies", "person_id": "P831284", "email_address": "", "orcid_id": ""}, {"name": "Brian Meeker", "author_profile_id": "81322500688", "affiliation": "University of Rochester", "person_id": "P831278", "email_address": "", "orcid_id": ""}, {"name": "Chen Ding", "author_profile_id": "81309499457", "affiliation": "University of Rochester", "person_id": "PP43124106", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1190216.1190227", "year": "2007", "article_id": "1190227", "conference": "POPL", "title": "Locality approximation using time", "url": "http://dl.acm.org/citation.cfm?id=1190227"}