{"article_publication_date": "01-17-2007", "fulltext": "\n InterpronousPrograms* ceduralAnalysisofAsynchro Ranjit Jhala UC San Diego jhala@cs.ucsd.edu Abstract \nAn asynchronous program is one that contains procedure calls which are not immediately executed from \nthe callsite, but stored and dispatched in a non-deterministic order by an external sched\u00aduler at a later \npoint. We formalize the problem of interprocedu\u00adral data.ow analysis for asynchronous programs as AIFDS \nprob\u00adlems, a generalization of the IFDS problems for interprocedural data.ow analysis. We give an algorithm \nfor computing the precise meet-over-valid-paths solution for any AIFDS instance, as well as a demand-driven \nalgorithm for solving the corresponding demand AIFDS instances. Our algorithm can be easily implemented \non top of any existing interprocedural data.ow analysis framework. We have implemented the algorithm \non top of BLAST, thereby obtain\u00ading the .rst safety veri.cation tool for unbounded asynchronous programs. \nThough the problem of solving AIFDS instances is EXPSPACE-hard, we .nd that in practice our technique \ncan ef.\u00adciently analyze programs by exploiting standard optimizations of interprocedural data.ow analyses. \nCategories and Subject DescriptorsD.2.4 [Software Engineer\u00ading]: Software/Program Veri.cation. General \nTermsLanguages, Veri.cation, Reliability. Keywordsasynchronous (event-driven) programming, data.ow analysis. \n 1.Introduction Asynchronous programming is a popular and ef.cient program\u00adming idiom for managing concurrent \ninteractions with the environ\u00adment. In addition to the usual, or synchronous, function calls where the \ncaller waits at the callsite until the callee returns, asynchronous programs have asynchronous procedure \ncalls which, instead of be\u00ading executed from the callsite, are stored in a task queue for later execution. \nAn application-level dispatcher chooses a call from the task queue, executes it to completion (which \nmight lead to further additions to the task queue), and repeats on the remaining pending calls. Asynchronous \ncalls permit the interleaving of several logical units of work, and can be used to hide the latency of \nI/O-intensive * This research was sponsored in part by the research grants NSF-CCF\u00ad0427202, NSF-CNS-0541606, \nand NSF-CCF-0546170. Permission to make digital or hard copies of all or part of this work for personal \nor classroom use is granted without fee provided that copies are not made or distributed for pro.t or \ncommercial advantage and that copies bear this notice and the full citation on the .rst page. To copy \notherwise, to republish, to post on servers or to redistribute to lists, requires prior speci.c permission \nand/or a fee. POPL 07 January 17 19, 2007, Nice, France. Copyright c &#38;#169; 2007 ACM 1-59593-575-4/07/0001. \n. . $5.00. Rupak Majumdar UC Los Angeles rupak@cs.ucla.edu tasks by deferring their execution to a point \nwhere the system is not otherwise busy. They form the basis of event-driven pro\u00adgramming, where the asynchronous \ncalls correspond to callbacks that may be triggered in response to external events. Further, if mechanisms \nto ensure atomicity, either by using synchronization [24] or by using transactions [15, 29], are used \nto ensure asyn\u00adchronous calls are executed atomically, then the scheduler can be multi-threaded, running \ndifferent asynchronous calls concurrently on different threads or processors [32]. There have been a \nvari\u00adety of recent proposals for adding asynchronous calls to existing languages via libraries, such \nas LIBASYNC [20], LIBEVENT [21], and LIBEEL [6, 5]. These libraries have been used to build ef.cient \nand robust systems software such as network routers [19] and web servers [25]. Further, several recent \nlanguages such as NESC [12], a language for networked embedded systems, and MACE [23], a language to \nbuild distributed systems, provide explicit support for asynchronous calls. The .exibility and ef.ciency \nof asynchronous programs comes at a price. The loose coupling between asynchronously executed methods \nmakes the control and data dependencies in the program dif.cult to follow, making it harder to write \ncorrect programs. As asynchronous programs are typically written to provide a reliable, high-performance \ninfrastructure, there is a critical need for tech\u00adniques to analyze such programs to .nd bugs early or \nto discover opportunities for optimization. For programs that exclusively use synchronous function calls, \ninterprocedural data.ow analysis [31, 28] provides a general framework for program analysis. In the setting \nof [28], interproce\u00addural data.ow problem is formulated as a context-free reachability problem on the \nprogram graph, i.e., a reachability problem where the admissible paths in the graph form a context free \nlanguage of nested calls and returns. Unfortunately, this approach does not im\u00admediately generalize to \nasynchronous programs, for example, by treating asynchronous calls as synchronous. In fact, such an anal\u00adysis \nyields unsound results, because the facts that hold at the point where the asynchronous call is made \nmay no longer hold at the point where the stored call is .nally dispatched. Though the val\u00adues passed \nas parameters in the asynchronous call remain unaltered till the dispatch, the operations executed between \nthe asynchronous call and its dispatch may completely alter the values of the global variables. Further, \nthe pairing of asynchronous calls and their ac\u00adtual dispatches makes the language of valid program executions \na non-context free language, and a simple reduction to context free reachability seems unlikely. This \npaper formalizes the problem of data.ow analysis for asyn\u00adchronous programs as Asynchronous Interprocedural \nFinite Dis\u00adtributive Subset (AIFDS) problems, a generalization of the IFDS problems of Reps, Horwitz \nand Sagiv [28] to programs that addi\u00adtionally contain asynchronous procedure calls. The key challenge \nin devising algorithms to solve AIFDS problems precisely, that is, to compute the meet over all valid \npaths (MVP) solutions for such problems, lies in .nding a way to handle the unbounded set of pending \nasynchronous calls, in addition to the unbounded call stack. We surmount this challenge through three \nobservations. 1. ReductionWe can reduce an AIFDS instance into a stan\u00addard, synchronous data.ow analysis \nproblem where the set of data.ow facts is the product of the original set with a set of counters which \ntrack, for each of .nitely many kinds of pend\u00ading calls, the exact number of instances of the call that \nare pend\u00ading. Though the reduced instance has the same solution as the AIFDS instance, we cannot use \nstandard data.ow analyses to compute the solution as the lattice of data.ow facts is now un\u00adbounded: \nthe counters can grow unboundedly to track the num\u00adber of pending asynchronous calls. 2. ApproximationGiven \nany .xed parameter k . N, we can compute approximations of the meet-over-valid path solutions in the \nfollowing way. We compute an under-approximation of the in.nite reduced instance using a counter that \ncounts up to k, dropping any asynchronous call if there are already k pending instances for that call. \nWe call this problem the k-reduced IFDS problem. We compute an over-approximation of the in.nite reduced \ninstance using a counter that counts up to k,and bumps up to in.nity as soon as the value exceeds k. \nThis has the effect of tracking up to k pending calls precisely, and then supposing that an unbounded \nnumber of calls are pending if an additional asynchronous call is performed. We call this problem the \nk8\u00adreduced IFDS problem. For each k, both the over-and the under-approximations are instances of standard \ninterprocedural data.ow analysis as the abstraction of the counters makes the set of data.ow facts .nite. \nThus, we can compute over-and under-approximations of the precise solution of the AIFDS instance by running \nstandard interprocedural data.ow analysis algorithms [28]. 3. ConvergenceIn a crucial step, we prove \nthat for each AIFDS instance, there always exists a k for which the solutions of the over-approximate \nIFDS instance and the under-approximate IFDS instance coincide, thereby yielding the precise solution \nfor the AIFDS instance. Thus, our simple algorithm for com\u00adputing the meet over valid paths solutions \nfor AIFDS instances is to run an off-the-shelf interprocedural analysis on the k and k8-reduced IFDS \ninstances for increasingly larger values of k, until the two solutions converge upon the precise AIFDS \nsolu\u00adtion.  The proof of the third observation, and therefore, that our al\u00adgorithm is complete, proceeds \nin two steps. First, we demonstrate the existence of a .nite representation of the backward or inverse \nMVP solution of the in.nite reduced instance. To do so, we design a backward version of the algorithm \nof Reps, Horwitz and Sagiv [28] and prove that it terminates with the .nite upward-closed back\u00adwards \nsolution by using properties of well quasi-orderings [1, 10]. Second, we prove that if the backward solution \nis the upward clo\u00adsure of some .nite set, then there exists a k at which the solutions of the .nite k-and \nk8-reduced IFDS instances converge. Though the correctness proof uses some technical machinery, its details \nare entirely hidden from an implementer, who need only know how to instantiate a standard interprocedural \ndata.ow analysis framework. We have implemented this algorithm on top of the BLAST in\u00adterprocedural reachability \nanalysis which is a lazy version of the summary-based interprocedural reachability analysis of [28]. \nThe result is an automatic safety veri.er for recursive programs with unboundedly many asynchronous procedure \ncalls. Our reduction technique enables the reuse of optimizations that we have previ\u00adously found critical \nfor software veri.cation such as on-the-.y ex\u00adploration, localized re.nement [18], and parsimonious abstraction \n[17]. While we cannot hope for an algorithm that works ef.ciently for all asynchronous programs (the \nAIFDS problem is EXPSPACE- Figure1.An Example Plb hard, in contrast to IFDS which is polynomial time), \nour initial ex\u00adperiments suggest that in practice the forward reachable state space and the k required \nfor convergence is usually small, making the algorithm practical. In preliminary experiments, we have \nused our implementation to verify and .nd bugs in an open source load bal\u00adancer (plb) and a network testing \ntool (netchat). We checked for null pointer errors, buffer overruns, as well as application-speci.c protocol \nstate properties. In each case, our implementation ran in less than a minute, and converged to a solution \nwith k =1. RelatedWork.Recently, the reachability (and hence, data.ow analysis) problem for asynchronous \nprograms was shown decidable [30], using an algorithm that we believe will be dif.cult to imple\u00adment \nand harder to scale to real systems. First, the algorithm works backwards, thereby missing the opportunities \navailable for opti\u00admization by restricting the analysis to the (typically sparse) reach\u00adable states that \nwe have found critical for software veri.cation [18]. Second, one crucial step in their proof replaces \na recursive syn\u00adchronous function with an equivalent automaton constructed using Parikh s lemma [26]. \nThus, their analysis cannot be performed in an on-the-.y manner: the language-theoretic automaton construc\u00adtion \nmust be performed on the entire exploded graph which can be exponentially large in software veri.cation. \nFinally, instead of multiset rewriting systems and Parikh s lemma, our proof of com\u00adpleteness relies \non counter programs and a version of context free reachability on well quasi-ordered state spaces [10]. \nCounters [22] have been used to model check concurrent C [16] and Java programs, via a reduction to Petri \nNets [7]. However, those algorithms were not interprocedural and did not deal with recursion. Our proof \ntechnique of providing a forward abstraction\u00adbased algorithm whose correctness is established using a \nbackward algorithm was used in [16] and formalized for a general class of in.nite state systems in [13]. \nNotice that in contrast to the decidability of AIFDS, the data.ow analysis problem for two threads each \nwith recursive syn\u00adchronous function calls is undecidable [27]. This rules out similar algorithmic techniques \nto be applied to obtain exact solutions for multithreaded programs, or models in which threads and events \nare both present.  2.Problem Figure 1 shows an asynchronous program Plb culled from an event-driven \nload balancer. Execution begins in the procedure main which makes an asynchronous call to a procedure \n(omitted for brevity) that adds requests to the global request list r,and makes another asynchronous \ncall to a procedure reqs that processes the request list (highlighed by a .lled box). The reqs procedure \nchecks if r is empty, and if so, reschedules itself by asynchronously call\u00ading itself. If instead, the \nlist is not empty, it allocates memory for the .rst request on the list, makes an asynchronous call to \nclient which handles the request, and then (synchronously) calls itself (highlighted by the un.lled box) \nafter moving r to the rest of the list. The procedure client handles individual requests. It takes as \ninput the formal c which is a pointer to a client t structure. In the second line of client the pointer \nc is dereferenced, and so it is critical that when client begins executing, c is not null. This is ensured \nby the check performed in reqs before making the asyn\u00adchronous call to client. However, we cannot deduce \nthis by treat\u00ading asynchronous calls as synchronous calls (and using a standard interprocedural data.ow \nanalysis) as that would additionally con\u00adclude the unsound deduction that r is also not null when client \nis called. We shall now formalize the asynchronous interprocedural .nite data.ow analysis (AIFDS) framework, \na generalization of the IFDS framework of [28], solutions of which will enable us to soundly deduce that \nwhen client begins executing, c is non-null, but that r may be null. 2.1AsynchronousPrograms In the AIFDS \nframework, programs are represented using a gener\u00adalization of control .ow graphs, that include special \nedges corre\u00adsponding to asynchronous function calls. Let P be a .nite set of procedure names. An Asynchronous \nControl Flow Graph (ACFG) Gp for a procedure p . P is a pair (Vp,Ep)where Vp is the set of control nodes \nof the procedure p, including a unique start node vps and a unique exit node vpe,and Ep is a set of directed \nintraprocedural edges between the control nodes Vp, corresponding to one of the following: an operation \nedge corresponding to a basic block of assign\u00adments or an assume predicate derived from a branch condition, \n a synchronous call edge to a procedure q . P,or  an asynchronous call edge to a procedure q . P. \n For each directed call edge, synchronous or asynchronous, from v to v' we call the source node v the \ncall-site node, and the target node v' the return-site node. EXAMPLE 1: Figure 2 shows the ACFG for the \nprocedures main, reqs and client of the program Plb. For each procedure, the start node (resp. exit node) \nis denoted with a short incoming edge (resp. double circle). The labels on the intraprocedural edges \nare either operations corresponding to assumes (in box parentheses), and assignments, or asynchronous \ncall edges, shown in .lled boxes, e.g., the edge at v1, or synchronous call edges, shown in un.lled boxes, \nsuch as the recursive call edge at node v9, for which the call-site and return-site are respectively \nv9 and v10. 0 A Program G * comprises a set of ACFGs Gp for each proce\u00addure in p . P. The control locations \nof G * are V *, the union of the control locations of the individual procedures. The edges of G * are \nE *, the union of the (intraprocedural) edges of the individual procedures together with a special set \nE' of interprocedural edges de.ned as follows. Let Calls be the set of (intraprocedural) syn\u00adchronous \ncall edges in G *. For each synchronous call edge from call-site v to procedure q returning to return-site \nv' in Calls we have: An interprocedural call-to-start edge from the call-site v to the start node vqs \nof q, and,  An interprocedural exit-to-return edge from the exit node vqe of  ' qto the return-site \nv. As in [28], the call edges (or call-to-return-site edges) allow us to model local variables and parameter \npassing in our framework. In Figure 2, the dotted edges correspond to interprocedural edges. The edge \nfrom call-site v9 to the start node v4 of reqs is a call-to-start edge, and the edge from the exit node \nv10 to the return\u00adsite v10 is an exit-to-return edge. An Asynchronous Program is a program G * that contains \na special dispatch procedure main (with ACFG Gmain ), which is not called by any other procedure, and \nthat has, for every other procedure, a self-loop synchronous call edge from its exit node ee vmain to \nitself. The exit node vmain is called the dispatch node,the self-loop synchronous call edges of the dispatch \nnode are called dispatch call edges, the call-to-start edges from the dispatch node are called dispatch \ncall-to-start edges, and the exit-to-return edges to the dispatch node are called dispatch exit-to-return \nedges. Thus, an Asynchronous Program is a classical supergraph of [28] together with special asynchronous \ncall edges, and a special dispatch procedure that has synchronous call edges for each proce\u00addure, which \nare used to model asynchronous dispatch. EXAMPLE 2: The ACFG for main shown in Figure 2 is the dispatch \nprocedure for Plb. The exit node v3, shaded in blue, is the dispatch node with dispatch edges to reqs \nand client.The interprocedural edge from v3 to v12 is a dispatch call-to-start edge to client and the \nedge from v15 to v3 is a dispatch exit-to-return edge. 0  2.2AsynchronousProgramPaths Executions of \nan asynchronous program correspond to paths in the ACFGs. However, not all paths correspond to valid \nexecutions. In addition to the standard requirement of interprocedural validity, namely that synchronous \ncalls and returns match up, we require that a dispatch can take place only if there is a pending asynchronous \ncall to the corresponding procedure. Paths.A path of length nfrom node vto v' is a sequence of edges \np =(e1,...,en)where vis the source of e1, v' is the target of en, and for each 0= k = n- 1, the target \nof ek is the source of ek+1. We write p(k)to refer to the kth edge of the path p. InterproceduralValidPaths.Suppose \nthat each call edge in Calls is given a unique index i. For each call edge i. Calls suppose that the \ncall-to-start edge is labeled by the symbol (i and the exit-to\u00adreturn edge is labeled by the symbol )i. \nWe say that a path p from vto v' is an interprocedural valid path if the sequence of labels on the edges \nalong the path is a string accepted by the following Dyck language, generated by the non-terminal D: \nM . E | M (i M )i for each i . Calls D . M | D (i M for each i . Calls ' We use IVP(v,v)to denote the \nset of all interprocedural valid ' paths from v to v. Intuitively, M corresponds to the language of perfectly \nbalanced parentheses, which forces the path to match the return edges to the corresponding synchronous \ncall sites, and D allows for some procedures to remain on the call stack.  Figure2.ACFGs for Plb Unlike \nin synchronous programs, not all Dyck paths correspond to potential executions of the Asynchronous Program, \nas we have not accounted for asynchronous procedure calls. For example, the path along the edges between \nnodes v0,v1,v2,v3,v12 of the ACFGs of Figure 2 is a valid interprocedural path, but does not correspond \nto a valid asynchronous execution as there is no pending asyn\u00adchronous call to client at the dispatch \nnode v3. To restrict analy\u00adses to valid asynchronous executions, we use schedules to map dis\u00adpatch call-to-start \nedges on paths to matching prior asynchronous call edges. Schedules.Let p be a path of length n.We say \ns : N . N is a schedule for piff sis one-to-one, and for each 0 = k = n,if p(k) is a dispatch call-to-start \nedge to procedure p, then: 0 = s(k)<k, and,  the edge p(s(k)) is an asynchronous call to procedure \np.  Intuitively, the existence of a schedule implies that at each syn\u00adchronous dispatch of procedure \np at step k, there is a pending asynchronous call to p made in the past, namely the one on the s(k)-th \nedge of the path. The one-to-one property of sensures that the asynchronous call is dispatched only once. \nThere are no asyn\u00adchronous executions corresponding to interprocedural paths that have no schedules. \nEXAMPLE 3: Figure 3 shows a path of Plb, abbreviated to show only the asynchronous call edges and synchronous \ncall-to-start edges. Ignore the boxes with the numbers on the left and the right for the moment. For \nthe pre.x comprising all but the last edge, there are two schedules indicated by the arrows on the left \nand right of the path. Both schedules map the dispatch call-to-start edge 2 to the asynchronous call \nat edge 1. The left (right) schedule maps the dispatch call-to-start edges 8,9 to the asynchronous calls \nat 5,3 respectively (3,5 respectively). If we include the last edge, there is no schedule as there are \nthree dispatch call-to-start edges to client but only two asynchronous calls, and so, by the pigeonhole \nprinciple there is no one-to-one map. 0  2.3AsynchronousIFDS An instance of a data.ow analysis problem \nfor asynchronous pro\u00adgrams can be speci.ed by .xing a particular asynchronous pro\u00adgram, a .nite set of \ndata.ow facts, and for each edge of the pro\u00adgram, a distributive transfer function that given the set \nof facts that hold at the source of the edge, returns the set of facts that hold at the target. AIFDSInstance.An \ninstance A of an asynchronous interprocedu\u00adral .nite distributive subset problem or (AIFDS problem), \nis a tuple A =(G * ,Dg ,Dl,M,n),where: 1. G * is an asynchronous program (V * ,E * ), 2. Dg,Dl are .nite \nsets, respectively called global and local data.ow facts we write D for the product Dg \u00d7 Dl which we \ncalled the data.ow facts, 3. M : E * . 2D . 2D maps each edge of G * to a distributive data.ow transfer \nfunction, 4. . is the meet operator, which is either set union or intersection.  Unlike the classical \nformulation for synchronous programs (e.g. [28]), the asynchronous setting requires each data.ow fact \nto be ex\u00adplicitly split into a global and a local component. This is because at the point where the asynchronous \ncall is made, we wish to capture, in addition to which call was made, the initial input data.ow fact \nre\u00adsulting from the passing of parameters to the called procedure. We cannot use a single global set \nof facts to represent the input con.g\u00aduration, as operations that get executed between the asynchronous \nFigure3.Path showing a sequence of asynchronous posts (in shaded boxes) and synchronous calls (in un.lled \nboxes). Two dif\u00adferent schedules are shown using the arrows from dispatch call-to\u00adstart edges to asynchronous \ncall points. call and the actual dispatch may change the global fact, but not the local fact. For example, \nin Plb (Figure 1), at the point where the asyn\u00adchronous call to client is made, the global pointer r \nis not null, but this fact no longer holds when client begins executing after a subsequent dispatch. \nHowever, the local pointer c passed via a parameter cannot be changed by intermediate operations, and \nthus, is still not null when client begins executing after a subsequent dispatch. Thus, our data.ow facts \nare pairs of global facts Dg and local facts Dl. By separating out global and local facts, when dispatching \na pending asynchronous call, we can use the current global fact together with the local fact from the \nasynchronous call to which the schedule maps the dispatch. EXAMPLE 4: The following is an example of \nan AIFDS instance. G * is the asynchronous program of Figure 2, Dg is the set {r,r\u00af}that respectively \nrepresent that the global pointer r is de.nitely not null and r may be null, and Dl is the set {rc,rc,c,c\u00af} \nthat respectively represent that the local pointer rc is de.nitely not null, rc may be null, c is de.nitely \nnot null and c may be null. We omit the standard transfer functions for these facts for brevity. Thus, \nthe pair (\u00afr,c)is the data.ow fact representing program states where r may be null, but c is de.nitely \nnot null. 0 PathFunctions.Let A =(G * ,Dg ,Dl,M,n) be an AIFDS instance. Given an interprocedural valid \npath p,we de.ne a path relation PR(A)(p). D\u00d7 D that relates data.ow facts that hold before the path to \nthose that hold after the operations along the path are executed. Formally, given an interprocedural \nvalid path p =(e1,...,en)from v to v ' we say that (d,d ' ) . PR(A)(p) if there exists a schedule s for \np and a sequence of data .ow facts d0,...,dn such that, d= d0, d ' = dn and, for all 1 = k = n: if ek \nis an asynchronous call edge, then dk = dk-1,  if ek is a dispatch call-to-start edge, then dk =(dg,dl)where \n dk-1 =(dg,\u00b7)and (\u00b7,dl). M(es(k))(ds(k)) otherwise dk . M(ek)(dk-1). We de.ne the distributive closure \nof a function f as the func\u00adtion: .S..x.S f(x). The path function is the distributive closure of: PF \n(A)(p)= .d.{d ' | (d,d ' ). PR(A)(p)} As a path may have multiple schedules, the path relation is de\u00ad.ned \nas the union of the path relation for each possible schedule, which, in turn is de.ned by appropriately \ncomposing the trans\u00adfer functions for the edges along the path as follows. We directly compose the transfer \nfunctions for the edges that are neither asyn\u00adchronous calls nor dispatch call-to-start edges. We defer \napplying the transfer function for asynchronous call edges until the matching dispatch call-to-start \nedge is reached. For each call-to-start edge, we use the given schedule to .nd the matching asynchronous \ncall edge. The global data.ow fact after the dispatch is the global fact just before the dispatch. The \nlocal fact after the dispatch, is obtained by applying the transfer function for the matching asynchronous \ncall edge to the data.ow fact just before the matching asynchronous call was made. EXAMPLE 5: Figure \n4 shows a path of the program Plb, together with the data.ow facts obtained by applying the path function \non the pre.x of the path upto each node. At the start of the .rst call to reqs, the global r and the \nlocal rc may both be null. After the .rst check, at v5, we know that r is de.nitely not null, hence the \nglobal fact is r. Similarly after the malloc and the subsequent check, the local fact at v7 is rc, i.e., \nrc is not null. After the subsequent assignment to r, it may again become null, hence the global fact \nis r\u00af. Note that at v7 where the asynchronous call to client is made, rholds, but not at v3 just before \nthe dispatch call to client.There is a single schedule for this path, that maps the dispatch edge from \nv3 to v12 to the asynchronous call edge from v7 to v8. Thus, the global fact at v12 is the same as at \nthe previous dispatch location, namely r\u00af,that r may be null. The local fact at v12 is obtained by applying \nthe transfer function of the matching asynchronous call to the data.ow fact (r,rc)that held at the matching \nasynchronous call site at v7. As the call passes the local rc as the formal c, the local fact is c, i.e., \nc is not null. 0 AIFDSSolutions.Let A =(G,Dg,Dl,M,n) be an AIFDS instance. The meet over all valid paths \n(MVP) solution to A is a map MVP(A): V * . 2D,de.ned as: s MVP(A)(v)= np.IVP(vmain ,v)PF(A)(p)(T) Thus, \ngiven an AIFDS instance A, the problem is to .nd an algo\u00adrithm to compute the MVP solution for A. If \na path has no schedule, then its path relation is empty, and so its path function maps all facts to .. \nThus, the MVP solution only takes into account paths that correspond to a valid asynchronous executions. \n 3.Algorithm There are two problems that any precise interprocedural analysis for asynchronous programs \nmust solve. First, it must keep track of the unbounded set of pending asynchronous calls in order to \nonly consider valid asynchronous program executions. Second, it must .nd a way to determine the local \ndata.ow facts corresponding to the input parameters, that hold after a dispatch call-to-start edge. \n Figure4.A path of the program Plb. The rectangles denote the data.ow facts obtained by applying the \npath function on the pre.x of the paths upto each node. The shaded grey box is the global fact, and the \nunshaded box the local fact at each point. To reduce clutter, we show the facts at nodes where they differ \nfrom the facts at the predecessor. This is challenging because these local facts are the result of ap\u00adplying \nthe transfer function to the data.ow facts that held at the point when the matching asynchronous call \nwas made, which may be unboundedly far back during the execution. Our approach to solving both these \nproblems is to reduce an AIFDS instance into a standard IFDS instance by encoding the pending asynchronous \ncalls inside the set of data.ow facts, by tak\u00ading the product with a new set of facts that count how \nmany asyn\u00adchronous calls to a particular function, with a given input data.ow fact are pending. However, \nas the pending set is unbounded, this new set of facts is in.nite, and so we cannot directly solve the \nin\u00adstance. Instead, we abstractly count the number of facts, thus yield\u00ading a .nite instance, and then \nuse the standard IFDS algorithm to obtain a sequence of computable under-and over-approximations of the \nexact AIFDS solution, which we prove, is guaranteed to converge to the exact solution. We .rst recall \nthe standard (syn\u00adchronous) Interprocedural Data.ow Analysis framework and then describe our algorithm. \nSolvingSynchronousIFDSInstances.A Synchronous Data.ow Analysis problem instance (IFDS [28]) is a tuple \nI = (G * ,D,{T},M,n)that is a special case of an AIFDS instance, where: 1. the program G * has no asynchronous \ncall edges, 2. there is a single global set of data.ow facts D.  For any valid interprocedural path \nfrom v to v ' all schedules are trivial as there no dispatch call edges. The MVP solution for an IFDS \ninstance I can be computed by using the algorithm of [28] that we shall refer to as RHS. THEOREM 1. [AlgorithmRHS \n[28]]For every IFDS instance I = (G * ,D,{T},M,n), we have RHS(I )= MVP(I ). Counters.A counter C is \na contiguous subset of N .{8}.We assume that 8. C whenever the counter C is an in.nite subset. For a \ncounter C, and a natural number n . N, maxC (n)is n if n . C and max C otherwise, and minC (n)is n if \nn . C and min C otherwise. For a map f, we write f[s .v]for the new . map: .x.if x= sthen v else f(x) \nA counter map f is a map from some set S to a counter C.For any s. S, we write f +C sfor the counter \nmap: f[s . . maxC (f(s)+1)] and we write f -C sfor the map: f[s . . minC (f(s)- 1)] Note that both f+C \nsand f-C sare maps from Sto C. Intuitively, we think of f+C s(resp. f-C s) as adding (resp. removing \n) an sto (resp. from) f. We de.ne the counter C8 as the set N .{8}, and for any k = 0, the counter Ck \nas {0,...,k}, and the counter Ck 8 as {0,...,k,8}. We write c0 for the counter map .s.0.A C8 counter \nmap tracks the exact number of sin f.A Ck counter map tracks the exact number of s in f upto a maximum \nvalue of k, at which point it ignores subsequent additions. A Ck 8 counter map tracks the exact number \nof sin f upto a maximum of k after which a subsequent increment results in the map getting updated to \n8, which remains, regardless of the number of subsequent removals. 3.1AlgorithmADFA We now present our \nAlgorithm ADFA for computing the MVP solution of AIFDS instances. The key step of the algorithm is the \nuse of counter maps to encode the set of pending asynchronous calls inside the data.ow facts, and thereby \nconverting an AIFDS instance into an IFDS instance. GivenanAIFDS instance A =(G * ,Dg,Dl,M,n),and a counter \nC we de.ne the C-reduced IFDS instance as the tuple (GC * ,DC ,{T},MC ,nC )where: GC * is obtained by \nreplacing each asynchronous call edge in G * with a fresh trivial operation edge between the same source \nand target node,  DC is the set (Dg \u00d7 Dl)\u00d7 (P \u00d7 Dl . C). The elements of the set are pairs (d,c)where \nd is a data.ow fact in Dg \u00d7 Dl and cis a counter map that tracks, for each pair of asynchronous call \nand input data.ow fact, the number of such calls that are pending.  MC is de.ned on the new data.ow \nfacts and edges as follows.  if e is an asynchronous call edge to p in G * then MC (e)(d,c)= {(d,c+C \n(p,d l' )) | (\u00b7,dl' ). M(e)(d)} if e is a dispatch call to start edge to p ' in G * ' then ' MC (e)(d,c) \n= {((dg,dl),c-C (p,d l)) | c(p,d l)>0,d =(dg,\u00b7)} otherwise MC (e)(d, c)={(d ' ,c)| d ' . (M(e)(d)}. \nnC is the union (resp. intersection) operation if n is the union (resp. intersection) operation. Intuitively, \nthe reduced transfer function for an asynchronous call adds the pair of the called procedure and the \ninitial local data.ow fact to the counter map. For a dispatch call-to-start edge to procedure p, the \ntransfer function returns the set of tuples of the current global data.ow fact together with those local \nfacts dl for which the counter map of (p, dl)is positive, together with the countermaps where the pairs \n(p, dl)have been removed. If for all pairs (p, \u00b7)the counter map value is zero, then the transfer function \nreturns the empty set, i.e. .. EXAMPLE 6: Figure 4 shows a path of the C8-reduced instances of Plb. On \nthe left of each (intraprocedural) path, we show the data.ow facts resulting from applying the path function \nto the pre.x of the path upto each corresponding node. The shaded box contains the global data.ow fact, \nthe white box the local fact, and the numbers i, j, k on top represent the counter map values for (reqs, \nT), (client,c),and (client,c\u00af)respectively. For all other pairs, the counter map is always zero. Note \nthat the value for (reqs, T)increases after the asynchronous call at v1, decreases after the dispatch \nat v3 and again increases after the asynchronous call at v11. At the second occurrence of v3 (the dispatch \nlocation), (client,c)is the only pair with client as the .rst parameter, for which the counter map value \nis positive. Thus, after the dispatch, the data.ow fact is the pair of the global r\u00affrom the dispatch \nlocation and the local c from the counter map. 0 Our .rst observation is that the MVP solution of the \nC8 \u00adreduced instance is equivalent to the MVP solution of the original AIFDS instance. This is because \nthe C8-reduced instance exactly encodes the unbounded number of pending asynchronous call and initial \nlocal fact pairs within the counter maps of the data.ow facts. Thus, for any interprocedural valid path \nthe (reduced) path function returns the union of the set of data.ow facts resulting from every possible \nschedule. For two sets s . B \u00d7 D and s ' . B \u00d7 D, we say that s=.s ' (resp. s..s ' )if {b | (b, \u00b7). s} \nis equal to (resp. included in) the {b ' | (b ' , \u00b7). s ' }. For two functions f : A . 2B\u00d7D and ' B\u00d7D' \nf : A . 2,we say f=.g (resp. f..g)if for all x,the set f(x).=f ' (x)(resp. f(x)..f ' (x)). THEOREM 2. \n[CounterReduction]For every AIFDS instance A, if I is the C8-reduced instance of A,then MVP(I ).=MVP(A). \nUnfortunately, this reduction does not directly yield an algo\u00adrithm for solving AIFDS instances, as the \nC8-reduced instance has in.nitely many data.ow facts, due to the in.nite number of possible counter maps. \nOur second observation is that we can generate .nite IFDS instances that approximate the C8-reduced instance \nand thus, the original AIFDS instance. In particular, for any k,the Ck-reduced and Ck 8 instances are, \nrespectively, an under-approximation and an over-approximation of the C8-instance. In the Ck-reduced \nIFDS instance, the path function returns . for any path along which there are k +1 (or more) successive \ndispatches to some function starting with some given local fact. This happens as because the number of \ntracked pending calls never rises above k,after the k successive dispatches, the map value must be zero, \nthus the k +1-th call yields a .. Thus, the MVP solution for the Ck-reduced instance is an underapproximation \nof the exact AIFDS solution that includes exactly those paths along which there are at most k successive \ndispatches to a particular procedure with a given local fact. Dually, in the Ck 8-reduced IFDS instance, \nonce a k +1-th pending call is added for some procedure, the counter map is updated to 8 (instead of \nk +1). As a result, from this point on, it is always possible to dispatch a call to this procedure. Thus, \nthe MVP solution for the Ck 8-reduced instance is an over-approximation of the exact AIFDS solution that \nincludes all the valid paths of the AIFDS instance, and also extra paths corresponding to those executions \nwhere at some point there were more than k pending calls to some procedure. EXAMPLE 7: Figure 3 illustrates \nhow the C1-reduced instance and the C1 8-reduced instance are respectively under-and over\u00adapproximations \nof the C8-reduced IFDS instance of Plb. Suppose that Dg and Dl are singleton sets containing T.Onthe \nleft and right we show the sequence of data.ow facts obtained by applying the path functions for the \nC1 and C1 8 respectively, on the pre.x of the operations upto that point on the path. The numbers i, \nj above the boxes indicate the counter map value for (reqs, T)and (client, T)respectively. As each asynchronous \ncall is made, the counter map for the corresponding call is updated, and for each dispatch call, the \nvalue is decremented. In the C1-reduced instance (left), the second asynchronous call to client is dropped, \ni.e., the counter is not increased above 1,and thus, the second dispatch to client results in .. Thus, \nthe effect of this path is not included in the (under-approximate) MVP solution for the C1-reduced instance. \nIn the C1 8-reduced instance (right), the second asynchronous call results in the counter for client \nis increased to 8. Thus, in this instance, the second dispatch to client yields a non-. data.ow fact. \nMoreover, any subsequent dispatch yields a non-. value, all of which get included in the (over-approximate) \nMVP solution for the IFDS instance. 0 THEOREM 3. [Soundness]For every AIFDS instance A, for every k = \n0,if I ,Ik,Ik 8 are respectively the C8-reduced, Ck-reduced and Ck 8-reduced IFDS instances of A, then: \n(a) MVP(Ik)..MVP(I )MVP(Ik 8) (b) MVP(Ik)..MVP(Ik+1) (c) MVP(I 8 .MVP(I 8)  k+1).k The proof of the \nsoundness Theorem 3, follows by observing that the Ck -(resp. Ck 8-) instance effectively only considers \na subset (resp. superset) of all the valid asynchronous executions, and for each path for which both \nthe AIFDS path function and the reduced instance s path function return a non-. value, the value s returned \nby the two are identical. As for each k, the counters Ck and Ck 8 are .nite, we can use RHS to compute \nthe MVP solutions for the .nite IFDS instances Ik and Ik 8, thereby computing under-and over-approximations \nof the MVP solution for the AIFDS instance. Our algorithm ADFA (shown in Algorithm 1) for computing the \nMVP solution for an AIFDS instance A is to compute succes\u00adsively more precise under-and over-approximations. \nAn immedi\u00adate corollary of the soundness theorem is that if we .nd some k for which the under-and over-approximations \ncoincide, then the approximations are equivalent to the solution for the C8-reduced instance, and hence, \nthe exact MVP solution for A. The next theo\u00adrem states that for every AIFDS instance, there exists a \nk for which the under-and over-approximations coincide, and therefore, the Al\u00adgorithm ADFA is guaranteed \nto terminate. THEOREM 4. [Completeness]For each AIFDS instance A there exists a k such that, if Ik and \nIk 8 are respectively the Ck -and Ck 8-reduced IFDS instances of A,then MVP(Ik).=MVP(Ik 8) This Theorem \nfollows from the following lemma. Algorithm1Algorithm ADFA Input:AIFDS instance A Output:MVP solution \nfor A k =0 repeat k = k +1 Ik = Ck-reduced IFDS instance of A Ik 8 = Ck 8-reduced IFDS instance of A \nuntilRHS(Ik).=RHS(Ik 8) returnRHS(Ik) LEMMA 1. [PointwiseCompleteness]Let A = (G * ,Dg ,Dl,M, n) be an \nAIFDS instance, and I be the C8-reduced IFDS instance of A. For every d . Dg \u00d7 Dl and v . V *, there \nexists a kd,v . N such that for all k = kd,v, .ck s.t. (d, ck). MVP(Ik)(v)iff .c s.t. (d, c). MVP(I )(v)iff \n.c 8 s.t. (d, c8). MVP(I 8)(v). kk k To prove Theorem 4 we pick any k greater than maxd,v kd,v (this \nis well de.ned since D and V * are .nite sets). Thus, the crux of our completeness result is the proof \nof Lemma 1 which we postpone to Section 5. THEOREM 5. [CorrectnessofADFA]For every AIFDS instance A, \nAlgorithm ADFA returns MVP(A). The proof follows from Theorems 1,3,4. 3.2Demand-drivenAIFDSAlgorithm \nWe now present an algorithm for solving a Demand-AIFDS prob\u00adlem. This algorithm works by invoking a standard \nDemand-IFDS Algorithm on Ck-and Ck 8-reduced IFDS instances of the AIFDS instance. Demand-AIFDSInstance.An \ninstance A of a Demand AIFDS problem is a pair (A,vE ) where A is an AIFDS instance, and vE is a special \nquery node of the supergraph of A. Given a De\u00admand AIFDS instance, the Demand-AIFDS problem is to determine \nwhether MVP(A)(vE ) =.. Demand-IFDSandDemRHS.We de.ne a Demand-IFDS In\u00adstance as an AIFDS instance (I \n,vE ) where I is an IFDS in\u00adstance. Let DemRHS be a Demand-IFDS Algorithm such that DemRHS(I ,vE )returns \nTRUE iff MVP(I )(vE ) =.. To solve a Demand-AIFDS problem, we use Ck-and Ck 8\u00adreduced under-and over-approximations \nas before. Only, instead of increasing k until the under-and over-approximations coincide, we increase \nit until either: 1. in the under-approximation (i.e.,the Ck-reduced IFDS in\u00adstance), the MVP solution \nis not ., in which case we can de\u00adduce from Theorem 3 that the exact AIFDS solution is also not ., or \ndually, 2. in the over-approximation (i.e.,the Ck 8-reduced IFDS in\u00adstance), the MVP solution is ., \nin which case we deduce from Theorem 3 that the exact AIFDS solution is also ..  The completeness theorem \nguarantees that this demand-driven al\u00adgorithm DemADFA (summarized in Figure 2) terminates. THEOREM 6. \n[CorrectnessofDemADFA]For each Demand-AIFDS instance (A,vE ), DemADFA terminates and returns TRUE if \nMVPA(vE ) =. and FALSE otherwise. Though we would have liked polynomial time algorithms for solving AIFDS \nand Demand-AIFDS problems, the following re\u00adsult (also in [30]), that follows by reduction from reachability \nof structured counter programs [11], shows that this is impossible. Algorithm2Algorithm DemADFA Input:AIFDS \ninstance A, Error node vE Output:SAFE or UNSAFE k =0 loop k = k +1 Ik = Ck-reduced IFDS instance of A \nIk 8 = Ck 8-reduced IFDS instance of A ifDemRHS(Ik)(vE ) TRUE =. thenreturnifDemRHS(Ik 8)(vE )=. thenreturnFALSE \nTHEOREM 7. [EXPSPACE-Hardness]The Demand-AIFDS problem is EXPSPACE-hard, even when there are no recursive \nsynchronous calls. 3.3Optimizations We now describe two general optimizations that can be applied to \nany AIFDS instance that reduce the number of states explored by the analysis. 1. EffectiveCountingThe \n.rst optimization is based on two obser\u00advations. First, the dispatch node is the only node where the \ncounter maps are read (have any effect on the transfer function). At other nodes, the counter map is \neither added to (for some asynchronous calls), or copied over. Thus, rather than exactly propagating \nthe counter maps in the data.ow facts, we need only to summarize the effect of a (synchronous) dispatch \non the counter map, and use the summaries to update the counter maps after each dispatch call re\u00adturns \nto the dispatch location. Second, between the time a dispatch call begins and the time it returns, the \ncounter map values only in\u00adcrease due to asynchronous calls that may happen in the course of the dispatch. \n Thus, we summarize the effect of a dispatch on the counter map as follows. Suppose that the counter \nmap at a (synchronous) callsite is c. For a call-to-start edge to procedure p, for each entry data.ow \nfact for p,we reset the counter map to c0 (all zeros) and only compute the data.ow facts reachable from \nsuch reset con.gurations. For each summary edge [28] for p with the target counter map c ' , we propagate \nthe summary edge at the callsite, by updating the counter map to: .x.maxC (c(x)+ c ' (x)),where C is \nthe counter being used in the reduced instance. The saving from this optimization is that for each procedure, \nfor each entry data.ow fact, we only compute summaries starting from the single reset counter map c0, \nrather than upto |C||Dl||P | distinct counter maps. 2. CounterMapCoveringThe second optimization follows \nfrom observing that there is a partial order between the counter maps. For two counter maps c, c ' , \nwe say that c = c ' if for all s,we have c(s) = c ' (s). It is easy to check that if c = c ' , then for \nany instance I , for all paths p, for all data.ow facts d . Dg \u00d7 Dl, the PF (I )(p)(d, c)..PF (I )(p)(d, \nc ' ). This implies that we only need to maintain maximal elements in this ordering. Thus, the set of \nfacts reachable from c is covered by the facts reachable from c ' , and so in our implementation of RHS, \nwhen we .nd two instances of the dispatch location in the worklist, with facts (d, c)and (d, c ' ) with \nc = c ' , we drop the former instance from the worklist.   4.Application:SafetyVeri.cation We now \ndescribe how the ADFA algorithm can be applied to the task of safety veri.cation, i.e., determining whether \nin a given asynchronous program, some user-speci.ed error location vE is reachable. EXAMPLE 8: Figure \n5 shows a typical idiom in asynchronous programs where different clients attempt to write .les to a device. \n Figure5.Example Race The main function spawns an asynchronous listen procedure that is nondeterministically \ncalled every time a new client joins on a socket. The procedure then calls new client with a unique gid \nor group id [6] which processes the request of the individual clients. A critical mutual exclusion property \nin such programs is that once a client, represented by its gid, has acquired and thus begun writing to \nthe device, no other client should be given access until the .rst client is .nished. To ensure mutual \nexclusion, many asynchronous programs use state-based mechanisms like that in Race. The device is stamped \nwith an owner .eld that tracks the last gid that wrote to the device, and a client is granted access \nif the owner .eld is 0, indicating there is no current client writing to the device. To verify the mutual \nexclusion, we encode the property as an assertion by creating a (skolem) constant k that represents some \narbitrary client id, and checking the assertion that whenever the device is written to in write, that \nthe id of the writer is k,then the owner of the device is also k. Thus, the program satis.es the mutual \nexclusion property iff the error location corresponding to the label ERR is not reachable. 0 To perform \nsafety veri.cation, we instantiate the general AIFDS framework with data.ow facts and transfer functions \nderived via predicate abstraction [2, 14]. The result is a Demand AIFDS in\u00adstance that we solve using \nthe DemADFA algorithm. If the MVP solution for the error node is ., then we can deduce that the error \nlocation is not reachable. If the solution is not ., then either the error location is reachable, or \nthe set of predicates is too impre\u00adcise, and we automatically learn new predicates from the infeasible \ncounterexample whose path function is not ., using the technique of [17]. We then repeat the veri.cation \nwith the new predicates, until we .nd an execution that reaches the error location, or the location is \nproven to be unreachable [4, 3, 18]. We now describe how to generate Demand AIFDS instances for a safety \nveri.cation problem by describing the corresponding AIFDS instances. 4.1PredicateAbstractionAIFDSInstances \nA Predicate Abstraction AIFDS instance is a tuple A = (G * ,Dg ,Dl,M,n),where: G * is an asynchronous \nprogram,  Dg is a .nite set of global predicates, i.e., predicates over the global program variables, \n Dl is a .nite set of local predicates, i.e., predicates over the local program variables,  M(e)is \nthe de.ned as the distributive closure of: '' '' .(dg,dl).{(dg,dl)|sp(e,dg . dl). dg . dl is satis.able \n} where sp(e,.)is the strongest postcondition [9] of .w.r.t. the operation e, n is the set union operator. \nThis is slightly different from the standard formulation of predicate abstraction [14], where the elements \nof Dg and Dl are all the possible cubes over some set of atomic predicates. We can generate an AIFDS \ninstance ARace for the safety ver\u00adi.cation problem for Race as follows. The set of global predi\u00adcates \nis {ow =0,ow >0. ow =k,ow >0. ow =k},where ow is an abbreviation for dev.owner, and the set of local \npredicates is {gid >0,gid =k,gid =k,id =k}. With =k,id these predicates, for example, the transfer function \nfor the edge dev.owner =0 is the distributive closure of .(dg,dl).(ow = 0,dl), i.e., the global predicate \nbecomes ow =0and the local predicate remains unchanged. Figure 6 shows the result of the optimized Demand \nIFDS anal\u00adysis for the C1 8 reduced IFDS instance of ARace . The grey box contains the global predicate \nand the white box the local predicate. The numbers i,j,k,labove the boxes correspond to the counter map \nvalues for (listen,T),(new client,gid > 0),(write,id = k) and (write,id = k)respectively. Execution begins \nin main, with no pending asynchronous calls, and proceeds to the dispatch location where the global predicate \now =0holds, and the only pending call is to listen.We analyze listen, the only pending call from the \ndispatch call site 1, from the counter map mapping all predicates to zeros (pstands for any of the global \npredicates). The exploded supergraph for listen shows that an execution of listen preserves the global \ndata.ow fact, makes an asynchronous call to listen, and may, if the call to new gid is successful (i.e., \nreturns a positive gid), make an asynchronous call to new client with a positive argument. We plug in \nthe summary edges from listen into the dispatch call site 1 the results are shown with the dotted edges \nlabeled L. For each generated (i.e., exploded ) instance of the dispatch call site, we compute the results \nof dispatching each possible pend\u00ading asynchronous call (together with the input data.ow fact). Thus, \nat the dispatch call site instance 2, there are pending calls to listen and new client. Plugging in \nthe summaries for listen,we de\u00adduce that the result is either a self loop back to dispatch call site \n2, or, if another asynchronous call to new client is made, then a dotted summary edge to dispatch callsite \n3where there are 8 calls pending on new  client because the actual value 2obtained by adding the effect \n1to the previous counter map value at the call site gets abstracted to 8 in the C1 8 reduction. Similarly, \nwe plug in the summaries for new  client and write (shown in the respective boxes), for each of the \n.nitely many dis\u00adpatch call site instances, resulting in the successors corresponding to dotted edges \nlabeled N and W respectively. The call site instances 3,5are covered by the instances 6,7respectively, \nand so we do not analyze the effects of dispatches from 3,5.  Notice that new client is always called \nwith a positive argu\u00adment, and that write is only called either when both id and owner are equal to k \nor when neither is equal to k, and so the mutual exclusion property holds.  4.2Experiences We have implemented \nthe DemADFA algorithm along with these optimizations in BLAST[18], obtaining a safety veri.cation tool \nfor recursive programs with asynchronous procedure calls. In our ex\u00adperiments, we checked several safety \nproperties of two event driven programs written using the LIBEEL event library [6]. These pro\u00adgrams were \nported to LIBEEL from corresponding LIBEVENT pro\u00ad  Figure6.Summaries for Race grams, and are available \nfrom the LIBEVENT web page [21]. plb is a high performance load balancer (appx 4700 lines), and nch is \na network testing tool (appx 684 lines). We abstract the event registration interface of LIBEEL in the \nfollowing way. We assume that external events can occur in any order, and thus, the registered callbacks \ncan be executed in any order. In particular, this means that we abstract out the actual times for callbacks \nthat are .red on some timeout event. With this abstraction, each event registra\u00adtion call in LIBEEL becomes \nan asynchronous call that posts the callback. While the predicate discovery procedure implemented in \nBLAST[17] is not guaranteed to .nd well-scoped predicates in the presence of asynchronous programs, we \nuse it heuristically, and it does produce well-scoped predicates in our examples. We think a predicate \ndiscovery algorithm that takes asynchronous calls into account is an interesting open research direction. \nNullPointer.The .rst property checks correctness of pointer deref\u00aderences in the two benchmarks. For \neach callback, we insert an as\u00adsertion that states that the argument pointer passed into the callback \nis non-null. Usually, this is ensured by a check on the argument in a caller up the asynchronous call \nchain. Hence, the correctness de\u00adpends on tracking program .ows across asynchronous as well as synchronous \ncalls. The results are shown in Table 1. There are 4 instances of these checks for plb, namely, plb-1 \nthrough plb-4 and 2 for nch. The instances for plb are all safe. There is a bug in one of the checks \nin nch where the programmer forgets to check the result of an allocation. All the runs take a few seconds. \nIn each example, we manually provide the predicates from the assertions to BLAST, but additional predicates \nare found through counterexam\u00adple analysis. ProtocolState.plb maintains an internal protocol state for \neach connection. The protocol state for an invalid connection is 0,on connection, the state is 1, and \nthe state moves to 2and then 3when certain operations are performed. These operations are dispatched \nfrom a generic callback that gets a connection and decides which operation to call based on the state. \nIt is an error to send an invalid connection (whose state is 0) to this dispatcher. We checked the assertion \nthat the dispatcher never receives a connection in an invalid state (.le plb-5). We found a bug in plb \nthat showed this property could be violated. The bug occurs if the client sends a too large request to \nread, in which case the connection is closed and the state reset to 0. However, the programmer forgot \nto return at this point in the error path. Instead, control continues and the next callback in the sequence \nis posted, which calls the dispatcher with an invalid connection. BufferOver.ow.Each connection in plb \nmaintains two integer .elds: one tracks the size of a buffer (the number of bytes to write), and the \nsecond tracks the number of bytes already written. The second .eld is incremented on every write operation \nuntil the required number of bytes is written. We check that the second .eld is always less than or equal \nto the .rst (.le plb-6). The complication is that the system write operation may not write all the bytes \nin one go, so the callback reschedules itself if the entire buffer is not written. Hence the correctness \nof the property depends ondata.owthroughasynchronouscalls. BLAST canverifythatthis property holds for \nthe program. We model the write procedure to non-deterministically return a number of bytes between 0and \nthe number-of-bytes argument. Our initial experiences highlight two facts. First, even though the algorithm \nis exponential space in the worst case, in practice, the reachable state space as well as the counter \nvalue required for convergence is small (in all experiments k =1was suf.cient). Second, the correctness \nof these programs depends on complicated data.ow through the asynchronous calls: this is shown by the \nnumber of distinct global states reached at the dispatch location.  5.Proof The foundation on which \nour technique for solving AIFDS is based is that the Ck-and Ck 8-reduced under-and over-approximations \nactually converge to the C8-reduced instance, and therefore to the precise MVP solution of the AIFDS \ninstance. The main techni\u00adcal challenge is to prove the completeness Theorem 4, which, as outlined earlier, \nproceeds from the proof of Lemma 1. We prove Table1.Experimental results. Timemeasures total time in \nsec\u00adonds. Predsis the total number of atomic predicates used. Totalstate is the total number of reachable \nexploded nodes. Dispatchis the number of reachable exploded dispatch nodes. (*) indi\u00adcates the analysis \nfound a violation of the speci.cation. Program Time Preds Total Dispatch plb-1 3.05 7 2047 30 plb-2 4.10 \n16 1488 18 plb-3 7.05 20 1583 20 plb-4 5.290 14 1486 25 nch-1 1.32 4 521 27 nch-2(*) 0.440 0 - - plb-5(*) \n30.20 55 - - plb-6 22.68 41 1628 22 Lemma 1intwo steps. In the.rst step(Lemma 3), weshow that the backward \nsolution of the C8-reduced IFDS instance is equiv\u00adalent to the upward closure of some .nite number of \nfacts. In the second step (Lemma 4), we show how, from the .nite set of facts, we can .nd a k such that \nthe Ck 8-reduced instance coincides with the C8-reduced solution. UpwardClosure.For two counter maps \nc,c ' from S to C,we write c = c ' if for all s . S,we have c(s) = c ' (s).Let Dg \u00d7Dl be a .nite set \nof data.ow facts. The upward closure of a set B .(Dg \u00d7Dl)\u00d7(Dl .C)is the set B=' ' = {(d,c )|.(d,c).B \ns.t. c =c } B= We say B is upward closed if B = . The ordering =on counter maps with a .nite domain is \na well quasi-order, that is, it has the property that any in.nite sequence c1, c2, ... of counter maps \nmust have two positions i and j with i<j such that ci =cj [8]. We shall use the following fact about \nwell quasi-orderings. LEMMA 2. [1] Let f be a function from counter maps to counter maps that is monotonic \nw.r.t. =.Let (f-1denote the re.exive ) * transitive closure of the inverse of f. For any upward closed \nset U of counter maps, there is a .nite set B of counter maps such that B= =(f-1) * (U). BackwardSolutions.For \nan IFDS instance I = (G * ,D,{T},M,n) (where D may be in.nite), for any v ' . V * and d . D,we de.ne \nthe backwards meet over valid paths solution MVP-1(I ,v ' ,d ' )(v)as: {d |.p .IVP(v,v ' )s.t. (d,d ' \n).PR(I )(p)} Intuitively, the backwards or inverse solution for v ' ,d ' is the set of facts at v, which \nget transferred along some valid path to the fact d ' at v ' .If D is .nite, we can compute the inverse \nsolution using a backwards version of the RHS algorithm. It turns out, that if D corresponds to the in.nite \nset of facts for a C8-reduced instance of an AIFDS, then the in.nite inverse solution is equivalent to \nthe upward closure of a .nite set. Recall that c0 is the map .x.0. LEMMA 3. [30, 10] Let A =(G * ,Dg,Dl,M,n)be \nan AIFDS instance, and I be the C8-reduced IFDS instance of A. For every '' ' v .V * and d .Dg \u00d7Dl, there \nexists a .nite set B(v ' ,d ,v). (Dg \u00d7 Dl) \u00d7 (P \u00d7 Dl . N) such that: B(v ' ,d ' ,v)= = MVP-1(I ,v ' ,(d \n' ,c0))(v). PROOF. Sketch. The proof relies on two facts: .rst, =forms a well quasi-order on (Dg \u00d7Dl)\u00d7(P \n\u00d7Dl .N), and second, that the data.ow facts and transfer function for A is a monotonic function on this \norder. Intuitively, the transfer function is not inhibited by adding elements to the counter map. With \nthese in mind, and using Lemma 2, we can devise a backwards RHS algorithm whose termi\u00adnation (shown in \n[10]) is guaranteed by the well quasi-ordering of (Dg \u00d7Dl)\u00d7(P\u00d7Dl .N)[8, 1] and guarantees the existence \nof the .nite set B(v ' ,d ' ,v). The backward RHS algorithm propagates data.ow facts backward and creates \nsummaries from return points to corresponding call points. An alternate proof of the above result is \nobtained following the proof in [30] which reduces the AIFDS instance via Parikh s lemma to a multiset \nrewriting system and uses well quasi-ordering arguments on multisets to guarantee termination. Parikh \ns lemma is used to replace the original program that may have recursive calls with an automaton which \nhas the same effect w.r.t. counter maps. In the the second step, we show that from the set B(v ' ,d ' \n,v), we can obtain a k, that suf.ces to prove the Completeness Lemma 1. Maxcount.Let A =(G * ,Dg ,Dl,M,n)be \nan AIFDS instance, and let I be the C8-reduced (in.nite) IFDS instance of A.We de.ne the maxcount of \nA,as: [ 1+max {c(s)|(d,c).B(v ' ,(d ' ,c0),v),s .S} d ' ,v ' ,v Note that as d ' ,v ' ,v range over .nite \nsets Dg \u00d7Dl and V * respectively, and from Lemma 3 B(v ' ,(d ' ,c0),v) is .nite, the maxcount of A is \nalso .nite. We observe that if kis the maxcount of the AIFDS instance, then if the fact d ' is not in \nthe MVP solution for v ' in the C8-reduced instance then it is not in the solution of the .nite Ck 8-reduced \nIFDS instance. LEMMA 4. Let A =(G * ,Dg,Dl,M,n)be an AIFDS instance, with maxcount k, and I (resp. Ik \n8)be the C8-(resp. Ck 8-) re\u00adduced IFDS instances of A. For every v ' .V * and d ' .Dg \u00d7Dl, ' ' s (a) \nif (T,c0). MVP-1(I ,v ,(d ,c0))(vmain )then for all v . V * , MVP(Ik 8)(v)nMVP-1(I ,v ' ,(d ' ,c0))(v)= \n\u00d8 '' '' (b) if c. (d ,c ' ) . MVP(I )(v ' ) then c. (d ,c ' ) . MVP(Ik 8)(v ' ). PROOF. First, note that \n(b) follows from (a) by observing from the de.nitions of solutions and backwards solutions that there \n''' ' exists a c such that (d ,c ) . MVP(I )(v ) iff (T,c0) . MVP-1(I ,v ' ,d ' ), then instantiating \nthe universal quanti.er in (a) with v ' , and .nally applying the fact that MVP-1(I ,v ' ,(d ' ,c0)) \nis upward closed (from Lemma 3). Next, we prove the following statement which implies (a). s IH.n . N,v \n. V * ,.p . IVP(vmain ,v) of length n,if (d,c) . PF (Ik 8)(p)(T,c0) then (d,c) . MVP-1(I ,v ' ,(d ' ,c0))(v). \nThe proof is by induction on n. The base case follows from the hypothesis that (T,c0) is not in the backwards \nsolution for v ' ,(d ' ,c0). For the induction step, suppose the IHholds upto n. Consider '''' '' apath \np = p,(v ,v) of length n +1 where the pre.x p is of length n. By the de.nition of the path function, \nwe know '' '' '' '''' there exists a (d ,c ) such that (1) (d,c) . M(v ,v)(d ,c ), '' '' '' (2) (d ,c \n)is in PF (Ik 8)(p )(T,c0), and (3) therefore, by the '' '' '' '' IH, that (d ,c ) .MVP-1(I ,v ,(d ,c0))(v \n). We shall prove the induction step by contradiction. Sup\u00ad ' ' '' pose that (d,c) . MVP-1(I ,v ,(d ,c0))(v \n). By Lemma 3, there is a (d,c*) . B(v ' ,d ' ,v), such that c* = c. Con\u00adsider the countermap LcJ = .x.min \n{c(x),k+1}.As c is a Ck 8 counter, and k is bigger than every element in the range of c* (it is the maxcount), \nit follows that c* =LcJ.Thus,as backwards solutions are upward closed (Lemma 3), (d,LcJ) . MVP-1(I ,v \n' ,(d ' ,c0))(v). By splitting cases on the possible op\u00aderations on the edge (v '' ,v), we can show that \nthat there exists '' '' '''' '' a c* such that: (i) (d,LcJ) .M(v ,v)(d ,c* ) and (ii) c* = '' '''' '' \n'' c .In other words, (d ,c* ) is in MVP-1(I ,v ,(d ,c0))(v ). '' '' By the upward closure of the backwards \nsolution, (d ,c ) . ' ' '' MVP-1(I ,v ,(d ,c0))(v ), thereby contradicting (3) above. ' ' '' Hence, (d,c) \n. MVP-1(I ,v ,(d ,c0))(v ), completing the proof of IHand therefore, the lemma. We can now prove Completeness \nLemma 1. PROOF. (of Lemma 1). Suppose that there exists a c such that (d,c) .MVP(I )(v). Then there exists \nsome path p . s IVP(vmain v,) such that (d,c).PF(I )(p)(v). Picking the length of p as kd,v suf.ces, \nas for all k greater than this kd,v we can prove that, (d,\u00b7) .MVP(Ik)(v), and from Theorem 3, (d,\u00b7) . \nMVP(Ik 8)(v). Suppose that there is no c such that (d,c) .MVP(I )(v).If we let kd,v be the maxcount of \nA, then Lemma 4 shows that there is no (d,\u00b7) .MVP(Ik 8), and from Theorem 3, there can be no (d,\u00b7).MVP(Ik). \n This concludes the proof of correctness. Notice that while the correctness proof relies on several technical \nnotions, these can all be hidden from an implementer, who only needs to call an interpro\u00adcedural data.ow \nanalysis algorithm with appropriate lattices. 6.Conclusion We believe that our AIFDS framework and algorithm \nprovides an easy to implement procedure for performing precise static analy\u00adsis of asynchronous programs. \nWhile theoretically expensive, our initial experiments indicate that the algorithm scales well in prac\u00adtice. \nThus, we believe the algorithms presented in this paper open the way for soundly transferring the data.ow \nanalysis based op\u00adtimization and checking techniques that have been devised for synchronous programs \nto the domain of asynchronous programs, thereby improving their performance and reliability.  References \n[1] P. A. Abdulla, K. .ans, B. Jonsson, and Yih-Kuan Tsay. General Cer\u00afdecidability theorems for in.nite-state \nsystems. In LICS 96, pages 313 321. IEEE Press, 1996. [2] T. Agerwala and J. Misra. Assertion graphs \nfor verifying and synthesizing programs. Technical Report 83, University of Texas, Austin, 1978. [3] \nT. Ball and S.K. Rajamani. The SLAM project: debugging system software via static analysis. In POPL 02: \nPrinciples of Programming Languages, pages 1 3. ACM, 2002. [4] E. M. Clarke, O. Grumberg,S.Jha,Y.Lu, \nand H.Veith. Counterexample-guided abstraction re.nement. In CAV 00: Computer-Aided Veri.cation, LNCS \n1855, pages 154 169. Springer, 2000. [5] R. Cunningham. eel: Tools for debugging, visualization, and \nveri.cation of event-driven software, 2005. Master s Thesis, UC Los Angeles. [6] R. Cunningham and E. \nKohler. Making events less slippery with Eel. In HotOS-X, 2005. [7] G. Delzanno, J.-F. Raskin, and L. \nVan Begin. Towards the automated veri.cation of multithreaded Java programs. In TACAS 02,LNCS 2280, pages \n173 187. Springer, 2002. [8] L.E. Dickson. Finiteness of the odd perfect and primitive abundant numbers \nwith r distinct prime factors. Amer. Journal Math., 35:413 422, 1913. [9] E.W. Dijkstra. A Discipline \nof Programming. Prentice-Hall, 1976. [10] M. Emmi and R. Majumdar. Decision problems for the veri.cation \nof real-time software. In HSCC 06, LNCS 3927, pages 200 211. Springer, 2006. [11] J. Esparza. Decidability \nand complexity of Petri net problems an introduction. In Lectures on Petri Nets I: Basic Models, LNCS \n1491, pages 374 428. 1998. [12] D. Gay, P. Levis, R. von Behren, M. Welsh, E. Brewer, and D. Culler. \nThe nesC language: A holistic approach to networked embedded systems. In PLDI 2003: Programming Languages \nDesign and Implementation, pages 1 11. ACM, 2003. [13] G. Geeraerts, J.-F. Raskin, and L. Van Begin. \nExpand, enlarge, and check: New algorithms for the coverability problem of WSTS. In FSTTCS 04, LNCS 3328, \npages 287 298. Springer, 2004. [14] S. Graf and H. Sa\u00a8idi. Construction of abstract state graphs with \nPVS. In CAV 97: Computer Aided Veri.cation, LNCS 1254, pages 72 83. Springer, 1997. [15] T. Harris and \nK. Fraser. Language support for lightweight trans\u00adactions. In OOPSLA 03: Object-Oriented Programming, \nSystems, Languages and Applications, pages 388 402, 2003. [16] T.A. Henzinger, R. Jhala, and R. Majumdar. \nRace checking by context inference. In PLDI 04: Programming Languages Design and Implementation. ACM, \n2004. [17] T.A. Henzinger, R. Jhala, R. Majumdar, and K.L. McMillan. Abstractions from proofs. In POPL \n04: Principles of Programming Languages, pages 232 244. ACM, 2004. [18] T.A. Henzinger, R. Jhala, R. \nMajumdar, and G. Sutre. Lazy abstraction. In POPL 02: Principles of Programming Languages, pages 58 70. \nACM, 2002. [19] E. Kohler, R. Morris, B. Chen, J. Jannotti, and M.F. Kaashoek. The Click modular router. \nACM Transactions on Computing Systems, 18(3):263 297, 2000. [20] Libasync. http://pdos.csail.mit.edu/6.824-2004/async/. \n[21] Libevent. http://www.monkey.org/%7Eprovos/libevent/. [22] B.D. Lubachevsky. An approach to automating \nthe veri.cation of compact parallel coordination programs i. Acta Informatica, 21:125 169, 1984. [23] \nThe mace project. http://mace.ucsd.edu/. [24] B. McCloskey, F. Zhou, D. Gay, and E. Brewer. Autolocker: \nSynchronization inference for atomic sections. In POPL 06: Principles of programming languages, pages \n346 358. ACM, 2006. [25] V.S. Pai, P. Druschel, and W. Zwaenepoel. Flash: An ef.cient and portable web \nserver. In Proc. USENIX Tech. Conf., pages 199 212. Usenix, 1999. [26] Rohit Parikh. On context-free \nlanguages. J. ACM, 13(4):570 581, 1966. [27] G. Ramalingam. Context-sensitive synchronization-sensitive \nanalysis is undecidable. ACM TOPLAS, 22(2):416 430, 2000. [28] T. Reps, S. Horwitz, and M. Sagiv. Precise \ninterprocedural data.ow analysis via graph reachability. In POPL 95: Principles of Programming Languages, \npages 49 61. ACM, 1995. [29] M.F. Ringenburg and D. Grossman. Atomcaml: .rst-class atomicity via rollback. \nIn ICFP 05, pages 92 104, New York, NY, USA, 2005. ACM. [30] K. Sen and M. Vishwanathan. Model checking \nmultithreaded programs with asynchronous atomic methods. In CAV 06,LNCS 4314, pages 300 314. Springer, \n2006. [31] M. Sharir and A. Pnueli. Two approaches to interprocedural data dalow analysis. In Program \nFlow Analysis: Theory and Applications, pages 189 233. Prentice-Hall, 1981. [32] N. Zeldovich, A. Yip, \nF. Dabek, R.T. Morris, D. Mazi`eres, and M.F. Kaashoek. Multiprocessor support for event-driven programs. \nIn USENIX Technical Conference, pages 239 252, 2003. \n\t\t\t", "proc_id": "1190216", "abstract": "An <i>asynchronous program</i> is one that contains procedure calls which are not immediately executed from the callsite, but stored and \"dispatched\" in a non-deterministic order by an external scheduler at a later point. We formalize the problem of interprocedural dataflow analysis for asynchronous programs as AIFDS problems, a generalization of the IFDS problems for interprocedural dataflow analysis. We give an algorithm for computing the precise meet-over-valid-paths solution for any AIFDS instance, as well as a <i>demand-driven</i> algorithm for solving the corresponding demand AIFDS instances. Our algorithm can be easily implemented on top of any existing interprocedural dataflow analysis framework. We have implemented the algorithm on top of B<sc>LAST</sc>, thereby obtaining the first safety verification tool for unbounded asynchronous programs. Though the problem of solving AIFDS instances is EXPSPACE-hard, we find that in practice our technique can efficiently analyze programs by exploiting standard optimizations of interprocedural dataflow analyses.", "authors": [{"name": "Ranjit Jhala", "author_profile_id": "81100198278", "affiliation": "UC San Diego", "person_id": "P343132", "email_address": "", "orcid_id": ""}, {"name": "Rupak Majumdar", "author_profile_id": "81100319213", "affiliation": "UC Los Angeles", "person_id": "P335105", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1190216.1190266", "year": "2007", "article_id": "1190266", "conference": "POPL", "title": "Interprocedural analysis of asynchronous programs", "url": "http://dl.acm.org/citation.cfm?id=1190266"}