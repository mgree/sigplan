{"article_publication_date": "01-17-2007", "fulltext": "\n Preferential Path Pro.ling: Compactly Numbering Interesting Paths Kapil Vaswani Aditya V. Nori Trishul \nM. Chilimbi Indian Institute of Science, Bangalore Microsoft Research India Microsoft Research kapil@csa.iisc.ernet.in \nadityan@microsoft.com trishulc@microsoft.com Abstract Path pro.les provide a more accurate characterization \nof a pro\u00adgram s dynamic behavior than basic block or edge pro.les, but are relatively more expensive \nto collect. This has limited their use in practice despite demonstrations of their advantages over edge \npro\u00ad.les for a wide variety of applications. We present a new algorithm called preferential path pro.ling \n(PPP), that reduces the overhead of path pro.ling. PPP leverages the observation that most consumers \nof path pro.les are only inter\u00adested in a subset of all program paths. PPP achieves low overhead by separating \ninteresting paths from other paths and assigning a set of unique and compact numbers to these interesting \npaths. We draw a parallel between arithmetic coding and path numbering, and use this connection to prove \nan optimality result for the compactness of path numbering produced by PPP. This compact path numbering \nenables our PPP implementation to record path information in an array instead of a hash table. Our experimental \nresults indicate that PPP reduces the runtime overhead of pro.ling paths exercised by the largest (ref) \ninputs of the SPEC CPU2000 benchmarks from 50% on average (maximum of 132%) to 15% on average (maxi\u00admum \nof 26%) as compared to a state-of-the-art path pro.ler. Categories and Subject Descriptors D.2.5 [Software \nEngineer\u00ading]: Testing and Debugging; E.4 [Coding and Information The\u00adory]: Data Compaction and Compression \nGeneral Terms Algorithms, Measurement, Reliability Keywords Pro.ling, preferential paths, arithmetic \ncoding, dy\u00adnamic analysis 1. Introduction Path pro.les are a succinct and pragmatic abstraction of a \npro\u00adgram s dynamic control-.ow behavior. Recording program paths has proved valuable in a wide variety \nof areas such as computer architecture, compilers, debugging, program testing, and software maintenance \n[4]. Path pro.les capture much more control-.ow in\u00adformation than basic block or edge pro.les, and are \nmuch smaller than complete instruction traces. Several compiler optimizations perform better when trade-offs \nare driven by accurate path pro\u00ad.les [1]. Program paths are also a more credible way of measuring Permission \nto make digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior speci.c permission and/or a fee. POPL 07 January \n17 19, 2007, Nice, France. Copyright c &#38;#169; 2007 ACM 1-59593-575-4/07/0001. . . $5.00 coverage \nof a given test suite. In addition, abstractions of paths can help automatic test generation tools generate \nmore robust test cases. Finally, program path histories often serve as a valuable debugging aid by revealing \nthe instruction sequence executed in the lead up to interesting program points. Unfortunately, the bene.ts \nof using path pro.les come at a cost pro.ling paths is expensive. Our measurements of an implemen\u00adtation \nof a state-of-the-art path pro.ler [3] indicate an average exe\u00adcution time overhead of 50% with as much \nas a 132% overhead in the worst case and other studies report similarly high overheads [6]. This high \noverhead has limited the use of path pro.les in favor of basic block or edge pro.les. Basic block and \nedge pro.les are cheaper to collect but less accurately capture a program s dynamic behavior as compared \nto paths. While Ball et al. found that 80% of program paths could be attributed from an edge pro.le [5], \nmore re\u00adcent work found that just 48% of paths could be attributed from an edge pro.le [6]. In both cases, \nthe most complex (and likely most interesting) paths were not predictable from an edge pro.le. This represents \nan opportunity as basic block and edge pro.les are still preferred over path pro.les for measuring test \ncoverage and driving pro.le-guided optimizations such as code placement, inlining, un\u00adrolling, and superblock \nscheduling. Apart from these traditional usage scenarios, we envisage the use of path pro.ling in several \nother cost-sensitive enviornments. For instance, in residual path pro.ling, a user is interested in de\u00adtermining \nthe set of paths that a deployed program executed in the .eld that were not exercised during testing. \nThis information could be used to improve and augment test suites, and if included with bug reports resulting \nfrom .eld failures, could help pinpoint the root cause of errors. Another scenario involves ascertaining \nwhether paths that were identi.ed as hot paths during testing and used to optimize the program continue \nto remain hot during .eld usage. In addition, we might want to gather detailed information about these \npaths, such as cache misses, page faults, and variations in execution time, without resorting to sampling \ntechniques [12]. Finally, we might be interested in ef.ciently tracking a subset of paths in deployed \nsoftware that meet a certain criteria, for example, paths that access safety or security critical resources, \nor that exer\u00adcise an error prone code region. A common trait in all these sce\u00adnarios is the need for \nef.ciently and accurately pro.ling a known subset of paths. Let us .rst examine why existing path pro.ling \nschemes incur relatively high overhead. The ef.cient path pro.ling scheme pro\u00adposed by Ball and Larus, \nwhich forms the basis of all path pro.l\u00aders, assigns weights to edges of a control .ow graph (CFG) such \nthat all paths are allocated unique identi.ers (i.e., the sum of the weights of the edges along every \npath is unique) [3]. During pro\u00adgram execution, the pro.ler accumulates weights along the edges and updates \nan array entry that corresponds to this path identi.er. Unfortunately, for functions with a large number \nof paths, allocat\u00ad procedure computeBLIncrements (G) Assume: (a) G =(V, E, s, t)is a DAG. (b) W :E . \nZ is an empty map. Returns: The map W de.ned for all edges such that every path in G is assigned a unique \nweight. 1: Nt :=1; 2: for all nodes v . V in reverse topological order do 3: Nv :=0; 4: for all edges \ne . out(v)do 5: W (e):=Nv; 6: Nv :=Nv +Ndest(e); 7: end for 8: end for Figure 1. The Ball-Larus Algorithm. \ning an array entry for all program paths is prohibitively expensive, if not infeasible. Consequently, \npath pro.ler implementations are forced to use a hash table to record path information for such func\u00adtions. \nAlthough using a hash table is space ef.cient as program s typically execute only a small subset of all \npossible paths, it in\u00adcurs signi.cantly higher execution time overhead as compared to updating an array \nentry. Previous work has shown that hash tables account for a signi.cant fraction of the overhead attributable \nto path pro.ling [8]. To address this problem, we propose preferential path pro.l\u00ading (PPP), a novel \npath pro.ling scheme that ef.ciently pro.les arbitrary path subsets, which we refer to as interesting \npaths. Our algorithm can be viewed as a generalization of the Ball-Larus al\u00adgorithm, which forms the \ncore of most existing path pro.ler im\u00adplementations. As mentioned earlier, the Ball-Larus algorithm as\u00adsigns \nweights to the edges of a given CFG such that the sum of the weights of the edges along each path through \nthe CFG is unique. Our algorithm generalizes this notion to a subset of paths; it as\u00adsigns weights to \nthe edges such that the sum of the weights along the edges of the interesting paths is unique. Furthermore, \nour algo\u00adrithm attempts to achieve a minimal and compact encoding of the interesting paths; such an encoding \nsigni.cantly reduces the over\u00adheads of path pro.ling by eliminating expensive hash operations during \npro.ling. In addition, our pro.ling scheme separates inter\u00adesting paths from other paths and is able \nto classify paths during program execution. The ability to classify paths is important for many scenarios \nsuch as residual path pro.ling described earlier. Interestingly, we .nd that both the Ball-Larus algorithm \nand PPP are essentially a form of arithmetic coding [13, 15], a tech\u00adnique commonly used for universal \ndata compression. We make use of this connection to prove an optimality result for the compactness of \npath numbering produced by PPP. We have implemented PPP and our experimental evaluation using benchmarks \nfrom the SPEC CPU2000 suite shows that PPP reduces the overheads of pro.ling paths exercised by their \nlargest (ref) inputs from 50% on average (maximum of 132%) to 15% on average (with a maximum of 26%) \nas compared to Ball-Larus pro.ling. This paper makes the following main contributions. First, we describe \na new algorithm, called preferential path pro.ling (PPP), for compactly numbering arbitrary path subsets \nthat improves upon Ball-Larus numbering (Section 3). Next, we draw a parallel be\u00adtween arithmetic coding \nand path numbering, and use this connec\u00adtion to prove an optimality result for the compactness of path \nnum\u00adbering produced by PPP (Section 4). Finally, we present an exper\u00adimental evaluation of our PPP implementation \nthat demonstrates that it results in signi.cantly lower overheads than Ball-Larus pro\u00ad.ling (Section \n5). Figure 2. Assignment of weights to edges using the Ball-Larus algorithm  2. Preliminaries In this \nsection, we brie.y describe the Ball-Larus algorithm for pro\u00ad.ling acyclic, intra-procedural paths through \na CFG of a program and motivate our problem using a simple example. 2.1 De.nitions Pro.ling algorithms \nfor acyclic, intra-procedural paths (hence\u00adforth referred to as paths) .rst convert the CFG of a proce\u00addure \ninto a directed acyclic graph (DAG). Each DAG is a graph G =(V, E, s, t), where V represents nodes or \nbasic blocks in the procedure, and E is the set of edges between nodes. The maps src(e) and dest(e)denote \nthe source and destination nodes re\u00adspectively, of an edge e. For every node v . V , out(v)denotes the \nset of edges emanating from v in G, and succ(v)represents all the immediate successor nodes of v. Each \nacyclic, intra-procedural path p is a sequence of nodes from s to t. The function paths(G) refers to \nall acyclic, intra-procedural paths in G. The function pathsG(e): E . 2paths(G) represents all paths \nin G that tra\u00adverse an edge e. Conversely, the function edges :P . 2E maps every path p in G to the set \nof edges that belong to p. An assignment of weights to the edges of G is represented as a map W : E . \nZ (where Z is the set of integers). The relation pathid :P . Z maps each path to a path identi.er, and \nis de.ned as follows. def pathid(p)= W (e) e.edges(p)  2.2 Ball-Larus Pro.ling Given a DAG G for a \nprocedure, the Ball-Larus algorithm as\u00adsigns weights to the edges of the graph such that for every path \np . paths(G), pathid(p)is unique, and is equal to a number between 0and N - 1, where N = |paths(G)|. \nThe algorithm computeBLIncrements, shown in Figure 1, performs one bottom\u00adup pass through G and processes \nits nodes in reverse topological order. With each node v, the algorithm associates a count Nv that indicates \nthe number of paths from v to the exit node t (Line 5). At each node, the computeBLIncrements traverses \nthe list of suc\u00adcessor nodes and assigns weights to the corresponding outgoing edges. This algorithm \nis based on a simple idea that is stated in the following lemma [3]. LEMMA 1. Let G =(V, E, s, t)be a \nDAG. The number of paths from any node v in G to the exit node t is equal to the sum of the number of \npaths from each of v s successor nodes to t. Figure 2 illustrates how computeBLIncrements uses this invari\u00adant \nto compute an edge assignment. Assume that the algorithm is processing node a with two successors b and \nc that have Nb and Nc paths to the exit node t. Also assume that these paths have already been assigned \nidenti.ers from 0to Nb - 1and Nc - 1respectively. The algorithm assigns a weight 0to the edge (a, b), \nand a weight Nb to the edge (a, c). This ensures that the paths from a to t are assigned identifers from \n0to Nb +Nc - 1, which is also equal to Na - 1(from Lemma 1). In general, the weight assigned to an Figure \n3. Motivating example for PPP. (a) A DAG G with 6 paths with edges numbered using the Ball-Larus algorithm. \n(b) G with edges having only PPP assigned numbers for three interesting paths I = {sacdt, sact, sbct}. \n(c) G with edges assigned two numbers, a PPP number and a Ball-Larus number (in parenthesis). The path \narray is accessed using the PPP counter.  procedure computePathIdenti.er (G,W,p) Assume: (a) G =(V, \nE, s, t)is a DAG. (b) The map W :E . Z. (c) A path p is a sequence of nodes through G. Returns: The \npath identi.er for the path p  1: return W (e); e.edges(p) Figure 4. Computing the Ball-Larus identi.er \nof a path from an edge assignment. edge is equal to the sum of the number of paths from all previously \nprocessed successor nodes to t (Line 6). The Ball-Larus path pro.ler instruments the edges of the CFG \nwith instructions to increment a counter by the weight assigned to the edge. When the instrumented program \nexecutes, it simulates the procedure computePathIdenti.er (Figure 4). When a path termi\u00adnates, the value \nin the counter represents the path that just executed and can be used for book-keeping.  2.3 An illustrative \nexample We start with an example that illustrates a drawback of the Ball-Larus pro.ling scheme, and also \nshows how our pro.ler works on this example. Consider the function in Figure 3(a). The DAG G in Figure \n3(a) is obtained from the CFG of the function. This .gure also shows the weights assigned by the Ball-Larus \nalgorithm to edges of G. Note that the sum of the weights of edges along every path from the start node \ns to the .nal node t is unique, and all paths are allocated identi.ers from 0to N - 1, where N is the \ntotal number of paths from s to t.If N is reasonably small (less than some threshold value), the pro.ler \ncan allocate an array of counters of size N, and track path frequencies by indexing into the array using \nthe path identi.er and incrementing the corresponding counter. However, the number of potential paths \nin a procedure can be arbitrarily large (exponential in the number of nodes in the graph) and allocating \na counter for each path can be prohibitively expensive, even infeasible in many cases. Path pro.lers \novercome this problem by using a hash table of counters instead of an array, relying on the fact that \nonly a small number of paths are traversed during any given execution. Therefore, a combination of a \nsuitably sized hash table and a good hash function almost always guarantees the absence of con.icts. \nIn the current example, if the threshold value is set to 4, the Ball-Larus pro.ler would use a hash table \nsince there are 6paths from s to t. Let us assume that we are interested in pro.ling only a sub\u00adset I \n={sacdt, sact, sbct} (interesting paths) of paths. The Ball-Larus identi.ers for the paths sacdt, sact, \nsbct are 0, 1and 5re\u00adspectively. This means that one would have to allocate a hash table even though \nthere are only 3paths of interest. In such a scenario, it would be ideal if we could compute an edge \nassignment that allo\u00adcates identi.ers 0, 1and 2to these paths, and identi.ers > 2to the other paths. \nIn Section 3, we show that computing an edge assign\u00adment W and a number \u00df such that (a) .p . I, pathid(p)= \n\u00df, and (b) .p .. I, pathid(p)>\u00df is not always feasible. Therefore, we relax the constraints on this problem \nby eliminating condition (b) (which is a condition over uninteresting paths), and ask the ques\u00adtion if \nit is possible to label the edges in G such that the paths in the set I have path identi.ers in {0, 1, \n2}. Figure 3(b) shows that such an assignment of weights to edges indeed exists, and this is precisely \nthe assignment computed by the preferential path pro.l\u00ading PPP algorithm described in Section 3. Therefore, \nour pro.ler incurs lower overheads since we can now use an array to track fre\u00adquencies instead of a hash \ntable. Note that while the interesting paths sacdt, sact, sbct have been assigned unique identi.ers from \n0to 2, the uninteresting paths sabct and sbcdt alias with the inter\u00adesting paths sacdt and sact respectively. \nWe resolve these aliases using Ball-Larus path identi.ers, which are unique for every path. In PPP, edges \nare annotated with a second weight computed using the Ball-Larus algorithm (these weights are shown in \nparentheses in Figure 3(c)). The pro.ler also stores the Ball-Larus identi.ers of all interesting paths \nalong with their counters. The occurrence of an interesting path can be detected by comparing the Ball-Larus \nidenti.er computed during the traversal with the Ball-Larus identi\u00ad.er stored in the array a match indicates \nthat an interesting path was just traversed and vice versa. For example, when the uninter\u00adesting path \nsbcdt (PPP identi.er is 2 and Ball-Larus identi.er is 4) occurs, before incrementing the count at index \n2 in the path array, the Ball-Larus identi.er at index 2 is compared with the Ball-Larus identi.er of \nsbcdt since they are different, the pro.ler infers that this path is not interesting (or it might be \na residual path not exer\u00adcised by the test suite) and takes necessary action.  3. Preferential Path \nPro.ling We will now address the problem of encoding arbitrary subsets of paths over a DAG G =(V, E, \ns, t). Informally, we wish to compute an edge assignment that allows us to uniquely identify paths as \nwell as differentiate interesting paths from uninteresting ones. First, consider the possibility of .nding \nan edge assignment  Figure 5. A counterexample for separation of paths. that separates interesting and \nuninteresting paths using a non\u00adnegative integer \u00df . Z=0 . LEMMA 2(Separation of paths). Given a DAG \nG =(V, E, s, t) and a set of interesting paths I . paths(G), a map W :E . Z that satis.es the following \nconditions may not always exist. 1. uniqueness: .p, q . I, pathid(p).. =pathid(q) 2. separation: .\u00df . \nZ=0 such that (a) .p . I, pathid(p)= \u00df, and (b) .p .. I, pathid(p)>\u00df.  Proof: Consider the simple DAG \nin Figure 5. Assume that we are interested in pro.ling paths sacet and sbcdt. Assume that there exists \nan edge assignment W that satis.es all conditions in the lemma. Let w, x, y and z represent the cumulative \nweights of the sub-paths sac, sbc, cdt and cet respectively. From condition 2(a), we have w +z = \u00df and \nx +y = \u00df, and from condition 2(b), it follows that w +y>\u00df and x +z>\u00df. This implies that x +y +w +z = \n2\u00df x +y +w +z> 2\u00df which is a contradiction and the lemma follows. We .nd that separating arbitrary sets \nof interesting and uninter\u00adesting paths is almost always infeasible, primarily due to the pres\u00adence of \nmany shared edges. We therefore simplify the problem by relaxing condition 2(b) in Lemma 2. Edge assignments \nthat cause interesting paths to alias with uninteresting paths are acceptable as long as the interesting \npaths are assigned minimal unique identi\u00ad.ers. As described in Section 2.3, a second counter that computes \nthe Ball-Larus identi.ers of all paths can be used to resolve the aliases. This relaxation allows us \nto reason about interesting paths only, an aspect critical to the solution we propose. However, it turns \nout the even this simpli.ed problem may not have a perfect solution as the following lemma indicates. \nLEMMA 3(Perfect edge assignment). Given a DAG G =(V, E, s, t)and a set of interesting paths I . paths(G),a \nmap W : E . Z that satis.es the following conditions may not always exist. 1. uniqueness: .p, q . I, \npathid(p).=pathid(q), 2. perfect assignment: .p . I, 0= pathid(p)< |I|. Proof: Consider the graph in \nFigure 6. Say we are interesting in pro.ling the paths sadft, sadgt, sbdet, sbdgt, scdet, and scdft. \nFor simplicity, we represent the sum of the edges along the sub\u00adpaths sad, sbd, scd, det, dft and dgt \nas u, v, w, x, y and z. Figure 6. A counterexample for perfect edge assignment. Consider the sum of the \nidenti.ers of all these paths. sum =(u +y)+(u +z)+(v +x) +(v +z)+(w +x)+(w +y) =2(u +v +w +x +y +z) Hence, \nthe sum of the path identi.ers of these paths is necessarily even. However, for a perfect assignment, \nthese paths must be allo\u00adcated identi.ers between 0 and 5. Since the sum of numbers from 0 to 5 is odd, \nwe conclude that a perfect edge assignment for this graph and set of interesting paths does not exist. \nSince a perfect edge assignment for interesting paths may not always exist, even an optimal edge assignment \nmay induce a path assignment with holes in the interval of path identi.ers. In light of this lemma, we \nrestate our problem as follows. Problem A (Optimal Edge Assignment): Given a DAG G = (V, E, s, t)and \na set of interesting paths I . paths(G), compute an edge assignment W : E . Z that satis.es the following \nconditions. 1. uniqueness: .p, q . I, pathid(p).=pathid(q), 2. compactness: The compactness measure d \nde.ned by def (maxp.I pathid(p)- minp.I pathid(p))+1 d = |I| is minimized. It is easy to see that d = \n1. A perfect edge assignment W in\u00adduces a d =1. Lemma 3 shows that a solution with d =1does not always \nexist. Hence solutions with lower values of d are pre\u00adferred. We .nd that for arbitrary graphs and arbitrary \nset of paths, even characterizing the optimal d seems to be a hard problem. In the next section, we propose \nan algorithm that computes an edge assignment that attempts to minimize d, and later prove an opti\u00admality \nresult for this algorithm by establishing a connection with arithmetic coding. 3.1 The Preferential \nPath Pro.ling Algorithm The preferential path pro.ling (PPP) algorithm is a generalization of the Ball-Larus \nalgorithm with the added capability of biasing the edge assignment towards an arbitrary set of interesting \npaths. Before we describe the algorithm, we introduce some notation and state some of the key observations \nthat the algorithm is based on. Let G =(V, E, s, t) be a DAG, and let I . paths(G) be a set of interesting \npaths. Consider a node v . V and an edge e . out(v). Let pathsI (e)represents the set of interesting \npaths Figure 7. Critical nodes for pairs of paths. that contain the edge e. Let prefix(p, e) denote \nthe sequence of nodes from s to src(e) along the path p. Then prefixI (e) denotes the set of all pre.xes \n{prefix(p, e)}p.I . Given a pair ii of interesting paths p, p. I, lcp(p, p) represents the longest i \ncommon pre.x of p and pi in G. Note that lcp(p, p) is trivially the start node s if p and pi are edge \ndisjoint. We de.ne the critical node for a pair of interesting paths p and pi as follows. idef i critical(p, \np)= thelastnodeinlcp(p, p) For any pair of paths, the critical node is unique since the longest common \npre.x is uniquely de.ned. For example, in Figure 7, node a is the critical node for paths p1 and p2, \nwhereas node b is the critical node for paths p1 and p3. Again, the critical node for a set of paths \nis trivially the start node s if the set of paths are edge disjoint in G. We also de.ne a map pid : I \n. Z to track partial identi.ers allocated to paths during the execution of an edge assignment algorithm. \nAssuming that all edges are initialized with a weight . (this denotes the unde.ned value), the partial \nidenti.er of a path is de.ned as follows. pid(p)= W (e) e.edges(p).W (e) =. From the statement of Problem \nA, it is evident that an edge as\u00adsignment must simultaneously satisfy two constraints, uniqueness and \ncompactness of path identi.ers. We now describe how PPP satis.es these constraints. Uniqueness. We .rst \nde.ne an invariant that any algorithm com\u00adputing an edge assignment by processing nodes in reverse topolog\u00adical \norder must satisfy, in order to ensure that all interesting paths are allocated unique identi.ers1. LEMMA \n4(Invariant for uniqueness). Consider a node v being processed by the algorithm. If v is the critical \nnode for any pair of interesting paths p and p, then p and pi should be assigned different partial identi.ers \nafter the node v has been processed. Proof: Follows from the de.nition of critical nodes, and the fact \nthat the algorithm assigns weights to edges in reverse topolog\u00adical order. 1 A similar invariant based \non suf.xes can be de.ned if the algorithm were to perform a top-down traversal, processing nodes in topological \norder. The PPP algorithm computes an edge assignment that attempts to achieve the most compact path numbering \n(low d) that maintains this invariant at every node. However, to make the algorithm sim\u00adpler and more \namenable for analysis (Section 4), PPP makes the following approximation. Instead of explicitly checking \nthe partial identi.ers of paths at a critical node, PPP works over intervals of path identi.ers. Given \nan edge e, the interval inte,q represents the range of partial identi.ers allocated to all interesting \npaths through e that have a pre.x q. Formally, inte,q =[ min pid(p), p.pathsI (e).prefix(p,e)=q max pid(p)] \np.pathsI (e).prefix(p,e)=q At every node, PPP computes an interval inte,q for every (edge, pre.x) pair, \nand assigns weights to the edges to maintain the fol\u00adlowing invariant. LEMMA 5(Invariant for uniqueness \nover intervals). Consider a node v being processed by PPP. Assume that a pre.x q induces a set of intervals \nSv,q = {inte,q | e . out(v)} on the outgoing edges of v. To ensure uniqueness, the intervals in Sv,q \nshould not overlap after v has been processed. Furthermore, this condition must hold for every pre.x \nq . prefixI (e). e.out(v) Proof: It is easy to see that if a pre.x q induces an interval at two or more \noutgoing edges of a node v, then v is the critical node for all paths p with the pre.x q. By preventing \noverlap between all such intervals for a given pre.x, PPP automatically ends up separating all paths \nwith pre.x q for which node v is critical. If this condition is satis.ed for all pre.xes, all interesting \npaths for which node v is critical are distinguished, and hence the Lemma 4 holds. Compactness. We will \nnow describe how PPP ensures compact\u00adness. At any node v . V , consider the set of intervals Sv,q = {[mini,maxi]i.[1,|out(v)|]} \ninduced on edges e1,e2 ...e|out(v)|by a pre.x q.If q is the only valid pre.x at v, PPP uses com\u00adpaction \nto compute a minimal edge assignment W (ei)i.[1,|out(v)|] which ensures that these intervals do not overlap. \nTo achieve com\u00adpaction, each W (ei)is computed as follows: W (ei)= (maxj - minj +1)- mini (1) j.[1,(i-1)] \n= cisi-1 - mini (2) where cisi-1 represents the cumulative interval size of all inter\u00advals induced on \nprevious edges. However, this simple compaction method cannot be used if multiple pre.xes induce intervals \non the outgoing edges of a node. In such situations, PPP performs a join operation over the intervals \nat all edges with two or more inter\u00advals. The join operation computes the weights induced by different \npre.xes on the edge, and conservatively assigns a weight equal to the maximum among all these weights. \nDue to the join, interesting paths associated with all but one of the pre.xes will be assigned a weight \nhigher than what is required to separate its intervals, cre\u00adating holes in the path numbering. However, \nit is easy to see that this choice of weight leads to the most compact numbering that is feasible. Figure \n8 illustrates the scenarios that PPP deals with. In Fig\u00adure 8(a), all interesting paths through the node \na have the same pre.x q1 and traverse the edge e2 (represented by the shaded re\u00adgion). Since the interval \ninte2,q1 is the only interval in the set Sa,q1 , no overlap between intervals exist and the invariant \nfor uniqueness (Lemma 5) is trivially satis.ed. A similar situation occurs in Figure 8(b), where paths \nthrough the edges do not share any pre.xes. The interval sets Sa,q1 and Sa,q2 are singletons and no con.icts \noccur.  Figure 8. The assignment of weights to edges under four scenarios Figure 8(c) represents a scenario \nwhere the interesting paths induce two intervals for the pre.x q1. PPP uses Equation 1 to compute weights \nand ensures that these intervals do not overlap. Finally, Fig\u00adure 8(d) illustrates the scenario where \nthe interesting paths through edges e1 and e2 share pre.xes q1 and q2. Figure 9 illustrates the ef\u00adfect \nof a join on two sets of intervals induced by pre.xes q1 and q2. Since we need a larger weight (say w \ncomputed by Equation 1) to separate intervals induced by pre.x q1 (as compared to the weight w i required \nto separate intervals induced by pre.x q2), PPP assigns w to e2, leading to a hole in the interval for \nthe pre.x q2. Figure 9. The effect of using the join operator to conservatively assign weights to edges. \n(a) Intervals induced by the pre.xes before the join, and (b) effective intervals after the join. Figure \n10 describes the PPP algorithm in detail. For each node v .V and each outgoing edge e .out(v), the algorithm \niterates over all pre.xes and computes the beginning of the interval inte,q (mine,q at Line 5). It uses \nan auxiliary map cis to determine the cumulative interval size of intervals through previously processed \noutgoing edges of v with the pre.x q (as per Equation 1) and computes the weight induced by q on the \nedge (Line 7). Finally, the join operation (Lines 10 and 11) selects the maximum over the weights induced \nby each pre.x and assigns this weight to the edge. After the edge is assigned a weight, PPP updates the \npartial identi.ers of all paths through the edge and also computes the new cis(q)for the next iteration \non this edge. In summary, at every node v . V , computePPPIncrements recursively merges intervals of \neach pre.x q into the most compact single interval intv,q. At the start node s, this interval de.nes \nthe range of identi.ers allocated to the interesting paths. The time complexity of PPP is O(|E|\u00d7|I|), \nwhere E is the set of edges in G, and I is the set of interesting paths. procedure ComputePPPIncrements \n(G) Assume: (a) G =(V, E, s, t)is a DAG. (b) a map pathsI :E .2P . (c) pid :P .Z=0 initialized to 0 \nfor all interesting paths. (d) cis : prefix . Z=0, initialized to 0 for all pre.xes of interesting paths. \nReturns: A edge assignment W :E .Z.  1: for all nodes v .V in reverse topological order do 2: for all \nedges e .out(v)s.t. e .edges(p)for some p .I do 3: for all pre.x q .prefix(e)do 4: // compute the beginning \nof the interval inte,q 5: mine,q :=minp.pathsI (e).prefix(p,e)=q pid(p); 6: // compute weight induced \nby pre.x q 7: weightq :=cis(q)-mine,q; 8: // the join: compute the maximum weight 9: if ((W (e)=.).(W \n(e)<weightq))then 10: W (e):=weightq; 11: end if  12: end for 13: // update partial identifers of all \npaths through e 14: for all paths p .pathsI (e)do 15: pid(p):=pid(p)+W (e); 16: end for 17: for all \npre.xes q .prefix(e)do 18: // determine new cumulative interval size for pre.x q 19: cis(q):=maxp.pathsI \n(e).prefix(p,e)=q pid(p)+1; 20: end for 21: end for 22: end for Figure 10. The preferential path pro.ling \n(PPP) algorithm for computing an edge assignment for a set of interesting paths. 3.2 Example We now walk-through \nan example illustrating how the PPP algo\u00adrithm works. Let us assume that we are interested in pro.ling \nthe paths sacdt, sact and sbct in the DAG from Figure 3. The follow\u00ading steps trace the manner in which \nPPP assigns weights to edges of the DAG. Step nj denotes that at step i, PPP processes node n. Step 1t \nInitialize the partial identi.ers of all paths to 0and cumu\u00adlative interval sizes of all pre.xes to 0. \nStep 2d Node d is not a critical node for any pair of paths since it has only one outgoing edge. The \npre.x sacd induces an interval [0, 0] on the edge (d, t). Since cis(sacd)=0, W ((d, t)) = 0-0=0. Step \n3c Node c is a critical node for paths sacdt and sact. Say the edge (c, d) is processed .rst. Both pre.xes \nsac and sbc induce an interval [0, 0] on this edge. Hence PPP assigns a weight W ((c, d)) =0 - 0=0 to \nthis edge. PPP updates the map cis as follows . cis(sac)=1 and cis(sbc)=1. Next PPP processes the edge \n(c, t). The pre.x sbc induces an interval [0, 0] on this edge. Since cis(sbc)=1, PPP assigns a weight \nW ((c, t)) = 1 - 0=1 The partial identi.ers of paths sact and sbct are also updated to 1. Step 4b Node \nb has one outgoing edge (b, c). The pre.x sb in\u00adduces an interval [1, 1] on this edge. PPP assigns a \nweight W ((b, c))= 0 - 1= -1 to this edge since cis(sb)=0. The partial identi.er of the path sbct is \nnow updated to 1+ -1=0. Step 5a Node a has two outgoing edges, but only the edge (a, c) has interesting \npaths through it. The pre.x sa induces an in\u00adterval [0, 1] at this edge. PPP assigns a weight W ((a, \nc)) = 0 - 0=0 to the edge. Step 6s Node s has two outgoing edges with three paths, all sharing a common \npre.x s. This pre.x induces an interval [0,1] at the edge (s, a), and the interval [0, 0] at the edge \n(s, b). PPP processes the edge (s, a) .rst and assigns a weight W ((s, a)) = 0 - 0=0 to the edge. Then \nPPP updates cis(s)=1+1=2, and processes the edge (s, b). Since cis(s)=2, the edge (s, b) is assigned \na weight W ((s, b)) = 2 - 0=2. The partial identi.er of the path sbct is also updated to 2. On termination, \nPPP assigns the identi.ers 0, 1 and 2 to the interesting paths sacdt, sact and sbct respectively. 3.3 \nDiscussion In summary, PPP attempts to achieve a compact path numbering by (1) only numbering the edges \nrequired to distinguish interest\u00ading paths, and (2) computing the smallest weights such that the interesting \npaths are assigned unique identi.ers. Our experiments suggest that PPP achieves good compactness measures \nfor a vast majority of the procedures, even when a large number of interesting paths are speci.ed. However, \ndespite its best efforts, PPP does not always achieve the best possible compactness measure. Figure 11 \nillustrates one scenario in which PPP fails to assign an optimal numbering although such a numbering \nclearly exists. Consider the graph G1 and assume that PPP has assigned identi.ers to a set of interesting \npaths. Also assume that the interval of allocated identi\u00ad.ers contains two holes of size k1 and k2 respectively. \nAs shown in the .gure, we can construct a new graph G and select a set of interesting paths such that \nthere exists an edge assignment which achieves the d =1. Here G is obtained via a parallel composition \nof three graphs G1,G2 and G3 such that |paths(G2)| = k1 and |paths(G3)| = k2. Furthermore, the set of \ninteresting paths now includes all paths through G1 and G2. An optimal numbering for this set of interesting \npaths is obtained by assigning a weight I1 to the edge (s, s2) and I2 to the edge (s, s3), .lling up \nthe holes in the interval of graph G1. However, PPP fails to compute such as\u00adsignments since we restrict \nourselves to the class of solutions where edge intervals do not overlap. As we show in the next section, \nthis constraint allows us to establish an optimality condition by drawing a connection with arithmetic \ncoding.   4. Path Pro.ling -an information theoretic perspective In this section, we give an information \ntheoretic characterization of the preferential path pro.ling algorithm. We begin by intro\u00adducing the \nnotion of arithmetic coding and context modeling. We then show that the Ball-Larus path pro.ling algorithm \nis a special instance of arithmetic coding. After drawing this connection, we reformulate Problem A described \nin Section 3 so that it is more amenable to analysis, and provide a theoretical analysis of our al\u00adgorithm \nfor preferential path pro.ling. 4.1 Arithmetic Coding Arithmetic coding [13, 15, 7] is a well-known \nuniversal, lossless compression technique that achieves close-to-optimal compression rates. Much like \nother compression schemes, arithmetic coding re\u00adlies on the observation that in any given input stream, \na small frac\u00adtion of characters/substrings are likely to occur frequently. Arith\u00admetic coding achieves \ncompression by encoding these frequently occurring characters/substrings using a smaller number of bits. \nAn arithmetic coder uses a probability model to identify frequent char\u00adacters. In the simplest of cases, \nthe probability model D is an as\u00adsignment of probabilities to characters of the input alphabet S, and \nis easily obtained from the frequency counts of characters in a rep\u00adresentative string. An arithmetic \ncoder encodes strings into a single positive num\u00adber less than 1. To compute this number, the arithmetic \ncoder main\u00adtains a range or an interval, which is set of [0, 1) at the beginning of the coding process. \nAs each symbol of the input string is processed, the coder iteratively narrows the range based on the \nprobability of the symbol. After the last symbol is read, any number within the resulting range uniquely \nrepresents the input string. Moreover, this number can be uniquely decoded to create the exact stream \nof sym\u00adbols that went into its construction. We illustrate the encoding an decoding process by way of \nan example (adapted from [15]). Symbol Probability Range a 0.2 [0, 0.2) b 0.3 [0.2, 0.5) c 0.1 [0.5, \n0.6) d 0.2 [0.6, 0.8) e 0.1 [0.8, 0.9) ! 0.1 [0.9, 1) Table 1. A sample probability model for the alphabet \nS= {a, b, c, d, e, !}. Example. Let S= {a, b, c, d, e, !} be the .nite alphabet, and let a .xed model \nthat assigns probabilities to symbols from S be as shown in Table 1. Suppose we wish to send the message \nbacc!. Initially, both the encoder and the decoder know that the range is [0, 1). After seeing the .rst \nsymbol b the encoder narrows it down to [0.2, 0.5) (this is the range that the model allocates to symbol \nb). For the second symbol a, the interval is further nar\u00adrowed to one-.fth of itself, since a has been \nallocated [0, 0.2). Thus the new interval is [0.2, 0.26). After seeing the .rst c the nar\u00adrowed interval \nis [0.23, 0.236), and after seeing the second c the new interval is [0.233, 0.2336). Finally on seeing \n!, the interval is [0.23354, 0.2336); knowing this to be the .nal range, the decoder can immediately \ndeduce that the .rst character was b. Now the de\u00adcoder simulates the action of the encoder, since the \ndecoder knows that the interval [0.2, 0.5) belonged to b, the range is expanded to [0.2, 0.26). Continuing \nthis way, the decoder can completely de\u00adcode the transmitted message. It is not really necessary for \nthe de\u00adcoder to know both ends of the range produced by the encoder. Instead, a single number in the \nrange (say 0.23355 in our example) will suf.ce. Note that the ranges for any probability model (for example, \nthe one in Table 1) are non-intersecting; this condition is critical for arithmetic coding to work. If \nthe ranges associated with the input symbols were intersecting, two or more strings could map to the \nsame interval and the decoder has no way of distinguishing these strings. From the discussion, it should \nbe clear that for arithmetic cod\u00ading to be effective, the frequency of occurrence of characters in the \ninput string must be skewed and the skew must be accurately re\u00ad.ected in the probability model. In other \nwords, better compression rates are achieved if the model makes accurate predictions about the nature \nof the input string. We now describe a technique known as .nite context modeling, which is commonly used \nto obtain more accurate probability models. Finite context modeling. In a .nite context scheme, the probabili\u00adties \nof each symbol are calculated based on the context the symbol appears in. In its traditional setting, \nthe context is just the sym\u00adbols that have been previously encountered. The order of the model refers \nto the number of previous symbols that make up the context. One way of compressing data is to make a \nsingle pass over the symbols to be compressed (to gather statistics), and then encode the data in a second \npass. The statistics collected are the relative frequencies of occurrences of the respective symbols. \nThese rela\u00adtive frequencies are then used to encode/decode the symbol as ex\u00adplained in earlier. Essentially, \nthe model in this setting consists of a set of tables for every possible context up to size k for any \norder k model. Each context is a state, and each entry corresponding to a symbol frequency is indicative \nof its probability of occurrence in that context. An optimal model is one that represents the best pos\u00adsible \nstatistics for the actual data that is to be compressed. Unfortu\u00adnately, computing an optimal model in \ngeneral is undecidable [9]. It can be shown that arithmetic coding achieves optimal com\u00adpression2 for \na given probability model D [7]. Speci.cally, if X is a random variable representing events over a set \nX with a proba\u00adbility distribution D, then the average number of bits required to encode any event from \nX using arithmetic coding is equal to the entropy [7] of D which is de.ned as follows. def H(D)= - P(X \n= x)log2 |P(X = x)| x.X Note: H(D) is the binary entropy function, and P(X = x) is the probability of \noccurrence of the event X = x. 2 In order to achieve optimal overall compression of data, the model must \nbe an optimal model for that data. procedure computeBLModel (G) Assume: (a) G =(V, E, s, t) is a DAG. \n(b) .v . V , a map D : E . [0, 1] that is initially unde.ned. Returns: a model D such that .v . V, D(e)=1. \ne.out(v) 1: Nt := 1; 2: for all nodes v . V in reverse topological order do 3: Nv := e.out(v) Ndest(e); \n4: for all edges e . out(v) do 5: D(e):= Ndest(e)/Nv; 6: end for 7: end for Figure 12. The Ball-Larus \nalgorithm as a model computation process. Example. For the model D described in Table 1, the entropy \nH(D)= -P(a)log2 |P(a)|-P(b)log2 |P(b)|-P(c)log2 |P(c)|-P(d)log2 |P(d)|- P(e)log2 |P(e)|- P(!) log2 |P(!)| \n= -(0.2log2 |0.2| +0.3log2 |0.3|+0.1log2 |0.1|+0.2log2 |0.2| + 0.1log2 |0.1| +0.1log2 |0.1|)=2.45 bits. \n 4.2 The Ball-Larus algorithm and Arithmetic coding We will now show that the Ball-Larus pro.ling algorithm \nis in fact an instance of arithmetic coding for paths in a DAG G =(V, E, s, t). We .rst observe that \nboth path numbering and arithmetic coding have similar objectives, i.e., to compactly and uniquely encode \nstrings from an input alphabet. In path numbering, the input alphabet is the set of edges through a DAG, \nand the input strings are paths through the DAG. We also .nd that the process of assigning weights to \nthe edges of the DAG corresponds to the process of computing a probability model. However, unlike arith\u00admetic \ncoding where computing an optimal model is undecidable in general, an optimal model for paths through \na DAG can in fact be computed for the following reasons: (a) the set of strings that can occur is known \na priori; this is precisely the set of all paths through the DAG, and (b) the order in which edges can \noccur in paths is determined by the structure of the graph, which is also known a priori. Based on these \nobservations, we derive a model computa\u00adtion procedure for paths through a DAG that is equivalent to \nthe Ball-Larus algorithm. The procedure computeBLModel, shown in Figure 12, takes a DAG G as an input \nand assigns a probability D(e) to every edge e in the graph. The resulting model is a .nite context model, \nwhere the context of an edge is its source node, and the probability assigned to an edge is the probability \nof the edge being traversed given that the source node has been reached. One can easily verify that .v \n. V , D(e)=1 e.out(v) For every path p . paths(G), the probability P(p) induced by the model D is de.ned \nas follows. def P(p)= P(e) e.edges(p) It also follows by a simple counting argument that for every p \n. paths(G), P(p)=1/|paths(G)| and P(p)=1. p.paths(G) Denote by DG, the probability distribution over \nthe set paths(G) it follows immediately that the entropy H(DG) is equal to log2 |N|. Symbol Probability \nRange e1 2/3 [0, 2/3) e2 1/3 [2/3, 1) e3 1/2 [0, 1/2) e4 1/2 [1/2, 1) e5 1 [0, 1) e6 1/2 [0, 1/2) e7 \n1/2 [1/2, 1) e8 1 [0, 1) Table 2. The Ball-Larus probability model DG for the graph G in Figure 13 computed \nby computeBLModel. procedure pathEncoder (p, D, N) Assume: G =(V, E, s, t) is a DAG and p =(v1,...,vk) \n. paths(G), {vi . V }1=i=k. Returns: path identi.er for p. 1: in := [0,N); 2: for all i =1 to k - 1 \ndo 3: e := (vi,vi+1); 4: [x, y):= in; 5: n := y - x; 6: let [r, r i) be the range for e de.ned by D; \n7: in := [x + LrnJ,x + ir i nl); 8: end for 9: [x, ):= in; 10: return x; Figure 14. The coding algorithm \nthat takes a model D and path p . paths(G) as input, and returns the path identi.er or the encoding for \np. Example. Consider the graph G shown in Figure 13. The model D computed by the procedure computeBLModel \nis given in Table 2. We will now describe the procedure pathEncoder that takes a path p . paths(G), the \nmodel D computed by computeBLModel and N = |paths(G)| as input, and computes its Ball-Larus iden\u00adti.er. \nThis is the analogous to the procedure computePathIdenti.er in Section 2.2. Since arithmetic coding is \noptimal [7], that is, it achieves the entropy of the input model, pathEncoder (which is an arithmetic \ncoder) is also optimal. We will make this connection explicit in the following example. Example. Consider \nthe graph G shown in Figure 13. Let D be the model computed by the procedure computeBLModel as given \nin Table 2. For an input path sbcdt, pathEncoder(sbcdt, D, N) works as follows. We have N =6, and the \nalgorithm starts by assigning in := [0, 6). The .rst edge encountered along this path is e2, and therefore \nthe interval in is set to [4, 6). After seeing the next edge e5, pathEncoder chooses the same interval \nin =[4, 6). For the next edge e6, the interval is narrowed down to in =[4, 5), and .nally for the last \nedge e8, the interval is set to in =[4, 5). Therefore, pathEncoder returns 4 as the path identi.er for \nthe path sbcdt. Note that this is precisely the Ball-Larus identi.er for this path as is evident from \nFigure 3. 4.3 The PPP algorithm and arithmetic coding In Section 3.1, we described an algorithm that \ncompactly num\u00adbers a subset of interesting paths I . paths(G) in a DAG G =(V, E, s, t). We now show that \nthe PPP algorithm is equiv\u00adalent to an arithmetic coding scheme that uses a maximal con\u00adtext model for \nencoding paths in G. As described in Section 3.1, the procedure computePPPIncrements computes for every \npair (e, q) . E \u00d7 prefix(p, e), an interval inte,q that represents the range of partial identi.ers of \ninteresting paths through e. At every node v . V , these intervals are used to compute the weights as\u00adsociated \nwith edges emanating from v. It can be shown that this procedure is equivalent to computing a .nite context \nmodel with pre.xes as the context. Consider the simple case of a node v with two outgoing edges e1 and \ne2. Assume that a single pre.x q induces intervals inte1,q and inte2,q on the edges e1 and e2 respectively. \ninte1,q De.ne cisv,q = inte1,q + inte2,q, and p = . Then the cisv,q model Dv,q at node v is de.ned as \nfollows. Symbol Probability Range e1 p [0,p) e2 1 - p [p, 1) Computing the model is more involved when \nmultiple pre.xes in\u00adduce intervals on the outgoing edges of a node. The problem arises because each outgoing \nedge may be associated with multiple prob\u00adabilities, one for each valid pre.x at node v. However, unlike \ntra\u00additional context models, an edge in the DAG cannot be associated with multiple probabilities. We \novercome this problem by using the join operator (de.ned in Section 3.1) to compute a conservative ap\u00adproximation \nof the individual models (which we refer to as Dv). Due to the join, certain edges may be assigned smaller \nprobabilites than required. Consequently, the number of bits required to encode interesting paths through \nthose edges may increase. The .nal model DG is a combination of all models Dv,v . V . Finally, the process \nof computing the PPP identi.er for an interesting path p . I corresponds to calling the procedure pa\u00adthEncoder \nwith parameters p, DG and N = |ints,s| (assigned to the start node s . V ). Example. Consider the graph \nG shown in Figure 13. Let the set of interesting paths be I = {sacdt, sact, sbct}. Then the probability \nmodel DG computed by computePPPIncrements is shown in Ta\u00adble 3. For the input path sact, pathEncoder(sact, \nDG, N) works as follows. We have N =3, and the algorithm starts by assigning in =[0, 3). The .rst edge \nencountered along this path is e1 (model = Ds,s), and therefore the interval in is set to [0, 2). After \nseeing the next edge e4 (model = Da,sa), pathEncoder chooses the same interval in =[0, 2). For the next \nedge e7 (model = Dc,sac), the interval is narrowed down to in =[1, 2), and therefore pathEn\u00adcoder returns \n1 as the path identi.er for the path sact. Note that Table 3. The PPP probability model DG for the graph \nG in Figure 13 computed by computePPPIncrements. Model Symbol Probability Range Ds,s e1 e2 2/3 1/3 [0, \n2/3) [2/3, 1) Da,sa e3 e4 0 1 empty [0, 1) Db,sb e5 1 [0, 1) Dc,sac e6 e7 1/3 2/3 [0, 1/3) [1/3, 1) Dc,sbc \ne6 e7 1/3 2/3 [0, 1/3) [1/3, 1) Dd,sacd e8 1 [0, 1) this is precisely the PPP identi.er for this path \nas is evident from Figure 3. This characterization of PPP as a model computer and encoder of paths works \ndue to the fundamental invariant that the intervals in PPP do not overlap (this follows from Lemma 5). \n 4.4 Analysis of the PPP algorithm The Ball-Larus algorithm computes an edge weight assignment such that \nd (the objective function for Problem A) is equal to 1. Therefore, it is an optimal algorithm for those \nproblem A instances for which the interesting paths are all paths, that is, I = paths(G). Information \ntheoretically, this corresponds to saying that all paths in the graph are equally likely (and there is \nno bias towards any set of paths) from the previous section, the entropy for such a distribution (say \nD) is equal to log2 |paths(G)|.paths(G)= 2H(D) . In the previous section, we also saw that the procedure \ncom\u00adputePPPIncrements computes a probability model for a DAG G and a set of interesting paths I .paths(G). \nIntuitively, this cor\u00adresponds to computing a probability distribution D that is biased towards the interesting \npaths over the uninteresting ones. Since PPP essentially mimics an arithmetic coder, the total number \nof bits required to represent the set of paths distributed according to the model D is equal to the entropy \nH(D) .the interval size 2H(D)l that PPP computes is equal to i. Therefore, the compact\u00ad 2H(D) ness that \nPPP achieves is , and this is parameterized over |I| how precise the model D is. In Theorem 1, we show \nthat this model D computed by computePPPIncrements is indeed optimal. We now state a variant of Problem \nA and prove that PPP computes the optimal solution to this problem. Problem B (Optimal Edge Assignment): \nGiven a DAG G = (V, E, s, t) and a set of interesting paths I .paths(G), compute an edge assignment W \n: E . Z that satis.es the following conditions. 1. uniqueness: .p, q .I, pathid(p)=.pathid(q), 2. compactness: \nThe compactness measure . de.ned by  2H(D) def . = |I| is minimized, where D is any probability distribution \non paths(G) induced by a model DG. We now state and prove the main result in our analysis. THEOREM 1. \nGiven a DAG G =(V, E, s, t) and a set I . paths(G), the procedure computePPPIncrements computes the optimal \nsolution to Problem B. Proof: It follows from Section 4.3 that the PPP algorithm computes a maximal context \nmodel DG for a given set of interest\u00ading paths in G. The model is a precise context model because it \nuses the largest context possible, which is the entire pre.x. Since H(Dpaths(G)) the model DG is optimal, \n. = 2 (where Dpaths(G) |I| is the probability model over paths(G) induced by DG) is also optimal (this \nfollows from the optimality of the pathEncoder pro\u00adcedure, which essentially mimics an arithmetic coder), \nand the theorem follows. From Section 3.3, it is clear that any algorithm (such as PPP) that maintains \nthe invariant stated in Lemma 5 will not be able com\u00adpute an optimal d. On the other hand, our experiments \ndescribed in Section 5 also indicate that the objective function . minimized by our algorithm is close \nto the minimal d (the objective function for Problem A) for most graphs and their associated interesting \npaths, and the interval size is small enough for the path pro.ler to use an array to track interesting \npaths.  5. Experimental evaluation We have implemented the preferential path pro.ling algorithm using \nthe Scale compiler infrastructure [10]. A few key features of our implementation are listed below. Representing \npaths and pre.xes. While a user is free to specify the set of interesting paths in several ways, we choose \nto rep\u00adresent the interesting paths using their Ball-Larus identi.ers. Similarly, we represent a pre.x \nusing the cumulative sum of the Ball-Larus weights along the edges of the pre.x. It is easy to see that \nthis sum is unique for each pre.x leading to a given node.  Register usage. Unlike traditional path \npro.ling, preferential path pro.ling requires two registers, one for PPP counts and one for Ball-Larus \ncounts. Our experiments suggests that the use of two registers instead of one does not add to the overheads \nof pro.ling.  Counter optimizations. All counter placement optimizations [3] used in the Ball-Larus \nalgorithm also apply to the PPP counter. These include reducing the number of initialization and incre\u00adment \noperations by placing weights only on the edges that do not belong to a maximal spanning tree of the \nDAG, pushing counter initialization downwards along the edges of the DAG and merging the initializations \nwith the .rst increments. In our implementation, we ensure that both PPP and Ball-Larus counter updates \noccur on the same edges.  Hash table usage policy. The default Ball-Larus pro.ler is con\u00ad.gured to use \na hash table instead of an array when the total number of paths through the procedure exceeds a threshold. \nHowever, the policy for hash table use in preferential path pro\u00ad.ling depends on the speci.c scenario \nin which the pro.ler is used. For instance, in residual path pro.ling, where the goal is to detect the \noccurrence of untested (uninteresting) paths, a hash table may never be used, even when the PPP identifers \nallocated to the tested (interesting) paths are large. Here, the pro.ler implementation may decide to \nignore all tested paths with PPP identi.ers greater than a threshold, in essence treat\u00ading them as untested \npaths. As a result, a few of the tested paths may appear as untested during program execution. Such false \npositives may be acceptable since PPP ensures that interesting paths are assigned compact numbers. In \naddition, they can be  easily weeded out off-line. A policy that switches to using hash tables when \nthe PPP identi.ers are large may also be used in other applications. Although our implementation supports \nboth modes, we report our results using the former policy. Additional checks. Before indexing the path \narray using PPP identi.ers, our pro.ler must check for an under.ow/over.ow, which can result when an \nuninteresting path occurs. Our exper\u00adiments suggest that these additional checks do not add to the cost \nof preferential pro.ling since they are highly biased and easy to predict. We evaluated our pro.ler implementation \nusing benchmarks from the SPEC CPU2000 suite. We simulated a realistic residual pro.ling scenario. We \n.rst collected a path pro.le of the bench\u00admarks using the Ball-Larus pro.ler for the standard reference \nin\u00adput. We then assumed that all paths exercised during the reference run were interesting (including \nprocedures where several hundred paths were exercised). These were fed to the preferential pro.ler, which \ngenerated a new instrumented binary. All binaries were run to completion using the reference input on \nan Alpha 21264 proces\u00adsor running Digital OSF 4.0. Each binary was executed 5 times and the minimum of \nthe execution times (measured using hardware cy\u00adcle counts) was used for comparison. Figure 15 shows \nthe percentage overheads of the two schemes relative to execution time of the un-instrumented binary. \nWe .nd that Ball-Larus pro.ling incurs an average overhead of 50% with a maximum of 132%. On the other \nhand, the preferential path pro.ler incurs an average overhead of 15%, with a maximum of 26%. We attribute \nthe low pro.ling overheads of PPP to (a) elimination of expensive hash operations, and (b) judicious \nallocation of counters for pro.ling (the size of the counter array is proportional to the number of interesting \npaths and not the number of potential paths). Although re.ected in the overheads, the real ef.cacy of \nthe preferential pro.ling algorithm lies in the compactness measure d that it achieves. We illustrate \nthe compactness measure achieved by our algorithm in Figure 16, which plots the size of the interval \nallocated to interesting paths vs. the number of interesting paths for procedures from programs in the \nSPEC CPU2000 benchmark suite. As aforementioned, all paths exercised during one reference run were selected \nas interesting paths. The .gure suggests that our pro.ling scheme achieves a dclose to 1 for a vast majority \nof the procedures, although the value tends to increase for procedures with a large number of paths (100-300). \nWe also found a very small number of cases with d>10 (not shown in this .gure), most of them in the benchmark \ncrafty, a chess program known to have complex control .ow.  6. Related work Several researchers have \nproposed a variety of techniques to re\u00adduce the overhead of Ball-Larus style path pro.ling [2, 8, 6]. \nSelec\u00adtive path pro.ling uses a variation of Ball-Larus numbering where edges are visited in a speci.c \norder to ensure that interesting paths are assigned a unique number that is higher than the non-unique \nnumbers assigned to other paths, while minimizing the number of counter updates needed to compute the \npath number. However, they found that once the number of interesting paths was .ve or larger, their edges \ncovered most of the DAG and their technique offered little advantage over Ball-Larus numbering. In addition, \nthey made no attempt to ensure that the interesting paths are compactly num\u00adbered. Instead of minimizing \nthe number of counter updates needed to compute a path number, we optimize the compactness of num\u00adbers \nassigned to interesting paths. This reduces overhead by en\u00adabling the use of a path array in place of \na hash table. Our compact numbering scheme is effective even when the number of interesting paths is \nlarge. Both targeted path pro.ling and practical path pro.ling attempt to ef.ciently pro.le hot program \npaths starting from an edge pro.le by eliminating unneeded instrumentation. Targeted path pro.ling eliminates \npro.ling cold paths by excluding cold edges and not instrumenting paths that the edge pro.le predicts \nwell. It uses Ball-Larus numbering for labelling the remaining paths. Practical path pro.ling attempts \nto improve over targeted path pro.ling using a variety of techniques to eliminate a larger number of \npaths. It also performs intelligent instrumentation placement to further reduce overhead. To minimize \noverhead, practical path pro.ling may need to classify warm edges as cold and consequently could compromise \nthe quality of the path pro.le. It also uses Ball-Larus numbers to uniquely identify the remaining paths. \nOur technique is orthogonal to both as it proposes a new dense numbering scheme for interesting paths \nthat minimizes the overhead of pro.ling these paths. It is also more general as it can be applied to \nscenarios such as residual path pro.ling (detecting paths not exercised by a test suite), where the techniques \nthat targeted/practical path pro.ling use to reduce instrumentation overheads do not apply. Other work \nin path pro.ling has focused on collecting richer path pro.les. Interprocedural path pro.ling extends \nBall-Larus pro\u00ad.ling beyond intraprocedural paths [11]. Tallam et al. proposed a technique to pro.le \noverlapping path fragments from which inter\u00adprocedural and cyclic paths can be estimated [14]. Both these \ntech\u00adniques have considerably higher overhead than the Ball-Larus tech\u00adnique for pro.ling intraprocedural, \nacyclic paths and our scheme can potentially help reduce this overhead. 7. Conclusion This paper presents \npreferential path pro.ling, a new technique that pro.les a speci.ed subset of all program paths with \nvery low overhead. Preferential path pro.ling labels the paths of interest compactly using a novel numbering \nscheme. By drawing parallels between arithmetic coding and path numbering we establish an optimality \nresult for our compact path numbering scheme. This compact path numbering allows our implementation to \nuse array\u00adbased counters instead of hash table-based counters for gathering path pro.les and signi.cantly \nreduces execution time overhead.  Acknowledgments We thank Sriram Rajamani, Stefan Schwoon and Aditya \nThakur for helpful comments on this work. Special thanks are due to Stefan for proving Lemma 3. References \n[1] G. Ammons and J. R. Larus. Improving data-.ow analysis with path pro.les. In ACM SIGPLAN Symposium \non Programming Language Design and Implementation (PLDI), pages 72 84, 1998. [2] T. Apiwattanapong and \nM. J. Harrold. Selective path pro.ling. In Workshop. on Program Analysis for Software Tools and Engineering \n(PASTE), pages 35 42, 2002. [3] T. Ball and J. R. Larus. Ef.cient path pro.ling. In International Symposium \non Microarchitecture (MICRO), pages 46 57, 1996. [4] T. Ball and J. R. Larus. Programs follow paths. \nTechnical Report MSR-TR-99-01, Microsoft Research, 1999. [5] T. Ball, P. Mataga, and S. Sagiv. Edge pro.ling \nversus path pro.ling: The showdown. In ACM SIGPLAN Symposium on Principles of Programming Languages(POPL), \npages 134 148, 1998. [6] M. D. Bond and K. S. McKinley. Practical path pro.ling for dynamic optimizers. \nIn International Symposium on Code Generation and Optimization (CGO), pages 205 216, 2005. [7] T. M. \nCover and J. A. Thomas. Elements of Information Theory. John Wiley &#38; Sons, Inc., N. Y., 1991. [8] \nR. Joshi, M. D. Bond, and C. B. Zilles. Targeted path pro.ling: Lower overhead path pro.ling for staged \ndynamic optimization systems. In International Symposium on Code Generation and Optimization (CGO), pages \n239 250, 2004. [9] A. Kolmogorov. Three approaches to the quantitative de.nition of information. Prob. \nPeredach Inform, 1(1):3 11, 1965. [10] K. S. McKinley, J. Burrill, M. D. Bond, D. Burger, B. Cahoon, \nJ. Gibson, J. E. B. Moss, A. Smith, Z.Wang, and C. Weems. The Scale compiler. http://ali-www.cs.umass.edu/Scale, \n2005. [11] D. Melski and T. W. Reps. Interprocedural path pro.ling. In Proceedings of the 8th International \nConference on Compiler Construction (CC), pages 47 62, 1999. [12] E. Perelman, T. M. Chilimbi, and B. \nCalder. Variational path pro.ling. In Parallel Architectures and Compilation Techniques 05 (PACT), pages \n7 16, 2005. [13] J. Rissanen and G. G. Langdon. Arithmetic coding. IBM J. Res. Develop., 23(2):149 162, \n1979. [14] S. Tallam, X. Zhang, and R. Gupta. Extending path pro.ling across loop backedges and procedure \nboundaries. In International Symposium on Code Generation and Optimization (CGO), pages 251 264, 2004. \n[15] I. H. Witten, R. M. Neal, and J. G. Cleary. Arithmetic coding for data compression. Communications \nof the ACM, 30(6):520 540, 1987.  \n\t\t\t", "proc_id": "1190216", "abstract": "Path profiles provide a more accurate characterization of a program's dynamic behavior than basic block or edge profiles, but are relatively more expensive to collect. This has limited their use in practice despite demonstrations of their advantages over edge profiles for a wide variety of applications.We present a new algorithm called preferential path profiling (PPP), that reduces the overhead of path profiling. PPP leverages the observation that most consumers of path profiles are only interested in a subset of all program paths. PPP achieves low overhead by separating interesting paths from other paths and assigning a set of unique and compact numbers to these interesting paths. We draw a parallel between arithmetic coding and path numbering, and use this connection to prove an optimality result for the compactness of path numbering produced by PPP. This compact path numbering enables our PPP implementation to record path information in an array instead of a hash table. Our experimental results indicate that PPP reduces the runtime overhead of profiling paths exercised by the largest (ref) inputs of the SPEC CPU2000 benchmarks from 50% on average (maximum of 132%) to 15% on average (maximum of 26%) as compared to a state-of-the-art path profiler.", "authors": [{"name": "Kapil Vaswani", "author_profile_id": "81100057042", "affiliation": "Indian Institute of Science, Bangalore", "person_id": "P715345", "email_address": "", "orcid_id": ""}, {"name": "Aditya V. Nori", "author_profile_id": "81320493380", "affiliation": "Microsoft Research India", "person_id": "P825356", "email_address": "", "orcid_id": ""}, {"name": "Trishul M. Chilimbi", "author_profile_id": "81100578606", "affiliation": "Microsoft Research", "person_id": "P285175", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1190216.1190268", "year": "2007", "article_id": "1190268", "conference": "POPL", "title": "Preferential path profiling: compactly numbering interesting paths", "url": "http://dl.acm.org/citation.cfm?id=1190268"}