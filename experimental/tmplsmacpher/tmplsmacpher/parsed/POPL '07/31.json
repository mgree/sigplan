{"article_publication_date": "01-17-2007", "fulltext": "\n Modular Veri.cation of a Non-Blocking Stack Matthew Parkinson Richard Bornat Peter O Hearn Computer \nLaboratory, School of Computer Science Department of Computer Science University of Cambridge Middlesex \nUniversity Queen Mary, University of London Cambridge CB3 0FD, UK London NW4 4BT, UK London E1 4NS, UK \nmatthew.parkinson@cl.cam.ac.uk R.Bornat@mdx.ac.uk ohearn@dcs.qmul.ac.uk Abstract This paper contributes \nto the development of techniques for the modular proof of programs that include concurrent algorithms. \nWe present a proof of a non-blocking concurrent algorithm, which pro\u00advides a shared stack. The inter-thread \ninterference, which is essen\u00adtial to the algorithm, is con.ned in the proof and the speci.cation to the \nmodular operations, which perform push and pop on the stack. This is achieved by the mechanisms of separation \nlogic. The ef\u00adfect is that inter-thread interference does not pollute speci.cation or veri.cation of \nclients of the stack. Categories and Subject Descriptors D.2.4 [Software Engineer\u00ading]: Program Veri.cation \ncorrectness proofs, formal methods validation; F.3.1 [Logics and meanings of programs]: Specifying, Verifying \nand Reasoning about Programs General Terms Languages, Theory, Veri.cation Keywords Separation Logic, \nConcurrency, Non-blocking 1. Introduction Concurrent separation logic [11, 4] is a program resource logic \nbased on the notion that separate parts of a program that depend on separated resources can be dealt \nwith independently. Dijkstra s advice [5] on the design of concurrent programs was to limit in\u00adterference \nto (rare) moments of synchronisation. So far, proofs in concurrent separation logic have followed this \nadvice, considering the resources of separate threads independently, and at synchronisa\u00adtion points temporarily \nadding the resources owned by a semaphore or some other unit of mutual exclusion. This has led to pleasingly \nmodular proofs of some well-known problems: for example, paral\u00adlel merge sort, pointer-transferring buffers, \nand the classic readers\u00adand-writers [11, 1, 2]. But synchronisation using units of mutual exclusion is \nnot the only way of controlling interaction between threads, and in multi\u00adprocessor concurrent programming \nit is widely criticised for ef\u00ad.ciency reasons because threads may often have to wait to gain exclusive \naccess to the unit. In so-called non-blocking concur\u00adrency, threads attempt to make concurrent changes \nto shared data structures, looping and trying again if a particular attempt fails to achieve the desired \nresult. It is beyond the scope of this paper to Permission to make digital or hard copies of all or part \nof this work for personal or classroom use is granted without fee provided that copies are not made or \ndistributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. POPL 07 January 17 19, 2007, Nice, France. Copyright c &#38;#169; \n2007 ACM 1-59593-575-4/07/0001. . . $5.00. judge if and when such mechanisms are to be preferred to mutual \nexclusion techniques. Previously, the most effective formal treatment of non-blocking concurrency has \nbeen as interference in the rely-guarantee ap\u00adproach [9, 12]. Each thread must specify properties which \nit guar\u00adantees to preserve in interaction with the shared data, and other properties which it relies \non other threads to preserve. The guaran\u00adtee must be preserved by each action of the thread, and if neces\u00adsary \nit is possible to appeal to the rely conditions in showing that the guarantee holds. Interference between \nthreads thus .oods the proof of every client: every single instruction in every single thread must ensure \nthe guarantees of its thread, because every speci.ca\u00adtion must include those guarantees. We show in the \nproof below that in separation logic interference may be contained even in non-blocking concurrency. \nWe equip a shared data structure with an invariant, and we provide operations on the data structure as \nprocedures. We allow non-blocking inter\u00adaction by allowing concurrent executions of the procedures in \ncon\u00adcurrent threads. The procedures have conventional pre-and post\u00adconditions which do not involve the \ninvariant. Within the bodies of the procedures the invariant is exposed, and the proof engages with it. \nEven though those proofs may be horrible, their horrors are con.ned, and we can consider threads as independent \nwhen outside those procedures. Non-blocking concurrent algorithms rely for their correctness on certain \nhardware properties. They all rely on the atomic nature of single-word access to the store: that is, \nthat reads and writes to any particular single-word location are effectively serialised and cannot overlap \nor be reordered. As in our example, many also rely on a CAS (compare and swap) instruction that can atomically \nread a location, compare its value with some previously-determined value and, if equal, overwrite it \nwith a new value. An atomic access to the data structure (a single read or write of a single-word value, \nor a single CAS) is treated as a unit of mutual exclusion: we temporarily add the resources described \nby the invariant, prove the effect of the atomic operation, re-establish the invariant, and once again \nseparate ourselves from it. 2. Concurrent Separation Logic We simplify the Brookes/O Hearn presentation \n[11, 4] by using a single invariant rather than a set indexed by resource names. We refer readers to \nthose descriptions, and give here only the differ\u00adences of our usage. We make use of procedure-call speci.cations \n{Q} f(x) {R} and an invariant for the data structure shared be\u00adtween threads. Then .\u00af .\u00af G; I f QCR means \nthat with procedure speci.cations G, and a shared-data in\u00advariant satisfying I, the command C satis.es \nthe speci.cation { Q} C{ R} . The shared data structure can only be validly accessed within atomic operations, \nand to do so brings the invariant temporarily into play. . \u00af.\u00af G; emp f Q*I C R*I .\u00af .\u00af G; I f Q atomic{C} \nR (1) Cmust be executed in mutual exclusion with other atomic com\u00admands. In principle this might be achieved \nby the mechanisms of transactional memory [8, 6], but in this paper we rely on the se\u00adrialisation properties \nof the hardware: C must invoke at most one single-word read or write in the shared data structure governed \nby I,or be a CAS instruction. Semantically we rely on Brookes proof of soundness, which enables us to \ntreat the shared data structure as a single resource and use a version of separation logic s CCR rule.1 \nParallel composition requires that we satisfy pre-conditions sep\u00adarately, and guarantees that the post-conditions \nare separate on ter\u00admination. The data-structure invariant is shared between the two parallel compositions, \njust like named resource invariants in con\u00adcurrent separation logic. .\u00af.\u00af .\u00af.\u00af G; I f Q1 C1 R1 G; If \nQ2 C2 R2 .\u00af .\u00af G; I f Q1 *Q2 C1 1 C2 R1 *R2 (2) The bodies of module procedures can make use of the invariant; \nthe rest of the program cannot. We state the module rule for a single procedure de.nition; extension \nto several procedures is obvious. Note that the invariant is not incorporated into the pre-and post\u00adconditions \nof Cf : atomic instructions inside Cf can make use of the invariant, but the procedure body is not itself \ntreated as an atomic instruction.2 .\u00af .\u00af G; If Qf Cf Rf .\u00af .\u00af G,{ Qf } f(xf ){ Rf } ; emp f QCR .\u00af .\u00af \nG; emp f Q*I module f(xf )= Cf in C R*I (3) In order to construct our invariant we have had to make some \nuse of permissions [1]. E . F can be read as a total permission for the heap cell at location E i.e. \npermission to read, write and dispose. It can be split into a collection of read-only permissions, which \ncan then be given to separate threads, enabling read-only sharing. In this paper we require only that \nthe invariant shares some permissions with each thread, so that we can split a total permission into \ntwo read permissions, one for the invariant and one for the thread: rr E E- . F . F .. . F*E -(4) To \nread a heap cell we only need a read permission: rr { E= N. N - . M. x= M} . M} x:= [E] { N - 1 It follows \nthat atomic{atomic{ ... }} would fail to terminate. 2 In this rule we have used emp as the invariant \nformula outside the proof of Cf . It would be possible to take a more general approach: the module rule \ncan be derived from a version of the make-named-resource rule .\u00af .\u00af G; I1 * I2 f QCR . \u00af.\u00af G; I1 f Q* \nI2 C R* I2 and the resource-weakening rule .\u00af .\u00af G; I1 f QCR .\u00af .\u00af G; I1 * I2 f QCR Note that these two \nrules can be combined to give the standard separation\u00adlogic frame rule. pop() { local t,n; push (b) { \nwhile(true) { local t,n; t = TOP; for(n=0; n<=THREADS; n++)if (t == nil) break; if (H[n] == b) H[tid] \n= t; return false; if (t != TOP) while(true) { continue; t = TOP; n = t->tl; b->tl = t; if (CAS(&#38;TOP,t,n)) \nif (CAS(&#38;TOP, t, b)) break; break; }}  H[tid] = nil; return true return t } } Figure 1. Source for \nnon-blocking stack. The hazard pointer code is highlighted. For compactness of the presentation we use \na for loop in push to perform the scan operation from Michael s algo\u00adrithm. If a read permission is in \nthe invariant, any thread can read a shared cell in an atomic operation, but only the thread that has \nthe matching permission can write it (also in an atomic operation). 3. Speci.cation of the algorithm \nMichael s algorithm, shown in .gure 1, implements a shared stack, with operations push and pop. In [10], \nMichael proves the correct\u00adness of the stack algorithm using hazard pointers. We can see this as a simple \nimplementation of a storage alloca\u00adtor for .xed-size heap records: pop isa kindof malloc or cons, push \nakind of free, the stack a kind of freelist. We can give separation-logic safety speci.cations for his \noperations, allowing for the possibility that pop will not work on an empty stack and, because of the \nvagaries of his mechanism, that push might fail: Method Pre-condition Post-condition pop() emp (ret ) \n. (ret =nil . emp) . push(x) x. (ret . emp) . (\u00acret . x ) . Note that this speci.cation need say nothing \nabout the way that push and pop are implemented. A client is completely insulated from those details. \n3.1 A client Because the interface presented by the stack is somewhat inconve\u00adnient, and to demonstrate \nthat the client is insulated from the inter\u00adference used in the stack, we show how to build a simple \nmemory manager. We require an alloc mechanism, which works even if the stack is empty. We presume that \nthe records on the heap are single-cell records, which originated from the system memory allocator new. \nAlloc tries pop, and if that fails, uses the (perhaps much slower) new. .\u00af emp alloc() { local y; y=pop(); \nif (y==nil) y=new(); return y; } .\u00af ret. We present a brief sketch of alloc s veri.cation: .\u00af emp y=pop(); \n.\u00af (y =nil. emp). y . if (y==nil) { .\u00af emp y=new(); .\u00af y . } .\u00af y . Veri.cation is trivial depending \nonly on the speci.cation of pop and the axiom {emp} y=new(); {y .}. The point is that we can make these \nproof steps without any reference to the complex interference that we will need to account for when proving \nthe module procedures, illustrating the sense in which interference is contained. We require a free which \ndoes not bother us if pushing fails. This is not hard: we can keep a local list fl of the elements freed \nwhich have not yet been pushed, and each time we call free,try to push the lot. Speci.cation requires \na list predicate for our single\u00adcell records. def list(x)=(x =nil. emp). (.n \u00b7 (x . n* list(n))) (5) \n. \u00af list(l) . \u00af x . pushall(l) { free(x) { local n; local y; if(l==nil) return l; y = fl; n = [l]; [x] \n= y; n = pushall(n); fl = pushall(x); if(push(l)) return n; } else { [l] = n; return l; } . \u00af emp } . \n\u00af list(ret) Free uses the local freelist fl, in a straightforward use of separation-logic s hypothetical \nframe rule: the body has precondi\u00adtion x . *list(fl)and postcondition emp *list(fl). Veri.ca\u00adtion of \npushall is straightforward, depending only on the speci.\u00adcation of push. An essential point is that veri.cation \nof alloc, free and pushall does not have to consider the non-blocking stack be\u00adyond the speci.cations \nof push and pop. The proof is completely isolated from the complexity of the interference that goes on \ninside those operations. 3.2 Without hazard pointers Michael assumes a .xed population of threads. Each \nthread has its own local variables, named in lower case t, n, b,etc.Theyshare the stack through a global \nvariable named TOP, which can be read atomically (e.g. in t=TOP) and is written in atomic CAS instruc\u00adtions \notherwise involving local variables (e.g. in CAS(&#38;TOP,t,b)). Michael describes .rst an algorithm \nin which the highlighted code of .gure 1 is elided. In this algorithm, push(b) works reliably and need \nnot return a success/fail result (i.e. it has postcondition emp): if it .nds a stack whose head is pointed \nto by t, it atomically replaces it with one whose head is pointed to by b. Pop is .awed, however: the \nfact that the head of the stack is pointed to by t does not imply that there is a second element pointed \nto by a value n which was previously found in the cell pointed to by t.By removing the top of the stack, \nmodifying what comes below and then replacing the original top element, another thread can destroy the \nTOP/n association in the temporal interval between n=t->tl and if CAS(&#38;TOP,t,n): see .gure 2. Michael \nrefers to this as the ABA problem: a stack may be in state A (with t on top), then change to state B \n(some other stack), then revert to state A (t on TOP TOP (2) TOP TOP  (4) Figure 2. ABA problem for \nstack algorithm without hazard point\u00aders. A thread begins to pop an element from the stack. It reads \nt and n, (1), but is then preempted. Another thread pops two ele\u00adments from the stack, (2), and pushes \nthe .rst element back onto the stack, (3). The original thread is rescheduled and attempts its CAS, which \nsucceeds and destroys the data-integrity of the stack, (4). top again) yet we know nothing else about \nthis second A-state other than the fact that the variable TOP stores the same value as before. 3.3 Stack \nusing hazard pointers To .x the ABA problem, Michael adds a global array H of haz\u00adard pointers . He assumes \na .xed collection of numbered threads: thread i alone writes to element H[i] of the array, but all threads \ncan read the elements written by others. In .gure 1 we assume that each thread stores its number in a \nlocal variable tid. The global array is, in fact, a heap record pointed to by H, and for readability \nwe write H[E] instead of [H+E]. Before pushing a cell, a thread scans H to see if the cell is recorded \nas a hazard pointer by some other thread: if it is, the push is delayed (return false); if not, then \nthe push proceeds, eventually returning true.Before popping a cell, a thread puts its address into its \nown element of H; then (and this is crucial!) checks that the cell is still on top of the stack before \ngoing on to read its tail and execute a CAS, clearing the hazard pointer before it returns. The effect \nis remarkable: the algorithm works, and it can be implemented on any machine that provides CAS. It is \na challenge to produce a formal proof in a program logic; in separation logic we can exploit the essential \nmodularity of the algorithm, con.ning use of the invariant to the bodies of push and pop.  4. The proof \nThe effectiveness of the hazard-pointer array in Michael s algo\u00adrithm derives from the following temporal \nproperty: Between execution of H[i]=t and assignment of another value to H[i], the cell pointed to by \nt will not be re-( ) moved from the stack and subsequently re-inserted. It is a subtle property: removal \nalone is possible; insertion alone is possible; insertion followed by removal is possible; only removal \nfollowed by insertion is prohibited. It is this property which dictates the test t==TOP in pop: if we \nknow that at some instant the cell is on top of the stack then, because H[i] =t, we know that it will \nnot be re-inserted until we change H[i]; it follows that if the CAS .nds it on top of the stack, then \nit has not been reinserted and the ABA problem is averted. A pushing thread can ignore hazard settings \nthat are made after it has popped a cell b from the stack, so it can 1 push (b) { 3 while(true) { 2 \nlocal t,n; 4 atomic{ t = TOP; } 3 for(n=0; n=THREADS; n++) { 5 if (t == nil) break; 4 atomic{ 6 atomic{ \nH[tid] = t; H [tid] = Req;} 5 if (H[n] == b) { G[tid] = Unset; return false;} 7 atomic{ if (TOP != t) \ncontinue; 6 G[tid] = NotHaz(b,{0,...,n}); 8 else H [tid] = Tail(t->tl); 7 } 9 } 8 } 10 atomic{ n = \nt->tl; 9 while(true) { 11 if (H [tid]!=Tail(n)) { H [tid]=Left; } 10 atomic{ t = TOP; } 11 b->tl = t; \n12 } 13 atomic{ if (CAS(&#38;TOP,t,n)) break; } 12 atomic{ if (CAS(&#38;TOP, t, b)) { G[tid] = Unset; \nbreak;} } 14 } 13 } 15 atomic{ H[tid] = nil; H [tid] = Unset ;} 14 return true; 15 } 16 return t; 17 \n} Figure 3. Algorithm with explicit atomicity assumptions and additional (highlighted) code to manipulate \nauxiliary state, which represent the informal temporal property certainly ignore any assignments that \noccur after it has begun to scan the array H. Clearly, the algorithm is over-cautious in the sense that \na popper uses the atomic test t==TOP as a surrogate for the more general property t is somewhere in the \nstack , and a pusher takes note of hazard-pointer settings that occur after it popped a cell but before \nit began to push but these are merely ef.ciency considerations. We also need to know a property of the \nstack: pushing and popping other cells does not affect the tail-value of cells already in the stack. \nOnce a cell is inserted into the stack, its tail value will ( ) not be altered until it is removed. In \norder to be able to express these properties in our invariant we consider a hazard pointer to be in one \nof four states: (Unset) H[tid] =nil; (Req) H[tid] = b but b has not been observed in the stack; (Tail(k)) \nwhile H[tid] = b, b has been observed in the stack with tail k; (Left) while H[tid] = b, b has been observed \nin the stack, but is now known not to be in the stack. If a hazard pointer is in Tail(k) or Left state, \nit ought not to be pushed: we call such an entry a con.rmed hazard. Unset and Req states merely help \ndeductions. We record these states in an auxiliary array H , and we use additional code in pop, shown \nin .gure 3, to manipulate this array. When we initially set a hazard pointer (line 6) its status is Req, \nbecause we have not observed the cell in the stack while it was hazard-pointered. If the atomic test \non line 7 fails, we know that t = TOP, and hence the value was in the stack while H[tid] = t, so we record \nthe tail-value of the cell (instantaneously, it is an auxiliary assignment) in H . When we perform n \n= t->tl on line 10, we may .nd that t->tl has changed: that is only possible if t has left the stack \n(sec\u00adond temporal property ( )), and then (because of the .rst temporal property ( )) it will not come \nback till we alter H[tid],sothe CAS will be certain to fail: we record that fact by altering H . On line \n15 we nullify our hazard pointer entry. In order to become a con.rmed hazard (state Tail() or Left)a \ncell must be observed in the stack. A cell that is being pushed is not in the stack:3 therefore, during \nthe push operation, no thread s hazard pointer can change from Req to Tail(). So a thread that is popping \ncan assign that cell s address to its H entry and it will not matter to a pushing thread that has gone \npast that entry in its hazard search: it cannot possibly become Tail() unless there is a successful CAS \nin push, and it cannot become Left unless it .rst becomes Tail(). For the pusher we need yet another \nauxiliary array G to capture that reasoning, with values Unset, if we are not currently trying to push \na value, and NotHaz(b, S), if we are trying to push b,and b is not a con.rmed hazard for the set of threads \nS. In .gure 3, we add the necessary code to push. 4.1 The Invariant Next we relate the informal temporal \nrestrictions and the auxiliary state added in .gure 3 in an invariant. We use T forthe setofthread identi.ers \n{0,..., THREADS}.We use h and h. as a mathematical representation of the contents of the arrays H and \nH . The hazard-pointer array h and hazard-status array h. restrict each node y of the stack with tail \nz as follows def RNode(y, z, h, h')= y . z ..i . T \u00b7 (h(i)= y) . (h'(i)= Tail(z) . h'(i)= Req) If y is \nhazard for thread i, h(i)= y,and y is in the stack then its status is either Tail(z) or Req: it cannot \nbe Left or Unset. We can then apply this restriction to every element of the stack, represented as a \nlist starting from y,as: 'def RList(y, h, h)= ` ''\u00b4 (y =nil.emp)..z. RNode(y, z, h, h) * RList(z, h, \nh)) Next we consider how G restricts the hazard pointer and status arrays. We want to record, for each \nthread i, two facts about the cell it might be pushing: (1) it is not a con.rmed hazard for any of the \nthreads whose hazard-pointers have already been checked in 3 This is a consequence of separation logic \ns separation principle: if all threads start with separated resources and the shared data structure has \na separated invariant, and if all threads communicate only with atomic commands, then separation will \nbe preserved. the for-loop; (2) it is not in the stack, and no other thread is pushing the same cell. \nHere we begin to play with permissions: crucially, we do not give G s total permission to the invariant, \nonly a read permission. This allows the invariant to depend upon the value of G, but does not give permission \nto any thread to modify G in an atomic operation. We give the other read permission for G[i] to thread \ni: that thread can then, in an atomic operation, modify that entry of G. Hence thread i can depend on \nthe value of G[i] even outside an atomic block. When pushing the cell pointed to by b, we should have \npreferred in the invariant to describe invariant properties of the stack and then to say also .\u00ac(b . \n* true) the cell does not appear anywhere in the stack. But this is not enough to guarantee that the \nsame cell cannot somehow be pushed by another thread (perhaps permission could be passed behind the scenes). \nWe need to know that any thread that has got part-way through the pushing process actually owns the cell \nit is pushing. To do this we have used two more kinds of permission: is given in .gure 4. We discuss \nthe four marked atomic commands in the rest of this sections. In (a), updating the hazards status requires \n' h(tid)= t .HCons(h, h) * t .n . HCons(h, h'[tid .Tail(n))]) * t .n This holds because of (8). In (b) \nwe case split on whether t is still in the stack. If t is in the stack, we can use the following to prove \nthe condition in the if will never hold. ' h(tid)= t .h(tid)= Tail(a). RList(y, h, h') .(t .x* true) \n.x = a If t is not in the stack, then we do not have permission to read the location, so we must use \nthe empty read rule: {emp}x=[E];{emp} (9) This allows us to perform a racy read on a location, but we \nknow - .F*E-. algorithm s optimism as we do the read, but will fail later whenA w permission allows writing \n(and reading, but we shall not need we get to the CAS. However, we know it is valid to set the status \ntoto appeal to that); an e permission allows nothing, but ensures that Left from the following property: \n the cell pointed to exists in the heap. We cannot have two existence permissions to the same cell:4 \n' h(tid)= t .RList(TOP,h,h) .\u00ac(t . * true) e w (6) nothing about x s value afterwards. This is really \ncapturing the E .F .. E ' ' (7) -. -. = EThread i, pushing the cell pointed to by b,gives b e e .E E*E \n' . RList(TOP,h,h[tid .Left]) . e- - .for itself: it has lost total permission and so no rogue thread \ncan begin to push a cell and then pass the cell to w to the This holds by induction on the de.nition \nof RList(x, h, h'),and because -.\u00ac(t . * true) distributes over *. In (c), we case-split on the hazards \nstatus. If the status is Left invariant and keeps b another thread that successfully pushes it. we know \nthe CAS will fail, because of the following implications: 'def ) r r -.t * H [tid]-.Left * Inv .TOP= \nt If the status is Tail(n) and the CAS fails, then the proof holds H[tid] HCons(h, h = 01 -. Unset) \n. (G[i]-.NotHaz(b, S) *b..j .S \u00b7(h(j)= b .h' r r (G[i] \u00ab trivially. If the CAS succeeds, then the proof \nfollows from the following implication: \u00aei.T \u00b7 e -. (j)= Req) ) @ A .b, S \u00b7 This has a particularly \nuseful property: h(tid)= t .h'(tid)= Tail(n) .RList(t,h,h' ) .t=nil '. t .n * RList(n,h[tid .(t, Tail(n))]) \n.b, i, h, t \u00b7 HCons(h, h) *b .x ' .HCons(h[i .b],h[i .Tail(x)]) *b .x (8) In (d), if the CAS succeeds \nwe require ' We combine the RList(TOP,h,h') and HCons(h, h') predi-HCons(h, h r -.NotHaz(b, T ) cates \ninto the algorithm s invariant. In addition, we also give a con-' ..i .T \u00b7h(i)= b .h(i)= Req crete representation \nto the auxiliary state h and h'. Note that in each case we give only a read permission to the invariant: \nit may depend to know the value being pushed it not a con.rmed hazard, and upon the whole array but only \nthread j can alter H[j] or H [j]. b .t* RList(t, h, h') * (.i .T \u00b7h(i)= b .h'(i)= Req) ) * G[tid] ' .RList(b, \nh, h) 01 r r -.h(j) * H [j]-.h'(j)) * HCons(h, h') * RList(TOP,h, h' (\u00aej.T \u00b7H[j] Inv def ' = .h, h \u00b7 \nA ) * to know that it is valid to add it to the stack. @ .i .T \u00b7h(i)=nil .h'(i)= Unset 5. Conclusions \nand Future work  4.2 The actual proof Finally we present some details of the proof. In particular, we \nshow how our informal reasoning earlier can be given formally, using the invariant and auxiliary state. \nIn what follows we use the shorthand: . E = E' to mean E = E' .emp. Additionally, for compactness, we \nuse Hoare s rule for jumps (break, continue and return), for example in a loop with invariant I and exit \ncondition X, break has pre-condition X and post-condition false and continue has pre-condition I and \npost-condition false. The outline of the proof 4 Permissions models require that there is no zero permission \nand that total permission is a maximum. We have given only the axioms that are required in our proof. \nWe do not claim to have made the .rst proof of Michael s algo\u00adrithm; nor do we claim that our proof is \nsimple; nor do we claim to have proved everything that is important, such as absence of live\u00adlock. We \nhave formally contained the inter-thread interference within the operations on the shared stack, as Michael \nclearly intended it to be contained. Modularity is essential for scalability of proof, and interference \ncontainment is an important kind of modularity. We are not aware of a proof of a non-blocking algorithm \nin another logic which provides such modularity. In all other proofs we have seen, the possibility of \ninterference .oods into speci.cations and proofs that do not have anything to do with it. . \u00af -. nil \n* H [tid]-. Unset pop() { . r r \u00af -. Unset * b . push (b) { r G[tid] local t,n; -. -. rr . \u00af -. Unset \n. \u00af r H[tid] local t,n; -. nil * H [tid] while(true) { r -. .\u00af . X = I* (t = nil . t . ) r I = H[tid]* \nH [tid] . \u00af -. Unset * b . for (n =0;n=THREADS; n++) { . NotHaz(b, {0,. r r G[tid] . \u00af H[tid] -. * H \n[tid] r atomic{ t = TOP; } j. .(G[tid] r-w - . -. Unset * b . { G[tid] = Unset; return false; } , n - \n1}) * b. n =0) . n > 0)(G[tid] .. . \u00af -. if (t == nil) break; r -. * H [tid] r H[tid] atomic{ if (H[n] \n== b) . \u00af -. =nil H [tid]=Req;} r H[tid] -. * H [tid] r atomic{ H[tid] = t; . t . NotHaz(b, {0,..., n}) \n. NotHaz(b, {0,..., n}) * b G[tid] r- = . \u00af -. t * H [tid]-. Req G[tid] atomic{ if (TOP != t) continue; \n= else H [tid]=Tail(t->tl); (a) } r r } H[tid] w . \u00af 9 - . . (n +1 > 0) w r . \u00af -. NotHaz(b,T) * b - \n. ; -. NotHaz(b,T) * b r while(true) {I = w w - . . \u00af - . } G[tid] . \u00af -. t * H [tid]-. Tail() 9 atomic{ \nn = t->tl; = atomic{ t = TOP; } r r H[tid] G[tid] . (b) \u00af if (H [tid]!=Tail(n)) {H [tid] = Left}; r \n-. NotHaz(b,T) * b b->tl = t; G[tid] ; } . \u00af w r -. t * (H [tid]-. Tail(n) . H ' [tid]-. Left) -. NotHaz(b,T) \n* b- . t 9 atomic{ if (CAS(&#38;TOP,t,n)) break; } atomic{ > (c) > = -. r rr r . \u00af H[tid] G[tid] o if \nCAS(&#38;TOP, t, b) then . \u00af (d) -. * H [tid] r H[tid] r {G[tid]= Unset; return true; } > -. NotHaz(b,T) \n} > ; } . \u00af . -. -. =nil . t . atomic{ H[tid] = nil; H [tid]=Unset;} -. Unset * (t =nil . t . r r r H[tid] \n* H [tid] * (t . ) \u00af G[tid] } . \u00af . -. nil * H [tid] return t; -. nil * H [tid]-. Unset * (ret r rr H[tid] \nr ) } . \u00af -. Unset * (ret = true) . (b . . . ret =false G[tid] } . \u00af . =nil . ret . Figure 4. Outline \nproof H[tid] ) The stack operations can be used by only considering the pre\u00adand post-conditions. This \nis similar to linearisability [7], which allows concurrent linearisable operations to be considered like \natomic actions. We do not require an operation to be linearisable to reason purely by its pre-and post-condition. \nIn this paper, we have not attempted to show more than the safety of certain operations, that they do \nnot leak memory and that they preserve the invariant of the shared stack. We are investigating adding \nliveness rules to separation logic to capture properties such as obstruction/lock/wait-freedom. Finally, \nwe intend to explore the combination of mechanisms from rely/guarantee and temporal logic with those \nof separation logic. The aim is a logic which exploits separation logic s modular\u00adity but provides a \nsimpler treatment of interference than is possible in that logic alone; hopefully, for example, that \nwould reduce the use of auxiliary state in proofs. Acknowledgments We should like to thank the East London \nMassive, Bart Jacobs, Rok Strnisa and Viktor Vafeiadis for feed\u00adback on this work. This work was supported \nby the EPSRC (Bor\u00adnat, O Hearn and Parkinson), the Royal Academy of Engineering (Parkinson) and Intel \nResearch Cambridge.   References [1] R. Bornat, C. Calcagno, P. W. O Hearn, and M. J. Parkinson. Permission \naccounting in separation logic. In Proceedings of POPL, pages 259 270, 2005. [2] R. Bornat, C. Calcagno, \nand H. Yang. Variables as resource in separation logic. In Proceedings of MFPS XXI. Elsevier ENTCS, May \n2005. [3] P. Brinch Hansen, editor. The Origin of Concurrent Programming. Springer-Verlag, 2002. [4] \nS. Brookes. A semantics for concurrent separation logic. Invited paper, in Proceedings of CONCUR, 2004. \n[5] E. W. Dijkstra. Cooperating sequential processes. In F. Genuys, editor, Programming Languages, pages \n43 112. Academic Press, 1968. Reprinted in [3]. [6] T. Harris, S. Marlow, S. Peyton-Jones, and M. P. \nHerlihy. Composable memory transactions. In Proceedings of PPOPP, 2005. [7] M. Herlihy and J. M. Wing. \nLinearizability: A correctness condition for concurrent objects. ACM Trans. Program. Lang. Syst., 12(3):463 \n492, 1990. [8] M. P. Herlihy and J. E. B. Moss. Transactional memory: architectural support for lock-free \ndata structures. In ISCA 93: Proceedings of the 20th annual international symposium on Computer architecture, \npages 289 300, 1993. [9] C. B. Jones. Speci.cation and design of (parallel) programs. In IFIP Congress, \npages 321 332, 1983. [10] M. M. Michael. Hazard pointers: Safe memory reclamation for lock\u00adfree objects. \nIEEE Trans. Parallel Distrib. Syst., 15(6):491 504, 2004. [11] P. W. O Hearn. Resources, concurrency \nand local reasoning. To appear in Theoretical Computer Science; preliminary version in CONCUR 04. [12] \nV. Vafeiadis, M. Herlihy, T. Hoare, and M. Shapiro. Proving correctness of highly-concurrent linearisable \nobjects. In Proceedings of PPoPP, pages 129 136, 2006. \n\t\t\t", "proc_id": "1190216", "abstract": "This paper contributes to the development of techniques for the modular proof of programs that include concurrent algorithms. We present a proof of a non-blocking concurrent algorithm, which provides a shared stack. The inter-thread interference, which is essential to the algorithm, is confined in the proof and the specification to the modular operations, which perform push and pop on the stack. This is achieved by the mechanisms of separation logic. The effect is that inter-thread interference does not pollute specification or verification of clients of the stack.", "authors": [{"name": "Matthew Parkinson", "author_profile_id": "81406598777", "affiliation": "University of Cambridge, Cambridge, UK", "person_id": "P707743", "email_address": "", "orcid_id": ""}, {"name": "Richard Bornat", "author_profile_id": "81100414897", "affiliation": "Middlesex University, London, UK", "person_id": "PP24016990", "email_address": "", "orcid_id": ""}, {"name": "Peter O'Hearn", "author_profile_id": "81332519314", "affiliation": "Queen Mary, University of London, London, UK", "person_id": "PP45027416", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1190216.1190261", "year": "2007", "article_id": "1190261", "conference": "POPL", "title": "Modular verification of a non-blocking stack", "url": "http://dl.acm.org/citation.cfm?id=1190261"}