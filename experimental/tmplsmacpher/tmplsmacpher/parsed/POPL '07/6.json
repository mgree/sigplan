{"article_publication_date": "01-17-2007", "fulltext": "\n Compositional Dynamic Test Generation (Extended Abstract) Patrice Godefroid * Microsoft Research pg@microsoft.com \nAbstract Dynamic test generation is a form of dynamic program analysis that attempts to compute test \ninputs to drive a program along a speci.c program path. Directed Automated Random Testing, or DART for \nshort, blends dynamic test generation with model checking techniques with the goal of systematically \nexecuting all feasible program paths of a program while detecting various types of errors using run-time \nchecking tools (like Purify, for instance). Unfortunately, systematically executing all feasible program \npaths does not scale to large, realistic programs. This paper addresses this major limitation and proposes \nto per\u00adform dynamic test generation compositionally, by adapting known techniques for interprocedural \nstatic analysis. Speci.cally, we in\u00adtroduce a new algorithm, dubbed SMART for Systematic Modular Automated \nRandom Testing, that extends DART by testing func\u00adtions in isolation, encoding test results as function \nsummaries ex\u00adpressed using input preconditions and output postconditions, and then re-using those summaries \nwhen testing higher-level functions. We show that, for a .xed reasoning capability, our compositional \napproach to dynamic test generation (SMART) is both sound and complete compared to monolithic dynamic \ntest generation (DART). In other words, SMART can perform dynamic test generation com\u00adpositionally without \nany reduction in program path coverage. We also show that, given a bound on the maximum number of feasible \npaths in individual program functions, the number of program exe\u00adcutions explored by SMART is linear \nin that bound, while the num\u00adber of program executions explored by DART can be exponential in that bound. \nWe present examples of C programs and preliminary experimental results that illustrate and validate empirically \nthese properties. Categories and Subject Descriptors D.2.4 [Software Engineer\u00ading]: Software/Program \nVeri.cation; D.2.5 [Software Engineer\u00ading]: Testing and Debugging; F.3.1 [Logics and Meanings of Pro\u00adgrams]: \nSpecifying and Verifying and Reasoning about Programs General Terms Veri.cation, Algorithms, Reliability \nKeywords Software Testing, Automatic Test Generation, Scala\u00adbility, Compositional Program Analysis, Program \nVeri.cation * This work was done mostly when the author was still af.liated with Bell Laboratories. It \nis also funded in part by NSF CCR-0341658. Permission to make digital or hard copies of all or part of \nthis work for personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior \nspeci.c permission and/or a fee. POPL 07 January 17 19, 2007, Nice, France. Copyright c . 2007 ACM 1-59593-575-4/07/0001. \n. . $5.00. 1. Introduction Given a program P , say a C program with a million lines of code, with a set \nof input parameters, wouldn t it be nice to have a tool that could automatically generate a set of input \nvalues that would exercise, say, even only 50% of the code of P ? This problem is called the test generation \nproblem, and has been studied since the 70 s (e.g., [Kin76, Mye79]). Yet, effective solutions and tools \nto address this problem have proven elusive for the last 30 years. What happened? There are several possible \nexplanations to the current lack of practically-usable tools addressing this problem. First, the expen\u00adsive \nsophisticated program-analysis techniques required to tackle the problem, such as symbolic execution \nengines and constraint solvers, have only become computationally affordable in recent years thanks to \nthe increasing computational power available on modern computers. Second, this steady increase in computational \npower has in turn enabled recent progress in the engineering of more practical software model checkers, \nmore ef.cient theorem provers, and, last but not least, more precise yet scalable static anal\u00adysis tools. \nIndeed, automatic code inspection tools based on static program analysis are increasingly being used \nin the software indus\u00adtry (e.g., [BPS00, HCXE02]). Recently, there has been a renewed interest on automated \ntest generation from program analysis (e.g., [BKM02, BCH+04, VPK04, CS05, GKS05, CE05]). Work in this \narea can roughly be partitioned into two groups: static versus dynamic test generation. Static Test Generation \nis Often Ineffective Static test generation (e.g., [Kin76]) consists of analyzing a pro\u00adgram P statically, \nby exclusively using symbolic execution tech\u00adniques to attempt to compute inputs to drive P along speci.c \nexe\u00adcution paths or branches, without ever executing the program.The idea is to symbolically explore \nthe tree of all computations the pro\u00adgram exhibits when all possible value assignments to input param\u00adeters \nare considered. For each control path ., that is, a sequence of control locations of the program, a path \nconstraint f. is con\u00adstructed that characterizes the input assignments for which the pro\u00adgram executes \nalong .. All the paths can be enumerated by a search algorithm that explores all possible branches at \nconditional state\u00adments. The paths . for which f. is satis.able are feasible and are the only ones that \ncan be executed by the actual program. The so\u00adlutions to f. exactly characterize the inputs that drive \nthe program through .. Assuming that the theorem prover used to check the satis.ability of all formulas \nf. is sound and complete, this use of static analysis amounts to a kind of symbolic testing. Unfortunately, \nthis approach does not work whenever the pro\u00adgram contains statements involving constraints outside the \nscope of reasoning of the theorem prover, i.e., statements that cannot be reasoned about symbolically \n. This limitation is illustrated by the following example. void obscure(int x, int y) { if (x == hash(y)) \nreturn -1; // error return 0; // ok } Assume that the function hash cannot be reasoned about symbol\u00adically. \nFormally, this means that it is in general impossible to gen\u00aderate two values for inputs x and y that \nare guaranteed to satisfy (or violate) the constraint x == hash(y). (For instance, if hash is a hash \nor cryptographic function, it has been mathematically de\u00adsigned to prevent such reasoning.) In this case, \nstatic test generation cannot generate test inputs to drive the execution of the program obscure through \neither branch of the conditional statement: static test generation is helpless for a program like this. \nThe practical implication of this simple observation is signi.\u00adcant: static test generation as proposed \nby King 30 years ago and much discussed since then (e.g., see [Mye79, Edv99, BCH+04, VPK04, XMSN05, CS05]) \nis doomed to perform poorly whenever symbolic execution is not possible. Unfortunately, this is frequent \nin practice due to complex program statements (pointer manipula\u00adtions, arithmetic operations, etc.) and \ncalls to operating-system and library functions that are hard or impossible to reason about sym\u00adbolically \nwith good enough precision. Dynamic Test Generation is More Powerful A second approach to test generation \nis dynamic test generation (e.g., [Kor90, GMS00]): it consists of executing the program P , typically \nstarting with some random inputs, gathering symbolic constraints on inputs gathered from predicates in \nbranch statements along the execution, and then using a constraint solver to infer variants of the previous \ninputs in order to steer the next execution of the program towards an alternative program branch. This \nprocess is repeated until a given .nal statement is reached or a speci.c program path is executed. Directed \nAutomated Random Testing [GKS05], or DART for short, is a recent variant of dynamic test generation that \nblends it with model checking techniques with the goal of systematically executing all feasible program \npaths of a program while detecting various types of errors using run-time checking tools (like Purify, \nfor instance). In DART, each new input vector attempts to force the execution of the program through \nsome new path. By repeating this process, such a directed search attempts to force the program to sweep \nthrough all its feasible execution paths, in a style similar to systematic testing and dynamic software \nmodel checking [God97]. In practice, DART typically achieves much better coverage than pure random testing \n(see [GKS05]). A key observation from [GKS05] is that imprecision in symbolic execution can be alleviated \nusing concrete values and randomiza\u00adtion: whenever symbolic execution does not know how to generate a \nconstraint for a program statement depending on some inputs, one can always simplify this constraint \nusing the concrete values of those inputs. Let us illustrate this important point with an example. Consider \nagain the program obscure given above. Even though it is stat\u00adically impossible to generate two values \nfor inputs x and y such that the constraint x == hash(y) is satis.ed (or violated), it is easy to generate, \nfor a .xed value of y,a value of x that is equal to hash(y) since the latter is known at runtime. By \npicking ran\u00addomly and then .xing the value of y, we can, in the next run, set the value of the other \ninput x either to hash(y) or to something else in order to force the execution of the then or else branches, \nrespectively, of the test in the function obscure. (DART does this automatically [GKS05].) In summary, \nstatic test generation is totally helpless to generate test inputs for the program obscure, while dynamic \ntest generation can easily drive the executions of that same program through all its feasible program \npaths! Dynamic test generation can thus be viewed as extending static test generation with additional \nruntime information, and is thus more general and powerful. Indeed, it can use the same symbolic execution \nengine and use concrete values to simplify constraints outside the scope of the constraint solver. This \nis why we believe that dynamic test generation is our only hope of one day providing effective, practical \ntest generation tools that are applicable to real\u00adlife software. And the purpose of the present paper \nis to discuss how to make this possible for large software applications. SMART = Scalable DART Obviously, \nsystematically executing all feasible program paths does not scale to large, realistic programs. This \npaper addresses this major limitation and proposes to per\u00adform dynamic test generation compositionally, \nby adapting known techniques for interprocedural static analysis (e.g., [RHS95]) that have been used \nto make static analysis scalable to very large pro\u00adgrams (e.g., [BPS00, DLS02, HCXE02, CDW04]). Speci.cally, \nwe introduce a new algorithm, dubbed SMART for Systematic Modular Automated Random Testing, that extends \nDART by testing func\u00adtions in isolation, encoding test results as function summaries ex\u00adpressed using \ninput preconditions and output postconditions, and then re-using those summaries when testing higher-level \nfunctions. We show that, for a .xed reasoning capability, our compositional approach to dynamic test \ngeneration (SMART) is both sound and complete compared to monolithic dynamic test generation (DART). \nIn other words, SMART can perform dynamic test generation com\u00adpositionally without any reduction in program \npath (and hence branch) coverage. We also show that, given a bound on the max\u00adimum number of feasible \npaths in individual program functions, the number of program executions explored by SMART is linear in \nthat bound, while the number of program executions explored by DART can be exponential in that bound. \nWe present examples of C programs and preliminary experimental results that illustrate and validate empirically \nthese properties. To the best of our knowledge, SMART is the .rst algorithm for compositional dynamic \ntest gen\u00aderation. We claim that a SMART search is necessary to make the DART approach scalable to large \nprograms.  2. The DART Search Algorithm In this section, we brie.y recall the DART search algorithm \n.rst introduced in [GKS05], later re-phrased in [SMA05] and (indepen\u00addently) in [CE05]. We present here \na simpli.ed version to facilitate the exposition, see [GKS05] for additional details. Like other forms \nof dynamic test generation (e.g., [Kor90]), DART consists of running the program P under test both con\u00adcretely, \nexecuting the actual program, and symbolically, calculating constraints on values stored in program variables \nand expressed in terms of input parameters. These side-by-side executions require the program P to be \ninstrumented at the level of a RAM (Ran\u00addom Access Memory) machine. The memory Mis a mapping from memory \naddresses m to, say, 32-bit words. The notation +for map\u00adpings denotes updating; for example, M' :=M+[ \nm.v]is the same map as M, except that M'(m)= v. We identify symbolic variables by their addresses. Thus \nin an expression, m denotes ei\u00adther a memory address or the symbolic variable identi.ed by ad\u00address m, \ndepending on the context. The program P manipulates the memory through statements that are specially \ntailored abstractions of the machine instructions actually executed. A statement can be a conditional \nstatement c of the form if (e) then goto f' (where e is an expression over symbolic variables and f' \nis a statement label), an assignment statement a of the form m .e (where m is a memory address), abort, \ncorresponding to a program error, or halt, corresponding to normal termination. The function get next \nstatement() speci.es the next statement to be executed. The concrete semantics of the RAM machine instructions \nof P is re.ected in evaluate concrete(e, M), which evaluates expres\u00adsion e in context Mand returns a \n32-bit value for e. A program P de.nes a sequence of input addresses 1 M0, the addresses of the input \nparameters of P .An input vector I1associates a value to each input parameter and de.nes the initial \nvalue of 11 M0 and M. Let C be the set of conditional statements and A the set of as\u00adsignment statements \nin P .A program execution w is a .nite2 se\u00adquence in Execs := (A .C) * (abort |halt). The concrete seman\u00adtics \nof P at the RAM machine level allows us to de.ne for each input vector I1an execution sequence: the result \nof executing P on 1 I (the details of this semantics is not relevant for our purposes). Let Execs(P ) \nbe the set of such executions generated by all possible 1 I. By viewing each statement as a node, Execs(P \n) forms a tree, called the execution tree. Its assignment nodes have one successor; its conditional nodes \nhave one or two successors; and its leaves are labeled abort or halt. The goal of DART is to explore \nall paths in the execution tree Execs(P ). To simplify the following discussion, we assume that we are \ngiven a theorem prover that decides a theory T (for instance, in\u00adcluding integer linear constraints, \npointer constraints, array/string constraints, bit-level operation constraints, etc.). DART maintains \na symbolic memory Sthat maps memory addresses to expres\u00adsions. Initially, Sis a mapping that maps each \nm .M0 to 1 itself. Expressions are evaluated symbolically with the function evaluate symbolic(e, M, S). \nWhen an expression falls outside the theory T , DART simply falls back on the concrete value of the ex\u00adpression, \nwhich is used as the result. In such a case, we also set a.ag complete to 0, which we use to track completeness. \nWith this evaluation strategy, symbolic variables of expressions in Sare always contained in 1 M0. To \ncarry out a systematic search through the execution tree, our instrumented program is run repeatedly. \nEach run (except the .rst) is executed with the help of a record of the conditional statements executed \nin the previous run. For each conditional, we record a done value, which is 0 when only one branch of \nthe conditional has executed in prior runs (with the same history up to the branch point) and is 1 otherwise. \nThis information associated with each conditional statement of the last execution path is stored in a \nlist variable called stack, kept between executions. For i, 0 =i< |stack|, stack[i] is thus the record \ncorresponding to the i +1th conditional executed. More precisely, the DART test driver run DART is shown \nin Figure 1 where the two lines marked by (*) should be ignored. This driver combines random testing \n(the repeat loop) with directed search (the while loop). If the instrumented program throws an ex\u00adception, \nthen a bug has been found. The completeness .ag complete holds unless a bad situation possibly leading \nto incompleteness has occurred. Thus, if the directed search terminates that is, if di\u00adrected of the \ninner loop no longer holds then the outer loop also terminates provided the completeness .ag still holds. \nIn this case, DART terminates and safely reports that all feasible program paths have been explored. \nBut if the completeness .ag has been turned off at some point, then the outer loop continues forever. \nThe instrumented program itself is described in Figure 2 where the lines marked by (*) should again be \nignored for now (^ denotes 1 To simplify the presentation, we assume that M 0 is the same for all executions \nof P . 2 We thus assume that all program executions terminate; in practice, this can be enforced by limiting \nthe number of execution steps.    list concatenation). It executes as the original program, but with \ninterleaved gathering of symbolic constraints. At each conditional statement, it also possible to check \nwhether the current execution path matches the one predicted at the end of the previous execution and \nrepresented in stack passed between runs. How to do this is described in the function compare and update \nstack of [GKS05]. When the original program halts, new input values are generated in solve path constraint, \nshown in Figure 3 while ignoring again all the lines marked with (*), to attempt to force the next run \nto execute the last3 unexplored branch of a conditional along the stack. If such a branch exists and \nif the path constraint that may lead to its execution has a solution I1' , this solution is used to update \nthe mapping I1to be used for the next run; values corresponding to input parameters not involved in the \npath constraint are preserved (this update is denoted I1+ I1' ). The main property of DART is stated \nin the following theorem, which formulates (a) soundness (of error founds) and (b) a form of completeness. \nTHEOREM 1. [GKS05] Consider a program P as previously de\u00ad.ned. (a) If run DART prints out Bug found for \nP , then there is some input to P that leads to an abort. (b) If run DART terminates without printing \nBug found, then there is no input that leads to an abort statement in P , and all paths in Execs(P ) \nhave been ex\u00adercised. (c) Otherwise, run DART will run forever. Proofs of (a) and (c) are immediate. \nThe proof of (b) rests on the assumption that any potential incompleteness in DART s search is (conservatively) \ndetected by setting the .ag complete to 0. 3. The SMART Search Algorithm We now present an alternative \nsearch algorithm that does not com\u00adpromise search completeness but is typically much more ef.cient than \nthe DART search algorithm. The general idea behind this new search algorithm is to perform dynamic test \ngeneration composi\u00adtionally, by adapting (dualizing) known techniques for interproce\u00addural static analysis \nto the context of automated dynamic test gener\u00adation. Speci.cally, we introduce a new algorithm, dubbed \nSMART for Systematic Modular Automated Random Testing, that tests func\u00adtions in isolation, collects testing \nresults as function summaries ex\u00adpressed using preconditions on function inputs and postconditions on \nfunction outputs, and then re-use those summaries when testing higher-level functions. We assume we are \ngiven a program P that consists of a set of functions. If a function f is part of P , we write f .P .In \nwhat follows, we use the generic term of function to denote any part of a program P that we want to analyze \nin isolation and then summarize its observed behaviors. Obviously, any other kinds of program fragments \nsuch as program blocks or object methods can be treated as functions as done in this paper. To simplify \nthe presentation, we assume that the functions in P do not perform recursive calls, i.e., that the call-.ow \ngraph of P is acyclic. (This restriction can be lifted using dynamic pro\u00adgramming techniques to compute \nfunction summaries, as is stan\u00addard in interprocedural static analysis and pushdown system veri.\u00adcation \n[RHS95, ABE+05].) As previously stated, we also assume that all the executions of P terminate. Note that \nboth of these as\u00adsumptions do not prevent P from possibly having in.nitely many executions paths, as \nis the case if P contains a loop whose number of iterations may depend on some unbounded input. 3 A depth-.rst \nsearch is used for exposition, but the next branch to be forced could be selected using a different strategy, \ne.g., randomly or in a breadth\u00ad.rst manner.     3.1 De.nition of Summaries For a given theory T of \nconstraints, a function summary ff for a function f is de.ned as a formula of propositional logic whose \npropositions are constraints expressed in T . ff can be computed by successive iterations and de.ned \nas a disjunction of formulas fw of the form fw = prew .postw,where prew is a conjunction of constraints \non the inputs of f while postw is a conjunction of constraints on the outputs of f. fw can be computed \nfrom the path constraint corresponding to the execution path w as will be described shortly. An input \nto a function f is any address (memory location) that can be read by f in some of its execution, while \nan output of f is any address that can be written by f in some of its executions and later read by P \nafter f returns. Preconditions in function summaries are expressed in terms of constraints on function \ninputs instead of program inputs in order to avoid duplication of identical summaries in equivalent but \ndifferent calling contexts. For instance, in the following program int is positive(int x) { if (x > 0) \nreturn 1; return 0; } void top(int y, int z) { int a,b; a= is positive(y); b= is positive(z); if (a \n&#38;&#38; b) then [...] [...] } the summary for the function is positive could be (x> 0 .ret =1) .(x \n=0 .ret =0) (if T includes linear arithmetic) where ret denotes the value returned by the function. This \nsum\u00admary is expressed in terms of the function input x, independently of speci.c calling contexts which \nmay map x to different program inputs like y and z in this example.4 Whenever a constraint on some input \ncannot be expressed within T , no constraint is generated. For instance, consider the following function \ng: 1 int g(int x) { 2 int y; 3 if (x < 0) return 0; 4 y = hash(x); 5 if (y == 100) return 10; 6 if (x \n> 10) return 1; 7 return 2; 8 } Assuming the constraint (hash(x)==100) cannot be expressed in T , the \nsummary fw of the execution path w corresponding to taking all the else branches at the three conditional \nstatements in function g is then (x =0 .x =10 .ret =2). A precondition de.nes an equivalence class of \nconcrete execu\u00adtions. All the concrete executions corresponding to concrete inputs satisfying the same \nprecondition are guaranteed to execute the same program path only provided that all the constraints along \nthat path are in T . In the example above, if the path w that takes all the else branches in function \ng was explored with a random concrete value, say, x =5, another value satisfying the same precondition \n(x =0 .x =10),say x =6 is not guaranteed to yield the same 4 Remember that symbolic variables are associated \nwith program or func\u00adtion inputs, i.e., memory locations where inputs are being read from. When syntactic \nprogram variables uniquely de.ne where those inputs are stored, like variables x, y and z in the above \nexample, we merely write an input x in the text to simplify the presentation. program path, because of \nthe presence of the unpredictable con\u00additional statement in line 5 (as hash(6) could very well be 100). \nThe execution of this conditional statement makes a DART search incomplete (the .ag complete is then \nset to 0). In that case, all the preconditions in a function summary may no longer be mutually exclusive: \na given concrete state may satisfy more than one pre\u00adcondition in a function summary when the function \ncontains con\u00additional statements whose corresponding constraints are outside T . 3.2 Computing Summaries \nFunction summaries can be computed by successive iterations, one path at a time. When the execution of \nthe function terminates, the DART-computed path constraint for the current path w in the function can \nbe used to generate a precondition prew for that path: prew is obtained by simplifying the conjunction \nof branch conditions on function inputs in the path constraint for w. If the execution of the function \nterminates on a return state\u00adment, a postcondition postw can be computed by taking the con\u00adjunction of \nconstraints associated with memory locations m .Write(f, 1 I,w) written during the execution of f during \nthe last execution w generated from a context (set of input values) I1.Pre\u00adcisely, we have ^ postw =(m \n= evaluate symbolic(m, M, S)) m.Write(f,.I,w) Otherwise, if the function terminates on a halt or abort \nstate\u00adment, we de.ne postw = false to record this in the summary for possible later use in the calling \ncontext, as described later. A summary for the execution path w in f is then fw = prew .postw. The process \nis repeated for other DART-exercised paths w W in f, and the overall summary for f is de.ned as ff = \nw fw. By default, the above procedure can always be used to compute function summaries path by path. \nBut more advanced techniques, such as automatically-inferred loop invariants, could also be used (see \nSection 4). Note that prew can always be approximated by false (the strongest precondition) while postw \ncan always be ap\u00adproximated by true (the weakest postcondition) without compro\u00admising the correctness \nof summaries, and that any technique for generating provably correct weaker preconditions or stronger \npost\u00adconditions can be used to improve precision. Given the call-.ow graph GP of a program P (which we \nhave previously assumed to be acyclic) and a topological sort of the functions in GP computed starting \nfrom the top-level function of the program, function summaries can then be computed in either a bottom-up \nor top-down strategy. With a bottom-up strategy, one starts testing functions at the deepest level in \nGP , one computes summaries for those, and then moves up the topological sort to functions one-level \nup while re\u00adusing the summaries for the functions below (as described in the next subsection), and so \non up to the top-level function of the program. While the bottom-up strategy is conceptually the easiest \nto understand, it suffers from two major limitations that make its implementation problematic in the \ncontext of compositional dynamic test generation. First, testing lower-level functions in isolation for \nall possible contexts (i.e., for all possible input values) is likely to trigger un\u00adrealistic behaviors \nthat may not happen in the speci.c contexts in which the function can actually be called by higher-level \npro\u00adgram functions; this analysis can be prohibitively expensive and will likely generate an unnecessarily \nlarge number of spurious sum\u00admaries that will never be used subsequently. Thus, too many sum\u00admaries are \ncomputed. Second, because of the inherent limitation of symbolic execu\u00adtion to reason about constraints \noutside the given theory T ,sum\u00admaries computed in bottom-up fashion may be incomplete in pres\u00ad run () \n= complete =1 (*) summary =[f .\u00d8|f .P ]// Set of summaries repeat stack = (); I1=[]; directed =1 (*) \ncontext stack =(( ,, 0))// Stack of contexts while (directed) do try (directed, stack, I1)= SMART instrumented \nprogram(stack, I1) catch any exception .print Bug found ; exit() until complete   Figure 1. run DART \nand (*) run SMART test drivers ence of statements involving constraints outside T . For instance, in \nthe case of function g presented in Section 3.1, analyzing g in isolation using DART techniques will \nprobably not be able to exer\u00adcise the then branch of the conditional statement on line 5, i.e., to randomly \n.nd a value of x such that hash(x) == 100.However, in its actual calling contexts within the program \nP , it is possible that the function g is often called with values for x that satisfy this constraint. \nIn this case, too few summaries are pre-computed, and it is necessary to compute later in the search \na summary for the case where hash(x) == 100 is satis.ed. To avoid these two limitations, we recommend \nand adopt a top\u00addown strategy for computing summaries on a demand-driven basis. A complete algorithm \nfor doing this is described next. 3.3 Algorithm A top-down SMART search algorithm is presented in Figures \n1, 2 and 3. The pseudo-code for SMART is similar to the one for DART with the exception of the new additional \nlines marked by (*). Indeed, SMART strictly generalizes DART and reduces to it in the case of programs \nconsisting of a single function. A SMART search performs dynamic test generation composi\u00adtionally, using \nfunction summaries as de.ned previously. Those summaries are dynamically computed in a top-down manner \nthrough the call-.ow graph GP of P . Starting from the top-level function, one executes the program (initially \non some random in\u00adputs) until one reaches a .rst function f whose execution termi\u00adnates on a return or \nhalt statement. One then backtracks inside f as much as possible using DART, computing summaries for \nthat function and each of those DART-triggered executions. When this search (backtracking) in f is over, \none then resumes the original execution where f was called, this time treating f essentially as a black-box, \ni.e., without analyzing it and re-using its previously computed summary instead. The search proceeds \nsimilarly, with the next backtracking point being in some lower-level function, if any, called after \nf returns, or in the function g that called f other\u00adwise, or some other higher-level function that called \ng if the search in g is itself over. This search order is thus different from DART s search order. A \nSMART search starts by executing the procedure run SMART described in Figure 1. The only differences \nwith the procedure run DART is the initialization of a set of summaries and of a con\u00adtext stack that \nrecords the sequence of calling contexts for which summaries still need to be computed along the current \nexecution, and is also used to resume execution in a previous context. The main functionality of SMART \nis presented in Figure 2. The key difference with DART is that function calls and returns are now instrumented \nto trigger and organize the computation of function summaries. Whenever a function f is called, a SMART \ninstrumented program checks whether a summary for f is already available for the current calling context. \nThis is done by checking instrumented program(stack, I1)= // Random initialization of uninitialized \ninput parameters in M10 for each input x with I1[x]unde.ned do I1[x]= random() Initialize memory Mfrom \nM10 and I1 // Set up symbolic memory and prepare execution S=[m .m |m .M10] k = 0// Number of conditionals \nexecuted (*) backtracking = 1 // By default, backtrack at all branch points (*) (*) (*) (*) (*) (*) (*) \n(*) (*) (*) (*) (*) (*) (*) (*) (*) (*) (*) (*) (*) (*) (*) (*) (*) (*) (*) // Now invoke P intertwined \nwith symbolic calculations s = get next statement() while (s /  .{abort, halt}) do match (s) case (m \n.e): S= S+ [m .evaluate symbolic(e, M, S)] v = evaluate concrete(e, M) M= M+[m .v] case (if (e)then \ngoto f): b = evaluate concrete(e, M) c = evaluate symbolic(e, M, S) if backtracking then if b then path \nconstraint = path constraint ^ (c) else path constraint = path constraint ^ (neg(c))if (k =|stack|) \nthen stack = stack ^ (0) k= k +1 case (f :call): // call of function f if backtracking then if (I1.summary(f)) \nthen // We have a summary for f in context I1 path constraint = path constraint ^ (summary(f)) // Execute \nf without backtracking until it returns backtracking =0 if (k =|stack|) then stack = stack ^ (1) k= k \n+1 else // Compute a summary for f in context I1Push (f, 1 I,k)onto context stack case (f :return): // \nreturn of function f if backtracking then // Stop the search in f // Generate a summary for the current \npath add to summary(f,path constraint) return solve path constr(k,path constraint,stack)  else if (Top(context \nstack)= (f, , )) then backtracking =1 // Extend the set of inputs by the return values of f M= M+[m .m \n|m .post(summary(f))] s =get next statement() od // End of while loop if (s==abort) then raise an exception \nelse // s==halt if backtracking then (f, , )=Top(context stack) add to summary(f,path constraint) return \nsolve path constr(k,path constraint,stack)                 Figure 2. DART and (*) SMART \ninstrumented program solve path constr(k,path constraint,stack)= j = k -1; kf =0 (*) (f, 1 I,kf )= Top(context \nstack) while (j =kf ) do if (stack[j]= 0) then path constraint[j] = neg(path constraint[j]) if (path \nconstraint[0,...,j] has a solution I1' ) then stack[j]=1 return (1, stack[0..j], I1+ I1' ) else j = j \n-1 else j = j -1 od (*) if (kf > 0) then (*) Pop (f, 1 I,kf ) from context stack (*) return (1, stack[0..(kf \n-1)], I1) return (0, , ) // This directed search is over Figure 3. DART and (*) SMART solve path constr \nwhether the current concrete function input assignment satis.es one of the preconditions currently recorded \nin the summary for f.5 If so, this summary is added to the current path constraint, and the execution \nproceeds by turning backtracking off in f and any function below it in the call-.ow graph of P . The \nlatter is done through the use of a boolean .ag backtracking. Backtracking is resumed later in the current \nexecution path when f returns: this is done in the else branch of the conditional statement included \nin the return case, where the set of inputs (in the function calling f) is also extended with the set \nof return values appearing in the set post(summary(f)) of postconditions included in the summary summary(f) \ncurrently available for f. If no summary is available for the current calling context, this calling context \nis saved by pushing it onto the context stack, and the algorithm will compute a summary for it by continuing \nthe search deeper in the called function f. When backtracking is on and the inner-most function terminates \neither on a return statement or a halt statement, add to summary(f,path constraint) computes a summary \nfor f and the last path executed as discussed in Sec\u00adtion 3.2. Note that a function summary for f includes \nin itself sum\u00admaries of lower-level functions possibly called by f itself. After computing a summary \nfor the current function and exe\u00adcution path, solve path constr, presented in Figure 3, is called to \ndetermine where the algorithm should backtrack next. When back\u00adtracking in a speci.c function f and calling \ncontext I1is over, the search resumes in the last calling context saved in the context stack. 3.4 Correctness \nThe correctness of the SMART search algorithm is de.ned with respect to the DART search algorithm, thus \nindependently of a spe\u00adci.c theory T representing the reasoning capability of symbolic execution. Speci.cally, \nwe can prove that, for any program P con\u00adtaining exclusively statements whose corresponding constraints \nare in a given decidable theory T (i.e., for which the .ag complete al\u00adways remains 1), the SMART search \nalgorithm provides exactly the same program path coverage as the DART search algorithm. Thus, for those \nprograms P , every feasible path that is exercised by DART is also explored , albeit compositionally, \nby SMART;and conversely, every compositional execution considered by SMART is guaranteed to correspond \nto a concrete full execution path. For\u00admally, we have the following. 5 Checking later that the output \nvalues of f for that run satisfy the corre\u00adsponding postcondition in the summary is not mandatory for \ncorrectness but can increase precision and hence coverage.    THEOREM 2. (Relative Soundness and \nCompleteness) Given any program P and theory T ,run SMART terminates without printing Bug found if and \nonly if run DART terminates without printing Bug found . In practice, programs P typically contain statements \ncorre\u00adsponding to constraints outside T (whatever T is). The SMART and DART searches may then behave \ndifferently because their search order vary, and calls to the function random() to initial\u00adize unde.ned \ninputs may return different values, hence exercis\u00ading the code randomly differently. Nevertheless, a \ncorollary of the previous theorem is that the SMART search algorithm is func\u00adtionally equivalent to DART, \nin the sense that it still satis.es the conditions identi.ed in Theorem 1 characterizing the correctness \nof the DART search algorithm (and of its various implementa\u00adtions [GKS05, CE05, SMA05, YST+06]). Formally, \nwe can prove the following. THEOREM 3. Consider a program P as previously de.ned. (a) If run SMART prints \nout Bug found for P , then there is some input to P that leads to an abort. (b) If run SMART terminates \nwithout printing Bug found, then there is no input that leads to an abort statement in P . (c) Otherwise, \nrun SMART will run forever. In summary, SMART is functionally equivalent to DART and, typically, whatever \ntest inputs DART can generate, SMART can too, although possibly much more ef.ciently. How much more ef.cient \n(hence scalable) can SMART be compared to DART? This question is addressed next. 3.5 Complexity Let \nb be a bound on the maximum number of distinct execution paths that can be contained in any function \nf of the program P .Ifa function f does not contain any loop, such a bound is guaranteed to exist, although \nit can be exponential in the number of statements in the code describing f.If f contains loops whose \nnumber of itera\u00adtions may depend on an unbounded input, the number of execution paths in f could be in.nite, \nand such a bound b may not exist. In practice, a bound b can always be enforced by simply limiting the \nnumber of execution paths explored in a function, i.e., by limiting the size of summaries; this heuristics \nhas been shown to work well in the context of interprocedural static analysis (e.g., see [BPS00]). Given \nsuch a bound b, it is easy to see that the number of execu\u00adtion paths considered by a SMART search (while \nthe .ag directed is kept to 1) will be at most nb,where n is the number of functions f in P , and is \ntherefore linear in nb. In contrast, the number of exe\u00adcution paths considered by a DART search (while \nthe .ag directed is kept to 1) can be exponential in nb, as DART does not exploit program hierarchy and \ntreats a program as a single, .at function. This reduction in the number of explored paths from exponential \nto polynomial in b is also observed with compositional veri.cation algorithms for hierarchical .nite-state \nmachines [AY98]. Although SMART can avoid systematically executing all the possibly exponentially many \nfeasible program paths in P , it does require the use of formulas ff representing function summaries \nwhich can be of size linear in b, and the use of theorem proving techniques to check satis.ability of \nthose formulas, with decision procedures which can, in the worst case, be exponential in the size of \nthose formulas, i.e., exponential in b. However, while DART can be viewed as always trying to systematically \nexecute all pos- W sible execution paths, i.e., all possible disjuncts in ff = w fw, SMART will try to \ncheck the satis.ability of ff in conjunction with additional constraints generated from a calling context \nof f, and hence try to .nd just one way to satisfy the resulting formula using a logic constraint solver. \nThis key point is illustrated next.   4. Example, Case Study, Discussion A Simple Example Consider \nthe function locate whose code is as follows: 1 // locate index of first character c 2 // in null-terminated \nstring s 3 int locate(char *s, int c) { 4 int i=0; 5 while (s[i] != c) { 6 if (s[i] == 0) return -1; \n7 i++; 8 } 9 return i; 10 } Given a string s of maximum size n (i.e, s[n] is always zero), there are \nat most 2n distinct execution paths for that function if c is non\u00adzero (and at most n if c is zero). \nThose 2n paths can be denoted by the regular expression: ((line5:then; line6:else)i (line5:else |(line5:then; \nline6:then)))for 0 =i =(n -1).There are n +1 possible return values: -1 (for the n paths ((line5:then; \nline6:else)i (line5:then; line6:then))for 0 =i =(n -1)), and 0, 1,..., (n -1), each returned by the path \n((line5:then; line6:else)i line5:else)where i is equal to the return value. Now, consider the function \ntop which calls the function locate: 11 void top(char *input) { // assume input is null-terminated 12 \nint z; 13 z = locate(input, a ); 14 if (z == -1) return -1; // error code 15 if (input[z+1] != : ) return \n1; // success 16 return 0; // failure 17 } In the function top, there are 3 possible execution paths: \n(line14:then), (line14:else; line15:then)and (line14:else; line15:else). Following the call to locate, \nthe outcome of the test at line 14 is completely determined by the return value from function locate \nstored in z. In contrast, the test at line 15 constraints the next element input[z+1] in the string input \nand its outcome depends on the value stored at that address. That input value could either be equal to \n: or not, except for input[n] which we assumed to be zero. Therefore, for the whole program P composed \nof the two functions top and locate,there are 3n-1 possible execution paths: n executions terminate after \nthe then branch of line 14, n executions terminate after the then branch of line 15, and n -1 executions \nterminate in line 16. Thus, the number of feasible paths in P is (roughly) the product of the number \nof paths in its functions locate and top. A DART search attempts to systematically execute all possi\u00adble \nexecution paths and would thus perform 3n -1 runs for this program. In contrast, a SMART search will \nsystematically execute all possible execution paths of the function locate and top sepa\u00adrately. Precisely, \na SMART search computing function summaries as described in Section 3.2 would compute 2n path summaries \nfor function locate, whose function summary ff would then be of the form ff =(s[0] = c .ret =0) .(s[0] \n. = c .s[0] = 0 .ret = -1) .(s[0] .. = c .s[0] =0 .s[1] = c .ret =1) etc. Then, the SMART search would \nexplore the feasibility of the 3 paths of the function top using ff to summarize function locate. 800 \n700 600 500 Runs 400 300 200 100 0  1 2 3 4 5 6 7 8 910 Packet size Figure 4. Experimental comparison \nbetween DART and SMART For this example, SMART would then perform 2n +3 runs, i.e., the sum of the number \nof paths in its functions locate and top. Observehow theaddress z+1 is de.ned relative to z and that \nits absolute value does not matter (as long as z +1 .n)when = proving the satis.ability of the constraint \ngenerated from the test input[z+1] != : and of its negation. This is captured by the SMART algorithm, \nwhich will not attempt to try all possible ways to satisfy/violate these constraints (as DART would), \nbut will only .nd one way to satisfy those. This observation explains intuitively the signi.cant speed-up \nthat SMART can provide compared to DART, while guaranteeing the same path (and hence branch) cov\u00aderage \n(100% branch coverage is achieved in this example). Case Study We have developed an implementation of \nthe SMART search algo\u00adrithm for the C programming language, extending the DART im\u00adplementation described \nin [GKS05]. We report here preliminary experiments comparing the ef.ciency of DART and SMART on the oSIP \nexample discussed in [GKS05]. oSIP is an open-source C library implementing the SIP protocol and consisting \nof about 30,000 lines of C code. SIP messages are transmitted as ASCII strings and a large part of the \noSIP code is dedicated to parsing SIP messages. Figure 4 presents the number of runs needed by DART and \nSMART to fully explore all the feasible program paths in a subset of the oSIP parser code. Experiments \nwere performed for several, small packet sizes. Runtime is linear in the number of runs for those experiments. \nAs is clear from Figure 4, SMART can fully cover all the feasible program paths of this example much \nmore ef.ciently than DART. In fact, for this example, the SMART search is optimal in the sense that its \nnumber of runs (and runtime) grows in a linear way with the size of the input packet.  Discussion Another \nway to limit the path explosion problem in a DART search is simply to allow backtracking only at branches \nof condi\u00adtional statements that have never been executed so far. If B denotes the number of conditional \nstatements in a program P , the number of execution paths (runs) explored by such a branch-coverage\u00addriven \nDART search is trivially bounded by 2B, i.e., is linear in the program size. The drawback of this naive \nsolution is obviously that full feasible path coverage is no longer guaranteed, even for programs containing \nonly statements with constraints in T .This, in turn, typically reduces overall branch coverage itself, \nand thus chances of .nding bugs. In contrast, SMART reduces the compu\u00adtational complexity of DART without \nsacri.cing full path coverage and hence provably without reducing branch coverage. In the presence of \nloops, loop invariants could be used to gen\u00aderate more general and compact function summaries than those \ngenerated by the path-by-path procedure for computing summaries presented in Section 3.2. For instance, \nconsidering again the func\u00adtion locate, a more compact and general function summary is ff =((.i =0:s[i]=c \n.(.j<i :(s[j].=c).(s[j].= References 0))).ret =i).((.i =0:s[i]=0.(.j<i :s[j]=.c)).ret =-1), which is \nindependent of any maximum size n for the string s. Concrete values known at runtime could be used to \ndetect partial loop invariants, i.e., simpli.ed loop invariants that are valid only when some input variables \nare .xed.  5. Conclusion DART [GKS05], and closely related work (e.g., [SMA05, CE05, YST+06]), is a \npromising new approach to automatically generate tests from program analysis. Actually, DART can be viewed \n[GK05] as one way of combining static (interface extraction, symbolic ex\u00adecution) and dynamic (testing, \nrun-time checking) program analy\u00adsis with model-checking techniques (systematic state-space explo\u00adration) \nin order to address one of the main limitations of previous dynamic, concrete-execution-based software \nmodel checkers (such as VeriSoft, JavaPathFinder and CMC, among others), namely their inability to automatically \ndeal with input data nondeterminism. But DART suffers from two major limitations. First, its effec\u00adtiveness \ncritically depends on the symbolic reasoning capability T available. Whenever symbolic execution is not \npossible, concrete values can be used to simplify constraints and carry on with a sim\u00adpli.ed, partial \nsymbolic execution. Randomization can also help by suggesting concrete values whenever automated reasoning \nis im\u00adpossible or dif.cult. Still, it is currently unknown whether dynamic test generation is really \nthat superior to static test generation, that is, how effective using concrete values and randomization \nhelp sym\u00adbolic execution for testing purposes in practice. More experiments with various kinds of examples \nare needed to determine this. Second, DART suffers from the path explosion problem: sys\u00adtematically executing \nall feasible program paths is typically pro\u00adhibitively expensive for large programs. This paper addresses \nthis second limitation in a drastic way, by performing dynamic test gen\u00aderation compositionally and eliminating \npath explosion due to in\u00adterprocedural program paths (i.e., paths across function boundaries) without \nsacri.cing overall path or branch coverage. A SMART search can be viewed as exploring the set of feasible \nwhole pro\u00adgram paths symbolically, i.e., by exploring simultaneously sets of such paths, instead of executing \nthose one by one. Our approach adapts known techniques for interprocedural static analysis to the context \nof dynamic test generation. While implementations of interprocedural static analysis are typically both \nincomplete (may miss bugs) and unsound (may generate false alarms) with respect to falsi.cation [GK05], \nour compositional dy\u00adnamic test generation is performed in such a way to preserve the soundness of bugs \n[God05]: any error path found is guaranteed to be sound, as every compositional symbolic execution is \ngrounded, by design, into some concrete execution. The only imprecision in our approach is incompleteness \nwith respect to falsi.cation: we may miss bugs by failing to exercize some executable program paths and \nbranches. The idea of compositional dynamic test generation was al\u00adready suggested in [GK05]; the motivation \nof the present paper is to investigate this idea in detail. Other recent related work in\u00adcludes [CG06], \nwhich proposes and evaluates several heuristics based on light-weight static analysis of function interfaces \nto par\u00adtition large software applications into groups of functions, called units. Those units can then \nbe tested in isolation without generating too many false alarms caused by unrealistic inputs being injected \nat interfaces between units. In contrast with the present work, no summarization of unit testing, nor \nany global analysis is ever per\u00adformed in [CG06]. Both types of techniques can actually be viewed as \ncomplementary. We refer the reader to [GKS05] for a detailed discussion of other automated test generation \ntechniques and tools, and to [GK05] for a discussion of other possible DART extensions. [ABE+05] R. Alur, \nM. Benedikt, K. Etessami, P. Godefroid, T. Reps, and M. Yannakakis. Analysis of Recursive State Machines. \nTOPLAS, 27(4):786 818, July 2005. [AY98] R. Alur and M. Yannakakis. Model Checking of Hierarchical State \nMachines. In FSE 98. [BCH+04] D. Beyer, A. J. Chlipala, T. A. Henzinger, R. Jhala, and R. Majumdar. Generating \nTests from Counterexamples. In ICSE 2004. [BKM02] C. Boyapati, S. Khurshid, and D. Marinov. Korat: Automated \ntesting based on Java predicates. In ISSTA 2002. [BPS00] W.R. Bush, J.D. Pincus, and D.J. Sielaff. A \nstatic analyzer for .nding dynamic programming errors. Software Practice and Experience, 30(7):775 802, \n2000. [CDW04] H. Chen, D. Dean, and D. Wagner. Model Checking One Million Lines of C Code. In NDSS 04. \n[CE05] C. Cadar and D. Engler. Execution Generated Test Cases: How to Make Systems Code Crash Itself. \nIn SPIN 2005. [CG06] A. Chakrabarti and P. Godefroid. Software Partitioning for Effective Automated Unit \nTesting. In EMSOFT 2006. [CS05] C. Csallner and Y. Smaragdakis. Check n Crash: Combining Static Checking \nand Testing. In ICSE 2005. [DLS02] M. Das, S. Lerner, and M. Seigle. ESP: Path-Sensitive Program Veri.cation \nin Polynomial Time. In PLDI 2002. [Edv99] J. Edvardsson. A Survey on Automatic Test Data Generation. \nIn Proceedings of the 2nd Conference on Computer Science and Engineering, pages 21 28, Linkoping, October \n1999. [GK05] P. Godefroid and N. Klarlund. Software Model Checking: Searching for Computations in the \nAbstract or the Concrete (Invited Paper). In IFM 2005. [GKS05] P. Godefroid, N. Klarlund, and K. Sen. \nDART: Directed Automated Random Testing. In PLDI 2005. [GMS00] N. Gupta, A. P. Mathur, and M. L. Soffa. \nGenerating Test Data for Branch Coverage. In ASE 2000. [God97] P. Godefroid. Model Checking for Programming \nLanguages using VeriSoft. In POPL 97. [God05] P. Godefroid. The Soundness of Bugs is What Matters (Position \nPaper). In BUGS 2005. [HCXE02] S. Hallem, B. Chelf, Y. Xie, and D. Engler. A System and Language for \nBuilding System-Speci.c Static Analyses. In PLDI 2002. [Kin76] J. C. King. Symbolic Execution and Program \nTesting. Journal of the ACM, 19(7):385 394, 1976. [Kor90] B. Korel. A Dynamic Approach of Test Data Generation. \nIn IEEE Conference on Software Maintenance, 1990. [Mye79] G. J. Myers. The Art of Software Testing. Wiley, \n1979. [RHS95] T. Reps, S. Horwitz, and M. Sagiv. Precise interprocedural data.ow analysis via graph reachability. \nIn POPL 95. [SMA05] K. Sen, D. Marinov, and G. Agha. CUTE: A Concolic Unit Testing Engine for C. In FSE \n2005. [VPK04] W. Visser, C. Pasareanu, and S. Khurshid. Test Input Generation with Java PathFinder. In \nISSTA 2004. [XMSN05] T. Xie, D. Marinov, W. Schulte, and D. Notkin. Symstra: A Framework for Generating \nObject-Oriented Unit Tests Using Symbolic Execution. In TACAS 2005. [YST+06] J. Yang, C. Sar, P. Twohey, \nC. Cadar, and D. Engler. Automat\u00adically Generating Malicious Disks using Symbolic Execution. In Proceedings \nof IEEE Security and Privacy 2006,Oakland, 2006.  \n\t\t\t", "proc_id": "1190216", "abstract": "Dynamic test generation is a form of dynamic program analysis that attempts to compute test inputs to drive a program along a specific program path. Directed Automated Random Testing, or DART for short, blends dynamic test generation with model checking techniques with the goal of systematically executing all feasible program paths of a program while detecting various types of errors using run-time checking tools (like Purify, for instance). Unfortunately, systematically executing <i>all</i> feasible program paths does not scale to large, realistic programs.This paper addresses this major limitation and proposes to perform dynamic test generation <i>compositionally</i>, by adapting known techniques for interprocedural static analysis. Specifically, we introduce a new algorithm, dubbed <i>SMART</i> for <i>Systematic Modular Automated Random Testing</i>, that extends DART by testing functions in isolation, encoding test results as function summaries expressed using input preconditions and output postconditions, and then re-using those summaries when testing higher-level functions. We show that, for a fixed reasoning capability, our compositional approach to dynamic test generation (SMART) is both sound and complete compared to monolithic dynamic test generation (DART). In other words, SMART can perform dynamic test generation compositionally without any reduction in program path coverage. We also show that, given a bound on the maximum number of feasible paths in individual program functions, the number of program executions explored by SMART is linear in that bound, while the number of program executions explored by DART can be exponential in that bound. We present examples of C programs and preliminary experimental results that illustrate and validate empirically these properties.", "authors": [{"name": "Patrice Godefroid", "author_profile_id": "81100504535", "affiliation": "Microsoft Research", "person_id": "PP40031134", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1190216.1190226", "year": "2007", "article_id": "1190226", "conference": "POPL", "title": "Compositional dynamic test generation", "url": "http://dl.acm.org/citation.cfm?id=1190226"}