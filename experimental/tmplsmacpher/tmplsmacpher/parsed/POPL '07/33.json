{"article_publication_date": "01-17-2007", "fulltext": "\n Specialization of CML message-passing primitives John Reppy Yingqi Xiao University of Chicago University \nof Chicago jhr@cs.uchicago.edu xiaoyq@cs.uchicago.edu Abstract Concurrent ML (CML) is a statically-typed \nhigher-order concur\u00adrent language that is embedded in Standard ML. Its most notable feature is its support \nfor .rst-class synchronous operations. This mechanism allows programmers to encapsulate complicated com\u00admunication \nand synchronization protocols as .rst-class abstrac\u00adtions, which encourages a modular style of programming \nwhere the underlying channels used to communicate with a given thread are hidden behind data and type \nabstraction. While CML has been in active use for well over a decade, little attention has been paid \nto optimizing CML programs. In this paper, we present a new program analysis for statically-typed higher-order \nconcurrent languages that enables the compile-time specialization of communication operations. This specialization \nis particularly important in a multiprocessor or multicore setting, where the synchronization overhead \nfor general-purpose operations are high. Preliminary results from a prototype that we have built demonstrate \nthat specialized channel operations are much faster than the general-purpose operations. Our analysis \ntechnique is modular (i.e., it analyzes and opti\u00admizes a single unit of abstraction at a time), which \nplays to the mod\u00adular style of many CML programs. The analysis consists of three steps: the .rst is a \ntype-sensitive control-.ow analysis that uses the program s type-abstractions to compute more precise \nresults. The second is the construction of an extended control-.ow graph using the results of the CFA. \nThe last step is an iterative analysis over the graph that approximates the usage patterns of known channels. \nOur analysis is designed to detect special patterns of use, such as one-shot channels, fan-in channels, \nand fan-out channels. We have proven the safety of our analysis and state those results. Categories and \nSubject Descriptors F.3.2 [Logics and Meanings of Programs]: Semantics of Programming Languages Program \nanalysis; D.3.2 [Programming Languages]: Language classi.ca\u00adtions Concurrent, distributed, and parallel \nlanguages; D.3.3 [Programming Languages]: Language Constructs and Features Concurrent programming structures; \nD.3.4 [Programming Lan\u00adguages]: Processors Optimization General Terms Languages, Performance Keywords \nML, concurrent languages, message passing, static analysis Permission to make digital or hard copies \nof all or part of this work for personal or classroom use is granted without fee provided that copies \nare not made or distributed for pro.t or commercial advantage and that copies bear this notice and the \nfull citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. POPL 07 January 17 19, 2007, Nice, France. \nCopyright c &#38;#169; 2007 ACM 1-59593-575-4/07/0001. . . $5.00. 1. Introduction Concurrent ML (CML) \n[Rep91, Rep99] is a statically-typed higher-order concurrent language that is embedded in Standard ML \n[MTHM97]. CML extends SML with synchronous mes\u00adsage passing over typed channels and a novel abstraction \nmech\u00adanism, called .rst-class synchronous operations, for building syn\u00adchronization and communication \nabstractions. This mechanism allows programmers to encapsulate complicated communication and synchronization \nprotocols as .rst-class abstractions, which encourages a modular style of programming where the actual \nunderlying channels used to communicate with a given thread are hidden behind data and type abstraction. \nCML has been used successfully in a number of systems, including a multi\u00adthreaded GUI toolkit [GR93], \na distributed tuple-space implemen\u00adtation [Rep99], and a system for implementing partitioned applica\u00adtions \nin a distributed setting [YYS+01]. The design of CML has inspired many implementations of CML-style concurrency \nprimi\u00adtives in other languages. These include other implementations of SML [MLt], other dialects of ML \n[Ler00], other functional lan\u00adguages, such as HASKELL [Rus01], SCHEME [FF04], and our own MOBY language \n[FR99], and other high-level languages, such as JAVA [Dem97]. While CML has been in active use for well \nover a decade, little attention has been paid to optimizing CML programs. In this paper, we present a \nnew program analysis for statically-typed higher-order concurrent languages that is a signi.cant step \ntoward optimization of CML. Our technique is modular (i.e., it analyzes and optimizes a single unit of \nabstraction at a time), which plays to the modular style of many CML programs. The analysis consists \nof three steps. The .rst is a new twist on traditional control-.ow analysis (CFA) that we call type-sensitive \nCFA [Rep06]. This analysis is a modular 0-CFA that tracks values of abstract type (i.e., types de.ned \nin the module that are abstract outside the module) that escape into the wild. Because of type abstraction, \nwe known that any value of an abstract type that comes in from the wild must have previously escaped \nfrom the module. The second step is construction of an extended control-.ow graph (CFG) from the result \nof the CFA. This extended CFG has extra edges to represent process creation, values communicated by message-passing, \nand values communicated via the outside world (a.k.a. the wild). The last step is an iterative analysis \nof the CFG, which computes an approximation of the number of processes that send or receive messages \non the channel, as well as an approximation of the number of messages sent on the channel. This information \nallows us to detect special patterns of use (or topologies), such as one-shot channels, fan-in channels, \nand fan-out channels. These special patterns can then be exploited by using more ef.cient implementations \nof channel primitives. The paper has the following organization. In the next section, we describe speci.c \npatterns of communication that are common in message-passing programs. We also describe channel operations \nthat are specialized to these patterns and which have measurably better performance than the general-purpose \nones. We also present an example of a prototypical server as is found in many CML ap\u00adplications and use \nit to illustrate the opportunities for specialized communication. In Section 3, we de.ne the small concurrent \nlan\u00adguage that we use to present our analysis and we give a dynamic semantics for it. This semantics \nhas the property that it explicitly tracks the execution history of individual processes; we use these \nexecution histories to characterize the dynamic properties of chan\u00adnels that must be guaranteed to safely \nuse the specialized forms. The main technical content of the paper is the presentation of our analysis, \nwhich we break up into four sections. In Section 4, we present the type-sensitive CFA for our language. \nThis analysis is de.ned for a single unit of abstraction and its result allows us to characterize a subset \nof the de.ned channels as known channels; i.e., channels whose send and receive sites are all statically \nknown. We then present the construction of the extended CFG in Section 5. The edges in this graph are \nlabeled with the set of known chan\u00adnels that are live across the edge. In Section 6, we describe the \nanalysis of the CFG that results in an approximation of the mod\u00adule s communication topology and the \nstatic properties that allow safe specialization of communication primitives. The .nal techni\u00adcal section \noutlines the proofs of correctness for our analysis (full proofs can be found in the second author s \nMaster s paper [Xia05]). We discuss related work in Section 8 and the implementation status and future \nwork in Section 9. Finally we conclude in Section 10. 2. Specialization of communication primitives Synchronous \nchannels are the main communication and synchro\u00adnization mechanism of CML. The underlying protocols used \nto implement these channels are necessarily general, since they must function correctly and fairly in \narbitrary contexts. Despite this gen\u00aderality, the existing implementation of CML is very ef.cient with \nminimal overhead on concurrent operations [Rep99]. We believe that this ef.ciency is important because \nit fosters a programming style that uses thread abstraction freely without incurring a debili\u00adtating \nperformance cost. One of the main reasons for this ef.ciency is that there is only one underlying thread \nexecuting at any time, which allows the implementation to easily single-thread critical re\u00adgions. With \nthe advent of inexpensive desktop multiprocessors and with multicore processors appearing even in laptops, \nthere is a real need to port CML to a multiprocessor environment. Unfortunately, implementing the CML \nprimitives in a multiprocessor environment incurs signi.cant additional runtime overhead, since more \ncompli\u00adcated protocols are required. The main motivation of the research described in this paper is to \ndevelop compiler optimizations that can reduce this overhead. As might be expected, most uses of CML \nprimitives fall into one of a number of common patterns, which are amenable to more ef.cient implementation. \nAs is often the case, the hard part of this optimization technique is developing an effective, but ef.cient, \nanalysis that identi.es when it is safe to specialize. Furthermore, we want this analysis to be modular \nso that it will easily scale to larger systems. Fortunately, CML s design emphasizes a modular programming \nstyle based on user-de.ned concurrency abstractions. While the motivation for this programming style \nis to promote more robust software, it also provides an opportunity for optimization, since the abstraction \nprovided by .rst-class synchronous operations allows modular analysis to determine the communication \ntopology. 2.1 Common patterns of communication Assuming that the basic communication primitive is a \nsynchronous channel, we consider the following possible communication pat\u00adterns: number of senders receivers \nmessages topology =1 =1 =1 one-shot =1 =1 >1 one-to-one =1 >1 >1 one-to-many (fan-out) >1 =1 >1 many-to-one \n(fan-in) >1 >1 >1 many-to-many  In this table, the notation >1 denotes the possibility that more than \none thread or message may be involved and the notation =1 denotes that at most one thread or message \nis involved. For example, the one-to-one pattern involves arbitrary numbers of messages, but at most \none sender and receiver. The many-to-many pattern is the general case. An analysis of message-passing \npatterns is safe if whenever it approximates the number of messages of threads as =1, then that property \nholds for all possible executions. It is always safe to return an approximation of >1. Another dimension \nof interest is whether a channel is used in choice contexts, since there is additional overhead in the \nimple\u00admentation of channels to support fairness and negative acknowl\u00adedgments in choice contexts. A channel \nthat is not used in choice contexts can have a simpler, and more ef.cient, implementation. For the bulk \nof this paper, we restrict ourselves to a language with\u00adout choice, but we describe how our analysis \ncan be extended to handle choice in Section 9. While programmers could specialize implementations by \nhand, doing so would complicate the programming model and could lead to less reliable software. Furthermore, \ncorrectness of a given proto\u00adcol often depends on the properties of the primitives used to imple\u00adment \nit. Changes to the protocol may require changes in the choice of primitives, which makes the protocol \nharder to maintain. For these reasons, we believe that an automatic optimization technique based on program \nanalysis and compiler transformations is neces\u00adsary. 2.2 Specialized channel operations In general, \na CML channel must support communication involv\u00ading multiple sending and receiving processes transmitting \nmulti\u00adple messages in arbitrary contexts. This generality requires a com\u00adplicated protocol to implement \nwith commiserate overhead.1 In a multiprocessor setting, the protocol used to implement channel communication \ninvolves multiple locks and other overhead. On the other hand, if we know that a channel has a restricted \npattern of use, then we can design a more ef.cient implementation. For example, consider a one-to-one \nchannel; i.e., a channel that has at most one sender and one receiver thread at any time and is not used \nin a choice context. Such a channel can only be in one of three distinct states: EMPTY neither thread \nis waiting for communication, RECV the receiver thread is waiting for the sender, or SEND the sender \nthread is waiting for the receiver. Furthermore, the channel s usage pattern means that its state tran\u00adsitions \nare restricted to the following state diagram:  Thus, the send operation can be implemented under the \nassump\u00adtion that the channel s state must be either EMPTY or RECV, 1 Chapter 10 of Concurrent Programming \nin ML describes CML s imple\u00admentation, while Knabe has described a similar protocol in a distributed \nsetting [Kna92]. structure SimpleServ : SIMPLE_SERV = struct datatype serv = S of (int * int chan) chan \nfun new () = let val ch = channel() fun server v = let val (req, replCh) = recv ch in send(replCh, v); \nserver req end in spawn (server 0); Sch end fun call (S ch, v) = let val replCh = channel() in send \n(ch, (v, replCh)); recv replCh end end Figure 1. A simple service with an abstract client-server protocol \nwhich means that a single atomic compare-and-swap instruction can be used to test for the EMPTY state \nand, if EMPTY, set the state to SEND. If the state was not EMPTY, then it must be RECV and the send operation \ncan be completed without further synchro\u00adnization.2 To understand the bene.ts of specialized channel \noperations in the multiprocessor setting, we have developed a prototype imple\u00admentation of CML channel \noperations that includes specialized implementations for the patterns described in the previous section. \nThis prototype is written in C and assembly language and we have tested it on both a dual single-core \nsystem and a quad dual-core system. While a detailed description of this prototype is beyond the scope \nof this paper, our preliminary results show that the spe\u00adcialized channels are signi.cantly faster than \nthe general-purpose channels. For example, the one-to-one channel described above is 3-4 times faster \nthan the general-purpose channel in our multipro\u00adcessor implementation and is as fast as the single-threaded \nCML channel on the same hardware.  2.3 An example To illustrate how the analysis and optimization might \nproceed, consider the simple service implemented in Figure 1. This service has the following abstract \ninterface:3 signature SIMPLE_SERV = sig type serv val new : unit -> serv val call : (serv * int) -> int \nend The new function creates a new instance of the service by allo\u00adcating a new channel and spawning \na new server thread to han\u00addle requests on the channel. The representation of the service is the request \nchannel, but it is presented as an abstract type. The 2 Of course, there is also the need to schedule \nthe receiver thread for execution, but that cost would be required no matter how the channel protocol \nis implemented. 3 To keep the example concise, we use direct operations on channels in\u00adstead of CML s \nevent operations, but event values can be handled without dif.culty (see Section 9).  Figure 2. Data-.ow \nof the server s request channel structure SimpleServ : SIMPLE_SERV = struct datatype serv =S of (int \n* int OneShot.chan) FanIn.chan fun new () = let val ch = FanIn.channel() fun server v = let val (req, \nreplCh) = FanIn.recv ch in OneShot.send(replCh, v); server req end in spawn (server 0); Sch end fun \ncall (S ch, v) = let val replCh = OneShot.channel() in end Figure 3. A version of Figure 1 with specialized \ncommunication operations call function sends a request to a given instance of the service. The request \nmessage consists of the request and a fresh channel for the reply. Because the connection to the service \nis represented as an abstract type, we know that even though it escapes out of the SimpleServ module, \nit cannot be directly accessed by un\u00adknown code. Figure 2 illustrates the data-.ow of the service s re\u00adquest \nchannel. Speci.cally, we observe the following facts: For a given instance of the service, the request \nchannel has a many-to-one communication pattern.  For a given client request, the reply channel has \na one-to-one communication pattern and is used at most once (i.e.,itisa one-shot channel).  We can \nexploit these facts to specialize the communication opera\u00adtions resulting in the optimized version of \nthe service shown in Fig\u00adure 3. We have highlighted the specialized code and have assumed the existence \nof a module FanIn that implements channels spe\u00adcialized for the many-to-one pattern and a module OneShot \nthat is specialized for one-shot channels. In our prototype implementa\u00adtion, the specialized version \nof this service has 60% higher through\u00adput than the version implemented using general-purpose channels. \nWhile the relative bene.t for a more computationally intensive ser\u00advice would be less, minimizing communication \noverhead encour\u00adages the use of thread abstraction for encapsulating light-weight state. Because of the \nsignature ascription, we know all of the send and receive sites for the ch and replCh channels, but if \nwe added the function fun reveal (S ch) = ch to the service s interface, then the above transformation \nwould no longer be safe, since clients could use the reveal function to gain access to the server s request \nchannel and use it to send and receive messages in ways not supported by the specialized channels. The \ntechnical challenge is to develop program analyses that can detect the patterns described in Section \n2.1 automatically when they are present, but also recognize the situation where access to the channel \nis not limited (as with the reveal function). Another issue that the analysis must address is distinguishing \nbetween multiple threads that are created at the same spawn point. For example, say we have fun twice \nf = (f(); f()) and we create two servers sharing a common request channel using the code twice (fn () \n=> spawn(server 0)); Then our analysis should detect that the request channel ch is not a fan-in channel. \nNote, however, that replCh is still a one-shot channel. 3. A concurrent language We present our algorithm \nin the context of a small statically-typed concurrent language. This language is a monomorphic subset \nof Core SML [MTHM97] with explicit types and concurrency prim\u00aditives. Standard ML and other ML-like languages \nuse modules to organize code and signature ascription to de.ne abstraction, but use the abstype declaration \nto de.ne abstractions in lieu of modules. We further simplify this declaration form to only have a single \ndata constructor. Figure 4 gives the abstract syntax for this simple lan\u00adguage. A program pis a sequence \nof zero or more abstype declara\u00adtions followed by an expression. The analysis that we present below is \nmodular and can be applied to each abstype declaration (d) inde\u00adpendently. Each abstype de.nition de.nes \na new abstract type (T) and corresponding data constructor (D) and a collection of func\u00adtions (fbi). \nOutside the abstype declaration, the type T is abstract (i.e., the data constructor Dis not in scope). \nThe sequential expres\u00adsion forms include let-bindings, nested function bindings, function application, \ndata-constructor application and deconstruction,4 and pair construction and projection. In addition, \nthere are four concur\u00adrent expression forms: channel de.nition, process spawning, mes\u00adsage sending, and \nmessage receiving. Types include abstract types (T), function types, pair types, and channel types. Abstract \ntypes are either prede.ned types (e.g., unit, int, bool, etc.) or are de.ned by an abstype declaration. \nThis language does not include CML s event types or the cor\u00adresponding event combinators, but based on \nexperience with our prototype implementation, we believe that it is straightforward to add these to the \nanalysis framework, so we omit them to keep the presentation more compact. 4 In a language with sum types, \ndeconstruction would be replaced by a case expression. p ::= e | dp d ::= abstype T = Dof t with fb1 \n\u00b7\u00b7\u00b7 fbn end fb ::= fun f(x)= e e ::= x |||||||||||| let x = e1 in e2 fun f(x)= e1 in e2 e1 e2 De let \nDx = e1 in e2 (e1,e2) #ie where i.{1,2}chan cin e spawn e send(e1,e2) recv e t ::= T ||| t1 .t2 t1 \u00d7t2 \nchan t Figure 4. A simple concurrent language We assume that variables, data-constructor names, and \nabstract\u00adtype names are globally unique. We also assume that variables and constructors are annotated \nwith their type. We omit this type in\u00adformation most of the time for the sake of brevity, but, when nec\u00adessary, \nwe write it as a superscript (e.g., x t ). One should think of this language as a compiler s intermediate \nrepresentation following typechecking. We use LVAR to denote the set of variables de.ned in the cur\u00adrent \nabstype declaration, GVAR to denote variables de.ned else\u00adwhere, and VAR = LVAR .GVAR for all variables \nde.ned or men\u00adtioned in the program. We denote the known function identi.ers by FUNID . LVAR (i.e., those \nvariables that are de.ned by function bindings) and the known channel identi.ers by CHANID . LVAR (i.e., \nthose variables that are de.ned by channel bindings). The set ABSTY is the set of abstract type names \nand DATACON is the set of data constructors. 3.1 Dynamic semantics Following Colby [Col95], the semantics \nfor our language tracks execution history on a per-process basis. This information is neces\u00adsary to characterize \nthe dynamic usage of channels. Since abstype declarations do not play a r\u00f4le in the dynamic semantics \nof the language, we think of a program as a sequence of nested function bindings. For example, abstype \nT = Dof t with fun f(x)= e1 fun g(y)= e2 end e 3 is treated as fun f(x)= e1 in fun g(y)= e2 in e3 For \na given program p, we assume that each expression in pis labeled with a unique program point a. PROGPT. \nWe write a: e to denote that eis the expression at program point a. Furthermore, we assume that for each \na .PROGPT, there is a a\u00af.PROGPT. The a\u00aflabels are not used to label expressions, but serve to distinguish \nbetween parent and child threads in control paths. A control path is a .nite sequence of program points: \nCTLPATH = PROGPT * . We use p to denote an arbitrary control path and juxtaposition to denote concatenation. \nWe say that p . p' if p is a pre.x of p'. Control paths are used to uniquely label dynamic instances \nof channels, which we write c@p, where c .CHANID. We also use k to denote dynamic channel values, and \nK to denote the set of dynamic channel values. Evaluation of the sequential features of the language \nfol\u00adlows a standard small-step presentation based on evaluation con\u00adtexts [FF86]. We modify the syntax \nof expression terms to distin\u00adguish values as follows: v ::= ||| (fun f (x)= e) k (v1,v2) e ::= v | \u00b7\u00b7\u00b7 \n The unit value ( ) was already part of the syntax, but we add function values, dynamic channel values, \nand pairs of values. With these de.nitions, we can de.ne the sequential evaluation relation e . e' by \nthe rules in Figure 5. Evaluation contexts are de.ned in the standard call-by-value way and are used \nin the de.nition of concurrent evaluation. E ::= [] | let x = E in e |let Dx = E in e | Ee |vE |DE | \nsend(E, e) |send(v, E) |recv E |(E, e)|(v, E)|#iE For the semantics of concurrent evaluation, we represent \nthe state of a computation as a tree, where the nodes of the tree are labeled with expressions representing \nprocess states and edges are labeled with the program point corresponding to the evaluation step taken \nfrom the parent to the child. The leaves of the tree represent the current states of the processes in \nthe computation. Because a tree captures the history of the computation as well as its current state, \nwe call it a trace. Nodes in a trace are uniquely named by control paths that describe the path from \nthe root to the node. In de.ning traces, it is useful to view them as pre.x-closed .nite functions from \ncontrol paths to expressions. If t is a trace, then we write t.p to denote the node one reaches by following \np from the root, and if t.p is a leaf of t, a is a program point, and e an expression, then t .{pa . \ne}is the trace with a child e added to t.p with the new edge labeled by a. For a program p, the initial \ntrace will be the map {E .p}, where E is the empty control path. We de.ne concurrent evaluation as the \nsmallest relation (.) between traces satisfying the following four rules. The .rst rule lifts sequential \nevaluation to traces. ' t.p = E[a : e] is a leaf e . et .t .{pa .E[e']} The second rule deals with channel \ncreation. t.p = E[a : chan c in e] is a leaf t .t .{pa .E[e[c.c@pa]]} The third rule deals with process \ncreation. t.p = E[a : spawn e] is a leaf t .t .{pa a.e} .E[ ],p\u00af The last rule deals with communication. \nt.p1 = E1[a1 : send(k, v)] is a leaf t.p2 = E2[a2 : recv k] is a leaf t .t .{p1a1 .E1[ ],p2a2.E2[v]} \n The set of traces of a program represents all possible executions of the program. It is de.ned as Trace(p)= \n{t |{E .p}. * t}  3.2 Properties of traces Let p be a program and let c be a channel identi.er in p. \nFor any trace t . Trace(p) and k = c@p occurring in t, we de.ne the dynamic send and receive sites of \nk as follows: Sendst(k)= {p |t.p = E[send(k, v)]} Recvst(k)= {p |t.p = E[recv k]} We say that c has \nthe single-sender property if for any t . Trace(p), k = c@p occurring in t, and p1,p2 . Sendst(k), either \np1 .p2 or p2 .p1. The intuition here is that if p1 .p2 then the sends can not be concurrent. On the other \nhand, if p1 and p2 are not related by ., then they may be concurrent.5 Note that the single-sender property \nallows multiple processes to send messages on a given channel, they are just not allowed to do it concurrently. \nLikewise, we say that c has the single-receiver property if for any t .Trace(p), k = c@p occurring in \nt, and p1,p2 .Recvst(k), either p1 .p2 or p2 .p1. We can now state the special channel topologies from \nSec\u00adtion 2.1 as properties of the set of traces of a program. For a channel identi.er c in a program \np, we can classify its topology as follows: The channel c is a one-shot channel if for any t .Trace(p) \nand k = c@p occurring in t, |Sendst(k)|=1.  The channel c is point-to-point if it has both the single-sender \nand single-receiver properties.  The channel c is a fan-out channel if it has the single-sender property, \nbut not the single-receiver.  The channel c is a fan-in channel if it has the single-receiver property, \nbut not the single-sender.  Our analysis computes safe approximations of these properties, which are \ndescribed in Section 6.1. 4. Type-sensitive control-.ow analysis for CML The .rst step of our analysis \nis a standard abstract-interpretation\u00adstyle control-.ow analysis of the program [Shi91, Ser95]. The goal \nof this analysis is two-fold: .rst we need to determine the control\u00ad.ow and data-.ow of the program, \nbut we also want to identify known channels (i.e., channels for which we know the creation site and all \nuse sites). One might use a whole-program analysis for this purpose, but we have developed a modular \nCFA instead. This CFA is based on Serrano s version of 0-CFA [Ser95], but with a couple of important \ndifferences. First, our source language is stat\u00adically typed and has concurrency operations. Second, \nour analy\u00adsis exploits the type abstraction in the program, such as provided by ML signature ascription \nor abstype de.nitions, to improve the quality of the results. We call our analysis Type-sensitive CFA. \nA detailed description of this algorithm can be found in a recent paper [Rep06], but we sketch the technique \nhere. The basic intuition behind our approach is that if a value s type is abstract outside the scope \nof a module, then any value of that type can only be allocated by code that is inside the module and \n5 There may be other causal dependencies, such as synchronizations, that would order p1 and p2, but our \nmodel does not take these into account. let x = v in e e[x.v] let Dx = Dv in e e[x.v] fun f(x)= e1 in \ne2 e2[f .(fun f(x)= e1)] (fun f(x)= e) v e[f .(fun f(x)= e),x.v] #i(v1,v2) vi Figure 5. Sequential evaluation \nany operation on the value s representation must also be inside the module. We re.ect this intuition \nby computing a mapping from the abstract types of a module to an approximation of the values of that \ntype that have escaped into the wild. The analysis then uses this mapping to approximate any unknown \nvalues of abstract type that might .ow into the module. Note that our approach should apply to any language \nthat has data abstraction mechanisms. In the remainder of this section, we discuss those aspects of our \nCFA that are important to the analysis of CML code. 4.1 Approximate values The analysis computes a mapping \nfrom variables to approximate values (ABSVAL), which are given by the following grammar: vb::= . | Dbv \n| |(vb1,vb2) | F |C | chan t |t.t Tb| . 1 .2 |T where D .DATACON, F .2FUNID, C .2CHANID, and T . ABSTY. \nWe use .to denote unde.ned or not yet computed values, Dvbfor an approximate value constructed by applying \nD to vb, (vb1,vb2)for an approximate pair, F for a set of known functions, and Cfor a set of known channels. \nOur analysis will only compute sets of functions and sets of channels where all the members have the \nsame type (see [Rep06] for a proof of this property) and so we extend our type annotation syntax to include \nsuch sets. In addition to the single top value found in most presentations of CFA, we have a family of \ntop values (tb) indexed by type. The value tbrepresents an unknown value of type t (where t is either \na function or abstract type). We de.ne the .operation in the standard way to combine two approximate \nvalues. 4.2 Type-sensitive CFA Our analysis algorithm iteratively computes a 4-tuple of approxi\u00admations: \nA=(V,C,R,T), where V. VAR .ABSVAL variable approximation C. CHANID .ABSVAL channel-message approximation \nR. FUNID .ABSVAL function-result approximation T. ABSTY .ABSVAL escaping abstract-value approximation \nOur Vapproximation corresponds to Serrano s A; Cis an ap\u00adproximation of the messages sent on a given \nknown channel; R records an approximation of function results for each known func\u00adtion, which is used \nin lieu of analyzing a function s body when the function is already being analyzed, and Trecords escaping \nvalues and is used to interpret abstract values of the form Tb. The main workhorse of our CFA is the \nfunction cfa that takes an expression eand an approximation Aand computes the approx\u00adimate value of eand \na, possibly different, approximation.  4.3 Analysis of message-passing operations When analyzing message \nsend and receive operations, we use the Capproximation to connect the send and receive sites for a given \nchannel. For example, when computingcfa([[recv e]], A) we .rst compute (b' )= cfa([[e]], A v,A ) If \nvbis of the form C, then the approximate result of the receive operation is the join of the approximate \nvalues carried by all of the channels in C: _ C(c) c.C Otherwise, vbis a top value, but if its type \nis chan T, we can use T(T) for the approximation of the receive operation. For send operations, if C \nis the approximation of the .rst argu\u00adment and vb2 is the approximation of the second, then we replace \nthe Ccomponent of Awith C ', which is de.ned to be j ' C(c) .vb2 if c.C C(c)= C(c) otherwise In this \nway, the CFA is able to propagate approximations from send sites to receive sites. 4.4 Escaping abstract \nvalues In Serrano s analysis (and any other modular CFA that we are aware of), escaping values are treated \nconservatively. For example, the analysis assumes that any escaping function can be called on any value, \nso the functions parameters are approximated as T. For escaping channels, this would mean assuming arbitrary \nsenders and receivers and arbitrary messages, which would make modular analysis of typical CML modules, \nsuch as our example, useless. To avoid this problem, our analysis tracks escaping values of abstract \ntype by recording them in the Tapproximation. In turn, Tis used to approximate values of abstract type \nthat come in from the wild. 4.5 Known channels Our CFA allows one to compute certain static approximations \nof the dynamic properties described in Section 3.2. Figure 6 gives the approximation of the send and \nreceive sites for a given channel. If the channel escapes (denoted Esc(c)), then we use Tto denote the \nset. A channel for which we know all of the send and receive sites is called a known channel. 4.6 An \nexample To illustrate our type-sensitive CFA, we revisit the example of Figure 1, but recast in the notation \nof our simple language (with a few syntactic liberties). Figure 7 shows the example with program\u00adpoint \nlabels included. Our CFA produces the following information for this example: SendSites(ch)= {a13} RecvSites(ch)= \n{a4} SendSites(replCh)= {a5} RecvSites(replCh)= {a14} Thus, both ch and replCh are known channels. j \n {a|a: send(e1,e2) .p.c.A(e1)} if \u00acEsc(c) SendSites(c)= T if Esc(c) j {a|a: recv e.p.c.A(e)} if \u00acEsc(c) \nRecvSites(c)= T if Esc(c) Figure 6. Approximation of channel send and receive sites a1 : fun new () \n= ( a2 : chan ch in a3 : fun serverv=( a4 : let (w , replCh ) = recv ch in a5 : send (replCh , v); a6 \n: server w ) in a7 : spawn (a8 : server 0); a9 : S ch) a10 : fun call (s, w) = ( a11 : letSch =sin a12 \n: chan replCh in a13 : send (ch , (w, replCh)); a14 : recv replCh) Figure 7. The simple service in our \nsimple language 5. The extended CFG With the information from the CFA in hand, the next step of our analysis \nis to construct an extended control-.ow graph (CFG) for the module that we are analyzing. We then use \nthis extended CFG to compute approximate trace fragments that can be used to analyze the topology of \nthe program. There is a node in the graph for each program point; in addition, there is an entry and \nexit node for each function de.nition. A node with a label acorresponds to the point in the program s \nexecution where the next redux is labeled with a. The graph has four kinds of edges. The .rst two of \nthese represent control .ow, while the other two are used to track the .ow of values outside the module. \n1. Control edges represent normal sequential control-.ow. 2. Spawn edges represent process creation. \nIf there is an expres\u00adsion a1 : spawn eand a2 is the label of the .rst redux in e, then there will be \na spawn edge from a1 to a2. 3. Message edges are added from send sites to receiver sites for known channels. \n 4. Wild edges are added to represent the potential .ow of abstract values from one function in the module \nto another.  The graph is constructed such that a control edge from a1 to a2 corresponds to a trace \nedge labeled with a1 that leads to a trace node labeled by a2. Similarly, a spawn edge from a1 to a2 \nin the CFG corresponds to a\u00af1 in a trace. More formally, the sets of nodes and edges are de.ned to be \nn.NODE = PROGPT .(FUNID \u00d7{entry,exit}) EGLABEL = {ctl,spawn,msg,wild} EDGE = NODE \u00d7EGLABEL \u00d7NODE NODE \nEDGE G.GRAPH =2\u00d72 The successors of a node n in a graph G are de.ned to be SuccG(n)= {n ' |(n,l,n ' ) \nis an edge in G}. Constructing the CFG is done in three steps. First we create the basic graph with \ncontrol and spawn edges in the obvious way. One important point is that we use the results of the CFA \nto determine the edges from call sites to known functions. Note that because we are only interested in \ntracking known channels, which by de.nition cannot have escaped the module, we can ignore calls to unknown \nfunctions when constructing the graph. Message edges are added in much the same way as control edges \nfor known function calls. Let a : send(e1,e2) be a send in the program and assume that the CFA computed \nCas the approximation of e1. Then for each channel c.Cand a ' .RecvSites(c), we add a send edge from \nato a ' to the graph. We add wild edges from any site where an abstract value escapes the module to any \nsite where such a value can return from the wild. Once we have constructed the graph, we use a liveness \nanalysis to label the edges with the set of known channels that are live across the edge. As described \nin the next section, we use these edge labels to limit the scope of the analysis on a per\u00adchannel basis. \n5.1 An example Figure 8 give the extended control-.ow graph for our running example. We have labeled \neach edge with the set of known channels that are live across the edge. This graph illustrates the three \nways that a channel can be shared among multiple threads (and thus have multiple senders/receivers): \n1. A process is spawned that has the channel in its closure. This is represented by the channel being \nin the label of the spawn edge (e.g., ch on the edge from a7 to a8). 2. The channel is sent in a message \nfrom one process to another. This is represented by the channel being in the label of the message edge \n(e.g., replCh on the edge from a13 to a4). 3. The channel escapes into the wild and then returns as \nthe argu\u00adment to an exported function. This is represented by the channel being in the label of a wild \nedge from the exit of one function to the entry of another (e.g., ch on the edge from the exit of new \nto the entry of call).  6. Analyzing the CFG The .nal stage of our analysis involves using the CFG \nto deter\u00admine the communication topology. We do this step independently for each known channel in the \nmodule. Because the analysis is con\u00adcerned with only a single channel cat a time, we can ignore those \nparts of the graph where cis not live (essentially remove any edge that does not contain cin its label \nset). The analysis computes a .nite map Pbthat maps program points to an approximation of the possible \ncontrol paths that execution follows to get to the program point. .nC;TLPATH b P.PATHTO = PROGPT .2 \nwhere the set of abstract control paths is de.ned by the syntax pb::= *:p | p1:p2 {ch} {ch} {ch, replCh} \n{replCh} {ch} {} Control edge Spawn edge Message edge Wild edge Figure 8. The CFG for the example For \nan approximate control paths pb, we split the path into a process ID part (the part before the : ) and \na path. The process ID can either be * , which is used to represent an unknown set of processes, or a \npath that uniquely identi.es the process. We de.ne an ordering .on abstract control paths as follows: \np1:p1 ' .p2:p2 ' if p1 = p2 and p1 ' p2' . In other words, pb1 .pb2 if they are paths in the same process \nand if pb1 is a pre.x of pb2. The following notation is used to project the process ID part from an approximate \ncontrol path: f Proc(*:p2)= * f Proc(p1:p2)= p1 We lift f Proc to sets of control paths in the standard \nway. If A is a set of approximate control paths, then we de.ne the number of distinct processes in A \nas follows: NumProcs(A)= 8 Proc(A) if *.f NumProcs(A)= |f Proc(A)| otherwise The analysis of a CFG G \nis de.ned by a pair of mutually recursive functions: c NG : NODE .CTLPATH .PATHTO .PATHTO c EG : EDGE \n.CTLPATH .PATHTO .PATHTO The de.nition of these functions can be found in Figure 9, where b P \u00d8= {a .\u00d8|a \n.PROGPT}is the .nite map that assigns the empty path set to every program point. c The NG function is \nde.ned by case analysis. For a function f s entry it follows the unique control edge to the .rst program \npoint of f. For the exit of f, it computes the union of the analysis for all outgoing edges. These edges \nwill either be control edges to f s call sites, when f is a known function, or wild edges, when f is \nan escaping function. For program-point nodes, we b have three subcases. If the approximation P already \ncontains a path pid:p1ap2 that precedes pband pid:p1 . Pb(a), then we have looped (the loop is a.p2.a) \nand can stop. If the number of processes that can reach the program point a is greater than ' one, then \nwe stop.6 Otherwise, we record the visit to a in Pband compute the union over the outgoing edges. c \nThe EG function is also de.ned by cases. When the edge is a control edge labeled by a, we analyze the \ndestination node by passing in the extended path b pa. When the edge is a spawn edge, we analyze the \ndestination node by passing in a new process ID paired with the empty path. For message edges, we analyze \nthe receive site using the send-site program point as a new process ID. This choice of process ID distinguishes \nthe send from other sends that target the same receive sites, but it con.ates multiple receive sites \nthat are targets of the same send, which is safe since only one receive site can actually receive the \nmessage. For wild edges, we analyze the destination node using * as the process ID, since any number \nof threads might call the target of the wild edge with the same dynamic instance of the channel c. 6.1 \nStatic classi.cation of channels For a known channel c that is de.ned at a : chan c in e,we c can statically \nclassify c by examining Pcc = NG[[a]]E:EPb\u00d8. First we de.ne the approximate send and receive contexts \nfor c as follows: [ cSc = cPc(a) a. ;SendSites(c) [ cRc = cPc(a) a. ;RecvSites(c) These are the static \napproximations of the Sends and Recvs sets from Section 3.2. We say that a known channel c has the static \nsingle sender (resp. static single receiver) property if it is the case that NumProcs(Scc) =1 (resp. \nNumProcs(Rcc) =1). The static classi.cation of channels then follows the dynamic classi.cation from Section \n3.1. If NumProcs(Scc) =1 and .bp2 .cwith b b p1, bSc p1= p2 and pb1 .pb2, then c is a one-shot channel. \n If c has both the static single-sender and static single-receiver properties, then it is a point-to-point \nchannel.  If c has the static single-sender property, but not the static single-receiver, then it is \na fan-out channel.  If c has the static single-receiver property, but not the static single-sender, \nthen it is a fan-in channel.   6.2 An example Once again, we turn to our running example to illustrate \nthe intu\u00adition behind our analysis. Recall that we have two known channels: ch, which is created at a2, \nand replCh, which is created at a12. We .rst consider replCh. Figure 10 give the restriction of the CFG \nfrom Figure 8 to the subgraph in which replCh is live. Notice that although replCh is received by the \nserver in its loop, the fact that replCh is not live after node a5 means that we do not analyze the loop \nand thus avoid confusing different instances of replCh with each other. Computing = NreplCh PreplCh G \n[[a11]]E:EPb\u00d8 6 Recall that we are interested in channels that have single senders or receivers. N c \nG[[(f,entry)]]bp bP = N c G[[a ' ]]bp bP 0 where SuccG(f,entry)= {a ' }1 N c G[[(f,exit)]]bp bP = bP \n. @ [ Ec G[[e]]bp bPA e.EdgeG(a) N c G[[a]]bp bP = bP if .pid:p1ap2 . bP(a) such that pid:p1ap2 bpand \npid:p1 . bP(a). = bP if NumProcs( bP(a)) = 2 0 1 = bP ' . @ [ Ec G[[e]]bp bP 'A where bP ' = bP .{a . \nbP(a) .{bp}} e.EdgeG(a) Ec N c G[[(a,ctl,n)]]pbPb= G[[n]]bP pa b j '' *:E if pb= *:p Ec p bN c b G[[(a,spawn,n)]]bP \n= G[[n]]pbP where pb= p1p2a\u00af:E if pb= p1:p2 j '' *:E if pb= *:p Ec p bN c b G[[(a,msg,n)]]bP = G[[n]]pbP\u00d8 \nwhere pb= a:E otherwise Ec N c G[[(a,wild,n)]]pbPb= G[[n]]*:EPb\u00d8 Figure 9. Analyzing the CFG Gfor channel \nc a12 {ch, replCh} a13  {replCh} {replCh} a14 a4 {ch, replCh} a5 Figure 10. The sub-CFG for replCh \nresults in the following mapping: PreplCh (a12)= {E:E} PreplCh (a13)= {E:a12} PreplCh (a14)= {E:a12a13} \nPreplCh (a4)= {a12a\u00af13:E} PreplCh (a5)= {a12a\u00af13:a4} From this map, we see that replCh is a one-shot \nchannel. The analysis for ch is more interesting, since it involves spawn\u00ading, loops, and wild edges. \nApplying the analysis algorithm to the relevant subgraph produces the following approximation: d Pch \n(a2)= {E:E} d Pch (a3)= {E:a2} d Pch (a7)= {E:a2a3} d Pch (a8)= {p:E} d Pch (a4)= {p:a8,p:a8a4a5a6} d \nPch (a5)= {p:a8a4,p:a8a4a5a6a4} d Pch (a6)= {p:a8a4a5,p:a8a4a5a6a4a5} d Pch (a9)= {E:a2a3a7} d Pch (a11)= \n{*:E} d Pch (a12)= {*:a11} d Pch (a13)= {*:a11a12} where p = a2a3a\u00af7. From this map, we see that c Sch \n= {*:a11a12} d Rch = {p:a8,p:a8a4a5a6} and thus ch is a fan-in channel. 7. Correctness of the analysis \nIn this section, we show that the static classi.cation of channels from Section 6 correctly follows the \ndynamic class.cation from Section 3.2. In other words, our analysis computes safe approxi\u00admations of \nthe properties from Section 3.2. The full details of the proofs can be found in the second author s Master \ns paper [Xia05]; here we cover the ideas underlying the proofs. First we introduce some notation. We \nuse p(i) to denote the i-th program point in p from left, and p(-i) to denote the i-th program point \nin p from right. Let p be a program and c be a channel identi.er in p. To prove our correctness results, \nwe need to instrument our se\u00admantics to record the communication history between the dynamic send and \nreceive sites. A communication history H is a subset H .{(p1,k,p2) | p1,p2 . CTLPATH,k . K} where (p1,k,p2) \n. H if there is communication between the dynamic send site p1 and receive site p2 on channel instance \nk in some trace t. For a program p, the initial communication history H will be the empty set. We extend \nthe . relation on traces to also track the communication history. This change only affects the rule for \ncommunication, which becomes t.p1 = E1[a1 : send(k, v)] is a leaf t.p2 = E2[a2 : recv k] is a leaf H, \nt .H ' ,t .{p1a1 .E1[ ],p2a2 .E2[v]}where H ' = H .{(p1,k,p2)} For the other evaluation rules, the history \ndoes not change; i.e.,if t . t ' then H, t . H, t '. We also extend the de.nition of the traces of a \nprogram: TraceH(p)= {H, t |{}, {E .p}. * H, t} Because our analysis is concerned with only a single channel \nat a time, we can ignore those parts of the trace where the channel is not live. We formalize this notion \nin the following de.nitions. De.nition 1 For any channel instance k of c in trace t .T race(p), the live \nprojection of trace t on k, denoted by t.k, is the forest cre\u00adated by removing all the nodes from t in \nwhich k does not occur. De.nition 2 Given a trace t .Trace(p), a channel instance k in t, and a control \npath p in t, the live projection of p on k is de.ned to be j p1 where p = p2ap1, p1 .t.k, ap(1) ./t.k \np otherwise Because our analysis is designed to modular, given any path p in t . T race(p), p may contain \nprogram points outside of the module being analyzed. But the following de.nitions show that for any path \nin the live projection of a trace, we can approximate the .1p= k (i)(i+1)that,ifforanytwoadjacentprogrampoints \nin.pGp,p(i)(i+1) m,thereisanedgefrom in,andwesaythat topppGpG c path by collapsing nodes outside the \nextended CFG into wild edges. De.nition 3 Let G be an extended CFG for some module in a program p and \nlet p be a path in t . T race(p). Then, we say ApproxPath(p)=iGc if there is no p(i) in the nodes of \nG. The function Partition : CTLPATH .CTLPATH * partitions a c path p into maximal sub-paths that are \neither in the module or in the wild. De.nition 4 Let G be the extended CFG for a module and let p be \na path in t .T race(p), then PartitionG(p)= (p1,p2,...,pm) where p1p2...pm = p and for any pi . Partition(p), \npi is the longest sub-path in p such that pi .G or p m G. De.nition 5 Let G be the extended CFG for a \nmodule and let p be a path in t .T race(p), then for any pi .PartitionG(p), j pi if pi .G E if pi m G \nThe following lemma asserts that for any path in the live projection of a trace, we can approximate the \npath by collapsing nodes outside the extended CFG into wild edges. Lemma 1 Let Gcc be an extended CFG \nand k = c@p ' a channel instance in t . Trace(p), then for any p . t.k, there exists a pb.Gcc, such that \npb= ApproxPath(p1) \u00b7\u00b7\u00b7ApproxPath(pm) Gc Gc Gc Recall that the approximate paths used to name send/receive \nsites start from channel creation sites, while the dynamic send/receive sites are labeled by paths starting \nfrom the trace root. The next def\u00adinition and lemma show that for each dynamic send/receive site of any \nchannel instance, there is a corresponding approximate path in the extended CFG, which starts from the \nchannel s creation site. Given any trace t of program p and channel instance k occur\u00adring in t, for any \ndynamic send/receive sites of k, the following de.nition of PathH tk : CTLPATH . CTLPATH * gives us a \nlist of paths that is a .ow history and shows how channel instance k .ows from its creation site to its \nsend/receive sites. For example, let p .Sendst(k) and PathH tk(p)= (p1,p2). This means that p1(1) is \nthe creation site of k, k .ows through p1 and then is sent as (-1) (1) a message from p1 to p2 on some \nchannel instance, and some value is sent on k at p2(-1) . De.nition 6 Given any H, t . Trace(p) and k \n= c@p ', for any p .Sendst(k) .Recvst(k), we de.ne j (p '' ) if p ''(1) = p '(-1) PathH tk(p)= (p1,p2,...,pm) \notherwise p ''' p '' , p '' 'p ''' p ''(1)p '' where p == p.k, pm = , pm = , (1) '(-1) (pi' ,ki,pi' \n+1) .H, pi = pi' .k, and p1 = p. Lemma 2 Given any t . T race(p) and k = c@p ', let the ex\u00adtended CFG \nbe Gcc. Then for any p . Sendst(k) .Recvst(k), .pb. c, and pb= pc1pc2... c, where PathH tk(p)= (p1,...,pm). \nGcpm This lemma shows that, for any dynamic send/receive site, our analysis computes the approximation \nof its channel instance .ow history in the extended CFG. The .nal step is to show that our static classi.cation \nof channels is safe with respect to the dynamic classi.cation, which we do in the following theorems. \nTheorem 3 ONE-SHOT SOUNDNESS Let c be a known channel in a module of p. Then, if there exists a trace \nt . T race(p) and instance k = c@p in t, such that |Sendst(k)| > 1, then .pc1,pc2 . Scc such that pc1 \n= pc2,or NumProcs(Scc) > 1. Theorem 3 shows that if there is only one approximate send path, there cannot \nbe more than one dynamic send site for any channel instance k of c. The idea underlying the proof is \nthat two different dynamic send sites have different channel instance .ow histories. In the extended \nCFG, they either have two different approximate paths or have the same approximate path. From De.nition \n5, we know that if two different .ow histories have the same approximation, then the channel instance \nmust escape from the module into the wild. In either case, the number of approximate send paths is more \nthan one. Thus our analysis is sound for the one-shot case. Theorem 4 SINGLE-SENDER SOUNDNESS If .t .T \nrace(p), .c@p in t, and .p1,p2 .Sendst(c@p), such that Proc(p1)= Proc(p2), then NumProcs(Scc) > 1. Theorem \n4 shows that if there is only one process in the approx\u00adimate send set, there cannot be more than one \nprocess in the dy\u00adnamic send sites. The idea underlying the proof is again to consider the channel instance \n.ow history for each dynamic send site. In the channel-instance .ow history, the channel instance either \nstays in the module, escapes into the wild, or is sent as a value to an\u00ad where (p1,...,pm)= Partition \n (p) other process. If in any of the above cases the number of processes involved in the dynamic send \nsites is more than one, then the ap\u00adproximate paths of .ow history have at least two different process \nID parts. Thus our analysis is sound for single-sender case. Theorem 5 SINGLE-RECEIVER SOUNDNESS If . \nt . Trace(p), .c@p in t, and . p1,p2 . Recvst(c@p), such that Proc(p1)= Proc(p2), then NumProcs(Rcc) \n= 2. Theorem 5 shows that if there is only one process in the approxi\u00admate receive set, then there cannot \nbe more than one process in the dynamic receive sites. The idea underlying the proof is similar to the \none in Theorem 4. 8. Related work There are a number of papers that describe various program anal\u00adyses \nfor message-passing languages such as CSP [Hoa78] and CML. We organize our discussion of these analyses \nby the tech\u00adniques used. A number of researchers have used effect-based type systems to analyze the communication \nbehavior of message-passing pro\u00adgrams. Nielson and Nielson developed an effects-based analysis for detecting \nwhen programs written in a subset of CML have .\u00adnite topology and thus can be mapped onto a .nite processor \nnet\u00adwork [NN94]. Debbabi et al. developed a type-based control-.ow analysis for a CML subset [DFT96], \nbut did not propose any appli\u00adcations for their analysis. In addition to being used as the basis for \nanalysis algorithms, type systems have been proposed that can be used to specify and verify properties \nof protocols. For example, Vasconcelos et al. have proposed a small message-passing language that uses \nses\u00adsion types to describe the sequence of operations in complex pro\u00adtocols [VRG04]. While this approach \nis not a program analysis, session types may be a useful way to represent behaviors in an anal\u00adysis. \nIn particular, they might provide an alternative to our sets of approximate control paths. There have \nalso been a number of abstract-interpretation-style analyses of concurrent languages that are closer \nin .avor to the analysis we described in Section 4. Mercouroff designed and im\u00adplemented an abstract-interpretation \nstyle analysis for CSP pro\u00adgrams [Mer91] based on an approximation of the number of mes\u00adsages sent between \nprocesses. While this analysis is one of the ear\u00adliest for message-passing programs, it is of limited \nutility for our purposes, since it is limited to a very static language. Jagannathan and Weeks proposed \nan analysis for parallel SCHEME programs that distinguishes memory accesses/updates by thread [JW94]. \nUn\u00adfortunately, their analysis is not .ne-grained enough for our prob\u00adlem since it collapses multiple \nthreads that have the same spawn point to a single approximate thread. Marinescu and Goldberg have developed \na partial evaluation technique for CSP [MG97]. Their algorithm can eliminate redundant synchronization, \nbut, like Mer\u00adcouroff s work, it is limited to programs with static structure. Mar\u00adtel and Gengler have \ndeveloped a control-.ow analysis that de\u00adtermines an approximation of a CML program s communication topology \n[MG00]. The analysis uses .nite automata to approxi\u00admate the synchronization behavior of a thread and \nthen extracts the topology from the product automata. Other researchers have used data-.ow techniques \nto analyze concurrent programs. Some of the earlier work was by Reif, who applied data-.ow analysis to \nan asynchronous CSP-like lan\u00adguage [Rei79]. His analysis produced an event spanning graph, which is similar \nto our extended CFG in that it includes edges from send sites to receive sites to track the .ow of data \nbetween processes. He used these graphs to compute an approximation of reachability for concurrent programs. \nMore recent work by Carlsson et al. uses data-.ow analysis to determine whether heap\u00adallocated values \nare sent to other processes (or not) [CSW03]. This information is used to specialize allocations to either \nthe local (per\u00adprocess) or global heap, which reduces message copying. Their analysis, like ours, starts \nwith 0-CFA to determine the control-.ow graph. They then run a .rst-order data-.ow analysis to track \nthe .ow of data values from their construction sites to where they are sent as messages. Colby s abstract-interpretation \nfor a subset of CML is probably the closest to ours [Col95]. His analysis is based on a semantics that \nuses control paths (i.e., an execution trace) to identify threads. Unlike using spawn points to identify \nthreads (as in [JW94]), con\u00adtrol paths distinguish multiple threads created at the same spawn point, \nwhich is a necessary condition to understand the topology of a program. The method use to abstract control-paths \nis left as a tunable parameter in his presentation, so it is not immediately obvious how to use his approach \nto provide the information that we need. His analysis is also a whole-program analysis. In addition to \nour prototype implementation of specialized channel operations, there is other evidence that specialized \nopera\u00adtions are signi.cantly faster than the general-purpose operations. Experience with the existing \nCML implementation has shown that even in the single-threaded implementation, specialized channel operations \ncan have signi.cant impact on communication over\u00adhead. For example, CML provides I-variables, which are \na form of synchronous memory that supports write-once semantics [ANP89]. Using I-variables in place of \nchannels for one-shot communi\u00adcations can reduce synchronization and communication costs by 35% [Rep99]. \nIn the distributed setting, Demaine describes a pro\u00adtocol for the ef.cient implementation of a generalized \nchoice con\u00adstruct, where fan-out and fan-in channel operations can be imple\u00admented with fewer network \nmessages per user-level communica\u00adtion than many-to-many channel operations [Dem98]. 9. Status and future \nwork The analysis presented in the paper does not include a number of important CML features, such as \nnon-deterministic choice and event combinators. It turns out that to add these features to the anal\u00adysis \nis mainly an issue of enriching the CFA to include a represen\u00adtation of approximate event values. We \ncan then further re.ne our characterization of known channels to distinguish between those that appear \nin a choice context versus those that do not. The ex\u00adtended CFA analysis also enables other CML optimizations, \nsuch as inlining wrapper functions. We have also considered the question of modelling asyn\u00adchronous (or \nbuffered) message passing. Our dynamic semantics would have to be extended with a mechanism to track \nin-.ight messages, but this extension is not dif.cult. We believe that our analysis is already correct \nfor buffered channels, since it is not sensitive to the order of messages. We have prototyped the analysis \nfor a language that is slightly larger than the one in the paper (it has tuples, basic values, con\u00additionals, \nand a subset of the CML event combinators). The next step will be to extend the analysis to the full \nset of CML primitives and SML features, such as modules, datatypes, and polymorphism (see [Rep06] for \na discussion of the latter). Eventually, we plan to implement the analysis and optimization as a source-to-source \ntool for optimizing CML modules. 10. Conclusion We have presented a new analysis technique for analyzing \ncon\u00adcurrent languages that use message passing, such as CML. Our technique is designed to be applied \non individual units of abstrac\u00adtion (e.g., modules). For a given module it determines an approx\u00adimation \nof the communication topology for the channels de.ned in the module. We have shown how this information \ncan be used to replace general-purpose channel operations with more special\u00adized ones. We have also described \npreliminary results from a pro\u00adtotype implementation of CML primitives in a multiprocessor set\u00adting. \nThese results demonstrate that the specialized operations have signi.cantly higher throughput than the \ngeneral-purpose operations (as much as a factor of 3-4 in some cases). Our analysis consists of three \nsteps. The .rst is a new variation of control-.ow analysis that we call type-sensitive CFA. The type \nsensitivity of the analysis is what allows us to effectively analyze modules independently of their use. \nThe second step constructs an extended CFG from the CFA results. And the third step analyses the CFG \nto approximate the numbers of messages and processes involved in communicating through known channels. \nAn impor\u00adtant property of this analysis is that it distinguishes between the situation of multiple threads \nusing the same channel and multi\u00adple threads using distinct channels. This distinction is key to en\u00adabling \nthe specialization of communication primitives. We have also stated and sketched the proof of correctness \nfor our analysis. We have presented the analysis for a simple concurrent lan\u00adguage, but we expect that \nit will be straightforward to extend to richer languages. The analysis may also be useful for statically \nde\u00adtecting other properties of concurrent programs (e.g., deadlock), but we have not explored this direction \nyet. References [ANP89] Arvind, R. S. Nikhil, and K. K. Pingali. I-structures: Data structures for parallel \ncomputing. ACM Transactions on Programming Languages and Systems, 11(4), October 1989, pp. 598 632. [Col95] \nColby, C. Analyzing the communication topology of concurrent programs. In PEPM 95, June 1995, pp. 202 \n213. [CSW03] Carlsson, R., K. Sagonas, and J. Wilhelmsson. Message analysis for concurrent languages. \nIn SAS 03, vol. 2694 of LNCS, New York, NY, 2003. Springer-Verlag, pp. 73 90. [Dem97] Demaine, E. D. \nHigher-order concurrency in Java. In WoTUG20, April 1997, pp. 34 47. Available from http: //theory.csail.mit.edu/~edemaine/papers/ \nWoTUG20/. [Dem98] Demaine, E. D. Protocols for non-deterministic com\u00admunication over synchronous channels. \nIn Proceedings of the 12th International Parallel Processing Symposium and 9th Symposium on Parallel \nand Distributed Processing (IPPS/SPDP 98), March 1998, pp. 24 30. Available from http://theory.csail.mit.edu/~edemaine/ \npapers/IPPS98/. [DFT96] Debbabi, M., A. Faour, and N. Tawbi. Ef.cient type-based control-.ow analysis \nof higher-order concurrent programs. In Proceedings of the International Workshop on Functional and Logic \nProgramming, IFL 96, vol. 1268 of LNCS,New York, N.Y., September 1996. Springer-Verlag, pp. 247 266. \n[FF86] Felleisen, M. and D. P. Friedman. Control operators, the SECD-machine, and the .-calculus. In \nM. Wirsing (ed.), Formal Description of Programming Concepts III, pp. 193 219. North-Holland, New York, \nN.Y., 1986. [FF04] Flatt, M. and R. B. Findler. Kill-safe synchronization abstractions. In PLDI 04, June \n2004. [FR99] Fisher, K. and J. Reppy. The design of a class mechanism for Moby. In PLDI 99, May 1999, \npp. 37 49. [GR93] Gansner, E. R. and J. H. Reppy. A Multi-threaded Higher\u00adorder User Interface Toolkit, \nvol. 1 of Software Trends, pp. 61 80. John Wiley &#38; Sons, 1993. [Hoa78] Hoare, C. A. R. Communicating \nsequential processes. Communications of the ACM, 21(8), August 1978, pp. 666 677. [JW94] Jagannathan, \nS. and S. Weeks. Analyzing stores and references in a parallel symbolic language. In LFP 94, New York, \nNY, June 1994. ACM, pp. 294 305. [Kna92] Knabe, F. A distributed protocol for channel-based communication \nwith choice. Technical Report ECRC-92\u00ad16, European Computer-industry Research Center, October 1992. [Ler00] \nLeroy, X. The Objective Caml System (release 3.00), April 2000. Available from http://caml.inria.fr. \n[Mer91] Mercouroff, N. An algorithm for analyzing communicating processes. In 7th International Conference \non the Mathe\u00admatical Foundations of Programming Semantics, vol. 598 of LNCS, New York, NY, March 1991. \nSpringer-Verlag, pp. 312 325. [MG97] Marinescu, M. and B. Goldberg. Partial-evaluation tech\u00adniques for \nconcurrent programs. In PEPM 97, June 1997, pp. 47 62. [MG00] Martel, M. and M. Gengler. Communication \ntopology analysis for concurrent programs. In 7th International SPIN Workshop, vol. 1885 of LNCS, New \nYork, NY, September 2000. Springer-Verlag, pp. 265 286. [MLt] http://mlton.org/ConcurrentML. [MTHM97] \nMilner, R., M. Tofte, R. Harper, and D. MacQueen. The De.nition of Standard ML (Revised). The MIT Press, \nCambridge, MA, 1997. [NN94] Nielson, H. R. and F. Nielson. Higher-order concurrent programs with .nite \ncommunication topology. In POPL 94, January 1994, pp. 84 97. [Rei79] Reif, J. H. Data .ow analysis of \ncommunicating processes. In POPL 79: Proceedings of the 6th ACM SIGACT-SIGPLAN symposium on Principles \nof programming languages, New York, NY, USA, January 1979. ACM Press, pp. 257 268. [Rep91] Reppy, J. \nH. CML: A higher-order concurrent language. In PLDI 91, New York, NY, June 1991. ACM, pp. 293 305. [Rep99] \nReppy, J. H. Concurrent Programming in ML. Cambridge University Press, Cambridge, England, 1999. [Rep06] \nReppy, J. Type-sensitive control-.ow analysis. In ML 06, New York, NY, September 2006. ACM, pp. 74 83. \n[Rus01] Russell, G. Events in Haskell, and how to implement them. In ICFP 01, September 2001, pp. 157 \n168. [Ser95] Serrano, M. Control .ow analysis: a functional languages compilation paradigm. In SAC 95: \nProceedings of the 1995 ACM symposium on Applied Computing, New York, NY, 1995. ACM, pp. 118 122. [Shi91] \nShivers, O. Control-.ow analysis of higher-order languages. Ph.D. dissertation, School of CS, CMU, Pittsburgh, \nPA, May 1991. [VRG04] Vasconcelos, V., A. Ravara, and S. Gay. Session types for functional multithreading. \nIn CONCUR 04, vol. 3170 of LNCS. Springer-Verlag, New York, NY, September 2004, pp. 497 511. [Xia05] \nXiao, Y. Toward optimization of Concurrent ML. Master s dissertation, University of Chicago, December \n2005. [YYS+01] Young, C., L. YN, T. Szymanski, J. Reppy, R. Pike, G. Narlikar, S. Mullender, and E. Grosse. \nProtium, an infrastructure for partitioned applications. In Proceedings of the Eighth IEEE Workshop on \nHot Topics in Operating Systems (HotOS), January 2001, pp. 41 46.    \n\t\t\t", "proc_id": "1190216", "abstract": "Concurrent ML (CML) is a statically-typed higher-order concurrent language that is embedded in Standard ML. Its most notable feature is its support for <i>first-class synchronous operations</i>. This mechanism allows programmers to encapsulate complicated communication and synchronization protocols as first-class abstractions, which encourages a modular style of programming where the underlying channels used to communicate with a given thread are hidden behind data and type abstraction.While CML has been in active use for well over a decade, little attention has been paid to optimizing CML programs. In this paper, we present a new program analysis for statically-typed higher-order concurrent languages that enables the compile-time specialization of communication operations. This specialization is particularly important in a multiprocessor or multicore setting, where the synchronization overhead for general-purpose operations are high. Preliminary results from a prototype that we have built demonstrate that specialized channel operations are much faster than the general-purpose operations.Our analysis technique is modular (<i>i.e.,</i>, it analyzes and optimizes a single unit of abstraction at a time), which plays to the modular style of many CML programs. The analysis consists of three steps: the first is a type-sensitive control-flow analysis that uses the program's type-abstractions to compute more precise results. The second is the construction of an <i>extended control-flow graph</i> using the results of the CFA. The last step is an iterative analysis over the graph that approximates the usage patterns of known channels. Our analysis is designed to detect special patterns of use, such as one-shot channels, fan-in channels, and fan-out channels. We have proven the safety of our analysis and state those results.", "authors": [{"name": "John Reppy", "author_profile_id": "81100590527", "affiliation": "University of Chicago", "person_id": "PP17010305", "email_address": "", "orcid_id": ""}, {"name": "Yingqi Xiao", "author_profile_id": "81322510236", "affiliation": "University of Chicago", "person_id": "PP37032523", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1190216.1190264", "year": "2007", "article_id": "1190264", "conference": "POPL", "title": "Specialization of CML message-passing primitives", "url": "http://dl.acm.org/citation.cfm?id=1190264"}