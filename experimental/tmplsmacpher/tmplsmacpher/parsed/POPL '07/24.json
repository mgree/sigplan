{"article_publication_date": "01-17-2007", "fulltext": "\n Assessing Security Threats of Looping Constructs Pasquale Malacaria Department of Computer Science, \nQueen Mary, University of London pm@dcs.qmul.ac.uk Abstract There is a clear intuitive connection between \nthe notion of leakage of information in a program and concepts from information theory. This intuition \nhas not been satisfactorily pinned down, until now. In particular, previous information-theoretic models \nof programs are imprecise, due to their overly conservative treatment of looping constructs. In this \npaper we provide the .rst precise information\u00adtheoretic semantics of looping constructs. Our semantics \ndescribes both the amount and rate of leakage; if either is small enough, then a program might be deemed \nsecure . Using the semantics we pro\u00advide an investigation and classi.cation of bounded and unbounded \ncovert channels. Categories and Subject Descriptors D.4.6 [Security and Protec\u00adtion]: Information .ow \ncontrols; H.1.1 [Systems and Information Theory]: Information theory General Terms Security,Theory, Languages \nKeywords Information Theory, Security, Language Semantics. 1. Introduction There is a basic conceptual \nissue that lies at the heart of the foun\u00addations of security: The problem is that secure programs do \nleak small amounts of information. An example is a password checking program if (l ==h)access else deny \nwhere an attacker will gain some information by observing what the output is (by observing deny he will \nlearn that his guess l was wrong). This makes non-interference1 [11] based models of security [24, 1] \nproblematic; they judge far too many programs to be insecure . As elegantly put in [22] In most non-interference \nmodels, a single bit of compro\u00admised information is .agged as a security violation, even if one bit is \nall that is lost. To be taken seriously, a non\u00adinterference violation should imply a more signi.cant \nloss. Even ...where timings are not available, and a bit per mil\u00adlisecond is not distinguishable from \na bit per fortnight ...a 1 Intuitively interference from x to y means changes in x affect the state of \ny. Non-interference is the lack of intererence Permission to make digital or hard copies of all or part \nof this work for personal or classroom use is granted without fee provided that copies are not made or \ndistributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. POPL 07 January 17 19, 2007, Nice, France. Copyright c . 2007 \nACM 1-59593-575-4/07/0001. . . $5.00 channel that compromises an unbounded amount of infor\u00ad mation is \nsubstantially different from one that cannot. Of course, using declassi.cation it is still possible \nto use a non\u00adinterference model to limit, rather than eliminate, the areas in a program where information \nwill be leaked. But, non-interference does not itself help us in deciding whether to declassify. Again, \n[22] raises the question: how we decide that a region is safe to declassify? To illustrate, consider \nthe following program containing a se\u00adcure variable h and a public variable l: l=20; while ( h < l) {l=l-1} \n The program performs a bounded search for the value of the secret h. Is it safe to declassify that program? \nOne could argue that the decision should depend on the size of the secret; the larger the secret the \nmore declassi.able it becomes. How to give a precise meaning to this argument? Is the previous program \nsecure if h is a 10-bit variable? Is it secure if h is a 16-bit variable? And shouldn t the answer depend \nalso on the attacker s knowledge of the distribution of inputs e.g. if she/he knew that 0 is a much more \nlikely value for h than any other value? The main objective of the present work is to develop a theory \nwhere this kind of questions can be mathematically addressed. To this aim we will develop an information \ntheoretical semantics of looping commands. The semantics is quantitative: outcomes are real numbers measuring \nsecurity properties of programs. The appeal of Shannon s information theory [23] in this context is that \nit combines the probability of an event with the damage the happening of that event would cause. In this \nsense information the\u00adory provides a risk assessment analysis of language based security. Consider again \nthe password checking program and suppose l, h are 2-bit variables and the distribution of values of \nh is uniform (all values are equally likely). We identify the damage associated to an event with the \ndifference between the size of the search space for the secret before and after the event has happened. \nThe more is revealed by an event-the larger the difference-the bigger the dam\u00adage. The damage for the \nevent observe access happening will be gaining information of the whole secret 2 =log(4)bits2 while the \ndamage for observe deny will be gaining information of one possibility being eliminated. Formally: 1. \nobserve access: probability = 14 ,  damage =log(4)- log(1)=log(41 )=2  2. observe deny: probability \n= 34 ,  damage =log(4)- log(3)=log(43 )  2 In the paper log stands for base 2 logarithm.   Combining \ndamages with probabilities we get 1 log(4 )+3 log(4 ) 4 14 3 P an instance of pi log(p1i ), Shannon s \nentropy formula. This paper introduces tools to compute the leakage in loops; .rst information theoretical \nformulas characterizing leakage are extracted by the denotational semantics of loops: these formulas \nare the basis for de.ning: 1. channel capacity: the maximum amount of leakage of a loop as a function \nof the attacker s knowledge of the input. 2. rate of leakage: the amount of information leaked as a \nfunction of the number of iterations of the loop.  These de.nitions are then used in a classi.cation \nof loops. This is an attempt to answer questions like: 1. is the amount of leakage of the loop unbounded \nas a function of the size of the secret? 2. How does the rate change when the size of the secret changes? \n Notice that in sequential programs there are no natural cases of unbounded covert channels unless loops \nare present; for this reason we claim that a major achievement of this work is the identi.cation of and \nmathematical reasoning about unbounded covert channels [22] Characterization of unbounded channels is \nsuggested as the kind of goal that would advance the study of this subject, and some creative thought \ncould no doubt suggest others. To motivate the relevance of this paper in the above contexts some case \nstudies are presented. We hope that by seeing the de.nitions at work in these cases the reader will be \nsatis.ed that the semantics is: 1. natural: i.e. in most cases agrees with our intuition about what the \nleakage should be and when it doesn t it provides new insights. 2. helpful: i.e. it provides clear answers \nfor situations where the intuition doesn t provide answers. 3. general: although some ingenuity is required \ncase by case, the setting is not ad hoc. 4. innovative: it provides a fresh outlook on reasoning about \ncovert channels in programs in terms of quantitative reasoning.  To complete the work we also address \nthe following basic ques\u00adtion: what is the meaning of information theoretical measures in the context \nof programming language interference? For example what does it mean that the above program leaks 2.6 \nbits for a 10\u00adbit variables under uniform distribution ? Based on recent work by Massey [17], Malone \nand Sullivan[16] it will be argued that this quantity is a lower bound on the attacker effort to guess \nthe secret using a binary search or a dictionary attack. 1.1 Contribution and related work Pioneering \nwork by Denning [7, 8] shows the relevance of infor\u00admation theory to the analysis of .ow of information \nin programs. She worked out semantics for assignments and conditionals, and gave persuasive arguments \nand examples. However, she did not show how to do a semantics of a full, Turing-complete program\u00adming \nlanguage, with loops. As a consequence, some of the exam\u00adples we consider involving unbounded channels \nare beyond the the\u00adory there. Further seminal work relating information theory and non\u00adinterference in \ncomputational systems was done by Millen, McLean, Gray [18, 13, 19]; none of this work however concentrate \non pro\u00adgramming languages constructs. In the context of programming languages the relations between \ninformation theory and non-interference [11, 20] relevant to the present work have been studied in a \nseries of papers by Clark, Hunt, Malacaria [2, 3, 4], where the background for the present work is introduced: \nthe main ingredients are an interpretation of programs and program variables in terms of random variables \nand a de.nition of leakage in terms of conditional mutual information. Other quantitative approaches \nto non-intereference have also recently been studied; Lowe [15] de.nes channel capacity in the context \nof CSP. Di Pierro, Hankin, Wiklicky propose a proba\u00adbilistic approach to approximate non-interference \nin a declarative setting[9] and more recently in distributed systems [10]. A prob\u00adabilistic beliefs-based \napproach to non-interference has been sug\u00adgested by Clarkson, Myers, Schneider [5]. Quantitative approaches \nto covert channel analysis in somewhat different contexts have been proposed by Gray and Syverson [12], \nWeber [25] and Wittbold [27]. To the best of our knowledge no work so far has provided a reasonable quantitative \nanalysis of loops in imperative languages; the bounds in [2, 3, 4] are over pessimistic (if any leakage \nis pos\u00adsible in a loop, the loop leaks everything). Hence the analysis here presented is original, and \nbecause of the relationship between un\u00adbounded covert channels and loops this paper provides an original \nquantitative analysis for covert channels in the context of program\u00adming languages. 1.2 Structure of \nthe work The article is structured as follows: Section 2 reviews some basic de.nitions from information \nthe\u00adory and presents an interpretation of program variables and commands in terms of random variables. \n Section 3 de.ne an information theoretical formula for the leak\u00adage of the command while e M. From \nthe leakage formula some de.nitions are derived, like rate of leakage, channel ca\u00adpacity, security, ratio \nof leakage.  Based on these de.nitions section 3.3 classi.es loops according to their leakage and rate \nof leakage.  Section 4 provides case studies justifying the usefulness of these notions.  Section 5 \nprovides a justi.cation of the information theoretical measures in this work. This justi.cation is based \non bounds on a dictionary attack scenario.   2. Preliminaries 2.1 Entropy, interaction, interference \nWe begin by reviewing some basic concepts of information theory relevant to this work; additional background \nis readily available both in textbooks [6] and on the web (e.g. the wikipedia entry for Entropy). Given \na space of events with probabilities P =(pi )i.N (N aset of indices) the Shannon s entropy is de.ned \nas H(P)=-Si.N pi log(pi ). It is usually said that this number measure the average uncer\u00adtainty of the \nset of events: if there is an event with probability 1 then the entropy will be 0 and if the distribution \nis uniform i.e. no event is more likely than any other the entropy is maximal, i.e. log(|N|). The entropy \nof a random variable is the entropy of its distribution. An important property of entropy which we will \nuse says that if we take a partition of the events in a probability space, the entropy of the space can \nbe computed by summing the entropy of the partition with the weighted entropies of the partition sets. \nWe call this the partition property; formally: given a distribution \u00b5 over aset S ={s1,1 ,...,sn,m } \nand a partition of S in sets (Si )1=i=n , Si ={si,1 ,...,si,m }: H(\u00b5(s1,1 ),...,\u00b5(sn,m ))= H(\u00b5(S1 ),...,\u00b5(Sn \n))+ P n \u00b5( si,1 ) \u00b5( si,m ) \u00b5(Si )H(,..., ) i= 1 \u00b5( Si ) \u00b5( Si ) P where \u00b5(Si )= \u00b5(si,j ). 1=j=m Given \ntwo random variables X,Y the conditional entropy H(X|Y) is the average of all entropies of X conditioned \nto a given value for Y, Y =y,i.e. SY= y \u00b5(Y =y)H(X|Y =y) where H(X|Y =y)=-SX= x \u00b5(X =x|Y =y)log(\u00b5(X =x|Y \n=y)). The higher H(X|Y)is the lower is the correlation between X and Y. It is easy to see that if X is \na function of Y, H(X|Y)=0 and if X and Y are independent H(X|Y)=H(X). Mutual information is de.ned as \nI(X;Y)=H(X)- H(X|Y)=H(Y)- H(Y|X) This quantity measures the correlation between X and Y. This fol\u00adlows \nfrom what we saw about conditional entropy: if X is a function of Y, I(X;Y)=H(X)- H(X|Y)=H(X)- 0 and \nif X and Y are inde\u00adpendent I(X;Y)=H(X)- H(X)=0. Mutual information is a measure of binary interaction. \nIn fact so far we have only de.ned unary or binary concepts. As we will see conditional mutual information, \na form of ternary interaction will be used to quantify interference. Conditional mu\u00adtual information \nmeasures the interference of a random variable on a binary interaction, i.e. I(X;Y|Z)=H(X|Z)- H(X|Y,Z)=H(Y|Z)- \nH(Y|X,Z) Conditional mutual information is always non negative; however it can affect interaction in \na positive or negative way as these examples show: I(X;Y|X . Y) = H(Y|X . Y)- H(Y|X . Y,X) = 1 - 0 > \n0 = I(X;Y) I(X;X . Y|X) = H(X|X)- H(X|X,X . Y) = 0 - 0 < 0.32 = I(X;X . Y) where X,Y are independent \nrandom variables taking boolean values and .,. are boolean conjunction and exclusive or. A positive interference \nI(X;Y|Z)means Z increase the interac\u00adtion between X and Y by contributing new relevant information, whereas \nnegative interference means Z removes information which was present in the interaction. In the previous \nexample X . Y contributes to the interaction of two independent random variables X,Y by bringing the \ninformation if they have the same value or not, whereas X doesn t bring any new information to the interaction \nbetween X and X . Y; in fact knowledge of X is detrimental to the interaction between X and X . Y because \nthat knowledge is removed from the interaction.  2.2 Random variables and programs The language we are \nconsidering is a simple imperative language with assignments, sequencing, conditionals and loops. Further \nin the paper we will add to this language a probabilistic choice op\u00aderator. Syntax and semantics for \nthe language are standard and so we omit them. The expressions of the language are arithmetic ex\u00adpression, \nwith constants 0,1,...and boolean expressions with con\u00adstants tt,ff. Following denotational semantics \ncommands are state trans\u00adformers, informally maps which change the values of variables in the memory \nand expressions are maps from the memory to values; we will denote by [[M]] the standard denotational \nseman\u00adtics of the program M [26]. We assume there are two input vari\u00adables H,L, the high (con.dential) \nand low (public) input, and we assume that inputs are equipped with a probability distribution, so we \ncan consider them as random variables (the input is the joint random variable (H,L)). A deterministic \nprogram M can hence be seen as a random variable itself, the output random variable where the probability \non an output value of the program is the sum of probabilities of all inputs evaluating via M to that \nvalue \u00b5(M =o)=S{\u00b5(h,l)|[[M]](h,l)=o)}. More formally 1. Our probability space is (O,A,\u00b5)where O ={s|s \n:{H,L}. N} A =P(O)(the power set) and \u00b5a probability distribution over O. An element s . O is a memory \nstate (environment), i.e. a map from names of variables to values. A state sis naturally extended to \na map from arithmetic expres\u00adsions to N by s(e(x1 ,...,xn ))=e(s(x1 ),...,s(xn )) i.e. the s evaluation \nof an expression is the value obtained by evaluating all variables in the expression according to s. \n2. A random variable M is a partition (an equivalence relation) over O3. For a command M the equivalence \nrelation would iden\u00adtify all s which have the same observable output state for the command; i.e. s =M \nt iff M(s).Ob =M(t).Ob . Herewewill take as observable the output values of the variable L,i.e. Ob =L; \nfor example if M is the command L =H then s =M t iff s[ L=[[ H]]] .Ob =t[ L=[[ H]]] .Ob iff s[ L=[[ H]]] \n.L =t[ L=[[ H]]] .L . The nota\u00ad tion sx=[[ e]] means s where the variable x is evaluated to [[e]]. Hence \ns[ L=[[ H]]] .L =t[ L=[[ H]]] .L holds for any s,t which agree (have the same value) on the variable \nH. The probability distribution on a command random variable M is de.ned as \u00b5(M =t.)=St.O {\u00b5(t)|M(t).Ob \n=t. .Ob } If M is a non terminating program the de.nition of random vari\u00adable as an equivalence relation \nstill holds; now we will have an additional class which is all states which will be non ter\u00adminating; \nFor the probability distribution we extend the above de.nition with the clause: \u00b5(M =.)=St.O {\u00b5(t)|M(t)=.} \nInstantiating the above de.nition we get the following random variables associated to particular commands: \n M is the command x =e: this is the equivalence relation s =x= e t iff sx=[[ e]] .Ob =tx=[[ e]] .Ob . \n M is if ec else c :then s =ifec else c. t iff if s(e)=tt =.ff =t(e) then [[c]](s).Ob =[[c .]](t).Ob \nand s(e)=t(e)and t(e)=tt implies s =c t and t(e)=ff implies s =c. t.  s =c; c. t iff s .=c t implies \n[[c]](s)=c. [[c]](t).  Given a command M we will use the random variable 3 The conventional mathematical \nde.nition of a random variable is of a map from a probability space to a measurable space. In those terms \nwe are considering the kernel of such a map. Mn = M;...;M for the n-th iteration of M. This is a generalization \nof the sequen\u00adtial composition. For example s =(x=x+1)5 t iff s =x=x+5 t and \u00b5((x =x +1)5 =s)=S{\u00b5(t)|(x \n=x +5)(t).Ob =s .Ob } 3. Similarly we will have random variables corresponding to boolean expressions \n(we take as boolean values the integers 0,1); again an equivalence class is the set of states evaluated \nto the same (boolean) value: s =e t . s(e)=t(e) \u00b5(e =tt)=St.O {\u00b5(t)| t(e)=tt} for example for e1 ==e2 \ns =e1 ==e2 t . s(e1 )=s(e2 )=t(e1 )=t(e2 ) \u00b5((e1 ==e2 )=tt)=St.O {\u00b5(t)|t(e1 )=t(e2 )} Given an expression \ne guarding a command M we de.ne the random variable e n as e where the variables in e are evaluated following \nn - 1 iterations of M. For example if e is x >0, M is x =x +1 then e 3 is x +2 >0. e is hence an abbreviation \nfor 1 e . Following [2] and inspired by works by Dennings, McLean, Gray, Millen [7, 8, 18, 13, 19], interference \n(or leakage of con.\u00addential information) in a program M is de.ned as I(O;H|L) i.e. the conditional mutual \ninformation between the output and the high input of the program given knowledge of the low input. Notice: \n1. O is just another name for the random variable corresponding to the program seen as a command, i.e. \nO =M. 2. This is a input-output model i.e. it doesn t model an attacker who could have knowledge of \nsome intermediate state of the program. One implication of this model is that only global timing attacks \ncan in principle be analyzed. 3. For deterministic programs we have  I(O;H|L) = H(O|L)- H(O|H,L) = \nH(O|L)- H([[M]](H,L)|H,L) = H(O|L) i.e. interference becomes the uncertainty in the output of M given \nknowledge of the low input. A motivating result for this de.nition of leakage is that for deter\u00administic \nprograms I(O;H|L)=0 iff the program is non-interfering [4]. To see why H(O|L) is not enough for measuring \nleakage in non-deterministic setting, consider the following simple program: l =random(0,1)i.e. the output \nis 0 or equally likely 1. Since the output is independent from the inputs H(O|L)=H(O)and H(O)=1. So we \nwould conclude that there is 1 bit of leakage. This is clearly false as there is no secret information \nin the program. However I(O;H|L)=H(O|L)- H(O|H,L)=H(O)- H(O)=1 - 1 =0 Let s now investigate the quantity \nH(O|L). Consider for example the program M = l =3;if (l ==5)l =h else l =0 Here H(M|l)=H(M)because l \nis initialized in the program, hence there is no dependency from low inputs outside the program. Also, \nbecause the above program is equivalent to l =0 there is no leak\u00ad age of information, i.e. H(M)=0. Consider \nnow the program where l is not initialized, i.e. M = if (l ==5)l =h else l =0 Then H(M|l) will be the \nweighted sum of H(M|l =5) and H(M|l =.5); formally H(M|l) = \u00b5(l =5)H(if (l ==5)l =h else l =0|l =5) + \n\u00b5(l .=5)H(if (l ==5)l =h else l =0|l .=5) = \u00b5(l =5)H(h)+0 However if the attacker were to choose the \ninput l =5 then M = l =h and so H(M)=H(h). Hence by considering the non conditional entropy maxv.. H(O|L \n=v) we will get an upper bound on H(O|L). This will provide the leakage of the attack where the attacker \ncan choose the inputs (to maximize his gain). The other extreme is minv.. H(O|L =v) The case of the least \ndevastating attack. Hence instead of computing H(O|L) we will compute a non conditional entropy H(Ml=v \n) where v is a de.ned value for l. According to the cases such v will be calibrated to the power of the \nattacker. As no confusion arises we will drop the subscript and just write H(M). Finally notice that \nwe do not model an attacker able to choose high inputs, i.e. we are not modeling a spy trying to communicate \nwith an external accomplice but an intruder (e.g. a trojan horse or a dictionary attack on passwords). \n 3. Analysis of loops 3.1 Loops as disjoint union of functions 3.1.1 Entropy of disjoint union of functions \nThis subsection contains the technical backbone of the main de.\u00adnitions of the paper. Consider a function \nf :X . Y, which is the union of a family of functions (fi )i.I with disjoint domains (dfi )i.I , i.e. \nfor each i,dfi . X is the domain of fi and (dfi )i.I is a partition of X. We will note f by Si.I (fi \n|dfi )when we want to stress that f(x)=fi (x)for the unique i such that x . dfi . De.ne {[y]=f-1 (y)|y \n. Y}; clearly this is also a partition of X. De.ne the entropy of f as the entropy of its inverse images, \ni.e. H(\u00b5([y1 ]),...,\u00b5([yn ])). The aim now is to characterize the entropy of f. Assume that f is collision \nfree i.e. the family (fi )i.I has also disjoint codomains. In that case (dfi )i.I can also be seen as \na partition on the partition [y]=f-1 (y): dfi is the set of all [y]for y in the codomain of fi . Let \ns write [y1 ]j ,...,[ym ]j for the classes in dfj From now on to ease the notation we will often use \nevents instead of their probability when no confusion arise, for example in a computation [y]will stand \nfor \u00b5[y]the probability of the event P [y],i.e. {\u00b5(x)|x . [y]}. Similarly H([y1 ],...,[yn ])will stand \nfor H(\u00b5[y1 ],...,\u00b5[yn ])etc. By using the partition property from section 2.1 we have: PROPOSITION 1. \nFor a collision free function f: X j ]j [y1 ][ym H([y1 ],...,[yn ])=H(df1 ,...,dfn )+ dfj H(,..., ) \nj.I dfj dfj Let s now consider the case where f has collisions. Remember a collision is a y . Y in the \nimage of two different functions, i.e. [y]n dfj ..for i =j. In this case let s de.ne Y. =\u00d8 =[y]n dfi \n.as Y extended with enough new elements to eliminate collisions and let f. :X . Y. be the derived function \nwith no collisions, so f. is the union of the family of funtions (df. i )i.I with disjoint domain and \ncodomain. f. i is de.ned as 8 < fi (x),if .j .= ifi (x) .= fj (x) fi . (x)= : (fi (x),i) otherwise (So \n(fi (x),i) are the new elements added to Y) Let s de.ne Cf (Y) as the set of collisions of f in Y, and \nwrite xy1 ,...,xym for the elements of [y]. By using again the partition property we have: PROPOSITION \n2. X yy .. 1 H([y1 ],...,[yn ]) = H([y1 ],...,[yn. ]) - [y]H( x,..., xm ) [y][y] y.Cf (Y) This means \nthat the entropy of a function de.ned as a union of functions with disjoint domains is given by the entropy \nof the derived function with no collisions minus the weighted sum of the entropies of the collisions. \nTo ease the notation we can rewrite Proposition 2 as. P H(f)= H(f.) - H(Cf (Y)) Let s call disambiguation \nof f the function f. . Notice that Proposition 2 implies that the the entropy of f is a lower bound on \nthe entropy of the disambiguation of f. As an example let s consider the function f = f1 .f2 .f3 de.ned \nby f1 (x1 )= y1 ,f1 (x2 )= y2 = f2 (x3 ),f2 (x4 )= y4 , f3 (x5 )= y5 = f3 (x6 ) and assume uniform distribution \non the inputs. f has one col\u00adlision y2 so to compute H(f) we .rst extend the codomain with a new element \ny . 2 so to have f. 1 (x2 )= y2 ,f2 . (x3 )= y . 2 Computing H(f) using proposition 2 gives: P H(f) = \nH(f.) - H(Cf (Y)) 1 11 1 = H( 1 ,, )+ 2 1 H( 1 , )+ 1 H(1,0) -1 H( 1 , ) 333 322 3 322 = 1.585 + 23 + \n0 -13 = 1.918  3.1.2 Entropy of loops Let while e M be a terminating loop. From a denotational point \nof view we can see it as a map F = S1=i=n Fi where n is an upper bound on the number of iteration of \nthe loop and all Fi have disjoint domain: each Fi is the map which iterates Mi times under the condition \nthat the guard has been true up to that moment and it will be false after the i -th iteration of M. The \ndomain of Fi is hence given by all states s such that Fj (s)(e)= tt,0 =j =i and Fi+1 (s)(e)= ff. Formally \ni <i> while e M = S0=i=n (M|e ) where 8 < e = ff,if i = 0 <i> 2 e = e = tt .e = ff,if i = 1 : ii+1 e \n= tt,...,e = tt .e = ff, if i >1 and M0 = skip. Notice 1. e <i> are events and not random variables \n2. the assumption that n is an upper bound on the number of iterations of the loop implies  S0=i=n \u00b5(e \n<i>)= 1 <0><n> 3. the events e ,...,e constitute a partition of the set of states: given any initial \nstate s the loop will terminate in <n iterations; exactly one of the e <i> must be true for s i.e. s \n.e <i>,e.g. for i >1 s(e)= tt .\u00b7\u00b7\u00b7.Mi (s)(e)= tt .Mi+1 (s)(e)= ff To prove that this is a partition \nsuppose it isn t, i.e. s .e <i> ne <i+j>;then Mi+1 s(e)= ff because of e <i> and Mi+1 s(e)= tt because \nof e <i+j>: a contradiction, hence the e <i> are disjoint sets, i.e. a partition. By applying proposition \n1,2 for a while we have: PROPOSITION 3. For a collision free loop while e M bounded by n iterations \n<0><n> H(while e M)= H(\u00b5(e ),...,\u00b5(e ))+ <i><i> S1=i=n \u00b5(e )H(Mi |e ) In the case of a loop with collisions, \nfollowing proposition 2 equality is achieved as follows: .<0>.<n> H(while e M) = H(\u00b5(e ),...,\u00b5(e ))+ \n.<i>.<i> S1=i=n \u00b5(e )H(M.i |e )- P ts ts [s]H( 1 ,..., m ) s.Cwhile e M (O)[s][s] Notice that the disambiguation \nof a collisions free loops is the loop itself. This entails: PROPOSITION 4. For a command while e M bounded \nby n itera\u00adtions .<0>.<n> H(while e M) = H(\u00b5(e ),...,\u00b5(e ))+ .<i>.<i> S1=i=n \u00b5(e )H(M.i |e ) with equality \niff the loop is collision free. Collisions do not present a conceptual change in the framework but add \nsome computational burden; also collisions are not very frequent in loops; for a collision in a loop \nto arise two different iteration of the loop should give the same values for all read and written low \nvariables in the loop and the guard should be false on these values. For example all loops using a counter, \na variable taking a different value at each iteration don t contain collisions. For these reason from \nnow on we will concentrate on collision free loops.  3.2 Basic de.nitions De.ne <0><n><i> W(e,M)n = \nH(\u00b5(e ),...,\u00b5(e ),1 -S0=i=n \u00b5(e ))+ P <i><i> \u00b5(e )H(Mi |e ) 1=i=n as the leakage of while e M up to n \niterations. PROPOSITION 5. .n =0, W(e,M)n =W(e,M)n+1 Proof: we only need to prove <0><n><i> H(\u00b5(e ),...,\u00b5(e \n),1 -S0=i=n \u00b5(e ))= <0><n><n+1> <i> H(\u00b5(e ),...,\u00b5(e ),\u00b5(e ),1 -S0=i=n+1 \u00b5(e )) which can be rewritten \nas H(p1 ,...,pn ,qn+1 + pn+1 ) =H(p1 ,...,pn ,pn+1 ,qn+1 ) the inequality then follows from H(p1 ,...,pn \n,pn+1 ,qn+1 ) = H(p1 ,...,pn ,pn+1 + qn+1 )+ n+1n+1 (pn+1 + qn+1 )H( p, q) p pn+1 +qn+1 +qn+1 n+1 The \nleakage of while e M is de.ned as limn.8W(e,M)n (1) In the case of a loop with collisions the de.nition \nis modi.ed in the obvious way: limn.8W.(e,M)n - X H(C(W.(e,M))) (2) i.e. we .rst compute the leakage \nin the disambiguation of the loop and then we subtract the weighted entropies of the collisions The rate \nof leakage is W(e,M)n limn.8,\u00b5(e<n>). =0 n Hence in the case of terminating loops the rate will be the \ntotal leakage divided by the number of iterations. This can be considered a rough measure of rate: for \nexample if the .rst iteration were to leak all secret and the following billion nothing the rate would \nstill be one billionth of the secret size. However as in our model the attacker can only perform observations \non the output and not on intermediate states of the program the chosen de.nition of rate will give indication \nof the timing behavior of the channel in that context. A fundamental concept in information theory is \nchannel capac\u00adity, i.e. the maximum amount of leakage over all possible input distributions, i.e. max\u00b5 \nlimn.8W(e,M)n (3) In our setting we will look for the distribution which will maximize leakage. Informally \nsuch a distribution will provide the setting for the most devastating attack: we will refer to this as \nthe channel distribution. Also we will use the term channel rate for the rate of leakage of the channel \ndistribution. Again this should be thought of as the average maximal amount of leakage per iteration. \nTo de.ne rate and channel capacity on the case of collisions the above de.nitions should be applied on \nthe de.nition of leakage for loops with collisions. 3.2.1 Leakage vs security Consider a simple assignment \nl =h where the variables are k-bit variables. We know that the assignment transfer all information from \nh to l, so we would be tempted to say that the leakage is k. That is not correct. Suppose h is a 3-bit \nvariable (so possible values are 0 ...7 ) and suppose the attacker knows h is even (so the possible values \nare 0,2,4,6). The uncertainty on h before 111 executing l =h is hence H(1 ,,, )=2. The leakage is not \n3 4444 but 1111 H(l =h)=H( ,,, )=2 4444 i.e. the information of h.The security of the program is the \ndiffer\u00adence between the uncertainty before execution and the leakage (the uncertainty after execution). \nHence security of the previous exam\u00adple of l =h is 2-2=0. Notice that when the program reveal every\u00adthing \nthis notion is invariant w.r.t. the chosen distribution, i.e. while the leakage of l =h will depend on \nthe distribution, its security will always be 0, all that can be revealed is revealed. Formally security \nis de.ned [2] as Sec(O)=H(H|L)- H(O|L)=H(H|L,O) The last equality is proven as follows: H(H|L,O) = H(H,L,O)- \nH(L,O) = H(H,L)- H(L,O) = H(H,L)- H(L)- H(L,O)+H(L) = H(H|L)- H(O|L) Using arguments similar to the ones \npresented at the end of section 2.2 most of the times we will consider the simpli.ed version where there \nare no dependencies on L,i.e. H(H)- H(O). In fact H(H|L)can be reduced to H(H)when (as it is normally \nthe case) the secret is independent of the public input. H(O|L) Another notion we will use is the leakage \nratio i.e. the H(H|L) amount leaked divided by the maximum amount leakable. This is a number in the \ninterval [0,1]which measures the percentage of the secret leaked by the program, so the ratio has minimum \n0 iff the leakage is 0 and maximum 1 iff all the secret is revealed by the program.  3.3 Classi.cation \nof looping channels The following classi.cation combines the previous de.nitions with variations in the \nsize of the secret. For example a bounded loop is one where even if we were able to increase arbitrarily \nthe size of the secret we would not be able to increase arbitrarily the amount leaked. For the purposes \nof this investigation loops are classi.ed as: a C-bounded if the leakage is upper bounded by a constant \nC. b Bounded if the leakage is C-bounded independently of the size (i.e. number of bits) of the secret. \nIt is unbounded otherwise. c Stationary or constant rate if the rate is asymptotically constant in the \nsize of the high input. d Increasing (resp decreasing) if the rate is asymptotically increas\u00ading (resp \ndecreasing) as a function of the size of the high input. e Mixed if the rate is not stationary, decreasing \nor increasing. Clearly all loops are C-bounded by the size of the secret and by the channel capacity; \nthe interesting thing is to determine better bounds. For example if we are studying a loop where we know \nthe input distribution has a speci.c property we may found better bounds than the size of the secret. \nFrom a security analysis point of view the most interesting case is the one of unbounded covert channels, \ni.e. loops releasing all secret by indirect .ows. Notice that a guard cannot leak more than 1 bit so \nthe rate of a covert channel cannot exceed the number of guards in the command. Notice also that the \nrate of leakage is loosely related to timing behaviour. In loops with decreasing rate if the size of \nthe secret is doubled each iteration will (on average) reveal less information than each iteration with \nthe original size. We will spell out the timing content of rates in some of the case studies.  4. Case \nstudies We will now use the previous de.nition. The aim is to show that the de.nitions make sense and \nthe derived classi.cation of channels helps in deciding when a loop is a threat to the security of a \nprogram and when is not. The programs studied are simple examples of common loops: linear, bounded and \nbitwise search, parity check etc. Most of the arguments will use a separation property of the def\u00adinition \nof leakage: in fact De.nition 1 neatly separates the informa\u00adtion .ows in the guard and body of a loop, \nso if there is no leakage in the body (e.g. no high variable appears in the body of the loop) (1) becomes \n<0><n><i> limn.8{H(\u00b5(e ),...,\u00b5(e ),1 - S0=i=n \u00b5(e ))} (4) On the other side if there is no indirect \n.ow from the guard (e.g. e doesn t contain any variable affected by high variables) then (1) becomes \n<i>i <i> limn.8S1=i=n \u00b5(e )H(M|e ) (5) Unless otherwise stated we are assuming uniform distribution \nfor all input random variables (i.e. all input values are equally likely). Also to simplify notations \nwe will consider that a k-bit variable assume values 0....,2k - 1 (i.e. no negative numbers). Table 1. \nSummary of analysis for loops; loop i is the loop presented in section 4.i of the paper loop 1 loop 2 \nloop 3 loop 4 loop 4a loop 5 loop 6 loop 7 Bound 8 4.3219 1 16 8 0 log(C) 8 Channel Rate . = = = = = \n. = Capacity k 4.3219 1 16 k 0 log(C) k 2 Channel leakage ratio 1 = 4.3219 k = 1 k = 16 k 1 0 = log(C) \nk = 1 2 A summary of this section results is shown in table 14. 4.1 An unbounded covert channel with \ndecreasing rate Consider l=0; while (!(l=h)) l=l+1; Under uniform distribution max W(e,M)n is achieved \nby X <0><2k -1><i>i <i> H(\u00b5(e ),...,\u00b5(e ))+ \u00b5(e )H(M|e ) 0=i=2k -1 Notice that no high variable appears \nin the body, so there is no leakage in the body, i.e X <i>i <i> \u00b5(e )H(M|e )=0 0=i=2k -1 We hence only \nneed to study <0><2k -1> H(\u00b5(e ),...,\u00b5(e )) notice now that 8 < 0 =h,if i =0 <i> e = : 0 ..=h, if i >0 \n=h,...,i =h .i +1 hence \u00b5(e <i>)= 21k . This means 11 <0><2k -1>k H(\u00b5(e ),...,\u00b5(e ))=H( ,..., )=log(2)=k \n2k 2k As expected all k-bit of a variable are leaked in this loop, for all possible k; however to reveal \nk bits 2k iterations are required. We conclude that this is an unbounded covert channel with decreas\u00ading \nrate 2kk . To attach a concrete timing meaning to this rate let t1 ,t2 be the time (in milliseconds) \ntaken by the system to evalu\u00adate the expression !(l =h)and to execute the command l =l +1 respectively. \nThen the above program leaks 2kk bits per t1 +t2 mil\u00adliseconds. Notice that uniform distribution maximizes \nleakage, i.e. it achieves channel capacity. Consider for example the following input distribution for \na 3-bit variable: 71 \u00b5(0)= ,\u00b5(1)=\u00b5(2)\u00b7\u00b7\u00b7=\u00b5(7)= 8 56 In this case the attacker knows, before the run of \nthe program, that 0 is much more likely than any other number to be the secret, so the amount of information \nrevealed by running the program is below 3 bits (below capacity). In fact we have 71 1 H( , ,..., )=0.8944838 \n8 56 56 Notice however that whatever the distribution the security of this program is 0 and leakage ratio \n1.  4.2 A bounded covert channel with constant rate l =20; while(h <l){l =l -1}After executing the program \nl will be 20 if h =20 and will be hif0 =h <20 i.e. h will be revealed if it is in the interval 0..19. \nThe random variables of interest are: Mn =l =20 -n The events associated to the guard are: 8 > h <20 \n-n .h =20 -(n +1) = < <n> h = 20-(n+1), n>0 e = > : h =20, n=0 and 8 2k -20 > if n =0 > 2k > < 1 \u00b5(e \n<n>)= 2k if 0 <n =20 > > > : 0ifn >20 Again since the body of the loop doesn t contain any high variable \n<i>i <i> S1=i=n \u00b5(e )H(M|e )=0 The leakage is hence given by <1><n> H(\u00b5(e ),...,\u00b5(e )) = H(2k -201 1 \n 2k , 2k ,..., 2k ,0,...,0) = -2k -20 log(2k -20 )-20(1 log(1 )) 2k 2k 2k2k This function is plotted \nin .gure 1 for k =6 ...16. The inter\u00adesting thing in the graph is how it shows that for k around 6 bits \nthe program is unsafe (more than 2.2 bits of leakage) whereas for k from 14 upwards the program is safe \n(around 0 bits of leakage). We conclude that this is a bounded covert channel with decreas\u00ading rate. \nHowever uniform distribution is not the channel distribution. The capacity of this channel is 4.321928 \nand is achieved by the distribution where the only values with non zero probability for h are in the \nrange 0 ...19 and have uniform distribution5. Notice that the channel distribution ignores values of \nh higher than 20, so the channel rate is constant 4.321928 =0.2160. 20  4.3 A 1-bounded channel with \nconstant rate Consider the following program h=BigFile i=0; l=0; while (i<N) {  4 In the Channel leakage \nratio row in the table quantities greater than 1 5 We are ignoring the case where k < 5 where the capacity \nis less than should be ignored. 4.321928 Figure 1. leakage in l=20; while(h< l) {l=l-1} l= Xor(h[i],l); \ni=i+1; } This program take a large con.dential .le and performs a parity check, i.e. write in l the Xor \nof the .rst N bits of the .le. The n-ary Xor function returns 1 if its argument has an odd number of \n1s and 0 otherwise. This is a yes/no answer so its entropy has maximum 1 which is achieved by uniform \ndistribution. Hence n <n> H(M|e )=H(h[0] . ... . h[n - 1])=1 Notice that e <n> = n <N . n +1 = N henceforth \n<i><N-1> \u00b5(e .e )=1 )=0ifi =N - 1 and \u00b5( We deduce the leakage is: H(\u00b5(e <0>),...,\u00b5(e <n>))+S1=i=n \u00b5(e \n<i>)H(Mi |e <i>) = <N><N> 0 +\u00b5(e )H(MN |e )=1 This is a 1-bounded channel with constant rate and capacity \n1. Notice however that if the number of iterations were a function of the secret size, for example by \ninserting in the second line of the program the assignment N =size(h),(where size(h)=k the size of the \nsecret) then it becomes a 1 bounded channel with decreasing rate 1k and capacity 1. Again there are distributions \nwhich do not achieve channel capacity, for example one where values of h with odd number of bits equal \nto 1 are less likely than other values.  4.4 A 16-bounded stationary channel Consider the program intc \n=16, low = 0; while (c >= 0) { int m = (int)Math.pow(2,c); if (high >= m) { low = low +m; high = high \n-m; } c=c-1; } System.out.println(low); Here the guard of the loop doesn t contain variables affected \nby high, hence we only need to use formula 5 where M is int m = (int)Math.pow(2,c); if (high >= m) { \nlow =low + m; high = high -m; } c=c-1; To compute H(Mn )notice that the n-th iteration of M test the \nn-th bit of high,i.e. high >=m is true at the n-th iteration iff the n-th bit of high is 1(this is because \nm =216-n ) and copies that bit into low The variables of interests are: Mn = low =nBits(high) e <n> =16 \n- n = 0 . 16 - (n +1)<0 8 < 1if n =16 \u00b5(e <n>)= : 0otherwise Because of this the leakage of the guard \nis 0 and for the total leakage we only need to compute H(M16 |e <16>)=16. This mean that the rate is \n1. This is hence an example of a 16-bounded stationary channel. However if we were to replace the .rst \nassignment int c= 16 with c = size(l) i.e. int c = size(l), low = 0; while (c >= 0) { int m = (int)Math.pow(2,c); \nif (high >= m) { low =low + m; high = high -m; } c=c-1; } System.out.println(low); then we would have \nan unbounded stationary channel (assuming that h,l be of the same size) with constant channel rate 1. \nAgain channel capacity is achieved by uniform distribution. For example a distribution where we already \nknow few bits of high will not achieve channel capacity, 4.5 A never terminating loop while (0== 0) \nlow = high; Here \u00b5(e <i>)=0 for all i, hence for all n the formula <0><n><i> W(e,M)n = H(\u00b5(e ),...,\u00b5(e \n),1 - S0=i=n \u00b5(e ))+ <i><i> S1=i=n \u00b5(e )H(Mi |e ) becomes i <i> H(0,...,0,1)+S1=i=n 0H(M|e )=0 from \nwhich we conclude that the leakage, rate and capacity are all 0. The reason why the program is secure \neven if the whole secret is assigned to a low variable is that only observations on .nal states of the \ncommand are allowed (none in this case because of non Figure 2. leakage for program in section 4.6 termination); \nagain this is feature of our model where the observer cannot see intermediate values of the computation, \nin which case this program would leak everything.  4.6 A may terminating loop l=0; flag=tt; while (flag \nor l<h) { if (h<= C) flag=ff; l=l+1; } This loop will terminate if h = C and in that case l =h. The event \ne <i> corresponds to i =h . h = C, hence 1C 1 \u00b5(e <i>)= = if i = C C2k 2k Notice that as the information \nh = C is known by knowing e <i> we conclude that for all i, H(Mi |e <i>)=0. The leakage of this channel \n(under uniform distribution) is hence 1 2k -C - 2k -C H(21k ,..., )= Ck log(2k -C ) 2k , 2k 2k2k 2k This \nfunction is similar to the one from section 4.2. Again channel capacity is achieved not by the uniform \ndistribution but from the one where the .rst C values have probability C1 :in that case the program reveal \nall the secret. Figure 2 shows the leakage for k between 10 and 20 and C between 400 and 500 under uniform \ndistribution.  4.7 Probabilistic operators When de.ning leakage in section 2.2 it was shown that the \ncon\u00additional entropy H(O|L)would overestimate leakage for a program like l =random(0,1) where random(0,1)a \nprobabilistic operator returning 0 with prob\u00adability p and 1 with probability 1 - p. However we could \ninterpret l =random(0,1)as the program l =x where x is an unknown input variable taking value 0 with \nprobability p and 1 with probability 1 - p. Then computing H(O|L,X)gives H(O|L,X)=H(O|X)=0, all uncertainty \nin the out\u00adput comes from the random x so it can be eliminated by condi\u00adtioning on it. This suggests \nthat an analysis of probabilistic programs can be developed by introducing a new random variable to cater \nfor the probabilistic operator; the leakage formula becomes H(O|L,X); the effect of this formula is to \nsubtract from the uncertainty in the output the uncertainty coming from the low input and from the probabilistic \noperator; i.e. the uncertainty in H(O|L,X)comes from the secret. As usual we can simplify the formula \nto H(O|X)by hardwiring the low inputs into the probability distribution as shown at the end of Section \n2.2. In the cases of loops using a probabilistic operator we take X as a stream of bits; the i-th bit \nin the stream is the i-th outcome of the operator. We can compute the leakage of probabilistic programs \nby using the de.nition of conditional entropy X H(O|X)= \u00b5(X =xi )H(O|X =xi ) As an example consider the \nprogram P int i=0; low = 0; while (i< size(high)) { if (Coin[i]==0 ) low[i] = high[i]; i=i+1; } System.out.println(low); \n where Coin is a stream of unknown bits such that Coin[i]=0 with probability pi . Then at the end of \nthe program the i - th bit of high will be copied in low with probability pi . To compute the leakage \nof the program , i.e. H(P|Coin) we proceed as follow: 1. Compute, using formula 1, the entropies H(Ps1 \n),...,H(Psn ) where H(Psi )is the above program where the vector Coin is instantiated to a speci.c sequence \nsi . P 2. Compute \u00b5(si )H(Psi )=H(P|Coin). Given a stream si and high a k-bit variable, the bits of \nhigh copied in low are those corresponding to the positions in s1 with value 0. For example if high is \na 4-bit variable and si =1001 ... then low will be the sequence 0h[1]h[2]0. The leakage of H(Psi )= number \nof 0s in si For example if we assume high,Coin are uniformly dis\u00adtributed, i.e. any bit in high,Coin \nhas 1/2 chance of being 0 or 1 and high is a 4-bit variable then there will be 4 sequences with 1 zero, \n6 with 2 zeros, 4 with 3 zeros and 1 with 4 zeros (the general formula is k! where i is the number of \nzeros) . The leakage (k-i)!i! will hence be 46 4113 + 2 + 3 + 4 =+=2 1616 16 16 22 the general formula \nbeing X X k! 1k! 11 k i = i = 2k-i 1=i=k1=i=k (k - i)!i! 2k (k - i)!i! 2i 2 This is hence an unbounded \nchannel leaking k2 bits with rate 12 . Notice that in the presence of probabilistic operators all de.ni\u00adtions \nintroduced, leakage, rate, channel, leakage ratio have an ad\u00additional parameter, i.e. the distribution \non this unknown input. The leakage for the above program given Coin[i]=0 with probabil\u00adity p is pk. This \nis obtained by the expected value of the binomial distribution: X k! ipi (1 -p)k-i =pk (k -i)!i! 1=i=k \nFor example by changing the distribution in Coin such that for all i, Coin[i]=0 with probability 1 the \nabove program become the unbounded stationary channel studied in section 4.4 whereas if for all i, Coin[i]=0 \nwith probability 0 the above program become secure. The analysis of probabilistic programs should hence \nreturn a number if the probabilities of the probabilistic operator are known and a distribution when \nthe probabilities are unknown.  5. Justifying entropy as a measure of leakage We now address the questions: \nhow is leakage as de.ned in this work related to computer security? A basic result proved in [4] is that \nfor a terminating determin\u00adistic program the leakage is 0 if and only if the program is non interfering. \nSimilar results had been previously proved in differ\u00adent contexts by Millen[19] and Gray [13]. The idea \nis to see a non interfering program as a function F(h, l)(its denotational se\u00admantics) which is constant \non the h component, i.e. for all h ., =h. F(h, l)=F(h. , l). Let s now consider H(F|l): because F is \ncon\u00adstant on the h component there will be no uncertainty on F if we know l, hence H(F|l)=0; on the other \nside any denotation of a program which satisfy H(F|l)=0 has to be constant on the h com\u00adponent so has \nto denote a non interfering program. Let s now address the remaining part of this section s question: \nif the leakage is n > 0 what does that mean? The idea here is that n is a lower bound on the effort of \nthe attacker in guessing the secret given observations on the output of the program. In the following \nwe will use work from Massey [17], Malone and Sullivan[16]. The following argument extends one from [2]. \nSuppose the attacker has available a distribution p =(pi )i.I for the secret. He can then mount a dictionary \nattack i.e. he will try to guess the secret starting from the most likely guess and so on. P The expected \nnumber of guesses is then G(p)= i ipi . In case of the uniform distribution G(p)= n+1 . This inspires \nthe information 2 theoretic de.nition +1 HG (p)= 2H(p2 ) In the setting of the present work, p is the \ndistribution after observing the program, and so H(p)is the uncertainty of the secret after running the \nprogram, i.e. Sec(M), the security of the program as de.ned in section 3.2.1. Massey has shown that 0.7HG \n(p)=G(p)(his precise bound is G(p)/HG (p)=2/e). This supports the view that security provides a lower \nbound on the average effort required to guess the secret using a dictionary attack. Another possible \nyet less realistic scenario of attack is where the attacker may guess sets of values and been told if \nthe secret is in that set. In this case the connection with entropy is even stronger as the average number \nof guesses becomes H(p) (again this is Sec(M)). In the case of the dictionary attack, how good is the \nlower bound 0.7HG (p)=G(p)? In an experimental study [16] one million ran\u00addom distributions for a set \nbetween 2 and 20 values were generated. These experiments show that the following relation holds: 0.7HG \n(p)=G(p)=HG (p) This suggest that in normal situations the bound is very tight. Massey however has shown \nthat there are distributions for which the inequality G(p)=HG (p) doesn t hold; an example is the distribution \np1 =1 -b/n, p2 =\u00b7\u00b7\u00b7=pn =b/(n 2 -n).For n .8we have G(p)tends to 1 +b/2 while H(p)tends to 1. Be\u00adcause \nb is arbitrary we conclude that G(p)can be arbitrary larger than H(p).  6. Conclusion and further work \nThis paper has given the .rst precise, information-theoretic account of the constructs in a Turing-complete \nprogramming language. The central point is our information theoretical semantics of leakage in loops. \nThe theory consists of several notions: absolute leakage, rate of leakage, channel capacity, and leakage \nratio. We have given a classi.cation of loops with the aim to determine which loop presents a security \nthreat, and then presented several case studies in an attempt to show that the de.nitions and classi.cation \nare useful in individuating security threats and are natural. We believe that the ideas in this paper \ncould provide a spring\u00adboard for further applications of information theory in security and programming \nlanguages. Some immediate directions for investiga\u00adtion are the following. 1. STATIC ANALYSIS. This work \ncould pave the way for more powerful static analyses based on information theory. As the case studies \nshow the analysis requires some ingenuity, for ex\u00adample to determine which events the e <i> represents. \nThis rea\u00adsoning usually involves the ability to detect interaction between several random variables. \nIt may be possible that by combining techniques from theorem proving, model checking and quanti\u00adtative \nstatic analysis like [4, 3] some reasonable static analysis may be built. The central point, though, \nis that with a precise semantics of loops in place, we have a reference semantics that potential abstract \ndomains should over-approximate, in which case loops could be soundly analyzed via .xed-point iteration. \n 2. TIMING ATTACKS. As already noted, there is some informa\u00adtion about timing in the notion of rate of \nleakage, rate being an indication of the average time needed to release some informa\u00adtion; for example \na low rate suggests little amount of secret is released in each iteration, a decreasing rate indicates \nthat the channel take longer to transmit information as the size of the secret increases. However many \ntiming attacks are not covered in our current model, for example those whose study requires intermediate \nstates of execution to be observable; hence more work is required to address important issues in timing \nattacks. 3. CONCURRENCY, NON DETERMINISM. Integrating this work with a concurrency framework could open \nthe way to the anal\u00adysis of interesting protocols. 4. SEPARATION LOGIC. O Hearn, Reynolds and Isthiaq[14, \n21] have introduced a logic to reason about heaps based on some sort of non-interference between different \nparts of the code. Quanti.ed interference may suggest a weaker separation logic which could be interesting \nto explore.  6.1 Acknowledgments I m very grateful to Fabrizio Smeraldi, Peter O Hearn and Sebas\u00adtian \nHunt for very useful comments on this work. This research was supported by the EPSRC grant EP/C009967/1 \nQuantitative In\u00adformation Flow.  References [1] D.Bell and L. LaPadula, Secure computer systems: Uni.ed \nexposition and Multics interpretation , Technical Report MTR-2997, MITRE Corp, 1997. [2] D. Clark and \nS. Hunt and P. Malacaria Quantitative Analysis of the Leakage of Con.dential Data Electronic Notes in \nTheoretical Computer Science volume 59, issue 3, Elsevier, 2002 [3] D. Clark and S. Hunt and P. Malacaria, \nQuanti.ed Interference for a While Language , Elsevier, Electronic Notes in Theoretical Computer Science \n112, pages 149 166, 2005. [4] D. Clark and S. Hunt and P. Malacaria, Quantitative Information Flow, \nRelations and Polymorphic Types , Journal of Logic and Computation, Special Issue on Lambda-calculus, \ntype theory and natural language, 2005, volume 18, number 2, pages 181 199 [5] M. R. Clarkson and A. \nC. Myers and F. B. Schneider, Belief in In\u00adformation Flow , Proc. 18th IEEE Computer Security Foundations \nWorkshop (CSFW 18), IEEE Computer Society Press, 2005. [6] T. M. Cover and J. A. Thomas, Elements of \nInformation Theory , 1991, Wiley Interscience. [7] D. E. R. Denning, A Lattice Model of Secure Information \nFlow , Communications of the ACM, volume 19, number 5, May 1976. [8] D. E. R. Denning, Cryptography and \nData Security , 1982, Addison-Wesley. [9] A. Di Pierro and C. Hankin and H. Wiklicky, Probabilistic \ncon.nement in a declarative framework , Electronic Notes in Theoretical Computer Science, volume 48, \nElsevier 2001. [10] A. Di Pierro and C. Hankin and H. Wiklicky, Quantitative static analysis of distributed \nsystems , Journal of Functional Programming,2005. [11] J. Goguen and J. Meseguer, Security Policies and \nSecurity Models , IEEE Symposium on Security and Privacy, pages 11 20, IEEE Computer Society Press, 1982. \n[12] J. W. Gray III and P. F. Syverson, A Logical Approach to Multilevel Security of Probabilistic Systems \n, Distributed Computing, volume 11, number 2, 1998, pages 73-90. [13] W. Gray, III, James, Toward a Mathematical \nFoundation for Information Flow Security , Proc. 1991 IEEE Symposium on Security and Privacy, Oakland, \nCA, May1991, pages 21 34. [14] S. Isthiaq and P.W. O Hearn, BI as an assertion language for mutable data \nstructures , pages = 14 26 , 28th POPL London 2001. [15] G. Lowe, Quantifying Information Flow , Proceedings \nof the Workshop on Automated Veri.cation of Critical Systems, 2001. [16] D.Malone and W. Sullivan, Guesswork \nand entropy , IEEE Transactions on Information Theory, volume 50, number 3, March 2004. [17] J. L. Massey, \n Guessing and entropy , Proc. IEEE International Symposium on Information Theory, 1994, Trondheim, Norway. \n[18] J. McLean, Security models and information .ow , Proceedings of the 1990 IEEE Symposium on Security \nand Privacy, 1990, Oakland, California. [19] J. Millen, Covert channel capacity , Proc. 1987 IEEE Symposium \non Research in Security and Privacy, IEEE Computer Society Press, 1987. [20] J. C. Reynolds, Syntactic \ncontrol of interference , Conf. Record 5th ACM Symp. on Principles of Programming Languages 1978. [21] \nJ. Reynolds, Separation logic: a logic for shared mutable data structures , Invited Paper, LICS 02, 2002. \n[22] P. Y. A. Ryan and J. McLean and J. Millen and V. Gilgor, Non\u00adinterference, who needs it? , Proceedings \nof the 14th IEEE Security Foundations Workshop, Cape Breton, Nova Scotia, Canada, June 2001, [23] C. \nShannon, A mathematical theory of communication , The Bell System Technical Journal, volume 27, July \nand October, 1948, pages 379 423 and 623 656. [24] D. Volpano and G. Smith, A Type-Based Approach to \nProgram Security , Proceedings of TAPSOFT 97 (Colloquium on Formal Approaches in Software Engineering), \nApril 1997, Lecture Notes in Computer Science, number 1214, pages 607 621. [25] D. G. Weber, Quantitative \nHookup security for covert channel analysis , Proceedings of the 1988 Workshop on the Foundations of \nComputer Security, 1988, Fanconia, New Hampshire, U.S.A.. [26] G. Winskel, The formal semantics of programming \nlanguages: an introduction , MIT Press 1993. [27] T. Wittbold, Network of Covert Channels , Proceedings \nof the 1990 Workshop on the Foundations of Computer Security, 1990.  \n\t\t\t", "proc_id": "1190216", "abstract": "There is a clear intuitive connection between the notion of leakage of information in a program and concepts from information theory. This intuition has not been satisfactorily pinned down, until now. In particular, previous information-theoretic models of programs are imprecise, due to their overly conservative treatment of looping constructs. In this paper we provide the first precise information-theoretic semantics of looping constructs. Our semantics describes both the amount and rate of leakage; if either is small enough, then a program might be deemed \"secure\". Using the semantics we provide an investigation and classification of bounded and unbounded covert channels.", "authors": [{"name": "Pasquale Malacaria", "author_profile_id": "81100509296", "affiliation": "Queen Mary, University of London", "person_id": "PP31058357", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1190216.1190251", "year": "2007", "article_id": "1190251", "conference": "POPL", "title": "Assessing security threats of looping constructs", "url": "http://dl.acm.org/citation.cfm?id=1190251"}