{"article_publication_date": "01-17-2007", "fulltext": "\n Logic-Flow Analysis of Higher-Order Programs Matthew Might Georgia Institute of Technology mattm@cc.gatech.edu \nAbstract This work presents a framework for fusing .ow analysis and the\u00adorem proving called logic-.ow \nanalysis (LFA). The framework it\u00adself is the reduced product of two abstract interpretations: (1) an \nabstract state machine and (2) a set of propositions in a restricted .rst-order logic. The motivating \napplication for LFA is the safe re\u00admoval of implicit array-bounds checks without type information, user \ninteraction or program annotation. LFA achieves this by dele\u00adgating a given task to either the prover \nor the .ow analysis depend\u00ading on which is best suited to discharge it. Described within are a concrete \nsemantics for continuation-passing style; a restricted, .rst\u00adorder logic; a woven product of two abstract \ninterpretations; proofs of correctness; and a worked example. Categories and Subject Descriptors D.3.4 \n[Programming Lan\u00adguages]: Processors Optimization General Terms Languages Keywords logic-.ow analysis, \nLFA, static analysis, environment analysis, lambda calculus, CPS, abstract garbage collection, ab\u00adstract \ncounting, Gamma-CFA .rst-order logic, theorem proving 1. The idea The main idea is really the product \nof two ideas: 1. Theorem prover as oracle to higher-order .ow analysis. 2. Higher-order .ow analysis \nas oracle to theorem prover.  The objective of this fusion is to continue pushing beyond the limitations \nof the k-CFA framework [17]. The key to this weaving is delegation: the tool best suited for an obligation \ndischarges it. For instance, the theorem prover avoids obligations where it might have to induct, e.g., \nthe introduction of a universal quanti.er, since that may require user interaction. To ac\u00adcomplish these \ntasks, the .ow analysis is out.tted with specialized abstract counting machinery [11]. Meanwhile, obligations \nthat ex\u00adceed the capabilities of the .ow analysis, such as reasoning about abstract constraints or canonicalization, \ngo to the prover. We call the threaded framework logic-.ow analysis (LFA). For robustness, LFA is engineered \nso that theorem prover failure is not catastrophic. As the power of the theorem prover decreases, LFA \ns result gracefully degrades toward a GCFA-level .ow analy\u00adsis [11]. Motication:ArrayaccesssafetyProvingthesafety \nivatingapplof indexing into an array serves as a motivating application. Some\u00adtimes, safety is syntactically \nobvious, as in: for (int i = 0; i < a.length; i++) println( a[i] ); But, in other cases, the access \ndoesn t occur within the scope of an explicit check: for (int i = 0; i < a.length; i++) foo( i ); // \nfoo touches a[i]. Complicating matters, the function foo might even touch the array a through an alias. \nSorting out such issues with suf.cient precision can quickly overwhelm existing analyses. At POPL 2006, \nTim Sweeney pointed this out when he issued a challenge to develop a robust analysis for the safety of \nvertex arrays. Vertex arrays are a technique frequently used in graphics programming and demonstrated \nby the following fragment: float[][3] vertices = .vector of points.; int[] mesh = .indices into vertices.; \n for (int i = 0; i < mesh.length; i++) // Safe if 0 = mesh[i] < vertices.length. emitv( vertices[mesh[i]] \n); The safety of the access to vertices depends upon the manner in which mesh is built and modi.ed. Furthermore, \nat certain points during its lifetime, mesh will only partially satisfy the invariants re\u00adquired to prove \nthe safety of the subsequent accesses to vertices. Section 6 steps through an example of vertex arrays \nto show how LFA proves safety under these circumstances. High-level mechanics In Section 2, an operational \nsemantics de\u00ad.nes a concrete state machine. The analysis then performs two ab\u00adstract interpretations \nof this machine: one where a concrete ma\u00adchine state (.) abstracts component-wise into an abstract machine \nstate (.b), and one where it abstracts into a set of propositions (.). However, rather than run each \ninterpretation independently, as in the following diagram: . . ... . .b.b. b\u00b7\u00b7\u00b7 . .. .   . ... \u00b7\u00b7\u00b7 \nthe analysis will weave them together, so that the next step of each interpretation is a joint product \nof the current steps for both: .b. .. . ... . \u00b7\u00b7\u00b7 bb     Permission to make digital or hard copies \nof all or part of this work for personal or classroom use is granted without fee provided that copies \nare not made or distributed                  for pro.t or commercial advantage and that \ncopies bear this notice and the full citation        . ... . .. onthe.rsh,topostonserversortoredistributetpage.Tocopyotherwise,torepublis \n.   \u00b7\u00b7\u00b7 to lists, requires prior speci.c permission and/or a fee. POThrough this weaving, the combined \ninterpretation is more precise PL 07January 17-19, 2007,Nice,France C.2007AC00. than running either \nalone. opyrightcM1-59593-575-4/07/0001...$5. const .CONST = Z+ {#f, #len} v .VAR = an in.nite set of \nidenti.ers lam .LAM ::= (. (v1 \u00b7\u00b7\u00b7vn) call ) e,f .EXP ::= v | const | lam | (prim e1 \u00b7\u00b7\u00b7en) call .CALL \n::= (fe1 \u00b7\u00b7\u00b7en) | (sprim e1 \u00b7\u00b7\u00b7en) | (letrec ((v lam) * ) call) prim .PRIM = APRIM + HPRIM + REL aprim \n.APRIM = {+, -, *, /,...}hprim .HPRIM = {aget} rel .REL = {equal?, <, <=:<, ...}sprim .SPRIM = {anew, \naset!, if, halt} Figure 1. A grammar for CPS. Contributions The chief contributions of this work are: \n1. A framework for weaving higher-order .ow analyses, .rst\u00adorder logic and theorem proving. 2. Soundness \nwith respect to this weaving. 3. An instantiation of this framework for proving the safety of array-bounds \ncheck removal, even in the presence of higher\u00adorderness and continuations.   2. Continuation-passing \nstyle (CPS) Logic-.ow analysis operates over a variant of continuation-passing style (Figure 1) extended \nwith basic values, primitive operations, letrec, conditionals and a store with arrays. 2.1 Concrete \nsemantics The concrete semantics is a hybrid call-by-value/call-by-reference state machine for CPS: a \ncall-by-identity machine. The identity of a value can be either (1) a reference to it, or (2) the value \nitself if no reference to it yet exists. A reference is a globally unique name for a value, such as store \nlocation plus an offset, or a variable name plus a time at which it was bound. The shift in perspective \npays off because both references and values have a machine-level scope; that is, references and values \nretain their meaning across lexical scopes and even across machine transitions. This, in turn, makes \npropositions involving these iden\u00adtities meaningful across environments and machine states. For ex\u00adample, \ninformally, we might say, The value bound to x at time 3 is equal to the value at store location 16 offset \n3, or The value bound to y at time 3 is equal to the value of y at time 4. Note that any machine state \nand any scope can judge the truth of these propositions. Later on, to make the analysis computable, propositions \nwill take forms such as: Any value bound to y at call site 10 is equal to 0, and Any value ever bound \nto x is equal to any value ever bound to y. (Yes, this last one implies that x and y have only been bound \nto one value, but they could ve been bound to that value repeatedly, e.g., in a loop.) Figure 2 gives \nthe domains for a concrete operational seman\u00adtics. The semantics are a two-stage argument-evaluation/procedure\u00ad \n. . State = Eval + Apply Eval = CALL \u00d7BEnv \u00d7Heap Apply = Proc \u00d7Id * \u00d7Heap Heap = VEnv \u00d7Store \u00d7Time b \n. Bind = VAR \u00d7Time \u00df . BEnv = VAR .Time ve . VEnv = Bind .D i . Index = Val . . Loc = an in.nite set \nof locations arr . Array = Index .D s . Store = Loc .Array bas . Bas = CONST clo . Clo = LAM \u00d7BEnv proc \n. Proc = Clo + SPRIM val . Val = Proc + Bas + Loc d . D = Val t . Time = an in.nite set of times (contours) \nx . LogVar = a set of logical variables . . Id ::= b |d |(prim .1 \u00b7\u00b7\u00b7.n) |x Figure 2. Concrete domains. \napplication transition relation . in the set State \u00d7 State.1 This machine factors the environment into \na lexical binding environment (\u00df)and a State-level, global, binding-to-value environment (ve), as in \nShivers work [16]. Given a variable v, the time \u00df(v) is the time that v was bound for the environment \n\u00df.To retrieve the value associated with this binding, we can index into the global environ\u00adment ve with \nve(v, \u00df(v)). Because of this factoring, the binding (v, \u00df(v)) acts as a reference for the value ve(v, \n\u00df(v)). The set of identities Id also supplies the terms in the upcoming logic. In the semantics, an identity \ncan be a binding (as explained earlier), a denotable value (if no reference to it is yet available) or \na compound identity. A compound identity allows us to describe a value as a function of other identities. \nAnticipating fusion with the logic, the set Id already includes logical variables; the concrete semantics \ndoes not make use of these. The choice of the set Time the contour set is left open; for de.ning the \nmeaning of a program, the naturals suf.ce. Alternate choices for the abstract set Time later on may require \na different choice for the set Time in order to show correctness. For instance, for a k-CFA-level .ow \nanalysis, the set Time should be the set of call strings. For Agesen s CPA [1], the set Time should be \na sequence of Cartesian products of arguments. The initial state of a program represented by a call term \ncall is: (call, ., ., ., t0). Execution proceeds until either a stuck state, or application of the halt \nprimitive. In their de.nition, the semantics make use of a few auxiliary functions; the .rst turns an \nidentity into the value it represents: 1 As a shorthand, we decompose states as (...,ve,s,t) instead \nof (...,(ve,s,t)). The domain Heap merely factors out components com\u00admon to both Eval and Apply states. \n De.nition 2.1. The function V. : Id .D obtains the value of an Conditional In transitioning through \nconditionals, the condition identity: is tested against the false constant #f and the appropriate branch \nis taken: V. (b)= ve. (b) ([[if]],.. c,. t,.f .,ve,s,t) .(proc,..,ve,s,t) V. (.t) V. (.c) .= #f V. (.f \n) ( V. (d)= d V. [[(prim .1 \u00b7\u00b7\u00b7.n)]] = O. (prim)(V. (.1),...,V. (.n)), where proc = otherwise. where \nArray creation The array-creation primitive allocates a fresh lo\u00ad O. [[aget]] = .(.,i).s. .i cation in \nthe store, inserts the array and applies the continuation to O. (aprim) is the appropriate arithmetic \noperation the new location: O. (rel ) is the appropriate relation. ([[anew]],..length ,. c),...,ve,s.c.,ve,s,t) \n.(V. (.,t) where len 8< The next function converts an expression into an identity: De.nition 2.2. The \nfunction I : EXP \u00d7BEnv .Id obtains the .= alloc(s) = V. (.length ) if V. (.length ) .N : s. = s[... .[#len \n.len]]. identity of an expression,a State-level reference to a value when such a reference is available, \nand the value itself otherwise: The function alloc, of course, returns a fresh location outside the domain \nof the current store. I(v,\u00df)=(v,\u00df(v)) I(const ,\u00df)= const I(lam,\u00df)=(lam,\u00df) Array modi.cation The array-modi.cation \nprimitive inserts an element into the supplied array, if the index is in bounds: I([[(prim e1 \u00b7\u00b7\u00b7en)]],\u00df)= \n[ (prim .1 \u00b7\u00b7\u00b7 .n)]] 0 =i<s. #len ([[aset!]],..loc,.ind ,.val ,.c.,ve,s,t) .(V. (.c),..,ve,s.,t) where \n.k = I(ek,\u00df). 8 >< >: Loosely, the V function is to C s pointer-dereference operator = V. (.loc) . = \n. * asthe I function is to the address-of operator, &#38; . i = V. (.ind ) . = . where d = V. (.val ) \n. = . s. = s[... .(s(.))[i.d]].  De.nition 2.3. The cases below de.ne the concrete transition relation, \n..State \u00d7State. Argument evaluation In an argument-evaluation state, execution 3.   An abstract space \nfor CPS has reached the application of a function expression f to arguments This section de.nes the abstract \ndomains for LFA (Figure 3) and e1,...,en. The purpose of this transition is to look up the pro\u00ad operations \nupon them. With the exception of the domain Count, cedure, create a vector of argument identities, and \nincrement the the structure of these domains is straightforward for a .ow anal\u00ad global time: ysis by \nabstract interpretation. The \u00b5bcomponent of each state ap\u00ad n)]],\u00df,ve,s,t) .(proc,..1,...,.n.,ve,s,t. \n([[(fe1 \u00b7\u00b7\u00b7eproc = V. (I(f,\u00df)) 8< ) proximates the number of concrete identities to which each abstract \nidentity corresponds: zero, one or more than one counterparts. The ability to count (at least to one) \nbecomes important when .k = I(ek,\u00df) if V. (I(ek,\u00df)) where = . : generating propositions that hold on \na state: it is much simpler to t= t+1. Procedure application In procedure application, a closure is be\u00ading \napplied to a vector of argument identities. Execution proceeds by moving to the call site within the \nclosure, evaluating the identi\u00adties to values, and updating the environment within the closure for these \nvalues: make a claim about all of the concrete counterparts to an abstract identity if, at the moment, \nonly one such counterpart exists. For example, if two sets Aand B are equal, and each set is a singleton \nset, then we can infer that any member of Ais equal to any member of B. Note that the approximation Nbonly \ncounts precisely up to one concrete counterpart. Certainly, we could generalize this to n) call)]],\u00df),.,ve,s,t) \n.(call,\u00df. ,ve .,s,t) an arbitrary number, but the previous exercise demonstrates that (([[.((\u00b7\u00b7\u00b7vv1j \n \u00df. diminishing returns sets in at one. (We can t infer much from two = \u00df[vk . .t] ve where equal sets \nof size two.) . = ve[(vk,t) . .V. (.k)]. The choice of the abstract domain Time is left open. For a \n8< Recursive procedure evaluation In transitioning through letrec, 0CFA-level analysis, the set Time \nis a singleton. For a 1CFA-level the . terms are closed over the extended environment \u00df. before analysis, \nthe set Time is equal to the set of call sites, and the next transitioning to the interior call site: \n .time is the current call site. For a CPA-level analysis, the set Time ([[(letrec ((v lam) * ) call)]],\u00df,ve,s,t) \n.(call,\u00df. ,s,t.),ve is the powerset of sequences of types, and the next time is the t. = t+1 Cartesian \nproduct of the types of the arguments. \u00df. where = \u00df[vk ..] Note that abstract identities include both \ndD, whereas .tVal and b : .V. (I(lamk,\u00df. ve . = ve[(vk,t.) .))]. concrete identities included only D, \nbecause D = Val. Side-effecting primitive call Calls to side-effecting primitives be-The concrete-to-abstract \nmapping The absolute-value notation, have much like argument evaluation, except that there is no need \nto |\u00b7|, denotes abstraction of, and the symbol |\u00b7|a represents a evaluate the procedure. A side effect \ncan be either a modi.cation to function in the space a . a . These functions de.ne the corre\u00adthe store, \nor a control-.ow effect: spondence between the concrete and the abstract. n)]],\u00df,ve,s,t) .(sprim,..1,...,.n.,ve,s,t.) \nFor basic values bas,we have |#len| = #len, |#f| = #f, == 1,or, for bas > 1, |bas| = pos,and for bas \n< 0, j ([[(sprim e1 \u00b7\u00b7\u00b7e.k = I(ek,\u00df) if V. (I(ek,\u00df)) .= . |0| 0, |1| = neg. One could choose a much richer, \neven in.nite, set of where t= t+1. |bas| basic values should one desire. However, thanks to abstract \ngarbage it is not dif.cult to generalize the concrete semantics (mostly by .b. State = Apply Eval + \nmaking D apowerset of Val) to obtain one if desired. CALL \u00d7 Heap Count-abstractor, M:(VEnv \u00d7Store). Eval \n= BEnv \u00d7 The Count,is * \u00d7 Apply = Proc \u00d7IdbHeap db VEnv \u00d7 Count \u00d7b = Heap = Store \u00d7 Time M(ve,s)b=size{b.dom(ve):|b|b} \ndb M(ve,s).b=size{..dom(s):|.|=.},where b \u00df . = Time BEnv VAR . d size(S)=if size(S).{0,1}then size(S)else \n8. b b . = Time Bind VAR \u00d7. vebVEnv Bind .Db . = For the domain Nb, the elements 0, 1 and 8are incomparable \nunder bdthe order ., and the function . : Nb\u00d7 Nb. Nbis the natural i . = Index Val abstraction of addition. \nb . . Locd= a .nite set of locations The abstract semantics need the following de.nitions: d. = Index \n.Db arr Array sb. Store = Loc .Array . : b dDe.nition 3.1. The function VbbId .Dbobtains the value of \nan abstract identity: c bas . d= Bas {neg,0,1,pos,#f,#len,...} cClo LAM \u00d7 Vbbb)= b. (b clo . d= BEnv \n. (bvebb) d d. = Clo +SPRIM Vb.b(db)=db proc Proc c. d val Val = Proc +dLoc Bas +d b V.b(cval} val)={c \nb. bP(d dD = Val) Vb.b[[(prim b.1 \u00b7\u00b7\u00b7b.n)]]= Ob.b(prim)(Vb.b(b.1),...,Vb.b(b.n)), \u00b5b. =(Loc).Nb Count \nBind +d where: b N = {0,1,8} GG Ob.b[[aget]]=.(db1,db2).sb.b.bbibt . = Time a .nite set of times (contours) \nb.bbi.b d1 d2 b. . Idb::= bval | b.1 \u00b7\u00b7\u00b7b.n) Ob.b(aprim)is a sound abstraction of aprim b | cd |(prim \nbOb.b(rel )is a sound abstraction of rel . Figure 3. Abstract domains. De.nition 3.2. The function bBEnv \nId obtains I : EXP \u00d7 . b  the abstract identity of an expression,a State-level reference to collection \n(introduced shortly), the .nite domains suf.ce for our a value when such a reference is available, and \nthe abstract value purposes. Note that with an in.nite set of abstract basic values, itself otherwise: \nwidening and narrowing may be required to ensure termination [4]. For the remainder of the concrete domains, \nthe abstraction op\u00ad bbb I(v,\u00df)=(v,\u00df(v)) eration is: bb |(call,\u00df,ve,s,t)|Eval =(call,|\u00df|,|ve|,|s|,M(ve,s),|t|) \nI(const,\u00df)=|const |Val bbb |(proc,.,ve,s,t)|Apply =(|proc|,|.|,|ve|,|s|,M(ve,s),|t|) I(lam,\u00df)=(lam,\u00df) \nbb |..1,...,.n.|Id* =.|.1|Id ,...,|.n|Id .I([[(prim e1 \u00b7\u00b7\u00b7en)]],\u00df)=[[(prim b.1 \u00b7\u00b7\u00b7b.n)]] G bb |ve|VEnv \n=.(v,bt). |ve(v,t)|D where b.k =I(ek,\u00df). |t|=bt G The set of concrete identities to which an abstract \nidentity cor\u00ad |s|Store =...b|s(.)|Array responds is useful in the upcoming logic and in proofs: b ||=. \nG |arr|Array =.bi. |arr(i)|D De.nition 3.3. The concretization of an abstract identity b. with |i|=bi \nrespect to a state . is the set Conc . b.,where: |(prim .1 \u00b7\u00b7\u00b7.n)|Id =(prim |.1|\u00b7\u00b7\u00b7|.n|) Conc . bb ={b \n:b.dom(ve. )and |b|.bb} |b|Id =|b|Bind |d|Id =|d|D Conc ..b={. :..dom(s. )and |.|..b}|d|D ={|d|Val }|sprim|Proc \n=sprim Conc . bas c={bas :|bas|. c bas} |(lam,\u00df)|Val =(lam,|\u00df|) |(v,t)|Bind =(v,|t|) Conc . proc d={proc \n:|proc|. d proc}|\u00df|BEnv =.v.|\u00df(v)||clo|Proc =|clo|D [Conc . dbConc . c = val As de.ned, there is not \na straightforward Galois connection with c val.db these domains; to see why, consider what the least \nimprecise con\u00adcrete counterpart to pos is within Val. (It doesn t exist.) However, Conc . [[(prim b.)]]={[[(prim \n.)]]:.k .Conc . b.k}. Any counter \u00b5bnaturally extends to abstract identities of all kinds: \u00b5b( csize(Conc \n. c bas)= dbas) `\u00b4 bb \u00b5b(lam,\u00df) = max {\u00b5b(v,\u00df(v)) : v .free(lam)}.{1} \u00b5b[[(prim b.)]] =max{1,\u00b5b(b.1),...,\u00b5b(b.n)} \n8 >0 n =0 < c \u00b5b{valc1,..., valn}= \u00b5b(valc1) n =1 > : 8 n =2. With the ability to count abstractly, \na tighter connection between abstract and concrete knowledge becomes possible: Lemma 3.1 (Counting). \nIf |.|. bsize(Conc . b\u00b5b.). .,then d.)= b. (b 3.1 Abstract garbage collection With abstract garbage collection, \nunreachable bindings and store locations are re-allocated as fresh. This prevents merging in the ab\u00adstract, \nand it boosts both the precision and the speed of the analysis simultaneously. The correctness of this \ntechnique is addressed else\u00adwhere [11]. The abstract semantics for LFA feature a built-in lazy collector, \nwhich waits until precision loss is otherwise imminent before trying to garbage collect a resource. Naturally, \nthis collector requires a de.nition of what it means for an abstract identity to be reachable from some \nabstract state; reachability, in turn, requires the concept of touching: De.nition 3.4. An abstract identity \nb.1 touches another abstract identity b.2 in an abstract state .biff b.2 .Tb.b(b.1),where: Tb.b( c bas)= \n\u00d8 Tb.b(bb)= Vb.b(bb) 8 [ bbb T.b(.b)= V.b[[(aget . |i|)]] i=0 bbb T.b(lam,\u00df)= {(v,\u00df(v)) : v .free(lam)} \nb T.b[[(prim b.1 \u00b7\u00b7\u00b7b.n)]] = {b.1,...,b.n}.Vb.b[[(prim b.1 \u00b7\u00b7\u00b7b.n)]] b. {ccb. ( c. b. ( c Tbval1,...,valn}= \nTbval1) .\u00b7\u00b7\u00b7Tbvaln). Touching extends naturally to an abstract state .b: bbb T.b(call ,\u00df, bs,bt)= {(v,\u00df(v)) \n: v .free(call )} ve,b\u00b5,b [Tb.b( d., bs,bt)= b. ( dTbbVb.k)). proc,bve,b\u00b5,bTbproc) .. (b. (b k An abstract \nidentity is reachable from a state if there is a chain of touching from the state to the identity: De.nition \n3.5. The identities reachable from an abstract state .b, written Rb(.b),is the set {b. : b.root .Tb.b(.b) \nand b.root .. * bb.},where b.1 ..bb.2 iff b.2 .Tb.b(b.1).   4. A logic for concrete states This section \nbuilds a restricted, .rst-order logic for propositions that describe a concrete state. Apart from the \nlack of an existential quanti.er, the inclusion of a ranged universal quanti.er, and the requirement \nthat all propositions be in prenex normal form, this logic is a standard .rst-order logic [5]. Three \nfactors led to these restrictions: (1) the safety proofs of concern require only universal quanti.cation; \n(2) the theorem prover s behavior becomes more predictable when restricted; and (3) abstract garbage \ncollection might destroy the witness for an existential quanti.cation, which forces existentially quanti.ed \npropositions to be discarded. Controlling the state-space explosion Within CPS, abstract garbage collection \nplays a role in controlling statespace explo\u00adsion. Continuations, or rather, the abstract bindings and \nloca\u00adtions which become associated with them, are candidates for garbage collection. Consequently, before \ninvoking a function, it is frequently possible to sharpen its continuation argument via a GC step. Once \ncollected, continuations do not merge in the abstract. Hence, the abstract interpretation returns di\u00adrectly \nto its proper return point, instead of forking to the return points of all previous callers of that function. \nMore precisely, abstract garbage collection of continuations leads to polyvari\u00adant control-.ow behavior. \nConsider a call site (f ... q) with continuation q. Sup\u00adpose that when the abstract interpretation reaches \nthis point, f is bound to a closure with .term (. (... k) ...). Under what circumstances can the analysis \ncollect the binding for k? If this call is a self tail-call, so that q = k, then there is no need to \ncollect as the merging will not harm precision. If this call is recursive but not tail recursive, then \nk will merge with the internal continuation, q. Consequently, it will look as though an internal, recursive \ncall to f could return to an exter\u00adnal call to f and vice versa. Fortunately, this is only a minor detriment \nto precision. Moreover, even this internal/external merging disappears when moving from a 0CFA contour \nset to a 1CFA contour set. Lastly, if this call is an external (non\u00adrecursive) call to the function f, \nthen unless the binding to k was previously captured by a call/cc-level continuation, the binding to \nk will be eligible for garbage collection, and this holds even in a 0CFA-level .ow analysis. As a result, \nin all but the pathological case of unrestricted us\u00adage of call/cc, control-.ow polyvariance is achieved. \n(Sev\u00aderal constrained usage patterns for call/cc still achieve poly\u00advariance.) It is this polyvariance \nthat is responsible for chop\u00adping off the spurious branches of the interpretation that lead to statespace \nexplosion and blurred precision. 4.1 Syntax for propositions The state logic includes basic propositions \n(f), quanti.ed proposi\u00adtions (.) and assumption bases (.): f . F ::= (= .1 .2) | (not f) | (or f1 f2) \n. . . ::= f | (forall x: b..) . . Assms . .. Universally quanti.ed logical variables are restricted to \nthe con\u00adcrete values of some abstract identity. At .rst glance, it seems that there is no relation in \nthis logic other than equality; this is because additional relations are encoded as functions mapping \nto truth val\u00adues. A shorthand (desugared below) lets us use the more familiar notation for relations \nin logic. For later convenience, the expression c ids(.) represents the set of abstract identities used \nwithin a proposition. 4.2 Structure and interpretation The terms in this logic are identities Id from \nthe concrete seman\u00adtics. Now we ll .nally make use of the logical variables included earlier. States \nthemselves de.ne the structure of the logic. The in\u00adterpretation of a term . in structure . is the value \nV. (.). Conse\u00adquently, the interpretation of a primitive operator O. (prim) is its conventional meaning; \ne.g., O. [[+]] = .(a,b).a+b. Note that inter\u00adpretations of a term are denotable values, which makes the \ndomain D the universe of discourse. Given a state .,an interpretation, I,is a pair (.,.) where . : LogVar \n. D maps from free logic variables to values. The notation I. is shorthand for the interpretation (.,.);and \nthe notation I[x .. .d] is shorthand for (.,.[x .d]) where I =(.,.). Lastly, when I =(.,.): ( .(.) . \n.LogVar I(.)= V. (.) otherwise. De.nition 4.1. An interpretation I justi.es a proposition . iff I |= \n.,where: I |= (= .1 .2) iff I(.1)= I(.2). I |= (not f) iff it is not the case that I |= f.  I |= (or \nf1 f2) iff I |= f1 or I |= f2.  I |= (forall x : b..) iff for each . .Conc . b., it is the case that \nI[x ..I(.)] |= ..  Justi.cation then extends naturally across sets of states and sets of propositions: \n For a set of states S, S |= . iff for each state . .S, I. |= ..  For an assumption base ., I |=. iff \nfor each proposition . .., I |= ..  For proving the correctness of interacting with a theorem prover, \nwe ll need the notion of entailment: De.nition 4.2. An assumption base . entails a proposition ., written \n. |= .,iff I |=. implies I |= .. In other words, an assumption base entails a proposition if all valid \ninterpretations of the assumption base justify the proposition. The correctness of the analysis also \nneeds the notion of correspon\u00addence a relationship between a concrete state, an abstract state and an \nassumption base: De.nition 4.3. Atriple (.,.,b.) constitutes a correspondence, denoted Cor(.,.,b.),iff \n|.|..band I. |=.. Using correspondence, we can select the set of concrete states that map to a given \nabstract state and satisfy some assumptions: De.nition 4.4. The .ltered concretization of an abstract \nstate .bunder assumption base ., written b. : Cor(.,b. ./.,is the set {.,.)} This leads to another convenient \nextension of justi.cation: (.,b.) |./. | = . iff b= .. 4.3 Syntactic sugar When used where a proposition \nis expected, the following desugar: (.= .1 .2) .(not (= .1 .2)) (rel .) .(. = #f (rel .)) (implies f1 \nf2) .(or (not f1) f2) (and f1 f2) .(not (or (not f1) (not f2))). Vector notation quanti.es over multiple \nvariables and identities: (forall .x1,...,xn.: .b.1,...,b.n..) .(forall x1 : b.1 (forall x2 : b.2 ... \n(forall xn : b.n .) ...)). It is often convenient to use an abstract identity where only a concrete identity \nis syntactically allowed. The convention is that if C[b.] is a proposition, where Cis a Felleisen-style \n[6] one-hole context of the identity, then this desugars to: (forall x : b. C[x]), where x is fresh. \nWhen multiple instances of the same abstract identity occur within a proposition, each has its own outer-level \nuniversal quanti.cation. 4.4 Syntactic inference rules: Theorem prover as oracle Rules for syntax-directed \nreasoning enable interaction with a theo\u00adrem prover through the concept of a derivation: De.nition 4.5. \nAn assumption base . derives a proposition . iff there exists a proof of . ... Table 1 gives the core \nsyntactic inference rules for this logic. These rules are complete for combinatorial logic, but due to \nthe restrictions on the logic, they are incomplete in general. Of course, the soundness of the analysis \nrequires the soundness of these rules: Theorem 4.1 (Syntactic soundness). If . ..,then . |= .. Proof. \nProofs for rules other than (Int) are standard, following the development found in basic texts [5]. For \n(Int), assume . . (forall x : b. f) and {f}.f.. Choose any I =(.,M) such that I |=.. Then we know I |= \n(forall x : b. f). Now choose any vector . such that .k . Conc . b= . .k.Let I. I[xk . .k]. We know I. \n|= f. Thus, I. |= f.. Hence, I. |= (and ff.) and therefore, we have that I |= (forall x : b. (and ff.)). \nHaving established soundness, a prover can (if desired) emit a veri.able proof tree when it claims that \n. .. holds.  4.5 Semantic inference rules: Flow analysis as oracle Semantic derivation rules, of the \nform (.,b.) . ., obey a tighter soundness theorem: Theorem 4.2 (Semantic soundness). If (b./. | .,.) \n..,then b= .. The proof of this theorem is provided with each nontrivial rule. Semantic derivation rules \nhave access to knowledge gathered from the .ow analysis, as codi.ed within a state .b. As a result, they \nare strictly more powerful than syntactic rules. With these rules, the .ow analysis acts as an oracle \nto the prover: Rule 4.1 (Absence). .b |d|D .V.b(b.) (.,b.) .(forall x : b. . (= dx)) Proof. By the de.nition \nof .and |\u00b7|. Rule 4.2 (Universal introduction). \u00b5b.b(b.)=1 (.,b.) .(forall .x1,x2.: .b.,b..(= x1 x2)) \n Proof. By the Counting Lemma. Rule 4.3 (Range swap). (.,b.) .(= b.1 b.2) (.,b.) .(forall x : b.1 .) \n(.,b.) .(forall x : b.2 .) With the Oracle Rule (below), the .ow analysis may consult the prover as an \noracle, and vice versa. By including this rule, LFA can alternate between the .ow analysis and the prover \nin justifying goals: Rule 4.4 (Oracle). (.,b.) ..1 \u00b7\u00b7\u00b7 (.,b.) ..n . .{.1,...,.n}... (.,b.) ... Proof. \nBy syntactic soundness. . ... .{f1}.f3 . .{f2}.f3 . .(= ...) . ..[./x] (Assm) (.Ant) (Subst) . .. . \n.{(or f1 f2)}.f3 . ..[../x] . .f . .{f1}.f2 . .{(not f1)}.f2 (Ant) . ... (Cases) . .{(not f1)}.f2 (Contr) \n. .{(not f1)}.(not f2) .. .f . .f2 . .f1 . .f1 . .(forall x : b. f) {f}.f. (Eq) . .(= ..) (.Cons) (Int) \n. .(or f1 f2),(or f2 f1) . .(forall x : b. (and ff.)) . ..x ..free(.). .(forall .x1,x2.: .b.1,b.2..) \n(.Intro) (.Swap) . .(forall x : b..) . .(forall .x2,x1.: .b.2,b.1..) Table 1. Syntactic inference rules. \n 5. Abstract semantics: LFA This section de.nes the analysis LFA as the reduced product of two abstract \ninterpretations. While either interpretation is sound by itself, each serves to enhance the precision \nof the other when com\u00adbined. The .rst interpretation is a straightforward state-machine\u00adbased .ow analysis \nplus abstract counting. The second interpreta\u00adtion abstracts each state to a set of propositions holding \non that state. The transition relation |=State \u00d7Assms) \u00d7( > in ( State \u00d7 Assms) de.nes the combined interpretation. \nRunning the analysis on a program call consists of exploring this relation when starting from the initial \nabstract state: ((call,.,.,.,.,bt0),{}). The correctness of LFA is a matter of proving that a correspon\u00addence \nis maintained under transition. The key inductive step for this proof is the following: Theorem 5.1 (|=> \nsimulates .). If Cor(.,.,b.) and . ...,there exists a (.b. ,..) such that: (.,b.) |=> (.b. ,..) and Cor(.. \n,.b. ,..). Proof Outline. The proof for the .ow-analysis half is largely straightforward [11]. Except \nfor the few places where this half differs from a straightforward proof, we ll skip discussion of cor\u00adrectness. \nThe other half of the proof, a proof of correctness for the ..-update rules, is novel and supplied for \neach nontrivial rule. De.ning the relation |=> There are many correct ways to de\u00ad.ne the relation |=>. \nThe shortest sound de.nition is, for instance, (.,b.) |=> (.,\u00d8). The concern in this work will be engineering \nthe transition relation so that (1) it fully exploits the information available in the state .band the \nassumption base .; and (2) it explicitly accounts for common programming idioms. The subsections ahead \nconstitute a case-by-case de.nition and .. discussion of the abstract transition, (.,b.) |=> (b,..). \nEach subsection contains a pattern describing an abstract state, .b,and a form for subsequent states, \n.b., matching that pattern, like so: .b= \u00b7\u00b7\u00b7 .b. = \u00b7\u00b7\u00b7 Each subsection may contain multiple cases, and \neach case contains rules for computing the new assumption base .. from the old assumption base . and \nthe old state .b. A guard on the state .band the assumption base . for each case determines when that \ncase applies. When guards on cases overlap, the .rst case has priority. 5.1 Argument evaluation A state \n.bis an argument-evaluation state if it is preparing to apply a function expression f to some arguments \ne1,...,en. The purpose of this transition is to look up the set of abstract procedures for the expression \nf, and to fork the analysis to each one. In the process, each argument ei is evaluated into an abstract \nidentity: .b=([ (fe1 \u00b7\u00b7\u00b7en)]], bve, b\u00b5,b \u00df, bs, bt) b=( d.1,...,b, bs, bt. .. proc,.b.n.ve, b\u00b5,b) 8 <dVbI(f,\u00dfb)) \nproc . b. (bwhere b.k = Ib(ek,\u00dfb) : bt. = succ(b dt) The function d succ returns an abstract time, and \nit satis.es following correctness constraint: |t|.bt .|t +1|. dt). =succ(b For instance, for 0CFA precision, \nonly one abstract time exists, so the function d succ always returns the same time; for 1CFA, the func\u00adtion \nd succ returns some label for the current call site [[(fe1 \u00b7\u00b7\u00b7en)]] itself.2 The subsequent assumption \nbase .. loses nothing, due to the following rule: Rule 5.1 (Complete preservation). `\u00b4 .. ../b.: (. ...) \nimplies s. = s.. and ve. = ve.. .. .. Proof. Choose any state . such that Cor(.,.,b.). Suppose . ... \n. Choose any proposition . in ... We know that I. |= .. Because the relation |= depends only upon the \nvariable environment ve and the store s, which are identical between states . and ..,we have that I.. \n|= .. Several cases below will also achieve a complete preservation of knowledge by avoiding modi.cations \nto the variable environment and the store. 5.2 Procedure application: More than zero arguments The apply \ntransition is the heart of LFA. This is where much of the weaving with the prover happens. It is in this \nstage that LFA can garbage collect, fork the analysis and expand or contract the assumption base. The \napply transition proceeds through the composition of sev\u00aderal smaller transitions one for each argument \npassed.3 Each sub\u00ad 2 Since the choice of contour set is not our focus, we use a simpli.ed .Time . .succ \nsucc : .Time function. For contour sets beyond 0CFA, the . operation takes the current state .bin addition \nto the current time. 3 In the full proof of correctness for the .ow analysis, we need to factor the concrete \napply transition similarly and then prove this equivalent to the original de.nition by induction on the \nlength of the argument vector .. transition examines the .rst identity passed; updates the state and \nassumption base; and moves to the remaining arguments: .b=(([,\u00df), .b.n., bs, bt) [(. (v1 \u00b7\u00b7\u00b7vn) call \n)]]b.1,...,bve, b\u00b5,b\u00dfb.). \u00b5. b .b. =(([,.2 ...,b.n., vebs, b,t) [(. (v2 \u00b7\u00b7\u00b7vn) call)]], .b, b 8 bb >b1 \n=(v1,t) > < \u00dfb. bv1 .b =\u00df[.t] where >b. =\u00b7\u00b7\u00b7 ve > : \u00b5. b=\u00b7\u00b7\u00b7 Case 5.2.1 (b.1 =bb1). In this case, the \ninterpretation is rebinding a variable to itself. Consider this case in the concrete. This situation \ncorresponds to having the argument .1 =(v, t1)and the binding b. b1 =(v, t2), such that |t1| = |t2| =t. \nInstead of setting ve = ve[(v, t2). . ve(v, t1)], the concrete execution could extend only the lexical \nenvironment \u00df. =\u00df[v ..t1]by mapping this variable v to the older time. The abstraction of this concrete \ntransition avoids bumping the allocation counter in the abstract, i.e.: veb. = b ve \u00b5b. =\u00b5b. Note that \nthe abstract lexical environment \u00dfb. remains the same, .b. t]= b.|t1|]= bbecause \u00dfb[v .\u00df[v .\u00df[v ..|t2|].Since \nve =ve and s =s. in the concrete, the Complete Preservation Rule for .. applies. This case catches a \ncommon higher-order recursion idiom, where a variable is explicitly rebound to itself while recurring, \nsuch as the variable f in: (define (map f lst) (if (pair? lst) (cons (f (car lst)) (map f (cdr lst))) \n())) In fact, by detecting f s invariance, we can turn this into: (define (map f lst) (letrec ((mp (. \n(lst) (if (pair? lst) (cons (f (car lst)) (mp (cdr lst))) ())))) (mp lst))) which allows the argument \nf to be inlined when map is inlined. Case 5.2.2 ((.,b.) . (= b.1 bb1)). Even if the argument b.1 is not \nidentical to the binding bb1, it may still be the case that the values they represent are equal. Unlike \nthe previous case, there is no clear analog to this in the concrete. It s nonsensical to have a fresh \nbinding be equal to the value it s going to be assigned: a fresh binding cannot possibly already have \na value. In the abstract, however, bindings are a .nite resource, and the analysis may be forced into \nallocating a stale binding one which is already in use. Hence, it s conceivable (and not uncommon) that \nthe abstract value Vb.b(b.1)may already be sitting at index bb1 within the global environment b ve. In \nthis case, the analysis can still update the state components as before: veb. = b ve \u00b5b. =\u00b5b. To prove \nthis behavior correct, we have to modify the concrete semantics so that before binding (v,t), the concrete \nexecution .rst searches through the domain of the global environment ve for a binding (v, t.) such that \n|t.| = |t| and V. (v, t)= V. (v, t.). If such a time t. exists, the concrete would instead modify the \nlexical environment \u00df. so that \u00df. = \u00df[v ..t.]instead of having the variable v map to the current time. \nOnce more, the Complete Preservation Rule applies. Lastly, note that this case doesn t drive a speci.c \noptimization or account for a speci.c programming idiom so much as it corrects a common source of precision \nloss for a .ow analysis. Case 5.2.3 (b.1 =C[bb1], Cis invertible, \u00b5b(b.1)=1and bb1 ..Rb(.b)). In this \ncase, the interpretation is rebinding a variable to an invertible context of itself.4 Before proceeding, \nwe need to de.ne what an invertible context is. De.nition 5.1. A context Cis invertible with respect \nto some term equivalence relation =if for all terms t, there exists a context C-1 such that C-1[C[t]]=t. \nIn this context, the equivalence relationship b. =b.. is: (.,b.).(forall .x, y.: .b.,b...(=xy)). In general, \nan inverse context may not exist, but for most loop\u00ading idioms, hard-coding rules like the following \nis suf.cient: \u00b5b(b.)=1and C =[[(+ [] b.)]] =.C-1 =[[(-[] b.)]] C =[[(cons x [])]] =.C-1 =[[(cdr [])]] \nThe.rst rulecovers the i++ idiom. After the i++ happens, what the old assumption base .knew about the \nbinding to i has become knowledge of the value i-1 in the new assumption base ...If desired, an algebraic \nsolver can .nd inverses for other contexts. To handle invertible rebinding, instances of the binding \nbb1 in the old assumption base .become the identity C-1[bb1]in the new assumption base ..: Rule 5.2.3.1 \n(Inverse propagation). b.1 =C[bb1] C-1 exists \u00b5b(b.1)=1 bb1 ..Rb(.b) .. =.[C-1[bb1]/bb1] Note that this \nalso acts as the preservation rule for this case. Af\u00adter updating the assumption base .., the analysis \ngarbage collects the old binding bb1 by assigning its new value with a strong update: . . b veb=veb[bb1 \n.V.b(b.1)] \u00b5b. \u00b5[b.. =bb1 .1] Case 5.2.4 (\u00b5b(bb1)= 1and bb1 ..Rb(.b)). In this case, the abstract interpretation \nis about to allocate a stale abstract binding that has become unreachable. As before, the analysis can \ngarbage collect: veb. ve[b.Vbb = bb1 . b. (.1)] \u00b5b. \u00b5[b.. =bb1 .1] Again, garbage collection consists \nof a strong-update overwriting of the abstract value living at index bb1 within the variable environ\u00adment \nveb.. At the same time, the counter \u00b5b. now re.ects that the abstract binding bb1 corresponds to a single \nconcrete binding. (It is a theorem [11] that if an abstract binding is unreachable, then all of its concrete \ncounterparts are also unreachable.) Making the collection, however, means that the new assumption base \n.. can t preserve propositions that necessarily depend on the binding bb1: Rule 5.2.4.1 (Partial preservation). \nb. c (.,b.).. b1 .ids(.) . ... 4 The context C is Felleisen s [6] one-hole context for the grammar of \nb Id. Why would the assumption base contract? Without a full understanding of the analysis, one might \nwonder why proposi\u00adtions would ever be discarded. Suppose that the proposition: (forall x : ([[x]],bt1) \n.) is in the current assumption base. This proposition makes a claim about all of the concrete counterparts \nto the abstract binding ([[x]],t1). More speci.cally, it is making a claim that b holds for all values \nof the variable x when it was bound at times that abstract to bt1. During the abstract interpretation, \nit may arrive at a point where it s going to bind x again at time bt1. As a result, the set of concrete \nbindings to which the abstract binding ([[x]],bt1) corresponds has expanded. In order to preserve this \nproposi\u00adtion, the analysis must show that the proposition . holds for the new additions to this set. \nIf the assumption base doesn t have enough information to show this, then the analysis cannot preserve \nthe universally quanti.ed proposition. Case 5.2.5 (\u00b5b(b.1)=1 and \u00b5b(bb1)=0). In this case, the abstract \nbinding is fresh, and the identity to which it will be bound has only one concrete counterpart, yielding: \nveb. = bb1 .V.b(b.1)] ve[b. b \u00b5b. = \u00b5b[bb1 .. 1]. After this step, both identities bb1 and b.1 have a \nsingle concrete counterpart, so any concrete counterpart of one will be equal to any concrete counterpart \nof the other in ..: Rule 5.2.5.1 (Fresh binding). \u00b5b(b.1)=1 \u00b5b(bb1)=0 (forall .x1,x2. : .b.1, bb1. (= \nx1 x2)) . .. In this case, there is also a partial preservation of the assumption base ., in that the \nanalysis must discard any propositions necessar\u00adily involving the binding bb1 while constructing the \nnew assump\u00adtion base ... In reality, this costs no precision, as any universally quanti.ed proposition \nranging over the empty set would have been both vacuously true and useless. Thus, the Partial Preservation \nRule (5.2.4.1) applies. Case 5.2.6 (Otherwise). If the analysis resorts to this case, it could not handle \nthe binding in a precision-enhancing or -preserving manner. Thus, the analysis must use the weak, merging \nconserva\u00adtive update: . . b veb= ve . [b.. (b bb1 Vb.1)] \u00b5b. = \u00b5b. (. .0)[bb1 .. 1]. As before, the analysis \ncan preserve assumptions that don t nec\u00adessarily involve the binding bb1. In this case, the reason is \nthat we would otherwise be expanding the range of a universally quanti\u00ad.ed variable. And, from Small \n. Big and .x . Small : .(x), we cannot infer .x . Big : .(x). Hence, the Partial Preservation Rule (5.2.4.1) \napplies. 5.3 Procedure application: Zero arguments Eventually, the apply transition runs out of arguments, \nand the analysis transitions with the following: .b= (([[(. () call)]], bve, b\u00b5,b \u00df), .., bs, bt) b .b. \n=(call, ve, b\u00b5,b \u00df, bs, bt) In this case, the Complete Preservation Rule (5.1) applies. 5.4 Recursive \nprocedure evaluation In LFA, the construct letrec behaves much like a speci.c instance of procedure application. \nTo simplify the presentation, this subsec\u00adtion covers the letrec of a single . term. However, it is not \ndif\u00ad.cult to handle a mutually recursive letrec by decomposing the transition as was done in procedure \napplication. .b=([ (letrec ((v lam)) call)]], bve, b\u00b5,b \u00df, bs, bt) .. \u00dfb.. \u00b5. b. b=(call, ve s, bt , \nb, b,) 8 >t= dt) >b. succ(b > > >bb =(v,bt) > > >\u00dfb. = \u00dfb[v ..bt.] > > > >\u00dfb. >b. =(lam, ) > > > < \u00b5b. \n= \u00b5b[bb .. nb] where strong?= bb ..Rb(.b) or \u00b5b(bb)=0 >(> > >veb[b..)] >. b .Vb.b(bstrong? > >veb= > \n>veb. [bb .. Vb.b(b.)] otherwise > >(> > > >1 strong? > >nb= : 8 otherwise. By replacing the terms b.1 \nwith b. and bb1 with bb, this case imports the Fresh Binding Rule (5.2.5.1) and has its own complete \npreservation rule: Rule 5.2 (Quali.ed complete preservation). (.,b.) . (= b. bb) .. . . If the analysis \ncan t preserve all propositions for the new as\u00adsumption base .., the Partial Preservation Rule (5.2.4.1) \napplies for propositions not necessarily involving the binding bb. 5.5 Side-effecting primitive call \nThe abstract interpretation handles side-effecting primitive-call states identically to argument-evaluation \nstates, except that there is no need to look up the procedure. 5.6 Conditionals The handling of conditional \ntransitions depends on how much in\u00adformation is known about the condition: .b=([ if]], .b.t,bve, b\u00b5,b \n.c,b.f ., bs, bt) .b. =( dve, b\u00b5,b proc, .., bs, bt) Case 5.6.1 ( (.,b.) . (.#f) or (.,b.) . (= b.c #f) \n). If there = b.c is enough information to prove that the condition either must be true or must be false, \nas in this case, then the abstract interpretation takes only the appropriate branch: (b V.b(b.t)(.,b.) \n. (.= b.c #f) d proc . b V.b(b.f )(.,b.) . (= b.c #f). Clearly, the Complete Preservation Rule applies \nhere. Case 5.6.2 (\u00b5b(b.c)=1). If the analysis can t precisely evaluate the condition, yet its count is \n1, then the interpretation forks in both directions. Meanwhile, the true branch asserts the condition \nin the new assumption base .., and the false branch asserts its negation. Thus, the analysis preserves \nall knowledge, and it adds the following to the true branches assumption base: (..c #f) . .. = b, while \nfor the false branches, it adds: (=b.c #f) ... . The abstract continuation is the join of both continuations: \nproc .b.b(.b dVbt)Vb. (b.f ). But, how could the condition have a count of one, and have an unknown truth \nvalue? In practice, if the analysis were run on a single function, a condition might evaluate to .if \nit depends on data outside the scope of the function. Case 5.6.3 (\u00b5b(b.c) > 1). In this case, the abstract \nidentity of the condition corresponds to multiple concrete identities. This case is handled identically \nto the previous one, except that the assumption bases do not expand, i.e., .. =.. Merging forked branches \nIn handling conditionals, the interpre\u00adtation sometimes had to fork. Left unchecked, this forking could \nlead to explosion. Once more, abstract garbage collection comes to our rescue. Using the non-lazy abstract \ngarbage collector from pre\u00advious work on GCFA [11], it is possible to merge forked branches. By garbage \ncollecting when each fork hits the joining continuation, it is often the case that their garbage-collected \nstates collapse back into the same state, or into states such that one is more precise than the other. \nWhenever this is the case, merging happens automati\u00adcally, and it costs no precision. 5.7 Array creation \nFor array creation, the analysis attempts to garbage collect the abstract location it s about to allocate. \nIf the abstract location is stale but unreachable, a merging is prevented. .b=([ ,..length ,bc., b\u00b5,b \nanew]]bve,bt) .s,b .b. =(d.., bs. ,\u00b5b. ,bt) proc,.bve,b 8 >procd.Vb.b(b.c) > > >b >.=alloc(sb) > > >s. \ns[b.b >>b=b...arrd[#len .V.b(b.length )]] >(> < . .b..Rb(.b)or \u00b5b(.b)=0 arr where d= >sb(.b) otherwise \n> > > >\u00b5. >b\u00b5[b.n] =b. .b>(> > >b. b\u00b5(b >1 ..R(.b)or b.)=0 > : nb= 8 otherwise. Like the function dalloc \nis constrained so that:succ, the function |s|.sb.|alloc(s)|.s). =alloc(b When allocating an array, a \npartial preservation rule applies: Rule 5.3 (Partial preservation, array). b. c (.,b.)....ids(.) . ... \nIf the identity b.length has a single counterpart, and the location .bis fresh, then the new assumption \nbase .. can chain them together: Rule 5.4 (Array length chaining). \u00b5b(b.length )=1 (.b..Rb(.b)or \u00b5b(.b)=0) \nb. (=b.length (aget . #len)) .. 5.8 Array modi.cation In handling array modi.cation, there are several \nissues to consider: 1. Arrays are updated one element at a time. 2. Not all elements may satisfy a given \nproperty all the time. 3. The properties an array satis.es can change over time.  The analysis can \nonly reason about a .nite number of concrete ob\u00adjects with perfect precision at any one moment. An array, \nhowever, may contain arbitrarily many elements. This necessitates a mech\u00adanism for handling abstract \nintervals of the array, and a way to merge these abstract intervals. For a traditional .ow analysis, \nthis task is dif.cult. For a theorem prover, this task is much simpler once the .ow analysis has peeled \naway the aliasing and the higher\u00adorderness. The rules in this subsection are concerned primarily with \ni++\u00adstyle array updates. As a result, the prover will be dealing with closed intervals such as [i,j]and \nhalf-open intervals such as [i,j). For steadily expanding or shrinking the interval, the prover can take \nadvantage of lemmas like [i,j-1]=[i,j). In LFA, the endpoints of these intervals are constrained to be \nthe concrete counterparts to abstract identities. The transition in this case is: .index ,bve,b .b=([ \naset!]],.b.loc,b.val ,b.c., bs,\u00b5,bbt) b=(d, b,bt) .. s. proc,..ve,b\u00b5,b 8 >dV. (bc) proc .bb. > > > >b \n< ..Vb.b(b.loc) where bi .Vb.b(b.index ) > >bb >d =V.b(b.val ) > > : sb. =sb[.b.s(.b))[bi ..db]]. .(b \nThe outline for the update of the new assumption base .. is: 1. Check for a property fprop holding on \nthe value for b.val . 2. Check for intervals adjacent to the index b.index where the prop\u00aderty fprop \nholds. 3. When found, update the assumption base .. to re.ect the newly expanded abstract interval. \n More formally, the prover looks for a property fprop where (.,b.). (forall .x,y1,...,yn.: .b.val ,b.1,...,b.n.fprop) \nholds. Finding fprop There are a number of ways to .nd propo\u00adsitions that qualify for the property fprop. \nFinding them all is clearly incomputable. A few heuristics, however, focus the search. The easiest approach \nis to look through the current as\u00adsumption base .for occurrences of the identity b.val in a propo\u00adsition. \nOf these propositions, those containing relational primi\u00adtives (REL) and those generated by a conditional \ntransition are good candidates. If no candidates emerge, the search expands to propositions that use \nidentities equivalent to b.val . If still no candidates emerge, the search is abandoned. Then, the prover \nchecks to see if the property fprop holds for all elements of an interval: (forall i : Nb(forall a: b.loc \n(.,b.). (forall j : b.index (forall .y1,...,yn.: .b.1,...,b.n. (implies finterval fprop[(aget ai)/x]))))) \nwhere the guard finterval checks whether j is adjacent to a known interval. For catching a start-at-zero-i++-style \niteration, the interval is: finterval =(<= 0 i (-j 1)). Putting this all together yields a rule for handling \nincremental array update: Rule 5.5 (Incremental array update). (.,b.) .(forall .x,y1,...,yn.: .b.val \n,b.1,...,b.n.fprop) (forall i : Nb (forall a: b.loc (forall j : b.index (.,b.) . (forall .y1,...,yn.: \n.b.1,...,b.n. (implies (<= 0 i (-j 1)) fprop[(aget ai)/x]))))) (forall i : Nb (forall a: b.loc (forall \nj : b.index ... (forall .y1,...,yn.: .b.1,...,b.n. (implies (<= 0 ij) fprop[(aget ai)/x]))))) The Partial \nPreservation Rule for arrays (5.3) also applies. The incremental rule also serves as a starting point \nfor more general rules. The next rule up the ladder of engineering complexity would be one that looks \nfor abstract intervals not starting at index 0. However, the frequency of the idiom for (i = 0; i < length; \ni++) and its equivalents makes the simple rule widely applicable. 5.9 Termination A branch of LFA terminates \nin stuck states; when the halt prim\u00aditive is applied; or when the current state is more precise than \n(via .) a state already visited while the current assumption base is stronger than (via |=) the assumption \nbase associated with the visited state. Formally, a branch terminates if its current state\u00adassumption \nbase pairing is (.,b.) and for some state-assumption base pairing (.bv,.v) already visited: .b..bv and \n(.,b.) |=.v. Of course, the prover .is the approximation for entailment (|=). As de.ned, termination \nof the analysis is not guaranteed be\u00adcause (1) the prover, as an external entity, may not halt, and (2) \nour abstract domains are not .nite in one place: the height of the syntax tree for Idbis unbounded. A \ntime limit on the prover removes the termination concern. Even if the prover fails or times out, LFA \nwill still continue, al\u00adthough its precision degrades toward GCFA as the assumption bases shrink. But, \nwhat if the prover always fails or times out during the termination check? If this is a concern, the \nalways-terminating approximation to (.,b.) |=.v: . ..v =. (.,b.) |=.v, eventually leads to termination. \nSmarter terminating approxima\u00adtions exist, but this is suf.cient. The identity-height concern applies \nsolely to the assumption base, since the identities produced within an abstract state .bare bounded in \nheight by the syntactic expression from which they came. For the assumption base, this concern is removed \nby bound\u00ading the height of an abstract identity s syntax tree at some .xed height h, and pruning identities \nthat break this height with a widen\u00ad b ing operation [4], Hh : Id .Idb: b H0 b.= V.b(b.) ( [[(prim Hh(b.1) \n\u00b7\u00b7\u00b7Hh(b.n))]] b. =[[(prim b.)]] Hh+1 b.= b. otherwise. Care must be taken, however, as the pruned identity \nmay be less precise. When the range of a universal quanti.er loses precision, the conservative action \nis to discard the entire proposition. The alternative to this widening is simpli.cation, wherein the \nprover symbolically manipulates an identity with the goal of reduc\u00ading it to a smaller identity. The \nanalysis utilizes this tactic explic\u00aditly when dealing with abstract intervals by, for instance, converting \n[i,j-1] into [i,j) and [i+1,j] into (i,j]. 5.10 Using imperfect provers In practice, of course, the \nanalysis uses an imperfect prover, rep\u00adresented by . *, which obeys the following partial completeness \nproperties: (.,.) . * . =. (.,.) .. (1) . . * . =. . ... (2) This prover may not be monotonic, i.e., \nthe following may not hold: * .* If . ...,then . . . =. .. .. Fortunately, the de.nition of the relation \n|=> has soundness in the face of partial completeness and non-monotonicity built-in. The cases in each \nsubsection are ordered by decreasing precision, with latter cases subsuming previous ones. If (.b. ,..) \nwould have been the subsequent state-assumptions pairing with a perfect prover, then there is at least \none subsequent state-assumptions pairing (.b*. ,..*) from the prover . * such that .b. ..b*. and (.b. \n,..) |=..*.  6. Worked example: Vertex arrays To build a better understanding of how LFA works, we \nll trace the analysis for the CPS-translated version of the vertex array code (Figure 4). This example \nin particular helps to illustrate the inductive interplay between the Inverse Propagation Rule and the \nIncremental Array Update Rule. Brie.y, the code works as follows: 1. Read in a vertex array from disk. \n 2. Read in the indices (into the vertex array) for a mesh, checking the safety of each index as it is \nread. 3. Emit the mesh, one vertex at a time.  LFA proves that when the function read-array exits, \nall of the entries within the array mesh are valid indices into the array vertices. This proves the array \naccess at the call to emitv is safe. As we trace, we ll highlight the relevant components of the state \n.band the key additions to the assumption base .. For this example, (alen e) desugars to (aget e #len).In \naddition, we re using a 1CFA contour set, not because it s required to prove safety, but because it allows \nus to avoid visiting only special (and perhaps misleading) cases of the rules presented for the analysis. \nWith a 1CFA contour set, the d succ function returns the current call site. Note that we ve labeled each \nrelevant call site in the example. We jump into the interpretation once LFA has reached the read-array3 \ncall site. This puts LFA into the following state: bb .b1 =([ (read-array3 mesh \u00b7\u00b7\u00b7)]], ve1,b\u00b51,t),where: \n\u00df1, bs1,bb veb1 =[...,([[verts]],t0) ..{.b0},([[mesh]],bt1) ..{.b1}] b sb1 =[b...1 .[...,#len .{pos}]] \n.0 .[...,#len .{pos}],.. .b0 ..b1 .b.1,([[mesh]],\u00b5b1 =[..., .1, .1,([[verts]],t0) .bt1) ..1] Next, we \nre in an apply state, applying the closure for read-array. b .b2 = (([[(. (a ilo hik) \u00b7\u00b7\u00b7)]],\u00df),b.2,...) \nbbc b.2 = .([[mesh]],t1),0,0,[[(alen ([[verts]],t0))]],clo. And, now, we re in an eval state, preparing \nto check whether we ve read in the entire mesh: (letrec ((read-verts (. (a k) ...)) ; Read vertices \ninto A. ... ; Reads vertex indices into A. (read-array (. (ailohik) (if (>=5 i (alen a)) k6 (. () (read-int7 \n(. (n) (if (<=:<8 lo nhi) (. () (aset!9 ain(. () (read-array11 a (+ i 1) lo hi k)))))) error12))))) ; \nEmit each vertex in a mesh. (emit-mesh (. (vrt msh i k) (if (>=13 i (alen msh)) k14 (. () (emitv15 (aget \nvrt (aget msh i)) (. () (emit-mesh17 vrt msh (+ i 1) k)))))))) (anew0 n(. (verts) (anew1 m(. (mesh) (read-verts2 \nverts (. () (read-array3 mesh 0 0 (alen verts) (. () (emit-mesh4 verts mesh 0 halt)))))))))) Figure \n4. A CPS translation of the vertex array code for rendering a3Dmesh. .b3 =([ (if (>=5 i (alen a)) ...)]] \n,...) After passing through the previous apply state, we picked up infor\u00admation for . 3: (=([ mesh]] \n,bt1)([ a]] ,bt3) ) .. 3 (= 0 ([[ i]] ,bt3) ) .. 3 (= 0 ([[ lo]] ,bt3) ) .. 3 (= (alen ([[ verts]] ,bt0) \n) ([[ hi]] ,bt3) ) .. 3 . Now we re in a conditional apply state: .b4 =([ if]] ,.[[ (>= ([[ i]] ,bt3) \n(alen ([[ a]] ,bt3) ))]] .,...) Because ( .b4,. 4) .(= 0 ([[ i]] ,bt3) ) and because there is more than \none point in the mesh: ( .b4,. 4) .(= #f (>= ([[ i]] ,bt3) (alen ([[ a]] ,bt3) ))). Hence, we don t \nhave to fork. This brings us to the apply state: b .b5 = (([[ (. () (read-int7 \u00b7\u00b7\u00b7))]] ,\u00df) ,...) . Now \nwe re in an eval state: .b6 =([ (read-int7 (. (n) ...))]] ,...) . And, this leads to the following apply \nstate: .b7 =([ read-int7 ]] ,.c clo.,...) . In the abstract, read-int returns {neg,0,1,pos}, which leads \nus to the following apply state: b b .b8 = (([[ (. (n) ...)]] ,\u00df) ,.{neg,0,1,pos}.,...,t7) . Next, we \nre in in the eval state, preparing to check whether the index we just read in is within the bounds of \nverts: .b9 =([ (if (<=:<8 lon hi) \u00b7\u00b7\u00b7)]] ,...) ,where veb9 =[ ...,([[ n]] ,b. t7) .{neg,0,1,pos}] \u00b5b9 \n=[ ...,([[ n]] ,bt7) ..1] . It helps to recall that: ( .b9,. 9) .(=([ lo]] ,bt3) 0) ( .b9,. 9) .(=([ \nhi]] ,bt3) (alen ([[ verts]] ,bt0) )). Now, we re in the conditional apply state: .b10 =([ if]] ,.[[ \n(<=:< ([[ lo]] ,bt3)([ n]] ,bt7)([ hi]] ,bt3) )]] .,...) The prover doesn t have enough information to \ndetermine whether or not this condition holds, so the analysis must fork. However, because \u00b5[[ (<=:< \n([[ lo]] ,bt3)([ n]] ,bt7)([ hi]] ,bt3) )]] = 1 , we can assume the condition holds on the true fork, \nand that it does not on the false fork. The false fork terminates quickly without touching verts,so \nwe follow only the true fork. By this state, we have: (<=:< ([[ lo]] ,bt3)([ n]] ,bt7)([ hi]] ,bt3) \n) .. 11. And, we re applying the true continuation: .b11 =([ (. () (aset!9 \u00b7\u00b7\u00b7))]] ,..,...) . And, then, \nwe enter the eval state for the array update: .b12 =([ (aset!9 ain \u00b7\u00b7\u00b7)]] ,...) . Finally, we ve reached \nthe point where we update the array: bbbc .b13 =([ aset!]] ,.([[ a]] ,t3) ,([[ i]] ,t3) ,([[ n]] ,t7) \n,clo.,...) When we transition from this state, LFA invokes the Incremental Array Update Rule. First, \nit looks for a fact .involving the binding ([[ n]] ,bt7) such that ( .,b. 13) ... Speci.cally, it looks \nfor a property fsuch that: ( .b13,. 13) .(forall .x,y1,...,yn.: .([[ n]] ,bt7) ,b.1,...,b.n.f). Searching \nthrough . 13 for ([[ n]] ,bt7) , the prover .nds: (<=:< ([[ lo]] ,bt3)([ n]] ,bt7)([ hi]] ,bt3) ), from \n. 11, which, importantly, is equivalent to the safety condition: (forall x: ([[ n]] ,bt7) (<=:< 0 x (alen \n([[ verts]] ,bt0) ))). Next, the prover tests for an interval starting at index 0 and adjacent to the \nindex ([[ i]] ,bt3) in the array ([[ a]] ,bt3) where the property f holds by testing: (forall k : Nb(forall \na: ([[ a]] ,bt3) (forall i : ([[ i]] ,bt3) (implies (<= 0 k (-i 1)) f[ (aget ak)/x] )))) Right now, this \ninterval is [0,-1] (empty), so this property holds triv\u00adially. Once found, the prover updates adjacent \nintervals to include this index. In this case, in . 13+1, the interval will become: (<= 0 ki). Looking \nforward, we ll eventually hit an application of the Inverse Propagation Rule in a recursive call to read-array,where \nwe increment the index into mesh. When the Inverse Propagation Rule is applied, the interval will be \ntransformed back into: (<= 0 k (-i 1)). This is exactly the precondition required for the next expansion \nof the range to succeed. The prover doesn t have to do induction explicitly, and yet induc\u00adtion s trademark \nsigns are apparent in this step. What s actually happening is that the .ow analysis is gradually breaking \nthe in\u00adductive proof into more manageable, .ow-speci.c, context-speci.c proofs. Next, we re in an apply \nstate for the continuation to aset!: b .b14 = (([[ (. () (read-array11 \u00b7\u00b7\u00b7))]] ,\u00df) ,..,...) Now, we re \nin an eval state for the recursive call to read-array: .b15 =([ (read-array11 a(+ i1) lo hik)]] ,...) \nNext, we re in the corresponding apply state: .b16 = (([[ (. (a ilohik) \u00b7\u00b7\u00b7)]] ,\u00dfb) ,b.16,...) ,where \nb.16 = .([[ a]] ,bt3) ,b.i++ ,([[ lo]] ,bt3) ,([[ hi]] ,bt3) ,([[ k]] ,bt3) .,and b.i++ =[[ (+ ([[ i]] \n,bt3) 1)]] We have the following new bindings: bb16 = .([[ a]] ,bt11) ,([[ i]] ,bt11) ,([[ lo]] ,bt11) \n,([[ hi]] ,bt11) ,([[ k]] ,bt11) . Previously, the bindings to these variables were made at time bt3,so \nwe suffer no precision loss from merging in vebyet.5 At this point, we re again in an eval state, preparing \nto check whether we ve .lled the array: .b17 =([ (if (>=5 i (alen a)) ...)]] ,...) As before, we have \nrelevant updates in . 17: (=([ a]] ,bt3)([ a]] ,bt11) ) .. 17 (=([ i]] ,bt11) (+ ([[ i]] ,bt3) 1)) .. \n17 (=([ lo]] ,bt3)([ lo]] ,bt11) ) .. 17 (= (alen ([[ hi]] ,bt3) ) ([[ hi]] ,bt11) ) .. 17 From this, \nwe see that the action of binding serves to chain the equality of identities formed within different \nenvironments. Now we re a conditional apply state: .b18 =([ if]] ,.[[ (>= ([[ i]] ,bt11) (alen ([[ a]] \n,bt11) ))]] .,...) This time, we cannot determine the truth of the condition, so fork\u00ading is inevitable. \nHowever, because the count of the condition is one, we can assert its truth or falsity on each branch. \nNext, we ll delve one state into the branch where the condition holds (that is, where we are done with \nread-array). After that, we ll switch back to the branch where the condition does not hold. In the case \nwhere the condition holds, we added to . 19: (.#f (>= ([[ i]] ,bt11) (alen ([[ a]] ,b = t11) ))) .. 19. \nNote that at this point, we can derive the following from ( .,b. 19) : b (forall k : N (forall a: ([[ \nmesh]] ,bt1) (implies (<=:< 0 k (alen a)) (<=:< 0 (aget ak) (alen ([[ verts]] ,bt0) ))))). That is, we \nhave proved that every index in mesh contains a valid index within verts. As this branch is about to \nreturn from read-array and enter emit-mesh, we ll switch back the false branch from the prior state. \nHaving switched back to the false branch, we re in an eval state: .b20 =([ (read-int7 (. (n) ...))]] \n,...) . To avoid tedious repetition, we ll skip straight ahead to the next apply state for aset!. . . \n. Jumping forward, we re in an apply state: bbbc .b22 =([ aset!]] ,.([[ a]] ,t11) ,([[ i]] ,t11) ,([[ \nn]] ,t11) ,clo.,...) Just as we did the last time execution reached aset!, we ll at\u00ad tempt a generalization \nover the array. This time, the prerequi\u00adsite to perform the generalization is not vacuously true: the \nrange [0,(-([[ i]] ,bt11) 1)] is non-empty. The prerequisite, however, for the array update rule is still \nderivable from ( .b22,. 22) . Conse\u00ad 5 If we were running with a 0CFA contour set, we would garbage collect \nall of these bindings to prevent merging, since all of them are unreachable. quently, the prover adds \nthe following to the next assumption base: (forall k : Nb (forall a: ([[ a]] ,bt11) (forall i : ([[ i]] \n,bt11) (implies (<= 0 ki) (<=:< 0 (aget ak) (alen ([[ verts]] ,bt0) )))))) Now, we ll jump ahead to \nthe recursive application of read-array. . . . After jumping forward, we re in an apply state: .b24 = \n(([[ (. (a ilohik) \u00b7\u00b7\u00b7)]] ,\u00dfb) ,b.24,...) ,where b.24 = .([[ a]] ,bt11) ,b.i++ ,([[ lo]] ,bt11) ,([[ \nhi]] ,bt11) ,([[ k]] ,bt11) .,and b.i++ =[[ (+ ([[ i]] ,bt11) 1)]] . We have the following new bindings: \nbb24 = .([[ a]] ,bt11) ,([[ i]] ,bt11) ,([[ lo]] ,bt11) ,([[ hi]] ,bt11) ,([[ k]] ,bt11) .. The variables \na, lo, hi and k are being rebound to themselves, so their bindings lose no precision. The new binding \nfor i,however, is not to itself the binding is to itself plus one. If we don t com\u00adpensate for this, \nwe ll have to throw out assumptions involving the binding ([[ i]] ,bt11) including those about which \nindices in mesh are safe. Fortunately, the binding ([[ i]] ,bt11) is eligible for abstract garbage collection, \ni.e., ([[ i]] ,bt11) .. Rb( .b24) . Hence, the Inverse Propagation Rule applies. Now, LFA is going to \nperform the following: 1. Replace ([[ i]] ,bt11) with inverse [[ (-([[ i]] ,bt11) 1)]] in . 24. 2. \nGarbage collect ([[ i]] ,bt11) in .b24. 3. Add the new binding for ([[ i]] ,bt11) in .b24.  What s \nimportant to us in this process is that it shifts the in\u00adterval in mesh whose entries are safe from [0,([[ \ni]] ,bt11) ] back to [0,(-([[ i]] ,bt11) 1)]. The causes the analysis to visit a state\u00adassumption base \npairing that it s already seen, and hence, it termi\u00adnate on this branch. Consequently, every path out \nof read-array leaves mesh satisfying the requisite safety condition.  7. Related work This work is embedded \nin the framework of abstract interpretation laid out by Cousot and Cousot [3]. In addition, the pruning \nop\u00aderation H used to move up the lattice of approximation for con\u00advergence is an instance of widening \n[4]. There are also relation\u00adships between LFA s propositions and Min\u00b4e s work in relational abstract \ndomains [12, 13]. The counting component \u00b5bis similar to the abstract reference counting by Hudak [7] \nin his static analysis of sharing, except that where his work counts references to, LFA counts concrete \ncounterparts to, as in G CFA [11]. The proposi\u00adtional abstract interpretation has connections to Ball, \net al. s work on predicate abstraction [2]. The .ow analysis portion of this work descends directly from \nShivers original work [17] and from earlier work on G CFA and . CFA [11, 10]. LFA has also been in.uenced \nby efforts in im\u00adproving speed and precision, such as work by Rehof, et al. [15] and Agesen [1]. Recently, \nMeunier, Felleisen and Findler [9] used a theorem prover in their work on a modular set-based analysis \nwith contracts. In that work, the prover is used to determine whether obligations have been met across \nboundaries. LFA differs in that it requires no contracts or user annotations. Given this, it would be \ninteresting to explore a hybrid modularized, contract-based logic\u00ad.ow analysis framework. The logic in \nthis work draws on Ebbinghaus, et al. s introduc\u00adtory text [5]. The method for the correctness of interacting \nwith the theorem prover is inspired by the approach taken in Nanevski, Mor\u00adrisett and Birkedal s recent \nwork [14], where they used the sound\u00adness of an assertion logic to safely employ a theorem prover in \ntype checking. The notion of a syntactic context descends from Felleisen s work [6]. The notion of an \ninverse context appears to be novel. Static analysis for the safety of array-bounds accesses is an old \nidea, and much progress has been made in the .eld as of late. Re\u00adcent work by Jia and Walker [8] has \neven begun working directly on pointers (as opposed to arrays) through the use of an Intuition\u00adistic \nLinear logic with Constraints. However, much of the work in the realm of verifying the safety of array-bounds \naccesses focuses purely on type systems, logics and theorem proving. LFA s mes\u00adsage is that these approaches \npick up more power when woven into an abstract interpretation. 8. Future work Given LFA s connections \nto relational abstract domains, fusing Min\u00b4e s work on weakly relational abstract domains [12] and oc\u00adtagon \ndomains [13] with the LFA framework presents a promising avenue for research. Future versions of LFA \nshould also generalize the basic value domain to an in.nite lattice, and introduce the ap\u00adpropriate widening \nand narrowing operations. Given that the choice of the set Time is also left open, it s easy to imagine \nmaking it an in.nite domain, and once again, applying widening and narrowing to achieve the appropriate \ndegree of polyvariance for the input program. Lastly, there is a wealth of work on shape analysis, which \ncould be brought to bear on improving the precision of handling abstract arrays. Acknowledgments I owe \nOlin Shivers a great deal of thanks for his original formula\u00adtion of higher-order .ow analysis, which \nI draw upon, and for his insights during our many discussions of the topic. The anonymous reviewers demonstrated \na complete mastery of the material with their exceptionally detailed comments, critiques and questions \nof the submitted paper. Much of their insight has made its way into this paper in the form of improvements \nto the framework and in\u00adcreased discussion. Those suggestions that I could not .t into this paper will \ncertainly be re.ected in future work. References [1] AGESEN, O. The cartesian product algorithm: Simple \nand precise type inference of parametric polymorphism. In Proceedings of ECOOP 1995 (1995), pp. 2 26. \n[2] BALL,T., MILLSTEIN,T., AND RAJAMANI, S. K. Polymorphic predicate abstraction. ACM Trans. Program. \nLang. Syst. 27, 2 (2005), 314 343. [3] COUSOT,P., AND COUSOT, R. Abstract interpretation: a uni.ed lattice \nmodel for static analysis of programs by construction or approximation of .xpoints. In ACM SIGPLAN Symposium \non Principles of Programming Languages (Los Angeles, California, Jan. 1977), vol. 4, pp. 238 252. [4] \nCOUSOT,P., AND COUSOT, R. Comparing the Galois connection and widening/narrowing approaches to abstract \ninterpretation, invited paper. In Proceedings of the International Workshop Programming Language Implementation \nand Logic Programming, PLILP 92, (1992), M. Bruynooghe and M. Wirsing, Eds., Leuven, Belgium, 13 17 August \n1992, Lecture Notes in Computer Science 631, Springer-Verlag, Berlin, Germany, pp. 269 295. [5] EBBINGHAUS,H.-D., \nFLUM,J., AND THOMAS,W. Mathematical Logic, 2nd ed. Springer-Verlag, New York, 1994. [6] FELLEISEN,M., \nAND HIEB, R. A Revised Report on the Syntactic Theories of Sequential Control and State. Theoretical \nComputer Science 103, 2 (1992), 235 271. [7] HUDAK, P. A semantic model of reference counting and its \nabstraction (detailed summary). In Proceedings of the 1986 ACM Conference on LISP and Functional Programming \n(Cambridge, Massachusetts, Aug. 1986), pp. 351 363. [8] JIA,L., AND WALKER, D. ILC: A Foundation for \nAutomated Reasoning About Pointer Programs. In European Symposium on Programming Languages (March 2006), \npp. 131 145. [9] MEUNIER,P., FINDLER,R. B., AND FELLEISEN, M. Modular Set-Based Analysis From Contracts. \nIn ACM SIGPLAN Symposium on Principles of Programming Languages (Charleston, South Carolina, January \n2006), pp. 218 231. [10] MIGHT,M., AND SHIVERS, O. Environment Analysis via .CFA. In ACM SIGPLAN Symposium \non Principles of Programming Languages (Charleston, South Carolina, January 2006), pp. 127 140. [11] \nMIGHT,M., AND SHIVERS, O. Improving Flow Analysis via GCFA: Abstract Garbage Collection and Counting. \nIn Proceedings of the 11th ACM SIGPLAN International Conference on Functional Programming (ICFP 2006) \n(Portland, Oregon, September 2006). [12] MINE\u00b4, A. Relational abstract domains for the detection of .oating\u00adpoint \nrun-time errors. In ESOP 04 (2004), vol. 2986 of LNCS, Springer, pp. 3 17. [13] MINE\u00b4,A.Theoctagonabstractdomain. \nHigher-Order and Symbolic Computation 19 (2006), 31 100. [14] NANEVSKI,A., MORRISETT,G., AND BIRKEDAL, \nL. Polymor\u00adphism and Separation in Hoare Type Theory. In ACM SIGPLAN International Conference on Functional \nProgramming (Portland, Oregon, September 2006). [15] REHOF,J., AND F\u00a8 AHNDRICH, M. Type-based Flow Analysis: \nFrom Polymorphic Subtyping to CFL-reachability. In Proceedings of the 28th Annual ACM Symposium on the \nPrinciples of Programming Languages (2001). [16] SHIVERS, O. Control-.ow analysis in Scheme. In Proceedings \nof the SIGPLAN 88 Conference on Programming Language Design and Implementation (PLDI) (Atlanta, Georgia, \nJune 1988), pp. 164 174. [17] SHIVERS,O. Control-Flow Analysis of Higher-Order Languages. PhD thesis, \nSchool of Computer Science, Carnegie-Mellon Univer\u00adsity, Pittsburgh, Pennsylvania, May 1991. Technical \nReport CMU\u00adCS-91-145. A. Conventions For all domains, we assume the natural meaning for the lattice \noperators . and . as well as the relation .; that is, a point\u00adwise lifting (for functions), or an index-wise \nlifting (for vectors and tuples). We assume implicit top . and bottom . element for domains lacking them. \nWhen a function is applied to an element outside of its domain, it yields .; thus, we get dom(f)= {x \n: f(x)=..}. The absolute value notation |x| should be read and interpreted as the abstraction of x. We \nuse boldface to denote vectors, i.e., d =.d1,...,dn..The function f[x1 .. y1,...,xn .. yn]is the function \nf except that when applied to xk, it yields yk. Operators are implicitly lifted point-wise over ranges \nfor functions; that is: if . :Y \u00d7 Y . Y and f,g : X . Y,then f . g = .x.f(x). g(x). For a tuple t=(a,b,...),we \nhave t=(ta,tb,...). The function free returns the set of free variables for a given piece of syntax. \nFor syntactic entities with lexically scoped bind\u00adings (such as .-calculus terms or logical formulae \nwith quanti.ers), the notation s[t./t]denotes the capture-avoiding substitution of t with t. in form \ns.  \n\t\t\t", "proc_id": "1190216", "abstract": "This work presents a framework for fusing flow analysis and theorem proving called <i>logic-flow analysis</i> (LFA). The framework itself is the reduced product of two abstract interpretations: (1) an abstract state machine and (2) a set of propositions in a restricted first-order logic. The motivating application for LFA is the safe removal of implicit array-bounds checks without type information, user interaction or program annotation. LFA achieves this by delegating a given task to either the prover or the flow analysis depending on which is best suited to discharge it. Described within are a concrete semantics for continuation-passing style; a restricted, first-order logic; a woven product of two abstract interpretations; proofs of correctness; and a worked example.", "authors": [{"name": "Matthew Might", "author_profile_id": "81309498719", "affiliation": "Georgia Institute of Technology", "person_id": "P767501", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1190216.1190247", "year": "2007", "article_id": "1190247", "conference": "POPL", "title": "Logic-flow analysis of higher-order programs", "url": "http://dl.acm.org/citation.cfm?id=1190247"}