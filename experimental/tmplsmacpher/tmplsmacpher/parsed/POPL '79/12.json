{"article_publication_date": "01-01-1979", "fulltext": "\n ;t Permission to make digital or hard copies of part or all of this work or personal or classroom use \nis granted without fee provided that copies are not made or distributed for profit or commercial advantage \nand that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, \nto post on servers, or to redistribute to lists, requires prior specific permission and/or a fee.&#38;#169; \n1979 ACM 0-12345-678-9 $5.00 SPACE-TIME TRADEOFFS FOR LINEAR RECURSION Sowmitri Swamy Coordinated Science \nLaboratory and Department of Electrical Engineering University of Illinois Urbana, Illinois 61801, USA \n Abstract A linear recursive procedure is one in which a procedural call can activate at most one other \npro\u00adcedural call. When linear recursion cannot be re\u00adplaced by iteration, it is usually implemented with \na stack of size proportional to the depth of re\u00adcursion. In this paper we analyze implementations of \nlinear recursion which permit large reductions in storage space at the expense of a small increase in \ncomputation time. For example, if the depth of recursion is n, storage space can be reduced tofi at the \ncost of a constant factor increaae in run\u00adning time. The problem is treated by abstracting linear recursion \ninto the pebbling of a simple graph and for this abstraction we exhibit the optimal space-time tradeoffs. \n1. Introduction Many high-level languages permit the use of recursion and hence allow linear recursion \nin which a procedural call can activate at most one other procedural call. Linear recursion is usually \nimplemented with a stack when a compiler either cannot or does not replace it with iteration. The size \nof such a stack will grow linearly with the depth of recursion and may in fact occupy much more storage \nspace than the procedure itself. In this paper we investigate a general method t.o re\u00adduce the storage \nspace required to implement lin\u00adear recursion at the expense of computation time. Our approach to this \ntradeoff issue is one intro\u00adduced by Paterson and Hewitt [1) and further analyzed by Chandra [2]. A schema \nfor linear recursion is given below in which F is the procedure variable, p is a unary predicate, h and \nf are unary functions, g is a binary function and each is uninterpreted. F(y): = Q p(y) then h(y) else \ng(y, F(f(y)) ) fi The semantics of such an expression are well under\u00adstood; F calls itself until the \npredicate i~s TRUE and the call sequence is then reversed and F is computed from inside out. We present \na simpLe (linear recursion) graph model of this sequence of steps and show that execution of linear recursion \nwith limited storage space can be abstracted as a !>pebble game on this graph. The pebble game models \n* This work was supported in part by the National Science Foundation under Grsnt MCS 76-20023. John 1%. \nSavage Program in Computer Science and Division of Engineering Brown lJniversity Providence, Rhode Island \n02912, USA register (or memcn-y) allocation, and the placement of a pebble on a node indicates that the \nvalue of the function at that node has been computed and placed in a register (or memory location). We \nfind the optimal space-time tradeoff for Pebblings of the linear recursion graph and state ~hese r~sults \nin an easily underst~od form. For example, if the depth of recursion is n and p pebbles are used, the \nnumber of moves required, Tp(n), has the following behavior for large n: d i/ P n (P/l+p) p << log2n \ni Tp(n) = K1 n log2n p = K2 log2n { log2n p >> log2n[ n log2p Here K1 is a function of K2. Thus , if \np is on the order of n~~k, Tp(n) remains linear in n which implies that a large decrease in space can \nbe achieved at the expens~ of a small increase in run\u00adning time. We also exhibit the class of algorithms \nwhich achieve these optimal space-time exchanges and show that at least one of these alg~rithms is easily \nimplemented. In the next section, we develop the grap~ model for linear recursion and in.che folluwing \nsection, Section 3, we derive the optimal algorithms and the statement of space-time exchange. In Section \n.4 we present a program implementation of partial stack u for linear recursion. algorithms Section 5 \nis concerned with two implementation issues, one of which is the determination of the amount of stor\u00ad \nage space that needs to be allocated to achieve a user or compiler space-time exchange. Paterson and \nHewitt [1] introduced the pebble game and demonstrated i!or linear recursion that for 1+1/k fixed temporary \nstorage, time grows as n . Chandra [2) demonstrated that the time must grow at least this rapidly under \nthese conditions even if counters are also avaiLable. He also gave some nonoptimal algorithms for linear \nrecursion which use small, medium, and large amounts of space. Our contribution is to simplify the analysis, \ndetermine the full class of optimal algorithms, derive in simple terms the space-time tradeoffs and show \nthat they can be achieved with some simple algorithms. We also observe that the prob lem studied by Paterson, \nHewitt, Chandra and us is really that of replacing stacks by partial stacks in which missing stack elements \nare recomputed. For this reason, the techniques of this paper are also applicable to some forms of recursion \nwhich are not strictly linear. The pebble game has been investigated by several authors. Hopcroft, Paul, \nand Valiant [3] have studied the pebble game on a graph on N nodes and show that they can be pebbled \nusing space O(N/logN), where the constant of proportionality depends on the maximum in-degree of nodes. \nPaul, Tarjan and Celoni [4] have shown the existence of arbi\u00adtrarily large graphs for which every pebbling \nstrategy uses space Q(N/logN), and Pippenger [5] has exhibited a graph onN nodes for which the time T \nand space S satisfy 22= log2 &#38;+ o(1) N so that more than linear time (in N) is necessary to pebble \nthe graph with space O(N/logN). Paul and Tarjan [6] have shown the existence of graphs on N nodes such \nthat reduction in S by a constant factor O(JR) causes T to expand from O(N) to 2 . Savage and Swamy [7] \nhave derived tight upper and lower bounds to the space-time exchanges for the FFT algorithm on n inputs \nwhich show that the space-time product is about 0(n2). These results are derived for pebblings of specific \ngraphs. However, Grigoryev [8] has de\u00adrived lower bounds to the product ST for n x n matrix multiplication \nmodulo-2 and for multiplica\u00ad tion of n-degree polynomials which are fl(n3) and Q(n2), respectively, and \nwhich apply to any straight-line algorithms for these problems. Tompa [9] has similar results for superconcentrators, \nwhich include bilinear algorithms for convolution, and matrix multiplication for special matrices, for \ngrates [10] and for the discrete Fourier Transform. 2. A Graph Model for Linear Recursion Given a linear \nrecursive procedure F and an input a, as defined above, F calls itself n times where n, the depth of \nrecursion, is the smallest integer such that p(f n)(a)) is True. (Here (n-l)(y)). f(o)(y) = y and for \nn > 1, f(n)(y) = f(f It follows that . F(f n)(a)) h(f(n)(a)) and in general for OS r < n F(f(r)(a)) = \ng(f(r)(a) F(f(r+l)(a))) , In a stack implementation, the depth of recursion is determined by successively \ncalling F until the predicate p is TRUE and pushing f r)(a) onto the stack for OS r < n-1. When f is \nan invertible function, we can compute f(r)(a) from f r+l)(a) by f(r)(a) = f 1 (f r+l)(a)) so that linear \nrecursion can be replaced by itera\u00adtion, as indicated schematically below. Y: =a while p(y) # TRUE do \ny : = f(y) od  z: = h(y) while y#adoy :=f-l(y); z := @ g(Y,z) F(a) :=z Thus , if f is invertible, as \nin the following example, then linear recursion can be realized in a fixed amount of space. Fat(n): = \nif n = O then 1 else n x Fac(n-l)fi However, f may not be invertible, difficult to invert or it may \nnot be clear to a compiler (or compiler writer) that it is invertible. We note that every partial recursive \nfunction can be realized by ALGOL-like programs which use linear recursion and elementary functions. \nWe consider methods for exchanging space for time that do not depend upon the specific inter\u00adpretations \ngiven to p, h, g, and f. Thus , these methods will compute functions defined by linear recursive procedures \nby simulating, perhaps with repetition, the computations carried out in a stack implementation. Figure \n1 shows a very simple directed acyclic graph Ln (called a chain) which is the basis for describing the \nsimulation of linear recursion. Node r, 1S r S n, corresponds to f(r)(a) by application of the function \nf, as in\u00addicated by the directed edge from node r to node r+l. In linear recursion the object is to compute \nthe items represented by the nodes in reverse order, namely in the order f n)(a), f(n-l)(a), f( -2)(a), \n. . . . f(2)(a), f(l)(a). In a stack implementation these items are stored in increas\u00ading order as they \nare computed, so they can be retrieved directly. However, if too much space is used by a stack, a partial \nstack can be retained in which intermediate stack results are saved. This requires the recomputation \nof results that have been discarded. We call such algorithms as partial stack algorithms. The space-tiine \ntradeoff problem is now abstracted as a pebble game on the chain L of n Figure 1. It is assumed that \nthe depth of recur\u00adsion n is known. (This is easily determined in fixed space, as indicated above for \nthe case of an invertible function f.) In the pebble game, pebbles are placed and removed on the nodes \nof the graph according to certain rules and when a node is pebbled this indicates that the function associated \nwith that node has been computed and the result has been placed in a register (or memory location). Any \ninput node can be pebbled at any time and a non-input node can be pebbled only when all nodes which have \nedges directed into that node have been pebbled. Pebbles can be removed at any time. We count only the \nmoves made to place pebbles on nodes. 3. An Optimal Pebbling Strategy The problem of computing a function \ndefined by a linear recursive procedure with depth of recur\u00adsion n has been reduced to the pebbling of \nthe nodes of the graph Ln of Figure 1 in the order n, n-1, n-2, ..,, 2, 1 while honoring the dependencies \nindicated by the directed edges. If p pebbles are allowed, we let T (n) denote the number of times P \npebbles are placed on nodes to complete this task. (Pebble removals are not counted. ) We note that if \nnode r has a pebble on it when it is to be visited, the visitation is not counted in Tp(n). The number \nof steps to visit as well as pebble is no more than Tp(n) + n-p. Consider the case in which one pebble \nis used to pebble Ln. This pebble must be placed successively on nodes 1,2 ,...,n to pebble node n. Once \nthis is done, the problem is reduced to pebbling the nodes of L in reverse order. Thus , n-1 Tl(n) = \nn + Tl(n-l) and since Tl(l) = 1, this reduces to Tl(n) = n(n+l)/2. The case in which two pebbles are \nused is more interesting and indicates how the general case should be approached. As two pebbles advance \ninto Ln there will be several points in time at which one pebble is on a node i and another on node j \nwhere j > i+l, that is, there is a gap between them. If the pebble on nodei is removed before it could \nbe used to make another advance intc~ the graph, then the pebbling is non optimal because over some portion \nof the graph the algorithm will have made use of only one of the two pebbles. This same argument applies \nto two adjacent pebbles on Ln when there are more than two pebbles, namely, in an optimal algorithm the \npebble on the nocle with lower index is not removed after a gap develc~ between them until it is used \nto make a subsequent advance. Consider a time-optimal pebbling of Ln with p pebbles, p< n. There will \nbe a point in time at which p pebbles are on Ln and such that the pebble on the node of lowest index, \nsay r, will be held in place while the remaining p-1 pebbles are used to pebble the subgraph of nodes \nr+l,r+2,.. .,n in reverse order. Since the pebble on node r is used for this advance, the problem is \nequivalent to pebbling Ln-r with p-1 pebbles. After nodes n, n -1, . . . . r+l and r are pebbled, the \np pebbles are used to pebble nodes 1,2, ...,1-1 in reverse order. Since r is chosen optimally, we have \nTp(n) = min (r+Tp(r-l)+Tp-l(n-r)) (1) l= r< n-p+l since r moves are necessary to bring a pebble to node \nr. We call this node a splitting node because it divides the pebbling problem into two subproblems. To \nsolve this recurrence we introduce a binomial number system. Given a positive integer p > 2 (to be interpreted \nlater as the number of pebbles), for each positive integer N there are unique non-negative integers m \nand A such that N=S -t-I,, o<lss -1 p-l,m p-2,m+l where The number system can be extended to the case \nP=l, which is important below, if we set S-l m = 1. s q,m  (:1) The uniqueness of these integers follows \nfrom the monotonicity of S p-l,m with m and the following identity s q,m+l = s q,m +s q-l,m+l (2) Then \nwhen p=l we have ,E=O and m=N. Als; s = p+l so if p ~ Nwe have m=l and L=N-l. p-1,2 Theorem 1: For all. \np ~ 1, the minimum number of placements of pebbles required to pebble the chain Ln of n 2 1 nodes with \nat most p pebbles, Tp(n), satisfies Tp(n) =~~ (m-l)S + m(,l,+l) (3) p-l,m where lSmand051i <s -1 are \nthe p-2,m+l unique integers such that n=S +,e (4) p-1.,m Proof: The proof is by $ndu~t+on m n and p. \nBasis: ~) The case of p=l, namely T1(n) = rL(n+l)/2 = m(mi-1)/2 has been established above which agrees \nwith (3). b) For p ~ n, Ln can be completely pebbled in n moves so Tp(n) = n. Alao, m=l and j=n-1 in \nthis case, which agrees with (3). The basis states expressions for Tp(n) on those boundaries which are \nshown in Figure 2. Inductive Hypothesi~: If Tp(n) is given by (3) for alllsn<pwhenp< P-1 and forl SnSN-1 \nwhen p=P, then TP(N) is also given by (3). Figure 2 also shows the order in which the induction sequence \nis carried out. We now state the conditions under which the minimum of equation (1) is achieved. Let \nG(r) = r +Tp(r-l) +T P-l(n-r) (5) Then. Tp(n) = rein G(r) (6) l= r< n-p+l Consider the forward difference \n7G(r) = G(r+l) -G(r) = 1 +VTp(r-l) -VT P-l(n-r-l) (7) Then, the minimum in (6) is achieved at a value \nof r such that vG(r) > 0. Therefore, we further evaluate vG(r). Since 1< r < n-p+l and p ~ 2 we invoke \nthe inductive hypothesis and use (3) to evaluate the forward differences in (7). TO do this we let (u,h) \nwhere u> O, 0< h< SP-2,U+1-1 and (v,i) where v>0,05 i:< S~-3,v+1-1, be the unique pairs of integers \nsuch that r.s +h p-l,u n-r = s -l-i (8) p-2,v Clearly, from (4) we have s +1=s +s + h+i (9) p-l,m p-l,u \np-2,v which will be used later. The forward difference VTp(r-l) is easily seen to be u when h > 1 (and \nr-1 k SP-l,u) and can also be shown equal to u when h = O by straightforward manipulation of binomial \ncoefficients. Similarly, vTp-l(n-r -1) is equal to v. Summarizing we have vTp(r-l) = u, 05h<S -1 p-2,u+l \nvTp-l(n-r-l) = v, O<i<s -1 p-3,v+l We will also encounter the forward differences vTp(r-2) and VT P-l(n-r) \nand we have (u h>O vTp(r-2) = ( ( u-1 h=O i=s (V+l -1 p-3,v+l VT ~-l(n-r) = ~ i<S -1 b p-3,v+l as a \ndirect consequence of the above analysis. From these observationa and (7) we have that vG(r)=l+u-v (lo) \nand since vG(r-1) = 1 + Tp(r-2) -T P-l(n-r) we have u-v h>O, i=S -1 (ha) p-3,v+l l-l-u-v h>O, i<S -1 \n(llb) VG(r-1) = p-3,v+l u-v-1 h=O, i=s -1 (llC) p-3,v+l u-v h=O, i<S -1 (lId) p-3,v+l I As indicated \nabove, the set of integers {r] which minimize (6), the optimal splitting nodes, satisfy VG(r) 2 0 although \nnot all such integers minimize this expression. We consider two classes of integers which minimize (6) \nand satiafy VG(r) > 0, name ly A = {r,r+llvG(r) = 0] B = {rIvG(r) 2 1, vG(r-1) S -1] The integers for \nwhich VG(r) z 1 and VG(r-1) > 1 do not minimize (6) while those for which vG(r) 2 1 and vG(r-1) = O fall \ninto A. Consider r e A such that vG(r) = O; then from (lo) VG(r)=l+u-v=O or v = u + 1. From (9) and the \nidentity (2) we have s + j = Sp-l,u+l + (h+i) p-l,m and since OS h+is SP ~,u+l-l + s -1=s -2 p-3>v+l \np-2,u+2 we conclude that u = m-1, v = m, ()< h+i=LSS -2 (12) p-2,m+l Consider next the case of r e B; \nhere we have U-V ~ O and VG(r-1) < -1 and the only case for which both conditions hold is (llc). This \nrequirea u=v, h=O,i=S p-3,v+l-l and from (2) and (9) we have s +.8=s +s -1 p-l,m p-l,u + p-2,u p-3,u+l \n=s -1 p-l,u sp-2,u+l from which it follows that u =m, v =m, J = S -l, h=O,i=S p-2,m+l p-3,m+l-1 (13) \nThus , if J = Sp.2,m+1 -1 there is exactly one value for r that minimizes G(r), namely, r = S p-l,m while \nifO<A<S -2 then the minimizing p-2,m+l value of r satisfies S Sr5S -1. p-1,m-1 p-l,m It remains to use \nthe minimizing values for r in (5) to show that the minimum is indeed Tp(n). This task is left to the \nreader. o We extract some additional information from this theorem that will facilitate the construction \nof a partial stack algorithm for linear recursion. Corollary. If r. is a splitting node of L n then it \nsatisfies the following conditions: Case a) For S <n~s -2 p-l,m p-l,m+l s p-1,m-1 s O% p-l,m-l s 5n-r. \nSS -1 p-2,m p-2 ,m+l Case b) If n=S -1 p-l,m+l then -1 O = p-l,m and -rO = p-2,m+l We now identify a \nsingle, simply computed integer r which is the label of a splitting node 1 of L n Lemma 1. The integer \nrl defined by n-S +1) (14) = max(Sp-l,m-17 1 p-2,m+l satisfies the conditions of the above corollary. \nProof The conditions of the corollary can be . stated as bounds on r when S o p-l,msn= P-l,M+l-2S as \nshown below. s sro<S -1, p-1,m-1 p-l,m n-S +l~ro <n-S p-2,m+l p-2,m It is easy tO demonstrate that rl, \nthe larger of the two lower bounds satisfies both upper bounds. When n = S -1, n-S +1=s so that p-l,m+l \np-2,m+l p-l,m =s , which is the optimizing value of r 1 p-l,m o in this case. o We now examine a number \nof implementation issues. 4. Partial Stack Algorithms The space-time exchange that will be obtained from \na partial stack algorithm will be determined by the number of temporary stack locations p available and \nthe depth of recursion n. We post\u00adpone until another section a discussion of this exchange and present \nhere a program for the realization of an optimal partial stack algorithm. Our program will use the rule \ngiven in Lemma 1 for the selection of a splitting node. The recurrence of equation (1) defines a partial \nstack algorithm when 2S p< n-1. It If m=l, then S =1 and S -l=p so p-1,1 p-1,2 consists of pebbling up \nto a splitting node r with 1 I<d<p and the full stack algorithm is used. [ a pebble being left on this \nnode followed by a pebbling of nodes r1+l,r1+2,. ..,n in reverse order with p-1 pebbles and then a pebbling \nof nodes l,2,...,r1-1 with p pebbles. If p = 1, the single pebble strategy is used to pebble nodes 1,2,. \n..,n in reverse order. If p 2 n; a standard stack algorithm is optimal. The partial stack algorithm that \nuses rl~ as a splitting node where = max(S n-S +1) (14) 1 p-1,m-1 p-2,m+l is given below in pseudo-ALGOL. \nThe determination of p, the size of the partial stack and the calcu\u00adlation of integers m and 1 are discussed \nin the next section. Observe that once the depth of re\u00ad cursion n is known f n)(a) and h(f n)(a)) :ire \navailable so the problem reduces to computing f(n-l)(a) in reverse order or f(l)(a),f 2)(a),. ... to \nthe pebbling of Ln-l. Comments are shown in brackets. [Determine the depth of recursion,.n, and com\u00ad \npute h(f n)(a) ).] =a; n:=o Y: while NOT(p(y)) do y: = f(y); n: = n+l od = h(y); d: =~1 [De~~rmine \np; compute M, and Sp-l,m SUCh that Sp-l,m= d= Sp.l,m+l -1.1 S:=s p-l,m [Observe that Sp-l,m+I = (m+p)s/m. \n] [Use the single pebble strategy if p=l,, a full! \\stackif p~d and thepartial stack algorithm~ ! otherwise. \n!.. ~ P=l then call SPL(d, a, F) ~ ~>d then cm STK(d,, a, F) == !-PS1 K@,a,m,p,s,F) fi fi [:~b~;l~owing \nprocedure computes F with one Procedure SPL(d,a,F) w i: = d. ,z:=a while i#Odoj: = i; while j +()* z: \n= f(z); j: = j-1. od; i: = i-1; F: = G(z,F~@ F: = G(a,F), end [Th~ollowing procedure computes F with \na complete stack.] Procedure STK(d,a,F) @@l i: =d-1; z: =a while i # O do z: = f(z); PUSH(z); i: =i.1 \nOd z: = f(z); i: = d-1; F: = G(z,F) while i # 0~ z: = POP; F: = G(z,F); i: = i-l Q F: = G(a,F) end ~cedure \nPSTK(d,a,m,s,p,F) [s= d< (m+p)s/m-1] J?Saw . [If p=l,z~he single pebble algorithm is optimal.] if p \n= 1 then call SPL(d,a,F); return fi if m=l then call STK(d,a,F); return fi , . The splitting node r=max(s \n,d-S +i p-1,m-1 p-2,m+l >s 1 when m>2. If r=S then -p-l,m-l p-1,m-1 Is <r-l<s -1 and if r>S I p-l,m-2 \n p-1,m-1 p-1,m-1 then S <r-l~s -1. In either case p-l,m-l p-l,m ~sP-2,m~ -r~~Sp-2,m+l-l. We use the identi\u00adties \nS =s(m-1)/(m+p-1), S p-1 ,m-1 p-2,m+l = splm, S = sp/(m+p-1) and S p-2,m p-l,m-2 = ((m-2)/ (m+p-2) \n)Sp-l,m-l 1 s.L:=s(m-1)/t(m+p-l); su:=sp/(m+p-l);m.P,:=m-l; PU:=P-1: u::=d-s(p/m)+l: ~sjz.u then r:=s,t; \nf~=m~l; sj:=s,t(mj)/(mMp) s-r : u i:=r;d :=r-l; du:=d~; [dl~O, du~ 1] [Pebble up to the splitting node.] \nwhile i # O do z:=f(z); i:=i-1 od  Nodes r+l, followed are r+z,...,n  Pebbled by nodes 1,2, ..., r+l \nunless r=l. l_ 1 call PSTK(du, z, m, SU, pu, F); if dJ=O then f:=G(a,F) else call  ~TK(dj, a, ml,, \ns1, p, F) fi end . The number of times that this algorithm computes function r)(a) for some l_ <r<n \n_ is n+Tp(n-l) af because the first loop that computes n computes each of these functions once and the \nremaining portion of the program pebbles the graph corre\u00ad sponding to the functions f l)(a), f(2)(a), \n..., f(n-l) (a) , since f n) (a) and h(f(n) (a)) are com\u00ad puted by the first loop. For each computation \nof f(r)(a) for some Isrsm-1, there is a fixed upper bound on the number of additional functions that \nare computed, assignments that are made and tests that are performed. In the next section we examine \nthe dependence of Tp(n) on p and n. We also examine ways to compute m, J and S from n. This information \np-l,m can be used to compute p based upon user or com\u00adpiler defined criteria. 5 . Implementation Issues \nThe number of moves required to pebble Ln with p pebbles, Tp(n), is given in Theorem 1 and is seen to \nbe a linear fufiction of n for values of n between S for m =l,2,3, . . . . Atn=S p-l,m p-l,m we have \nTp(n) = A (m-l)n (15)p+l Whenp=2, n =(m+l)m/2, m~fi and 3/2 Tp(n)~ (~/3)n for large n. Also, when m=3, \nn=(p+2)(p+l)/2, p~fin and Tp(n) ~ 2n 139 for large n. If m=4 the coefficient of n is in\u00adcreased to 3 \nand p is reduced to about ~. These results demonstrate the strong reduction in space that can be achieved \nat the expense of a small in\u00adcrease in running time. From (15) it is clear that the size of m largely \ndetermines the gap between n and Tp(n) since p/(p+l) lies between 2/3 and 1 when p>2. Thus , if m is \nto be bounded from above this will deter\u00admine p if n is known. On the other hand, we may wish to bound \np at the expense of m or perhaps attempt to achieve a balance between p and m. Thus, we explore the relationships \nbetween m, p and nwhenn=S p-l,m The function m+p -1 n. (16) P [) is symmetric in p and m-1 and monotone \nincreasing in both. Thus > it is easy to show from the following inequalities [10, p. 530] that the smaller \nof p and (m-1) is no larger than 2 log2n when n>4. (Here, N=m+p-1)  Here l~k~N-1, N~2 and H(x) is the \nentropy function. H(x) = -x log2 x -(1-x)log2(l-x) (18) Since n is one term in the binomial expansion \nof m+p -1 (1+1), we have m+p -1 n<2 (19) or that the sum m+p-1 is at least log n. FuKther\u00ad2 more, from \n(17) if m and p a~e comparable in size and n is large, it follows that they are both comparable to log \nn. Thus, we consider three 2 cases when n is large s m<<p m << log2n and p<<m ~ p << log2n p comparable \nto m -p/(m+p-l)=L, O<h<l .. and examine Tp(n). we have (m+p-l) (m+p-2)---(m) > m p n= () P: -:, which \nis also a good approximation when p << m. This implies llp m<pn and by the symmetry of n in (m-1) and \np we have l/(m-1)p+l ~ (m-l)n which is a good approximation when m << p. Re \u00adworking this equation we \nhave log2n 10g2n . (m-l) 5 log2p -10g2(m-1) log2p and the approximation holds when m << p. Since m << \np if p ~~ log2n (note that m+p-l~log2n), we have (2 1+1/p v p << log2np+l Tp(n) ~, , n log2n L. p > \nlog2n %2P In the remaining case, when p is proportional to log2n, we use (17) to approximate n. If for \nO<k<l and if n is large, then taking loga\u00ad rithms we have log2n ~ (m+p-1) H(L) P* which implies that \np is proportional to log2n. Then, l-h Tp(n) ~ n log n H(h) 2 when .~ p = H(k) 10g2n The expressions i/H(h) \nand (l-h)/H(k) are shown in Figure 3. Summarizing, we find that Tn(n) grows as l+l/p for p Small, nlog \nn/10~ p fOr P pn > log2n and as nlog n if k is neither near zero (p << m) nor near l(m << p), that \nis, for p proportional to log2n. The three different rates of growth of Tp(n) with n can be selected \nby choosing p as a function of n which grows more slowly than log2n, such as ~~ or a constant, more rapidly \nthan 2 1/5 log2n, such as (log2n) orn, or which grows in proportion to log2n, respectively. The actual \nvalue of p may also be determined by an upper limit on temporary storage space. Once p is chosen, the \nnext step is to determine m and S such that p-l,m s <n<s (20) p-l,m p-l,m+l By a previous argument, \nthe smaller of p and m-1 is no larger than 2 log2n for n~4, hence S p-l,m can be computed in at most \n8 log2n multiplications or divisions from one of the following two expressions : (m+p-l)(m+p-2)---(m)_ \n p-l,m = s P: (m+p-1) (m+p-2)---(p+l) m! In fact, many fewer multiplications may suffice if either \np or m are very small. To compute m, start m at n (note that S = m) and use binary search o,m by halving \nm until (20) is satisfied. This will take 0(log2n) steps so the entire process can be done in 0(log2n) \nsteps. 6. Conclusions May 1-3, 1978. Linear recursion has been modeled as a pebble game on adirected \ngraph which is asimple chain of n nodes, n the depth of recursion. We have exhibited the class of optimal \npartial-stack algorithms for this pebbling problem and have derived simple explicit expressions for the \ntime-space tradeoff for linear recursion. We have also given a simple program for implementing an optimal \npartial stack algorithm and we have studied the asymptotic be\u00adhavior of the time-space tradeoff function \nfor the purpose of providing rules for selection of the amount of space that should be used to achieve \nvarious degrees of performance. 10. 11. , Gallager, R. G., Information Theory and Reliable Communications, \nJohn Wiley and New York, 1968. A. J. Guttman, Prograrmning Recursively fined Functions in FORTRAN Intl. \nJournal of Computer and Info. Sciences, Vol. 5, 2, 1976. Sons, De-No. While practice, example, recursion \nthen F is linear other if H and not recursion occurs frequently in forms of recursion do also. For is \na procedure defined by linear F is the procedure defined below linear. F(y): = if p(y) then h(y) else \nG(H(y), F(f(y))) However, above can the be partial used to stack realize algorithm H and F described \nif individual stacks said of are used for other forms each. Also of recursion. the same It is could not \nbe known, however, or near whether optimal this approach programs. will yield optimal References 1. Paterson, \nM. S. and C. E. Hewitt, Schematology, Proj. MAC Conf. on Systems and Parallel Computation, MA, pp. 119-127, \nJune 2-5, 1970. Comparative Concurrent Woods Hole, 2. Chandra, A. K., Efficient Linsar Recursive Programs, \nRC4517, 10 pp. August 29, ceedings of the Fourteenth on Switching and Automata October 15-17, 1973. Compilation \nof IBM Research Rept. 1973 and the Pro-Annual Symp 0 s l.um Theory, pp. 16-25, 3. Hopcroft, J. E., W. \nJ. On Time Versus Space, pp. 332-337, 1977. Paul, JACM, and Vol. L. G. 24, Valiant, Nc). 2, 4. Paul, \nW. J., R. E. Tarjan, and J. Space Bounds for a Game on Graphs, Ann. Symp. on Theory of Computing, pp. \n149-160, May 3-5, 1976. R. Celcmi, Ei.@th Hershey, -  PA, 5. Pippenger, preprint, N., May A Time-Space \n1977, to appear Tradeoff, in JACM. IBM 6. Paul, W. J. and R. E. Tarjan, Time-Space Trade\u00adoffs in a Pebble \nGame,{! preprint, May 1977 and Fourth Colloquium on Automata, Languages and Programming g, Turku, Finland, \n1977. 7. Savage, J. on the FFT 1978 issue E. and S. Swamy, Space-Time Algorithm,ll to appear in the of \nthe IEEE Transactions on Tradeoffs Sept. Infor\u00ad mation Theory. 8. Grigoryev, D. YU, An bility and Independence \nLower Bounds of Circuit Scientific Seminars, Leningrad Branch, Vol. Application of Separa-Notions for \nProving Complexity, Notes Steklov Math. Inst., 60, pp. 38-48, 1976. of 9. Tompa, M., Time-Space Tradeoffs \nfor Computing Functions, Using Connectivity Properties of Their Circuits, Proceedings of the Tenth Annual \nACM Symp osium on Theory of CompuC;~&#38;, FIG. I THE CHAIN Lo FOR n=7 5 4 p3 2 ,1 1 I 12 I I 34 n I \nI 5 FIG. 2 BOUNDARIES SEQUENCE AND INDUCTION FOR THEOREM 1. 2.0 1.8 1.6 I .4 1.2 I ,0 .8 .6 ,4 .2 c O \n.1 .2 .3 I .4 I .5 I .6 I .7 I .8 I ,9 I 1.0 FIG. 3 THE FUNCTIONS ~/H(k) m-AND (i- A)/ FI(k) \n\t\t\t", "proc_id": "567752", "abstract": "A linear recursive procedure is one in which a procedural call can activate at most one other procedural call. When linear recursion cannot be replaced by iteration, it is usually implemented with a stack of size proportional to the depth of recursion. In this paper we analyze implementations of linear recursion which permit large reductions in storage space at the expense of a small increase in computation time. For example, if the depth of recursion is n, storage space can be reduced to &radic;n at the cost of a constant factor increase in running time. The problem is treated by abstracting linear recursion into the pebbling of a simple graph and for this abstraction we exhibit the optimal space-time tradeoffs.", "authors": [{"name": "Sowmitri Swamy", "author_profile_id": "81100559802", "affiliation": "University of Illinois, Urbana, Illinois", "person_id": "PP39048550", "email_address": "", "orcid_id": ""}, {"name": "John E. Savage", "author_profile_id": "81100570572", "affiliation": "Brown University, Providence, Rhode Island", "person_id": "PP15036498", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/567752.567765", "year": "1979", "article_id": "567765", "conference": "POPL", "title": "Space-time tradeoffs for linear recursion", "url": "http://dl.acm.org/citation.cfm?id=567765"}