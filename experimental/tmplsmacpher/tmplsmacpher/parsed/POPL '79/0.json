{"article_publication_date": "01-01-1979", "fulltext": "\n Permission to make digital or hard copies of part or all of this work or personal or classroom use is \ngranted without fee provided that copies are not made or distributed for profit or commercial advantage \nand that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, \nto post on servers, or to redistribute to lists, requires prior specific permission and/or a fee.&#38;#169; \n1979 ACM 0-12345-678-9 $5.00 A COMPACT, MACHINE-INDEPENDENT PEEPHOLE OPTIMIZER Christopher W. Fraser \nDepartment of Computer Science The I-!niversity of Arizona Tucson, Arizona 85721 Abstract Object code \noptimizers pay dividends but are usually ad hoc and machine-dependent. They would be easier to understand \nif, instead of performing many ad hoc optimi\u00adzation, they performed a few general optimizations that \ngive the same effect. They would be easier to implement if they were machine-independent and parametrized \nby symbolic machine descriptions. This paper describes such a compact, machine\u00adindependent peephole optimizer. \nIntroduction Of all optimizations, those applied to object code are among the least-understood. Ad hoc \ninstruction sets complicate elegant treatment and portability, diverting research to higher-level, machine-independ\u00adent \nglobal optimization. However, experi\u00adence shows the value of object code opti\u00admization; even the BLISS1l \ncompiler [Wlf], with thorough global optimization, reduces code size by 15-40% with object code optimization. \nExamining compiler design shows why. To be machine-independent, global optimiza\u00adtion usually precedes \ncode generation; to be simple and fast, code generators usually operate locally; so the code generator \nproduces, perhaps, locally optimal fragments, but these may look silly when juxtaposed. For example, \nlocal code for a conditional ends with a branch; so does local code for the end of a loop. So a conditional \nat the end of a loop becomes a branch to a branch. Correcting this in the code generator complicates \nits case analysis combinatorially, since each combination of language features may admit some optimization \nCHarrison]. It is better to simplify the code generator and optimize object code. Consequently, object \ncode optimization and its problems --a This work was supported in part by the National Science Foundation \nunder contract ~~CST8-oz54~, machine-dependent, informal nature -\u00addeserve attention. Background Little \nhas been published on object code optimization, and many early object code optimizations [Bagwell, Lowry, \nMcKeemanl (eg, constant folding, exponentiation via multiplication) are now performed at a higher level \n[Allen, Standish]. A notable exception is redundant load elimination, the poor man s global register \nallocation; many code generators simulate register contents to do this and to replace, where possible, \nmemory references with equiva\u00adlent register references. FINAL optimizes the object code generated by \nthe optimizing BLISS1l compiler [Wulf]. FINAL collects several effective but ad hoc optimizations: deleting \ncomparisons that are unnecessary because a previous instruction incidentally set the condition code; \nexploiting special case instructions and exotic address calculations; coales\u00adcing chains of branches; \nand deleting un\u00adreachable code. The optimizer described below complements the more ambitious FINAL by \nconcentrating on one general optimiza\u00adtion; it sacrifices a little code quality for simplicity and machine-independence. \nComparisons with FINAL quantify this trade-off. Overview PO is a compact, machine-independent peephole \noptimizer.. Given an assembly language program and a symbolic machine description, PO simulates pairs \nof adja\u00adcent instructions and, where possible, replaces them with an equivalent single instruction. PO \nmakes one passto determine the effect of each instruction, a second to reduce pairs, and a third to replace \neach instruction with its cheapest equivalent. optimizers, PO is organized in a simple Further, all instructions \nare fully manner and is easily retargetted by decoded: mxn pairs of patterns are used to changing machine \ndescriptions. Moreover, define m instructions with n addressing it is not cluttered by ad hoc case variants \neach. PO scans the instruction analysis because it combines all possible list in order, so the person \nwho describes adjacent pairs, not just bran~chains or the machine should decide which instruc\u00adconstant \ncomputations or any other special tions are cheapest and put them first. cases. Compared with conventional \nobject code optimizers, PO s effect can be Since PO knows target machines only described especially concisely: \nthrough these patterns, it is retargetted by supplying a different instruction set. PO replaces each \npair of adjacent Its few machine-dependencies are assump\u00adinstructions with an equivalent tions built \ninto its algorithms and single instruction, if possible. machine description language. For example, the \nsimulator assumes that the machine PO replaces each instruction with uses a program counter and that \nvariables, its cheapest equivalent. once set, stay set; PO cannot optimize code that uses changing device \nregisters. Later sections explain adjacent , equiv-PO assumes that one instruction is better alent , \nand cheapest . than two; adding instruction timings to machine descriptions would correct this. The two-instruction \nwindow catches ineffi-Finally, instructions with internal loops ciencies at the boundaries between frag-(eg, \nblock moves) are hard to describe in ments of locally-generated code. It the language above. In general, \nsuch misses many others, so PO is best used assumptions are removed by extending PO. with a high-level, \nmachine-independent As it stands, PO optimizes a useful class global optimizer. of assembly language \nprograms. Nachine descriptions Determining the effects of instructions  To simulate an instruction, \nPO must know Initially, PO determines the effect of its syntax and its effect. Examples each assembler \nstatement in isolation illustrate the notation, which is based (so PO assumes that programs do not modify \non Bell and Newell s ISP [Bell]. The themselves) . Given an assembler statement, example below defines \none of the PDP1l the simulator seeks a matching assembler CLR instructions, which clears ( + O ) the \nsyntax pattern and returns the correspond\u00admemory cell ( MC. . .1 ) addressed by a ing register transfer \npattern, with register ( ll[dl , where d is a register pattern variables evaluated. For example, index \nfrom an instruction field) . It the instruction also sets the condition code (N and Z bits) . ADD {12,R3 \n CLR @Rd MIR[dIl + O; N + O; Z + 1 matches the syntax pattern The first column gives an assembler ADD \nk.,ltd language syntax pattern wi~h lower case pattern variables ( d ) for variable so PO substitutes \n2 for s and 3 for d in fields, The corresponding pattern in the register transfer pattern the second \ncolumn describes the effect of R[d] +R[d] + S; N+R~cl]+s < (); Z+R[d]+s = (Jthe instruction using these \nvariables. PO assumes that the program counter is automatically incremented, so this needn t and obtains \nbe made explicit. Other details irrele\u00ad R[31+R[31 + 2; N-R[3]+2 < O; z+R[3]+2 = Ovant to the object \ncode (eg, setting the carry bit) are omitted for conciseness. Programs typically ignore some effects \nof This second example defines the PDP1l some instructions. For example, a chain EEQ instruction, which \nbranches ( PC + l ) of arithmetic instructions may set and if the Z bit is set ( Z + ). Read + reset \ncondition codes without ever testing as implies . them. PO can do a better job if such useless register \ntransfers are removed BEQ 1 Z+pc+l from an instruction s register transfer list. For example, the full \neffect of the PO assumes that PC names the machine s instruction above includes assignments to program \ncounter. the N and Z bits. If the next instruction changes N and Z ~-ithout testing them, its To simplify \nthe simulator, all register useful effect is just transfers occur simultaneously, so each R[31 +R[3] \n+2reference to R[d] refers to its initial If the previous instruction references R[31 indirectly, the \nuseful effect may be had by auto-incrementing instead and removing the ADD instruction (auto-incre\u00admenting \nis a PDP1l address calculation that references indirectly through a register and then increments the \nregister). The full effect requires the ADD instruc\u00adt ion, since auto-incrementing does not set the condition \ncode. Consequently, when initially determining each instruc\u00adtion s effect, PO ignores effects on such \ndead variables. TO do this, the initial pass scans the program backwards and associates with each instruction \nits useful effect and a list of variables that are dead from that instruction forward. Each instruction \ns list is that of its lexical successor, plus the variables it sets, minus the variables it examines. \nIf the instruction branches, its list is just the variables it sets without exam\u00adining, since. those \nthat are dead from the branch on depend on where it jumps. Full dead variable elimination (considering \ncontrol flow and subscripted variables) [Hecht] is an unnecessary expense; this simpler analysis permits \nthe first pass over the code to eliminate most extra effects such as condition code setting. As a bonus, \ncode not subjected to dead variable elimination at a higher level enjoys a measure of it now: instructions \nwith no effect are removed. Choosing instruction pairs Once the initial pass determines the iso\u00adlated \ne~fect of each instruction, PO passes forward over the program and considers the combined effect of lexi\u00adcally \nadjacent instructions; where possible, it replaces such pairs with a single instruction having the same \neffect. PO learns a pair s effect by combining their independent effects and substituting the values \nstored in variables in the first for instances of those variables in the second. The effect of ADD #177776,R3 \ncm @R3  is (ignoring dead variable elimination) R[31 + R[3] + 177776; N + R[31+177776 < O; Z + R~3]+177776 \n= O M[R[311 + O; N+ O; Z+ 1 which simplifies to R[~]++oR[j]++1177776; M[R[31 + 1777761 + O; ; PO now \nseeks a single instruction with a register transfer pattern matching this effect. It finds the auto-decrement \nversion of CLF! CLR -(R3). A register transfer pattern matches if it performs all register transfers \nrequested and if the rest of its register transfers set harmless dead variables (eg, the condition code). \nAfter each replacement, PO backs up one instruction --to consider the riew adjacency between the new \ninstruc\u00adtion and its predecessor --and continues. It is harder to combine pairs that start with a branch. \nThe combined effect of Z+PC+L1 Pc + L2 L1 :  is Z+PC+-L1; --z+Pc+L2 L1 :  or just -z+pci-L2 L1 :  \nWhen PO combines instructions, i~ treats assignments to the PC as a special case, adding inverted relational \nand removing useless assignments to the PC. Labels prevent the consideration of some pairs . Combining \npairs whose second instruction is labelled changes, erroneous\u00adly, the effect of programs that jump to \nthe label to include the effect of the first instruction. PO must ignore such pairs and assume that all \nbranches are to explicit labels. To improve its chances PO removes any labels it can. When it encounters \na label, it looks for a refer\u00adence to it; if it finds none --possibly because optimization~ like the \none above have removed them all --PO removes the label and tries combining the two instruc\u00adtions that \nit separated. This enabled PO to remove the last three branches in the large example in the appendix. \nWhen PO removes the last reference to a label that it has passed, it could back up to reconsider the \ninstructions the label separated: new optimizations are possible with the label gone. This happens only \nwith labels referenced after their definition. However, when optimizing code generated locally from a \nprogram with structured control flow, loop and sub\u00adroutine heads are the only such labels, and PO seldom \nremoves these. So backup was discarded as an excessive generality. Branches make extra pairs. If an instruc\u00adtion \nbranches to L, PO simulates it with instruction L and replaces it (leaving alone) if possible, For example, \nBEQ L1 ... Ll: BR L2  z+Pc+IJ ... Ll: PC+ L2 This combines to z+PC+L2 ... Ll: PC +L2 and PO replaces \nL1 with L.2 in the first instruction. Note that the second instruc\u00adtion may now be unreachable. Had the \nsecond instruction done Ll: R[31 +O the combined effect would have been z +R[3] + Cl; 2 +PC +next(Ll) \n... Ll: R[31 +O That is, PO behaves as though the first branch jumped over one extra instruction and \nthe target were conditional on that branch, and then it simulates as before. However, even if there were \nan instruction with the first effect, PO would not re\u00adplace the first instruction, because introducing \nthe new label it requires ( next(LI) ) complicates other optimiza\u00adtion. PO combines only physically adja\u00adcent \ninstructions and branch chains. Example The appendices show PO optimizing a pro\u00adgram that has been used \nto illustrate FINAL . Appendtx 1 gives the initial code, produced by earlier phases of the BLISS1l compiler \nfor a program that prints trees. Comments guide the user new to the PDP1l. In addition to the effects \nshown, each non-branch sets the condition code accord\u00ading the value it assigns; TSTS set the condition \ncode but do nothing else. PO optimizes the pairs shown in Appendix 2, in the order given. Each line gives \nthe pair reduced, the resulting instruction, and some explanation. In most cases, it replaces the pair \nwith one equivalent ins~ruction. Three pairs are non-adjacent branch chain members and so only the first \ninstruction is changed; comments note these. Note that, by simply combining adjacent ins~ructions, PO \ncollects branch chains , uses special-purpose addressing modes , combines jumps-over-jumps, and deletes \nuseless TSTS and unreachable code. Appendix 3 gives the result. BLISS1l S FINAL goes one step further \nwith an optimization called cross-jumping . It changes the last branch to go to L8 instead of L9 and \neliminates the second NOV/JSR sequence. This , in turn, admits BEQ and BR can be combined into a BNE \n-\u00adbut, by itself, it does not make the pro\u00adgram faster, only smaller. Hence, it differs fundamentally \nfrom PO S optimiza\u00adtion ; even a wider window would not help. Cross-jumping could be added to PO, but \nthe larger need is for a space-optimizer that reduces code size through more gen\u00aderal reordering. Implementation \nPO is a 180-line LISP program that runs in 128K bytes on a PDP1l/70. It was developed in three man-weeks. \nA description of the PDP1l sufficient to optimize all examples in this paper is 40 lines and was written \nin an hour. PO has also optimized several short PDP1O and IBM360 programs. PO is experimental, so it \nstill needs thorough documentation, diagnostics, test\u00ading and, most of all, optimization. PO treats only \n2.5 instructions each second. LISP, which simplified programming, en\u00adcourages sub-optimal algorithms: \nideally, programs would be stored as doubly-linked lists and, for a program of length n, PO would take \nO(n) steps; using LISP s singly-linked lists requires. so m ny extra ? implicit passes that PO takes \nO(n ) steps. Quick-and-dirty algorithms slow PO even more (eg, PO uses linear search to find instructions \nin the machine description) and hide some machine-dependencies (eg, PO doesn t know that some multiplications \ncan be done by shifting). These problems seem simple, and dramatic speedups seem possi\u00adble, but further \ndevelopment is needed. Conclusions The success with pO suggest re-examinin the division of labor between \nthe globs ? optimizer, code generator and object code optimizer. For example, the easy availa\u00adbility \nof a peephole optimizer may simplify code generators: they might produce only loadladd-register sequences \nfor additions and rely on a peephole optimizer to, where possible, discard them in favor of add-memory, \nadd-immediate or increment instructions. Experiments underway indicate that a very naive code generator \ncan give good code if used with PO. Global optimizers might also be simplified: since PO eliminates (much) \nunreachable code anyway, should global optimizers bother? Perhaps other global optimization should be \npushed down to the object code level : register transfers resemble quadruples; perhaps a machine-independent, \nglobal optimizer [Hechtl could be adapted to take a more global view of object code and so catch inefficiencies \nmissed by PO s narrow window. JSR RI, SAV3 # call SAV3 ;; MOV Mov S+31O,R3 12(R5),R2 ~L R[3] ~ pi[2] \n+ M[,5+3101 .+ T1[R[51+121 :; ADD #177776,R3 # R[31 + R[31 -2 5) CLR @R3 #M[R17311 + o 6) L5:L6: TST \nLEPT(R2) # test M[R[21+LEFT1 BNE L7 #-Z+PC+L7 i; BR # PC + L8 9) L7 : ADD k!?77776,R3 #R[31 + R[31 -2 \n10) 1fov R2,@R3 #M[R[311 + R[21 11) 130v LEFT(R2),R2 # R[21 + M[R[21+LEFT1 12) BR L6 # Pc + L6 13) L8 \n, MOV 1NFO(R2),R1 # R[l] + M[R[21+INFOI 14) JSR R7,PRINT # call PRINT 15) L9 : Mov RIGHT(R2),R2 # R[21 \n+ M[R[21+RIGHT1 16) TST R2 # test R2 17) BEQ L1O {jZ+PC+LIO 18) BR Lll # PC + Lll 19) L1O : 1IOv @R3 \n, R2 # R[2] + I I[R[311 20) ADD #2, R3 # R[31 + R[31 + 2 21) TST R2 # ~est R.2 22) BNE L12 #Z+PC+L12 \n23) BR L13 # PC * L13 24) L12 : Mov INFO(R2),R1 # R[ll -S M[R[21+INF01 25) JSR R7,PRINT # call pRINT \n26) BR L14 # PC + L14 27) L13: BR. L4 # PC + L4 28) L14 : BR L9 # PC -L9 29) Lll: B??. L5 # pe + L5 30) \nL4 : RTS R7 # return Appendix 2. Pair-wise optimizations on tree printer a) 4,5 CLR -(R3) # use auto-decrement \nb) 7,8 BEQ L8 # remove label L7 c) 9,10 >~ov R2,-(R3) # use auto-decrement d) 15,16 L9 : IX3V RIGHT(R2),R2 \n# remove TST e) 17,18 BNE Lll # remove label L1O f) e,29 BNE L5 # remove label Lll, retain 29 g) 19,20 \nMov (R3)+,R2 # use auto-increment h) g,21 Mov (R3)+,R2 # remove TST i) 22,23 BEQ L13 # remove label ~12 \nj) i,27 BEQ L4 # remove label L13, retain 27 k) 26,27 BR L14 # 27 unreachable without L13 1) k,28 BR \nL9 # remove label L14, retain 28 m) 1,28 BR L9 # 28 unreachable without L14 n) m,29 BR L9 # 29 unreachable \nwithout Lll Appendix 3. Optimized tree printer JSR RI, SAV3 # call SAV3 :] Mov s+310,PL3 # IU 31 + M[S+3103 \nMov 12(R5),R2 # R[21 + M[R[51+121 :; CLR -(R3) #MERC31-21 + o; decrement R[31 5) L5:L6: TST LEF T(R2) \n%! test M[R[21+LEFTI 6) BEQ L8 #z+Pc+L8 7) Mov R2,-(P.3) #M[R[31-21 + R[21; decrement R[31 8) Mov LEFT(R2),R2 \n# R[21 + PKR[21+LEFT1 9) BP. L6 # Pc + L6 10) L8 , MOV INFO(R2),R1 # R[l] + M[R[21+1NF01 11) JSR R7,PRINT \n# call PRINTT 12) L9 : Ilov RIGHT(R2),R2 # g[21 + M[R[21+RIGHT1 13) BNE L5 #Z+?C+L5 14) Mov (R3)+,R2 \n# R[21 + M[R[311; increment PL[33 15) BEQ L4 #Z+PC+L4 16) Mov INFO(R2),R1 # R[ll -M[RE21+INF01 17) JSR \nR7,PRINT # call PRINT 18) BR L9 # PC + L9 19) L4 : RTS R7 # return \\ [Allen] F. E. Allen and J. Cocke. \nA catalogue of optimizing transformations. In R. Rustin, editor, !#%%%%%%e--%d [Bagwelll J. T. Bagwell. \nLocal optimiza\u00ad tion . SIGPLAN Notices 5(7):52-66, July 1970.  [Bell] C. G. Bell and A. Newell. Computer \n. Structures: Readin s and Examples. McGraw-Hill, + [Harrison] W. Harrison. A new strategy for code \ngeneration --the general purpose optimizing compiler. l?OPL 4:29-37, 1977. [ Hecht I M. S. Hecht. Flow \nAnalysis of Computer Programs. North-Holland, 1~7. [Lowry 1 E. S. Lowry and C. W. Medlock. Object code \noptimization. CACM 12(1):13-22, January 1969. [McKeemanl W. M. McKeeman. Peephole optimization. CACM \n8(7):443-444. [Standish] T. A. Standish, D. C. Harriman, D. F. Kibler and J. M. Neighbors. The Irvine \nprogram transfor\u00admation catalogue. Dept.of Information and Computer Science, UC Irvine, 1976. [Wulfl \nW. Wulf, R. K. Johnsson, C. B. Weinstock, S. O. Hobbs and C. M. Geschke, The Design of an O timizin Compiler< \nAmerican El~vler, ~  \n\t\t\t", "proc_id": "567752", "abstract": "Object code optimizers pay dividends but are usually ad hoc and machine-dependent. They would be easier to understand if, instead of performing many ad hoc optimizations, they performed a few general optimizations that give the same effect. They would be easier to implement if they were machine-independent and parametrized by symbolic machine descriptions. This paper describes such a compact, machine-independent peephole optimizer.", "authors": [{"name": "Christopher W. Fraser", "author_profile_id": "81100364566", "affiliation": "The University of Arizona, Tucson, Arizona", "person_id": "P47620", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/567752.567753", "year": "1979", "article_id": "567753", "conference": "POPL", "title": "A compact, machine-independent peephole optimizer", "url": "http://dl.acm.org/citation.cfm?id=567753"}