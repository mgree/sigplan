{"article_publication_date": "01-01-1979", "fulltext": "\n Permission to make digital or hard copies of part or all of this work or personal or classroom use is \ngranted without fee provided that copies are not made or distributed for profit or commercial advantage \nand that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, \nto post on servers, or to redistribute to lists, requires prior specific permission and/or a fee.&#38;#169; \n1979 ACM 0-12345-678-9 $5.00 Inductively Corn utable Constructs in Very High L evel Languages Amelia \nC. Fong * Dept. of Computing and Information Science University of Guelph Guelph, Ontario Canada Abstract \n In this paper we study the profitability of applying the reduction in strength technique to programs \nin set-theoretic languages, focusing on the high level constructs involving set-formers. We define recursively \ntwo classes of expressions we shall call inductively computable set-formers and inductively computable \npredicates which can be evaluated with an order of magnitude im\u00adprovement of the asymptotic running time \nas compared to the straightforward evaluation. The quantity developed for this comparison can be used \nto derive further results when additional in\u00adformation is known. For programs written in very high level \nlanguages, which often consist mostly of nested iterative constructs, this tech\u00adnique amounts to altering \nthe algorithm used to compute the program by replacing it with an asymptotically faster algorithm. I \nIntroduction Recently there has been much interest in and study of very high level languages [El, E2, \nFe, M, S2]. It is generally recognized that optim\u00adization in these languages is particularly impor\u00adtant, \nsince a straightforward translation may pro\u00adduce very inefficient programs. One powerful op\u00adtimization \ntechnique is an extension of the tech\u00adnique called reduction in strength , i.e., the re\u00adplacement of \nmultiplication with addition in a loop [Al, AC, AU, CS, CK, ACK, FKU]. This extension was first investigated \nby Earley [E2] and was called it erator inversion . Further stu\u00addies include [FU, S1, PS] . In [PS], \nit was called formal differential ion . The basic idea is as follows. If a set-valued expression such \nas A -B is to be computed within a loop that is executed repeatedly, then  * Work supported by the National \nSctence md Eng,nesring Research Council of C.. n ada rather than computing its value directly from the \noperands A and B, it may be easier to com\u00adpute it from its previous value, provided that the modifications \nthat had occurred to its operands can also be computed easil y . Most set opera\u00adtions, such as set union,, \nset intersection, and set difference, are amenable to this type of optimiza\u00adtion [FU, S1, PS]. However, \nwhether the tech\u00adnique should be applieci to an expression at a particular point in a loIop depends also \non the structure of the control flow [FU]. If the modification in the operand, no matter how small, were \nto be effected frequently enough, the work expended to update this change would be too much for the optimization \nto be worthwhile. It is to this problem of profitability that we wish to address ourselves in this paper. \nIt is clear that this technique is profitable only if the work spent in computing the changes to the \noperands involved in an expression and in updating the current value of the expression from its previous \nvalue is less than that spent in com\u00adputing it directly from its operands. Unfor\u00adtunately, as [E2] points \nout, it is not clear how to tell in advance whether a transformation is help\u00ading or hurting the running \ntime. [PS] gives some heuristics &#38; to the conditions under which the technique may be appliedl profitably. \nIn [FU] a set of profitable transfcmmations are built up from the bottom , starting with transforma\u00adtions \nwhich obviously improve the running time of the program, and developing additional transformations recursively. \nFong and Unman [FU] formalize the intui\u00adtive notion of easy to compute . They define an induction variable \nto be one such that the change in its value arouncl the loop can be com\u00adputed using a bounded amount \nof work. Induc\u00adtion expressions can be built from induction vari\u00ad ables. The changes in their values \naround the loop can be computed using a bounded amount of work so that they can be evaluated with an \norder of magnitude improvement in running time when reduction in strength is applied. Howev\u00ader, the most \nimportant application of this tech\u00adnique is not in operations such as set union, but in expressions such \nas set-formers and predicates involving set-formers. In the spirit of [FU], we shall give recursive definitions \nof inductively computable set-formers and inductively comput\u00adable predicates which are two classes of \nexpres\u00ad sions such that, if they are evaluated properly, an order of magnitude improvement over the \nstraight forward evaluation can be obtained. In Section II the main results are present\u00aded. In Section \nIII we discuss various interesting extensions of the results to special cases. II Main Results Before \ngoing on to the theorems, we need some basic definitions, We shall adopt the infor\u00admal versions of the \ndefinitions given in [FU]. Definition: A (w) is an induction variable (induc\u00adtion expression) of loop \nL at edge e if one can m edify the program such that there is a constant upper bound on the work taken \nby the extra steps added to co repute the change in the value of A (w) between any two consecutive times \nthat control is at e, staying within L. If w is a boolean expression, we define w to be an induc\u00adtion \nexpression of L at edge e if its value (rather than its increment) can be computed with a con\u00adstant amount \nof work. Definition: A set-valued expression w is of limit\u00aded perturbation at edge e in loop L if the \ndifference in its values between any two consecu\u00adtive times that control is at e, staying within L, is \nof bounded size. As pointed out in [FU], these two notions are not the same, nor does one imply the other. \nTo be specific, let us consider the set\u00adformer {x ~ A I ~ (x)]. In a straightforward im\u00adplementati on, \nthis is actually a loop of its own, in which x runs through every element of A. As given in VU], it cannot \nbe an indication expres\u00adsion in a loop outside its own internal loop, ex\u00adcept under the most trivial \nof circumstances, such as in the case where t(x) is a constant function independent of x. In the following \ndiscussion, we shall assume that l(x) is a non-constant function of x, Theorems 1 and 2 are essentially \nfrom [FU] and form the basis for the definitions of in\u00adductively computable set-formers and inductively \ncomputable predicates. Theorem 1 Let L be a loop containing the set former {x c A I ~(x)] Suppose A \nis an induction variable and is of limited perturbation at e. Sup\u00adpose for all x which could ever be \nmembers of A, ~(x) is an induction expression at e. Then the value of {x c A I ~(x)) at e can be computed \nwith work proportional to IA I + cost (~), where by cost(~) we mean the work necessary to com\u00adpute ~(x) \nfor any one x that may be in A. (Note that the straightforward evaluation of the set former requires \nIA I * cost(+) work.) Proof: Between any two consecutive times when control is at e, the number of x \ns in A such that the value of ~(x) changes is bounded by IA 1. Since each ~(x) is an induction expression \nat e, the total work taken by the added steps to com\u00adpute them is bounded by a constant times IA\\. Between \nany two consecutive times that control is at e, the number of new elements added to or deleted from A \nis also bounded because A is an induction variable and is of limited perturbation at e. The work involved \nin the addition and deletion is proportional to cost(+). Hence the total work taken by the added steps \nis propor\u00adtional to IA I + cost(+). 1 Theorem 2 Let L be a loop containing the predi\u00adcate P(A). Let \nP(A) be either {Vx c A : ~(x)] or {Ix e A : ~ (x )]. Suppose A is an induction vari\u00adable and is of limited \nperturbation at e. Suppose for all x which could ever be in A, ~(x) is an in\u00adduction expression at e \nin L. Then the value of P(A) at e can be computed with work propor\u00adtional to IA I + cost(+), where cost(+) \nis the work necessary to compute ~(x) for any x that may be in A. (The straightforward evaluation of \nP(A) requires IA I * cost(~) in the worst case.) Proof: Let P(A) be Vx c A : +(x). The value of P(A) \nat e can be obtain~d by computing t, the cardinality of the set {x c A I -~ (x )], which can be computed \nwith work proportional to IA I + cost (~). P(A) is true iff t = 0. In actual imple\u00admentation, only t, \nrather than the set itself, is computed. Similarly, if P(A) is ]X c A : ~(x), its value is true iff the \nset {x 6 A I ~(x)] is nonempty. o In actual implementations, only those t(x) which have actually changed \nsince control was last at e should be recomputed. The mapping which gives us those +(x) to be updated \nis the inversion function as used in [E2]. Note that this does not improve the asymptotic running time \nsince #(x) may need to be updated for all x in A, Also, the inversion function itself may need to be \nupdated, not to mention the space re\u00adquired to store this function. In the above theorems the improvement \ndoes not depend on creating the inversion function. In section III we shall discuss conditions in which \nit is desirable to create such a mapping and show how further im\u00adprovement is achievable. Example: Consider \nw ={xc AIx GB] If B is an induction variable and is of limit\u00aded perturbation at e, then ~(x) = x G B \nis an induction expression and is of limited pert urba\u00adtion at efor all x in A. If A is also an induction \nvariable and is of limited perturbation at e, then the value of w can be computed by computing the change \nin w since control was last at e, Let t(x) = Ix-B/. Let A + (w, e), A (w, e) be new set-valued identifiers, \nwhich have the value of the change in w since last time control was at e, A + (w, e ) maintains the ele\u00adments \nadded to w and A (w, e) maintains the elements deleted from w since control was last at e. A+(A,e), \nA-(A,e), A+(B,e), A (B, e) are new set-valued identifiers with similar interpretations. A+(w, e) and \nA (w, e) can be computed as follows: A+(w, e)=@ A (w, e)=@ forxinA-(A, e)do A-(w, e)= A (w, e) U{x) \nfor x in A (A, e)do begin compute t(x) = /x-B/ if t(x) = O then A+(w, e)=A + (w, e) U {xl end for yin \nA (B, e)do for zinAdo if yin zthen begin if t(z) = O thlen A (w, e)=A (w, e) U {z] t(z) =t(z) + 1 \nend for yin A + (B, e)do for zinAdo if yin zthen begin t(z) =t(z)-1 if t(z) = O then A+(w,e)=A+(w,e)u \n{z] end Hence w can be computed using work pro\u00adportional to )A ) + cost(~~), where cost(~) in this case \nis the work necessary to compute I x-B I for any x that may be in A. If we use an inversion function \nas suggested in [E2], we may define FIND(y) to be {x c A I y E x], The above pro\u00adgram can then be modified \nto become the follow\u00ading: for xinA+ (A, e)do A+(w, e)=@ A (w, e)=@ forx inA (A, e) do begin A (w, e)= \nA (w, e)lJ {x] for y in x do FIND(y) = FIND(y) -{X] end begin compute t(x) = I x-B I if t(x) = O then \nA+(w, e)=A + (w)e) (J {x} for yinxdo FIND(y) = FIND(y) U {X] end for yin A (B, e)do for z in FIND(y) \ndo begin if t(z) = O then A (w, e)= A (w, e) U{z] t(z) =t(z) + 1 end for yin A + (B, e)do for z in FIND(y) \ndo begin t(z) = t(z) -1 if t(z) = O then A+(w, e)=A + (w, e) (J {z] end If FIND is precomputed outsi~ \nthe loop, then the work necessary to obtain A (w,e) and A (w,e) is proportional to c + cost(~,), where \nc is an upper bound on the size of FIND(y) for any y in A, and cost(~) is the maxim urn over all cost(~(x)) \nfor all x in A, where each cost( ~(x)) is proportional to I x-B 1. However, we have intro\u00adduced the extra \nstatements to update FIND whenever the set A is changed, and the added cost is proportional to \\x I for \nany x that may be in A. Since in the worst case c may be IA I , in\u00ad troducing the inversion function \nhere may not improve the running time of the program. If from the program one can conclude that c is \nbounded by a constant independent of I A 1, then A + (w,e) an d.A (w,e) can be computed using work proportional \nto cost(~) using the function FIND. The work necessary to compute Ix-B I is proportional to \\ x I if \nwe take each element in x and test for membership in B, where a member\u00adship test is assumed to take constant \ntime. Let M be the maximum Ix I over all x that may be in A. The straightforward implementation takes \ntime proportional to I A I * M, while the above program takes time proportional to IA I + M. Hence, From \nthe set-former and the predicates given in Theorems 1 and 2 we build up more complicated set-fo rmers \nand predicates which can be evaluated with an order of magnitude im\u00adprovement in running time over the \nstraightfor\u00adward implementation. We shall call this induc\u00adtively computable set-formers and inductively \ncomputable predicates. For any set-former S and any predicate P(A), we define cost(S) and cost(P(A)) \nto be the cost of evaluating S and P(A) in the straightfor\u00adward manner. That is, if S = {x ~ A I ~(x)], \ncost(S) is defined to be I A I *cost(~), where cost (~) is as defined in Theorem 1. If P(A) is the predicate \n{Vx E A : ~(x)] or the predicate {]x CA : ~x)], cost(P(A)) is defined to be lAl*cost (~). We shall define \nan inductive y computable set-former S at edge e in loop L and the ivcost( S) of S; an inductive y computable \npredicate P(A) at edge e in loop L and the ivcost(P(A)) of P(A) inductively as foil OWS: (1) If S = {x \nc A I ~(x)] satisfies the conditions in Theorem 1, then S is an inductively comput\u00adable set-former at \ne. We define ivcost(S) to be IA I + cost(~), where cost(V) is as defined in Theorem 1. Define the level \nof S to be 1. (2) Let P(A) be the predicate {V x ~ A : ~(x)] or the predicate {]x c A : ~(x)]. If P(A) \nsatisfies the conditions in Theorem 2, then P(A) is an induc\u00adtively computable predicate at e. We define \nivcost(P(A)) to be IA \\ + cost (+), i.e. the cost to evaluate it as given in the proof of Theorem 2. \nDefine the level of P(A) to be 1. (3) Let S = {x cA I ~(x)]. Suppose A is an in\u00adduction variable and \nis of limited perturbation at an edge e in loop L. Suppose for all x which could ever be members of A \n, ~(x) is an induc\u00adtively computable predicate at e. Then S is an inductively computable set-former at \ne. We define ivcost(S) to be IA I *ivcost(~) + cost(~), where ivcost(~) is the maximum of ivcost(~(x)) \nover all x that may be in A, and cost(#) is the maximum of cost($(x)) over all x that may be in A. Define \nthe level of S to be one more than the level of #(x). (4) Let P(A) be the predicate {V x ~ A : ~(x)] \nor the predicate {Ix c A : ~(x)]. If S = {x eA I ~(x)] is an inductively computable set-former at e, \nthen P(A) is an inductively computable predi\u00adcate at e. We define ivcost(P(A)) to be the same as ivcost(S). \nDefine the level of P(A) to be one more than the level of ~(x). The definition of ivcost is motivated \nby the following lemma. Lemma Let S be an inductively computable set\u00adformer and P(A) be an inductively \ncomputable predicate. Then it is possible to evaluate S and P(A), using work proportional to ivcost(S) \nand ivcost (P(A)) respective y. Proof: We shall prove this by induction on the level of S and P(A). Basis: \nLet level of S be 1. That the lemma is true follows from Theorem 1. Let the level of P(A) = 1. That the \nlemma is true follows from Theorem 2. Inductive step: Let S be of level k. Assume the lemma is true for \nS and P(A) of level k-1. Let S be {x c A I ~(x)}. Since S is an inductively com\u00adputable set-former, A \nis an induction variable and of limited perturbation at e, and ~(x) is an inductively computable predicate \nof level k-1. Between any two successive times that control is at e, the number of x in A such that the \nvalue of #(x) changes is bounded by IA/. Since each ~(x) is an inductively computable predicate at e \nof level k-1, by the inductive hypothesis, its value at e can be computed using an amount of time equal \nto ivcost(~(x)). Hence the total work in\u00advolved is IA I * ivcost(~), where ivcost(~) is the maximum of \nivcost(~(x)) over all x that could ever be in A. Between any two successive times that control is at \ne, the number of new elements added to or deleted from A is also bounded, be\u00adcause A is an induction \nvariable and is of limited perturbation at e. The work involved in the ad\u00addition and deletion is promotional \nto cost(~), which is the maximum of cost(~(x)) over all x which could ever be in A. Hence the total cost \ninvolved is-IA / * ivcost(~) + cost(~) which is ivcost(S) as defined in (3) of the definition. The proof \nfor P(A) (of level k is similar. 1 The following theorem shows that an induc\u00adtively computable set-former \nS and an inductive\u00adly computable predicate P(A) can be evaluated much faster than by the straightforward \nevalua\u00adtion. Theorem 3 Let L be a loop containing the set former S = {x c A I ~(x) ]. If S is an inductive\u00adly \ncomputable set-former at e, then where IA I is the size of A and [x I is the size of any x that may be \nin A. Let P(A)) be the predicate {Vx ~ A : ~(x)) or the predicate {]x c A : ~(x)]. If P(A) is an in\u00adductively \ncomputable predicate at e, then where IA I is the size of A and Ix I is the size of any x that may be \nin A. Proofi We shall prove this by induction on the level of S and the level of P(A). Basis: Let S \nbe of level 1. Then by definition,  J!zQKQ= 1/4 ,, I -5X1 +~ cost(s) ,! /.4 I +cost (*) = Llaxl+y Iq \ncost(t) 11 We assumed ~(x) to be dependent on x, so Let P(A) be of level 1. Similarly, by definition, \nivcost (P (xl )) = 1A I +!~xl +~ cost(P(A)) 1A [ +cost (~) = [A/+!rnlx/+ce [Al *cost(*) IP7XI-%T+ (;I \n0 Inductive step: Let P(A) be of level k. Assume the theorem is true for any inductively comput\u00ad able \npredicate of level k-1. Then putable predicate in loop L at edge e. If within the loop all assignments \nto A are deletions from A, then S and P(A) can be evaluated in time proportional to I A I * ivcost(~). \nsince ivcost (~(x) ) = ~ 1A I +qxl +@ Cost(+ (.x)) for all x in A by the inductive hypothesis, as ~(x) \nis an inductively computable predicate at e of level k-1. Similarly, if S is an inductively computable \nset-former of level k, then III Extension to Special Cases In the previous section, we defined an in\u00adductively \ncomputable set-former S = {x c A I ~(x)] and an inductively computable predicate P(A) = {Vx cA :~(x)} \nor (]x cA :~(x)] such that they can be evaluated using time proportion\u00adal to I A I *ivcost(~) + cost(+), \nwhile the straight\u00adforward implementation takes time proportional to j A I *cost(~). The expression I \nA I *ivcost (~ ) + cost(~) is the sum of two terms, the first one being the time involved in updating \nthe predi\u00adcates ~(x) for possibly all x in A, while the second term is the work involved when additions/deletions \nare made to A. There are various conditions under which the second term cost (~) may be dropped, or the \nfirst term may degenerate to simply ivcost (~ ). These are given in the following ,theorems. Let us first \nmotivate the theorems. In many applications it is found that within the loop, all the assignments to \nA are either all dele\u00adtions from A or are all additions to A. This is because usually a certain subset \nof A is desired which is obtained by adding elements one at a time, starting from the null set, until \na certain condition is met, or by deleting from A until a certain predicate holds. In the following theorems \nwe let S be of the form {x c A I V(x)} and let P(A) be of the form {Vx t A : ~(x)] or {]x eA : ~(x)}. \nTheorem 4 Let S be an inductively computable set-former and let P(A) be an inductively com-Proof: By \nthe lemma, S and P(A) can be evaluated using time proportional to I A I * ivcost(+) + cost(~), where \nthe second term is the time involved in additions or deletions made to A. If all assignments to A within \nL are deletions from A and there is only a bounded number of them, A being an induction variable, the \nwork in\u00advolved is bounded, assuming each deletion takes unit time. Hence the total time taken is propor\u00adtional \nto I A I * ivcost(+). o Theorem 5 Let S be an inductively computable set-former and let P(A) be an inductively \ncom\u00adputable predicate in loop L at edge e. If between any two successive times control is at e, the number \nof x in A such that the value of @(x) changes is bounded by a constant, and that there exists a function \nto obtain those ~(x), then S and P(A) can be evaluated using time proportional to ivcost(~) + cost(~). \nProof: Since each ~(x) for all x in A is an induc\u00adtively computable predicate the work involved in updating \neach ~(x) is ivcost (~). If the number of such predicates that needed to be updated is bounded by a constant, \nthe total work involved is proportional to ivcost (~ ). The second term cost (~) comes from the work \ninvolved in a bounded number of additions or deletions made to A. o In the above theorem, we are assuming \nthat the function that gives us those ~(x) which need to be changed can be set up outside the loop L, \nand that it is not necessary to update the func\u00adtion within L. This in general would not be true. However \nthere exists some very useful cases in which this restrictive condition applies. For ex\u00adample, consider \nthe expression {x ~ A I f(x) = y]. A function f may be changed by the statement f(x) = z. If we consider \na function as a set of ordered pairs with distinct first elements, then if (x,z ) is an element of f, \nthe change f(x) = z can be considered as deletion of the element (x,z ) from f and addition of element \n(x,z) to f. If we let W be the variable which maintains the value of the set {x c A I f(x) = y] then \nfor each change to f such as f(x) = z, it can be replaced by the following code: . if f(x) = y then \nW=w -{x] ifz= y then  W=wu {x] f(x) =z For each change to the operand f of the predicate ~(x), namely \nf(x)=y, there is at most only one x in A for which the corresponding ~(x) is changed. Hence if we assume \nthat testing for f(x)=y takes constant time, maintaining the change made to W takes bounded time, provid\u00ading \nthat the number of changes made to A and f are bounded. The straightforward evaluation of W from its \noperands takes time proportional to IAI. If all assignments to A in loop L are dele\u00adtions from A, then \na condition can be relaxed, but S and P(A) can still be evaluated with im\u00adprovement over the straightforward \nevaluation. Theorem 6 Let S be {x c A I ~(x)] and let P(S) be{Vx cA :~(x)} or {Ix cA :~(x)} If ~(x) isan \ninductively computable predicate in loop L at edge e and all assignments to A in L are dele\u00adtions from \nA (A need not be an induction vari\u00adable at e), then S and P(A) can be evaluated us\u00ading time proportional \nto IA [ * ivcost(~). Proof: Let w = [x cA I ~(x)]. If A is not an in\u00adduction variable of L at e, the \nnumber of dele\u00adtions from A and hence from w between succes\u00adsive times control is at e may be proportional \nto IA 1. However since deletion takes unit time, the cost of deleting each element x from w may be charged \nto the cost of deleting x from A, Hence the total cost of updating a possible IA I number of predicates \nfix) is proportional to IA [ * ivcost (~ ), as each predicate is an inductively computable predicate. \no By a careful accounting of the work spent in evaluating an expression, similar results can be obtained \nif all assignments to A within L are ad\u00additions to A. For any {x ~A I ~(x)} at edge e in a loop L to \nbe evaluated incrementally within the loop, its initial value has to be computed directly from the operands \nbefore entering the loop. This cost is usually charged to the initialization of the loop, as it is induced \nonly once, independent of the number of times the loop is executed. If all assignments to A in L are \nadditions to A, for each element x added to A, an amount of work proportional to cost(~) is needed to \ncompute the initial value of ~(x). Successive values of ~(x) at e within L can be obtained using work \npropor\u00adtional to ivcost (~ ), if ~(x) is an inductively com\u00adputable predicate. Now for each x added to \nA within L, the corresponding ~(x) is computed from the operands only once, as there is no dele\u00adtion \nwithin the loop. Successive evaluation of #(x) at e will still be computed incrementally . Hence theorem \n4 and 6 still apply with deletions changed to additions, the cost of computing those ~(x) for each x \nadded to A, i.e. cost(~) may be charged to the initialization of the loop L, as the computation is done \nonly once, in\u00addependent of the number of times the loop is ex\u00adecuted. References [Al] F.E. Allen, Program \nOptimization, in Annual Review in Automatic Programming, Vol. 5, Pergamon, 1969, pp. 23!1-307, [AC] F.E. \nAllen and J. Cocke, A Catalogue of Optimizing transformations, in Design and Op\u00adtimization of Compilers \n(R. Rustin, cd.), Prentice Hall, 1972, pp. 1-30. [ACK] F. E. Allen, J. Cc)cke and K. Kennedy, Reduction \nof Operator Strength, TR 476-093\u00ad6, Dept. of Math. Sciences,, Rice Univ., Houston, Aug., 1974. [AU] A.V. \naho and J.D. IJllman, The theory of parsing, Translation and Compiling, Vol. II, Compiling, Prentice \nHall, 1973. [CK] J. Cocke and K. Kenmedy, An Algorithm for Reduction of Operator Strength, TR 476\u00ad093-2, \nDept. of Math. Sciences, Rice Univ., Houston, March, 1974. [CS] J. Cocke and J.T. Schwartz, Programming \nLanguages and Their Compilers, Courant Insti\u00ad tute, New York, 1971. [El] J. Earley, Relational Level \nData Struc\u00adtures in Programming Languages, Technical Report, Computer Science Department, Universi\u00adty \nof California at Berkerly (1973). [E2] J. Earley, High Level Operations in au\u00adtomatic programming, Proc. \nSIGPLAN Sympo\u00adsium on Very High Level Languages, March, 1974. [Fe] J.A. Feldman et al., Recent Developments \nin SAIL--An ALGOL Based Language for Artificial Intelligence, Rept. CS 308, Standard University, August \n1972. [FKU] A.C. Fong, J.B. Kam and J.D. Unman, Application of Lattice Algebra to Loop Optimi\u00adzation, \nProc. 2nd ACM Symp. on Principles of Programming Languages, Jan,, 1975. [FU] A.C. Fong and J.D. Unman, \nInduction Variables in Very High Level Languages, Proc. 3rd ACM Symp. on Principles of Programming Languages, \nJan.,1976. [M] J.B. Morris, A comparison of MADCAP and SETL, Los Alamos Scientic Lab., Univer\u00ad sity \nof California, Los Alamos, New Mexico (1973). [PS] B. Paige and J. T. Schwartz, Expression Continuity \nand the Formal Differentiation of Al\u00adgorithms, Proc. 4th ACM Symp. on Principles of Programming Languages, \nJan., 1977. [S1] J. T. Schwartz, On Earley s Method of Iterator Inversion , SETL Newsletter, No. 138, \nCourant Institute, 1974. [S2] J. T. Schwartz, On Programming, Vols. I and II, Courant Institute, 1971 \nand 1973.  \n\t\t\t", "proc_id": "567752", "abstract": "In this paper we study the profitability of applying the \"reduction in strength\" technique to programs in set-theoretic languages, focusing on the high level constructs involving set-formers. We define recursively two classes of expressions we shall call inductively computable set-formers and inductively computable predicates which can be evaluated with an order of magnitude improvement of the asymptotic running time as compared to the straightforward evaluation. The quantity developed for this comparison can be used to derive further results when additional information is known. For programs written in very high level languages, which often consist mostly of \"nested\" iterative constructs, this technique amounts to altering the \"algorithm\" used to compute the program by replacing it with an asymptotically faster algorithm.", "authors": [{"name": "Amelia C. Fong", "author_profile_id": "81100096482", "affiliation": "University of Guelph, Guelph, Ontario, Canada", "person_id": "P328981", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/567752.567755", "year": "1979", "article_id": "567755", "conference": "POPL", "title": "Inductively computable constructs in very high level languages", "url": "http://dl.acm.org/citation.cfm?id=567755"}