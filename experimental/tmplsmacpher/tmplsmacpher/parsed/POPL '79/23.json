{"article_publication_date": "01-01-1979", "fulltext": "\n Flow Analysis and Optimization of LISP-like Structures* Permission to make digital or hard copies of \npart or all of this work or personal or classroom use is granted without fee provided that copies are \nnot made or distributed for profit or commercial advantage and that copies bear this notice and the full \ncitation on the first page. To copy otherwise, to republish, to post on servers, or to redistribute to \nlists, requires prior specific permission and/or a fee.&#38;#169; 1979 ACM 0-12345-678-9 $5.00 by Neil \nD. Jones &#38; Steven S. Iluchnick Department of Computer Science The University of Kansas Lawrence, \nKansas 66045 USA I. Introduction In [12] the authors introduced the concept of binding time optimization \nand presented a series of data flow analytic methods for determining some of the binding time characteristics \nof programs. In this paper we extend that work by providing methods for determining the class of shapes \nwhich an unbounded data object may assume during execu\u00adtion of a LISP-like program, and describe a number \nof uses to which that information may be put to improve storage allocation in compilers and interpreters \nfor advanced programming languages. We are concerned chiefly with finding, for each program point and \nvariable a finite descrip tion of a set of graphs which includes all the shapes of values the variable \ncould assume at that point during the execution of a program. If this set is small or regular in structure, \nthis information can be used to optimize the program s execution, mainly by use of more efficient storage \nallocation schemes. In the first part we show how to construct from a program without selective updating \na tree grammar whose nonterminals generate the desired sets of graphs; in this case they will all be \ntrees. The tree grammars are of a more general form than is usually studied [8,191, so we show that they \nmay be converted to the usual form. The resulting tree grammar could naturally be viewed as a recursive \ntype definition [111 of the values the variables may assume. Further, standard algorithms may be employed \nto test for infiniteness, emptiness or linearity of the tree structure. In the second part selective \nupdating is allowed, so an alternate semantics is introduced which more closely resembles traditional \nLISP implementations, and which is equivalent to the tree model for programs without selective updating. \nIn this model data objects are directed graphs. We devise a finite approximation method which provides \nenough information to detect cell sharing and cyclic structures whenever they can possibly occur. This \ninformation can be used to recognize when the use of garbage collection or of reference counts may be \navoided. The work reported in the second part of this paper extends that of Schwartz [17] and Cousot \nand * The work reported here was performed under the partial support of National Science Foundation grant \nMCS76-80269. Cousot C71. They have developed methods for determining whether the values of two or more \nvariables share cells, while we provide information on the detailed structure of what is shared. The \nability to detect cycles is also new. It also extends the work of Kaplan [13], who distinguishes only \nbinary relations among the variables of a program, does not handle cycles, and does not distinguish selectors \n(so that his analysis applies to nodes representing sets rather than ordered tuples). II. Programs with \nTree like Data In the first part of this paper, we shall carry out our analyses on a simple programming \nlanguage called SL (Structure Language) whose syntax is as follows program + {[label:] stint}+ s tmt \n+ assi~n I if I goto assign + var := exp I var := input I output := exp if + if test goto test + atom \nexp I nuL2 exp I var {=1+} var goto + goto label exp + atom I var I var.sel I eons(exp {, exp}*) We \nassume that instances of the syntactic classes var, sel, atom and label are members, respectively, of \nthe sets Var, Sel, Atom and Label which are pairwise disjoint. Informal Discussion The langua~e SL closely \nresembles LISP with the PROG feature but without functions or p lists, and extended to allow arbitrary \nnumbers of selectors. See Reynolds [16] for met~~ds to handle recursively defined functions. The semantics \nof SL are essentially those of LISP, with two minor exceptions: the customary uses of NIL (a special \natom in LISP) must be done $, A Reynolds work came to our attention after this development was completed. \nHe treats a subset of LISP with recursive function calls and without sequential execution. It seems clear \nthat the two methods could be combined. via the empty or undefined data structure L; and COW(T ,.. .,Tm) \n= any attempt to apply a selector (e.g., CAR, CDR) 1 to an atom or L will result in program abortion \ninstead of being undefined . i~l {seli.tl . ..tn.atom I tl. ..tatomom c Ti} In LISP without selective \nupdating operations it is natural to view the value of a variable as a The definitions are naturally \nextended as tree without regard to cells, pointers, etc. Each follobs: Let A,BI, . . ..Bm be sets of \ntrees. Then internal node will have an edge labeled s leading to a subtree, for each s in the set of \nselectors A.s = {T.s I T c A-ATOM -{1}} Sel = {sell, . . ..selm}. If Tl, . . ..Tm are trees, cofis(Bl, \n. . ..Bm) = eons(T ,... ,Tm) denotes the tree consisting of a 1 {cons(TI, ..., Tm) I TI EB1, . . ..mT \nE Bm} root node, with edges labeled sel . . ..sel 1 m leading to the roots of Tl, . . ..Tm. respectively. \nFollowing the style of denotational semantics [15,181, we define the meanings of the various If T has \nthis structure, then T.sel denotes the i constructs in terms of the domains ATOM, subtree T.. 1 AL = \n2Sel* x ATOM and STORE = [Var + VALI (a All trees in examples in this paper will use store a c STORE \nis a function mapping each the fixed set of selectors Sel = {hd,tl}. Trees variable to its current value). \nWe only define will be given pictorially with the root at the the meaning of expression evaluation and \nassign top, leaves at the bottom, and atoms labeling the ment statement execution. The other features \nof leaves. Selectors may be omitted for convenience, SL can be formally defined by well-known means, \nin which case the edge directed southwest (south\u00adincluding continuations. east) from a node goes to the \nhd subtree ( tl subtree). ~: Exp + [STORE + VALI is a partial function given by: Semantics of SL ~[atoml \no = {~ [atom]} The value of an atom is an element of the set ~[varl u(var) o= ATOM and is given by the \nfunction ~ Atom +ATOM. ~I[var.s] o = o(var).s [undefined if We assume that atoms are otherwise unspecified \no(var) = 1 or is an atom] simple data objects --numbers, bounded-length character strings, booleans, \netc. ?~[cons(el, . . ..em)l u= Trees are defined formally by the sequences ccms(~llello,.. .,~[e 10) \nof labels encountered on paths from the root to the leaves. Such a sequence is written as ~~: Assign+ \n[STORE+ STOREI ~s given by ... ..sn.a where n > 0, s c Sel and 1.s2 I . .. sn ~~[var := exp] u = lx c \nVar. a E ATOM. The set of all such label sequences is (i~x = var tllenj~[exp]o eZse o(x)) naturally described \nby Sel* x ATOM. Execution of a statement and the whole By definition a tree is a finite subset T of ,f \nprogram will be aborted if Sel x ATOM such that if s ... ..s .a ~ T, then 1 n a) an expression which \nmust be evaluated is s . . . . .s .bc Tonlyifp=Oand a=b. undefined 1 n.si.....sp b) an if statement compares \ntwo values either of The null tree T = ~ is written as L (read which is a nonempty, nonatomic tree bottom \n). c) a nonempty, nonatomic tree is read by a The definition allows trees with missing statement var \n:= input . branches such as {hd.l,tl.hd.2}. In diagrams the missing branches will be drawn, and filled \nin with Note that point (b) implies that these tests model J.. Some examples are shown in Figure 1. EQ \nin LISP, rather than EQUAL. III. Structure Shapes and Data Flow Equations We now show how to construct \na system of Diagram: I 1 A forward data flow equations from a program. Let 12 X be a program variable \nin Var, and I a program point. The system will then have a variable F(I,X) . The least fixed point solution \nof the system will associate with F(I,X) a set of tree Set: @ {l} {hd.l,tl.2} {tl.hd.hd.2,tl.tl.3} shapes \nwhich includes all possible shapes X could have at point I in any possible execution of the Figure 1 \nprogram. The set of shapes is defined simply by:By definition if T,Tl, . . ..Tm are trees and * x {o} \nse Sel then Shape = 2se1 T.s = {tl . ..tn.atom I s.tl. ..tatomom E T} That is. we have replaced all elements \nof ATOM by the symbol O which represents an arbitrary atom. and Shape Next, we form the lattice 2 of \nall subsets of Shape, with the usual subset ordering. The variables in the equation system will have \nelements of *Shape as values. As in [12] we first convert an SL program to a flowchart and annotate it \nwith program points in the set ,.. .,n], one for each arc in the Consider the flowchart segments of lowchart~~= \n{0,1 Figure 2. I ... I 11 P %P %P s s J JJ /) ye nodb (a) Non-if statement (b) if statement Figure \n2 The equations are formed as follows: Form of S Equation X := atom or X := input F(J,X) = {O} X:=Y F(J,X) \n= F(I1,Y) U . . . u F(IP,Y) x := Y.s F(J,X) = F(II,Y).s U . . . u F(IP,Y).s x := eons(yl, . . ..ym) F(J,X) \n= i~leons(F(Ii,Yl), . . ..F(Ii.Ym)) F (J ,X) = i~l F(Ii,X) yes n {o} if atom X F(Jno,X) = i~l F(Ii,X) \n-{o} { F (J ,X) = i;l F(Ii,X) yes n {1} [ if nuZ2 X F(Jno,X) = ill F(Ii,X) -{1} 1 F (J ,x) = i~l (F(Ii,X) \nyes n F(Ii,Y)) n {O,L}  F(Jno,X) = i~l F(Ii,X) ifx=Y n {0,1} F (J ,Z) = F(Jno,Z) = yes F(II,Z) U . . \n. U F(IP,Z) forZ+X, Y  i other F(J,X) = F(II,X) U . . . u F(ID,X) Finally, the equation F(IO,X) = {L] \n is included for the initial program point I and o each variable X. Note that the only way in which \nstatements constrain the shapes of values flowing to them is through the possibilities for abortion of \nexecution. Taking these constraints into account through backward flow analysis, as discussed in our \n[12] or Kaplan and Unman s [14], could provide more specific information about shapes. However, we ignore \nthis possibility for the present since the extension is straightforward. To obtain the maximal information \navailable from forward flow analysis about the program s data values the F(I,X) sets should be as small \nas possible, as long as they include every value which may be computed. It is for this reason that the \nsets {O}, {1} and {O,L} appear in the equations to conclude as much as possible from the program s assumed \ncorrectness. IV. Solving the Flow Equations There are at least two methods available to solve the data \nflow equations. One is iteration in either its regular or chaotic form (see [12] and [61) starting with \nevery F(I,X) = d. It should be clear that the functions involved are all continuous, so solutions always \nexist. This method is appropriate if the solution is finite. Unfortunately this is not generally the \ncase for the systems under consideration here. Instead, we shall introduce here a method based on regular \ntree grammars which handles the finite and infinite cases equally well. The objective is to obtain a \nregular tree grammar such that the language it generates is a safe approximation to the minimal fixed \npoint of the system of flow equations. This is useful, since tree grammars are a well understood extension \nof regular string grammars; consequently existing algorithms can be used to test for finiteness, linearity, \netc. The approach is to form from each data flow equation a production in an extended regular tree grammar, \nwhich is then transformed into an ordinary tree grammar. Tree Grammars A regular tree grammar (see, for \nexample, [81 or [19]) is a grammar <N,Z,P,S> with N a finite set of nonterminal symbols, Z a ranked alphabet \nof terminal symbols such that N n Z = 0, S e N is the initial nonterminal, and P is a finite set of productions \nof the form A + t where A~Nandt~TZ(N). Here TX(N) is defined by (i) N UZ. s TZ(N) (ii) if k> 1, a c \nZkand T1, . . ..Tk E TZ(N),  then a[T .,Tk] E TX(N) 1 (iii) nothing else is in TX(N) Note that the \nlinear representation a[Tl, . . ..Tk] in (ii) corresponds to the tree a A 1  k 7 ..,-,, ., .. . . \n m oraer co aescrlDe our cage la~elea trees by tree grammars, we choose k = m and =ATOM u {-L}, xl = \nSel, X2 = ... = Zm_l =@, o Zm= {6}. If T is a tree as used in our description of SL, the corresponding \nelement B(T) of TX(N) is recursively defined by: ~~(atom) = atom g(l) =1 sel sel 1m A sel sel 1m A \n\\ T-, ... Tm . .. II @l) ~(Tm) For example, we have the equation below: hd tl A hd tl ~ahd= tl bc m \n a A b c We assume the semantics of a regular tree grammar is defined by least fixed points, in the \nsame manner as was done by Ginsburg [9] for context free languages. That is, nonterminals are interpreted \nas sets of trees, and the productions are viewed as a system of set equations. It should be clear that \nthis gives the same generated set L(G) as the usual tree-rewriting semantics since the analogy between \nregular tree grammars and context-free grammars is very close. We will also write tree productions in \nthe SL notation for convenience. This does no harm since it is easily seen that T, + T. by production \n\\cexample, the natural interpretation of A + ~z is that for all Tb <B, Tc cC, the tree AT b C is in A. \nTranslating into TX terminology, the production is as shown in Figure 3(a) and means I B c #(Tb) ~(Tc) \nI I I that if ~(Tb) c B, ~(Tc) c C, then the tree in Figure 3(b) is in A. Having shown the connection \nto ordinary tree grammars, we now proceed to assume that all grammars are expressed and interpreted in \nterms of SL trees. Definition An extended regular t~ee grammar is a quadruple G = <N,Z,P,S> where N, \nZ , P and S are as above, except that P is now allowed to contain productions of the form A + B.s where \ns e Sel. The semantics of such a production in terms of sets is simply the assertion that B.s s A. For \nexample the three productions hd tl A+B,A+ , A+A.tl A c c would correspond to the set equation hd tl \nu is in Afor someT { A 2 1 1 2 } The new production type clearly gives rise to a continuous function, \nso the solution of the extended regular tree grannnar may be found, as before, by least fixed points. \nExamining the flow equations we see that they are nearly in the extended tree grammar form except for \nrestrictions involving 0 and 1. Removing these is safe, since the result is only to enlarge the solution \nvalues. Referring to Figure 2(a), let I be any of the ll,...,lP preceding S; then the construction of \nthe-grammar can now be expressed by: s Production var := var F(J,varl) + F(I,var2) 12 var := atom or \nF(J,varl) + 0 1 var := input 1 var := var .sel F(J,varl) + F(I,var2).sel 12 var := cons(var , F(J,varo) \n+ o 1 . . .,varm) ,,,,v,var , 1m otherwise F(J,var) + F(I,var) An Example Consider the program in Figure \n4 which builds a linear tree X from input items, and then transfers them to Y so they appear on Y in \ntheir original order. (a) (b) Figure 3 247 ... .. ... .. .. . . ---- I0 Note tnat the right llnearity \nor finiteness of each variable is clearly evident. L: Z := input Theorem If G = <N,I,P,S> is an extended \nregular 1 tree gramm%r. there is an ordinary regular tree 1 grammar G = <N,Z,P ,S> with L(G) = L(G ). \nx := Cons(z, x) 2 Proof We give the construction, which uses Btichi s / ( method of derived rules [41. \nDefine the relation yes if~+Ogoto L %onNU1 to be the smallest reflexive, no 4 transitive relation such \nthat I o yes.if nu~~ X goto N (a) A+ X implies A %X no 5 / 8 Y := cons (X. hd, Y) (b) A+ B. sel< , \nBNCand A1 6 sel sel im 1  x := X.tl c+ imply A WT. 1 7 1 1 Ti Tm goto M provided Tl, . . ..Tm all \nderive nonempty 2 sets of terminal trees. N: Z:=z Now define G = <N,E,P ,S> where Figure 4 P =PU{A+XIA6N \nandA~X} and The productions obtained from this are pr =pl? -{A+-B.s c P}. (omitting those for Z): Essentially \nthe same theorem was proved in F(O,X) + L F(O,Y) + 1 Reynolds [161 by another (more complex) method, \nso F(l,X) +F(O,X) I F(3,X) F(l,Y) +F(O,Y) I F(3,Y) we omit the proof that L(G) = L(G )= L(G ). D F(2,X) \n+ F(2,Y) +F(l,Y) The Example Revisited +l,X) o Elimination of productions with selectors on F(3,X) +F(2,X) \nF(3,Y) + F(2,Y) the right proceeds as follows: F(4,X) +F(2,X) F(4,Y) +F(2,Y) 1. A 1 and C ~J. follow \nfrom the F(5,X) +F(4,X) I F(7,X) F(5,Y) +F(4,Y) I F(7,Y) productions F(6,X) +F(5,X) F(6,Y) + 2. B -A \nfollows from B + B.tl, B %B and F(5,X).hd F(5,Y) B+ F(7,X) +F(6,X).tl F(7,Y) + F(6,Y) A oA F(8,X) +F(4,X) \nI F(7,X) F(8,Y) +F(4,Y) I F(7,Y) D o follows from D + B.hd and Simplifying this by compressing chains \nof productions and renaming, we get: B+ 3. A o1 (A = F(l,X)); A+l I ~A ~ The revised grammar has productions \n(B= F(5,X)): B+. A II B.tl A+ll A (C= F(5>Y)): C+L I~A ~ B+ ~ I ~ oA (D =F(5,X).hd): D +B.hd C+ll A \noA The solutions are: D+o A A=B=C={L, OL,O and it is easily checked that it has the same ,... } solution \nas the grammar with selectors on the\\ right. 01 This method does not yield a perfect solution D = {O} \nto the original problem, for two reasons. First, the flow analysis method associates with each node and \neach variable a set of values. While this makes grammatical analysis possible, it can lose some information, \nas in the following example: x := 1; L: x := eons(x,x); -goto L if The values X may actually have at \nL are the complete binary trees of heights 0,1,2, . . . . However the method above leads to productions \nA+o iA A which have all binary trees as solution. The second reason is the restrictions concerning \n0 and L in the flow equations which were ignored in constructing the productions. We conjecture that \nthese restrictions do not destroy the regularity of the solutions, although they may increase the complexity \nof obtaining them. v. Relating The Tree Grammars To Storage Allocation A simple and fairly efficient \nimplementation of SL may be organized as follows. Each internal node is represented by a record with \nfields one for each selector in Sel. Any 1  ?ln nonatomlc tree is identified by a pointer to the record \nfor its root node. Each program variable is bound to a Toot word, contained in a fixed runtime location, \nwhose content is a pointer to the root record of its current value (or the value itself if atomic) thus \nan assignment X := Y merely copies one root word into another; X := Y.s copies the s field of Y s root \nrecord into X s root word; and x := eons(Y1, . . ..Ym) makes the root word of X point to a newly allocated \nrecord whose fields s l  sm are initialized to the values of Y I  ym his method involves only a bounded \namount of work for each statement type, and provides maximal natural storage sharing, i.e. all that can \nbe achieved without the use of a hashing cons [101. There are some obvious inefficiencies common to \nLISP-like languages which are amenable to data flow analysis. We now briefly discuss those which can \nbe handled by use of the tree grammars just presented. The main tool used is the fact that familiar context \nfree and regular grammar algorithms generalize directly to tree grammars. In particular, infiniteness \nis easily decidable. 1. Let X be a variable, and consider V(X) = U F(I,X), our upper bound on the set \nof .p$l values X &#38;a$ assume during execution. a) If V(X) contains at most one shape other than ~, \na fixed location may be assigned to the root record of X. so its subfields may be addressed directly \nwithout need for the root word. b) If V(X) is finite, a storage area for X may be allocated statically \nfor X before execution. This area need not participate in storage reclamation activities. c) Now consider \nV(X) .seli. If this is empty, no record within a value of X needs to contain an seli field. 2. Let statement \nX := Y.s be preceded by program point I. If O or J is in F(I,Y), a runtime error is possible; if F(I,Y) \n~ {0,1}, a runtime error will definitely occur. More will be said about optimization of LISP\u00ad like programs \nin the second part of this paper, particularly concerning storage reclamation by reference counts and \ngarbage collection, and the use of CDR-coding [11. VI. Elimination of Reference Counts and Garbage Collection \nIn the remainder of this paper we assume an implementation like that described in the last section; \nin addition we extend the language (to a more powerful version called SUSL) by the addition of selective \nupdating, in a manner similar to RPLACA and RPLACD in LISP. Two standard methods for storage management \nare the use of reference counts and garbage collection. Garbage collection is the more powerful method, \nbut the collection process is quite expensive and, in its classical forms, disruptive to the computation, \nespecially in interactive and real time contexts. When cyclic data structures cannot occur, as in SL, \nthe method of reference counting may be used; However, this method requires both space overhead to store \nthe counts and time to update them. In this part of the paper we describe a method to reduce both types \nof overhead, often to zero, by a pre execution program analysis. The analysis constructs finite approximations \nto the actual runtime data structures which may occur, and is guaranteed to detect cyclic structures \nand nodes with reference counts greater than one, if they can possibly exist. In this way, runtime data \ncells may be put into three classes: 1. Those whose reference counts never exceed one. These may be returned \nto free storage as soon as pointers to them are destroyed. No reference counts need be maintained. 2. \nThose which may not appear in cycles, but whose reference counts may exceed one. These may be allocated \nwith reference count fields which are maintained during execution. 3. Other cells, which may appear \nin cycles. The overhead of reference counts may be avoided at the expense of using garbage collection. \n  In Clark &#38; Green [53 it is observed that only 2% to 8% of LXSP cells are ever pointed to more \nthan once. so this optimization should result in substantial savings. Further, our method for detecting \nopportunities for optimization appears to be significantly more general than that of Barth [2]. Before \nproceeding to give an alternate In the example in Figure 6(a) node Zu is the upper\u00ad semantics for SL \nbased on the ideas sketched above and presenting methods for analyzing its storage allocation properties, \nwe shall extend the language to include selective updating in a manner which models the functions RPLACA \nand RPLACD in LISP and assignment to records with pointers in languages such as PASCAL and PL/I. The \nnew operation is written as X.s := Y and its intended effect is to replace the s-labeled edge from the \nroot of X by an edge leading to the root node of Y. This selective updating operation makes it possible \nto create cyclic structures, as shown in Figure 5, where performing X.hd := X on the structure in (a) \nresults in that shown in (b). Thus the language with selective updating is more powerful than without \nit. We call the language with selective updating SUSL (Selective Updating Structure Language). x f--=x \na A bc (a) (b) Figure 5 We now give a semantics for SUSL which incorporates within it an alternate semantics \nfor SL equivalent to that in Section II and based on the implementation ideas in Section V. To do this \nwe first redefine the STORE to consist of all directed graphs of the following sort: 1. each internal \nnode has one son for each selector in Sel 2. each leaf is labeled with an atom or L (the null tree) \n 3. each variable in Var labels one and only one node 4. each node is accessible from a variable labeled \nnode 5. each node is a member of a universal set NODEof nodes  For example, the graph in Figure 6(a) \nis a store corresponding to the values of X, Y, and Z in (b). (a) (b) Figure 6 The following auxiliary \nfunction is used in the semantic definition: node: Var + [STORE + NODEI node vu = the node in u labeled \nv rightmost node. The effect of the assignment statement is a function ~~ : Assign + [STORE + STOREI, \n defined below. In general, ~~[s]~ is found by modifying u (unless S aborts), as described in the following \ntable. Afterwards, all nodes which are inaccessible from variables are removed from the new a. Form of \nS Mhsb var := atom add a new leaf node labeled ~(atom) ; move var to the new node var := var move var \n~ to label node var20 12 var := cons(var make a new node n and move o 1 var to label it; for . . ..varm \no i=l ,...,m, add an seli edge from n to node vario var := var .sel if node var20 has an sel 12 descendant \nn then move varl to node n else ~5~S]IJ is undefined var .sel := var if node varlu has an sel edge 12 \nfrom it then replace it by an sel edge leading to node var.o eZse~@S]o is und~ined Execution is aborted \nin exactly the same situations as in the semantics of SL given in Section II. Let o be an acyclic STORE \ngraph and X a variable occuring in a. Define tree XIJ to be the tree which results from performing node \nsplitting on the directed acyclic graph comprising all nodes and edges reachable from node Xu. For example \nif o is shown in Figure 6(a) then tree Zu is the tree labeled Z shown in Figure 6(b). Theorem Let assign \nbe any SL assignment statement (or, equivalently, and SUSL assignment statement other than a selective \nupdating operation) and a an acyclic SUSL store with variables Xl,. . . ,Xn. Then ~~SLIIassignl {Xl * \ntree Xla, . . . ,X * ih+ee X a} nn where {a bl, . . ..an S bn} denotes the finite functionlf: {al, . \n. ..an}+{bl. bm},bm} satisfying f(ai) = bi for i = 1,. ..,n. Thus the two languages are semantically \nequivalent if we ignore the selective updating operation. We omit the proof of this since it just amounts \nto showing that the usual LISP implementation strategy is valid. Define a node in a STORE graph to be \nshared if it is included in a cycle or labeled c. if there are two or more distinct paths from variables \n(or possibly from the same variable) to the node, and to be c~eZ.-ie if it is contained within a cycle \nin the graph. VII. Modeling the Sharing Semantics As is usual in flow analysis, our approach is to define \na system which is finite and whose solution in effect symbolically executes the program in parallel over \nall possible execution paths. The structures just described may grow unfoundedly in two ways: in depth \n(i.e. path length from a variable to a leaf); and there may be an unbounded number of inaccessible (garbage) \nnodes. To remedy this we discard inaccessible nodes, and consider only bounded approximations to the \ngraphs (annotated with sharing and circularity information to aid in the reference count analysis). Define \na directed graph to be k-limited if each node is accessible from a node labeled with a variable by a \nselector labeled path of length s k. Then the flow analysis lattice Share is the set of all sets of directed \nk limited graphs of the following form: 1. each variable X c Var labels one node, denoted node X, and \neach node may be labeled by one or more variables 2. there are two sorts of nodes: unknown, labeled \n? and known, not labeled ?(an  unknown node represents a set of nodes whose internal structure is not \nrepresented in the k-limited approximation) 3. unknown nodes may be labeled with either of the following \n(and possibly a variable): s indicating that the unknown structure represented by the node may contain \nsharing c indicating that the unknown structure may contain a cycle 4. each leaf is labeled with o (indicating \n an atom), 1 (indicating the null tree), or ? and possibly s or C 5. each known node has one outgoing \nsolid sel edge ~for each sel ~ Sel 6. each unknown node may have any number of outgoing unlabeled dotted \nedges - ->, each going to a different node  The lattice operations are set union (join) and intersection \n(meet). Given a fixed set of selectors and a fixed set of variables, the number of k-limited graphs with \nno inaccessible nodes is clearly finite. As an example of a k limited graph, consider the 3-limited graph \nin Figure 7. A node in a Share graph is defined to be shared if there are two or more distinct paths \nfrom variables (or possibly the same variable) to the node, or if it is accessible from a node labeled \ns, or if it is itself labeled c. It is cycZie Xk---, A 1 z ?,w oI / , /I 01 s) ?C ?s o Y Figure 7 VIII \n. Constructing the Data Flow Equations Our epproach is to associate with each program point I a set of \nk-limited graphs F(I), each graph modelling a store resulting from one or more execution paths. Consider \nthe flowchart segments of Figure 2. The equations are formed as follows, where D= F(I1)U. ..UF(I) P \nThe functions clean and next are defined below. Form of S Equation assignment F(J) = U cZean(next[S];) \n;ED F(Jyes) ={~ cDInode var ~is labeled o or ?} if atom var F(Jno) ={~ cD Inode var ~is unknown, or \nis known and not labeled O}{ F(J yes) = {~ ~D Inode var ~is labeled .L or ?} ;f nuZZ var F(Jno) ={~ \n=D Inode var ~is [ unknown. or is known and not labeled L) [ if var =var F(J yes) =F(Jno) ={; 6DI ifvar~ \n+ var~ node var ~ and 11 node var2~ are both labeled O or both labeled L} other F(J) = D = F(I1) U . \n. . UF(lP) TO define czean and next, let XShare be the set of all sets of graphs satisfying conditions \n1 through 6 of the definition of Share; however, they need not be k-limited. The functionalities are \nnow next: Assign + Share + XShare and ezean: XShare + Share. The idea is that next applies the statement, \nand clean makes the resulting graph(s) k-limited. The function node can be carried over to the graphs \nin Share and XShare naturally. We now explain how to compute next [S] ~ for an assignment statement S \nand set of graphs ~. First, If {~1 e Share, nezt [s] {7} will normally consist of one graph, obtained \nby modifying 7 as described in the table below. However ne$t [S] {~} will be empty if S aborts, and may \nhave more than one element if a variable is moved to a descendent of an unknown node. Form of s next \n[S] {7} next[X := a]; = I I 6 Y x o var := atom or Add a new leaf labeled O to ~; var var var }{:= Zkp,Lt \n1 := var 2 o := eons(var . . .,varm) 1 Move var to the new node Move varl to label node var25 Make a \nnew node n and move varo to label it; for i = 1,...,m add an sel i edge from n to node next[Z := X.hd]~ \n= x~.-., l? 4+~1 Zx / \\ 0/ \\ / _\u00ad0 1 ?C ,?s / \\ / vari~ 0 var .sel1 := var 2 ease Y node varl~ has an \nsel edge: var 1 := var .sel 2 replace it by an sel edge leading to node var27 node var ~ is labeled o \nor ~: next [~] {7} = 0 node varl~ is unknown: Add an edge ---> from node var27 (if not present) case \nnedi Y := Cons(x,z)];= .. 1 4$xc., z \\ ?/\\w o I /, / / /=-\u00ad0 1. ?C ,?s I node var27 has an sel descendent \nn: move var ~ to node n node var2~ label O next [S1 is known or 1: {7} = @ but has next[Y :=W.tl]; = \nz xc-\\ A?,\\Yw o /\\/ ~-- \\ I) node var27 is unknown with o 1 immediate nl, . . ..n descendants : * ?C \n,?s next [Slr{T} = {7.,71, . . . ,;r} 0 , where: 7.,71>. with var 1 moved node var~ nl,.2 respectively. \n. .,?r to ..,n are nodes r ~ z Xu.. A \\ ?,., W o , //<_, \\ ~ / In Figure 8 we illustrate next [s] ~ for \nseveral statements S, where ~ contains only the single 3-limited graph of Figure 7. The purpose of the \nfunction clean is to restore the k limited character of the graphs in next [S] ;. We first define for \nany XShare ~, a subgraph @~) which consists of all nodes n which are not accessible from any variable \nlabeled node by a selector labeled path of length k -1 less, together with all edges in ~ between such \nor nodes. o , Figure * o ?C 8 1 ,?s Now clean is the set of k-limited graphs resulting from applying \nthe f~llow~ng transformation to each graph Y in o: 1. Remove all nodes which are inaccessible from variables \n 2. Partition @ti) into s~rongly connected components C~,c2, . . . 3. for each Ci do  if Ci contains \nat least one edge then coalesce C into a single unknown i node, labeled c 4. Let the resulting graph \nbe called ~ . partition ~fi ) into undirected connected components C;, C;, . . . 5. jfop each C; do \n <f c; contains more than one node then coale~ce C~ into a single unknown node n; if c; contains a node \nlabeled c then label ~ with c else if C; contains a shared node or a node labeled s then label ~ with \ns The coalescing operation above is done by merging the nodes of C into a single node n, preserving \nincoming and outgoing edges and variable labels within C. More precisely, 1. Create a new node ~ 2. \nLabel it with ? and with all variables labeling nodes in C 3. Redirect any edge coming into C to point \nto n 4. Replace any edge coming out of C by a dotted edge from ~ ---> to the same endpoint, provided \nsuch an edge does not already exist 5. Delete all nodes of C and edges between them.  As an example \nof clean, suppose we start with the graph in Figure 9(a). Steps 1 through 5 result in the graphs in Figure \n9(b) through (f), assuming the resulting graph is to be 2-limited. Note that our comments about backward \nflow analysis in Section III apply here as well. IX. Solution of the J?1OW Equations Note that Share \nis finite. It is not hard to see that F( ) is monotonic, so the minimal fixed point solution may be \nobtained by regular or chaotic iteration. As an example of the kind of information that can be obtained \nfrom the equations, suppose no node accessible from node Xo in any Share graph in the solution is in \na cycle or labeled c. Then no node accessible from X in any computation can be part of a cycle, so the \ndescendants of X need not be managed by garbage collection. Similar remarks apply to sharing: a non shared \nnode can be deallocated as soon as any reference to it is destroyed. (a) xY (b)  Y (c) (d) Figure \n9  x Y {1 XY F(0) = 11 (e) x z 1&#38; ?C ,?s I AZ (f) Figure 9 (continued) Theorems establishing \nthese facts will be proved in the next section. First we give an example of the flow equations and their \nsolution, using again the program in Figure 4. For illustrative purposes we compress this program a bit \nand insert a few program points to obtain the flowchart and forward flow equations in Figure 10. z := \nO&#38;/t x := Cm w(z,x) 1 LI yes Z+o no 1 null X~j no 2 1 Y := cons(X.hd,Y) 4 3 ![ x := X.tl F(1) = \nF(0) U~ ~[X := ~fl~] F(0) F(2) = F(l) U F(4) A F(3) =~ ~[X :=x.hd y] F(2) F(4) =~ ~[X :=Y.tl] F(3) Figure \n10 To solve the equations we proceed by the method of chaotic iterations, iterating F(1) to stability \nand then in turn iterating F(2), F(3) and F(4) until the whole system stabilizes. The solution for F(1) \nand F(2) with k = 2 is indicated by the table in Figure 11. No shared or cyclic structures occur, so \nthe simplest storage allocation method may be used. Further, X := X.tl frees one cell which can be used \nimmediately by the Y := eons(X.hd,Y). xYxY xY Figure 11 x. Theorems on Detection cf Sharing and Cycles \nWe show in this section that the Share model is capable of detecting any sharing or cycling which may \noccur in the data structures of a SUSL program. Of course, since the model is finite and based on conservative \nassumptions, it may also indicate the possibility of sharing or cycling where none occurs in the actual \nprogram. To state the results we first need to define a compatibility relation between STORE and Share \ngraphs which will embody t?le intuitive notion that, if a STORE o results from a SUSL computation leading \nto program point I,, then the set Of share graphs F(I) contains a graph 7 representing o. For example, \nfor the STORE graph in Figure 12(a), the Share graph in (b) is compatible, while that in (c) is not. \ngraph X7 for a variable X and a Share graph T to be the subgrap~ of T comprising all nodes reachable \nxx YY  a ~M, ?s 2Ao b 1bd (a) (b) (c) Figure 12 Let nodes(u) be the set of nodes in graph u. By definition \nan adm{s,s{ble nod~ comespondence from o to 7 is any function r: nodes(o) + nodes(~) such that 1. r(~ode \nXU) = node X7 for all X c Var 2. Let there be an edge in u from n to n] with selector label sel. Then \n (a) if r(n) is knownz there is an sel\u00adlabeled edge in y from r(n) to r(nl) (b) if r(n) is unknown, \neither r(n) = r(nl)   or there is a dotted edge from r(n) to r(nl) Further, o and 7 are compatible \n(written o -~) iff 1) there is an admissible node correspondence rfrom uto 7 2) if node n is shared in \na then either r(n) is shared in T or r(n) is accessible in I from a node labeled s or c ~) if node n \nis contained in a cycle in u then either I (n) is contained in a cycle in Y or r(n) is labeled c Thus \nit is easy to see that the graph in Figure 12(c) is not compatible with that in Figure 12(a) because, \namong other reasons, the tail descendant of node Xa is node Yu, while the tail descendant of node X7 \nis not node Y?. We next show that the transition functions ~~( ) and ~ ~( ) preserv. compatibility. Theorem: \nLet assign be an SUSL assignment statement, u a STORE graph and ~ a Share graph such that u -7. Then \nthere exists ;g%%;i:~ $! Uch hat Proof: The proof (omitted for brevity) compares the effects of ~~, \nnext and clean on G and ~, by an enumeration of cases, to show that the diagram in Figure 13 commutes. \nIn the diagram o = ~~[assignla, r is t~e admissible node correspondence given, by u _ Y, Y is a graph \nin nert [assign]{;}, 7 = e~ean Y , and r and r are appropriate admissible node correspondences. Cl To \nrelate the above theorem to the identification of situations where storage management niethods simpler \nand more efficient than gargabe collection can be used, we first define from node X Y and all edges \nbetween them. We then have the fo~lowing two corollaries. -byr -by r byr  I l\\ \u00ad 7 +; ~? nert [assignl \ne Lean Pigure 13 Corollary If graph X? contains no shared nodes for all $ f F(I) for alll program points \nI, then variable X requires neither reference counting nor garbage collection, i.e.,, any node reachable \nfrom the root of X may be deallocated immediately when a reference to it is removed. Proof By the theorem, \nall sharing which can occur in the STORE semantics is recognized in t~e compatible Share graphs. If no \nnode in graph XY is ever shared in. any possible ~, then nodes reachable from X can never be shared. \nCl Corollary If graph XT contains no cyclic nodes for all 7 : F(I) for all I, then variable X requires \nno garbate collection, i.e., management of storage for nodes reachable from the root of X may be done \nby reference counting. Proof Similar to that for preceding corollary. D The above results are global \nin two respects-\u00adthey concern the behavior of a variable throughout the execution of a program and concern \nall nodes reachable from it. The information present in the F(I) is sufficient, however, to obtain results \nwhich are local in both senses. This could be applied to make very efficient use of a dynamic storage \nmanagement system in which cells are divided into three types: those which are immediately deallocated, \nthose which are reference counted and those which are garbage collected. Then a particular cell which \ncan be identified as never being shared in the future can be allocated as the first type, one which may \nbe shared but will never be cyclic as the second type, and the remainder as the last type. We leave the \ndetailed development and analysis of this approach to later work. It should be noted in closing that \nBaker [11 has studied the situation in which reference counting is an appropriate method for dynamic \nstorage management. His findings indicate that our intended use is an appropriate one. REFERENCES 15. \nMilne, Robert and Christopher Strachey, A Theorg of Programming Language Semant<cs, Chapman and Hall, \nLondon; Halsted Press, John 1. Baker, Henry G., Jr., List Processing in Real Wiley, New York, 1976. Time \non a Serial Computer, CACM, vol. 21, no. 4, 1978, pp. 280 -294. 16. Reynolds, John C., Automatic Computation \nof Data Set Definitions, PYOC. of IFIF Congress 2. Barth, J. M., Shifting Garbage Collector 68, August \n1968, pp. B69 -B73. Overhead to Compile Time, CACM, vol. 20, no. 7, 1977, pp. 513 -519. 17. Schwartz, \nJ. T., Optimization of Very High Level Languages -I: Value Transmission and  3. Brainerd, W. S., Tree \nGenerating Regular Its Corollaries, Computer Languages, vol. 1, Systems, Infomat{on and Control, vol. \n14, 1975, pp. 161 -194. 1969, pp. 217 -231. 18. Stoy, Joseph E., Denotational Semantics: 4. Biichi, \nJ. R., Regular Canonical Systems, The Scott-St~aehey Approach to P~ogrming Archiv f. Math. Logik und \nGrund., vol. 6, Language Theoryj, MIT Press, Cambridge, MA, 1964, pp. 91 -111. 1977. 5. Clark, D. W. \nand C. C. Green, An Empirical 19. Thatcher, J., Tree Automata: An Informal Study of List Structure in \nLISP, CACM, Survey, in Aho, Alfred (cd.), Currents in the vol. 20, no. 2, 1977, pp. 78 -87. Theory of \nComputing, Prentice-Hall, 1973, pp. 143 -172.  6. Cousot, Patrick and Rhadia Cousot, Automatic Synthesis \nof Optimal Invariant Assertions: Mathematical Foundations, Proc. ACM Symp. on Artif. Intel. and Prog. \nLang., SIGPLAfl Notices, vol. 12, no. 8, August 1977, pp. 1-12. 7. Cousot, Patrick and Rhadia Cousot, \nStatic Determination of Dynamic Properties of Generalized Type Unions, SIGPLAN Notices, vol. 12, no. \n3, March 1977, pp. 77 -94. 8. Engelfriet, J., Tree Automata and Tree Grammars, DAIMI Report FN-10, Department \nof Computer Science, University of Aarhus, Denmark, 1975. 9. Ginsburg, Seymour, The Mathematical Theo~y \nof Context-Free Languages, McGraw-Hill, New York, 1966. 10. Goto, E., Monocopy and Associative Algorithms \nin an Extended LISP, University of Tokyo, Japan, May 1974. 11. Hoare, C. A. R., Recursive Data Structures, \nInte~. J. Comp. and Sys. Sei., vol. 4, no. 2, 1975, pp. 105 -132. 12. Jones, N. D. and S. S. Muchnick, \nBinding Time Optimization in Programming Languages: Some Thoughts Toward the Design of an Ideal Language, \nProc. 3rd ACM SIGACT -SIGPLAN Symp. on Prine. ofProg. Lang. January 1976, PP. 77-94. 13. Kaplan, Marc, \nRelational Data Flow Analysis, Technical Report 243, Dept. of Elec. Eng. and Comp. Sci., Princeton University, \nApril 1978 (revised). 14. Kaplan, Marc and J. D. Unman, A General Scheme for the Automatic Inference \nof Variable Types, Conf. Record of .5th ACM Symp. on PrZne. ofProg. Lang., Tucson, AZ, January 1978, \npp. 60-~~.  256 \n\t\t\t", "proc_id": "567752", "abstract": "In [12] the authors introduced the concept of binding time optimization and presented a series of data flow analytic methods for determining some of the binding time characteristics of programs. In this paper we extend that work by providing methods for determining the class of shapes which an unbounded data object may assume during execution of a LISP-like program, and describe a number of uses to which that information may be put to improve storage allocation in compilers and interpreters for advanced programming languages.We are concerned chiefly with finding, for each program point and variable a finite description of a set of graphs which includes all the shapes of values the variable could assume at that point during the execution of a program. If this set is small or regular in structure, this information can be used to optimize the program's execution, mainly by use of more efficient storage allocation schemes.In the first part we show how to construct from a program without selective updating a tree grammar whose nonterminals generate the desired sets of graphs; in this case they will all be trees. The tree grammars are of a more general form than is usually studied [8, 19], so we show that they may be converted to the usual form. The resulting tree grammar could naturally be viewed as a recursive type definition [11] of the values the variables may assume. Further, standard algorithms may be employed to test for infiniteness, emptiness or linearity of the tree structure.In the second part selective updating is allowed, so an alternate semantics is introduced which more closely resembles traditional LISP implementations, and which is equivalent to the tree model for programs without selective updating. In this model data objects are directed graphs. We devise a finite approximation method which provides enough information to detect cell sharing and cyclic structures whenever they can possibly occur. This information can be used to recognize when the use of garbage collection or of reference counts may be avoided.The work reported in the second part of this paper extends that of Schwartz [17] and Cousot and Cousot [7]. They have developed methods for determining whether the values of two or more variables share cells, while we provide information on the detailed structure of what is shared. The ability to detect cycles is also new. It also extends the work of Kaplan [13], who distinguishes only binary relations among the variables of a program, does not handle cycles, and does not distinguish selectors (so that his analysis applies to nodes representing sets rather than ordered tuples).", "authors": [{"name": "Neil D. Jones", "author_profile_id": "81452616043", "affiliation": "The University of Kansas, Lawrence, Kansas", "person_id": "PP95034660", "email_address": "", "orcid_id": ""}, {"name": "Steven S. Muchnick", "author_profile_id": "81332517217", "affiliation": "The University of Kansas, Lawrence, Kansas", "person_id": "PP31023637", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/567752.567776", "year": "1979", "article_id": "567776", "conference": "POPL", "title": "Flow analysis and optimization of LISP-like structures", "url": "http://dl.acm.org/citation.cfm?id=567776"}