{"article_publication_date": "01-01-1979", "fulltext": "\n Permission to make digital or hard copies of part or all of this work or personal or classroom use is \ngranted without fee provided that copies are not made or distributed for profit or commercial advantage \nand that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, \nto post on servers, or to redistribute to lists, requires prior specific permission and/or a fee.&#38;#169; \n1979 ACM 0-12345-678-9 $5.00 AUTOMATIC DATA STRUCTURE SELECTION IN SETL Edmond Schonberg and Jacob T. \nSchwartz and Micha Sharir Illinois Institute of Technology Courant Institute of Mathematical Sciences \nAbstract SETL is a very high level programming language supporting set theoretical syntax and semantics. \nIt allows algorithms to be programmed rapidly and succinctly without requiring data structure declarations \nto be supplied, though such declarations can be manually specified later, without re\u00adcoding the program, \nto improve the effici\u00adency of program execution. We describe a new technique for automatic selection \nof appropriate data representations during compile-time for undeclared, or partially declared programs,and \npresent an efficient data structure selection algorithm, whose complexity is comparable with those of \nthe fastest known general data-flow algorithms of Tarjan [TA2] and Reif [RE]. 1. Introduction The level \nof a programming language is determined by the power of its semantic primitives, which influence the \nease and speed of programming in the language pro\u00adfoundly. (See [HA] for an attempt a t quantifying these \nabstract concepts.) Thus a language of very high level shc]uld provide high level abstract objects and \noperations between them, high level cc)n\u00adtrol structures and the ability to select data representation \nin an easy and flexi\u00adble manner. It is the third property of very high level languages that we address \nin this paper. =S work was supported by the National Science U.S. D.O.E. Office of Energy Research Contract \nIn relatively low level programming languages, data-structures have to be selected in advancer before \nstarting to code the program; the code to be written then depends heavily on this selection and large \nsections of it are solely devoted to the manipulation of the selected data\u00adstructures. These lengthy \ncode sections constitute a significant source of bugs and become deeply imbedded in the program logic \nto the extent that they have to be replaced or modified when we want to change the data representation. \nThe programming language SETL, being designed and implemented at New York Uni\u00adversity, will serve in \nthis paper as a prototype of a very high level language which treats the selection of data struc\u00adtures \nin a different way. We will describe an automatic technique which enables the SETL programmer to code \nhis program in a high level, relatively inde\u00adpendent of specific data-structure, and yet allows a reasonable \nlevel of effici. ency to be achieved. In the SETL system, the data represen\u00adtation used to realize an \nalgorithm depends on its code and not vice versa. More specifically, algorithms are coded without specifying \nany concrete data structures at all. The objects appearing in a program are (dynamically) assigned appropriate \nabstract data types from among the basic data types supported by the Foundation Grant MCS-76-00116 and \nthe EY-76-C-02-3077. 197 language. In the optimizing version of SETL, each such data type is viewed as \na generic indication representing a collec\u00adtion of more specific data-structures, all of which are capable \nof representing the same abstract data type. Thus the data type set can be represented as a hash table, \nor a linked linear list, or a bit-string, etc. However the semantic features of the abstract SETL data \ntypes, as well as of the operations on them, are independent of any specific data struc\u00adture selection. \nThus program code need not be modified when this selection is made. Once an algorithm in SETL has been \ncoded, debugged and executed successfully (and in future endeavors, also proved correct) , data representations \ncan be selected in order to improve execution efficiency. Two selection techniques are provided. The \nmore conservative technique is manual. In khis technique the program text is supplemented with declarations \nspecifying data structure representation for some (or all) program variables to be used. If these representations \nare con\u00adsistent with the abstract data types actually acquired by the declared vari\u00adables during execution, \nthen the supple\u00admented program will be precisely equiva\u00adlent to the original purely abstract one, but \ncan run much more efficiently. In the second, more ambitious data\u00adstructuring mode, which is the one \nto be described in this paper, data-structure selection is performed automatically by an optimizing compiler. \nThis is in general a complex task because each particular data structure will usually be more efficient \nfor some instructions and less efficient for others, so that in order to arrive at a realistic evaluation \nof the cost of us\u00ading alternative data structures for a given program variable, we must perform an appropriate \nglobal analysis of the way in which program objects are used and related to each other. Our main aim \nin this paper is to pre\u00adsent an automatic data structure choice algorithm. While this algorithm reflects \nthe particular semantic environment of SETL, and therefore cannot be regarded as a general purpose automatic \ndata structure selection procedure, it does establish the possibility of performing automatic data structure \nselection in a reasonably effi\u00adcient manner in a language of very high level. This paper is organized \nas follows: In Section 2 we briefly review SETL and its manual data structuring system, known as the \nbasing system. Section 3 will describe our data-structure selection algorithm in detail, including examples \nillustrating the way in which this algo\u00adrithm applies to sample programs. We take this opportunity to \nthank several members of the SETL project at New York University, namely: Robert B. K. Dewar, Ssu-Cheng \nLiu and Arthur Grandr for numerous suggestions concerning auto\u00admatic data-structure selection. In particular, \nour work has been greatly influenced by earlier work of Liu [LI]. 2. The SETL Language and its Basing \nSystem We now summarize the principal features of the SETL language. SETL admits finite set theoretical \nobjects, such as arbitrary finite sets, maps and tuples, and supports most of the opera\u00adtions between \nthese objects. The language also supports the major elementary data types, found in most programming \nlanguages, in particular integers, reals and strings. true and false are strings used to represent boolean \nvalues. Tuples in SETL are arbitrary-length, dynamically extensible ordered sequences of component values, \nwhich can be either primitive or themselves structured. Tuple concatenation, indexed retrieval and storage \nof components, subtuple retrieval and storage operations, and a tuple length operator are provided. Sets \nin SETL are unordered collections of elements (these elements being p.cimi\u00adtive or structured), such \nthat no element can appear in a set more than once. SETL provides the usual set-theoretic opera\u00adtions \n(union, intersection, etc.) , and some special operators, summarized below. SETL also provides general \nset former expressions, universal and existential quantifiers, compound operators on sets and similar \nhigh level constructs. Maps in SETL are simply sets of tuples of length 2 (called pairs) , and can repre\u00adsent \nboth functions and relations. SETL provides functional style constructs for map retrieval and storage, \nas well as several special operators, e.g. domain and range, which compute the domain set and the range \nset of a map. Since maps are sets, all set-valued operators can also be used with maps. Data structures \nsuch as trees and graphs are represented in SETL using maps which give the relationships between the \nelements of the structure, without having to specify the detailed storage structure to be used. A special \nvalue OM (for omega) is used to indicate an undefined value. All vari\u00adables are initialized to OM and \ncertain operations can also yield OM if domain constraints are not met. For example, v(i) yields OM if \nv is a tuple whose pre\u00ad sent length is less than i. The SETL control structures are gener\u00adally conventional, \nand include the if-then\u00adelse clause, case statements, while loops, numerical iteration, etc. , and also \na few additional very high level control struc\u00adtures, e.g. iteration over a set, univer\u00adsal and existential \nquantifiers, etc. SETL, like APL, has value semantics rather than pointer semantics. This means that \nthe value of each program variable is essentially independent of the values of other variables and will \nnot be affected when other variables are modified. Fc}r . storage optimization, several variables can \nshare a common value using pointer mechanisms, but care will always be taken to create new copies of \nthat value when\u00adever logically necessary. This value semantics also implies that subprocedure parameters \nare passed by value rather than by name, and that no explicit reference and manipulation of pointers \nare allowed. The following table summarizes the main operations of SETL. For additional details, see \n[DE] and [SC3]. Table 1. Some of the Primitive SETL Operations. Operation Remarks X+y integer and real \naddition, set union, tuple and string concat\u00ad enation. x- Y integer and real subtraction, set difference \nx*y integer and real multiplication, set intersection, string and tuple repetition x/y arithmetic division \nx and y, Boolean operations x or y, x implies Y not x #x cardin~lity of sets, length of tuples and strings \nX=y, equality and inequality x /=Y comparisons X<y, arithmetic comparisons, X>y, x >= y, string lexicographical \nX<=y comparisons x with y set insertion, tuple increment\u00ad ing x less y set deletion xiny set membership \ntest arb x select arbitrary element of x X from y select and delete an arbitrary,: element of the set \ny {x/y,...} set with specified elements [X,yr. ..l tuple with specified elements f(x) function or subprocedure \ncall, indexed retrieval from a tuple or a string. If f is a map, f(x) is the unique y such that [x,y] \nin f, if,such y exists; otherwise f(x) = om (undefined) f{x} the set of all y such that [xry] in f; \nf must be a map. domain f, operators producing domain and range f range of maps x := Y simple assignment, \nyielding y as a value f(x) := y map storage (which will cause all other pairs [x,z] to be deleted from \nf) indexed assign\u00ad ment for tuples and strings f{x} := y map storage corresponding to the retrieval operator \nf{x} f(x. ..y) extract subpart starting at component number x and ending at y of the tuple or string \nf (corresponding storage operator is also available) . newat generate a new unique atom type x the current \ntype of x In addition to the above primitives SETL provides several high level con\u00adstructs involving \nexplicit or implicit iterations over sets and tuples. Table 2 summarizes some of these constructs. In \nthis table, the clause-type iterator denotes a construct of the form ine,x *in e2(xl), . . ..xn in 1 \n1 en(x 1,...,x n_l) I c@lrx2r. ..,q where e r...r e n are set-valued expres 1 sions , and iteration is \ncarried out by assigning all possible elements in the corresponding sets to x ~rx2, . . ..xn. Moreover, \nC(X , . . . ,x ) is a Boolean 1n valued expression, which defines the sets of values to be bypassed in \nthe iterations. Table 2. Additional High Level SETL Constructs Name Form iteration (if iterator) over \nsets set former {exp(xl,. ..,xn) : iterator} where x 1 f...! x n are the free variables of the iterator. \ntuple former [exp(x, where ,.J. x l . ..,)-) 11 .. xn : iteratorl are as above (if order is important \nall iterations should be numerical. ) existential 3 iterator quantifier the free variables x, ,. 1 . \n. , x\u00ad11 are set to their first value during the iteration for which the condition c is true, and are \notherwise undefined. The expression yields true or false respectively. The special constant nl denotes \nthe null set (or map) . Let us now describe the data-struc\u00adtures which SETL supports. Tuples are represented \nas dynamic arrays of consecu tive memory locations (which have to be re-allocated if their length increases \nbeyond their current allocation); sets are represented as breathing linked hash tables, so that iteration \nover a set is fairly rapid, but most of the other set operations involve hashing into a set, an operation \nwhich can be expected to be relatively costly. In fact, as will be seen later, the main optimization \nthat our automatic selection of data structures aims to achieve is to minimize the number of hashing \noperations performed during the execution of a program. The default representation of maps is either \ntheir standard set representation, or, if it can be asserted during compilat\u00adion that an object will \nalways assume a map value, then it is represented as a linked hash table, hashed on the domain elements \nof the map, where each entry in the table points to the range value of the corresponding domain element. \nThis expe\u00addites map-related operations, such as map retrieval and storage, but makes glcbal set operations, \nsuch as set union, ,slighti\u00adly more cumbersome than they are for the default set representation. In order \nto obtain a coherent extended class of efficient data representations, we introduce new program objects, \ncalled bases, which are used as universal sets of program values (cf. [DE] for a more detailed description). \nBases enable us to access related groups of program variables in specially efficient manner. This is \nthe only use of bases. Once one or more bases have been introduced, other program variables can be described \nby their relationship to these bases. The set of these declarations will determine the run time value \nof these bases, because bases are constantly updat\u00aded to maintain the validity of these relationships. \nFor example, we can declare a program variable x to be an element of a base B by writing If this declaration \nis made, then during execution, any value assigned to x is auto\u00admatically inserted into B, unless this \nvalue is found to be already in B. Internally, a base B is represented as a linked hash table of element \nblocks . Each such block contains a program value (or a pointer to such a value) and as many additional \nfields as are needed to store program variables which have been declared to be based on B. i + variable \nx declared as E B is represented by a pointer tic> some element block of B, so that the actual value \nof x can be obtained simply by de\u00adreferencing this pointer. Whenever x is represented in this way we \nshall say that x is located in B. Computation of such a po%nter, which may imply insertion of a new value \ninto B, will be called a base locate operation. A main aim of our data structuring system is to minimize \nthe number of these search and insert opera\u00adtions (normally realized as hashing oper\u00adations) which are \nperformed during program execution. In addition to the element of a base representation described above, \nwhich can be declared for any program variable, related based representations are avail\u00adable for sets \nand maps. A set s can be declared as a subset of a base B by writ\u00ading one of the following declarations: \ns: local set (C B);  s: indexed set (~ B) ;  s: sparse set (~ B) ; If the first declaration is used, \na single bit position is reserved in each element block of El; this bit is on iff the element represented \nin the block belongs to s. This representation of s supports very fast insertion and deletion of elements \nfrom s and membership tests, operations that would have otherwise required hashing into s, and it corres\u00adponds \nto the familiar notion of an attri\u00adbute bit (or flag) in a plex structure. If the second declaration \nis used, then all these bits will be grouped into a bit-string, stored independently of B. In order to \naccess this string via B, a unique index is assigned to each element block of B when this block is created. \nThe bits of s are arrahged in the order of these indices. Thus , to perform the insertion operation s \nwith x ; where x is declared as E B, the following steps are taken: (A) retrieve i(x), the index of the \nelement block to which x points.  (B) turn on the i(x)-th bit of s. Note that in both cases the set \ninser\u00ad  tion operation s with x; is fast only if some set of integers. x is represented as an element \nof B. In any other case, the value of x must first be hashed into B to find the corresponding element \nblock (or create a new element block if the value of x is not yet in B). This observation indicates that \nbased representations are profitable when con\u00ad sistent basings are given to all the vari\u00adables involved \nin hashing operations, using the same base. Even when this is done, hashing operations will still be \nrequired to create B; however, they will be fewer than in the unbased case. The first two declarations \nhave one common disadvantage. To iterate over s we must iterate over B and perform a membership test \nin s for each element in B. This is certainly less efficient than a direct iteration over s as would \nhave been done in the unbased case, and is especially so if the cardinality of s is much smaller than \nthat of B. When itera\u00adtion over such a sparse set is performed often,we can use the third representation, \nin which s is represented by a linked hash table whose entries are pointers to element blocks of B. This \nrepresentation suPports fast iteration over s, and is slightly more advantageous than the unbased representation \nfor search, inser tion and deletion from s, since equality of pointers can be checked rapidly whereas \nequality of general values is much more expensive. Similar alternative representations are available \nfor maps. A map f whose domain is a subset of a base B can be declared in one of the following ways: \nf: local map (G B) *;  f: indexed map (~ B) *;  f: sparse map (E B) *; where * denotes any representation \nfor the range of f. We will temporarily assume that f is single-valued, to simplify the description of \nits corresponding represen\u00adtations. Thus for example, map (~B) int denotes a map from some subset of \nB to If the first declaration is used, a fixed field within each element block x of B is allocated and \nreserved for storing the value, or a pointer to the value, of f(x) (or om if f(x) is undefined). This \nmakes map retrieval and storage operations very efficient. For eammple, the instruc\u00adtion := f(x); , where \nx is represented Y as an element of B, can be implemented as a simple indexed load, and the need to hash \non x in f is eliminated. This local map representation captures the familiar notion of data structures \nconsisting of plexes (base element blocks) within which various fields contain either values (if the \nrange of the corresponding map is unbased) or pointers to other plex nodes. Local representation of sets \nand maps have one basic disadvantage. Since they are allocated in static fields within each element block \nof a base, their number must be predetermined at compile time. Also, no other variable can share their \nvalue without violating the value semantics of the language. Thus whenever such variables are assigned \nto other variables, or incor porated within other composite objects, or are passed as parameters to a \nprocedure, their value must be copied first. Hence, if such a variable is frequently subjected to operations \nof this kind, its represen tation may be quite inefficient. The indexed representation is provided to \navoid such problems. If the indexed declaration is used for f, then an array V, disjoint from B, is allocated \nfor f and contains the range values of f in the order of the element-block indices in B. Thus to retrieve \nf(x) , provided that x is represented as E B , two indexed loads are performed, as follows: (A) retrieve \ni(x), the index of the element block to which x points.  (B) retrieve V(i(x)).  We will refer to each \nof the data structures which can be declared in the above-described basing system as a mode. . Modes \nthus range from primitive modes such as int, ~, atom and string, to compos\u00adite modes which can be nested \nto any level. For example, a two dimensional map can be represented as local map (e Bl) indexed map (C \nB2) int A value of a map declared to have this representation can be retrieved using three indexed loads, \nwhich is even faster than retrieval of a component of a two dimensional array in FORTRAN. 17e now illustrate \nthe preceding con\u00adsiderations by an example showing the process of declarative data-structure selection \nin SETL. The following program computes a minimum cost path between two nodes in a directed graph (statement \nnumbers are given for later reference) : Program minpath; (1) read (graph,cost,x,y) ; $ read in graph \nand auxiliary infor\u00ad$ mation. graph is a set of eclges, $ cost is an integer-valued map on $ these edges, \nx is the source node $ and y is the target.  (2) prev := g; $ prev is a map from $ each encountered \nnode to its  $ cheapest predecessor.  (3) val := ~; $ val maps each encounter\u00ad $ ed node to the minimal \ncost c)f a $ path reaching it from x.  (4) val (x) := O; $ start node has zero $ cost path  (5) newnodes \n:= {x}; $ initially, start $ node is newly reached  (6) (while newnodes /= nl) $ while there $ exist \nnewly reached nodes  (7) n from newnodes; $ select such a node  (8) (Vm~graph{n}) $ and for each of \nits successors  (9) newval := val(n)+cost n,m) ; $ calculate new path cost  (10) j&#38; val(m)=om or \nval(m, $ cheaper path  (11) val (m) := newval;  $ note path value (12) prev(m) := n; $ and cheapest \npredecessor (13) i&#38;m/=y~ $ keep searching if goal $ not reached  (14) newnodes with m; end if; \nend if;  end V; end while;  if val(y) = om then $ y is not reachable $ from x print( y is not reachable \nfrom x ); else path := [y]; $ build up reversed path z := y; $ starting with the last node (while (z \n:= prev(z)) /= om) $ chain to preceding node path with z; end while; $ and now reverse path path := \n[path(#path+l-i) :i:=l. ..#pathl ; print(path) ; end if; end program minpath; This is the pure SETL program \nin which no specification of data structures has been supplied. After this program has been coded and \ntested, we can select effi\u00adcient data structures for its variables. A typical choice might be as follows: \nintroduce a base nodes, which will be the set of all nodes in the graph. Then declare: graph: local \nmap (G nodes) C nodes; . cost: local map (~ nodes) indexed map (6 nodes)=; newnodes: local set (~ nodes); \n. prev: local smap (~ nodes) e nodes; . (i.e. single-valued map) val: local smap (G nodes) int; . x,yrm!n: \nE nodes: path: tuple (~ nodes); _ In the presence of these declarations, a hash table will be generated \nfor the base nodes. This hash table will be filled in automatically as soon as the graph is read in by \nour program. Each block in this hash table will contain several fields which store values, bits and pointers \nto other entries in this table. After the input phase, the rest of the program is executed without performing \na single hashing operation. We pay a very small price for this huge saving when we print path, since \neach component must be dereferenced to its actual value before being printed out. We stress the point \nthat we can map our objects onto lower level data structures and generate code sequences quite close \nto those which would appear in a lower level variant of our algorithm (such as one written in PASCAL \nor PL/1) and attain comparable efficiency without recoding the original form of our algorithm at all. \nThis example indicates that the trade\u00adoff between programming language level and execution efficiency \nneed not be as unfav\u00adorable as is generally expected. Of course, in our example we lower somewhat the \nlevel of pure SETL, by supplying the data-structure declarations . However, this program supplement \nis very small, compared with the labor that would be involved in an actual recoding of the algorithm \nin a lower level language. Furthermore, as will be shown in the followlng sections, the data structures \nthat we have declared can be selected automatically, thus relieving the program\u00admer of the task of specifying \nany data structures at all. 3. An Automatic Data-Structure Selection Algorithm The algorithm to be sketched \nbelow is not our first attempt at performing auto\u00admatic data-structuring in SETL; see [SC] , [SLI and \n[DE]. A strategy common to these approaches, and to our new algorithm as well, is to generate provisional \nbases and corresponding based representation for variables involved in operations otherwise requiring \nhashing. These provisional bases initially reflect only local infor\u00admation, and separate bases are generated \nfor each hashing operation. To integrate these local provisional basings into an overall basing structure, \nthey are prop\u00adagated globally. During propagation individual bases are equivalence whenever logically \nappropriate. We feel that the efficiency and simplicity of our present algorithm makes it our best candidate \nfor performing automatic data structuring in SETL . To illustrate the strategy which this algorithm employs, \nconsider the following SETL code fragment (taken from the MINPATH program shown in the previous section) \n: (1) n from newnodes; (2) prev(m) := n; ...  (3) newnodes with m;  In this code, instructions (2) \nand (3) implicitly require hashing operations. Thus we first generate two bases 2 ~3 and provisionally \nestablish the represen\u00ad tations prev2 :m-(~ B2) *; m2 :~B2; newnodes : set (~ B3) ; :EB. 3 33 (where \nvi denotes the occurrence of a variable v at instruction i, and * denotes any mode) . Note that we generate \nrepre\u00adsentations for variable occurrences rather than for the variables themselves; this is because SETL \nis only weakly typed, so that each variable may assume more than one data-type in the course of execution. \n(Various problems arising in view of this fact will be addressed later on.) However, since there exists \na data flow link between m2 and m3 ~ it follows that m2 and can assume the same value, which must 3 therefore \nbe an element both of B and B3. 2 We therefore merge the representations of and m into one common representation \n2 3 by identifying B2 with B3. We can then  propagate the resulting basing to instruc\u00adtion (1) , which \nis a retrieval operation whose execution speed is virtually inde\u00adpendent of the representation of its \narguments. Then, in view of the data-flow link between newnodesl and newnodes3, we give newnodesl the \nsame representation as newnodes3, and also put nl : C 133. An additional propagation step, using the \n-n2 link, gives prev2 the representa\u00ad 1 tion map (e B3) ~ B3 . Our algorithm mechanizes the strategy \nthat has been sketched above in a rela\u00adtively efficient and simple manner. Before describing this algorithm, \nlet us sketch the form of input it assumes. We suppose that the program to be analyzed has already undergone \nseveral other analyses and optimization, including a modified version of the definition-use chaining \nanalysis (cf. [AL]) which computes a data\u00adflow map, called BFROM, whose definition is as follows: Let \nVO1, V02 be two occur\u00adrences of a variable V. Then Vol E BFROM{V02} iff V02 is a use of V and their exists \nan execution path leading from VO1 to V02 which ~s free of other occurrences of V (cf. [SC2] for more \ndetails) . We also assume that type analysis has been performed using Tenenbaum s approach [TEI , so \nthat type information will have been assigned to each variable occurrence, in a manner ensuring that \nthe calculated type of each variable occurrence dominates every data type that the variable can acquire \nat that program point during execution. Assuming all this, our algorithm consists of the following phases: \n.. (a) base generation (b) representation merging and base equivalencing  (c) base-pruning and representation \nadjustment  (d) name-splitting.  (a) base generation: This phase performs a linear pass through the \ncode being analyzed. For each instruction I we introduce enough bases and corresponding based representations \nfor arguments of I to ensure that execution of I with these based representations for its arguments is \nnot slower than execution of I with unbas\u00aded arguments. For example, (i) Suppose that I is S with x \n. Then we introduce a base B1, and provisionally assume the representations set (~ B1) ;  1: .EB Note \nthat here the introduc\u00ad 1-1. tion of BI speeds up the execution of I considerably. (ii) Next, suppose \nthat I is f(x) := y . Here we introduce two bases, B;, B:, and provisionally represent fl: map (G B~)~B~; \n12 :GB. : G B1; here only B; is I 1 I essential, since its introduction elimi\u00ad nates a hashing operation, \nand we refer to B: as an effective base. The introduction of B: does not speed up execution of I (but \ndoes not slow it down either), and 2 we refer to B1 as a neutral base. The utility of neutral bases \nwill become clear in our description of the next phase of the data structure choice algorithm. (iii) \nNext, suppose that I is x := v(j) , where v is a tuple. Here no speed-up is possible, but nevertheless \nwe introduce a (neutral) base B= , and provisionally establish the representations : tuple (~ B1); X1 \n: ~ B1; . I (iv) Finally, suppose that I is x :=x,+ 1 . Here no base can be intro\u00adduced without running \nthe risk of slowing I down significantly, because of the pos\u00adsible introduction of conversion opera\u00adtions \nfor the newl~ created values of x , between their int mode and their tenta tive (~ B) mode. Hence we \nintroduce no base for I. In this first phase we also build up a map EM, mapping each generated base to \nthe mode of its elements. During this phase all these modes are unbased, but they may be transformed \ninto based modes during phase (b) . (b) representation merging and base equivalencing: This phase executes \n a linear pass through the map BFROM. For each pair of variable occurrences such that both VO and(V01,V02) \n~ BFROM 1 Vo ~ have the same type, and supposing that VOl and V02 have both received based representations \nin phase (a) , we perform the following representation merging oper\u00adation (recursively) : Let RI, R2 \ndenote the based represen tations of VOl, V02 respectively. Three cases are possible: (i) Both R1 and \nR2 are base pointers, i.e. R isEB and R isEB In this 1 12 2 case, equivalence B1 and B2. (ii) One of \nthese representations, say isEB ~ and the other is a composite  1 based representation. In this case, \nmerge EM(B1) with R2, setting EM(B1) to R2 if the former is an unbased mode. (iii) Both R1 and R2 are \ncomposite repre\u00adsentations . Since the gross set-theoretic types of VO1 and V02 are ass umed to be equal, \nthe composite structures specified by RI and R2 must also have the same gross type (i.e. both must be \nsets, or maps, or tuplesr if one is) . In this case merge the element-mode of RI with that of R2 (if \nRI and R2 represent maps, merge their domain element-modes and their range element-modes separately; \nif Rl and R2 represent tuples of the same known length n, each component then having its own type and \nrepresentation, perform n componentwise merges) . This merging/equivalencing process can be made highly \nefficient by using a compressed balanced tree representation for the set of all generated bases (cf. \n[T?+] ) . This representation allows execu\u00adtion of a sequence of equivalencing opera\u00adtions in almost \nlinear time. The element mode map EM need be kept only for tree roots (i.e. we only need to keep one \nvalue per equivalence class) . Whenever two trees whose roots are Bl,B2 are merged into one, we also \nupdate the map EM of the new root (which is either B1 or B2) , as follows: (i) If both EM(B1) and EM(B2) \nare unbased, they must be equal, so that no updating need take place. (ii) If one of them is based and \nthe other is not, set EM of the new root to the based mode.  (iii) If both are based, then leave either \nof the two basings at the new root, but merge them with one another. Note that phase (b) uses the neutral \nbases introduced in phase (a) to transmit\u00adbasing information between instructions without actually having \nto use any global propagation technique. This point is illustrated by the following example: (1) Swith_ \nx; $Sis a set (2) V(i) := S; $V is a tuple (of sets)  (3) T := V(j); (4) y_from T; $Tis a set  Phase \n(a) will have generated the follow\u00ading provisional epresentations: : set (G Bl); x:EB 1 11 : tuple (e \nB2 ; :EB 2 22 : tuple (~ B3 : :EB V3 33 set (~ B4) ; Y4: GB4 4: Using the S1-S2 link, phase (b) \nwill set EM(B2) = set (~ Bl). In virtue of the V2-V3 link we then equivalence B2 and B3, setting EM of \nthe new root to set (~ Bl) ! Then, in virtue of the T3 T 1 link, we merge EM(B3 z B2) with set (~ B4) \n, i.e. merge Set (~ Bl) t and Set (~ B4) which causes B and B q to be equivalence. If 1 and B3 were \nunavailable, this deduc 2 tion would have been more difficult, and rather complex propagation of basing \ninformation through the code would have been required. (c) base pruning and representation adjustment: \nWhen phase (b) termi\u00adnates , the set of all initial bases will have been split into equivalence classes. \nEach such class corresponds to one actual base B, and the map EM maps the root of this class to the common \nrepresentation of the elements of B. Howeverr it is possi\u00adble that such a base B may be useless, in the \nsense that its introduction cannot. speed Up the execution of any instruction, or, even if some instructions \nare made more efficient due to the introduction of B, all these instructions involve only the same composite \nobject s based on B. In this latter case, introduction of B will simply replace the hash table of s by \nthat of B, which gains us nothing. Such cases are detected and eliminated by phase (c) , as follows: \n(i) We find all actual bases which are useless according to the criterion stated above, and flag them \nas such. (ii) We update all based representations of variable occurrences and also the element modes \nof all actual bases, in the following recursive manner:  (ii.1) unbased modes are left unchanged; (ii.2) \neach provisional base B appearing in a mode is replaced by the corresponding A actual base $ if ~ is \nnot useless. If B is useless, we replace the submode ~ B by EM(;) . (iii) We enter all useful bases into \nthe symbol table. It can be shown that the preliminary type-analysis phase of the optimizer can be adjusted \nin such a way as to guarantee that this recursive adjustment operation will always converge. Note also \nthat it is only after this adjustment that a variable occurrence can have a based representation involving \nmore than one level of specification, e.g. set (tuple (~ B)) . As an example, consider the case of an \ninkeger-valued bivariate map f. In compilation of SETL, each retrieval of the value f(x,y) is expanded \ninto the code sequence (1) t := f{x};  (2) z := t(y); Phase (a) of our algorithm will generate the following \nprovisional representations:  :EB. : map (~ Bl) ~ B2; 1 2 1 : map (~ B3) C B4; 2 Now we assume that \n(the equivalence class of) B2 turns out to be useless (which is the case, e.g., if f is always accessed \nas a bivariate map) and also that B4 is use\u00ad less (e.g. no set manipulation of the range of f occurs \nin the program being analyzed) . Phase (b) will merge the representations of tl and t2 to obtain EM(B2) \n= map (E B3) ~ B4. Hence phase (c) will update the representation of fl to map [~ Bl) map (c B3) e B4 \nand again to map (~ Bl) map (~ B3) int , which is probably the representation that a program\u00admer would \nhave chosen manually, and is also the best representation for sparse multivariate maps defined on-arbitrary \ndomains. (d) Name-splitting: This is the final phase of our data-structuring algorithm. At the end of \nphase (c) , based rePreSenta\u00ad tions, as well as type information, will have been computed for each variable \noccur\u00adrence, rather than for each variable. We use occurrences rather than variables because the weak \ntyping of SETL allows a variable to assume more than one data-type during execution; moreover, even objects \nwith the same data-type may be represented in different ways at difference occur\u00adrences of the same variable. \nNevertheless the information collected by our algorithm must finally be stated on a per variable basis, \nsince subsequent compiler phases (e.g. our machine-code generator) cannot support more than one type \nor detailed representation per variable. Furthermore, the first three phases of our algorithm ignore \nthe operations which actually insert elements into a base B. For example, consider the code (1) x := \nx+1; (2) s with x; The first three phases of our algorithm will assign int as the representation of \n and e B as the representation of x2t 1 where B is some base. This means that instruction (2) can assume \nthat the value of x is represented by a pointer into B, but of course such a pointer must be first created. \nThese two problems of information integration and base insertion are solved simultaneously in the name-splitting \nphase of our algorithm. In this phase we first scan all occurrences of each program vari\u00adable x. For \neach representation R assigned to at least one of these occur\u00adrences, we generate a new symbol-table \nentry, which we denote by XR, and which is said to be split from x. We then replace all occurrences of \nx having the represen\u00adtation R by occurrences of XR. If two occurrences of the same original variable \nx having different representations RI, R2 are linked by BFROM, then we must of course ensure that conversion \nof the value of x from representation R ~ to representa tion R2 will take place as control advances from \nthe first occurrence to the second one. To enforce this conversion, we insert an assignment := xR into \nR2 1 the code at some optimal (lowest-frequen cy) place separating these two occur\u00adrences . The algorithm \nwhich inserts such conversions into the code is rather simple and is based on the interval structure \n[AL] of the flow graph of a SETL program. For each occurrence VO of a variable V which is linked by BFROM \nto other occur\u00adrences of V which have different represen\u00adtations, our algorithm will insert a conversion \nto the split variable of VO either just before VO, or just before the head of some interval containing \nVO. For example, in the code fragment (v ...) x := x+1; end t; . (v . ..) s with x; end V; A conversion \nfrom the unbased representa\u00adtion appropriate for x within the first loop to the based representation \nappro\u00adpriate for x within the second loop will be inserted just before the start of the second loop. \nThe choice of the interval at whose head we insert a conversion involves safety criteria which will not \nbe mentioned here. For a full description of this aspect of our method, the reader is referred to [GS] \n. It is important to note that the set of all program points at which there occur conversions to representations \nbased on a base B is also the set of all points at which elements are added to B (by hashing operations) \n. In general the number of these conversions will be smaller (and for typical programs, substantially \nsmaller) than the number of hashing operations which would have been required without the presence of \nB. A comment on the complexity of our algorithm: Phases (a) and (c) are linear in the length n of the \ncode. Phase (b) is almost linear in the number m of BFROM links, which for typical programs wili be linear \nin n. Phase (d) is O(m + n) . Thus the overall complexity of our algorithm is of order O(n + ma(m)) , \nwhere a is an extremely slowly growing function (cf. [TA] for details). An Example: We will now ~ndicate \nthe way in which our algorithm applies to the MINPATH program g~ven earlier. For simplicity, we consider \nonly the first section of the MINPATH program, which searches through the graph within which a path is \nsought. Phase (a) will generate the following provisional based representations: graphl, cost , x , y \n: general; 111 prev2: map (G B21) G _ B22; va13: ~ (G B31) e B32; va1 ~ (~ B41) ~ B42; x4: ~ B41; \n 4: newnodes5: set (~ B5) ; X5: ~ B5; newnodes : set (C B6) ; 6 newnodes7: set (C B7) ; n,: E B7; graph8: \n_(e B81) ~ map B82; n:CB 8 81; 8: E 82; Instruction 9 of the MINPATH code expands into roughly the following \nsequence (91) tcost := cost{n};  (92) Vcost := tcost(m);  (93) vval := val(n);  (94) newval := vval \n+ vcost; and thus generates the following provi\u00adsional representations:  : m-(~ B911) G 912;c0st91 \n 91: = 911; cost91 : = 912; tcostg2: map (e B921) E 922; Vcost e 9:!2~ 92 E 921; 92: : map (e B931) \n~ a~93 932; 93; 6 931; va193: E 932!; no bases are generated by instruction 94. : map (e B al-lo 101) \ne 102; : E BLol; 1 o ewvallo: =; : map (~ Bill) ~ alll 112; U: = 111; ewvalll: = 112; w=~12 _ : map \n(G B121) C B122; 12: E 121; 12: E 122; m13,y13: E B13; : ~ (~ B14); m14: E B14; ewn0des14 Among the \nabove bases, only B41, 5 81 911 921 931 101 111 121 are effective. and 14 Phase (b) will perform \nthe following merging and equivalencing: In view of the prev2-prev12 link, we equivalence B21 with In \nview of the 121 and 22 ith 122 va13-va14 link, we equivalence B31 with In view of the 41 and 32 ith \n42 x -xlink, B41 and B5 are equivalence, 45 etc. Only one additional merge deserves extra comment, namely \nthe step which merges the representation of tcost91 and tcost92, and which belongs to case (ii) of the \ngeneral description of merging given above. This merge step causes us to set ) = map (~ B921) e B922. \nThe infor\u00ad ~~(B912 mation thereby generated will be used by phase (c) to update representations con\u00adtaining \nB912, which will turn out to be useless. At the end of phase (b), the followin9 equivalence classes of \nbases will have been formed: {B 912} {B ]  ,.,.-922 :3 = {B32JB42JB932JB112} a class containing all \nthe remaining 4 = bases. . Since only B4 is effective, the first three classes are useless. Phase (c) \nwill thus eliminate any reference to bases in these classes, e.g. the representation of will be updated \nto c0st91 . map (~ 64) map (~ B4) int As for 63 we simply replace the submode A EB by int r since, \nas is easily 3 checked, EM(~3) = _int at the end of phase (b). Phase (c) will thus assign the repre\u00adsentations \nsuggested in section 2 to most of the occurrences of the variables graph, cost,newnodes, prev, val, x, \nY, m and n. However, the occurrences graph2, cost2, X2 and y2 remain unbased. This will cause phase (d) \nof our algorithm to split each of these variables into two symbol-table entries, a based one and an unbased \none, and to insert four conversions from each of the unbased split variables to the corresponding based \none before entering the while loop (but the conversion of x will precede instruction 4, as X4 has already \nthe based representation) . The base ;4 is built precisely at these places, and then remains constant \nduring execution of the remainder of the MINPATH program. A few final words concerning refine\u00adment of \nthe coarse based representations selected by our algorithm: A main goal of our algorithm is to speed \nup program execu\u00adtion by reducing the number of hashing operations performed by the program. Having achieved \nthis optimization we can expect a considerable speed-up in the execution of the program. However addi\u00adtional \nimprovement is possible if we refine the based representations thereby selected, choosing suitable local, \n~ndexed or sparse representations for based sets and maps. However, to choose refined data-structures \neffectively may require frequency information, object size estimates, etc. , which are unavail able during \ncompile time (but cf. [LO] for various interactive and run-time techniques which can help gather such \ninformation) . Since our aim is to develop a fully automatic data-structur ing technique, we abandon \nany attempt to collect and use such information, and make do with a coarser and more modest approach, \nwhich can be summarized roughly as follows: . (i) If a based object A is iterated upon, choose the sparse \nrepresentation for A.  (ii) If not, but A is ~nvolved in global set theoretic operations, or is assigned \nto any variable, or passed as a procedure parameter, or incorporated into other objects or used destructively \nin a manner requiring value copying, choose the index\u00aded representation for A.  (iii) In all other cases,A \ncan have local representation. References [AL] Allen, F. E., Control flow analy\u00adsis, Proc. Symp. Compiler \nOptimiza\u00adtion, SIGPLAN Notices 5 (1970) , 1 19. [DE] Dewar, R.B.K., Grand, A., Liu, S.C., Schonberg, \nE. and Schwartz, J.T., Programming by refinement, as exemplified by the SETL representa tion sublanguage, \nto appear in CACM . [GS] Grand, A. and Sharir, M., On name splitting in SETL optimization, SETL Newsletter \n206, Courant Inst. Makh. Sci., New York, 1978. [HAI Halstead, 11. H., Elements of soft\u00adware science, \nElsevier North-Holland, New York, 1977. [LI] Liu, S. C., Automatic data\u00adstructure choice in SETL, Ph.D. \nthesis, Courant Inst. Math. Sci., New York, 1978 (to appear). [LO] Low, J. R., Automatic data\u00adstructure \nselection: an example and overview, CACM 21 (1978) , 376-384. [RE] Reif, J. H., Ph.D. Thesis, Harvard \nUniversity, (to appear). [SL] Schonberg, E. and Liu, S. C., Manual and automatic data\u00adstructuring in \nSETL, Proc. 5th Annual III Conference, Guidel, France, 1977, 284-304. [Se] Schwartzr J. T.,, Automatic \ndaka structure choice in a language of very high level, Proc. 2nd POPL Conference, Palo Alto, Calif., \n1975, 36--40. [SC21 Schwartz, J. T., Use-use chaining as a technique in typefinding, SETL Newsletter \n140, Courant Inst. lflath. Sci.r New York, 1974. [SC3] Schwartz, J. T., On Programming: an interim report \non the SETL project, 2nd edition, Courant Inst. Math. Sci., New York, 1975. [TA] Tarjan, R. E., Efficiency \nof a good but not linear set-union algorithm, JACM 22 (1975), 215-225. [TA21 Tarjan, R. E., Solving path \nproblems on directed graphs, STAN-CS 75-512 Tech. Rep., Stanford University, Calif.r 1975. [~E] Tenenbaum, \nA. M., Type determina tion for very high level languages, Computer Sci. Rep. 3, Courant Inst. Math. Sci., \nNew York, 1974. \n\t\t\t", "proc_id": "567752", "abstract": "SETL is a very high level programming language supporting set theoretical syntax and semantics. It allows algorithms to be programmed rapidly and succinctly without requiring data structure declarations to be supplied, though such declarations can be manually specified later, without recoding the program, to improve the efficiency of program execution. We describe a new technique for automatic selection of appropriate data representations during compile-time for undeclared, or partially declared programs,and present an efficient data structure selection algorithm, whose complexity is comparable with those of the fastest known general data-flow algorithms of Tarjan [TA2] and Reif [RE].", "authors": [{"name": "Edmond Schonberg", "author_profile_id": "81100198217", "affiliation": "Illinois Institute of Technology", "person_id": "PP79023457", "email_address": "", "orcid_id": ""}, {"name": "Jacob T. Schwartz", "author_profile_id": "81100404411", "affiliation": "Courant Institute of Mathematical Sciences", "person_id": "PP39041299", "email_address": "", "orcid_id": ""}, {"name": "Micha Sharir", "author_profile_id": "81410596192", "affiliation": "Courant Institute of Mathematical Sciences", "person_id": "PP77044733", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/567752.567771", "year": "1979", "article_id": "567771", "conference": "POPL", "title": "Automatic data structure selection in SETL", "url": "http://dl.acm.org/citation.cfm?id=567771"}