{"article_publication_date": "01-01-1979", "fulltext": "\n CHARACTERIZATION AND ELIMINATION OF REDUNDANCY IN RECURSIVE PROGRAMS* Permission to make digital or \nhard copies of part or all of this work or personal or classroom use is granted without fee provided \nthat copies are not made or distributed for profit or commercial advantage and that copies bear this \nnotice and the full citation on the first page. To copy otherwise, to republish, to post on servers, \nor to redistribute to lists, requires prior specific permission and/or a fee.&#38;#169; 1979 ACM 0-12345-678-9 \n$5.00 Norman Harvard Abstract. Many well-known functions are computed by interpretations of the recursion \nschema procedure f(x) ; Q p(x) then return a(x) else return b(x,f(cl(x)), . . ..f(cn(x) )) . Some of \nthese interpretations define redundant computations because they lead to multiple calls on f with identical \nargument values. The existence and nature of the redundancy depend on properties of the functions Ci. \nWe explore four sets of assumptions about these functions. We analyze directed acyclic graphs formed \nby merging the nodes of the computation tree for f(x) which are known to be equal for each set of assumptions. \nIn each case there is a transformed program which computes f(x) without redundancy, provided that certain \nadditional assumptions about p, a, and the Ci are satisfied. The transformed programs avoid redundancy \nby saving exactly those intermediate results which will be needed again later in the computation. These \nprograms are all valueless recursive procedures which leave intermediate and final results in specified \nglobal locations; in each case recursion can be eliminated without use of a stack. We compare the storage \nrequirements of the transformed programs, discuss the applic\u00adability of these transformations to an automatic \nprogram improvement system, and present a general criterion for establishing the existence of redundancy. \n1. INTRODUCTION Functions often have elegant recursive definitions which are interpretations of the recursion \nschema gn, defined by Pn: P rocedure f(x) ; U p(x) MQQ return a(x) else return b(x,f(cl (x)),. ..,f(cn(x) \n)) . *-Research suPPorted by an IBM research fellowship and by U.S. Department of the Navy under Naval \nElectronic Systems Command contract NOO039-78-G-0020. H. Cohen University However, many of these definitions \nare inefficient because they lead to redundant computation. That is, they cause f to be called recursively \nseveral times with identical arguments. For instance:, if we interpret the schema J4 by letting p(x) \n:: (x=O~x=l), a(x) = 1, Cl(x) = x-1, c2(x) = x-2, and b(x,y,z) = y+z, we obtain the following function, \nwhich, given a non\u00ad negative integer X, returns the Xth element of the Fibonacci series: ~rocedure f \n(x) ; U x.O or x.1 Q&#38;;return 1 else return f(x-1) + f(x-2) The resulting computation for f(5) is \npictured in Figure 1-1. Note that f(3) is computed two times, and f(2) is computed three times. In all, \nthere are fifteen calls on f with only six distinct argument values. For certain interpretations of 9n \n, transformations eKist which, when applied to the procedure f, yield a non-redundant program which is \npartially equivalent to f. The applicability Figure 1-1. A tree representation of the computation of \nf(5) under the Fibonacci interpretation of ~2. Each node represents a call on f, and is labelled with \nthe argument value for that call. The sons of a node represent recursive cal 1s arising directly from \nthe call represented by that node. of these transformations depends on the ~alidity of two conditions \n--a descent condition relating the functions ci~ I<i<n, to each other and a frontier condition relating \nthe to the predicate p and the function a. For t~~ Fibonacci interpretation, such a transformation would \nyield the following non-redundant program: Drocedure f(x) ; = f2(x) ; ~BACKO S@ ; procedure f2(x) ; fix.owx.l \nthen begin BACKO := 1; BACKI := 1 Q@ else begin f2(x-1) ; BACK2 := BACKI ; BQCKI := BACKO ; BACKO := \nBACKI + BACK2 @lcJ The variables BACKO, BACKI, and BACK2 are global to f2. The recursive procedure f2(x) \nis executed for the side-effect of placing the value of f(x) in BACKO . (It also Dlaces the value of \nf(x-1) in BACKI and the value o; f(x-2) in BACK2 for x>I.) We present four transformations, each applicable \nunder a weaker descent condition than the previous one. In Sections 2 through 5 we exDlain these transformations \nin terms of the schema ~. .$?2> defined by ~: procedure f(x) ; Q p(x) then return a(x) else return b(x,c(x),d(x \n). (For simplicity we use c and d in place of c, and C2.) We explore the constraints which each dkscent \ncondition places on the domain and present a program which, given the validity of an appropriate frontier \ncondition, exploits these constants to compute f without redundancy. In Section 6 we sketch the generalization \nof these transformations to ~n for n)2. We are interested both in exploring the mathematical nature of \nredundant programs and in the automatic improvement of redundant programs. After we consider the mathematical \nimplications of each descent condition and present a transformed program reflecting the full generality \nof the schema ~ we discuss the applicability of more specialized improvements which result in simpler \nand more efficient programs. RelationshiD to Dvnamic Programming As in dvnamic DroKrammin K (Aho, Hopcroft, \nand Unman [1974]), redundant computation is avoided by saving results which may be needed many times. \nThe approach resembles in some ways the method of tabulation presented by Bird [1977b]. However, we use \nknowledge about the interpretation to reuse storage for results which will not be needed again. In many, \nbut not all, cases a fixed amount of storage can be shown to be sufficient for all inputs. (In general, \nthere are interpretations of 9 whose computations inherently require unbounded amounts of storage (Paterson \nand Hewitt [19701, Strong [19711). We show in Section 5 that among these interpretations are some for \nwhich f is redundant.) Sophisticated knowledge about the interpretation allows us to produce a program \nwhich makes recursive calls only when such calls are necessary, and references stored previous results \nat all o,ther times. Traditional approaches to dynamic programming determine when to make recursive calls \nby checking whether the storage location identified with a particular argument value has been written \ninto. Since we intend to reuse storage locations for many argument values, we will have to be more clever. \nGeneral Strategv The transformed program, f2, uses global variables to store these previous results. \nWhen -called with argument x, the program first checks whether p(x) is true. If not, it calls itself \nrecursively with some new value x? . This has the effect of leaving the value of f(x ), as well as any \nother values which may be needed later on, in specified global locations. Using these values and taking \nadvantage of the descent condition, the program computes the value of f(x). This and any other values \nwhich might be needed by still-active invocations of f2 (which called itself recursively with argument \nx) are placed in appropriate global locations, and f2 returns. In the case where p(x) is true, we simply \ninitialize the global locations with the value a(x) and any other values needed as Ifprevious resultsll \nby the caller of f2. We assume that all these Tprevious results can be computed without further recursion. \nThe frontier condition is a formal statement of this assumption; we conjecture that it is valid for most \nnaturally\u00adarising cases. The transformed programs in Sections 2 through 5 are recursive procedures which \ndo not return a value , but have the side effect of leaving their results in a designated location. These \nprograms are in a form which, at the expense of clarity, can easily be made iterative. Methods for eliminating \nthe recursion are discussed in Section 6; in each case this can be done without use of a stack. In the \nFibonacci example, the result of eliminating the recursion would be essentially this program: ~rocedure \nf(x) ; Qe&#38;L?l BACKO := 1; BACKI := 1; @ i ~ 1until x-1 IQ bs.am BACK2 := BACK1 : BACKI := BACKO ~ \nBACKO := BACKI + BACK2 Q2&#38;; return BACKO &#38; Descent Conditions We partition the domain of f into \ntwo subsets, @ and .%? . % consists of the base cases, those values in the domain of f for which p is \ntrue; % consists of the recursive cases, those values in the domain of f for which p is false. We shall \nrefer to c and d as the descent functions of % (In general, Ci is a descent function of Yn for I<i<n.) \nA descent condition IS a requirement that certain relationships hold among the descent functions on %. \nIn Section 2 we consider the very strong descent condition that c(x)=d(x) for all x in 4?. In Section \n3 we require the existence of a function g and integers m and n such that c(x)=gm(x) and d(x).gn(x) for \nall x in iZ?. (The notation gi(x) means x if i=O and g(gi-l(x)) otherwise.) This weaker descent condition \nimplies that cn(x)=dm(x) and c(d(x)).d(c(x)) on % . In Section 4 our descent condition asserts that Cn(x)=dm(x) \nand c(d(x))=d(c(x)) on f%? for some m and n, but we do not insist on the existence of g. The descent \ncondition for Section 5 is the relatively weak requirement that c(d(x)).d(c(x)) on .$%?. (We say that \nc and d commute on %?.) Not surprisingly, as we consider progressively weaker descent conditions, our \ntransformations become more intricate. The Frontier Condition The purpose of the frontier condition is \nto assure that there is a !Inicett boundary between @ and %, and that the descent functions map values \nmonotonically toward Ieasierft cases. Formally, let S be a set of functions. (Typically, these functions \nwill be compositions of descent functions.) The frontier condition for S is the requirement that for \nall x in % and all h in S, 1) a(h(x)) (and thus h(x)) is defined, and 2) if f(h(x)) is defined then f(h(x)) \n= a(h(x)). (This is always the case, for instance, when h maps @ into %, i.e., when p(x) implies p(h(x)) \nfor all h in S.) This condition assures us that whenever we are asked for the value of f(h(x)), where \nh is in S and x is in %, we may supply the answer a(h(x)) and be sure that we have supplied the wrong \nanswer only if we were asked for an undefined value. Descent Trees and Compressed Descent DaRs A descent \ntree will be a tree representing the computation of f(x) for some x. Each node will correspond to a call \non f: The root will correspond to the original call, and each node will have children corresponding to \nthe calls arising from the call on f represented by that node. Sometimes we will identify nodes with \nthe argument values of the corresponding calls, and sometimes we will identify them with the result values \nof the corresponding calls. In the former case, we will often abbreviate an argument value by indicating \nonly a sequence of descent functions which must be applied to x to obtain that value. Each of these conventions \nis depicted in Figure 1-2. The tree in Figure 1-1 is a descent tree for the computation of f(5), where \nf is the Fibonacci function. If we make certain assumptions about c and d, then we may comrn ess the \ndescent tree into a directed acyclic graph (dag) by merging nodes which correspond to calls with the \nsame argument value. (Any two such nodes are roots of identical subtrees, and we merge the nodes by merging \nthose subtrees.) The resulting graph will be called a compressed descent L@. Figure 1-3 shows a compressed \ndescent dag for the descent tree of Figure 1-1. a) @GJ%Q ,...,,.. .  > (b(x, b(c(x), o(c(c(x )):', \na(d(c(x)))), b(d(x), o(c(d(x ))), o(d(d(x))))~ b[d(x), o(c(d(x))), o(d(d[x))))(b) b(c(x),o(c(c(x))), \na(d~a =. / ;= (c) cd cc dc cdd &#38; ,,- Figure 1-2. Descent trees for the computation of of f(x). \nTree (a) has nodes labelled with argument values, tree (b) with result values (under the assumption \nthat p is true for every value in the third value of tree (a) and false for every value in the first \nand second levels of that tree). Tree (c) is a convenient abbreviation for tree (a). A 5 4 3 2 [I Jo \n Figure 1-3. A compressed descent dag for the computation of f(5), where f is the Fibonaoci function. \n(Since each of the nodes has a different value, this is the minimal compressed descent dag for that \ncomputation.) When we have merged all the nodes of a descent tree which can be shown to have the same \nvalue under a given set of assumptions, we call the resulting graph the minimal compressed descent dag \nfor that set of assumptions, Formally, if there is an interpretation obeying a given set of assumptions \nfor which no two nodes of a compressed descent dag have the same value, the compressed descent dag is \nminimal. The minimal compressed descent dag of a computation is uniquely defined. Assumptions It will \nbe assumed throughout that evaluating a descent function, a, b, or p entails no side-effects. lie guarantee \nthat the transformed function returns the same value as f throughout the domain of f, but we do not attempt \nto characterize the behavior of the transformed function on those values for which f is not defined. \nThe parameter x may be regarded either as a scalar or as a tuple. In the latter case, the descent functions \nwill be tuple-valued. 2. EXPLICIT REDUNDANCY Definition of ExDlicit Redundance An interpretation of J% \nexhibits ex~licit redundancy if c(x)=d(x) for all x in %. Consider, for example, the domain of LISP lists. \nLet p(x) be the predicate NULL(x); let a(x) be the constant function whose value is the list whose only \nelement is NIL ; let b(x,y,z) = APPEND(Y, DISTRIBUTE(CAR(X) ,2)) ; let c(x) = d(x) = CDR(X). DISTRIBUTE \nis a function taking an atom and a list of lists and returning the result of CONSing the atom onto each \nsublist. ( For example, DISTRIBUTE( A, (()(B)(C D))) is w-ml to the list ((A)(A B)(A C D)). ) This interpretation \nleads to the following explicitly redundant definition mapping sets, represented as lists, to their powersets: \n~rocedure f(x) ; ~ NULL(x) then return (()) else return APPEND(f(CDR(x)) , DISTRIBUTE(CAR(x),f(CDR(x)) \n)) Solution to Explicit Redundanc~ We can transform f into a recursive procedure f2(x), which does not \nreturn a result, but always leaves the value of x in a global variable T, as follows: procedure f2(x) \n; u p(x) ~T := a(x) -!wi.in f2(c(x)) ; T := b(x,T,T) @ This transformation is valid without the imposition \nof a frontier condition. (Formally, we require that the frontier condition hold for all functions in \nthe empty set.) Of course d may be used in place of c in the transformed program if it is preferable \nto do so. Explicit redundancy need not arise from poor programming. It is likely to be found in mechanically-generated \nprograms, or in programs written to maximize clarity rather than efficiency. The program above could \nhave been a mechanical translation from pure LISP --in which there is no notion of assignment --to an \nAlgol-like language. The original LISP programmer could have introduced an auxiliary function (LAMBDA \n(XY) (APPEND Y (DISTRIBUTE (CAR X) Y))) and called it with arguments x and (f (CDR(X)) to avoid the \nredundant computation, since (f (CDR x)) would be evaluated once and bound to Y. The resulting program \nwould have been less clear, however. It is quite reasonable (and, in fact, desirable) for a programmer \nto write clear but inefficient programs if he has competent software tools at his disposal to improve \nthose programs mechanically. 3. COMMONGENERATORREDUNDANCY Definition of Common Generator Redundance Suppose \nthat for a given interpretation of ~ there exist a function g and positive integers m and n such that \nc(x).gm(x) and d(x)=gn(x) for al 1 x in E?%. We shall call g a common generator of c and d, and say that \nthe interPretation exhibits common generator redundance. We assume without loss of generality that m \nand n are relatively .. if this is not the case, we consider &#38;%~,n), m/gcd(m,n), and n/gcd(m,n) in \nplace of g, m, and n respectively, without affecting the value of gm or gn. Figure 3-1 shows the structure \nof the descent tree under these assumptions. In this section we present a transformation which  A .\u00ad \nnl+n( ,, gqx] 9 + (X) 9gz (x) Znl+ Wzo(x) ~ zm.n[x) ~m.2n(x rn.zn 93m(x) g (x) g2m+ (x) (x g3n(x)  &#38;JQ$i?f \n ,. ..., ,. ,, ,, ,. ... ., ,., The function f is then simply: .. Figure 3-1. Descent tree for common \ngenerator procedure f(x) ; redundancy. The kth level of the tree has Pk-1 nodes, but at most k distinct \nvalues. -f2(x);~T~ 146 eliminates common generator redundancy, lprovided that the frontier condition \nholds for the set of functions {gi!O<i<max(m,n)}. The Fibonacci program discussed in Section 1 provides \nan example of common generator redundancy. Recall that under that interpretation c(x) . x-2 and d(x) \n. x-1. If we define g(x) . x-1, then c(x)=gm(x) and d(x)=gn(x) for m=2, n=l, and x in%= {2,3,...}. The \nfrontier condition requires that for each function h in S m {lambda[x;x-1], lambda[x;x-2]} and each x \nin@ = {0,1}, a(h(x)) is either undefined or equal to a(h(x)). The possible values of h(x) are 0-1=-1, \nO-2=-2, 1-1.0, and 1-2.-1. The function a is a constant function, equal to 1 at all these points. Since \nf(0)=l=a(O) and f(-1) and f(-;?) are undefined, the frontier condition is satisfied. Implications of \nCommon Generator Redundanc7~ The set of expressions occurring in the descent tree of Figure 3-1 is {gim+jn(x) \nI iehJ, jelhJ1. Since m and n are relatively prime, almost every natural number can be expressed in the \nform im+jn. Specifically, it is shown in the Appendix that all but (m-l)(n-1)/2 of the natural numbers \ncan be so expressed; the highest number which cannot be is (m-1)(n-1)-1. Typically, m and n will be small \nnumbers. Thus the tree contains expressions equivalent to almost every expression of the form gk(x), \nk@&#38;j . This makes it reasonable to compute f(x) by computing, in turn, each of the values f(gk(x)), \nf(gk-l(x)), . . . . f(gl(x)), f(go(x))=f(x) for some appropriate k. We shall write a procedure f2(x) \nwhich calls itself recursively on g(x) anti then, using previous results, calculates the value of f(x). \nThe resulting descent tree for f;? is a simple chain, as shown in Figure 3-2. This tree is also a minimum \ncompressed descent dag. x g(x) 92(X) 93(X) i Figure 3-2. The minimal compresaea descc!nt dag for a computation \nof f2(x) based on a recursive call f2(g(x)). The chain COntinUeS clownward until, for some k, p(gk(x)) \nis true. Suppose we want to find the value of f at a g~ven node in the chain, say that corresponding \nto gl(x), and P(gi(x)) is false. If we look at the value computed m nodes down the chain, we will find \nthe value of f(gi+m(x)) . f(gM(gi(x))) . f(c(gi(x))). Similarly, the value computed n nodes down the \nchain is the value of f(d(gi(x))). Using these, we can compute b(gi(x), f(c(gi(x))), f(d(gi(x))) ) = \nf(gi(x)). We wil 1 employ a global array BACKg[O:max(m,n)] to remember values which may be needed later, \nand transform f into a recursive procedure, f2(x), which does not return a value but.has the side effect \nof leaving the value of f(gl(x)) in BACKg[i], O<i<max(m,n). (If f(gj(x)) is undefined for some j, an \narbitrary value is left in BACKg[j].) When p(x) is true, the frontier condition assures us that either \nf(gi(x)).a(x) or f(gi(x)) is undefined, O<i<max(m,n), so we may .. simply set BACKg[i] to a(gi(x)) for \neach i in this range. If p(x) is false, we call f2(g(x)), to place f(gi(g(x))) = f(gl+l(x)) into BACKg[i], \nO<i<max(m,n), shift BACKg[i] up to BACKg[i+l], O<i<max(m,n)-1, and then place b(x,BACKg[m], BACKg[n]) \n. b(x,f(c(x)),f(d(x))) . f(x) in BACKg[O]. The program is: procedure f(x) ; -f2(x) ; -BAW401 a!! procedure \nf2(x) ; 12S@?2 U p(x) ~ bQ!3Q .l@ i-O Q (max(m,n)) Q2 begin BACKg[i] := a(x) ; x := g(x) QId ; return \nQ@; f2(g(x)) ; ti i Hg (max(m,n)) ~ -1 ~ 1 L@ BACKg[i] := BACKg[i-1] ; BACKg[OIl := b(x,BACKg[m],BACKg[n]) \n~ Note that BACKg behaves as a queue: Newly calculated values are placed in BACKg at the low end of the \narray and migrate towards the high end of the array as we ascend from the recursion. The values are referenced \nas they pass through the mth and nth positions, in order to calculate a new value to be placed at the \nbeginning of BACKg. It is possible that a $alue will be calculated and placed in BACKg, but never migrate \nfar enough to be referenced, or that it will be referenced only to compute results which themselves never \nmigrate far enough to be referenced, etc. However, if the depth of the recursion exceeds max(m,n), every \nvalue of the form f(gim+jn(x)) which we calculate is in fact needed. Thus the nature of {im+jnlic~, je~} \nassures us that for any x, no more than (m-l)(n-1)/2 useless values are computed. (Any value a(gi(x)) \nplaced in BACKg, such that f(gi(x)) is undefined, must tUrn out to be one of these useless values.) ImDroving \nthe Solution to Common Generator Redundancy The efficiency of this program can be improved at the expense \nof clarity. Recall that m and n are constants, whose values will, in practice, be small. Thus the ~ loops \nabove may be unfolded into a sequence of assignments. Since array references in these assignments will \nhave constant subscripts, we may replace BACKg by max(m,n)+l scalar variables. If there is a form which \nwill evaluate to a(gi(x)) in an amount of time independent of i, we may replace the sequence !BACKg[i] \n:= a(x) ; x:= g(x) in the first ~-100p by the equivalent of BACKg[i] := a(gi(x)). 4. PERIODIC REDUNDANCY \nConsider the following shortest path problem, which we shall call the Impatient Commuter Problem: There \nare two parallel highways into a city, and k interchanges at which a commuter may switch from one highway \nto the other. The time involved in switching is negligible,. but all interchanges exit from the left \nlane and enter into the right lane. Since it takes time to cross several lanes of rush hour traffic safely, \nwe do not permit the commuter to switch highways at two consecutive interchanges. (That is, if a commuter \nswitches highways at interchange i, he may not do so again until interchange i+2.) The problem is to \ndetermine the minimum amount of time in which the commuter can reach the city, given the amount of time \nit takes to travel between each pair of consecutive interchanges on each highway. Suppose we represent \nthe highways by the integers 1 and 2 (so that if we are on highway h, the other highway is 3-h), and \nwe initialize the array DELAYIO:k+l, l:2] so that DELAY[i,h] is the time needed to travel between interchanges \ni and i+ 1 on highway h, l~i<k, and DELAY[i,h].O for i.O and i.k+l. (The interchange numbers are assumed \nto be in ascending order as we approach the city, and lfinterchange k+l!! is the final destination.) \nThen the following recursive program computes the solution to the Impatient Commuter Problem: pocedure \nf(i,h) ; ~ i~k ~ return min(DELAY[i,h],DELAY[i,3-h] ) else return min(DELAY[i,h]+f(i+l,h) , DELAY[i,3-h]+ \nDELAY[i+l ,3-h]+ f(i+2,3-h) ) The value of f(i,h) is the minimum time to reach the city if we are now \napproaching interchange i on highway h. If i.k then we are at the last interchange, and we can simply \nselect whichever highway takes us from interchange k to interchange k+l (i.e., the destination) most \nquickly. The time required to do this is min(DELAY[k,h] , DELAY[k,3-h]). If i=k+l, we are already at \nthe destination, so that the time required is min(DELAY[k+l,h],DELAY[k+l ,3-h]) . min(O,O) = O. Otherwise \nwe take the smaller of the minimum time to the city if we stay on highway h and the minimum time to the \ncity if we switch to highway 3-h . The first of these values is DELAY[i,h] + f(i+l,h); the second is \nDELAY[i,3-h] +DELAY[i+l,3-h] + f(i+2,3-h), since the commuter is constrained to stay on highway 3-h until \nat least the (i+2)th interchange. The (k+l)th row of DELAY allows the base cases to be handled uniformly; \nthe Oth row of DELAY allows us to assume without loss of generality that the commuter strarts out at \nInterchange O!! on highway 1, so that we may solve the problem by calling f(o,l). .Tlne function f is, \nof course, an instance of 9, with x corresponding to the pair <i,h>. Then P(x) (xl~k) , a(x) m min(DELAY[xl \n,X2], DELAY[x;,3_x2]) , b(x,y,z) = min(DELAY[xl ,x2]+Y, DELAY[xl, 3-x2]+DELAY[x1+I ,3-x21+Z), c(x) m \n<X1+1,X2>, and d(x) = The <xl+2~~~~>-in the computation of f is redundant, as descent tree for f(O,l) \nwith k.4, in Figure 4-1. This is not surprising when we observe that c4(x) = d2(x) m <X1-4,X2> and c(d(x)) \n= d(c(x)) m <x1+3,3-x2>. Definition of Periodic Redundance In general, we say that an interpretation \nof L7 exhibits periodic redundance whenever there exist integers i and j such that ci(x)=dj(x) for all \nx in ~, and c(d(x)).d(c(x)) for all x in % . This is certaidy the case when the interpretation exhibits \ncommon generator redundancy: If c and d have a common generator g such that c(x).gm(x) and d(x)=gn(x) \non a, then en(x).dm(x).gmn(x) and c(d(x))=d(c(x)).gm+n(x) on ~. However, c and d in the solution to the \nImpatient Commuter Problem have no common generator. Thus periodic redundancy is a strict generalization \nof common genrator redundancy. lie present a transformation for programs exhibiting periodic redundancy \nwhich is valid whenever the frontier condition holds for the set of functions {cmdn[o~m<i, O~n<j, m+n>O}. \nFigure 4-1. Descent tree for the call f(o,l), where f is the program computing the solution to the Impatient \nCommuter Problem for k.4. Each node is labelled with a pair (i,h) representing the argument values at \nthat call. 148 Implications of Periodic Redundancy Let e. ci . dj on .$%?. Since c and d commute, all \nvalues in the minimal compressed descent dag of f(x) are of the form cmdn(x), where m and n are non-negative \nintegers. But m can be expressed as ri+s, where r and s are non-neg,ative integers and s<i, and n can \nbe expressed as tj+u, where t and j are non-negative integers and u<j. Then cmdn . eri+sdtj+u . ~rcsetdu \n= er+tcsd . Thus each value in the compressed descent dag of f(x) has a unique representation of the \nform evcsdl.l(x), where s<i and U<j. Consider the subgraph of the compressed descent dag consisting \nof the values ekcsd,u(x), O~s<i, O~u<j, for a given k. This subgraph is depicted in Figure 4-2. Notice \nthat this subgraph forms an i by j rectangle. We can envision a comparable i by j rectangle whose upper \nright-hand side lies adjacent to the lower right-hand side of this one, and a third i by j rectangle \nwhose upper left-hand side lies adjacent to the lower right-hand side of the original rectangle. In fact, \nthe entire descent dag may be compressed into a network of rectangles, as shown in Figure 4-3. Given \nresult values corresponding to the upper sides of, for instance, rectangles D and E, it is possible to \ncompute the result values corresponding to rectangle B. Let us assume that the rectangle labeled A represents \nthe subgraph shown in Figure 4-2. The node in the uppermost corner of B is the one just below and to \nthe left of the node in A corresponding to ekci-ldo; thus it corresponds to c(ekci-ldo) . ekcido. The \nnode in the uppermost corner of C occurs just below and to the right of the node in A corresponding to \nekcodj-l; thus it corresponds to d(ekcodj-1) m ekcodj. e% d n ~k,fl~].l a  However, ci = dj = e on \n.%?. Thus ekcido = ekcOdj = ek+lcOdC1. That is, the nodes in the uppermost corners c)f rectangles B and \nC correspond to the same value. (These rectangles would not be part of the desc!ent dag if and ekci-l(x) \nekdj-l(x), respectively, were not in %.) It follows that all the nodes in B correspond to the same values \nas their counterparts in C. In other words, in the compressed descent dag, rectangles B and C are coextensive. \nSimilarly, rectangles D, E, and F are mutually coextensive. Thus the descent dag may be further compressed \ninto the form shown in Figure 4-4. This compression preserves connecticms between rectangles, i.e., if \nthere is an edge between a certain node in rectangle x and a certain node in rectangle y in Figure 4-3, \nthere is a connection between the corresponding nodes in the representatives of rectangles x and y in \nFigure 4-4. Figure 4-3. An abstraction of the descent dag. Each rectangle represents i.j nodes arranged \nin the configuration of Figure 4-2. Figure 4-2. The subgraph of a compressed descent Figure 4-4. The \nresult of compressing the descent ekcsdu, OQ<i, Osu<j. dag consisting of the nodes dag by merging identical \nrectangles. Observe that each rectangle in Figure 4-3 has an uppermost corner corresponding to a value \nof the form ekcodo for some k. Every other node in that rectangle corresponds to a value of the form \n~kcsdu , where s and u are uniquely determined by the position of the node within the rectangle. There \nis a linear ordering among the rectangles in the descent dag of Figure 4-4, corresponding to the various \nrectangles values for k. It follows that the compressed descent dag of Figure 4-4 is minimal. Solution \nto Periodic Redundance These observations lead us to the fOllOWing method for computing the value of \nf(x) without redundancy: We will maintain a global array, RESULTIO:i-l,O:j-l]. We will transform f into \na procedure f2 with the property that a call on f2(x) will leave the value of f(csdu(x)) in RESULT[S,U], \nO@<i, O~u<j. If p(x) is true this can be done immediately, according to the frontier condition, by setting \nRESULT[S,U] to a(csdu(x)). Otherwise, we call f2 recursively on e(x) = ci(x) = dJ(x), which has the effect \nof initializing RESULT to the values appropriate for the next lower rectangle in the compressed descent \ndag. Given these values we can set RESULT to the appropriate values for the current rectangle as folllows \n(where (+i) and (+1) represent addition mod i and mod j, res~ectively~: &#38;KQ arrav ARGUMENTIO: (i-l),O:(j-1)] \n; ~M~ O until (i-1) L@ 12e&#38;irl ARGUMENTIM,O] := U ii.O then x else c(AFiGUMENT[M-,01) ; ~N~ 1 until \n(j-1) @ ARGUMENT[M,N] := d(ARGUMENT[M,N-1) -; ~Mfrom(i-1)~-1~ OQQ ~N~ (j-1) ~-l until O@ RESULT[M,N] \n:= ~ P~ARGUMENT[M,N]) m a(ARGUMENT[M,N]) ~ b(ARGUMENT[M,N], RESULTIM(+i)l, N], RESULTIM,n(+,i) l]) gncJ \n(Of course the roles of c and d are interchangeable, simply by interchanging the roles of iand j.) ImDrovin!2 \nthe Solution to Periodic Redundancy A slightly less clear version of this program uses i+j locations \nto store intermediate results, instead of the i.j locations in RESULT . Observe that the only locations \nin RESULT which are important either at the beginning or the end of the second ~ M loop are those in \nrow zero and column zero of result. We will introduce global vectors BACKc[O:i-1] to represent column \nzero of RESULT and BACKd[O:j-1] to represent each row of RESULT in turn. (Just before the M=k iteration \nof the loop below, BACKd will represent the (k(+i)l)th row of RESULT ; just after that iteration, it \nwill represent the kth row of RESULT.) In the improved version of the algorithm, the second != M loop \nis rewritten as follows: &#38;M~(i-l) a-1 until O.dQ !E@l BACKd[(.i-1)] := ~ p(iRGUMENT[M, (j-1)]) ~ \na(ARGUMENTIM,(j-l )]) ~ b(ARGUMENT[M,(j-1)], BACKd[(j-1)], BACKC[M]) ; ~Nm(j-2)sJw-l XL?QLOS!Q BACKd[N] \n:= ~ P(ARGUMENT[M,N]) ~ a(ARGUMENT[M,N]) ~ b(ARGUMENT[M,N], BACK C[M] := BACKd[N], BACKd[N+l]) BACKd[O] \n; m each which In addition, M and can be if N, be evaluated the value expressed in of in an cMdN(x) an \nexplicit amount can, of for form time independent of M and N, we maY eliminate the arraY ARGUMENT: We \nomit the first ~ M looP and modify the second by replacing assignments of the form BACKd[n] := U p(ARGUMENT[M,n]) \n~ a(ARGUMENT[M,n] ) ~ b(ARGUMENT[M,n], ..., ..-) by: T :. (explicit form for cMdn(x)) ; BACKd[n] := \nfiP(T) ~a(T) ~b(T,... )...) When this improvement is possible, the final result takes on the following \nform: Drocedure f(x) ; 12QQQ fp(x) ; return BACKCIO] QQQ procedure f2(x) ; m U p(x) then begin ~M~O until \n(i-l) ti BACKC[M] := a((explicit form for cMdO(x))) ; ~N~ O until (j-1) @ BACKd[N] := a((explicit form \nfor cOdN(x))) ; return m; f2(e(x)) ; ~M~(i-1)~-l QQ!.Z.LOJ2Q l!!2Q?2 T := (explicit form for cMd(j-l)(x)) \n; BACKd[(j-1)] := ~ p(T) ~ a(T) ~ b(T,BACKd[(.j-l )],BACKc[M] ) ; T := (explicit form for cMdN(x)) ; \nBACKd[N] := U P(T) ~ a(T) ~ b(T,BACKd[N],BACKd[N+l 1) Q@; BACKC[M] := BACKd[O] (Again, the use of ~-loops \nand arrays is primarily a notational convenience. Loops can be replaced by a sequence of assignments \nto scalar variables, and we may have a different explicit form for each value c dn(x) occurring n hat \nsequence.) 5. COMMUTATIVE REDUNDANCY Definition of Commutative Redundance Finally, we consider interpretations \nfor which the only descent condition is that c(d(x)) m d(c(x)) for all x in .%. This generalization \nof periodic redundancy will be called -~ redundance. Figure 5-l(a) shows the minimal compressed descent \ndag under this descent condition. Our transformation for eliminating commutative redundancy will be valid \nwhenever the frontier condition holds for the set of functions {Cidj ~ ie~, jeiN}. Whenever this is the \ncase, the dag for the entire computation takes on the shape shown in Figure 5-l(b) (where the dag has \nbeen turned on its side for clarity), i.e., the frontier between ~ and S proceeds monotonically upward \nand rightward from point I to point II. Formally, min{k I p(ck(d(x)))} ~min{k \\ p(ck(x))} and min{k I \np(dk(c(x)))l x min{klp(dk(x))}, for all xin J%. An example of commutative redundancy is the following \nrecursive definition of the binomial coefficient of n and k, n!/k!(n-k)!, defined for n~O and Oikin: \n(o) ,.. ... .. (b) d(x) dz(x) . . . C?X) cd(x) cd2(x) . . . c2(x)c2d(x) c2d2(x) .<. :: .. : ,;(Y2 \n*~9 (0 Figure 5-1. The minimal c!om~essed degcent dag for commutative redundancy. In (b), the dag of \nrotated 45o counterclockwise tO (a) has been illustrate the structure of the frontier whenever procedure \nf(n,k) ; ~ k=O QL k=n then return 1 else return f(n-l,k-l )+f(n-l,k)  This is an instance of f? with \nx=<n,k>, P(<x1,x2>) = (X70 ~ X2=X1 ) ! a(x) m 1, b(x,y,z) = Y+Z, C(<XI,X2>) = <X1-.1,X2-I>, and d(<xl,x2>) \n= <XI-1,X2>. Since c(d(<xl,x2>)) = d(c(<xl,x2>)) = <XI-2,X2-1>, c and d commute on %?. For i and j in \nIN, c~d.d(<xl,xz>) <xl-i-j,x2-j>. To establish the frontier cond~tion, we show that for all i and j in \n~, if x2=0 or x2.x1, the binomial coefficient of xl-i-j and x2-j is either equal to 1 or undefined. If \nx2=0, then either x2-j=O (in which case the binomial coefficient is 1 if xl-i-j is in the legal range, \nand undefined otherwise) or x2-j<O (in which case it is undefined); if x2.x1, then either x2-j=xl-i-j \n(in which case the binomial coefficient is either 1 or undefined) or x2-j>xl-i-j (in which case it is \nundefined). Implications of Commutative Redundance Paterson and :Hewitt[1970] showed that for each n \nthere exist interpretations for the schema g for which the function f cannot be computed using fewer \nthan n storage locations for partial results. An adaptation of their proof shows that this is true even \nif the interpretations are restricted to be commutatively redundant. Consequently, if we are to eliminate \ncommutative redundancy by using global locations to store needed temporary results, we will, in general, \nhave to allocate that storage dynamically. The adapted proof differs from the original only in that it \nreplaces a descent tree with the corresponding minimal compressed descent dag. For each n, we consider \nthe interpretation ~, which is free except for the assumptions that c(d(x))=d(c(x)) whenever p(x) is \nfalse and that P(Cidj(xO)) is true if and only if i+~n, for constant XO. The value of f(xo), along with \nthe descent trees and minimal compressed descent dags, is displayed in Figure 5.2 for interpretations \n$(, ~, and @2. ce~: f(x,J = a (XJ Xo Xo oo .!.,1 f(xo)=b (~a(c(xo)), o(d(xt) ))) &#38;&#38; ./2$ f(xol=b(~, \nb(c(xo), o(c2(xo)),0 (cd(xo))). b(d(%)). o(d2(xo)))) Xo Xo C(xol d(x~ C(XJ d(xo) de(xo)C2[XO) cd (Xo) \ncd (x,J dz(xo) C2(XO) cd(x.J C41 &#38;!C)&#38; Figure 5-2. The value of f(xo) under each of the tree \n interpretations J%, @l, $2 he esCent forand minimal compressed descent dag are shown the frontier condition \nholds. each interpretation. We will show that the computation of f(xo) under interpretation @n requires \nat least n+l storage locations. Each step of the computation involves either placing the value corresponding \nto a leaf of the dag in a storage location, or taking the values corresponding to children of a given \ninterior node from two storage locations, colnput\u00ading the value corresponding to that interior node, \nand placing that value in a storage location. We call the dag open if there is a path from the root to \na leaf consisting only of nodes whose cor\u00adresponding values are not currently held in any storage location. \nOtherwise the dag is closed. The computation begins with the dag oPen and concludes with the dag closed \n(with the value corresponding to the root in a storage location). Thus it suffices to show that n+ 1 \nstorage locations are required at the moment an open dag, shaped like the minimal compressed descent \ndag for @n, becomes closed. We do this by observing that the computation Of f(xo) under @O trivially \nrequires one storage location and proving that computation of f(xo) under $n requires at least one storage \nlocation more than under Consider the following .%1 . isomorphic mapping from a subgraph of the dag for \n@n to the dag for LZn-l: The ith node on the kth level of the dag for @n is mapped to the (i-l)th node \non the (k-l)th level of the dag for ~n-1 , l<i~k. (Intuitively, this mapping creates a graph the same \nshape as the dag for @n-l by cutting off the nodes on the upper left-hand edge of the dag for e%n. See \nFigure 5-3.) Now consider a sequence of operations which closes the dag for @n leaving the fewest possible \nstorage locations in use at the moment the dag is closed. We can map this sequence to a sequence of operations \nclosing the dag for @n-l, by deleting operations involving the ficst node on any level of the dag for \n@n and applying the node mapping just described to all other operations. However, for the dag for @n \nto be closed, at least one value corresponding to the first node on some level of the dag must be in \na storage location; otherwise the leftmost path in the dag would be open. Since this node plays no role \nin the sequence of operations we have created to close the dag for Jln-1, there is a way for the dag \nfor n-l tO become closed using one storage location less at the moment of closure than the minimum number \nof storage locations required by ~n at the moment of closure. That is, the minimum number of locations \nrequired by @n at the moment the dag becomes n Dug forc$f-n closed is at least one more than the minimum \nfor @n-1. Thus we have established that there exist interpretations for ~ for which there is no way \nto calculate f using a fixed amount of storage, even though these interpretations exhibit commutative \nredundancy. Solution to Commutative Redundance Define the d-depth of x to be the smallest natural number \nn such that p(dn(X)) is true. Clearly, d-depth(x) has a value whenever f(x) is defined. This value can \nalways be computed by the program ]rocedure d-depth(x) ; l?egi?l i := o; while not p(x) Q -x:= d(x) \n;i := i+l -; return i &#38;, or, depending on the interpretation of p and d, it may be computable directly. \n(For instance, if d(x) = x-1, p(x) is x=O, and X.SIN, d-depth(x) = x.) Our solution to commutative redundancy \nwill use a global array of d-depth locations to compute f(x). We will use the function d-depth without \nspecifying how that function is to be computed. The solution is to think of the dag as drawn in Figure \n5-l(b), and let the global array, BACKd, represent the result values in a row of that rotated dag. Another \nglobal array, ARGUMENT, will hold the argument values in that row. We transform f into a procedure f2(x,n) \nwhich leaves the value of f(di(x)) in BACKd[i], O~i~n. When d-depth(x) is O (i.e., ,P(x) is true), this \nis done simply by placing a(dl(x)) in BACKd[i], O<iQ. (The frontier condition guarantees that th~s is \nvalid.) Otherwise, we call f2 recursively with arguments c(x) and d-depth(x) -1, which places f(di(C(X))) \nin BACKd[i], O~i<d-depth(x). We then set the array ARGUMENT so that ARGUMENT[i] . di(x), O~i<d-depth(x), \nplace a(di(x)) in BACKd[i], d-depth(x)~i~n, and execute the 100P ~ i ~ d-depth(x)-l x-l ~ O I@ BACKd[i] \n:= b(ARGUMENT[i], BACKd[i], BACKd[i+l]),  Dccj Figure 5-3. A mapping from the dag for @n to the dag \nfor $n-l The ith node on the kth level of the first dag is mapped to the (i-l)th node on the (k-l)th \nlevel of the second dag. which leaves f(di(x)) in BACKd[i], O~i@. (At the beginning of the i.k passage \nthrough the loop, BACKd[j] . f(dj(x)), k<.@, and BACKd[j] . f(dj(c(x))) = f(cdj(x)), O~~k. Since k < \nd-depth(x), p(x) is false, so b(ARGUMENT[i], BACKd[i],BACKd[i+l ])=b(dk(x) ,f(cdk(x)),f(dk+l (x)) = b(dk(x),f(c(dk(x))),f(d(dk(x) \n))) = f(dk(x)). Thus , after the i=k passage through the loop, BACKd[k] holds f(dk(x)). The entire program \nlooks like this: procedure f(x) ; !Z2&#38;LiQ D := d-depth(x) ; &#38;2&#38;.! ~. BACKd[O:D], ARGUMENTIO:D] \n; f2(x,D) ; return BACKd[O] ; procedure f2(x,n) ; M U p(x) ~ w ~i~Ountiln@2 w BACKd[i] := a(x) ; X := \nd(x) Q@; return d; rir := d-depth(x) ; f2(c(x),m-1) ; ~ i from Ountil n @ l&#38;&#38;QL ARGUMENT[i] :. \nx ; X := d(x) &#38;; ~ i from muntil n &#38; BACKd[i] :. a(ARGUMENT[i]) : ~i~m-l~-l until 0~ BACKd[i] \n:. b(ARGUMENT[i], BACKd[i], BACKd[i+l]) ~ ImDroving the Solution to Commutative Redunda~ As before, \nthis program can be improved in many cases: If di(x) can be computed in an amount of time independent \nof i (e.g. when d(x).x-1, di(x)=x-i), then ARGUMENT can be eliminated. If neither di(x) nor d-depth(x) \ncan be computed directly, then time would be saved by combining the initialization of ARGUMENT with the \ncalcula\u00adtion of d-depth. However, this would be expensive in terms of space, since we would then have \nto make ARGUM ENT local to save it across recursive calls on f2. Of course the roles of c and d can \nbe interchanged if appropriate changes are made throughout the program. (For instance, b(ARGUMENT[i],BACKd[i],BACKd[i+l \n]) would become b(ARGUMENT[i],BACKc[i+l],BACKc[i] ), and d-depth would be replaced by c-depth.) Such \nchanges would result in a more space-efficient program if c-depth(x) is usually smaller than d-depth(x). \nIf the value of a(x) is the same for all x in 93, then BACKd cam be initialized with every element equal \nto that constant value. Then the two loops which set elements of BACKd to a(x) for some x can be eliminated. \n(This is legitimate because the array elements which are set by these loops are not referenced by the \nprogram before the loops are encountered.) 6. CONCLUSIONS We have presented a recursion schema, ~, defining \na function, f, together with various sets of assumptions under which the definition of f calls for redundant \ncomputation. For each set of assumptions we have investigated the nature of the redundancy, and presented \na recursive program, using the predicate and function signs of J?, which computes f without redundant \nrecursive calls. As expected, we found that weaker assumptions require more intricate computations. We \nalso found that weaker sets of assumptions required greater amounts of global storage. The use of global \nvariables is only one of a number of implementations which could have been used. An obvious alternative \nwould be to rewrite f2 to return a tuple consisting of those results which will be needed later, instead \nof leaving those results in global variables. (There would be an obvious one-to-one correspondence between \nelements of the tuple and elements of the global arrays.) This approach seems mathematically cleaner \nthan the use of global variables, but it was felt that the use of global variables would be more familiar \nto most readers, being more consistent with programming languages such as PL/I and Algol 60. RemovinQ \nthe Recursis Each of the versions of f2 presented earlier is of the general fcjrm ~rocedure f2(x) ; U \np(x) ~ A(x) else begin f2(h(x ) ; B(x) ~, where x may be a tuple (in which case h maps tuples to tuples) \nand A and B are procedures executed for their side effects on global variables. References to the global \nvariables may occur free in A or B. (The version of f2 developed for commutative redundancy is not quite \nin this form, but this is easily remedied by moving the assignment to m to just after the recursive call, \nat the cost of computing d-depth(x) twice.) Using the direct method of recursion removal described by \nBird [1977a], We can rewrite f2 non-recursively by using a stack S to maintain an itinerary of values \non which B must still be executed,, The resulting version of f2 preserves the order of the calls on A \nand B: procedure f2(x) ; &#38;lX&#38;l stack S ; S := empty-stack ; while not p(x) do &#38;@QS <=x ;x \n:= h(x) ~ ; A(x) ; while not empty(S) &#38; -x<=S; B(X)@ Q@ This program may be improved. As suggested \nby Darlington and Burstall [1976], recursion can be eliminated without the use of a stack if there is \na single recursive call whose descent function is invertible; the resulting program can be made even \nmore efficient when there is a unique base case. When h has an inverse, x may be set to h-l(x) directly \ninstead of by popping a stack as above. The resulting program: lrocedure f2(x) ; !2!2@2 Y :=x; while \nnot p(y) ~ Y := h(y) ; A(y) ; a y+x @ h-l(y) ; B(Y) ~ a WY:= (This assumes that neither A(Y) nor B(Y) \nmodifies If this is not the case, we can replace A(y) by Z := y ; A(z)t! or IfB(y) by Iz := y ; B(z),t! \nas necessary.) If there is a unique base case X. Y. such that p(x) is true if and only if X=xo? the initial \nassignment and while-loop may be replaced by the single assignment y :. Xo. Even if h has no inverse, \nwe can always eliminate the stack by using a counter, as follows: Drocedure f2(x) ; 12f@l Y :=x ; n \n:= o; while not p(y) @ -Y:= h(y) ; n := n+l E@ A(y) ; fii~n-1~-luntil O@ 12Qk3LQ Y:=x; @C j from 1until \ni@ y:. h(y) ; B(Y) &#38; gnJ Although this increases the number of calls on h from some value n to (n2-n)/2, \nit is a reasonable approach if h can be computed very quiokly, so that most of the time is still spent \nin the computation of A and B. In particular, if there is a way to compute the value of hi(x) in an amount \nof time independent of i, the body of the outer ~-loop can be replaced by the equivalent of !tB(hl(x)) \n.!! Even if none of these conditions holds, we can compute f2 with a fixed amount of storage in time \nproportional to nl+(l/k), where k is an arbitrary integer, using the construction of Chandra [1973]. \nThe amount of space required is O(k). In any case, we have established that the recursive versions of \nf2 do not nitie their space complexity in the stack. The number of global locations used by each program \nis a fair measure of these programst space requirements. Inter\u00adpretations of q displaying explicit, common \ngenrator, or periodic redundancy can be computed using a fixed amount of storage; commutative redundancy \nreally has a higher space complexity. Generalization to 9,, n>2 The methods we present for eliminating \n redundancy in 9 =92 can easily be extended to yn, n=3, 4, . . . . In general we will require that the \ndescent conditions hold for all of the descent functions simultaneously. Explict redundancy can be eliminated \nfor Pn in exactly the same way as for 9, provided that all of the ci are equal on %. (If the ci can be \npartitioned into k<n groups such that all members of any one group are equal on .%?, then a similar solution \nexists using k global variables.) Common generator redundancy can be eliminated for ~n in exactly the \nsame way as for ~ provided that there is a function which is a common generator of all the ci. A general \nsolution to periodic redun~cy exists, provided such thatthat there exist il, . . ..in c1 ll=C 2i2-. \n. ..=cnin on a. The minimal compressed descent dag would consist of a chain of n-dimensional oblongs. \nThe values corresponding to one such oblong couldbe stored in a il x i2 x . . . x in array, or, by applying \nan improvement similar to that presented in Section 4, by n arrays, each of n-1 dimensions and representing \na cross-section of the n-dimensional oblong. The solution for commutative redundancy for P can also be \nextended to ~n. We require that all the descent functions be pairwise commutative on L%, and we use an \n(n-l)-dimensional array BACK whose ith length in the dimension is set to ci-l-depth(x) , 2<i@. The procedure \nf2 takes on the following for;: procedure f2(x,y2, . . ..yn) ; u Q p(x) then begin Set BACK[i2,. ,.,in] \nto a(c212. ..Cnln(x)), o<i~z, . . ..O~i&#38;n ; return d; f2(cl(x),c2-depth(x), . . .,cn~depth(~)) ; \nSet BACK[i2, ..., in] to a(cz=z.. .cn=n(x)) for all <i2, ..., in> such that (i22c2-depth(x) QE . . . \nJ2C in~cn-depth(x)) @ (O~i2fi2 m . . . ~ O~infin) ; e~~;;;;t;~~i~~~ ~JY,in],..., BACK[i2, . . ..in_l.in+ll) \nfor all <i2, . . ..in> such that 0~i2ic2-depth(x) ,. ..,O~ificdepth(x)x) , in an order which guarantees \nthat BACK[i2, . . ..+l.l, in],in] is always set earlier than &#38;ACK[i2,..., ij, in],in], 2~jin @@ \nFor each of the four kinds of redundancy, the frontier condition for ~2 can be generalized in an obvious \nway. We have not addressed the question of what we can do when a particular form of redundancy exists \namong certain subsets of the descent functions in pn, but not among all of them simultaneously. Additional \nproblems arise when we consider the possibility of different subsets of the descent functions sharing \ndifferent forms of redundancy, or the possibility that only a proper subset of the descent functions \nare in any way redwdant. Neither have we discussed the more ,general situation (not an instance of &#38; \nn for any n) where the number of recursive calls invoked by tlhe call f(x) depends on x. The dynamic \nprogramming examples of Aho, Hopcroft, and Unman [19 74] are of this form. ADDliCakiOnS The program transformations \npresented here can be applied to actual progranrs. More importantly, they can be incorporated in automatic \nprogram transformation systems. One such system is described by Burstall and Darlington [1977]. That \nsystem uses heuristic methods to spply a series of simple but powerful transformatic>ns to recursively \ndefined functions. One of the most effective transformations, however, the definition of new functions \nin terms of old ones, has not yet been automated. Among the examples of definition presented by Burstall \nand Darlington is the definition of a function which returns a tuple consisting of the nth &#38; (n.l)th \nelements of the Fibonacci series, by calling itself recursively to obtain the values of the (n-l)th and \n(n-2)th elements of the series. But that is exactly the program that would result by applyirlg our transformation \nfor common generator redundancy to the usual recursive Fibonacci program (provided that we used tuple-valued \nreturn values instead of global variables). The methods presented in this paper provide both a way to \nrecognize a particular situation in which a definition would be useful, and a way to formulate the definition \nitself. Effective procedures exist to find true descent conditions, given !!blocksn of straight\u00ad line \ncode which compute descent functions. Let Ul,... ,uq and Vi,... ,vr be blocks; let blb2 represent the \nblock obtained by executing block bl and then using its output valus as input values to block b2; and \nlet bn=bbn-l for n>l, with bl=b. Lewis [1977] proves that it is possible to compute th~se values of it, \n. . ..iqand jl, . . ..jr for which u~+l.. .uq+q computes the same function as VIJ1 . .. VrJr. A General \nDefinition of Redundancy Each form of redundancy we have discussed has involved descent functions which \ncommute. Redun\u00ad dant calls arise because arguments are just repeated applications of the same two (or \nn) descent functions, in differing but irrelevant orders. (In fact, each minimal compressed descent dag \nwe have discussed is a compression of the minimal compressed descent dag for commutative redundancy.) \nThis is not meant to imply that the descent functions must commute in order .For an interpretation or \na redundant ~n to define program. To the contrary, redundancy exists whenever the maximum number of distinct \nvalues obtainable by k applications of the descent functions to some value is less than nk for all sufficiently \nhigh k. Clearly, whenever this condition holds, there is going to be a duplication of ar gument values \non the kth level of the descent tree. In fact, a sufficient condition for redundancy is that the number \nof values obtainable by k gr fewer applications of the n descent functions is less than (nk+l-1)/(n-1) \nfor sufficiently high k. This guarantees that there is a duplication of argument values somewhere in \nthe first k levels of the descent tree. One example of redundancy not due to commutativity is a recursive \nprogram which returns the solution to the Tower of Hanoi Problem as a string. The program is: procedure \nHANOI(n,from,to,using) ; ~ n.O then return ~ else return HANOI(n-l,from,using,to) ! ! move(from,to) I \nI HANOI(n-l,using ,to,from) This is an instance of 9 , with x = mm <n,from,to,using>, p(x) (x1=O), \na(x) e, b(x,y,z) = y! lmove(x2,x3){lz, c(x) m <xl-l,x2,x4,x3>, and d(x) = <x1-I,x4,x3,x2>. Observe that \nc and d do not commute. (If they did, this would be an example of periodic redundancy, since c2(x) = \nd2(x) <x1-2,x2,x3,x4>. m On each level of the descent tree n is constant, and there are only six possible \npermutations of from, to, and using, so there can be no more than six distinct values at each level of \nthe descent tree. (In fact, there are at most three distinct values at each level of the tree, since \nthree of the permutations can only result from an odd number of applications of c and d, and the other \nthree only from an even number.) The descent tree and minimal compressed descent dag for a typical computation \nof HANOI are shown in Figure 6-1. 5,0,b,c c d 4,c,b,a4,o,c,b cdd 3,0,b, c 3,c, o,b c 3, b.c,o dc dc \nd..* .. + 2,c,b,o 2, b,o,c ~ 2,a, c,b ddc*c--d c +--* I,o, b,c I,c,a,b dc C* = l, b,c,a dc d . O,C,b,a \nO,b,a,c s~ 0,o, c,b Figure 6-1. The descent tree and minimal compressed descent dag for the call HANOI.(5,a,b,c). \nIn the tree, left sons represent applications of c and right sons represent applications of d. In the \ndag, edges are labelled according to the descent functions they represent. Edges coming in from the right \nedge of the page are continuations of edges leaving the leftmost node of the previous level, and visa \nversa; the dag does not get any wider after the third level. As this example demonstrates, our taxonomy \nof redunancy is far from complete. The characterization of other forms of redundancy, and the formulation \nof program transformations applicable to those forms of redundancy, are topics for future research. It \nmay be possible to formulate a general theory of redundancy which encompasses all these forms. (The characteriza\u00adtion \nof redundancy in terms of the number of distinct values obtainable by k or fewer applications of the \ndescent functions would be a typical component of such a theory.) APPENDIX. THE SET miN+n~. m AND n RELATIVELY \nPRIME We will prove the following results about the set Sm,n m {am+bn ! m and n are relatvely prime positive \nintegers, aclt4, belN}: 1. The largest natural number not included in Sm,n is (m-1)(n-11-1. 2. In all, \nthere are exactly (m-l)(n-1)/2 natural numbers not included in .$m,n. (Note that if m and n are relatively \nprime, all integers are expressible in the form am+bn, where a and b are integers. Sm,n is the subset \nof the integers obtained by restricting a and b to be non-negative.) Proof of the First Result It well-known \nthat the set {Om,lm, . . ..~.-l)m} s Sm,n contains exactly one representative of each congruence class \nmod n. In fact, each of the n numbers in that set is the smallest member of its congruence class which \nis in Smn. Once a congruence class is introduced 9 into Sm,n by one of these n numbers, all larger numbers \nin that congruence class may be generated by adding appropriate multiples of n. Clearly, any number generated \nby adding a multiple of n to a member of Sm,n is itself a member of Sm,n. The number (n-l)m is the last \nnumber in Sm n which introduces a new congruence class. it follows that all numbers greater than (n-l)m \nare in Sm,n. Furthermore, the n-1 integers immediately preceding (n-l)m belong to congruence classes \nintroduced into Sm,n, and therefore belong to Sm,n themselves. The number preceding these n-l integers, \nhowever, belongs to the mod n congruence class which is not introduced until (n-l)m, so it cannot be \na member of Sm n. This number, (n-1)m-(n-1)-1 = (m-l)(n-1)-1, is therefore the highest number not in \nSm n, establishing the first result. (A similar prhof of this result appears in a different context in \nRangel [1974].) Proof of the Second Result In place of our original proof of the second result, we present \na much shorter and more elegant proof by Christos Papadimitriou. The proof makes use of the following \nlemma: .Lemma Let m and n be relatively prime positive integers. Any x in ~ not expressible in the form \nam+bn, ae~, belhl, is expressible either as am-bn, with ()~a<n and b>O, or as bn-am, With O<b<m and a>O. \nProofBy Euclid s algorithm, any integer can be . expressed in at least one of the four forms am+bn, am-bn, \n-am+bn -am-bn, By hypothesis, x cannot be expressed in the first form. Since x>O (x is in ~ and not zero, \nSince O=Om+On), x T~nO~ be expressed in the fourth form. is expressible either as am-bn or bn-am. Assume \nx=am-bn and let q.floor(a/n). Then x . am-bn m (a-qn+qn)m-bn = (a-qn)m+qnm-bn . (a-qn)m-(b-qm)n. Since \na-qn = (a mod n)! Oia-qn<n. Since x cannot be expressed in the form am+bn, -(b-qm) must be negative, \ni.e., b-qm>O. If we assume that x=bn-am, the proof is analogous. lx! Let k . (m-1)(n-1)-1 = inn-m-n. \nWe have shown that k is the highest integer not expressible in the form am+bn. suppose i, 04iSk, is expressible \nas am+bn. Then k-i cannot be, for if k-i=a m+b n, then k . i+(k-i) = (am+bn)+(a m+b n) . (a+a )m+ (b+b \n)n. Conversely, suppose that i cannot be expressed as am+bn. BY the lemma, i can be expressed either \nas am-bn, with a<n and b>O, or as bn-am, with b<m and a>O. In the first case, k-i = (mn-m-n)-(am-bn ) \n= (n-a-l )m+(b-l)n, where n-a-1~0 and b-l~O; in the second case, k-i = (mn-m-n)-(bn-am) = (a-l)m+(m-b-l)n) \nwhere a-l~O and m-b-120, Thus for OSiSk, i is expressible in the form am+bn if and only if k-i is not. \nFurthermore, o~k-i~k whenever OLiLk. This establishes a one-to-one correspondence between those members \nof {0,1,. ... k} which are in Sm n and those which are not. It follows that exactiy half, or (k+l)/2, \nof these integers are in Sm,n. Since k is the highest integer not in Sm n, there exist in all (k+l)/2 \n. (m-l)(n-1)/2 eleme~ts of ~ which are not in Sm,n. Acknowledgements. The author is grateful to Professors \nThomas Cheatham, Harry Lewis, and Christos Papadimitriou for their careful reading of the manuscript \nand many helpful suggestions. REFERENCES Aho , Hopcroft, J.E., and Unman, J.D. [:i;ii. The Design and \nAnalvsis of Computer Algorithms, Addison-Wesley, Reading, Massachusetts Bird, R.S. [1977al. Notes on \nrecursion elimination, ~ &#38; No. 6 (June), pp. 434-439 Bird, R.S. [1977b]. Improving programs by the \nintroduction of recursion, CACM 20, No. 11 (November), pp. 856-863 Burstall, R.M., and Darlington, J. \n[19771. A transformation system for developing recursive programs, ~ ~, No. 1 (January), PP. 44-67 Chandra, \nA.K. [1973]. Efficient compilation Of linear recursive programs, Conference Record ~ -14th Annual SvMDosium \ng . Switchin~&#38; Automata Theorv, pp. 16-25 Darlington, J., and Burstall, R.M. [1976]. A system which \nautomatically improves programs, &#38;&#38;@ Informatica ~, No. 1, pp. 41-60 Lewis, H.R. [1977], A new \ndecidable problem, with applications, Proceedings, IEEE 18th A\u00ad . SvmQosium QQ Foundations ~ ComDuter \nScience, pp. 62-73 Paterson, M.S,, and Hewitt, C.E. [19701. Comparative schematology, Record of Pro.iect \n~ Conference ~ Concurrent Svstems ~ parallel Commutation, pp. llg-127 Rangel, J.L. [1974]. The equivalence \nproblem for regular expressions over one letter is elementary, Conference Record. ~ti Annual SvmDosium \n~ Switching and Aut~mata Theorv, pp. 24-27 Strong, H.R. [19711. Translating recursion equations into \nflow charts, JCSS ~, INo. 3 (June), PP. 254-285 , ERRATUM In the solution to commutative the sequence \nMi&#38;l ARGUMENT[i] := x ; x := d(x) Q@; &#38; i from muntil n @ BACKd[i] :. a(ARGUMENT[i]) should \nbe replaced by: ~i from O until max(m,n) L@. &#38;Ya!l ARGUMENT[i] := x ; x := d(x) Q@; -i ~o~m until \nmax(m,n) @ BACKd[i] := a(ARGUMENT[i 1) Without this correction, a problem arises when d-depth(c(x) ).d-depth(x) \nfor some x: In the invocation of f2 with arguments c(x) and d-depth(x) -1, m will be set to n+l, and \nthe body of the second ~-loop will not be executed even once. Consequently, the first time that the body \nof the loop ~ i ~m, m-1 * -1 until O@ BACKd[i] := b(ARGUMENT[i ], BACKd[i], BACKd[i+l]) is executed, \nBACKd[i+l ].BACKd[m] will be undefined. The correction assures that at least one element, BACKd[m], is \ninitialized using the base rule in each invocation of f2. 157   \n\t\t\t", "proc_id": "567752", "abstract": "Many well-known functions are computed by interpretations of the recursion schemaprocedure f(x) ;if p(x)then return a(x)else return b(x,f(c<inf>1</inf>(x)),&#8230;,f(c<inf>n</inf>(x)))Some of these interpretations define redundant computations because they lead to multiple calls on f with identical argument values. The existence and nature of the redundancy depend on properties of the functions c<inf>i</inf>. We explore four sets of assumptions about these functions. We analyze directed acyclic graphs formed by merging the nodes of the computation tree for f(x) which are known to be equal for each set of assumptions. In each case there is a transformed program which computes f(x) without redundancy, provided that certain additional assumptions about p, a, and the c<inf>i</inf> are satisfied. The transformed programs avoid redundancy by saving exactly those intermediate results which will be needed again later in the computation. These programs are all valueless recursive procedures which leave intermediate and final results in specified global locations; in each case recursion can be eliminated without use of a stack. We compare the storage requirements of the transformed programs, discuss the applicability of these transformations to an automatic program improvement system, and present a general criterion for establishing the existence of redundancy.", "authors": [{"name": "Norman H. Cohen", "author_profile_id": "81100145684", "affiliation": "Harvard University", "person_id": "PP14061219", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/567752.567766", "year": "1979", "article_id": "567766", "conference": "POPL", "title": "Characterization and elimination of redundancy in recursive programs", "url": "http://dl.acm.org/citation.cfm?id=567766"}