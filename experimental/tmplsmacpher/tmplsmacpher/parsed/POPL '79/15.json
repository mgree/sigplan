{"article_publication_date": "01-01-1979", "fulltext": "\n AN INTERPRETER GENERATOR USING TREE PATTERN MATCHING # Permission to make digital or hard copies of \npart or all of this work or personal or classroom use is granted without fee provided that copies are \nnot made or distributed for profit or commercial advantage and that copies bear this notice and the full \ncitation on the first page. To copy otherwise, to republish, to post on servers, or to redistribute to \nlists, requires prior specific permission and/or a fee.&#38;#169; 1979 ACM 0-12345-678-9 $5.00 Christoph \nM. Ho ffmann* Michael J. O!Donnell* Purdue University W. Lafayette, IN 47907 Abstract: Equations provide \na rich, intuitively under\u00adstandable notation for describing nonprocedural computing languages such as \nLISP and Lucid. In this paper, we present techniques for automatically generating interpreters from equations, \nanalagous to well-known techniques for generating parsers from context-free grammars. The interpreters \nso generated are exactly faithful to the simple traditional mathematical meaning of the equations\u00adno \nlattice-theoretic or fixpoint ideas are needed to explain the correspondence. The main technical problem \ninvolved is the extension of efficient practical string matching algorithms to trees. We present some \nnew efficient table-driven match\u00ading techniques for a large class of trees, and point out unsolved problems \nin extending this class. We believe that the techniques of this paper form the beginnings of a useful \ndiscipline of interpreting, comparable to the existing dis\u00adcipline of parsing. 1. Introduction Languages \nfor computation may be classified into (a) procedural languages, e.g. ALGOL, PASCAL, which directly describe \nsequences of actions to be performed, and (b) descriptive languages, e.g. LISP, Lucid, which allow definitions \nof mathemat\u00ad ical objects, functions and relations without direct reference to computational techniques. \nSince pro\u00adcedural languages have the advantage of allowing a programmer to ensure efficiency by specifying \na computation in detail, they have received much at\u00ad *This research was supported by NSF Grant MCS 78 \n\u00ad01812 and-by a Purdue University Summer XL g,rant. tention, and their implementation may be based on \nwell-understood standard techniques for parsing, table maintenance, and code generation. The de\u00ad signer \nof an interpreter for a descriptive lan\u00adguage cannot draw on a comparable body of uniform techniques. \nDescriptive languages have the potential for extremely simple semantics, based on the tradi\u00adtional semantics \nof mathematical expressions. Carefully exploited such languages could, for many applications, make up \nin clarity and ease of veri\u00adfication what they lose in efficiency. John Backus! recent Turing Lecture \n[Ba78] makes a strong case for descriptive languages. In the past, standard interpreters of descriptive \nlanguages have often degraded this semantic simplicity both by aug\u00admenting such languages with procedural \nconstructs (e.g., assignment and goto) and by sometimes fail\u00ading to produce results even when the language \nsemantics entail an answer. For instance, car (cons(X,Y)) = X, according to [MCC60], but stan\u00addard LISP \ninterpreters fail to discover this when Y is ill-defined. We believe that a rigorous approach to de\u00adscriptive \nlanguages may yield efficient inter\u00adpreters which precisely satisfy the language specification. Moreover, \nthe difficulty of de\u00adsigning such interpreters can be significantly less than that of designing compilers; \nin fact, most, and in some cases all, of the process may be automated. For the purposes of this paper, \nthe specifica\u00adtion of a descriptive language consists of a set I of expressions allowed as inputs, a \nsubset O of I containing those expressions which are simple enough to be given as output, and a set A \nof axioms which may be used to deduce the equivalence of cer\u00adtain expressions in I. We restrict attention \nto axioms which may be written as equations. Goguen [G077] claims that any reasonable computational process \ncan be specified purely equationally. Whether or not this thesis holds, equations provide a plausible \nstarting point, which might be extended by further work, and which is already capable of expressing general \npurpose languages such as LISP and Lucid. An interpreter satisfies a specification of the form I, O, \nA above whenever, given any expres\u00adsion E in I for which there exists an equivalent o expression Ein \nO, the interpreter produces such f an EFor instance, LISP might be specified by f letting I include all \nM-expressions, i.e. expres\u00adsions formed from atomic symbols, cons, car, cdr, atom, eq, cond, and eval. \nO would contain exactly the S-expressions: those using only atomic symbols and cons. A would contain \nthe defining equations from [MCC60]. h interpreter based on those spec\u00adifications would take an M-expression, \nespecially one of the form eval(exp, env), and return an equivalent S-expression if such exists. Previous \nwork on interpreters for purely de\u00adscriptive languages includes two types of work: theoretical studies \nof equational definitions [BL77,Cad72,DS76,Ro73,Sta77,Vu74,0 D77] , all of which lack important implementation \ndetails; and specific studies Of individual languages [AW76> HM76,FW76,1] including actual implementations \nfaithful to the precise semantics [Car76,Fa77,Jo77]. This paper attempts to maintain the generality of \nthe theoretical studies while providing some of the details needed to build practical implementations. \nUsing the subtree replacement systems of [0 D77 and R073] as theoretical basis,, we outline the steps \nnecessary to apply that theory to spe\u00adcific languages,, and develop algorithms for inter\u00adpreters. Part \n~ explains briefly how the theory of subtree replacement systems applies, and what restrictions are required \nby the present theory. Part 3 sketches a specification language for inter\u00adpreters which includes interfacing \ntree replace\u00adments with simple (e.g. arithmetic) operations. Parts 4 and 5 treat the structure of the \ndata and algorithms to be used in an interpreter. We have applied the techniques of this paper to implement \nan interpreter generator, and have generated interpreters for several languages in\u00adcluding LISP and Lucid. \nBecause of the faithful\u00adness to the mathematical semantics, the generator can be used to provide immediate \nimplementations of defined data types from their specifications, as suggested by [GHM76] and [Wa76]. \nThe central issue in mechanizing interpreter generation is how to extend efficient pattern matching algorithms \nfrom strings to trees. We outline briefly the technique employed in our present implementation. We believe \nthat our ap\u00adproach provides techniques which may form the basis for a discipline of interpreter construction \ncom\u00adparable to the present discipline of compiler con\u00adstruction. 2. Reduction Sequences Applied to Interpreting \nGiven I, O, A as in the introduction, a theo\u00adretical interpreter might work as follows: take an expression \nE. and enumerate expressions Ef such that E. = Ef, until an Ef in O is found. Such a scheme is obviously \ninefficient, unless the e\u00adnumeration is done in a particularly clever manner. In many cases, equations \nmay be ordered so that (*) the righthand side of each equation is in some sense simpler or clearer than \nthe lefthand side, and so that expressions in O do not contain lefthand sides as subexpressions. In such \ncases, a better interpreter might produce a sequence Eo, El, E2, . . . of progress\u00adively simpler expressions \nby replacing lefthand sides of equations which appear as subexpression in some Ei by the corresponding \nrighthand sides, until (hopefully) an Ef in O is found. Such re\u00adduction sequences are studied in [BL77, \nCad72, DS 76, R073, Sta77, VU74, O D77]. In a reduction sequence, each occurrence of a lefthand side \nof an equation is called a redex. An expression which contains no redices is in normal form, and must \nbe the last expression in the sequence. Under the (*) assumptions, every expression in O is in normal \nform. Since a single expression may contain several different redices, there may be many different reduction \nsequences starting with the same Eo. In order to use re\u00adduction sequences for interpreters, we must know \nhow to choose an appropriate reduction sequence: one that terminates with an E in normal form when\u00ad f \never A entails EO = Ef, and one that is nest too long. To guarantee such behavior, we need a few reasonable \nrestrictions on equations. (1) No variable maybe repeated on the le&#38;\u00adhand side of an equation. For \ninstance, if X then Y else Y = Y is prohibited. (2) If two different lefthand sides match the same expression, \nthen the corresponding right\u00adhand sides must be the same. So the pair of equa\u00adtions g(O,X) = O and g(X,l) \n= 1 is prohibited, since g(O,l) could be replaced by O or 1. (3) When two (not necessarily different) \nlefthand sides match two different parts of the same expression, the two parts must not overlap. E.g., \nthe pair of equations first(pred(X)):=pred and pred(succ(X))=X are prohibited, since the left\u00adhand sides \noverlap in first(pred(succ(0))),  [0 D77] shows that, with these three restric\u00adtions, (1) For each expression \nEo, there is at most one normal form Ewhich may be obtained by re\u00ad f ducing Eo. (2) Any strategy for \nchoosing reduction se\u00adquences which guarantees that every possible out\u00adermost replacement in an expression \nis eventually done will produce Ef in normal form such that A implies that Eo=Ef, whenever Ef exists. \nWe can use (2) above to prove that an inter\u00adpreter satisfies its specifications. Strategies for choosing \nreduction sequences fall into two classes: (a) Parallel strategies, in which several redices are reduced \nsimultaneously (in practice, I?simultaneous!! reductions are scheduled sequen\u00ad tially according to some \nfair queueing discipline). (b) Sequential strategies, in which a single redex is chosen at each step. \nThe most common sequential strategy chooses redices in preorder, i.e. leftmost Outermost first. Every \nset of equations satisfying the restrictions above may be handled by a parallel strategy,, See [0 D77] \nand [VU74] for a general discussion of the additional restrictions needed to allow sequen\u00ad tiality. For \nLISP, the leftmost outermost strategy is correct and optimal, but for Lucid, because of the equations \nor(T,X)=T and or(X,T)=T, a parallel strategy is required. Using subtree replacement systems as a model, \nwe may organize the task of implementing an inter\u00adpreter into the following steps: (1) Specify the language \nto be interpreted in terms of I, O, A, with A in the form of equa\u00adtions. (2) Convert the equations A \ninto a form sat\u00adisfying (*) and the additional three restrictions above. (3) Pick a data structure to \nrepresent ex\u00adpressions and an algorithm for performing single reductions. (4) Pick a strategy for choosing \nthe next redex to be replaced, and develop an efficient algorithm to find the redex specified by that \nstrategy. Step (1) is inherently intuitive, but the other steps may be partially or fully automated. \nTo automate Step (2) requires further research.  [KB70] gives automatic techniques which sometimes succeed \nin eliminating overlap in equations; but at present, a language designer must usually per\u00adform Step (2) \nintuitively. Section 3 of this paper gives a specification language in which (1) and (2) may be presented, \nand an example of such a presentation. Sections 4,5 and 6 discuss two different methods for performing \n(3) and (4) auto\u00admatically. 3. The Interpreter Specification Language To specify the allowable input \nexpressions one need only list a set of symbols with their arities. The input expressions will be the \nusual terms composed of the listed symbols. Most inter\u00adesting programing languages include large sets \nof standard symbols such as integer constants. To avoid excessively long specifications, there are standard \nprimitive sets of symbols which may be specified by a single name. In this paper we will use the sets \ninteger, containing all integer con\u00adstants as zeroary symbols, the zeroary boolean symbols T and F, as \nwell as the binary symbols +, -> *, div, mod, eq, ne, lt, gt, le, ge. We also use the set unspecified \ncontaining as zeroary symbols all alphanumeric strings not otherwise ac\u00adcounted for, on which eq and \nne are defined. Those sets of primitive symbols containing con\u00adstants are primitive domains, and symbols \nof higher degree will be called standard functions. Now, a useful subset of M-expressions may be specified \nas follows: SYMBOLS cons: 2; car: 1; atom: 1; eval: 2; evcon: 2; integer; boolean; cond: 3; cdr: 1; pair: \n2; evlis: 2; assoc: 2; unspecified; QUOTE: O; ATOM: O; EQ: O; COND: O; CAR: O; CDR: O; CONS: O; LABEL: \nO; LAMBDA: O. Many important equations may be given direc\u00adtly, e.g. car(cons(X,Y)) = X. Theoretically, \nsuch equations are sufficient to define general purpose programming languages. For practical purposes, \nhowever, standard functions such as + defined on primitive domains should be computed by program. That \nis, the set of equations +(0,0) = O; +(0,1) =1; +(1,0) = 1; +(1,1) = 2; . . . is implicitly spec\u00adified \nby a program which, given that a subexpres\u00adsion +(X,Y) where X and Y are integer constants is to be reduced, \nreplaces it with an integer constant Z such that +(XjY) = Z. Whknever primitive sets of symbols are introduced \nin the language specifica\u00adtion, these programs representing equations defin\u00ading standard operations are \nalso included. One may wish to visualize the effect of these pro\u00adgrams as specified by schemata, such \nas +(X,Y) = Z where X,Y,Z in integer, and Z is the sum of Xand Y, with which is associated some subroutine \ncomputing, in this case, the value of Z as the sum of X and Y. In this view we can implement predefined \nstandard functions by a small extension of the mechanisms needed for user defined reductions. It is tempt\u00ading \nto extend this approach and to allow the user to associate subroutines with reductions in gen\u00aderal, analogous \nto the way in which so called semantic processing is associated with syntactic reduction steps performed \nby a parser. However, we wish to limit our device to a few standard functions defined on primitive domains, \nto ensure that the language semantics is correctly specified, avoiding tedious separate proofs. A syntactic \ndifficulty arises when a large number of equations must mention each expression in a large class. For \ninstance, one specification of LISP would include a separate equation atom(const)=T for each unspecified \nsymbol and each integer con\u00adstant. We condense such sets of equations into one by using a variable restricted \nto range over a union of primitive domains. Thus, the LISP axioms may be given as follows: AXIOMS FOR \nALL X,Y: car(cons(X,Y)) = X; cond(T,X,Y) = X; cdr(cons(X,Y)) = Y; cond(F,X,Y) = Y atom(X) = T where X \nin integer u unspecified boolean: atom(cons(X,Y)) = F; . . . The complete set is given in the Appendix \nA. The phrase FOR ALL X,Y indicates that the symbols X and Y are to be taken as variables, rather than \nas un\u00adspecified constants, in the axioms. The set of output expressions is given im\u00adplicitly as the set \nof normal form expressions as determined by the axioms. A separate specification of o could lead to a \nuniform treatment of certain errors (see [0 D77] p. 83), but this possibility has not yet been explored. \n4. Reduction With Backpointers The problem of reducing an expression E. to normal form divides naturally \ninto (a) finding redices, (b) choosing the next redex to be replaced, (c) performing a single reduction \nstep. Finding redices is essentially a pattern match problem with trees instead of strings. The choice \nof a suitable algorithm is complicated by the fact that each reduction step alters the expression tree \nlocally. It is unacceptable to rescan the entire structure after each reduction step. Expressions may \nbe stored as dags in which each node is connected by a circular pointer struc\u00adture to each of its fathers. \nas sketched: m rep:sen+ In this way, from any node we can find any son or father. Without sharing, we \ndouble the pointer space required. Sharing (i.e., multiple fathers) cuts down on this wasted space, and \nmay reduce the number of reduction steps needed to reduce Eo. Since the technique for a single reduction \nstep is straightforward, we omit discussing it. The key 5. Matching Tree Patterns idea for identifying \nredices is to associate with each node a code indicating which part(s) of left\u00adhand sides of equations \nmatch the subtree at that node. We compute these match states from the leaves up, using a precomputed \ntable giving the state at node p as function of the label at p and the states ofp s sons. In this way, \nall redices may be lo\u00adcated in EO in time proportional to the size of the dag. The problem of matching \ntree pattezns and of generating these tables is studied in detail in [H078 ]. For the purposes of this \npaper, we limit the discussion of the technicalities of tree match\u00ading to Section 5. When a reduction \nis performed, match states must be computed for any new nodes which have been added, and for some of \nthe ancestors of the redex. The back pointers make it easy to find all affected ancestors. The length \nof any path along which an update can occur is limited by the maximum ,depth of equation lefthand sides. \nDuring this local update, new redices may be discovered. Given that every node in the dag representing \nan expression has been assigned the appropriate match state, a simple parallel strategy for choosing \nthe next redex would be to keep all redices in a queue, reducing the first redex in it, and adding newly \ndiscovered redices at the rear. A standard reference count detects if any redex is remcnred from the \nexpression as side effect of a redex pre\u00adceding it in the queue. The strategy is correct but not optimal. \nIn cases where an a priori sequential order\u00ading of redices is given, an appropriate depth-first traversal \nof the dag implements the reduction strategy. A single additional bit maintained at each node indicating \nwhether the subexpressi,on rooted at that node is in normal form will prevent useless rescanning of the \nsame (shared) normal form subexpression repeatecIly. We have implemented a generator system based on \nthese ideas in PASCAL. The implementa\u00adtion effort has been approximately 5 man weeks. We have generated \nno frills interpreters for LISP, Lucid, and the combinator calculus. Actual reduc\u00adtions have been very \nfast. We wish to generate tables from a set of tree patterns (the axiom lhs) with which to drive the \nlinear matching algorithm outlined in Section 4. All possible sets of partial matches need to be known, \nsince they are used to index into the tables during the matching process. During the process of this \ngeneration, the restrictions on axiom lefthand sides of Section 2 will also be checked. Note that there \nis a straightforward matching algorithm which works on O(n . m) steps, where m is the pattern size to \nbe matched in a subject of size n. This algorithm requires no preprocessing, and works for all patterns. \nThe algorithm of Section 4, in contrast, matches in O(n) steps, after suitable tables have been gener\u00adated \nas explained now. Given a forest F of tree patterns {tl,. ..,tk}, a match set M for F is a set of (sub)trees \nin F . such that there is a subject tree t such that every member of M matches t at the root, and every \n(sub) tree in F which is not in M does not match t at the root, M is thus the set of all (partial) matches \nat the root of it. A table of match sets may be genqrated straightforwardly in time k+l 0(s . m) where \ns is the number of sets, k is the maximum arity of any symbol (so the table is size O(sk)), and m is \nthe total size of all pattezms. We discuss only more efficient methods. Given distinct patterns t and \nt , we define two relations: t subsumes t , t > t~, if a match of t always implies a match of tt at the \nsame node. For example, a(b,c) > a(b,v), since v matches wherever c does. t and t are independent, t-t \n, if we can find subject trees t ~, t2 and t3 such that t matches tl and t2 at the root, but not t3, \nwhereas t matches tl and t3 at the root, but not Thus, a(b,v) -a(v,c), because of trees a(b,c), 2 a(b,b), \nand a(c,c). It is not hard to show that each match set M may be partitioned uni@ely into a base set MO \nOf . pairwise independent trees, and a set Ml of trees each of which is subsumed by some tree in MO. \nBecause of the transitivity of subsumption, each match set is completely determined by its base set. \nUsing the defined relations, [H078] shows that (1) The number of distinct match sets may grow exponentially \nwith the pattern size. (2) If there are no (sub)trees in the pat\u00ad  tern forest F which are independent, \nthen the number of possible match sets is equal to the size of F. Because of these results, we restrict \naxioms such that their lhs form pattern sets in which no two (sub)trees are independent. Such patt, m \nsets are called simple. Define immediate subsumption, >i, by >i t!, iff t > t and there is no (sub)tree \nt!t in the pat\u00ad tern forest F such that t > t and t > tt The directed acyclic,,graph G~ of the immediate \nsubsump\u00adtion relation is called the subsumption graph of F. For simple forests, [H078] shows that (1) \nG~ is a tree. (2) The base set of each match set M is a singleton. (3) The match set Mwith base set \n{t} is pre\u00adcisely the set of trees on the path from the base set tree t to the root of G  s Let m be \nthe size of the pattern forest F, and d the depth of Gs. There is a straightforward algorithm for computing \nthe transitive closure of G~ in 0(m2) steps. Using an indexing scheme, we can design an 0(m2 d) algorithm, \nwhich is slower in the worst case, but can be expected to run sig\u00adnificantly faster than the 0(m2) algorithm, \nwhich is quadratic for all inputs, Both algorithms, at the same time, can check that F is simple, and \nthat the restrictions of Section 2 are satisfied, with\u00adout affecting the running time. The 0(m2.d) algorithm \nis given in Appendix B. If there are no function symbols with arity exceeding 2, then there is an O(m.d) \nalgorithm for computing Gs. The algorithm can be adapted to per\u00adform the actual matching too, leading \nto a matching algorithm of O(n.d) steps in a subject of size n. The algorithm can be adapted to compute, \nin the same time bound, Gs for alphabets of higher degrees, but will then be unable to process certain \nsimple forests. The details are covered in [H078]. Once G~ has been computed, the tables to drive the \nO(n) matching algorithm can be constructed easily. If k is the highest occuring arity in the alphabet \nZ, then the tables require O (mk) space and take O(mk.d) steps to construct. Unfor\u00adtunately then, table \ngeneration is the bottleneck of the preprocessing. Since the maximum arity k of alphabets affects the \nsize and time of table genera\u00adtion so critically, it is useful to reduce k by introducing a set of pairing \nfunctions. We have used this technique successfully to speed up the interpreter generation, but it should \nbe noted that pairing sometimes transfoms simple pattern forests into forests in which independence occurs, \nThis phenomenon is also responsible for the failure of the adapted O(m.d) algorithm to process all simple \nforests for alphabets of higher arities, since the intermediate graphs constructed by the algorithm conceptually \nimitate argument pairing. Although table generation is the bottleneck of the preprocessing, it is well \nworth while to investigate ways to speed up the computation of Gs further, because these algorithms can \nbe adapted to perform the actual pattern matching without the need for generating large tables. This \nmay best be understood by observing the analogy of tree pattern matching and string pattern matching \nin the style of [KMP77] and [AC75]. Consider a string pattern al, ,,.,ak as non\u00ad branching tree ak (a \nk_~(. ..a~(v) )..)). The graph Gs for a forest of such nonbranching trees is pre\u00adcisely the graph of \nthe failure function of [KMP77] and [AC75]. For this, note that a subtree of a non\u00adbranching tree is \na pattern prefix. Now t > t , for trees t and t , if t! matches t at the root, therefore, in the case \nof nonbranching trees, t! is a pattern prefix which is, at the same time, a suffix of tl. Thus t ~i t \niff t is the largest proper pattern prefix which is also suffix of t, Thus, it is reasonable to look \nfor matching algorithms which use principally Gs as data struc\u00adture. The adaptation of the O(m.d) algorithm \nis designed in just that way. 6. Reduction Without Back Pointers Up to half the pointers in the implementation \nof Section 4 may be eliminated by representing dags without back pointers. At present we do not have \na nice algorithm for parallel strategies without back pointers. For sequential strategies, a simple implementation \nuses the match states and normal form bit of Section 4. A depth-first trav~rsal is used to find the next \nredex, skipping any subtrees which are marked as being in normal form. Whenever a node is found whose \nstate or normal form bit may be changed by recomputing from its sons, the change is propagated upwards, \nbut only along the path by which the node was reached (this path is known from the standard stack used \nfor the traversal). Informal Development of a More Powerful Algorithm The simple algorithm outlined above \ndoes not address the problem of finding an acceptable sequen\u00adtial strategy. We have a method which finds \noptimal sequential strategies automatically. In addition, this method generalizes a trick applied by \nFriedman and Wise to LISP [FW76,2] in which portions of an expression which have become stable are output \nand eliminated to save space. A fuller development of the algorithm is being prepared for journal publica\u00adtion, \nwith a proof that the method finds a secpen\u00adtial strategy whenever such success is possible without considering \nthe right hand sides of eclua\u00adt ions. The match state idea from section 4 is suffi\u00adcient for recognizing \na redex once we have scanned the appropriate part of a tree. The additional problem is to decide which \nparts of the tree to scan. This decision must account for the possibility that some match states are \nout of date, since a shared subtree may be changed through one path without the change being noticed \non other paths. The main new concept needed is that of a possibility state. The match state M(n) at a \nnode n represents all partial matches known to hold at n. The nonoverlapping property guarantees that \nexisting matches at n will never be destroyed by reductions at descendants of n, but new matches could \nbe created. The possibility state P(n) foT n represents a set containing all partial matches which might \never hold at n as the result of re\u00adductions at descendants. At any given time, some matches in P(n)-M(n) \nmay already hold at n due to reductions not yet noticed by n. The true set of matches holding at n must \nalways be a superset of M(n) and a subset of P(n). To avoid obvious unde\u00adcidable questions, P(n) is computed \nwithout knowl\u00adedge of equation right hand sides by assuming that a redex nay be replaced by any tree. \nFinally, we need one more kingof state, called a search state, to keep track of those partial matches \nwhich might be useful to the reduction. Match states and possibility states are stored at each node. \nSearch states S are stored on the traversal stack, and contain all partial matches which might make a \nreduction possible at some node on the stacked path from the root. The algorithm is developed from the \nfollowing observations: 1) If M(n) contains a complete match, then a reduction may be performed; . 2) \nIf M(n)fl S # j?J then an interesting change has occured which should be propagated up the tree; 3) If \nM(n)fiS = P(n)l)S = ~ then no node presently on the stack will ever be changed; 4) If M(n)flS = j? but \nP(n)fiS # @ then further processing of descendants is needed, and the appropriate son to visit may be \nrecognized by his match state and possibility state. A precise statement of the algorithm is attached \nas Appendix C. The problem of precomputing states for the new algorithm is even trickier than for the \nold. So far we know that in some cases possibility sets are exponential in number even though match sets \nare very few. More study is needed to discover those cases in which the total number of combinations \nis not too big. Appendix A --LISP Equations McCarthyfs original LISP equations [McC60] have several apparent \nmistakes. The following equations re\u00ad present a correction and reordering of McCarthy s definition. AXIOMS \nFOR car,( cons ( cdr ( cons ( atom ( cons ( atom ( X ) ALL V, X, Y) ) X, Y) ) X, Y) ) W, X, = X; = Y; \n= F; =T Y, Z: where X in integerLJbooleanUunspecified U{QUOTE, ATOM, EQ, COND, CAR, CDR, CONS, LABEL, \nLAMBDA}; cond cond (T, (F, X,Y)=X; X,Y)=Y; eval eval eval ( x, Z) = assoc(X,Z) where X in unspecified; \n. (cons( X, Y)jZ) = apply (eval(X,Z),evlis(Y,Z)) where X in unspecifiedu{ATOM, EQ, COND, CAR, (cons(cons(W,X),Y),Z) \n= apply (eval(cons(W,X),Z), CDR, CONS}; evlis(Y,Z)); apply(eval(ATOM, apply(eval(EQ, apply(eval(COND, \napply(eval(CAR, apply(eval(CDR, apply(eval(CONS, Z),cons(X, Y)) Z),cons(W,cons(X,Y))) z) , x) Z),cons(X, \nY)) Z),cons(X, Y)) Z),cons (W,cons(X,Y))) = = = = = = atom(X); eq(W,X); condlis(X); car(X); cdr(X) ; \ncons(W,X); apply(eval(cons(LABEL,cons (V, cons(W,X))),Z),Y) apply(eval(W,cons (cons(V,cons(cons (LABEL, \napply(eval (cons(LAMBDA,cons (V, cons(W,X))),Z),Y) eval(W,append(pair(V,Y) ,Z)); = cons(V,cons(W,X)) \n= )),NIL),Z),Y) ; evlis(NIL, evlis(cons(X,Y), Z) Z) = = NIL; cons(eval (X,Z),evlis(Y,Z) ); append(NIL, \nappend(cons(W,X), Y) Y) = = Y; cons(W,append(X,Y)); pair(NIL, NIL) pair(cons(V,W),cons(X,Y)) = = NIL; \ncons(cons(V,cons(X,NIL)),pair(W,Y) ); condlis(cons condlis (cons (cons(T,cons (cons(F,cons(X (X,NIL)),Y)) \n,NIL)),Y)) = = X; condlis(Y) assoc(X,Y) = cond(eq(car(car(Y) ),X) ,car(cdr(car(Y )),assoc(X,cdr(Y))). \nw Appendix B --Pattern Preprocessing Algorithn! The preprocessing of simple pattern forests for generating \ntables divides into the computaticm of the subsumption graph GS (which is a tree for simple forests), \nand the generation of tables from CS. The computation of GS, with suitable changes not indi\u00ad . cated \nhere, also verifies that the pattens presented form a simple forest. Verifying the nonoverlap property \ncan also be incorporated. Assume patterns tl, . . ..tk are given. Let T denote the set of all (sub)trees \n(of) the ti, and denote a directed edge from t to t in GS by f(t) = t! --i.e. t directly subsumes t . \nThe computation of GS is now as follows: Algorithm A Compute Subsumption Graph GS for Linear Forest F \nInput: Linear forest F of patterns Output: Tree GS(with edges pointing to ancestors) Method: 1. Order \nall trees in T by their depth. 2. For each t = v in T of depth 1 enter f(t)=v; Comment: f(t) = t iff \nthere is a directed edge from t to t ; 3. For p := 2 to MAXDEPTH IN FOREST do  For each t=a(tl ,.. \n.,tk) in T of depth p do begin . . 4. s. v; 5, For i := 1to kdobegin .. 6. t! := f(ti) ; 7. while \nthere is no tree t of (maximal) depth ~p which is subsumed by t and has t? as i-th sub\u00adtree and t $vdo \n .  8. t! := f(t ); 9. For each tree t with t as i-th  . subtree which is subsumed by t and of maximal \ndepth < p &#38;  10. if tl! >s then s := t!!; .  11. &#38;; 12. enter f(t) = s; 13. ellJ;  Note \nthat, since we process trees ordered by increasing depth, the test t subsumes t can be done by verifying, \nfor each immediate subtree pair ti and ti , that ti subsumes t. . Since ~he depth 1 of both ti and t: \nmust be strictly smaller than p, this test involves tracing through the existing portion of GS. Since \nthere may be, in some cases, up to O(m) trees t! with i-th subtree t!, where m is the cardi\u00adnality of \nT, and since tracing through the existing portion of GS for testing subsumption may involve up to O(d) \nsteps, where d is the depth of GS, the algorithm requires 0(m2.d) steps. Given GS, we can then construct \ntables in the following manner. Algorithm B Input: Subsumption graph GS of linear forest F output : \nTables driving the fast matching algorithm of Section 2. Method: 1. Traverse GS in post order. For each \ntree t = a(tl, ..., tk) visited, k > 0, do the following: 2. Enter t into all portions of the table \nfor a  which are not yet assigned and are indexed ! by tuples <t , t t; > where, for 1 2  l<i<k, t!>t \n.~ t;= t;. . 1i 3. Enter v into all ~emaining unassigned table positions in each table. For linear forests, \nit can be proved that Algorithm B cannot attempt assigning t to an entry already assigned t!, unless \ntt>t; hence the O(nk.d) time bound, where k is the highest occurring arity in X. Appendix C --Reduction \nWithout Backpointers Let n be any node in a dag representing a tree. Labels: !.(n) is the alphabet symbol \nat n; Match States: M(n) is the set of all partial matches known to hold for the subtree rooted at n; \nPossibility States: P (n) is a set of partial matches which might arise at n as the result of reductions \nat proper descendants of n; P(n) is a set of partial matches which might arise at n as the result of \ndeductions at n and its descendants; C is the set of complete matches of patterns; U is the set of all \npartial matches. Note that P(n) = P (n) if P (n)nC = @ P(n) = U if P (n)nc # $ A stack of pairs ~ n,S> \nis used to control the traversal. The nodes on the stack form a branch from the root. The search state \nS is the set of all partial matches whose occurrence at n could produce a complete match at some node \nbelow < n,S > on the stack. When T is a set of partial matches, Soni(T) is the set of all ith subtrees \nof roots of members of T; Fatheri(T) is the set of all partial matches whose ith subtrees are in T; Ua \nis the set of all partial matches with root labelled a.  The Algorithm . Initially: M(n) = P (n) = Ui[n) \nfor each leaf n; M(n) =~Fatheri(M(soni(n)))f\\ UL[n) P (n) =~Fatheri(P(soni(n))) n Ul[nl for every nonleaf \nn; P(n) = (P (n) if P (n) n C = @ { UifP (n)(lC#fd for every node n; The stack initially contains < Root, \nC >. While the tree to be reduced has not been SPLIT Do ~ n,S ~ := top of stack; If n is not a leaf then \nM(n) :=~ Fatheri(M(Soni(n) ))nUL[n); P (n) := ~Fatheri(P(Soni(n)))flUi(nl ; P (n) := P (n) if p (n)nc \n= g { U if P-(n)nc # p; End if; IF M(n)flC # p then REDUCE Else if M(n)nS # ~ then POP Else if P(n)nS \n= O then SPLIT Else Choose i such that Soni(P-(n)fl(SUC)~(Soni(n)) = ~; PUSH (&#38;oni(n),Soni(P (n)n(SUC) \n)>) End else End While REDUCE, PUSH AND POP have their intuitive meanings. SPLIT is invoked when the \nnodes on the stack have all stabilized (i.e., future reductions cannot possibly change them). SPLIT outputs \nthe nodes on the stack, freeing them for garbage collection, and initiates processing of the remaining \nsubtrees in any order, or simultaneously. References [AC7S] Auo, A, and M. Corasick, Efficient String \nmatching: An Aid to Bibliographic Search CACM 18:6, 333-343. [AW76] Ashcroft, E. and W. Wadge, Lucid-A \nFormal System for Writing and Proving Programs. SIAM J on Computing 5:3, 336-354. [Ba78] Backus, J. \nCan Programming Be Liberated from the vonNeumann style? A Functional Style and its Algebra of Programs, \nCACM 21:8, 613-641.  [BL77] Berry, G. and J. J. Levy, Minimal and Optimal Computations of Recursive \nPrograms. 4th ACM Symp on POPL, 21S-226. [Cad72] Cadiou, J., Recursive Definitions of Partial Functions \nand Their Computations, Ph.D. Diss, Comp. Science Dept., Stanford University. [Car76] Cargill, T., Deterministic \nOperational Semantics for Lpcid, Res. Rept. CS-76-19, Univ. of Waterloo.  [DS76] Downey, P., and R. \nSethi, Correct Computa\u00adtion Rules for Recursive Languages. SIAM J on Computing 5:3, 378-401. [Fa77] Farah, \nM., Correct Compilation of a Useful Subset of Lucid, Ph.D. Diss., Dept. of Comp. Science, Univ. of Waterloo. \n[FW76,1] Friedman, D., and D. Wise, Cons should not evaluate its arguments, 3rd Int. Colloq. on Automata, \nLanguages and Programming, Edinburgh. [FW76,2] Friedman, D., and D. Wise, Output Driven Interpretation \nof Recursive Programs, or Writing Creates and Destroys Data Struc\u00adtures. Inf. Proc. Letters 5:6, 85-89. \n [GHM76] [G077] [HM76] [H078] [J077] [KB70] [KMP77] [MCC60] [0 D77] [Sta77] [Wa76] [R073] [VU74] Guttag, \nAbstract tion, 76-48, Evaluator, Hoffmann, Technical Purdue Johnson, Language Pattern Computing McCarthy, \nSymbolic tion by O Donnell, Described Notes in Staples, J., E, Horowitz and D. Musser, Data Types and \nSoftware Valida- Inf. Sci. Inst. Res. Rept. ISI/RR- Univ. of Southern Cal. Goguen, J., Data Types, Descr. \nof North-Holland, Henderson, Abstract E-rors for Abstract IFIP Working Conf. on Formal Progr. Concepts, \nJ. Dennis, cd., P., 3rd C,, Rept. University, S.D., Based and J. H. Morris, A Lazy ACM Symp. on POPL, \n95-103, Matching Tree Patterns, 291, Dept. of Comp. Sci., 1978. An Interpretive Model for a On Suspended \nConstruction, Tech. Rept. 68, Dept. of Comp. Sci., Indiana University. Knuth, D., and P. Bendix, Simple \nWord Problems in Universal Algebras. Computa\u00ad tional Problems in Abstract Algebra, J. Leech, cd., Pergamon \nPress, Oxford> 263-297. Knuth, D., J. Morris, and V. Pratt, Fast Matching in Strings, SIAM J on 6:2, \n323-350. J., Recursive Functions of Expressions and Their Computa-Machine, CACM 3:4, 184-195. M., Computing \nin Systems by Equations, Springer Lecture Comp. Science f58. J., A Class with Simple Optimality the Australian \nMath. Wand, M., First Order of Replacement Systems Theory, Bull. of Sot., to appear. Entities as Defining \n Language. Techn. Rept. 29, Dept. of Comp. Science, Indiana University. Rosen, B. K,, Tree Manipulation \nSystems and Church-Rosser Theorems, JAC14 20:1, 160-187. Vuillemin, J., Correct and Optimal Implementations \nof Recursion in a Simple Programming Language. JCSS 9:3, 332-354.  \n\t\t\t", "proc_id": "567752", "abstract": "Equations provide a rich, intuitively understandable notation for describing nonprocedural computing languages such as LISP and Lucid. In this paper, we present techniques for automatically generating interpreters from equations, analagous to well-known techniques for generating parsers from context-free grammars. The interpreters so generated are exactly faithful to the simple traditional mathematical meaning of the equations-no lattice-theoretic or fixpoint ideas are needed to explain the correspondence. The main technical problem involved is the extension of efficient practical string matching algorithms to trees. We present some new efficient table-driven matching techniques for a large class of trees, and point out unsolved problems in extending this class. We believe that the techniques of this paper form the beginnings of a useful discipline of interpreting, comparable to the existing discipline of parsing.", "authors": [{"name": "Christoph M. Hoffmann", "author_profile_id": "81100074601", "affiliation": "Purdue University, W. Lafayette, IN", "person_id": "PP14036210", "email_address": "", "orcid_id": ""}, {"name": "Michael J. O'Donnell", "author_profile_id": "81339520288", "affiliation": "Purdue University, W. Lafayette, IN", "person_id": "PP43117037", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/567752.567768", "year": "1979", "article_id": "567768", "conference": "POPL", "title": "An interpreter generator using tree pattern matching", "url": "http://dl.acm.org/citation.cfm?id=567768"}