{"article_publication_date": "01-01-1979", "fulltext": "\n Permission to make digital or hard copies of part or all of this work or personal or classroom use is \ngranted without fee provided that copies are not made or distributed for profit or commercial advantage \nand that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, \nto post on servers, or to redistribute to lists, requires prior specific permission and/or a fee.&#38;#169; \n1979 ACM 0-12345-678-9 $5.00 DATA FLOWANALYSIS OF COMMUNICATINGPROCESSES --Extended Abstract John H. \nReif Computer Science Department University of Rochester Rochester, New York 14627 0. Abstract Data flow \nanalysis is a technique essential to the compile-time optimization of computer pro\u00adgrams, wherein facts \nrelevant to program optimi\u00adzation are discovered by the global propagation We are interested in data \nflow analysis of . communicating processes: the discovery of facts of facts obvious locally. This paper \nextends flow analysis techniques about distant processes and propagation of thesedeveloped for sequential \nprograms to the analysis facts across process boundaries. An analysis of communicating, concurrent processes. \nproblem of particular interest here is 1. Introduction reachability: can a given program statement We \nconsider a set of distant concurrent ever be reached in some execution? Reachability processes, each \nsequentially executing a distinct is perhaps the simplest of data flow analysis program and communicating \nby the transmission and problems. reception of messages. By distant it is meant Section 2 describes our \nflow model for a that there is no interference between processes system of communicating processes, in \nwhich the by shared variables, interrupts, or any other control flow through each process s program is \nsynchronization primitives beyond the message represented by a flow graph. This model allows primitives. \n(The processes might in fact reside for all executions valid in the usual semantics of in the same machine. \n) communicating processes, but also may allow for Various channels are available for communi\u00adadditional, \nspurious executions. Of course, a cation between processes, and each channel has a statement unreachable \nin our model is also unique process which is the destination of messages unreachable in the usual, more \npowerful semantics. transmitted through this channel. The Unfortunately, we show in Section 3 that communication \nbetween processes is static if the testing reachability in our flow model is channel arguments to message \nprimitives are recursively undecidable, using a reduction from constants, and otherwise is dynamic. We \nconsider state reachability of two-counter machines. In both these cases. In the usual semantics for \nthe case of static comriwnication, we show that message communication, [Feldman, 1976], the testing reachability \nin our flow model is process transmitting a message need not wait until log-space reducible to and from \ncoverability in reception of the message, and thus a queue of Petri nets, which [Lipton, 1975] has shown \nto yet-to-be-received messages is associated with require at least exponential space, infinitely each \nchannel. (In a more restrictive semantics often. Nevertheless, we have developed proposed in [Hoare, \n1977], the transmitter of polynomial-time algorithms for approximate a message M is required to wait \nunt il acknow\u00ad solutions to analysis problems. ledgement (a handshake ) of reception of kl.) As an aid \nto our flow analysis, we define in Section 4 a special acyclic directed graph called Example 2.1 s an \nevent spanning graph containing a spanning tree of each process s flow graph, as well as certain edges \n(called message links), connecting pairs of TRANSMIT and RECEIVE statements between which a message may \nbe sent. If no event spanning graph exists, then some program statement is unreachable. message may be \nsent. If no event spanning graph exists, then some program statement is unreachable. Section 4 presents \na linear time algorithm for constructing, when possible, an event spanning graph for the case of static \ncommunication. In Section 5, we describe an iterative technique for data flow analysis of communicating \nprocesses with statis communication, which generalizes a technique for data flow analysis of sequentially \nexecuted programs due to [Hecht and Unman, 1975]. Their algorithm repeated (until convergence) a pass \nthrough the flow graph of a single program, in topological order of its DAG (directed acyclic graph); \nour proposed algorithm repeats a pass through all the flow graphs of a set of communicating processes, \nin topological order of their event spanning graph. Section 6 extends our data flow analysis to the case \nof dynamic communication. We present an algorithm which builds an event spanning graph while simultaneously \nperforming data flow analysis. Section 7 concludes the paper. 2. The Flow Model for Communicating Processes \nWe describe here a flow model for a system of communicating processes. Let C be a set of symbols denoting \ncommunication channels. Each process P sequentially executes a distinct program, represented by a process \nflow graph G = (N,E,s). Each node nsN corresponds to a single (non-control) program statement. The edge \nset Es2NxN consists of pairs of nodes between which control may transfer. An execution ~ is a path of \nG beginning at the start node s. Occasionally, we will distinguish a final or exit node nfsN from which \ncontrol may be considered to exit. 1 \\ Qnl\\ 3 n2 The above flow graph has execution path (s, nl, n2, \nnl, n3), among others. The state local to the process P is specified by a program counter (pointing to \nthe currently executed program statement), the value bindings of the program variables local to P, and \nthe message queues for channels with destination P (to be described shortly). Program statements include: \n(1) Assignment statements of the form X+-a where X is a program variable 10cal tO P and a an expression, \nwith the usual effect of setting X to the result of evaluating a. (2) A transmit statement of the form \nTRANSMIT(al,a2). The first argument al must evaluate to a message channel cc C , and a2 evaluates to \nthe message to be transmitted, say M. The message M cannot be a pointer value, but is otherwise unrestricted \n(in particular, M may be a communication channel). The second argument, a2, may be absent, in which case \nsome fixed default message is transmitted. (3) A receive statement of the form x+RECEIVE(a) where the \nargument a mLISt evaluate to a communication channel CEC, and X is a program variable local to P assigned \nthe value of the message received. We assume no start node of a flow graph is a RECEIVE statement. will \nalso be allowed. (4) No-op (empty) statements Control statements are not found in N since the contro \nl flow is specified in our model by the edges of f low graph G. The sets of program variables local to \ndistinct processes are disjoint, and are assumed to have no shared values. Thus , there Is no interference \nbetween processes except that induced from our message primitives. An event is the execution of a statement \nby a process and will be assumed to occur instan\u00adtaneously; it might be preceded and succeeded by other \nevents and might also occur simultaneously with others, A sequential execution of a single process flow \ngraph G is a total ordering of events resulting from the sequential execution of the statements occurring \nin an execution path of G. Given process flow graphs Gl,. ..,Gr, an execution in our flow model is a \npartial ordering OF events (E,+) in which those events associated with any given process flow graph Gi \nform a sequential execution of Gi, and such that (S,->) is Consistent with the semantics of the message \nprimitives TRANSMIT and RECEIVE as defined below. Intuitively, e+e if event es~ preceeds event e :~. \nIf es~ is an event resulting from the execution of statement RECEIVE (a) and devaluates to channel c, \nthen e must be preceeded by a unique event e c~ resulting from the execution of a TRANSMIT statementwhose \nfirst argument evaluates to channel c and whose second argument evaluates to the message received. In \naddition, we assume that if a process sends two successive messages, Ml and M2, over a channel c, they \narrive in the order that they were transmitted. Hence, if e1,e2s&#38; are events resulting from the reception \nof MI,M2 and e l,e 2 SE are the corresponding message transmitting events, e--+.e implies e l+ e z . \nNo further assumptions about message behavior are made. Note that the resulting semantics are nondeterministic \n--two simultaneous message transmissions by different processes over the same channel must arrive in \nsequential order, but we make no assumptions about this order. An operational semantics for communicating \nprocesses is specified by designating for each channel CCC a message queue Q(c), listing the messages \ntransmitted but yet-to-be-received over channel c. A RECEIVE statement has the effect of deleting the \ncut-rent message M at the front of the appropriate message queue, and evaluates to M. The order of messages \nappearing in the queue need only be consistent with our assumption that successive transmissions over \na given channel from a given process be received in order of transmission. 3. Complexity of Reachability \nin the Flow Model A program statement n is reachable if it appears in some execution in our flow model. \nIn this section we consider the complexity of testing reachability. In the general case of dynamic communication, \nit will be shown that the reachability problem is undecidable. Even in the case of a static communication \n(where the channel arguments to all TRANSMIT and RECEIVE statements are constants), testing reachability \nrequires exponential space, infinitely often. A two-counter machine is a finite-state automata augmented \nwith a pair of counters which may b e incremented, decremented, and tested for zero. The halting problem \nfor two-counter machines is known to be recursively unsolvable [Minsky, 19611. Our proof that reachability \nof program statements is undecidable utilizes a simple recursive reduction from the halting problem for \ntwo-counter machines. This proof exploits the ability of a single TRANSMIT statement to communicate over \nvarious channels, depending on the evaluation of its channel arguments. Theorem 3.1: Reachability is \nundecidable in the flow model of communicating processes. Proof: Consider a two-counter machine (S, qo, \nqf, 4, Cl, C2) with state set S, initial state qoES, final state qfeS, set of instructions Jl, and counters \nC1,C2. Associated with each state qeS -{qf} is an instruction IqC ~ of one of the following forms: (1) \n(increment, i,q ) where in state q the counter Ci is incremented by 1 and state q is entered. (2) (decrement, \ni , q ) where in state q the counter Ci is decremented by 1 and state q is entered. (3) (~, i, ql,q2) \nwhere in state q  if counter Ci=O then state ql is entered and otherwise state q2 is entered. A computation \nbegins in the initial state qO with both counters set to O. Computations resulting in transitions to \nundefined states or negative counters are undefined. The computation halts at state qf. To simulate this \ntwo-counter machine, we build a process P which communicates to and from itself on channels {Cl, C2, \n#, $}, (that is, P is the origin and destination of all messages). The flow graph of P is G = (N,E,nqo). \n Initially, the message queues Q(CI), Q(c2), Q(#), and Q($) are empty. Associated with each instruction \nIq CJ is a subgraph Gq of G which simulates this instruction. We shall claim that if Ci contains the \ninteger k zO, then Q(Ci) = #k. (1) If Iq = (increment, i, q ) then G q is of the form: Hence, an execution \nof q results in the addition of # to Q(Ci). (2) If Iq = (decrement, i, q ) then Gq is of the form: q \nQ@EiEzD and so an execution of q results in the deletion of some #cQ(Ci) if this message queue is not \nempty, and otherwise if Q(Ci) is empty, then the process P hangs. (3) If Iq = (q, i, ql, q2) then Gqis \nof the form: TRANSMIT (X) p \u00ad -~, - a TRANSMIT (X) L *:$$D -... . It may easily be shown that if Q(Ci) \nis empty on is reached and otherwise Xecut on f hen ql is reached, In either case the queues are qz \nrestored to their state just before execution of n q Hence, the node nqf is reachable in some execution \nof P just in the case the given two\u00adcounter machine halts.Q Next, let us assume the channel arguments \nto all TRANSMIT and RECEIVE statements are constants\u00ad-so that the communication is static. With this \nrestriction, there is a strong resemblance to a synchronization structure called a Petri net. In fact, \nwe can show that reachability of statements in this case is polynomial-time reducible to and from coverability \nof Petri net markings. A petri net is a bipartite directed qraph PN = (TUT, EPN) with a set of places \nm, a set of transitions T , and a set of directed edges EPN, each edge containing exactly one place and \none transition. A marking of PN is a mapping p from the places T to the non-negative integers. Given \na marking u and a transition tcT with no predecessor marked by pwith O, t is fired by decrement ing the \nmarkings of predecessors of t by 1 and incrementing the markings of successors of t by 1. The resulting \nmarking u is said to be derived from v . A marking N is derivable from initial marking uoif there exists \na sequence of markings (PO, ~l,. -.,pk = p) such that ui is derived from vi-l for i=l, . . . . k. A marking \np is coverable if there exists a derivable marking u such that u (x)~u(x) for all places Xcll. A survey \nof Petri nets appears in [Peterson, 19771. Theorem 3.2: Coverability in Petri nets is log\u00adspace reducible \nto reachability in our flow model with static communication. Proof: LetPN= (ITUT, EPN) be a Petri net \nwith initial marking I-IO and suppose we wish to test if marking u is reachable in PN. For each transition \nt eTwith predecessors xl, . . . . xj and successors xi, . . . . xi , we have a Process flow graph Gt: \nstart: e-(~Y3 . ----------\u00ad ,7 $ + .RECEIVE (<xj,t>) CA I Observe that on receiving messages from channels \n<x ,t>, . . . . <xj,t>, Gt transmits messages 1 over channels xi, . . . . Xi . For each place XSTI with \nsuccessors tl, . . . . tk there is a process flow graph Gx: start: 4 R(x,Po(x)I J qaD M TRANSMIT (<x,tl> \n. . TRANSMIT (<x,tk> r.> ~&#38;:&#38; Observe that for each message received over channel x, Gx transmits \nover one of the channels <x,tl>, . . . . or <x,tk>. For any place xcm and integer k, R(x,k) is a subgraph \nwhich adds k messages to Q(x) if k~O, and otherwise attempts to delete I k I messages from Q(x) if k<O. \nIf k<O and Ikl> IQ(c) I then the process hangs and no successors of R(x,k) are reached. A simple but \nsomewhat tedious construction produces an R(x,k) of size O(log Ikl). The first time the node nx is reached, \nQ(x) contains Uo(x) messages. The exit node of process flow graph Gx is reached just in the case Q(x) \ncontains 2P(x) messa9es on some visit to nx. F inally, we have a process flow graph Go: start: W exit: \n where II ={xl, . . . . xs} is the set of places. It follows that the exit node of GO is reachable if \nand only if the marking P is coverable in Petri net PN with initial marking Uo. D 261 Example 4.1: The \nfollowing are portions of flow By using Petri nets to simulate Turing graphs G1 and G2: Machine computations \nwith exponential space bounds, [Lipton, 19751 has shown coverability of Petri net markings is EXP-SPACE \nhard. By Theorem 3.2, we have that Corollary 3.1: Reachability in the flow model with static communication \nis EXP-SPACE hard. It is also straightforward to show: Theorem 3.3: Reachability with static communi\u00adcation \nis log-space reducible to coverability in Petri nets. 4. Event Spanning Graphs A message link is a pair \nof TRANSMIT, RECEIVE statements which communicate through the same channel. Throughout this section we \nassume static communication; the channel arguments to TRANSMIT and RECEIVE statements are assumed constant. \nThus, we may statically determine the set ML of al 1 message 1 inks. (Since the number of message links \nmay be quadratic In size of the process flow graphs, for efficiency the set ML is never explicitly constructed \nby our algorithm). Fix Gr = (Nr, Er, Sr) as the Gl= (Nl, El> sl)> .-., process flow graphs. Let N = uNi \nbe the set of program statements. Let an event spannin~ graph be an acyclic directed graph ESG such that \n(1) For each i=l, . . . . r the subgraph ESGi of ESG, induced by dropping all nodes but those of Ni and \ndeleting all edges except those between nodes of Ni, is a spanning tree of Gi . (2) For each RECEIVE \nstatement n . N there is a path in ESG from a start node to n, containing a TRANSMIT statement communicating \nover the same channel as n.  ~@ii=@s2= 0 = RANSMIT(C ,X ml = Y+RECEIVE(C1  1A CL. ... . n2 =RECEIVE(C \n) m2 ++ &#38;  The corresponding portion of an event spanning graph ESG is: 1 J. First, we characterize \nthe case where there exists no event spanning graph. A et B=N f RECEIVE statements is a blocking set \nif for each TRANSMIT statementmcN with the same channel argument as an element of NE, all execution paths \nfrom a start node to m contain some element of NB. Example 4.2: The set B = {nl,ml} is a blocking set \nfor the program flow graphs illustrated below: start: sl =\u00ad nl= RECEIVE (c ) d n2= TRANSMIT (C2) b It \nis easy to show Lemma 4.1: There is a nonempty blocking set NB, just in the case there is no event spanning \ngraph. 262 Theorem 4.1: If there is no event spanning graph, then some state nsN is unreachable. Proof: \nBy Lemma 4.1, there must be a nonempty blocking set NB. Suppose some ncNB is reached in some exe\u00ad cution \n(:,+), and no other element of NB is reached strictly before n. Since n is a RECEIVE statement, by definition \nof ESG there must be an execution of a TRANSMIT statement m over the same channel as n, previous to the \nfirst execution of n. This implies that in the flow graph Gi containing m, there is a path from the start \nnode of Gi to m which contains no element of NB, contradicting the assumption that NB is a blocking set. \nD We now present an algorithm for constructing either an event spanning graph or a blocking set. Algorithm \nA INPUT Process flow graphs G1 = (NI,E1,sI), . . . . Gr=(Nr,Er,sr) with program statements N =UNi and \nmessage channel set C. OUTPUT An event spanning graph ESG, if it exists, and otherwise a nonempty blocking \nset NB. !x.$.@l VISIT-Q+@; for each ngN do . If n is a s~rt node then w add n to VISIT-O: VISIT(n) \n+true; Q, else VISIT(n) +false; for each channel CEC do !K211! SILENCE(c) +true; WAITING(c)+~; e@; \nInitialize the digraph ESG to node set {sl,. ... Sr} and edge set ~: until VISIT-Q = !ZJdo  E?.!2u. \nchoose and delete some statement n eVISIT-Q; VISIT-NEXT+J3; if n is a TRANSMIT statement over a channel \nc and SILENCE(c) then begin SILENCE(c) +false; . VISIT-NEXT+WAITING(C); WAITING(c) +f J; ~; if n is a \nRECEIVE statement over a channel c and SILENCE(c) then add n to WAITING(c); else for each successor \nm of n such that (not VISIT(m)) do add m to VISIT-NEXT; fo~~ach mc VISIT-NEXT&#38;o &#38;!m if not VISIT(m) \nthen m add m to the node setof ESG:, VISIT(m) -+true;\u00ad end: add~to VISIT-Q; add (n,m) to the edge set \nof ESG; ~; encJ; NB+@; for each channel C. CC&#38; NB+NBU WAITING(C); U NB=@ then return event spanning \ngraph ESG; .~= blocking set NB; etlg; In the above algorithm, a node nEN has been visited if VISIT(n) \nhas been set to true. It is easy to verify that for each channel CSC, if no transmit statement over c \nhas been visited, then (1) SILENCE(c) ~ (2) WAITING(c) contains all RECEIVE state\u00ad  ments over channel \nc so far visited. Otherwise, when a TRANSMIT statement over c is visited then (1 ) SILENCE(c) is set \nto false (2 ) All RECEIVE statements in WAITING(c) are added to ACTIVE, and WAITING(C) is set to !3. \n Theorem 5.1: If an event spanning graph exists then the above algorithm returns one, else it returns \na nonempty blocking set. Proof: It is easy to verify that if the algorithm returns a digraph, then it \nis an event spanning graph. On the other hand, suppose the algorithm returns the set NB=C:CWAITING(C). \nWe claim this is a blocking set. Suppose not. Then there exists a RECEIVE statement nsNB, and a TRANSMIT \nstatement m communicating over the same charm c as n, and a path p in a flow graph Gi from,a start node \nSi to m and avoiding all elements of NB. In the above algorithm Si is initially added to VISIT-Q. It \nis easy to show that all other elements of p are also eventually added to VISIT-Q including m. Thus WAITING(c) \nis eventually set to fl, a contradiction with the assumption that nsNB.tl 5. Data Flow Analysis of Communicating \nProcesses with Static Communication Data flow analysis yields information about a program. This information \nis too weak to suffice for program correctness proofs; however, it is sufficient for the usual compile \ntime progran optimizations. For example, it may be discovered that certain program variables or express-ions \nalways evaluate to constants; hence the compiler may substitute single load instructions for more complex \nsequences of instructions which compute the same constant value. We wish to extend data flow analysis \ntechniques to the analysis of concurrent programs. ([ Hecht, 1977] is an infor\u00ad mative text on data flow \nanalysis of sequential programs, ) Let D be a set of predicates. !de assume a semi-lattice (D,v), where \nV:D2+D is the usual lattice meet operation, which is binary, asso\u00ad ciative, commutative, and idem potent \non D. We require V to be the following weakening of logical disconjunction: for each p, qsD, if p orq \nhold, then pvq holds. The lattice partial order => is defined: q > P ifandonly ifpvq = p for all p, q&#38;D. \nNote that => is the restriction of logical implication to domain D; q > P just in the case q implies \np, for all p, qeD. We assume D contains TRUE, FALSE as minimum and maximum values, respectively, so (p \n=> TRUE) and (FALSE => p) for all pcD. We also assume that (D,V) is well founded, containing no infinite \nstrictly decreasing chains P1=>P2 =>... . Similar data flow analysis frameworks appear in [Graham and \nWegman, 1977; Hecht, 1977; Wegbreit, 1975].  Example 5.1: We consider a domain D1 containing finite \nsets of inclusion relationships of the form: XES where X is a variable and S is a nonempty set of constants. \nWe assume that the sets of inclusion relationships contained in D are normalized, I containing no more \nthan one inclusion relationship for any variable, and no more than a fixed number kO of constants in \nany inclusion relationship. The domain also contains FALSE (representing inconsistent inclusion relationships \nof the form XC{ }) and TRUE (considered the empty set 0). Given p,qsD1, the meet operation VI combines \nsets p and q, and normalizes the results. The relation p =>Iq holds for p,qsD1 if for each inclusion \nrelationship XSS of q there is a corresponding inclusion relationship X&#38;S of p where SZS . Since \neach element of D1 is a finite set and each inclusion relationship is of bounded size, the semilattice \n(D1,V1) is well\u00ad founded. (end of Example 5.1) A function f: D+D is isotone if f(p)=> f(p ) for all \np,P ED, such that p =>p . Similarly g: D2+D is isotone ifg(p,q) => g(p ,q ) for all p,p , q,q cD such \nthat p => p and q=>q . If a program statement is neither a TRANSMIT or RECEIVE statement, then we assume \na isotone function (the transfer function of n) An: D+D weakly describing the change of state on More \nprecisely execution of program statement n. if psD holds just before execution of program statement \nn, then An(p) holds on exit from n. ( An(p) need not be the strongest such predicate, as would be required \nin a Hoare Logic.) Example 5.2: A X+y+x({xs{l>z}, YE{3}}) = {XE{4,5}, YE{3}}. That is, if X is known \nto be either 1 or 2 and Y is known to be 3, then after execution of the state\u00adment X+Y+X, the variable \nX may be either 4 or 5. We associate with each TRANSMIT statement n an isotone function ~n: D+D such \nthat is p: D holds on input to n, then Tn(p) iS a predicate describing the value of the message transmitted \nby n. A symbol Mc is used to represent the collection of all messages transmitted over channel c. Example \n5.3: If n = TRANSMIT(C,X*Y) =, hen Tn({XE {2,3}, Ys{4}}) = {MC C{8,12}}. 264 ( For each RECEIVE statement \nn, we assume a 1P = TRUE an isotone function pn:D2+D such that if pcD holds just on input to n and qcD \nholds for each y+l 9 message received by n, then on(p,q) holds on output to statement n. a={y 1 2 \n1:;J: :Wn ;g::tatement , YC{5}},{MCC{3,41})={XS{3,4}, Yc{5}}.  w={y {2 10]: The objective of our data \nflow analysis is Recall that an event spanning graph of the to compute for each program statement n&#38;N \nthe last section is acyclic and has node set N, the predicates IPn, OPnSD where: set of program statements. \nA topological (1) IPn holds an input to n on all executions. ordering of an acyclic directed graph is \na total (2) OPn holds an output from n on all ordering of its node set ordering predecessors executions. \nbefore successors. A topological ordering may be Let C be the set of channels occurring as argument easily \ncomputed in linear time as described in to TRANSMIT and RECEIVE statements. For each [Knuth, 19681. channel \nCSC we wish also to compute MPCED, a We now present an algorithm which repeatedly predicate holding for \nall messages transmitted computes approximations to the above data flow over channel c. analysis equations. \nIn each pass, the algorithm Specifically, we wish minimal visits each node in N in topological order \nof an IPn,OPnc D for each program statement nE N, event spanning graph. and minimal MPCS D for each channel \ncs C Algorithm B satisfying: INPUT Program statements N, channel set C, (1) IPn = TRUE if n is a start \nnode process flow graphs G1,. ..,Gr, an event = VOPm otherwise spanning graph ESG, and the functions \nAn, all predecessors m of n. Tn, On defined for each appropriate neN. (2) OPn = IPn if n is a TRANSMIT \nstatement  OUTPUT Minimal IPn, OPn, MPC, for nsN, = pn(IPn,MPc) if n is a RECEIVE CSC satisfying the \ndata flow analysis equations statement over channel c (1), (2), and (3). = An(IPn) otherwise. !L%!preach \nncN do (3) MPC ~in(IPn). !2sQ.!l all TRANSMIT statements nsN over channel c. if n is a start node then \nIPn+TRUE  Example 5.5: Illustrated below is an example else IP~+FALSE; containing maximal solutions \nto the data flow OPn+FALSE; equations in the case of the domain of inclusion etlJ; e for each CEC do \nNPC+FALSE;  relations: compute a topological ordering of ESG; 1P= TRUE until no change do for each \nneN in the topological orderin9 of ESG&#38; !?s4Lm for each predecessor m of n do IPn+IPnvOPm; if n \nis a TRANSMIT statement then M let c be the channel over &#38;=={xE{1Y51}MPc~{Mc:{5} which n transmits; \nMpc+MPcVTn(IPn); OPn+IPn; end i else if n is a RECEIVE statement then m let c be the channel through \nwhich n communicates; OPn+pn(IPn,MPc); end else OPn+-An(IPn); erKJ; end;  Let ML be the set of all message \nlinks. Let E be the set of all the edges of the process flow graphs Gl, . . . . Gr. Clearly, each iteration \nof the algorithm requires O(IN{ + IEl + \\ML\\) operations. In the worst case, this algorithm converges \nafter INIA iterations, where a is the longest strictly decreasing chain of (D,V). Thus the total time \ncost is O(INI(INI + IEI + IMLI) a). (There is evidence, however, that the algorithm converges much faster \nin practice. ) On convergence, the data flow analysis equations (1), (2), and (3) are clearly satisfied, \nand we can show Theorem 5.1: Algorithm B yields a minimal solution to the data flow analysis equations \n(l), (2), and (3). 6. Data Flow Analysis in the Case of Dynamic Communlcatlon In the previous section \nwe assumed that all message arguments of TRANSMIT and RECEIVE statements are constants. We to further \nare able refine our data flow analysis techniques to the case of a dynamic communication: where channel \n the arguments of the communication primitives are expressions which must evaluate to channels but not \nnecessarily the same channel on all executions. Our analysis is made more difficult by the fact that \nthe messages communicated between processes may be channel names. For example, a given process may inform \nother processes of new channels over which they may communicate. Let C be the set of all channels over \nwhich processes might communicate. Let (D,V) be a semilattice with ordering => and let Tn, Pn, and An \nbe the isotone functions describing transformations of predicates in D through TRANSMIT, RECEIVE, and \nnon-communication statements, respectively as defined in the last section. We also assume for each TRANSMIT \nor RECEIVE statement nEN the isotone function CHANNELSnmapping from D to the power set of C. Approximation \nis an essential technique in global flow analysis. Here we use CHANNELSnto approximate the possible channels \nover which n may communicate. More formally, given a predicate peD weakly describing the state just before \nexecution of statement n, Channels must con\u00ad tain at least all channels over which n may communicate. \nBy isotone, we mean here that if P > q then CHANNELSn(p) ~CHANNELSn(q) for all p,qeD. Example 6.1: In \nthe domain D1 of inclusion relationships defined in the last section, CHANNELSRECEIVE(Y)(YE{C1,C2})={C1 \n,C21 We seek minimal IPn, OPn, MPcsD for all program statements nsN and channels csC satisfying: (l ) \nIPn = TRUE if n is a start node = vOPm otherwise. all predecessors m of n. (2 ) OPn = IPn if n is a TRANSMIT \nstatement VPn(Ipn,Mpc) if n is a RECEIVE statement all csCHANNELSn(IPn). = An(IPn) otherwise. (3 ) MPC \n= VTn(IPn) all TRANSMIT statements with cc CHANNELSn(IPn) . For any solution of the above data flow \nequations, IPn and OPn are predicates in D holding on input and output, respectively, to program statement \nnsN, and MPC holds for all messages transmitted over channel CZC. Note that the above equations differ \nfrom those given in the last section for static communication, only in that we use CHANNELSn(IPn) to \nestimate the channels over which each TRANSMIT or RECEIVE statement n may communicate. The following \nalgorithm yields a ninimal solution to the equations (l ), (2 ), and (3 ) as long as the data flow domain \n(D,V) has no infinite strictly decreasing chains. Our algorithm combines Algorithms A and B of the end; \n previous sections, building an event spanning graph ESG while simultaneously carrying out data flow \nanalysis. As in Algorithm B, the event spanning graph ESG is used to order the nodes through which we \npropagate data flow information. Algorithm C INPUT The channel set C, process flow graphs G1,=(N1,E1>s1), \n. . ..Gr=(Nr.Er,sr) with the set of program statements N=uNi, and the functions Tn, Pn, An and CHANNELSndefined \nfor each appropriate ncN. OUTPUT Blocking set NB, or event spanning graph ESG with minimal Ipn, Opn, \nMpc for n~N and CEC satisfying the data flow analysis equations (l ), (2 ), and (3 ). +Ocedure p~opAGAT~(n); \nbegin VISIT-NEXT+fJ; for each predecessor m of n do IPn+IPnVOpm;  if n is a TRANSMIT statement then \n begin OPn+IPn; for each cc CHANNELSn(Ipn) S&#38; w MPc+MPcVTn(Ipn); if WAITING(C) # O = !!?@!. for \neach ms WAITING(c) such that WAIT(m) S!I?  !EKu.E. WAIT(m)+~, add mto VISIT-NEXT; end; WAITING(C) +fJ; \nE?@; end; end; else ifi is a RECEIVE statement then ____( ~ WAIT(n) then _ for each C: CHANNELSn(Ipn) \n&#38; add n to WAITING(C); else for each cECHANNELSn(Ipn) ~ opn+OPnV pn(ipn,Mpc)); else OPn+An(Ipn); \nif not WAIT(n) !!!2 for each successor m of n such that (not VISIT(M)) ~ add m to VISIT-NEXT; for each \nms VISIT-NEXT&#38; WnotvlslT(m) then qd m to the node set of ESG; VISIT(m)+~, add~to VISIT-Q; add (n,m) \nto the edge set of ESG; end; end; INITIALIZE: VISIT-Q+fJ; ~ncNQ if n is a start node then m VISIT(n)+~; \nIPn+TRUE; OPn+FALSE; add n to VISIT-Q; end: \u00ad else !2S@.11 VISIT(n) +false; Ipn+OPn+ FALSE; if n is \na RECEIVE statement then WAIT(n) +true; else WAIT(n) +false; ~; for each channel CCC do  J?!21Jl WAITING(C)+~; \nMPC+FALSE; ~; Initialize digraph ESG to node set {sl, . . ..sr} and edge set { }; MAIN: until no change \ndo w until VISIT-Q = 0 do  W choose and delete some ne VISIT-Q; PROPAGATE(n); eng; for each node n of \nESG in topological order do PROPAGATE(n); g; NB+@; for each cc Cc&#38; NB+NBU WAITING(c); = O then \nreturn blocking set NB; U B else return ESG and IPn,OPn, MPC for each n&#38;N and CCC; etlJ; A proof \nof algorithm C will appear in the full version of this paper. 7. Conclusion The data flow analysis algorithms \npresented in this paper are currently being implemented in a program analysis system for a language Zeno \n[.Ball et al. 1978] with concurrent communicating processes. We are developing direct (non\u00aditerative) \ndata flow analysis algorithms which run very efficiently in the case the process s programs and their \ncommunications are well structured. Bibliography [1] Ball, J.E., Williams, G.J. and Low, J.R., Preliminary \nZENO language description , TR41, Computer Science Department, University of Rochester, October 1978. \n [21 Graham, S. and Wegman, M., A fast and usually linear algorithm for global flow analysis , ~, 23, \nNo. 1, (January, 1977), pp. 172-202. [3] Feldman, J.A., A programming methodology for distributed computing \n(among other things) , TR9, Computer Science, Dept., University of Rochester, Rochester, New York, (1976). \n [41 Hanson, P.B., Concurrent programming concepts , Computing Surveys, Vol. 5, No. 4 (Dec. 1973), pp. \n223-245. [51 Hecht, M.S., Data Flow Analysis of Computer Pro rams, American Elsevier, New York * [61 \nHecht, M.S. and Unman, J.D., Analysis of a simple algorithm for global flow problems , SIAM J. of Computing, \nVol. 4, No. 4 (Dec. 1975T~. 519-532. [71 Hoare, C.A.R., Communicating sequential processes , Computer \nScience Dept., Queen s University, Belfast (March, 1977). [81 Jones, N.D., Landweber, C.H., and Lien, \nY.E., Com~exitv of some problems in Petri nets Theo~etic~l Computer Science, No. 4 (1977), pp. 277-309. \n[91 Kam, J.B. and Unman, J.D., Monotone data flow analysis frameworks , TR167, Computer Science Dept., \nPrinceton University (Jan. 1976). [101 Killdall, G.A., A unified approach to program optimization , \nProc. ACM, Symp. on Principles of %7ji-aiiiiiiTng Languages (Jan. 1975), pp. 10-21. [11] Knuth, D.E., \nThe Art of Computer Programming, Vol. 1: Fundamental Al orithms, Addison-Wesley, Reading, Mask [111 Knuth, \nD. E., The Art of Computer Programming, Vol l:Fundamental ~gorithms, Addison- Wesley, Reading, Mass. \n(1968). [121 Lipton, R., The reachability problem and boundedness problem for Petri nets and exponential-space \nhard , Conf. on Petri nets and Related Methods, M.I.T. (July, 1975); also Yale Research Report #62, (1976). \n [13] Minsky, M., Recursive unsolvability of Post s Problem , Ann. of Math., 74 (1961), pp. 437-454. \n[14] Peterson, M.L., Petri nets, Computing Surveys, Vol. 9, No. 3 (Se~, pp. 223-252 [15] Tarjan, R.E., \nDepth-first search and linear graph algorithms , SIAM J. of Computing, Vol. 1, No. 2 (June 1972~~. 46-100. \n[161 Wegbreit, B., Property extraction in well\u00adfounded property sets , IEEE Trans. on Software Engg. \n1, 3, 1975, pp. 270-285. APPENDIX Graph-Theoretic Definitions A directed graph (N,E) consists of a finite \nset N of nodes and a set E of ordered pairs (n,m) of distinct nodes, called edges. If (n,m) is an edge, \nm is a successor of n and n is a predecessor of m. A graph (N ,E ) is a subgraph of (N,E) if N c=N and \nE cE. A@of length k from n to m is a sequence of nodes (n = no,nl, . . . . m) such that (ni,ni+l) SE \nfor O~i< k. k The path is simple if no, . . . . nk are distinct (except possibly no = nk) and the path \nis a cycle if no = nk. A graph is acyclic if it contains no cycles. A directed graph (N1uN2, E) is bipartite \nif N~, N2 are disjoint and E5(N~x N2)u(N2x N1). A flow graph G = (N,E,s) is a directed graph (N, E) with \na distinguished start node s such that for any node ncN -{s} there is a path from s to n. A@l_CS!@ rooted) \n!2X?!2T = (N>EJS) is a flow graph such that IEl = INI-1. The start node s is the root of the tree. Any \ntree is acyclic, and if n is any node in a tree T, there is a unique path from s to n. If G = (ti,E,s) \nis a flow graph and T = (N ,E ,s ) is a tree such that (N ,E ) is a subgraph of Ganci N=N and s=s , then \nT is a spanning tree of G. 268   \n\t\t\t", "proc_id": "567752", "abstract": "Data flow analysis is a technique essential to the compile-time optimization of computer programs, wherein facts relevant to program optimizations are discovered by the global propagation of facts obvious locally.This paper extends flow analysis techniques developed for sequential programs to the analysis of communicating, concurrent processes.", "authors": [{"name": "John H. Reif", "author_profile_id": "81100567232", "affiliation": "University of Rochester, Rochester, New York", "person_id": "PP14196622", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/567752.567777", "year": "1979", "article_id": "567777", "conference": "POPL", "title": "Data flow analysis of communicating processes", "url": "http://dl.acm.org/citation.cfm?id=567777"}