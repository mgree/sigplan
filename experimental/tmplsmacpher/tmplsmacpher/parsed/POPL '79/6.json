{"article_publication_date": "01-01-1979", "fulltext": "\n Permission to make digital or hard copies of part or all of this work or personal or classroom use is \ngranted without fee provided that copies are not made or distributed for profit or commercial advantage \nand that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, \nto post on servers, or to redistribute to lists, requires prior specific permission and/or a fee.&#38;#169; \n1979 ACM 0-12345-678-9 $5.00 FIRST ORDER PROGRAMMING LOGIC Robert Cartwright Computer Science Department \nCornell University John McCarthy ComDuter Science Department Stanford Univer~ity Abstract First Order \nProgramming Logic is a simple, yet powerful formal system for reasoning about recursive programs. In \nits simplest form, it has one major limi\u00adtation: it cannot establish any prcperty of the least fixed \npoint of a recursive program which is false for some other fixed point. To rectify this weakness, we \npresent two intuitively distinct ap\u00adproaches to strengthening First Order Pro\u00adgramming Logic and prove \nthat either ex\u00adtension makes the logic relatively com\u00adplete. In the process, we prove that the t Wo approaches \nare formally equivalent. The relative completeness of the extended logic is significant because it suggests \nit can establish all Tordinarytt properties (obviously we cannot escape the Godelian incompleteness inherent \nin any programming logic) of recursive programs including those which compute partial functions. The \nsecond contribution of this paper is to establish that First Order Program\u00adming Logic is applicable to \niterative pro\u00adgrams as well. In particular, we show that the intermittent assertions method--an informal \nproof method for iterative programs which has not been formalized--is conveniently formalized simply \nas sugared First Order Programming Log ic applied to the recursive transla\u00adtions of iterative programs. \n1. Introduction . Many theoretical computer scientists (e.g. Hitchcock and Park [19731) have dismissed \nfirst order logic as too weak a formalism for reasoning about recursively defined functions. Nevertheless, \nthe au\u00adthors [Cartwright 76a; Cartwright 76b; McCarthy 781 have demonstrated that first order logic can \nserve as a powerful, yet convenient formal system for establishing properties of recursively defi$ed \nfunc\u00adtions. The key idea underlying our formal system is that recursive definitions of partial functions \ncan be interpreted as equations extending a first order theory of the program data domain. The resulting \nprogramming logic (henceforth called First Order Programming Logic) is very simple and convenient to \nuse; yet, it is suffi\u00adciently powerful to prove most theorems of practical interest about recursively \nde\u00adfined functions. -In fact , First Order Programming Logic seems to be the logic of choice for reasoning \nabout recursive pro\u00adgrams. The most recent incarnation of the Boyer-Moore LISP verifier [Boyer and Moore \n75] is based on a restricted variant of First Order Programming Logic. From a theoretical viewpoint, \nthe ma\u00adjor weakness of ordinary First Order Pro\u00adgramming Logic is its inability to prove true statements \nabout the least fixed point solution of a recursive program 36, 381 used to prove the consistency of \nwhich are false for some other fixed point solution. For recursive programs which compute total functions \nthis limitation is irrelevant, since the least fixed point is the only fixed point. For example, prov\u00ading \nthat a particular recursive program computes a total function (e.g. Ackermann s function) is remarkably \nsimple in most cases. Nevertheless, the few re\u00adcursive programs encountered in practice which do not \ncompute total functions (e.g. interpreters) may have simple properties which cannot be established within \nordi\u00adnary First Order Programming Logic. To remove this limitation, the authors [Cart\u00ad wright 78; McCarthy \n78] have independently developed extensions to First Order Pro\u00ad gramming Logic which use different tech\u00ad \nniques to capture the concept of least fixed-point. In this paper, we prove that either extension to \nFirst Order Programming Logic makes the logic relatively complete with respect to the theory of the underlying \ndata domain. In the process, we prove that the t Wo extensions are inter\u00adderivable, i.e. that either \nextension can be formally derived from the other. The relative completeness of the extended log\u00adic is \nsignificant because it indicates that the logic can prove all tordinarytf properties of the partial functions \ncom\u00adputed by recursive programs. Of course, the extended logic cannot escape the essential Goedelian \nincompleteness of any non-trival programming logic. But the fact that the extended logic is relatively \ncom\u00adplete demonstrates that the logic com\u00adpletely captures the semantics of recur\u00adsive programs--that \nthe only source of in\u00adcompleteness in the logic lies in the first order axiomatization of the underly\u00ading \ndata domain . Fortunately, we can overcome the incompleteness of the data domain axiomatization when \nnecessary by following the same approach that [Gentzen Peano arithmetic (Peano s Axioms); we sim\u00ad ply \naugment the axiomatization of the data domain when necessary by the appropriate transfinite induction \naxiom. An interesting consequence of the inter-derivability of the two extensions to ordinary First Order \nProgramming Logic is that both extensions can be added to the unadorned logic without interference. In \npractice, First Order Programming Logic with both extensions is probably more con\u00ad venient to use than \nit is with either ex\u00adtension alone. The second contribution of this paper is to show that First Order \nProgramming Logic is applicable to iterative programs (e.g. PASCAL programs) as well. Recently, the intermittent \nassertions method developed by [Burstall 1974] and [Manna and Waldinger 19781 has attracted consid\u00aderable \nattention as a possible alternative to the standard inductive assertions method for reasoning about iterative \npro\u00adgrams. However, all of the published descriptions of the method have been very informal. In this \npaper, we formalize the intermittent assertions method by showing that it can be interpreted simply as \nsugared First Order Programming Logic ap\u00adplied to the recursive translations of iterative programs. 2. \nRecursive Programs An adequate background for this paper is contained in [Enderton 1972] and either [Manna \n19741 or [Manna, Ness and Vuillemin 19731. Before we can formally define the concept of a recursive ~rogram, \nwe must introduce some preliminary definitions. A data domain D consists of a set ID{ of data objects \n(called the universe) togeth\u00ader with a set of primitive operations (functions) gl ,..., .gk on IDI. Constants \nare treated as O-ary operations. While all our results easily generalize to typed 3. The primitive operations \ninclude (sorted) data domains, we will restrict the standard binary equality function ourselves to typeless \n(unsorted) data equal and the Boolean characteristic func\u00addomains for the sake of notational and tions \ni SD t and isDf for the sets lDlt and -  conceptual simplicity. Consequently, we IDI f respectively. \nimpose the restriction that all primitive To facilitate writing recursive de\u00ad operations must be total \nfunctions on the finitions, we also assume that D has the universe ID! (of the appropriate arity). following \nproperties: To accommodate Boolean operations 1. The primitive operations include within D, we can represent \nthe Boolean the standard ternary conditional expres\u00ad values TRUE and FALSE by ordinary data ob\u00adsion operator \nif-then-else defined as fol\u00ad lows : jects in ID]. Since all operations must be total on IDI, we must \nassociate a Boolean value with every data object in ! y if isDt(x) IDI. The programming language LISP \nfollows if x then y else z = i zif isDf(x) !this convention. In LISP, the data domain consists of the \nsingle type S-expression. 2. The universe D! forms a well-The atom NIL represents the Boolean value founded \npartial order. ng under the primi-FALSE; every other S-expression represents tive binary Boolean operation \nless. the Boolean value TRUE. The atom T serves as the nominal representative of the set Let L be a first. \norder language with of objects denoting TRUE; most Boolean equality for the program data domain D, operations \n(e.g. EQUAL) always use T to and let ~ denote a sequence of distinct represent a TRUE result. variables \n. . ..x of L. A recursive program on the data domain D consists of a To enforce these conventions, we \nas\u00ad 1 m set of function definitions of the form sume that every data domain D has the fol\u00adlowing properties: \nfi(;i) <\u00adti[fl, . . ..fnl(Ii). i=l, . . ..n. 1. The universe ~D~ is partitioned into two non-empty disjoint \nsets ~D~t and fn are function symbols not here 1  :D; Primitive operation requiring in L and ti[fl, \n. . ..f nl(~i) is a term in L f Boolean arguments must interpret objects (extended to include the new \nfunction sym\u00ad in }Dlt (lDlf) as the Boolean value TRUE f ) with free variables ~i. bolsfl  n (FALSE). \nFor example, a recursive program on For example, in LISP lD~t and ID; are f the natural numbers N which \ncomputes the {xIxfNIL} and {NIL} respectively. factorial function is 2. The set of primitive operations \n(1) fact(n) <-if n equal O then 1 includes the constant symbols true and else nxfact(n-l) . false denoting \nthe nominal representatives of the Boolean values TRUE and FALSE where we have used infix notation for \nthe respectively. binary operators equal and *. Obviously, true and false must belong to the sets IDI \nt and ;D; ~ respectively. In the sequel, we assume without loss Boolean valued operations such as isDt \nof generality that every recursive program must return either true or false. P consists of a single recursive \ndefini\u00adtion. All of our results easily general\u00ad ize to recursive programs with an arbi-, :D+; = :D~ u \nL (a function f an ~D+~ is trary number of mutually recursive defini\u00adtions. 3. The Functional Equation \nfor a Recur\u00adsive Program. Assume we are given a first order ax\u00adiomatization AD for the data domain D \nsuch that AD includes a structural induction ax iom schema (e.g. Peano s axioms for the natural numbers \nN). To formally state and prove theorems about the function f de\u00adfined by the recursive program (2) f(x) \n<-t[fl(x) we would like to extend D and its corresponding axiomatization AD to include the defined operation \nf by converting the recursive definition (2) into the the de\u00ad fining axiom (3) vi [f(I)t[fl(~)l and \ninterpreting f in D as the function computed by evaluating (2). Unfortunate\u00adly, this naive approach does \nnot work, be\u00adcause the function defined by (2) may not be total. In many cases, the proposed ax\u00adiom (3) \nis inconsistent with AD. A simple example illustrating this problem is the recursive program f(x) <-f(x) \n+ 1 over the natural numbers N. The corresponding first order axiom is obvi\u00adously inconsistent with Peano \ns axioms. We can salvage our basic approach to axiomatizing recursive programs by enlarg\u00ading the data \ndomain D to explicitly in\u00adclude the undefined element L ( bottom ). We construct the augmented data domain \nD+ from D by adding L to the universe ID! and appropriately extending all the opera\u00adtions of D. With \nthe exception of if\u00adthen-else, we extend every operation of D to the corresponding strict function on \nThe corresponding modification to strict iff f has the value 1 if any of its arguments is 1). We extend \nif-then-else to ~D+l as follows: # ,, ii if p . then d, else d2 = ~ dl if ~~t(p) D e\u00adquired to form \nthe axiomatization AD+ for D+ is a simple mechanical construction. The details are left to the reader. \nFor notational convenience, we let x:D abbreviate the formula X*J. Intui\u00adtively, x:D means that x is \ndefined, i.e. that it belongs to the proper universe ,D;.I Given the augmented domain D+ and the corresponding \naxiomatization AD+, we can successfully convert recursive programs on D into first order axioms augmenting \nAD+ as suggested above. However, before proceeding further, we need to clarify what we mean by evaluating \nrecursive programs. Which partial function is defined by a recursive program P (such as (l)) depends \non whether a call-by-name or .. call-by-value computation rule (in the terminology of Manna, Ness and \nVuillemin [19731) is used to evaluate the program. From the standpoint of : denotational se\u00admantics, \nthe function computed by applying a call-by-name (call-by-value) computation rule to P is the least fixed \npoint of the call-by-name (call-by-value) functional for P [deBakker 751. The call-by-name functional \ncorresponding to the recursive program (1) is simply: Ag .A7. t[gl(l) . The call-by-value functional \nis slightly more complex syntactically; it has the form : Ag . ix . _if f(~) then t[gl(~) else I. . \nwhere 1 ! true if Y:D 6(1) ~~ otherwise Fortunately, we can capture either meaning within first order \nlogic by con\u00adverting the recursive program into the ap\u00adpropriate first order axiom. The call\u00adby-name \n(call-by-value) interpretation for the partial function defined by an arbi\u00adtrary recursive program (2) \nsatisfies the first order sentence (4) VI [f(I)=m[fl (I) where Ag . h; .cx[gl(I) is the call-by-name \n(call-by-value) func\u00adtional corresponding to P. A formal proof that the call-by-value interpretation \nsa\u00adtisfies equation (4) appears in [Cart\u00adwright 76bl. The corresponding proof for the call-by-name interpretation \nis nearly identical. In the call-by-value case, sentence (4) is equivalent to the conjunction of the \ntwo simpler sentences: (4a) W~:D [f(~)=t[f] (~) In subsequent call-by-value examples, we will use these \naxioms in preference to equation (4). As an illustration of our approach to axiomatizing recursive programs, \nconsider the factorial program (1) presented as an example in the previous section. The call-by-name \naxiom 4) corresponding to the program is: k#n [fact(n) . ~ n equal O then 1 . else n*fact(n_l) # The \ncorresponding call-by-value axioms [(4a), (4b)] for the same program are: b%:N [fact(n) = ~ n equal O \n~ else n*fact(n-1) dn [7n:N -> fact(n) = ~1 . In practice, the call-by-value ax iom (4b) is rarely used, \nbecause it describes the behavior of f when it is applied to an undefined (divergent) argument. In the \nsequel, we will explicitly state which type of computation rule--call-by-name or call-by-value--that \neach program uses. 4. Reasoning about Recursive Programs in First Order Programming Logic. To reason \nabout a recursive program P, we simply append the first order sentence(s) characterizing P to the ax\u00adiomatization \nAD+ of the augmented data domain D+ and apply standard first order deduction. lie call the resulting \nformal system Weak First Order Programming Logic. For example, assume AD+ is an augmented Peano axiomatization \nfor the augmented na\u00adtural numbers N+ including the primitive functions +, -, *, equal, and less. Let \nthe Ackermann function ack be defined by the call-by-name recursive program on N ack(x,y) <-if x equal \nO then y . else ack(x-l, ack(x,y-1)) . The first order sentence correspond\u00ading to the preceding program \nis Vx y [ack(x,y) = if x equal Othen y _ else ack(x-l, ack(x,y-l ))) . We can prove that ack is total \nby proving the theorem b x Y [x:N &#38; Y:N -> ack(x,y):Nl . The proof proceeds by structural induction \non the pair (x,y) under the standard lexi-order programming logic to solve this cographic ordering. The \nbase case, x.O is trivial. For the induction step, we assume the induction hypothesis v x y [x less x \nI x =x &#38; y less y -> ack(x ,y ):N]. Since X*O, ack(x, y)=ack(x-l,ack( x,Y-l )). By hypothesis, ack(x,y-l):N. \nApplying the hypothesis a second time, we deduce ack(x-l, ack(x,y-l)):N. Q.E.D. 5. Capturing the Concept \nof Least Fixed Point. The functional equation corresponding to a recursive program does not completely \ncharacterize it in some cases. For ex am\u00adple, consider the call-by-name program on the natural numbers \n(5) Q(x) <-Q(x) and the corresponding first order sentence (6) b x Q(x) = Q(x). Although the program \n(5) clearly does not terminate for any x, the sentence (6) is satisfied by any interpretation for the \nfunction Q--not just the everywhere unde\u00ad fined function. The problem is that the first order sentence \ncorresponding to an arbitrary call-by-name (call-by-value) program P is satisfied by any fixed-point \nof the call-by-name (call-by-value) func\u00adtional for P. If the function computed by P is total, this ambiguity \ndoes not arise because the functional for P has a unique fixed point. For some programs which com\u00adpute \npartial functions, however, the pres\u00adence of fnon-standard~f fixed points prevents Weak First Order Programming \nLog\u00adic from proving any property of a function which is not true for all fixed points of the defining \nfunctional . The authors [Cartwright 1978; McCarthy 19781 have pro\u00ad posed different ways to augment weak \nfirst problem. [Cartwright 781 extends Weak First Order Programming Logic by introducing the concept \nof a complete recursive program--a . program whose functional has a unique fixed point. Given an arbitrary \nprogram P (computing f) on the (data domain D, it is possible to mechanically construct a corresponding \nprogram 1? (computing f ) on the extended domain Se:~p(D) (where Sexp(D) denotes the set of S--expressions \nover D) with the following properties: 1. P computes the computation se\u00adquence for P. 2. P is a complete \nrecursive pro\u00adgram.  The construction is described in detail in [Cartwright 781. Let last denote the \nstrict unary function on Sexp(D) which extracts the last element of a list (represented as an S-expression) \n. Then f satisfies the sen\u00ad tence (7) v x:D [last(f (x))=f(x)) . . Obviously, this sentence is not provable \nin general in Weak First Order Programming Logic, since it forces f to have a unique fixed point interpretation \n(with respect to a particular data domain model). Con-\u00adsequently, to fully characterize the re-\u00adcursive \nprogram P on the data domain D, [Cartwright 781 augments the axiomatiza-\u00adtion of Sexp(D)+ (constructed \nfrom AD) by the following three axioms describing P: 1. The recursion equatiorl corresponding to P. \n2. The recursion equatioul corresponding to Pt. 3. The equivalence axiom (7) assert\u00ading the equivalence \nof lastaft and f on D.  It is a straightforward exercise to The function loop defined by (8) is clear\u00ad \nconstruct Peano-like axiom atizations for ly diverges (equals i) everywhere, yet Sexp(D) and Sexp(D)+; \nfor one possible ap\u00ad this fact is not provable within Weak proach see [Cartwright 76a; 76bl Moreover, \nFirst Order Programming Logic since any under suitable assumptions (the hypothesis constant function \nis a fixed point of the of the relative completeness theorem corresponding call-by-value functional. \npresented in the next section) which any However, the divergence of loop is prov\u00ad plausible data domain \nsatisfies, it is able from either the equivalence axiom possible to implement (encode) S\u00ad (asserting \nthe equivalence between (8) and expressions over D as objects in D (using the corresponding complete \nrecursive pro\u00ad pairing functions) and avoid extending the gram) or the minimization sc hem a. The data \ndomain D altogether. proof from the equivalence axiom proceeds as follows. The complete recursive pro- \nIn contrast to the complete recursive gram corresponding to (8) is: program approach, [McCarthy 781 extends \nWeak First Order Programming Logic by ad\u00ad (9) loop (x) <\u00ad cons(x+l, loop (x+l)) . ding an axiom schema \n(called the minimization schema) for each recursive By the equivalence axiom (7) and the program P. The \nschema asserts that the strictness of last, proving that loop function computed by the program P is diverges \nreduces to showing less-defined or equal to any function (10) ~x:N [lOOP (X) = J] . which is a fixed \npoint of the same func\u00ad tional . Let kg . Ax . t[g](x) be the We can prove (10) by structural induction \nfunctional corresponding to the program on the value of loop (x). Assume (10) defining the function \nf. The minimization holds for all x such that x is a proper schema has the form tail of x. If X=1, \nthen the theorem holds. Otherwise, b x:D (t[G](x):D -> G(x) = t[Gl(x)) -> dx:D (f(x):D -> G(x)=f(x))o \nloop (x) = _cons(x+l, loop (x+l)) where G is an arbitrary function symbol. where The schema can be stated \nmore succintly by introducing Scottts partial ordering c on 100P (X+I )+1 D+ [Scott 701. We define (since \ncons is strict). But, by the in- VX y (Xgy <-> X=i ! X=Y). duction hypothesis, Using this notation, the \nminimization 100p (X+l)=J , schema becomes generating a contradiction. Q.E.D. b x:l) (t[G](x)~G(x)) \n-> dx:l) (f(x)c_G(x)). The derivation of the same theorem from the minimization schema is trivial. Let \nus illustrate and compare these Let the function g on N+ be defined by the t Wo extensions to Weak First \nOrder Pro\u00ad call-by-value program gramming Logic by consid~ring the follow\u00ad ing call-by-value program \nover the natural g(X) <- J. numbers N: The function g clearly satisfies the hy\u00ad (8) 1OOP(X) <\u00ad 1ooP(x+I). \npothesis of the minimization schema. Hence , implying that VX:N [loop (x)=J]. For simple theorems involving \npartial functions where suitable comparison func\u00adtions like g are easily constructed , the minimization \nschema tends to produce shorter, simpler proofs than the equivalence axiom. For more complicated examples, \nhowever, (such as interpreters) the equivalence axiom may be more useful, because the complete recursive \nprogram construction automatically generates the complex comparison function required to successfully \nutilize the minimization schema. The close relationship between com\u00adplete recursive programs and suitable \nminimization comparison functions G sug\u00adgests the following theorem: Theorem. The equivalence ax iom \nand the minimization schema corresponding to a call-by-value (call-by-name) recursive program P are inter-derivable. \nProof. In each direction, the proof is a rOUtine but tedious induction on the structure of the body of \nthe recursive program P. An outline of the proof ap\u00ad pears in an Appendix. Since either extension to \nWeak First Order Programming Logic can be derived from the other, we can safely extend the weak logic \nto include both extensions. We call the resulting system Strong First Order Programming Logic. 6. Relative \nCompleteness of Strong First Order Programming Logic. In Weak First Order Programming Log ic, there are \nsimple properties of recur\u00adsive progiams which we can express but cannot prove. Does the strong logic \nsuffer from the same weakness? Fortunate\u00adly, the answer is no. We can prove a re\u00adlative completeness \ntheorem which suggests tha t the strong logic is, for all practi\u00adcal purposes, as powerful a deductive \nsys\u00adtem for reasoning about recursive programs (excluding programs which take functions as arguments) \nas we can hope to devise. Informally, the theorem asserts that any sentence in the strong logic for a \nrecur\u00adsive program P logically reduces to a sen\u00adtence in the pure logic of the data domain. In other \nwords, the inevitable Godelian incompleteness of the strong log\u00adic lies entirely in the data domain ax\u00ad \niomatization, not in the axiomatization of the recursive program P. Before we can precisely state the \ntheorem, we need to introduce the follow\u00ading definitions. 1. An , interpretation a of a first order theory \nT in the language L into a theory (possible different) 1 n a language is a function which maps the 1 \nformulas of L into corresponding formulas of LI such that @[T] T1. A more precise definition of the concept \nappears in [Enderton 72, p. 1621. 2. Let S be an arbitrary set of sen\u00adtence in a first order language \nL. The set of consequences of T (denoted Cn S) is the set of formulas logically implied by T. Theorem. \nLet P be an arbitrary call-by-value (call-by-name) recursive program f(Y) <-t[fl(i) on the data domain \nD. If there is an in\u00adterpretation of Cn into Cn AD, Sexp(D) then any formula e in the language L u {f} \n(i.e. the language of the Sexp(D)+ strong logic for P) is provably equivalent (within the strong logic) \nto a formula in the language LD. Proof. From the standpoint of mathematical logic, First Order Program\u00ad \nming Logic provides a method for systemat\u00adically introducing partial functions de\u00adfinitions on a data \ndomain D into an ax\u00adiomatic theory Cn AD for D. In the termi\u00ad nology of [Enderton 721, this theorem sim\u00ad \nply asserts that , under suitable assump\u00ad tions, these definitions are eliminable. The key step in the \nproof is to con\u00adstruct a formula f3(~,y) in the language of Sexp(D) such that the sentence (11) V~:D \ny:D [e(~,y) <-> f(~)=y]. is provable in the strong logic for P. The construction for call-by-value pro\u00ad \ngrams is described in [McCarthy 781; the corresponding construction for call-by\u00ad name programs is similar \nbut slightly more complicated . Given an arbitrary formula Y in the language LSexP(D)+ u {f}, we can \nlogically reduce it to an equivalent formula Y in by replacing refer\u00ad he language Sexp(D) ences to f \nby references to 9 and subse\u00adquently performing a simple case analysis to eliminate references to 1. \nThen we can use cr to convert Y to an equivalent for\u00admula Y* in LD. Q.E.D. 7. Reasoning About Iterative \nPrograms in First Order Programming Logic. The intermittent assertions method developed by Burstall [1974] \nand Manna and Waldinger [19781 has been widely promoted as a better proof method than the standard inductive \nassertions method for reasoning about iterative programs. However, the descriptions of the method appearing \nin the literature have all been very infor\u00ad mal, making the relative merits of the method difficult to \nassess. In this paper we provide a simple formalization of the intermittent method within First Order \nProgramming Logic. In fact, we will demonstrate that intermittent assertions method can be viewed simply \nas convenient notation for proving theorems in first order programming logic about the recur\u00ad sive translations \nof iterative programs. The fundamental idea in the formali\u00ad zation is the interpretation of the state\u00ad \nment  (12) Sometime at L, Q(x,Y) where L is a node in the flowchart program (a directed graph with assignment \nstates and boolean tests attacted to the edges) and Q(~,7) is an arbitrary first order formula in the \nlanguage of the data domain with the free variable lists ~, ~ ranging over the program data domain D. \nThe vari\u00ad able lists ~ and 7 denote lists of program variables and arbitrary free variables, respectively. \nWe formalilize statement (12) within First Order Programming Logic for the program data domain D as \n(13) a ~:D [Q(~,y) &#38; fL(l)=fStart(~O))l .  where x ~ denotes the initial progam state and are recursive \nprograms L and Start equivalent to executing the flowchart starting at nodes L and Start respective\u00adly. \nOf course, more complex formaliza\u00adtion which explicitly mention labeled ex\u00adecution traces are possible. \nOur transla\u00adtion seems to be the simplest one which works. Informally, (13) asserts that there is a program \nstate ~ such that exe cuting the program from L in state = pro\u00adduces the same output as executing the \nprogram from Start in state x This o statement is weaker than the obvious (but more complex) translation \ninvolving the labeled execution trace (Zo) , or Start because the state mav no t occur at during the \nexecution of the program from Start in state z ~. In fact, L may be a node which is never reached by \nany execu\u00ad tion path beginning at Start. However, this apparent weakness does not diminish the deductive \npower of the intermittent assertions method; the logic is still re\u00adlatively complete with respect to \nthe theory of of the underlying data domain. Moreover, all of the arguments used in Manna and Waldinger \ns sample proofs hold for our interpretation of Isometimetf statements. In intuitive terms, our for\u00admalization \nof intermittent asserts permits non-standard program executions , but forces them to produce standard \nresults. Our interpretation of Isometime assertions does retain the crucial intui\u00ad tive property that \nan assertion of the form . Sometime Q(x,y) at Finish (where Finish is the termination node of the flowchart) \nimplies that the program terminates for input state ZO. Otherwise, the condition f Finish(x) = f Start(~O) \nwould be false since (x) .I Finish out where the output variable list Tout is a non-empty subset of the \nprogram variables z. Manna and Waldinger describe the input-output specifications for a program using \n! sometime!! formulas by stating (14) Sometime In(~,7) at Start -> Sometime Out(~,~) at Finish. Using \nour interpretation of sometime!! formulas, (14) becomes V?O:D Y:D [~z:D (In(~,~) &#38; f Start(x)=f Start \n(zO)) ->o~~:D (Out(~,F) &#38; f.. F ~n,sh(~)=fStart (~O))] which is equivalent to [In(x,y) -> f Started \n&#38; out(fstart(~))l . Given our translation of sometime formu\u00adlas into First Order Programming Logic, \nall of the lemmas and theorems appearing Manna and Waldinger s examples can be in\u00adterpreted simply as \nsentences in a first order programming logic for the program data domain. Similarly, their informal arguments \ncan be translated into ordinary first order programming logic proofs. As an illustration, we will show \nhow to for\u00admalize the tip-counting example described in [Manna and Waldinger 781. The recur\u00adsive translation \nof the tip-counting pro\u00adgram is: (tree) <-fMore(<tree>,O) Start f More(stack,count) <\u00adif stack equal \n<> then f Finish( stack,count) else if istip head stack then f More(tail stack,count+l) else f More([left \nhead stack] . [[right head stack] . [tail stack]]) fFinish (stack,count) <-count where we have used the \nsame primitive operation names and variable names as the original program. In the interest of clarity, \nwe have changed the notation slightly. First, we have omitted the parentheses enclosing unary function \nargu\u00adment lists. Sec6nd , we have used the ,no\u00ad tation > instead of (cl,...,en) eI  en to denote the \nlist consisting of the ele\u00ad ments el, . . ..en. To precisely define the number of tips in a tree, Manna \nand Waldinger pro\u00advide the following recursive program: tips (tree) <\u00adif istip(tree) then 1 else tips(left \ntree) + tips(right tree) . Given the definition of tips, they prove the following input-output specifications \nfor the tip-counting program: (16) If Sometime tree = t at Start, then Sometime count = tips(t) at Finish \n. (17) V tree:TREE  In our formalization, the statement (16) immediately reduces (by (15) and the sub\u00ad \nstitution of tree for t) to [f Start(tree) :INTEGER &#38; (tree) =tips(tree)l Start where :TREE and \n:INTEGER are postfix operators denoting characteristic predi\u00adcates for trees and integers respectively. \nThe key step in Manna and Waldinger s proof is proving the following lemma: (18) If Sometime count = \nc &#38; stack . t .sat More, then Sometime count . c + tips(t) &#38; stack . sat More . The formal translation \nof (18) reduces to (19) Vt:TREE s:LIST c:INTEGER (t.s,c) = f ~ore(s,c+tips(t)) More where :LIST is a \npostfix operator denoting the characteristic predicate for lists of trees. If we assume that tips(t) \nis de\u00ad fined for any tree t, it is very easy to prove (19) by structural induction on the value of [t \n. s]. Moreover, we can trivi\u00ad ally prove within First Order Programming Logic that tips(t) is defined \nfor any tree t, i.e. (20) v t:TREE tips(t):INTEGER , by induction on t. Manna and Waldinger implicitly \nutilize this lemma in their in\u00ad formal proof without mentioning it. Given the lemmas (19) and (20), and \nthe recursion equations corresponding to the tip-counting program, the input-output theorem (17) immediately \nfollows. Note that we formalized the tip\u00adcounting proof within Weak First Order Programming Logic; the \nextra power of the strong logic was not necessary. In fact, all of Manna and Waldinger s sample correctness \nproofs can be formalized within the weak logic. The weak logic suffices because totally correct iterative \nprograms correspond to total recursive programs. On the other hand, formalizing inter\u00admittent assertions \nproofs of partial correctness requires the strong logic. While Manna and Waldinger do not present any \nsample partial correctness proofs , they do show how to transform arbitrary inductive assertions (partial \ncorrectness) proofs into intermittent assertions proofs. The resulting proofs cannot be formalized within \nthe weak logic because they utilize induction on the length of the program computation sequence rather \nthan induction on the structure of the program data. We believe our formalization of the intermittent \nassertions method sheds some light on why the method produces surpris\u00adingly simple correctness proofs \nfor cer\u00adtain programs. If an iterative program has a particularly simple recursive trans\u00adlation, then \nthe intermittent assertions method , in essence, applies structural in\u00ad duction to the corresponding \nrecursive program. As [Boyer and Moore 75] and [Cartwright 76al have observed, it is much easier to reason \nabout naturally recursive programs using structural induction than it is using other methods (e.g. inductive \nassertions, fixed point induction). On the other hand, we doubt the in\u00adtermittent assertions method will \nfare as well when applied to typical iterative programs. Manna and Waldinger s choice of examples tends \nto support this conjecture; all of their sample programs have natural recursive translations. It hardly \nseems advantageous to uniformly convert non\u00adrecursive programs into corresponding re\u00adcursive programs \nin order to reason about them; yet this is precisely the course that the intermittent assertions method \nfollows. References Boyer, R. and J Moore. (1975): Proving Theorems about LISP Functions, J. ACM 22(l): \n129-144 (January). Burstall, R. (1974): Program Proving as Hand Simulation With a Little Induc\u00adtion , \nin Information Processing 74, pp. 308-312, North-Holland, Amsterdam. Cartwright, R. (1976a): User-Defined \nData Types as Aid to Verifying LISP Pro\u00adgrams, in S. Michelson and R. Milner (eds.), Automata Languages, \nand Pro\u00adgramming, pp. 228-256, Edinburgh Press, Edinburgh. Cartwright, R. (1976b): A Practical Formal \nSemantic Definition and Verification System for TYPED LISP, Stanford A. I. Lab. Memo AIM-296, Stanford \nUniversity, Stanford, California. Cartwright, R. (1978): First Order Seman\u00adtics: A Natural Programming \nLogic for Recursively Defined Functions, Cornell University Computer Science Dept. Tech. Report TR78-339, \nIthaca, New York. DeBakker, J. W. (1975): The Fixed Point Approach in Semantics: Theory and Ap\u00adplications, \nMathematical Centre Tracts 63, Free University, Amsterdam. Floyd, R. (1967): Assigning Meaning to Programs, \nin J. T. Schwarz (cd.), Proc. Symp. in Applied Math. vol. 19., pp. 19-32, Amer. Math. SoC., Providence, \nRhode Island. Gentzen, Gerhard. (1936): Die Widerspru\u00adchsfreiheit der reinen Zahlentheorie, Math. Annalen \n112: 493-565; English translation in M. E. Szabo (cd.), The Collected Works of Gerhard Gentzen, PP. 132-213, \nNorth Holland, Amsterdam, 1969. Gentzen, Gerhard. (1938): Neue Fassung des Widerspruchsfreiheitbeweiss \nfur die reine Zahlentheorie, Forschungen zur Log. U.Z. Grund. der exacten Wiss., New Series No. 4, Pp \n19-44; English trans\u00ad lation in M. E. Szabo (cd.), The COl\u00adlected Works of Gerhard Gentzen, pp. 252-286, \nNorth Holland, Amsterdam, 1969. Hitchcock, P. and D. Park (1973): Induc\u00adtion Rules and Proofs of Program \nTermi\u00adnation, in M. Nivat (cd.), Automata, Languages, and Programming, pp. 225-251, North-Holland, Amsterdam. \nHoare, C. A. R. (1969): An Axiomatic Basis of Computer Programming, Comm. ACM 12(10): 576-580 (October). \nManna, Z. (1974): Mathematical Theory of Computation, McGraw-Hill. Manna, Z., S. Ness, and J. Vuillemin \n(1973): Inductive Methods for Proving Properties of Programsf , Comm. ACM 16(8): 491-502 (August). Manna, \nZ., and J. Vuillemin (1972): Fix\u00adpoint Approach to the Theory of Compu\u00adtation, Comm. ACM 15: 528-536. \nManna, Z., and R. Waldinger (1978): Is IISometim@ll Sometimes Better Than Al\u00adways! ?, Comm. ACM 21 (2): \n159-171. McCarthy, J. (1963): A Basis for a Mathematical Theory of Computation, in P. Bra ffort and D. \nHirschberg (eds. ), Computer Programming and Formal Sys\u00adterns), pp. 33-70. North-Holland Pub\u00adlishing \nCompany, Amsterdam. McCarthy, J. (1978): Representation of Re\u00ad cursive Programs in First Order Logic, \nunpublished draft, Computer Science Department, Stanford University, Stan\u00ad ford , California. Morris, \nJ. H., and B. Wegbreit (1977): Program Verification by Subgoal Induc\u00ad tion, Comm. ACM 20(4): 209-22 (April). \n Park, D. (1969): Fixpoint Induction and Proofs of Program Properties, in Machine Intelligence 5, pp. \n59-78, Edinbrugh University Press, Edinburgh. Scott, D. (1970): Outline of a Mathemati\u00adcal Theory of \nComputation, Proceedings of Fourth Annural Princeton Conference on Information Science and Systems, Princeton, \npp. 169-176. Appendix: Equivalence of the Equivalence Axiom and the Minimization Schema To establish \nthat the equivalence ax\u00adi om for a recursive program P is provable from the minimization schema for P \nwe: 1. Construct the complete recursive program P corresponding to P, generating the first order sentence \ncharacterizing P . 2. Show that the sentence b x:D (last(f (x))~f( x)) is provable by structural induction \non fl (x) within Weak First Order Programming Logic. The metaproof proceeds by struc\u00adtur al induction \non the right hand side of the recursive definition of f in P. 3. Show that the sentence dx:D (t[lastCf \n](x)c last f (x)) is provable by structural induction on f (x) within the weak logic. As in 2) above, \nthe metaproof proceeds by structur\u00adal induction on the right hand side of P. 4. Deduce v x:D (f(x)c . \nlast(f (x))) from the minimization schema and 3) above. 5. Combine the conclusions of 2) and 4) to finally \nprove v x:D (f(x) =last(f (x)). To prove that the minimization schema cs derivable from the equivalence \naxiom, we must show that given an arbitrary pro\u00adgram P f(x) <-t(x) and an arbitrary function g b x:D \n(t[gl(x)~g(x)) -> v x:D (f(x)~g(x)) which reduces (by the equivalence axiom) to (*) VX:D (t[gl(x)~g(x)) \n-> b x:D (last(f (x))~g(x)). But (*) follows immediately from the lemma b x:D (t[g](x)~g(x)) -> F/x:D \n(last(f (x))~t[g] (x)) which we can prove (by induction on the structure of t) is deducible within the \nweak logic augmented by the equivalence axiom .\n\t\t\t", "proc_id": "567752", "abstract": "First Order Programming Logic is a simple, yet powerful formal system for reasoning about recursive programs. In its simplest form, it has one major limitation: it cannot establish any property of the least fixed point of a recursive program which is false for some other fixed point. To rectify this weakness, we present two intuitively distinct approaches to strengthening First Order Programming Logic and prove that either extension makes the logic relatively complete. In the process, we prove that the two approaches are formally equivalent. The relative completeness of the extended logic is significant because it suggests it can establish all \"ordinary\" properties (obviously we cannot escape the Godelian incompleteness inherent in any programming logic) of recursive programs including those which compute partial functions.The second contribution of this paper is to establish that First Order Programming Logic is applicable to iterative programs as well. In particular, we show that the intermittent assertions method--an informal proof method for iterative programs which has not been formalized--is conveniently formalized simply as sugared First Order Programming Logic applied to the recursive translations of iterative programs.", "authors": [{"name": "Robert Cartwright", "author_profile_id": "81406592800", "affiliation": "Cornell University", "person_id": "PP79027423", "email_address": "", "orcid_id": ""}, {"name": "John McCarthy", "author_profile_id": "81406600150", "affiliation": "Stanford University", "person_id": "PP79027042", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/567752.567759", "year": "1979", "article_id": "567759", "conference": "POPL", "title": "First order programming logic", "url": "http://dl.acm.org/citation.cfm?id=567759"}