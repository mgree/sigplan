{"article_publication_date": "01-01-1999", "fulltext": "\n Aggregate Structure Identification and its Application to Program Analysis G. Ramalingam John Field \nFrank Tip IBM T.J. Watson Research Center, PO. Box 704, Yorktown Heights, NY, 10598, USA {rama,jfield,tip}@watson.ibm.com \nAbstract In this paper, we describe an efficient algorithm for lazily decomposing aggregates such as \nrecords and arrays into simpler components based on the access patterns specific to a given program. \nThis process allows us both to identify implicit aggregate structure not evident from declarative information \nin the program, and to simplify the representation of declared aggregates when references are made only \nto a subset of their components. We show that the structure identification process can be exploited to \nyield the following principal results: - A fast type analysis algorithm applicable to program maintenance \n;z;lications such as date usage inference for the Year 2000 prob- - An efficient algorithm for atomization \nof aggregates. Given a pro- gram, an aggregate atomization decomposes all of the data that can be manipulated \nby the program into a set of disjoint atoms such that each data reference can be modeled as one or more \nreferences to atoms without loss of semantic information. Aggregate atomization can be used to adapt \nprogram analyses and representations designed for scalar data to aggregate data. In particular, atomization \ncan be used to build more precise versions of program representations such as SSA form or PDGs. Such \nrepresentations can in turn yield more accurate results for problems such as program slicing. Our techniques \nare especially useful in weakly-typed languages such as Cobol (where a variable need not be declared \nas an aggregate to store an aggregate value) and in languages where references to statically-defined \nsubranges of data such as arrays or strings are allowed.  Introduction Many algorithms for static analysis \nof imperative programs make the simplifying assumption that the data manipulated by a pro-gram consists \nof simple atomic values, when in reality aggregates such as arrays and records are usually predominant. \nThere are sev- eral straightforward approaches to adapting analysis algorithms de- signed for scalars \nto operate on aggregates: 1. Treat each aggregate as a single scalar value. 2. Decompose each aggregate \ninto a collection of scalars, each of which represents one of the bytes (or bits!) comprising the aggregate. \n Permission to make digital or hard copies ofall or part of this work for personal or classroom use \nis granted without fee provided that copies arc not made or distributed for prolit or commercial advantage \nand that copies bear this notice and the full citation on the first page. To copy otherwise. to republish. \nto post on sewers or to redistribute to lists, requires prior specific permission anJ:or a fee. 3. Use \nthe declarative information in the program to break up each aggregate into a collection of scalars, each \nof which represents a declared component of the aggregate containing no additional substructures of its \nown. Unfortunately, each of these approaches has drawbacks. (1) can yield very imprecise results. While \n(2) is likely to produce precise results it can be prohibitively expensive. At first blush, (3) appears \nto be the obvious solution. How-ever, it is unsatisfactory in weakly-typed languages such as Cobol, where \na variable need not be explicitly declared as an aggregate in order for it to contain composite data. \nEven in more strongly- typed languages, declarative information alone can be insufficient because (i) \nloopholes in the type system (such as typecasts) may permit aggregate values to interoperate with non-aggregate \nvalues; and (ii) programmers may pack several scalars, each encodedusing one or more bits, into a single \nword. Moreover, (3) may produce unnecessarily many scalar components when the program only ac- cesses \na subset of those components. In addition, in the presence of unions this approach can produce scalars \nthat overlap one an- other in storage inexactly. The operation of determining whether two scalars in \na program refer to overlapping storage (such checks are often required in the inner loops of analysis \nalgorithms) can be costly. In this paper, we present an efficient algorithm for lazily de-composing aggregates \ninto simpler components based on the access patterns specific to a given program. This process allows \nus both to identify implicit aggregate structure not evident from declarative information, and to simplify \nthe representation of declared aggre- gates when references are made only to a subset of their compo- \nnents. After atomization, each reference to an aggregate can be ex- pressed as a set ofreferences to \ndisjoint atoms. The resulting atoms may then be treated as scalars for the purposes of analysis, and \nchecks for overlapping storage reduce to equality tests on atoms. Atomization can thus serve as an enabling \ntechnique for perform- ing various program analyses (e.g., computing reaching definitions [l] and program \nslicing [I SJ), as well as constructing their underly- ing representations (e.g., PDG [6] or SSA form \n[4]) in the presence of aggregates. We also present a variant of the algorithm that can be used to efficiently \nsolve certain type analysis problems. One instance of such a problem is date usage inference for programs \naffected by the Year 2000 problem. This is an instance of a general class of problems that require inferring \nundeclared but related usages of type information for various software maintenance and verification activities \n[lo]. The type analysis algorithm described here has been incorporated into several recent IBM products \n. POPL 99 San Antonio Texas USA IBM VisualAge 2000 for Cobol and IBM VisualAge 2000 for PL/I. Copyright \nACM 1999 I-581 13-095-3/99/01...$5.00 01 A. 05 Fl PIC 99. 05 F2 PIC 99. 05 F3 PIC XX. 05 F4 PIC XX. \n 01 B PIC X(8). 01 c PIG x(8). 01 D. 05 F5 PIC 99. 05 F6 PIG 99. 05 F7 PIC XX. 05 FE PIG xx 01 RESULT \nPIC 99. MOVE 17 TO Fl. MOVE 18 TO F2. MOVE A TO B. MOVE B TO C. MOVE C TO D. MOVE F5 TO RESULT.  Figure \n1: Example Cobol program illustrating assignments be-tween aggregate and non-aggregate variables. 1.1 \nMotivating Examples Consider the Cobol fragment shown in in Fig. 1. For Cobol-illiterati, the declarations \nin the example behave as follows: The program contains top-level declarations of variables A, B, C, D, \nand RESULT. Variables A and D are both declared as records of four fields: Fl through F4, and F5 through \nF8, respectively. The types of these fields are declared using so-calledpicture clauses, which are a \ncom- pact means of expressing both the length and the allowable types of the sequence of characters that \nconstitute the fields. The characters that follow the keyword PIC specify the types of characters that \nare allowed at corresponding locations in the field. For instance, a 9 character indicates numerical \ndata, whereas an x character indicates that any character is allowed. Hence, variables A and D both consist \nof 4 numeric characters followed by 4 unconstrained characters. A picture character may be followed by \na parenthesized repetition factor. The non-aggregate variables B and C thus each consist of eight unconstrained \ncharacters. The example program contains a number of assignments. Note that in Cobol, it is not necessary \nto name parent structures in data references when field references alone are unambiguous (e.g., in the \nassignment of 17 to field Fl of A). Suppose we are interested in computing the backwards pro-gram slice \n[ 17, 151 with respect to the final value of RESULT, i.e., the set of assignments that could affect the \nfinal value of RESULT. Since our example program does not contain any control flow con- structs, the \nslice contains any statement on which the final value of RESULT is transitively data-dependent. We assume \nthat the fol- lowing model is used to compute these data dependences: . All variables are decomposed \ninto disjoint atoms by some means. . Each MOVE statement is modeled as a set of atomic assign- ments \nbetween corresponding atoms. . Data dependences are determined by tracing def-use chains between atomic \nassignments. Clearly, an atomization that is too crude will lead to redundant statements in the slice. \nFor example, treating the statement MOVE 01 DATA-RECORD. 02 DATE. 03 YY PIC 99. //year 03 MM PIC 99. \n//month 03 DD PIC 99. //day 02 PRINTABLE-DATE REDEFINES DATE PIC X(6). 02 . . . 01 OUTPUT-BUFFER. 02 \nLINE PIG X(80). 02 COLUMNS REDEFINES LINE. 05 COLUMN-l PIC XX. 05 COLUMN-2 PIC XX. 05 . . . 01 PRODUCT-INFORMATION. \n02 COLUMN-l-INFOPIC XX. 02 COLUMN-2-INFOPIC XX. 02 . . . MOVE FUNCTION CURRENT-DATE TO DATE OF DATA-RECORD. \nM&#38;E PRINTABLE_DATE(1:2) TO COLUMN-l. . . . MOVE PRODUCT-INFORMATION TO OUTPUT-BUFFER. Figure 2: \nExample illustrating type analysis for the Y2K problem. B TO C as a scalar assignment between two atomic \nvariables2 will lead to the inclusion of the superfluous statement MOVE 18 TO F2 in the slice. On the \nother hand, if the atomization is too fine-grained, the number of data dependences that must be traced \nto compute the slice will be larger than necessary and represen- tations that capture these dependences \n(such as PDGs) will also be larger than necessary. For example, breaking up each variable into character-sized \natoms leads to the desired slice (one that omits MOVE 18 TO F2). However, the same result can be achieved \nwith the following, much coarser-grained atomization, which is produced by our atomization algorithm: \natomization(A) = (A[1:2], A[3:4], (~[5:8]) atomization(B) = (B[l:2], B[3:4], B[5:8]) atomization(C) = \n(~[1:2], C[3:4], C[5:8]) atomization(D) = (D[l:2], D[3:4], ~[5:8]) Here, we use array notation to indicate \nsubranges of the characters occupied by a variable, E.g., B[8:4] denotes the subrange consist- ing of \ncharacter 3 and character 4 of variable B. There are a few interesting things to note about this solution: \n. Fields Fl and F2 cannot be merged into a single atom with- out a loss of precision, and therefore correspond \nto separate atoms. . Field F3 and F4 are merged, because the distinction between these fields is irrelevant \nfor this particular programs. In gen- eral, merging fields can lead to faster dataflow analysis and more \ncompact program representations. Note that this is a very reasonable choice, especially if we use only \ndeclarative information to perform the atomization. Unused fields occur frequently in Cobol applications. \nCobol-based systems typ- ically consist of a collection of persistent databases and a collection of programs \nthat manipulate these databases. Although the declared record structure reflects the format of the database, \na single application typically only uses a subset of the fields of the retrieved records. Hence, analysis \nof individual applications can benefit by coalescing or eliminating uninteresting fields. YY OF DATE \nOF DATA-RECORD + fear} MM OF DATE OF DATA-RECORD --f {notYear} DD OR DATE OF DATA-RECORD + {notYear} \nPRINTABLE-DATEt1:2] OF DATA-RECORD --f bear} PRINTABLE-DATE[3:4] OF DATA-RECORD + {notYear} PRINTABLE_DATEtS:6] \nOF DATA-RECORD -+ {notYear} LINE[1:2] OF OUTPUT-BUFFER -+ bear} COLUMN-l OF COLUMNS OF OUTPUT-BUFFER \n-+ bear) COLUMN-l-INFO OF PRODUCT-INFORMATION + bear} Figure 3: Result of type analysis applied to example \nin Fig. 2 . Although variables B and C are both declared as scalar vari- ables, both must be partitioned \ninto three atoms in order to obtain precise slicing results. Fig. 2 shows a program fragment that manipulates \ndates in ways similar to those of Cobol programs affected by the Year 2000 ( Y2K ) problem. Here, DATA-RECORD \nrepresents a record con- taining date and non-date information. The storage for date infor- mation is \nredefined in two different ways: DATE is a structured record containing separate fields for month, day, \nand year digits, while PRINTABLE -DATE is an unstructured string of uncon- strained characters intended \nto be used for input or output. Since the YY field of DATE is only two digits long, it would have to \nbe expanded to four digits to account for post-1999 dates. In addi- tion, COLUMN - 1 of OUTPUT -BUFFER \n(here representing a multi- purpose string used for input/output purposes) would have to be expanded \nto account for the fact that years are now larger. This could in turn affect PRODUCT - INFORMATION as \nwell, since even though the latter never actually contains a year value, it would prob- ably have to \nbe updated to account for the fact that the first column of OUTPUT - BUFFER is now two characters wider. \nSection 5 discusses how our aggregate structure identification algorithm can be extended to assist in \nremediating field expan- sion problems such as the Y2K problem by viewing it as a flow- insensitive, \nbidirectional type analysis problem. The basic idea is as follows: We first define a semi-lattice of \nabstract types. In the case of the Y2K problem, a lattice of subsets of the set { year, notYear ) (where \nyear and notYear are atomic types representing fields inferred to be year-related or not year-related, \nrespectively) would suffice, although more complicated lattices could also be used. Known sources of \nyear-related values, such as the year char- acters returned by the CURRENT-DATE library function in Fig. \n2 are initialized to year. Sources of values known not to contain years (e.g., the non-year characters \nreturned by CURRENT-DATE) are initialized to notYear. After applying the algorithm described in 5, the \nresults of the type analysis are depicted in Fig. 3. The interesting aspect of our analysis is not the \ntype lattice it- self, which is trivial, but the way in which the analysis is carried out efficiently \nand accurately on aggregates. This kind of analysis is applicable not only to the Y2K problem, but to \nother problems in which similar types must be propagated through aggregates, e.g., any problem involving \nfield expansion of variables holding values of a particular logical (i.e., non-declared) type. Fig. 4 \ndepicts a more problematic example, in which an ar-ray, MONTH, is overlaid with a record, MONTHS -BY-NAME. \nEach field of MONTHS -BY -NAME corresponds to an element of the ar- ray MONTH).Overlaying of records \nand arrays is a fairly common idiom in Cobol. This allows programmers to refer to array ele- ments by \nname as well as by index (e.g., when iterating uniformly through the collection represented by the array), \nand is also used to initialize arrays, as in this example. The use of such idioms makes it desirable \nto avoid overly conservative treatment of such 01 M. 02 MONTH OCCURS 12 TIMES. 05 NAME PIG X(3). 05 NIJM-DAYS \nPIC g(2). 02 MONTHS-BY-NAME REDEFINES MONTH.  05 JAN. 10 NAME PIC X(3) VALUE IS \"JAN\". 10 NUM-DAYS \nPIC 9(2) VALUE IS 31. 05 FEB. 10 NAME PIC X(3) VALUE IS \"FEB\". 10 NUM-DAYS PIC 9(2) VALUE IS 28. . . \n. . . . MOVE NDM-DAYS OF MONTH(I) TO ND.  Figure 4: Example Cobol program illustrating overlaid arrays \nand records. overlays in the context of program analysis. For instance, in the context of reaching-definitions \nanalysis, it is desirable to infer that the initializing definition of NAME OF JAN will not reach the \nuse of NUM -DAYS OF MONTH [I] , but that the initializing definition of NUM -DAYs OF JAN might reach \nthe same use. Our aggregate structure identification algorithm differentiates between references to the \narray as a whole, references to array sub- ranges with statically-determinable indices (references to \nthe ele- ments of MONTHS -BY -NAME in the example of Fig. 4 are treated as single-element instances of \nsubrange references), and references to arbitrary elements via indices computed at run-time. These dis- \ntinctions can be exploited to yield better atomizations that accu- rately differentiate among these cases. \n1.2 Overview The remainder of the paper proceeds as follows: In Section 2, we describe a tiny programming \nlanguage that contains only the lan- guage features relevant to our results. Section 3 outlines the ba- \nsic ideas behind the structure identification algorithm in terms of solving equivalence constvaints on \nranges of abstract memory lo- cations; this algorithm manipulates a new data structure called the Equivalence \nDAG. In Section 4, we observe that the algorithm of Section 3 can be viewed as computing the solution \nto a unc@mion problem. Among other things, this alternate view allows certain other problems that can \nbe expressed in terms of unification (e.g., Steensgaard s flow- insensitive pointer analysis [11, 121) \nto be incorporated smoothly into our framework. Sections 5 and 6 cover two refinements of the basic algorithm \nand their applications: Section 5 extends the framework of Sec- tion 3 to add inequality constraints \ninvolving elements of an ab- stract type lattice. The resulting type analysis algorithm is applica- ble \nto the Y2K problem. In Section 6, we formalize the atomization problem, and provide a solution based \non another extension to the framework of Section 3. The complexity of the Equivalence DAG construction \nalgorithm (in its most general form) is discussed in Section 7. Extensions to the algorithm, including \nrepresentation of variables of indeter- minate length, pointer analysis, and uses of SSA renaming, are \ncovered in Section 8. Section 9 is devoted to related work. Sec-tion 10 discusses possible future work. \nFinally, the appendix pro- vides the details of the type analysis and atomization algorithms in pseudocode \nform. denote this set by V[cQ Formally, we define V by: Pm ::= E I Stmt Pgm Stmt ::= DataRef t DataRef \n  DataRef ::= ProgVars I DataRef [Int+ : Int+] I  DataRef \\ Int+ Figure 5: The mini-language under \nconsideration. Here, Int+ de- notes the set of positive integers, and ProgVarsdenotes a set of top level \nprogram variables. 2 A Mini Language In order to facilitate the discussion of problems studied and the \nalgorithms presented in this paper, we will use a small language, the grammar of which is shown in Fig. \n5. The language of Fig. 5 may be thought of as a traditional imperative language trimmed down to the \nbare essentials necessary to discuss the problems at hand. Since the algorithms we present are flow-insensitive, \ncontrol-flow aspects of a program are irrelevant, and we consider a program P E Pgm to simply consist \nof a set of statements. A statement di t CZZE Stmt represents an assignment which copies the contents \nof data reference d2 into dr . A data reference d E DataRef is a reference to some sequence of abstract \nlocations ( bytes ) and takes one of the following forms: . a program variable z E ProgVars (the length \nof which will be denoted by Iz]) . a subrange d[Cj] of locations i through j of some data ref- erence \nd . a single, statically indeterminate element of an array of n elements, denoted by d\\n, where d is \na data reference repre- senting the complete array Subranges are used to represent a sequence of locations \nat a statically-determinedposition in a data reference. For example, if d refers to a record, then cZ[i:j] \ncan be used represent a reference to a field of the record. A data-reference created by indexing an array \nis abstracted in our mini language into a reference of the form d\\n, where d identifies the complete \narray, and n is the number of elements in the array. (Thus, our abstraction omits the actual array index \nex-pression. If the index expression is a constant, however, the data reference can alternatively be \nrepresented as a subrange cZ[i:j] of the array, where i and j delimit the range of locations occupied \nby the single array element denoted by the index expression.) The no- tation cZ\\n is intended to suggest \nthat if we break up the sequence of locations that d denotes into n subsequences of equal lengths, then \ncZ\\n denotes one of these n different subsequences. We now define the set of all locations as: Lot = \n((5, i) I I E ProgVars, 15 i < Iz]} (Different elements of Progtirs thus represent disjoint sets of \nloca- tions.) For convenience, we will denote location (z, i) as simply z[;]. At execution time, every \ndata-reference denotes a sequence of locations. At analysis time, however, we may not know the precise \nsequence of locations referred to by a data-reference d in the general case (e.g., due to a reference \nto an array element at a statically-indeterminate index). Hence, we treat a data-reference d as a reference \nto one of a set of sequences of locations, and we will D[z] = { z[l] . z[2] . . . z[[z[] } if z E ProgVars \nD[d[i:j]] = { u[i] . c7[i + 11. *. a[j] 1 u E D[dj} D[d\\nJ = { CT[S] . a[s + 11. . . u[e] 1 uE?)[cq,lIiI:a, \ns = (i -1) * (]u]/n) + 1, e = i * (]u]/n) } where u[;] indicates the i-th element of sequence u, and \n]u] denotes the length of a sequence u. Note that all elements of D[cZl have the same length, which we \nwill denote loll. For example, let z, y E ProgVars. Then, 2[3:5] denotes the singleton set {z[3] . z[4] \n. z[5]}. A more interesting example is ((y[1:10])\\2)[2:3]. Here, y[l:10]\\2 is a reference to an arbitrary \nelement of a 2-element array; the array as a whole occupies the first 10 locations ofy. The subrange \n[2:3] (which could, e.g., represent a single field when the array element is a record) is then selected \nfrom the element. As a result, the set of locations referred to consists of {y[2] . y[3], y[7] . y[8]}. \nIn other words, ((y[l:10])\\2)[2:3] is a reference to either locations y[2] and y[3] or locations y[7] \nand  YPI. We will now define an abstract semantics S(&#38; t &#38;) for the assignment statement cZl \nt cZ2, which simply consists of the set of all pairs of locations (21,12), written symbolically as 21 \nt 12, such that the assignment statement might copy the contents of location 12 to location 11. This \nsemantics is defined as follows: S(&#38; + d2) = {m(i) + c72(4 I Ql E qdl],az E D[dz], 15 i 5 min(lml, \n1~721)) In the rest of the paper, we will assume that for every statement dl i-d2, l&#38;l = Idzl. The \nabstract semantics for assignments that occur in a given program P E Pgm can be used to define an equivalence \nrelation on locations that will be useful in the sequel (e.g., as the basis for inferring equivalent \ntypes). To this end, we first define: E = u S(&#38; t dz) &#38;+daCP Now, let EP denote the smallest \nequivalence relation containing the set of pairs of locations E (i.e., the equivalence closure of E). \nWe will omit the subscript P if no confusion is likely. 3 The Equivalence DAG: The Basic Ideas Behind \nthe Al-gorithm In this section we focus on the problem of computing the equiva- lence relation up, given \na program P. The goal of this section is to give the reader an understanding of the essential algorithmic \ncon-tributions of this paper, primarily through examples. We will de- scribe extensions and applications \nof this algorithm in subsequent sections. Rather than generate an explicit representation of the equiva- \nlence relation =P, we will actually generate a more compact rep- resentation of the equivalence relation \nthat can be used to answer queries about whether two locations are equivalent or not. We will also refer \nto a statement dl c d2 as an equivalence constraint dl 11 d2 for notational convenience. 3.1 The Simple \nEquivalence Closure Problem We start with a simple version of the problem, where every con- straint has \nthe form I N y, given CC, y E ProgVars. In this case, -N induces an equivalence relation on ProgVars. \nThis is sufficient to answer questions of equivalence of locations since (2, ;) EP (y, j) if and only \nif variables 2 and y are in the same equivalence class and i = j. Thus, in this case, the set of equivalence \nclasses of ProgVars provides a compact representation of the equivalence relation on vocations. The partitioning \nof PmgVars can be done in the standard way: initially place every program variable t E PmgVars in an \nequiv- alence class by itself, and then process the equivalence constraints one by one; a constraint \nz -N y is processed by merging the equiv- alence classes to which I and y belong into one equivalence \nclass, using the well-known union-find data structure4 (see [14,3]). 3.2 The Range Equivalence Closure \nProblem Now, consider a version of the problem where every constraint is of the form z[i:j] 2: y[k:Z], \nwhere 2, y E PmgVars. There are two aspects to the original solution that we would like to preserve when \nwe address this generalized problem. The first aspect is that the algorithm processes every constraint \nin C exactly once, instead of using an iterative (e.g., transitive-closure-like) algorithm. The second \nis that we would like to identify ranges of locations that are equivalent to each other and partition \nthem into equivalence classes. This can represent EC more compactly than a partition of the set of all \nlocations into equivalence classes. We now illustrate through an example how we can achieve these goals. \nAssume that W, X, Y, 2 E Pmgtirs, and that (W( = 6,1X1 = 12, IYi = 8, and 121 = 12. Assume that C consists \nof three equivalence constraints, X[5:8] N Y[1:4], 2[1:6] 31 W[1:6], and X[3:12] 21 Z[l:lO]. We begin \nby placing every variable in an equivalence class by itself. We then process the first con- straint X[5:8] \nN Y[1:4] as follows. We split the range X[1:12] into three sub-ranges X[1:4], X[5:8], and X[9:12] and \nplace them each in an equivalence class by itself. We refer to this as adding breakpoints . We similarly \nsplit range Y[1:8] into two sub-ranges Y[1:4] and Y[5:8], placing them each in an equivalence class by \nitself. We then merge the equivalence classes to which X[5:8] and Y[1:4] belong into one. Given this \nkind of a partition of every program-variable into a collection of sub-ranges, every location belongs \nto a unique sub- range of the partition. We can map every location 1 into a pair (el, 01) where et is \nthe equivalence class of the unique sub-range containing 1 and 01 is the offset of the location within \nthat sub- range. Further, locations II and 12 are equivalent with respect to the relation =p if and only \nif the el, = ela and or1 = 01~. For exam- ple, location X[6] will be mapped to (ec(X[5:8]), 2) where \net(r) denotes the equivalence class containing sub-ranger. Similarly, lo- cation Y[2] will be mapped \nto (ec(Y[1:4]), 2). Since ec(X[5:8]) = ec(Y[1:4]), these two locations are equivalent. Let us re-visit \nthe step where we split a range, say X[1:12], into a sequence of sub-ranges, say X[1:4], X[5:8], and \nX[Q:12]. It turns out to be convenient to keep both the original range and the new sub-ranges around, \nand to capture the refinement relation be- tween these into a tree-like representation (rather than, \nfor instance, replacing the original range by the new sub-ranges). Fig. 6(a) and Fig. 6(b) illustrate \nhow we represent the refinement of X and Y for the above example. Each rectangle in the figure, which \nwe will re- fer to as a node , denotes an equivalence class of sub-ranges, and the number inside indicates \nthe length of each sub-range contained in the equivalence class. Fig. 6(c) indicates that the equivalence \nclasses containing the nodes representing X[5:8] and Y[1:4] have been merged into a single equivalence \nclass . It can be done even more etkiently using the linear time algorithm for comput- ing the connected \ncomponents of an undirected graphs. However,wewill need the flexibility of the union-find data structure \nin a generalized version of the problem. Note that edges whose targets are nodes representing equivalence \nclasses to be merged are not literally redirected to a common node, instead, the union-find data stroctore \nis used to merge the classes to which the edges refer. The next constraint (2[1:6], W[1:6]) is processedjust \nlike the first constraint, as illustrated by Fig. 6(d-e). In the general case, processing a constraint \ndl N c&#38; consists of the following steps. (i) We first add break-points to the repre- sentation before \nthe starting-location and after the ending-location of both $1 and &#38;. (ii) The sub-ranges dr and \n&#38; can then be represented by a sequence of nodes, say ~1 = [sl , . . . , sk] and ua = [t1,*.. , tm] \nrespectively. We make these two sequences equivalent to each other as follows: if sr and tr denote ranges \nof the same length, we simply merge the two into one equivalence class and proceed with the remaining \nelements of the sequence. If the two denote ranges of different lengths, we then split the bigger range, \nsay ~1, into two sub-ranges, s; and s; , such that si has the same length as tr. We then merge si with \ntl, and continue on, making the sequences [sr, ~2, . + . , sk] and [tz, . . - , t,,J equivalent. The \nthird constraint X[3:12] N Z[l:lO] illustrates the more general scenario described above. After adding \nthe necessary break- points, the range Z[l:lO] is represented by the sequence [sl, sa] (see Fig. 6(t)), \nwhile the range X[3:12] is represented by the se- quence [tl , t2, ta]. s1 is longer than tl , and is \nbroken up into sub- ranges si and sr, as shown in Fig. 6(g). We then merge tl with s;, t2 with s: , and \nts with ~2. Fig. 6(h) shows the resulting represen- tation. Clearly, given a location 1, we can walk \ndown the DAG (shown in Fig. 6(h)), from the appropriate root to a leaf et to map the loca- tion to a \npair (er, 01) such that 11 = 12 if and only if (ell ,oll) = (eta, 01~). We call the representation generated \nby this algorithm an Equivalence DAG. In the above description of the algorithm, we assumed that the \nnodes in the sequences ur and (~2 were leaf nodes. Even if that were true when the processing of the \ntwo sequences begins, when we get around to processing elements s; and tj, the processing of the earlier \nelements of the sequences could have had the effect of adding breakpoints to either si or tj or both, \nconverting them into internal nodes. Our algorithm handles this by converting the sub- ranges involved \ninto a sequence of leaf nodes lazily rather than ea- gerly. A simple example that illustrates this is \nthe single constraint A[1:12] = A[5:16]. Adding breakpoints corresponding to the endpoints of the two \nsubranges A[l:l2] and A[5:16] generates the representation shown in Fig. 7(b), The processing of the \nconstraint then proceeds as below: A[1:12] N A[5:16] u [u2] N [us, us] u (replace 2~2 by its children) \n[u4, U6] = [ias, 2131 ij- (split ~6 into ~6 and ~7) [u4, u5] N [ue,u7, us] JJ (merge ~4 and ~6) [us] \n= [u7, us] u (replace 21s by its children) [U&#38;217] N [U7,%] .lJ (merge ~6 and ~7) [2L7] N [us] .I) \n(merge u7 and US) 0 = 0 This example illustrates the motivation behind our representa- tion. Note that \nif we maintained for each variable only the list of subranges into which it has been refined (instead \nof the tree 12 I 8 4 4   (W kil 0 (g) Figure 6: An example illustrating our range equivalence algorithm. \n124 (a) Figure 7: Another example illustrating our range equivalence algorithm. representation of the \nrefinement), processing constraints such as A[1:12] N A[5:16] will be more difficult. Our algorithm may \nbe easier to understand if it is viewed as a sort of unification, with the leaf nodes denoting unbound \nvariables and internal nodes de- noting bound variables. We will explore this connection briefly in Section \n4. 3.3 The General Problem In the most general version of the problem we consider here, an equivalence \nconstraint dr N da may consist of arbitrary data ref- erences as defined by the grammar in Fig. 5, including \nreferences to (statically indeterminate) array elements. Consider, for exam- ple, a case in which we \nhave P, Q, R E ProgVars, with IPI = 20, IQ1 = 10, and /RI = 2. Assume we have two constraints P[l:lO] \nci Q[l:lO] and (P[1:20])\\10 N R[1:2]. The first con- straint is processed as before, producing the representation \nshown in Fig. 8(a). Processingthe secondconstraint, which includes an ar- ray reference, produces the \nrepresentation shown in Fig. 8(b). The nodes labeled u and w represent arrays consisting of 5 elements \nof size 2 each. We will explain in detail how our algorithm handles arrays and similar constructs in \nSection 6. A complete algorithm for the gen- eral version of the problem appears in pseudocode form in \nthe ap- pendix. 4 The Equivalence DAG as a Unifier Readers familiar with unification may have observed \nthat our algo- rithm has a unification flavor. Our algorithm can, in fact, be thought of as unifying \nterms belonging to a term language rv (defined be- low), with the following distinction: unlike in standard \nunification, we do not treat the operators @ and @ in this term language as free, i.e., uninterpreted \noperators; instead, we are interested in unifica- tion with respect to a specific interpretation of these \noperators. We explore this connection briefly in this section. For any set X, let rz denote set of terms \ndefined by:  rX::= x 1 rxwx 1 Int+ @ix (1) where Int+ denotes the set of positive integers, Occasionally \nwe will omit 8, abbreviating i @ r to in. Let V = U;>cV, denote a set of variables. A variable belonging \nto V; is said to have a length i. We will use the notation c : i to indicate that a variable z has a \nlength i. Consider now the set of terms I v. Observe that we may in- terpret the trees rooted at any \nnode in the Equivalence DAG as terms belonging to rv: leaves are interpreted as variables belong- ing \nto V; internal nodes denoting the concatenation of two ranges, such as those in Figure 6, may be interpreted \nas uses of the oper- ator $; nodes such as u and 2, of Fig. 8, representing arrays, are interpreted as \nuses of the operator 8. Let X denote the set of sequences of elements from X. Given a term T E l?x, the \nvalue ([T] E X is obtained from 7 by interpret- ing $ as sequence concatenation and @ as repeated concatenation \n(of a sequence with itself, as many times as indicated) Define the length of a term T E l?v to be the \nsum of the lengths of all variables in the sequence [r]I. A substitution u is a length- preserving mapping \nfrom V to rv:i.e., a function that maps every variable to a term that has the same length as the variable. \nThe Equivalence DAG can be thought of as a substitution, restricted to a set of variables denoting program \nvariables. For example, the DAG in Fig. S(b) represents a substitution (~8 e 5zR, ZP c) @XR)$@XR), XR \nI-$ XR}. Two substitutions ui and u2 are said to be equivalent if [oi(z)l = [ua(z)~ for all I. Every \nsubstitution u can be extended to map every term T E rv to a term U(T) E I?v. A substitution u is said \nto be a unifier for a set of unification constraints S c I v x l?v if [Us = l[u(-rz)] for every (~1, \nTV) E S. Further, it is said to be a most general unifier for 5 if every other unifier ur for S can be \nexpressed as the composition of some substitution ua with a sub- stitution u3 that is equivalent to u: \nu1 = 62 0 ua. The translation rules in Fig. 9, specified in the Natural Seman- tics style, show how a \nset of unification constraints can be generated from a program. In particular, the translation rule S \nshows how a statement dl t cl2 can be translated into a set of unification con-straints of the form ~1 \nrY 72 where ~1~7-2 E rv. The auxiliary rules D1, D2, and Ds show how a data-reference d E DataRef can \nbe translated into a variable I E V and a set of unification constraints C constraining variable x (which \nwe denote by d *D (x, C)). For example, if we have a data reference of the form d\\n (rule Da), then we \nrepresent d by a variable, say x, and d\\n by a vari- able, say y, where the variables x and y are related \nby the constraint n @ y E Z. The constraints generated from a program are simply the union of the constraints \ngenerated from the statements in the program. Let us illustrate this using the example of Figure 8. Here \nwe have P, Q, R E ProgVars, with IP( = 20, IQ] = 10, and IR( = 2. We have two constraints P[l:lO] N Q[l:lO] \nand (P[1:20])\\10 1! R[1:2]. We represent every program variable V E ProgVars by a constraint variable \nzv of length IV]. Processing the constraint P[l:lO] N Q[l:lO] producesthe substitution {ZQ I+ ZI : 10, \nXP e (U : lO)$(v : lo)}, where u and 2, are two new variables, which is represented by the Equivalence \nDAG of Figure S(a). Now consider the constraint (P[1:20])\\10 N R[1:2]. This is effectively trans-lated \ninto the unification constraint GP Z 10 @ CR (ignoring some superlluous variables that may be generated \nby a direct application of the translation rules of Figure 9). Since xp is already bound to (u:lO)@(v:lO), \nour algorithmunifies (u:lO)$(w:lO) with 10 &#38;3 (Z:R:2). Thisrequires splitting up lO@(Zi&#38;) into \n5@(~~:2)$5@ (2&#38;). The subsequent unification binds both u and 2) to 5 @ (2R:2). It is easy to see \nthat the Equivalence DAG construction algo-rithm can be interpreted as computing a unifier for the constraints \n Figure 8: An example illustrating our array equivalence algorithm. V E PmgVars where a unique variable \nxv is used for every pro- Dl VS-D (q7,Cl) gram variable v d =k-D (CZ, c) where ~1, x2, and XJ are fresh \nvariables of lengths D2 d[i : j] *D (22, C U {~1@~2~~s 2 s}) i -1, i -i + 1, and 1x1 - j respectively \nd *D (2, C) Da where y is a fresh variable of length /xi/n d\\n=F-D (y,CU{n@ysY}) Figure 9: Generating \nunification constraints from a program. generated in Figure 9. We also conjecture that unifier it computes \nis most general. 5 Application I: Type Analysis 5.1 The Problem In this section we extend the basic \nalgorithm of Section 3 to ad- dress a generalization of the type analysis problem discussed in Section \n1. Consider the example depicted in Fig. 6. Assume the programmer wants to modify the representation \nof a field of the variable W, say W[1:2]. The question we wish to answer is What other variables are \nlikely to be affected by this change, requiring corresponding modifications to their own representation? \nWe now present a more precise formulation of the problem. Let ,C denote some semi-lattice with a join \noperator U. We may think of the elements of L as denoting abstract types. An example is the lattice of \nsubsets of the set bear, notYear} used for year usage inference in the example of Fig. 2 of Section 1. \nA function A from Lot to L represents a typing for the set of locations. We say that ?r is valid with \nrespect to a program P if r(Zr ) = ?r(lz) for all 11~~12, In other words, a typing r is valid with respect \nto P if the expressions on both sides of every assign- ment in P have the same type under 17. Now consider \na constraint of the form d 2 c, where d C DataRef and c E L. We say that ?r satisfies this constraint \nif and only if ~(2) > c for every 2 E U set(o) u-L4 where set(c) denotes the set of elements in a sequence \nu. Given two typing functions al and 7r2, we say that ?rl 17~ if and only if ~1 (I) 2 ~a@) for all I \nE Lot. Given a program P and a set C of constraints of the form d >_ c, where d E DataRef and c E L, \nwe are interested in computing the least typing valid with respect to P that satisfies every constraint \nin C. 5.2 The Solution We now illustrate how we can efficiently compute the desired solu- tion, using \nthe data structure and algorithm presented in Section 3. We first process the equivalence constraints \ninduced by program P to produce the Equivalence DAG. We then associate every leaf of the resulting DAG \nwith a value from L, which is initially the least element of L. We then process every constraint d > \nc in C as follows. The data-reference d can be mapped onto a sequence of leaves in the Equivalence DAG \n(possibly after adding new break- points). We update the value associated with each of these leaves to \nthe join of their current value and c. The example in Fig. 10 illustrates this. Assume we start with \nthe Equivalence DAG of Fig. 6(h), and process constraint W[1:2] 2 bear). (We are using the lattice L \nof subsets of the set beur, notYear) described above.) W[1:2] corresponds to the single leaf tl (shown \nas a bold rectangle in Fig. lo), whose value is then updated to bear}. The resulting DAG can be viewed \nas a compact representation of tbe desired solution. In particular, it can be interpreted as a func- \ntion 7r from locations to C: the value associated with any location I is obtained by traversing the DAG \nto map location 1to a leaf el in the DAG (as explained earlier), whose value yields n(Z). In par- ticular, \nthe DAG in Fig. 10 maps locations X[3], X[4], Z[l], 2[2], W[l], and W[2] to type bear} and every other \nlocation to type {}. Equivalently, the DAG in Fig. 10 may be viewed as a function mapping every program \nvariable to a type term belonging to I c, where I is defined as in Equation 1. 6 Application II: Atomization \nIn this section, we address the aggregate atomization problem through another extension of the basic \nalgorithm of Section 3. We first con- sider some examples that illustrate several of the more subtle \nas-pects of the problem. @, 8 09 Figure 10: An example illustrating our type inference algorithm. We \nhave used N as an abbreviation for {} and YR as an abbreviation for { year}. 6.1 Motivation Overlapping \nData References Consider the problem of com- puting the reaching definitions for the use of NUM-DAYS \nOF MONTH [II in the example shown in Figure 4. In the absence of aggregates, determining whether any \ntwo direct (i.e., non-pointer) data references in a program can refer to the same set of locations is \nusually straightforward. However, in this example Cobol s aggre- gate model considerably complicates \nmatters: we note that the ini- tialization of NAME OF JAN (M[1:3] in our mini-language) does notreachtheuseofNtJM-DAYS \nOF MONTH111 ((M[l:60] \\ 12) [4:5]), but the initialization of NUM-DAYS OF JAN (M[4:5] in our mini-language) \ndoes. This follows from the fact that M[4:5] overlaps (M[1:60] \\ 12) [4:5], while M[1:3] does not. It \nshould be evident from this example that testing for overlap between two data references has the potential \nto be quite expensive, especially in the presence of arrays. Partially Killing Definitions Reaching definitions \nanalysis is further complicated by the fact that one definition may partially kill another definition. \nConsider the following example in our mini language: Sl: z[l:lO] t y[l:lO] s2: z[1:5] t t[l:5] s3: ul[l:5] \nt z[1:5] S4: 2[1:5] t r[6:10] Clearly, the definition of z[l:lO] at Sl does not reach the use of 2[1:5] \nat S3, but it does reach the use of z[6:10] at S4, because the definition ofz[l:5] at S2 partially kills \nthe definition of z[l:lO] at S 1. Although reaching definitions analysis can be performed in several \ndifferent ways, the example illustrates the need to handle partially killing definitions accurately in \norder to compute precise results. The goal of atomization is to transform the input program into a semantically \nequivalent program in which all data references are atomic, thereby simplying program analyses such as \nthe computa- tion of reaching definitions. In the case of the example above, we can transform statements \nS 144 so that each of the assignments is defined in terms of atoms zl, ~2, yl, y2, zl, and wl. The set \nof atoms partitions the set of locations such that every atom identifies a set of locations, and distinct \natoms refer to disjoint sets. For in- stance, atom zl identifies the set of locations 2[1:5], and atom \nz2 identifies the set of locations z[6:10]. (The statement Sl can be thought of as an abbreviation for \ntwo assignment statements.) Sl: (Zl, 22) t (yl, y2) s2: zl t 21 s3: wl t rl s4: 21 t x2 As with reaching \ndefinitions analysis, many other standard pro- gram analysis techniques or transformations (e.g., partitioned \nanal- ysis, SSA-form construction, or program slicing) are not imme-diately applicable to programs containing \noverlapping data refer- ences or partially killing definitions. However, once a program is transformed \ninto an equivalent one containing only operations on atoms, these complications can be ignored, since \nthe atoms can be treated as simple scalar variables. 6.2 The Basic Ideas It should be clear from the \npreceding examples that an atom is in- tended to denote a set of locations that may be treated as a single \nlogical unit for purposes of program analysis (we will make this notion more precise in the sequel). \nRecall from Section 5 that the leaves of the Equivalence DAG identify subranges of locations that can \nbe treated as a single logical unit during type analysis. It should therefore not be surprising that \nthe Equivalence DAG can also be used as the starting point for atomization. However, we need to exercise \nsome caution in treating the leaves of the Equivalence DAG as atoms. Due to the sharing present in the \nDAG structure, a leaf may identify a number of different subranges of locations in the program, and each \nof these subranges must be treated as a distinct atom. This can be done by first flattening the Equivalence \nDAG into a forest (by duplicating shared subgraphs), then treating the leaves of the resulting forest \nas atoms. To formalize this intuition, we first note that the Equiva-lence DAG can be interpreted as \na function cp mapping ev-ery V E ProgVars to a term in I?v. Let p+ be the sub-stitution obtained by renaming \nthe variable occurrences in the set of terms {p(x) 1 z E ProgVars} such that no variable oc-curs twice. \nFor example, if ProgVars = {A, B}, and Q = {A ti w&#38;yew, B I+ y@z}, then a suitable renaming is 'pr \n= {A I+- x~@Q@x~, B r-) x4$x5}. By renaming multiple occur-rences of the same variable, cpr abstracts \naway type equivalence information, while allowing us to determine for any program vari- able the aggregate \nstructure that was inferred during the construc- tion of the Equivalence DAG. In the absence of arrays, \nwe can then identify atoms with the variables occurring in the set of terms {Q?(Z) 1 x E ProgVurs}; we \nwill use Atoms to denote this set. Once the atoms have been identified, the next step is to ex- press \nall data references in the program in terms of the atoms. In particular, we can replace every data reference \nby a sequence of atomic references. In the above example, the reference to x[l:lO] can be replaced by \nthe sequence (xl, ~2). Our atomization algo-rithm guarantees that every assignment statement in the resulting \nprogramwillhavetheform(al,az,~.~,a,) t (bi,ba,...,b,), where every o; and bi is an atom. If desired, \nsuch a statement can be replaced by n simpler statements of the form a; t b;. 6.3 Dealing With Arrays \nConsider the following example: Ll: MOVE . . . TO A(5). L2: MOVE . . . TO A(I). L3: MOVE . . . TO A(6). \n L4: MOVE A(5) TO . . .  A precise reaching definitions analysis can establish that definitions Ll and \nL2 reach the use in L4, while definition L3 does not. To ex- tend the atomization approach discussed \nin the previous section to arrays in such a way that no information is lost, we must ensure that our \natomization distinguishes references to statically indeterminate array elements from references to statically \ndeterminate elements. Among other things, this means that A (5 ) and A (6 1 must be rep- resented by \ndistinct atoms (say, US and ae), and that A (1) must refer to one of a set of atoms (containing a~, a6, \nand additional atoms as required to represent the other elements of A). More generally, consider the \nexample of Figure 8. Here we must be able to model a reference to an entire array (P[1:20]), a reference \nto an array subrange (P[l:lO]), as well as a reference to a statically indeterminate array element (P[1:20]\\10. \nTo properly account for the various classes of array references discussed above, we must slightly relax \nthe assumption in the pre- vious section that all data references in the original program could be redefined \nentirely in terms of sets of atoms. Borrowing from the notation for array element references in our mini-language, \nwe will instead replace data references in the original program by atomic data references, which are \ndefined as follows:  ::= Atoms \\ Int+DataRef,,,;, Intuitively, z\\n E DatuRefAtomic represents an indeterminate \nreference to exactly one element of a set of n determinate refer-ences. We will refer to n as the multiplicity \nof the reference. Note that the union of locations referred to by s\\n need not itself be contiguous sequence \nof locations, e.g., when c\\ta refers to a single field of an indeterminate element of an array of multi-field \nrecords. In the limit case, z\\l is used for all determinate references. Not unexpectedly, such references \ninclude contiguous ranges of loca- tions such as simple scalar variables and references to entire arrays. \nHowever, determinate references may also denote noncontiguous ranges of locations, e.g., given an array \nof records, each of which contains two fields Fl and F2,the collection of references to the F2fields \nof all array elements. Consider, for example, the Equivalence DAG of Figure 8, which represents the substitution \n+ = { Q ti 5zR, z:p t-b (5ZR)$(5zR), tR I+ IR) Renaming the variables gives us pr = { Q I-) 5a1, GP \n++ (5a2)$(5as), 2R I-) a4) with atoms ~1~~x2, aa, and ~4. Note that atoms 02 and aa represent different \nparts of the array xp. The data reference P[1:20]\\10 rep-resents an arbitrary element of the array. Consequently, \nits repre- sentation in terms of atomic data references is given by (a2 \\5, as \\5 In contrast, the full \narray P[1:20] is represented by the singleton set {aa\\l . aa\\l}.We see from this example that if an array \nhas been fragmented into several sub-arrays, then a reference to an arbitrary element of the original \narray must be represented as a ref- erence to an element of one of the array subranges resulting from \nthe fragmentation. In the general case, a data reference must be replaced by a set of sequence of atomic \nreferences. Recall from Section 2 that we defined the semantics ZJ[q of a data reference d as a set of \nsequence of locations. The atomization process allows us to think of an arbitrary data references as \na set of sequence of atoms instead of set of sequence of locations. Because of this connection, we denote \nthe function that maps every data reference to a set of sequences of elements of DataRefAtomic by V1. \nThe pseudocode in Fig. 13 shows how 2)1 is defined in its full generality. 6.4 Atomization versus Unification \nIt is worth noting that atomization imposes stricter requirements on the construction of the Equivalence \nDAG than does type analysis or unification. In the context of type analysis or unification, the two terms \nT@T and 27 are completely equivalent. However, this is not true for atomization. The term T@T leads to \ntwice as many atoms as the term 27,since it indicates that atomization should distinguish the first half \nof the array from the second half. Consider the example in Fig 8. Unification of (u: lO)$(v:lO) and 10 \n@ (x~:2) creates the bindings u e 5 @ (zR:2) and v e 5 @ (z&#38;) and the unified terms can be represented \nby either 5 @ (z~:2)$5 8 (zR:2) or 10 @ (lR:2). To perform atomiza-tion, however, it is important to \nchoose 5 @ (z~:2)$5 @ (dR:2) as the representation of the unified terms. The algorithm presented in the \nappendix takes this stricter requirement into account. 6.5 Atomic Data References and Reaching Definitions \nWe note that two atomic data references x\\i and y\\j are disjoint whenever their atomic components differ, \ni.e., when a: # y. Thus for the purposes of computing reaching definitions, two data ref- erences z\\; \nand y\\j can overlap if and only if x = y. However, when determining whether a definition of one atomic \ndata refer- ence kills (i.e., completely covers ) another, the multiplicity in-formation comes into play: \nz\\i kills y\\j if and only if x = y and i = 1. In other words, only a determinate reference can kill another \nreference. Thus for the purpose of computing reaching definitions, there is no reason to distinguish \nbetween different multiplicity val-ues greater than 1. However, the full range of values will be useful \nin the sequel to establish certain formal properties of the atomiza- tion. 6.6 Correctness Properties \nIn this section, we formalize a notion of equivalence between a program and its transformed version, \nin which data references are replaced by corresponding atomic data references. We first define a function \nSi that maps every statement s E Stmt to a set of state- ments S C S~t.&#38;rn;c where ShtAtcmic ::= \nDataRefAtomic t DataRefAtcmic  SI(d1 + d2) = { m(i) +-uz(i) I (21 E 2)l[dl], CT2 E np21, 1 5 i I min(lml, \n1~21) 1 By ordering the set of locations identified by an atom a in as- cending order, we get a sequence \nof locations which we denote d[a]. Define a function 2)~ that maps every element ofDatuReS,,_,, to a \nset of sequence of locations as follows: D2[a\\nl = { (o[sl, a[3 + 11,. . . , +I) I 1 5 i 5 TZ, u = d[a], \n9= (i-1) * (IuI/n) + 1, e = i * (lul/n) } Define function Se mapping every element of StmtAtomic to \na set of ordered pairs of locations as follows: S,(a, t Q2) = { (Ul(i), u2(i)) I Ql E Dz[a1],a2 E D2[a2], \n15 i 5 min(lall, 1~21) 1  Observe that: Using SSA Renaming to Improve Precision The flow- for every \nstatement dl c d2 in the given program. Thus, we can think of our atomization algorithm as decomposing \nthe semantic function D into VI and Vz. In particular, note that the abstract semantics S(s) of a statement \ns can be fully recovered from the atomized statement Sa (s) . This formalizes the sense in which our \natomization transformation is lossless . Note that S1 oniy models flow-insensitive program properties, \nsince it does not distinguish between cases in which one of a set of possible assignments is executed, \nand cases in which all of a set of assignments are executed. It is straightforward to generalize SI to \nyield a program transformation that correctly models Sow-sensitive program properties. 7 Complexity Analysis \n Let d denote the maximum number of atoms (as defined in Sec- tion 6) and arrays identified in a single \naggregate. (For example, if we have a single aggregate whose atomization is ~1 $b(za@Ca), then d is 4. \nEquivalently, we may think of d as the maximum size of the atomization-trees produced.) Let f denote \nthe total num- ber of atoms identified in the program and let s denote the total number of statements \nin the program. Our algorithm runs in time O(sd.cu(sd, f)) in the worstcase, wherea(., .) denotes the \ninverse Ackermann function. 8 Extensions Variables of Unknown Length Our basic algorithm assumes that \nall variables have a statically-determined finite length. We can extend our algorithm to deal with variables \nof statically indetermi- nate length (e.g., variable-length strings) by representing them as variables \nof (potentially) infinite length. One interesting issue that comes up in this context is the need to \ndo an occurs check . Note that the algorithm presented in this paper binds variables only to terms of \nthe same length. When all lengths are finite, this ensures that a variable can never be bound to a complex \nterm containing the same variable. However, this is no longer true once variables of infinite lengths \nare allowed. We detect the creation of a cyclic term during unification, and replace it by a suitable \narray consist- ing of an unbounded number of elements. For example, unifying (Z : 2)$(y : co) with y \n: co results in binding y to 00 @ (Z : 2). Pointer Analysis Our full algorithm incorporates a points-to \nal- gorithm similar to that of Steensgaard [ 11, 121. Since both our algo- rithm and the points-to algorithm \nare unification-style algorithms, it is straightforward to perform both the analyses in parallel, in \na common framework. This is not only convenient, it turns out to be necessary since in the presence of \nboth (implicit) aggregates and pointers, the points-to analysis depends on atomization information while \nthe atomization algorithm requires points-to information. The essential idea in this approach is as follows: \nFor each pointer- valued variable p, we maintain a term rp describing the range of locations pointed \nto by p. Whenever two pointer-valued variables p and q need to be unified, the two corresponding pointed-to \nterms TV and TV are also unified. Note that any location (or range of lo- cations) may potentially store \na pointer value. Hence, we associate a points-to term ~1 with every leaf 2of the Equivalence DAG. This \neffectively amounts to expanding our term language I v to encode points-to information. insensitive nature \nof our algorithm can introduce imprecision, es-pecially when variables are used for completely different \npurposes in different parts of the program. The technique of Static Single Assignment [4] renaming can \nbe used to improve the precision of the results produced by our algorithm. One interesting issue that \narises here is the interdependence be-tween the atomization problem and the SSA renaming problem: our \natomization algorithm can produce more precise results if ap- plied after SSA renaming, while SSA renaming \nis easier to do after atomization since it does not have to deal with aggregates. One possible solution \nto this issue is to run the atomization al-gorithm once, apply SSA renaming, and then run the atomization \nalgorithm again to produce a more precise atomization. (Iterating any further will not improve the atomization \nresults.) 9 Related Work A substantial body of work exists in the area of type inference, following [9]. \nWhile our algorithm belongs to this family, what distinguishes it is that it is aimed at low level languages, \nwhere aggregate structure has to be inferred from the way memory (loca- tions) are accessed and used. \nThe algorithm presented in this paper can be thought of as unification in the presence of an equational \ntheory. Much previous work [S] has been done on unification in the presence of equational axioms (e.g, \nassociativity) but we are unaware of previous work in this area for the specific equational theory that \nwe are interested in. Several other authors [l 1, 10, 7, 161 have explored the appli- cation of type \ninference based techniques to program maintenance and software reengineering as well as program analysis \nfor imper- ative languages. van Deursen and Moonen [ 161 present a type inference system for Cobol and \ndescribe its applications. What distinguishes our al- gorithm from theirs is the way we handle the unification \nof records. In their algorithm, the unification of two records causes the cor- responding fields of the \ntwo records to be unified only $ the two records have the same structure, i.e., only if they have the \nsame number of fields, with corresponding fields having same length. Our algorithm is also similar in \nsome respects to a points-to algorithm presented by Steensgaard [1 l] which accommodates C-style s tructs \nand unions. The problem common to both our paper and Steensgaard s is the unification of two aggregates \nwith di$ering structure. In our approach, the result of unifying two structures Sl and Sa is a structure \nthat is more refined than both &#38; and SZ. For example, unifying 2:4@y:4@2:4 with a:4@:2@:6 results \nin the structure z:4@b:2&#38;~:2@z:4 with the additional bind- ings a I+ Z, y ++ b@w, c t-+ w@z. In Steensgaard \ns algorithm, on the other hand, the unification of &#38; and 5 2 produces a structure that is less refined \nthan both Si and Sa. In the above example, Steensgaard s algorithm [131 will stop distinguishing between \nthe fields y, .a, b and c, and produce the unified structure ~:4@t:8, with the a being bound to x, and \ny, Z, b and c all being bound to t. As a result, our algorithm computes more precise results than Steensgaard \ns algorithm. Our algorithm was primarily designed to analyze legacy applications written in languages \nsuch as Cobol and PUI, where variables are commonly used to store aggregate data without necessarily \ndeclaring the aggregate structure of the vari- ables. We believe that in such a context our approach \nis prefer- able. However, whether our approach produces more precise re-sults when applied to typical \nC or C++ applications remains to be seen. O Callahan and Jackson [IO] use type inference to C programs \nto identify sets of variables that must share a common representa-tion and outline various applications \nbased on this. 10 Future Work Future directions we wish to pursue include: [S] KNIGHT, K. Unification: \nA multidisciplinary survey. ACM Computing Surveys 21,1(1989),93-124. [9] MILNER, R. A theory of type \npolymorphismin programming. Journal of Com- puterand System Sciences I7 (1978), 348-375. Other Notions \nof Atomization Does our atomization algo-rithm produce the optimal (i.e., the least refined) atomization? \nWe believe that it does, with respect to one reasonable definition of at- omization, though we have attempted \nno formal proof. However, if we relax the notion of an atom implicit in our algorithm, the atom- ization \nproduced by our algorithm is not necessarily optimal. As an example, consider the program ~[l:lO] t y[l:lO]; \n2[1:2] t ~[5:6]. In this case, it is possible to generate the following atom- ized program (~1, ~2) t \n(yl, ~2); zl t x2 where atom zl de-notes the union of the ranges ~[1:4] and ~[7:10], while atom 12 denotes \nthe range ~[5:6]. (The remaining atoms are defined in a corresponding manner.) However, our algorithm \nbreaks up z into three atoms 2[1:4], ~[5:6], and 2[7:10], producing a more refined atomization than is \nnecessary. It is possible to take the atomization produced by our algorithm and to improve it further \nby applying an algorithm somewhat sim- ilar to the finite state minimization algorithm (grouping atoms \nthat need not be distinguished from each other into equivalence classes). It would be more interesting \nto see if such an improvement can be integrated directly into our atomization algorithm. Applications \nto Sparse Analysis The equivalence class parti- tioning of atoms produced by our algorithm can be used \nto con- struct (flow-insensitive) sparse evaluation representations for vari- ous analysis problems. \nDisjoint Unions Cobol programs may use REDEFINES for two purposes: to define disjoint unions or to define \nmultiple views of the same data. The inability to distinguish between these two us- ages forces our algorithm \nto handle REDEFINES conservatively. It would be worth developing analysis techniques to infer the use \nof disjoint unions so that they can be handled less conservatively. More Sophisticated Type Systems One \ncould extend the sim- ple type framework introduced in Section 4 in various directions, including adding \nmore complex constructors and incorporating in-equality (or set) constraints [2, 51. Such extensions \nmight enable more precise treatment of data located at variable offsets, over-loaded operators, and pointer \narithmetic than are possible with our current approach.  References PI AHO, A., SETHI, R., AND ULLMAN, \nI. Compilers. Principles, Techniquesand Tools. Addison-Wesley, 1986. 121 AIKEN,A., AND WIMMERS, E. Solving \nsystems ofset constraints. In Sympo-sium on Logicin ComputerScience (June 1992) pp. 329-340. [31 CORMEN, \nT., LEISERSON, C., AND RIVEST, R. Introduction to Algorithms. MIT Press, Cambridge, MA, 1990. CYTRON, \nR., FERRANTE, J., ROSEN, B. K., WEGMAN, M. N., AND ZADECK, F. Efficiently computingstatic single assignment \nformand the control dependence graph. ACM Transactions on ProgrammingLanguages and Systems z3,4 (199I), \n45 l-490. [41 FAHNDRICH, M., AND AIKEN, A. Program analysis using mixed term and set constraints. in \nProceedings of the 4th International Symposium on Static Analysis (September 1997), vol. 1302 of Lecture \nNotes in Computer Science, Springer-Verlag, pp. 114-l 26. PI FERRANTE, J., OTTENSTEIN, K., AND WARREN, \nJ. Theprogramdependence graph and its use in optimization. ACM Transactions on Programming Lun- guagesandSystems9.3 \n(1987),319-349. [61 KAWABE, K., A. MATSUO, UEHARA, S., AND OGAWA, A. Variable clas- sification technique \nfor software maintenance and application to the year 2000 problem. In Conference on Sofmare Maintenance \nand Reengineering (1998).J?. [71 Nesi and F. Lehner, Eds., IEEE Computer Society, pp. 44-50. (lo] O CALLAHAN, \nR., AND JACKSON,D. Lackwit: Aprogramunderstandingtool based on type inference. In Proceedings of the \n1997 International Confenxce on Sojiware Engineering (ICSE.96) (Boston, MA, May 1997), pp. 338-348. [I \nl] STEENSGAARD,B. Points-to analysis by type inferenceof programswith struc- tures and unions. In Proceedings \nof the 1996 International Conference on Com- piler Construction (Linkoping, Sweden, April 1996) vol. \n1060 of Lecture Notes in Computer Science, Springer-Verlag, pp. 136150. [12] STEENSGAARD, B. Points-to \nanalysis in almost linear time. In Proceedings of the Twenty-Third ACM Symposium on Principles of ProgrammingLanguages \n(St. Petersburg, FL, January 1996), pp. 32-41. [ 131 STEENSGAARD,B. Personal communication, Oct. 1998. \n[141 TARJAN, R. E. Data Sttwtures andNetwork Algorithms. Society for Industrial and Applied Mathematics, \nPhiladelphia, PA, 1983. [151 TIP, F. A survey of program slicing techniques. Journal ofProgrammingLan-guoges3,3 \n(1995), 121-189. [16] VANDEURSEN,A., AND MOONEN, L. Typeinferenceforcobolsystems. In5th Working Conference \non Reverse Engineering (1998), IEEE Computer Society, pp. 22&#38;230. [171 WEISER, M. Program slices: \nfomtal, psychological, and practical investiga- tions of an automatic program abstraction method. PhD \nthesis, University of Michigan, Ann Arbor, 1979.  Appendix We now present a complete description of \nour algorithm in SML- like pseudo-code. We assume that an implementation of the fast union-find data \nstructure[ 14,3] is available with the signature shown in Fig 11. The function newvar creates a new element \nnot equiv- alent to any other element (i.e., belonging to an equivalence class all by itself). The function \nunion merges two equivalence classes into one, while the function equivalent indicates if two ele- ments \nare equivalent or not. In addition, every equivalence class has a value associated with it, whose type \nis the parameter a of the parametrized type a eqclass. The value associated with an equivalence class \ncan be retrieved and modified by the functions f indval and setval respectively. The functions newvar \nand union take a parameter specifying the value to be associated with the newly created/merged equivalence \nclass. We also assume the existence of a semi-lattice L (of types ) with a join operator U. In our implementation, \nwe have a set of term variables (repre-sented by the type terravar in Fig 1 l), which are partitioned \ninto a collection of equivalence classes (using the union-find data struc- ture). Every equivalence class \nhas an associated value, which has the type ter%alue. The function 1x 1 returns the length of a variable \nx. In an actual implementation it will be more efficient to store (cache) the length with the variable, \nrather than compute it every time it is needed. The function . newvar. is a convenient wrapper for function \nIt creates a new equivalence class with the initial value w, unless Y denotes a one element array, in \nwhich case the array element itself is returned. The function split (x, n) adds a breakpoint to the DAG \nrooted at x after position n. The function refine (x, n) returns the children of a concatenation node \nx. If x is not a con- catenation node, it is first converted into one by adding a break- point, preferably \n(but not necessarily) after position n. Note that every leaf of the Equivalence DAG has a value (belonging \nto semi- lattice L)associated with it. The function update (x, c) updates the value associated with every \nleaf of the DAG rooted at x by c. The main unification algorithm as well as our type analysis al- gorithm \nappear in Fig. 12. The basic ideas behind the algorithm were explained earlier in Sections 3, 4, and \n5. The four different unify functions implement the actual unification. The functions // An implementation \nof union-find with the // following signature is assumed type 'a eqClass val newvar: 'a -> 'a eqClass \nval union: ('a eqClass * 'a eqClass * 'a) -> unit val equivalent: ('a eqClass * 'a eqClass) -> boo1 val \nfindval: 'a eqClass -> 'a val setval: ('a eqClass * 'a) -> unit // A semi-lattice L of types is assumed, \nwith a // join/meet operator U type L val U : L*L->L datatype termvalue = atomic (L, int) 1 termvar $ \ntermvar I int @ termvar withtype termvar = termvalue eqClass fun 1x1 = case (findval x) of atomic (c, \n1) => 1 I Xl @ x2 => IX1 + 1x21 I n@e lel => n * fun jTJ = case v of l@e => e 1 otherwise => newva \n fun split (x, n) = if (0 < n) and (n < 1x1) then case (findval x) of atomic (c, 1) => setvallx, atomic \n(c, n) $ atomic (c, 1-n) ) I x1 $ x2 => if (n < 1x11) then split (xI,n) else split (x2,\"-1x11) fi I \nm @ e => let val p = max(1, n/lel) in setval(x, F]e($ /(m-p)); split (x,n) end fi fun refine (x, n) \n= case (findval x) of Xl $ x2 =' [Xl, x21 I atomic (c, 1) => split (x,n); refine hn) I mc%e=> let val \np = max(1, n/lel) in setval(x, m+GiTG); refine (x,n) end fun update (x, c) = case (findval x) of atomic \n(c', 1) => setval(x,atomic (c U C , 1))) I x1 @ x2 => update (xl, c); update (x2, C) 1 m @I e => update \n(e, c) fun unify (x.y) = if (not (equivalent(x,y))) then // Merge the two vars into one, setting the \n// value of the merged var to . . . unioncx, y, // . . . the join of the values of x and y case (findval \nx, findval y) of (atomic (c, 11, _) => unifyatom(Y,c) I (_, atomic (c, 1)) => Unifyatom(x,C) I (Xl @ \nx2, _I => UnifYlist ( [x1,x21,[Yl ) I (_, yl $ y2) => UnifYli,t([xl, [Yl,Y21) I (nl 63 el, n2 8 e2) => \nunifyarrays((nl,el), (n2,e2)) ) fi; findval x // return the value associated with // the merged equivalence \nclass  fun unifyatom(x,c) = update(x,c); findval x fun unifylist(xl::r~, x2::r2) = if (1x11 = 1x21) \nthen case (rl,r2) of elsif (1x11 > 1x21) then Unifyli,t( refine(xl,Ix2l) 0 rl, x2::r2) else /* (1x11 \n< 1x21) */ Unifyli,t( xl::rl, refine(x2,Ixll) Q '2) fi  fun Unifyarraysc (nl,el), (n2,e2) ) = let fun \nexp (t,l) = t I exp (t,i) = t $ ex] 3 (t,i-1) 1 val m = least-common-mu: ii :iple~le~l,le2l~/le~l val \nx1 = exp(el, m) val x2 = exp(e2, m) val z = (nl* lell / 1x1 I in unify (xl, x2); z end  // The main \nprocedures datatype constraint = termvar E+ termvar I termvar > L fun processConstraint (x S? y) = unify \n(x,y) I processConstraint (x k c) = update(x,c) fun solve 1istOfConstraints = apply processConstraint \n1istOfConstraints Figure 12: The unification and type analysis algorithms. Figure 11: Type definitions \nand auxiliary procedures. // Assume some suitable way of generating // names for new atoms. type AtomId \nval gensym: unit -> AtomId datatype AtomicReference = AtomId \\ int datatype AtomicTree = atom of (AtomId \n* int * int) 1 AtomicTree gl AtomicTree 1 int &#38; AtomicTree // Assume a suitable implementation of \nprogram // variables\" with the following signature type PgmVar val termVar0f: PgmVar -> termvar val setAtomicTree: \nPgrrVar * AtomicTree -> unit val gethtomiclree: PgmVar -> (AtomicTree option) datatype DataRef = ProgVar \nof PgmVar ( DataRef [ int : int I 1 DataRef \\ int fun flatten(x,mu) = case (findval x) of atomic (c, \n1) => atom (gensymO,l,mu) I y $ z => (flatten(y,mu)) $ (flatten(z,mu)) 1 i @ y => i B (flatten(y,mY*i)) \nfun atomicTreeOf pgmvar = case (getAtomicTree pgmvar) of SOME atomicTree => atomicTree 1 NONE => ( \nsetAtomicTree (pgmvar, rightAssociate( (flatten (termVarOf pgmvar,l)) ); atomicTreeOf (pgmvar); ) fun \ntreesOf(ProgVar x) = { atomicTreeOf x } 1 treesOf(d [i:jl) = { subrange (t, i, j) 1 tEtreesOf(d)} I treesOf \n(d\\n) = UtCtreesOf(d) breakup (t, I t I /n) fun leaves(atom (a,l,mu),m) = [ a \\ (mu/m) 1 I leavestxl \ng x2.m) = leaves(xl,m) Q leaves(xg,m) leaves(i &#38; x,m) = leaves(x,m*i) fun Dl[d] = { leaves(t,l) \n1 t E (treesOf d) } fun rightAssociate ( (xl e x2) 2 x3) = rightAssociate (xl a (x2 a x3) ) I rightAssociate \n(xl a x2) = (rightAssociate xl) &#38; (rightAssociate x2) rightAssociate (i B x) = i &#38; (rightAssociate \nx) rightAssociate (atom (a,l,mu)) = atom (a,l,mU) fun breakup (t,s) = if (ItI = s) then { t } else case \nt of x1 B x2 = if ( I x1 I > s) then breakup (xl, s) U breakup (x2, s) else { head(t,s) } U breakupc \ntail(t,s+l), s) fi ( i &#38; x => breakup (x, s) fun subrange (t, i, j) = head ( tail(t,i), j-i+1 ) Figure \n13: The atomization algorithm (part 1). fun tail (t, 1) = t I tail (xl e x2, i) = tail (x2, i -1x11) \n fun head (xl e x2, i) = if (i > 1x11) then x1 &#38;head (x2, i -/xl]) else x11 head (t, s) = t Figure \n14: The atomization algorithm (part 2). solve and processconstraint show how the Equivalence DAG can \nbe constructed and how type analysis can be performed. The atomization algorithm appears in Figs. 13 \nand 14 and was explained in Section 6. We assume the existence of a function gensym that can be used \nto generate new symbols to represent atoms. As explained earlier, every top level program variable is \nassociated with a node (a term variable) in the Equivalence DAG. The function termvarof is assumed to \nreturn this. The first step in atomization is to take the DAG rooted as this node and flatten it to construct \nan atomization tree . The function flatten (x, mu) shows how a termvar x can be flattened into a AtomicTree. \nThe secondparametermu is used to compute the multiplicityof the leaves in the atomic tree, where the \nmulti- plicity of a leaf in an atomic-tree is defined to be the product of the cardinalities of all arrays \nin the tree that contain the given leaf. In order to simplify some of the other fimctions, the gener- \nated atomic-tree is normalized to be in right-associative form (by the function rightAssociate). We assume \nthat the imperative functions setAtomicTree and getAtomicTree let us asso- ciate an atomic-tree with \nevery program variable. (The function atomicTreeOf constructs the atomic-tree and associates it with \nthe corresponding program variable.) Once an atomic-tree has been constructed for a program vari- able \nX, any data-reference based on X can be mapped onto a set of subtrees of the atomic tree corresponding \nto X. The function treesOf does this. Please note that the input data-reference can not be an arbitrary \none. It is assumed to be one of the data-references in the original program with respect to which the \nEquivalence DAG was constructed. (Hence, the Equivalence DAG and the atomic- tree are guaranteed to have \nbreakpoints corresponding to the end- points of data-reference d.) Given an atomic reference a\\ i, let \nus refer to i as the denom- inator of the atomic reference. The tin&#38;ion leaves shows how any subtree \nS of an atomic-tree T can be converted into a sequence of atomic references. This essentially is the \nsequence of leaves of the subtree combined with an appropriate denominator. The de- nominator of a leaf \n2 is simply the multiplicity of 1 in the tree T divided by the multiplicity of 1 in the subtree S. \n\t\t\t", "proc_id": "292540", "abstract": "", "authors": [{"name": "G. Ramalingam", "author_profile_id": "81100519054", "affiliation": "IBM T.J. Watson Research Center, PO. Box 704, Yorktown Heights, NY", "person_id": "PP31045870", "email_address": "", "orcid_id": ""}, {"name": "John Field", "author_profile_id": "81100419562", "affiliation": "IBM T.J. Watson Research Center, PO. Box 704, Yorktown Heights, NY", "person_id": "PP43119729", "email_address": "", "orcid_id": ""}, {"name": "Frank Tip", "author_profile_id": "81100333471", "affiliation": "IBM T.J. Watson Research Center, PO. Box 704, Yorktown Heights, NY", "person_id": "PP15029416", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/292540.292553", "year": "1999", "article_id": "292553", "conference": "POPL", "title": "Aggregate structure identification and its application to program analysis", "url": "http://dl.acm.org/citation.cfm?id=292553"}