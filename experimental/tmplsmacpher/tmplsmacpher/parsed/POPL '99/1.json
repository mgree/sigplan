{"article_publication_date": "01-01-1999", "fulltext": "\n Once Upon a Polymorphic Type Keith Wansbrough* Computing Laboratory University of Cambridge Cambridge \nCB2 3QG, England kw217Qcl.cam.ac.uk uuw.cl.cam.ac.uk/users/kw217/ Abstract We present a sound type-based \nusage analysis for a realistic lazy functional language. Accurate information on the us-age of program \nsubexpressions in a lazy functional language permits a compiler to perform a number of useful optimi- \nsations. However, existing analyses are either ad-hoc and approximate, or defined over restricted languages. \nOur work extends the Once Upon A Type system of Turner, Mossin, and Wadler (FPCA 95). Firstly, we add \ntype poly- morphism, an essential feature of typed functional program- ming languages. Secondly, we include \ngeneral Hsskell-style user-defined algebraic data types. Thirdly, we explain and solve the poisoning \nproblem , which causes the earlier anal- ysis to yield poor results. Interesting design choices turn \nup in each of these areas. Our analysis is sound with respect to a Launchbury-style op- erational semantics, \nand it is straightforward to implement. Good results have been obtained from a prototype imple- mentation, \nand we are currently integrating the system into the Glasgow Haskell Compiler. Introduction The knowledge \nthat a value is used at most once is ex-tremely useful to an optimising compiler, because it jus-tifies \nseveral beneficial transformations. (We elaborate in Section 2.) Furthermore, it is a property that is \ninvariant across many program transformations, which suggests that the used-once property should be expressed \nin the value s type. Thus motivated, we present Usage%, a new type system that determines a conservative \napproximation to the used-at-most-once property (Section 5). Our system builds on existing work, notably \nOnce Upon A Type [TWM95a], but makes the following new contributions: . we handle a polymorphic language \n(Section 6.2). Research supported in part by the Commonwealth Scholarship Commission under the Commonwealth \nScholarship and Fellowship Plan, and by UK EPSRC grant GR/M04716. Simon Peyton Jones Microsoft Research \nLtd St George House, 1 Guildhall St Cambridge CB2 3NH, England  simonpj0microsoft.com research.microsoft.com/Users/simonpj/ \nWe handle arbitrary, user-defined algebraic data types (Section 6.4). Built-in data types are handled \nuniformly with user-defined ones, so there is no penalty for using the latter instead of the former. \nWe identify the poisoning problem and show how to use subsumption to address it, dramatically increasing \nthe ac-curacy of usage types (Section 6.3). The first two extensions are absolutely necessary if the \nanal- ysis is to be used in practice, since all modern functional lan-guages are polymorphic and have \nuser-defined data types. The extensions required are not routine, and both involve interesting design \ndecisions. All three are also addressed to some degree by the Clean uniqueness-typing system [BS96], \nin a different but closely-related context (Section 2.2). Our system comes with: A type inference algorithm, \nso that it can be used as a compiler analysis phase, without ever exposing the type system to the programmer \n(Section 7). A soundness proof that justifies our claimed connection between a value s type and its actual \noperational uses (Section 8). We have a prototype implementation, which confirms that usage-type inference \nis both relatively simple to implement and computationally cheap. The work we describe here has convinced \nus that a full-scale implementation would be both straightforward and effective, and so we are currently \nin the process of adding the analysis to the Glasgow Haskell Com-piler 2 Background and motivation What \nis the opportunity that we hope to exploit? In a lazy1 functional language, bound subexpressions are \nevalu-ated only as needed, and never more than once. In order to ensure this, an implementation represents \nan unevalu-ated expression by a thunk, and overwrites this thunk with its computed value after evaluation \nfor possible reuse. This mechanism involves a great deal of expensive memory traf-fic: the thunk has \nto be allocated in the heap, re-loaded into registers when it is evaluated, and overwritten with its \nvalue when that is computed. In this paper, the term lat refers to the use of a call-by-need semantics \n[Lau93, PJL92, AFM ?!951. 1.5 Compiler writers therefore seek analyses that help to reduce the cost of \nthunks. Strictness analysis figures out when a thunk is sure to be evaluated at least once, thus enabling \nthe replacement of call-by-need with call-by-value [PJ96]. In contrast, this paper describes usage analysis \nwhich deter- mines when a thunk is sure to be evaluated at most once. Such knowledge supports quite a \nfew useful transformations: Update avoidance. Consider the simple expression let x = e in f x. If it \ncan be demonstrated that f uses the value of x at most once, then Z S thunk need not be overwritten after \nthe evaluation of e. The thunk is still constructed, but it is less expensive than before. For implementations \nthat use self-updating thunks, such as the STG machine [PJ92], it is a simple matter to take advantage \nof such update-avoidance information. Inlining inside lambdas. Now consider the more complex expression \nlet 2 = e in Xy case x of ., and suppose that x does not occur anywhere in the case alternatives. We \ncould avoid the construction of the thunk for x en-tirely by inlining it at its (single) occurrence site, \nthus: Xy case e of . . Now e is evaluated immediately by the case, instead of first being allocated as \na thunk and then later evaluated by the case. Alas, this transformation is, in general, a disaster, because \nnow e is evaluated as often as the lambda is applied, and that might be a great many times. Hence, most \ncompilers pessimistically refrain from inlining redexes inside a lambda. If, however, we could prove \nthat the lambda was applied at most once, and hence that x s thunk would be evaluated at most once, then \nwe could safely perform the transformation. Floating in. Even when a thunk cannot be eliminated en-tirely, \nit may be made less expensive by floating its bind- ing inwards, towards its use site. For example, consider: \nlet 2 = e in Xy . . (f (g x)) . (14 If the lambda is known to be called at most once, it would be safe \nto float the binding for x inwards, thus: Xy . (f (let 2 = e in g x)) (lb) Now the thunk for x may never \nbe constructed (in the case where f does not evaluate its argument); further-more, if g is strict then \nwe can evaluate e immediately in-stead of constructing a thunk for it. This transformation is not a guaranteed \nwin, because the size of closures can change, but on average it is very worthwhile [PJPS96]. Full laziness. \nThe full laziness transformation hoists in-variant sub-expressions out of functions, in the hope of sharing \ntheir evaluation between successive applications of the function [PJLSlb]. It therefore performs exactly \nthe opposite of the inlining and float-in transformations just discussed above; for example, it would \ntransform (lb) into (la). As we have just seen, though, hoisting a sub-expression out of a function that \nis called only once makes things (slightly) worse, not better. Information about the usage of functions \ncan therefore be used to re-strict the full laziness transform to cases where it is (more likely to be) \nof benefit. The latter two transformations were discussed in detail in [PJPS96], along with measurements \ndemonstrating their ef-fectiveness. That paper pessimistically assumed that every lambda was called more \nthan once. By identifying called-once lambdas, usage information allows more thunks to be floated inwards \nand fewer to be hoisted outwards, thus im-proving the effectiveness of both transformations. To summarise, \nwe have strong reason to believe that accu-rate information on the usage of subexpressions can allow \na compiler to generate significantly better code, primarily by relaxing pessimistic assumptions about \nlambdas. A great deal more background about transformations that support the compilation of lazy languages \nis given by [San95], [Gi196], [PJPSSG], and [PJSG]. 2.1 The problem So what is the problem? At first \none might think that it is quite a simple matter to calculate usage information: sim-ply count syntactic \noccurrences. But of course this is not enough. Consider let x = e in f x. Even though x occurs syntactically \nonce, whether or not its thunk is evaluated more than once clearly depends on f. The same applies to \nthe free variables of a lambda abstraction: let x=1+2 (2) in let f=X.z.x+z in f3+f4 Here x appears only \nonce in the body of f, but since it is a free variable of f, its value is demanded every time f is applied \n(here twice). Hence x is here used more than once. Here is another subtle example, arising from Gill \ns work on performing foldr/build deforestation on the function fold1 [Gi196, p. 771. This particular \nexample results from fusion of fold1 (+) 0 [l (n -l)]: let sumUpTo :: Int -+ Int -+ Int sumUpTo = Xx. \nif x < n then let v = sumUpTo (X + 1) in Xy .21 (xc+) else Xy y in sumUpTo 10 @a) A little inspection \nshould convince you that the inner lambda is called at most once for each application of the outer lambda; \nthis in turn justifies floating out the inner lambda to join the outer lambda, and inlining v, to get \nthe much more efficient code: let sumUpTo :: Int t Int + Int sumUpTo = Xx Xy if x < n then sumUpTo (x \n+ 1) (x + y) else y in sumUpTo 10 (3b) What is tricky about this example is that in the original expression \nit looks as though sumUpTo is partially applied to one argument (in the binding for v), and hence perhaps \nthe inner lambda is called more than once. Gill used an iterative fixpointing technique to prove that \nsumUpTo is indeed not partially applied. To summarise, computing usage information in a higher-order \nlanguage is distinctly non-trivial. 2.2 Previous work With this semantics he produces an analysis that \naims to re- Strictness analysis has a massive literature. Usage analysis, as an optimisation technique \nfor lazy languages, has very lit- tle. The first work we know of is that of Goldberg [Go187], who used \nabstract interpretation to derive sharing infor-mation, which he then used to optimise the generation \nof supercombinators. This optimisation is essentially equiv-alent to the improvement we described above \nunder full laziness [PJLSlb]. Marlow [Mar931 implemented a similar analysis which has been part of the \nGlasgow Haskell Com-piler since 1993. However the analysis is extremely expensive for some programs, \nand its results are only used to enable update avoidance, not the other transformations described earlier. \nA related analysis is presented in [Gil96, pp. 72ffj. A more promising approach is based on types. Compilers \ngo to a great deal of trouble to avoid duplicating work, so a thunk that is evaluated at most once before \nsome trans-formation should also be evaluated at most once after that transformation. That in turn suggests \nthat the usage infor- mation might be encoded in the thunk s type, since a type is conventionally something \nthat is invariant across optimi-sations. What is such a usage type ? It is tempting to think that a thunk \nthat is used at most once has a linear type, in the sense used by the (huge) linear-types literature \n(e.g., [Gir95, Lin92, Wad93]). It turns out that most of this work is not immediately ap-plicable, for \ntwo reasons. First, linear types are concerned with things that are used exactly once, and brook no ap-proximations.2 \nSecond, linear types are transitive, whereas we need an intransitive system. For example, consider: let \n2=e (4)y=2+1 in y+y Here y is certainly used twice, but what about x? A classic linear type system would \nattribute a non-linear type to x too, but under lazy evaluation x is evaluated only once, on the occasion \nwhen y is evaluated for the first time. (When y is evaluated a second time, its already-computed value \nis used, without reference to z.) With this in mind, Turner, Mossin, and Wadler in Once Upon A Type [TWM95a] \npresented a type system based on linear logic and on an earlier working paper [LGH+92], but adapted for \nthe usage-type problem we have described. This paper builds directly on their work, but develops it significantly \nas mentioned in Section 1. Mogensen [Mog98] presents an extension of the [TWM95a] analysis handling a \nlarger class of data types and adding zero usage annotations. This analysis is significantly more expensive \nto compute, and does not possess a proof of cor- rectness (indeed, the authors identified several errors \nin the published version of the system [Mog97]). However, our use of subsumption (Section 6.3) was inspired \nby this work. Gustavsson [Gus981 extends Turner et. al. s analysis in a different direction, using a \nrather lower-level operational se-mantics that explicitly models update markers on the stack. 2However, \nafine types [Jac94] do permit the kind of approxima- tion made here. duce the number of update marker \nchecks, as well as avoid- ing unnecessary updates. Gustavsson s analysis does not treat polymorphism, \nand its only data structure is the list. The type rules for our system are similar to those of the uniqueness-type \nsystem of Clean [BS96]; both are based on the same linear-logic foundations, augmented with a notion \nof subytping. The Clean type system identifies uniquely-typed values, whose operational property is that \nthey can be updated in place. This is also a sort of used-once property, but it is, in a sense, dual \nto ours: a function with a unique argument type places an obligation on the caller, but gives an opportunity \n(for update-in-place) to the function. In our system, a function with a used-once argument type places \nan obligation on the function, in exchange for an optimisation opportunity in the caller. The systems \nare dual in the sense that the type rules are similar except that the direction of the subtyping relation \nis reversed. Despite the similarity of the type rules, there are numerous differences in detail between \nour system and that of [BS96]: our system is aimed at a different target, so the proof of soundness is \nnecessarily quite different; our operational se-mantics is based on a Launchbury-style presentation rather \nthan graph rewriting; we handle full System-F polymor-phism; we handle a arbitrarily-nested higher-order \nlanguage in which lambda abstractions and case expressions can ap-pear anywhere; we lay out the rather \nsubtle design space for the treatment of data structures, rather than making a particular design choice; \nand there are some technical differ-ences in subtyping rules. Because of these differences, the exact \nrelationship between the two systems is not yet clear. It would be interesting to see whether the duality \ncan be made more precise.  2.3 Plan of attack A usage type can only encode an approximation to the actual \nusage of a thunk, since the latter is in general unde-cidable. So the picture we have in mind is this: \n. The compiler infers usage types for the whole program. This process can be made computationally tractable \nby being willing to approximate. . Guided by this usage information, the compiler now per- forms many \ntransformations. The usage type information remains truthful across these transformations. . At any point \nthe compiler may perform usage-type in-ference again. This may yield better results than before, because \nsome of the transformations may have simplified parts of the program to the point where the (necessarily \napproximate) inference algorithm can do a better job. . More transformations can now take place, and \nso on. In contrast to systems requiring programmer-supplied an-notations, our system is intended as \na type-based compiler analysis. The programmer never sees any usage types. Figure 1 The language of UsageSP. \nFigure 2 The restricted language. Terms e ::= x ( c71cq 1 case e of Ci + ei Decls T : data T z = Ci v \n::= TK Types ~~4 (unannotated) ( Cl + fl2 Types c,p ::= 7 (annotated) Usages u ::= 1 1 w The variables \nx, y, z range over terms; QI and ,0 range over types. 3 The language The language covered by UsageSP \nis presented in Figure 1. Our language is a variant of the Girard-Reynolds polymor-phic X-calculus, extended \nwith many of the features of Core, the intermediate language of the Glasgow Haskell Compiler (GHC) [GT98]. \nCompared with the language of [TWM95a], UsageSP re-places nil and cons with general constructors and \na corre- sponding general case construct, adds type abstraction and application, and requires explicit \ntypes on lambda abstrac-tions and let bindings. In addition, it permits the applica- tion of functions \nto expressions (rather than only to vari-ables). We write c to abbreviate ei, e2,. . . ,e,. The form \nof the case expression may require some expla-nation. The conventional expression case e of Ci zij -+ \ne, both selects an alternative and binds variables; thus there are usually three constructs that bind \nvariables: lambda, le-tree, and case. The form of case we use here avoids binding variables, and instead \npasses the constructor arguments to the selected expression as function arguments to be bound by a lambda. \nThis slightly complicates the statement of the typing rule for case, but simplifies our proofs. Figure \n1 also gives the syntax of types. Notice their two- level structure. A a-type bears a usage annotation \n(1 or w), indicating the use made of a value of that type; a r-type does not. The annotations denote \nused at most once and possibly used many times , respectively. We define a func- tion 1 1 to obtain the \n(outermost) annotation on a a-type: ]rU] = u. Int and other primitive data types are treated as nullary, \npre- declared type constructors. For arrow types and Int the sys- tem is equivalent to that of [TWM95a]. \nFor lists and other algebraic data types, however, there is an annotation on the type itself but not \non the type arguments (e.g., (List Int)w); this choice differs from [TWM95a] and is discussed further \nin Section 6.4. Terms e ::= a (normalised) 1 :; : u. e ] ea. e ret Xi : 0; = ei in e I c7lcq I case e \nof Ci + ei  Atoms a ::= zr ar Values v ::= I Xx:a.e n ( l CKq Ra.v Two key design decisions relate \nto the handling of polymor- phism. Firstly, a type abstraction abstracts an unannotated (r-) type, rather \nthan a u-type: Vcu. 7 rather than Vcv u. Secondly, type variables a! range over r-types, rather than \nover u-types. That is, the argument of a type application is a r-type, and the arguments of a type constructor \nare also r-types. The reason for these decisions will become clear shortly: the first decision is a consequence \nof the opera-tional semantics and is discussed in Section 4.2; the second is deeper and is discussed \nin Section 6.2. 4 Operational semantics Our analysis is operationally motivated, so it is essential to \nformalise the operational semantics of our language. We base this on that of [TWM95a] (which is itself \nbased on the standard operational semantics for lazy functional lan-guages, [Lau93]), with alterations \nand extensions to handle user-defined data types and polymorphism, and to support our proofs. The natural \n(big-step) semantics is presented in Figure 3, and is largely conventional. H denotes a heap consisting \nof typed bindings of the form (x : a) +-+ e. The heap is unordered; all variables are distinct and the \nbindings may be mutually recursive. A configuration (H) e denotes the expression e in the context provided \nby the heap H; variables free in e are bound in H. The big-step reduction (HI) e vq (H-J) v signifies \nthat given the heap HI the expression e reduces to the value v, leaving the heap HZ (values are defined \nin Figure 2). The significance of the type context G is explained in Section 4.2. Standard capture-free \nsubstitution of the variable y for the variable x in expression e is denoted e[x := y]; similarly substitution \nof the type r for the type variable (Y everywhere in expression e is denoted e[a := r]. Type substitution \nextends pointwise to heaps. The reduction relation 4. is defined over a language slightly smaller than \nthat of Figure 1; this is given in Figure 2. Fol- lowing Launchbury [Lau93] we use letrec bindings to \nname arguments of applications and constructors, in order to pre- serve sharing. The translation into \nthe smaller language is straightforward. We allow applications of functions and constructors to atoms \nrather than just variables so the sub- stitutions in (&#38;LETREC) can work (see Section 4.2). Figure \n3 Natural semantics for UsaeeSP. Ial = 1 (Hi) e UK 032) v /u[ =w WI) e UK (HZ) 2, (&#38;VAR-ONCE) (u-VAR-MANY) \n (Hl,a::a~e)a:U~(H2)v (Hl,z:a~e)zU~(H2,~:~~)) ~ fresh yZ s = ( j) (HI, yi : (KG Ti)ui * AZ ei[S]) e[S] \nU, (Hz) v (u_LETREC) (HI) letrec zi : 7; UC= e; in e U, (Hz) v (HI) e u-, (Hz) AZ : u e (Hz) e [x := \nu]UK (HZ) 2, (U-*pp) (H)Xx:u.e~~(H)Xx:u.e( ~ABS) (HI) e a UK (ff3) 2, (HI) el U, (HZ)721 (Hz) e2 UK \n(H3) n2 (U-CON) (&#38;PRIMOP) (H)C71eiqq.q((H)C7 Ca, (HI) el + e2 UK (H3) n1 cE 722 (HI) e k (H2) Cj \n7r, al.. a, (HZ) ej oi . . am UK W3) 9~ (u_CASE) (H) n Uw (H) n. (@INT) (H1)caseeofCi+e;Uq(H3)u fresh \n(Y (HI) e[o := CX ] J,&#38;,~~ (H2) v (Hl)e.l&#38;-(Hz)h. 2)(Q-TYABS) (Jl-TYAPP) (Hi) ha. e UK (H2) h \n2) (HI) e 7- U, (Hz) w[Q! := 7-1 4.1 Usage control A conventional semantics would have only one rule \nfor vari- ables, the (&#38;VAR-MANY) rule. This rule performs the usual call-by-need overwriting of the \nheap binding. But as in [TWM95a] we restrict this rule to apply only when the vari- able s usage annotation \nis w; when the annotation is 1 a sec- ond rule, (&#38;VAR-ONCE), applies. This latter rule deletes the \nbinding after use; hence an incorrect usage annotation will lead to a stuck expression to which no reduction \nrule ap- plies. The proof in Section 8 shows that the annotations we generate can never result in a stuck \nexpression, and hence guarantees that our usage annotations are correct with re-spect to the operational \nsemantics. 4.2 Type information As in many implementations of typed functional languages, GHC erases \nall type information prior to program execution; thus type abstractions and applications do not normally \nap-pear in the operational semantics. In order to support our proofs, however, our operational semantics \ndoes carry type information in expressions and inside the heap. This information is entirely ignored \nby the semantics, apart from the topmost annotation on bound variables which is used as described above \nto guide the ap-plication of the (J,l-VAR-ONCE) and (JJ-VAR-MANY) rules. Indeed, deleting this type information \nand merging the two rules yields a semantics essentially the same as Launchbury s original semantics \n[Lau93]. This leads to the unusual form of the type rules (U-TYABS) and (JJ-TYAPP). These rules define \ntype abstractions and applications as being transparent : evaluation is permit-ted underneath a type \nabstraction (hence only a type ab-straction of a value is a value according to the definition in Figure \n2), and a type application performs type sub-stitution after the abstraction has been evaluated. There \nis an exactly equivalent operational semantics involving no types (other than the topmost annotations \non bound vari-ables), and in this erased operational semantics (&#38;TYABS) and (.I-TYAPP) do precisely \nnothing. The presence of types in the operational semantics is purely to aid the proofs in Section 8. \nWhile evaluating underneath a type lambda, we keep track of the type context by an annotation ?i% on \nthe reduction relation. This type context is simply an ordered list of the variables bound by enclosing \ntype lambdas. We abbreviate UIj by U. The type context is manipulated only in (U-TYABS) and (lJ-LETREC). \nThe former rule simply adjusts the annotation while evaluating under a type lambda; it applies both to \ne r and to a 7 since an atom a is also an expression. The latter rule maintains the invariant that heap \nbindings never have free type variables, using a technique similar to that used in GHC for let-floating \nin the presence of type lambdas. Before placing a binding in the heap, potentially-free type variables \nare bound by fresh type lambdas; a substitution is performed to ensure that the types remain correct3 \nNotice that this yields call-by-name behaviour for type ap-plications. We do not share partial applications \ne 7. This is as we expect, since the partial applications have no real operational significance. The \nalternative would require cre-ating an extra thunk for e r in order to share it, distinct from 3The notation \nm e abbreviates V oll VW e; similarly for type lambdas and applications. the thunk for e. This does not \ncorrespond to the behaviour of the evaluator. The transparent nature of the (JJ-TYABS) and (&#38;TYAPP) \nrules determines the design decision mentioned in Section 3 regarding the syntax of polymorphic types. \nAll expressions must be given a u-type. Say expression e has type u1 = r , and the expression ha . e \nhas type (~2. Since type abstrac- tion has no operational significance and usage is an opera- tional \nproperty, we expect that ]cri] = 1~21. Were 02 to be (Va TzL)% (i.e., were type abstraction to abstract \nover a u-type), we would have two usage annotations u and u con- strained always to have the same value. \nInstead, we define them to be the same variable by defining type abstraction to be over a r-type: 02 \n= (Vo T)~. Notice that in a language featuring intensional polymor-phism [HM95] this decision would be \ndifferent. Typing infor- mation would have operational significance, types would be retained at run time, \nand the (Q-TYABS) and (JJ-TYAPP) rules would appear differently; hence type abstraction would be over \na u-type, reflecting the distinct usages of the type abstraction and its body (r~ and 11 in the example \nabove). 5 Type system In this section and the next we present a type system that approximates which subexpressions \nare used more than once during evaluation, according to the operational semantics. The typing judgements \nof the system are shown in Figure 5. 5.1 Usages and usage constraints Usage annotations are as defined \nin Figure 1, and are per- mitted to be 1 (used at most once) or w (possibly used many times). Usages \nare ordered, with 1 < w. Constraints on us- age annotations are either a relational constraint u. 5 u \nusing this ordering, or a simple equality constraint 2~= w.  5.2 Subtypes A subtype ordering 4 on both \nu- and r-types is obtained as the obvious extension of 5. Since we wish to consider 4 as a subtyping \nrelation, however, the ordering under this relation is opposite to the orderin of annotations. For example, \n1 < w and hence Int =$ Int7 This is because a value that is permitted to be used more than once (e.g., \nI&#38;) may safely be used in a context that consumes its argument at most once (e.g., In?). Subtyping \nis used in Section 6.3 to solve the so-called poisoning problem . The subtyping relation is defined \ninductively in Figure 4. The ordering is contravariant on function types. Universally-quantified types \nare related by unifying quantified variables (we treat types as o-equivalence classes, so this is implicit). \nSaturated type constructors4 are related if and only if all instantiated constructor argument types are \nrelated. The relation CY E fv (r) states that type variable (Y has a free 40~r language fragment handles \nonly saturated type constructors, thus remaining essentially in System F rather than F,. There is a straightforward \nbut pessimistic approximation for higher-order type constructors: if the type constructor is a variable \ncy rather than a con- stant T, simply assume that all its arguments occur both covariantly and contravariantly. \nFigure 4 The subtyping relation =$ . . (*-ANNOT) u3 d u1 u2 u4 (*-ARROW) CT1 + u2 4 (53 + u4 71 d r2 \nx (+TYVAR) ( *-FORALL) , vff . l-1 =$ da!. rz c-ve occurrence in r. The subtyping relation is discussed \nfurther in Section 7.1. 5.3 Occurrences To aid in the statement of the type rules, we define a syn- \ntactic occurrence function. The expression e) gives OCC L~T(Z, the number of free syntactic occurrences \nof the variable 2 in the expression e. It is defined inductively. The definition is trivial except for \nthe case statement, for which we define occ1LT(z, case e of Ci -+ e;) = occur(z, e) + maxy==, occw(z, \ne;) Here we conservatively approximate by taking the maximum number of syntactic occurrences in any alternative. \nTurner et. al. avoid using this function in their technical report [TWM95b], and instead detect multiple \noccurrences through special operators for combining contexts. This ne- cessitates tighter control over \ncontext manipulation; our ap- proach is exactly equivalent, but leads to a simpler inference algorithm \nand easier proofs. 5.4 Contexts A context l? is a set of term and type variables, the for- mer annotated \nwith their types. Standard rules define well- formed contexts and well-kinded types; these are omitted \nhere (refer for example to [SP94]). All term and type vari- ables appearing in the context are distinct. \nWe write I ,z : u for the extension of the set l? with the element (z : u), and P,CY for the extension \nof I with the element 01. z E P expresses that the term variable x is found in the context I (with some \nunspecified type), and Q: E P that the type variable o is found in the context I . Since the term variables \nin a context are all distinct, P can be seen as a function from term variables to their types: I (x) \nis the type of term variable x. Figure 5 Type rules for UsageSP. occu~(x,e) > 1 * 1~711 w = occur(y,e) \n> 0 * [l?(y)1 2 u for all y E l? (F-Ass) r t- AS : n1 e : (ul + az) r, xj : 0; I-ei : U; u: < ui for \nall i r,xj :u~E~:u OCCUT(X~,e) + cjn=l occw(xi, ej) > 1 * Iui( = w for all i r k letrec xi : ui = ei \nin e : u data T G = Ci G r k ej : a&#38; ~7:~ 4 (7ij[ak])lL for a11 (,---CON) lY!-C;~~: (Tc) r,ak-e:? \n(I--TYABs) r t- h. e : (VCX 7)  5.5 Type rules The type rules for UsageSP are given in Figure 5. The \njudgement form l? l- e : u states that in context l? the ex-pression e has type LT. Usage constraints \nappearing above the line in the rules are interpreted as meta-constraints on valid derivations. Notice \nthat an expression may have multi- ple incomparable typings: for example, the term Xx : Int . x may be \ntyped as either Id 4 Id or irk +@ Id . For this reason, the system of Figure 5 does not enjoy principal \ntypes. However our use of subsumption means we can still assign each variable a type that fits every \nuse of that variable; indeed we show in Section 7 that we can choose an optimal such type. We also describe \na system that employs usage polymorphism to recover principal types, in Section 6.3. The notation is \nconventional, and the rules are essentially the same as the usual rules for a lambda calculus with sub- \nsumption (see, e.g., [CG94]) except that we omit bounded quantification. Notice that the rules have been \npresented here in syntax-directed form: each rule corresponds to an alternative in the term syntax presented \nin Figure 1. The most complex rule is (~--CASE). In the subtype con-straint, the substitution Tij[ak \n:= ~kk] instantiates the ap-propriate component type from the data type declaration, and the notation \n(F + c)l is shorthand for the used-once multiple-argument application (~1 + (u, --f ( . + (un -+ u) . \n..) ) ) . Notice that because of subsumption each al- ternative may accept a supertype of the actual \ntype of each component, and may returp a subtype of the result of the entire case expression. 6 UsageSP \nHaving presented the boilerplate of our type system, we now introduce its less-standard features. r t- \nel : (aI -+ f12)u r I- e2 : U; u u1 Q-_App) r k el e2 : u2 r I- el : Id1 r' e2 lntu2 (/-_PRIM~P) (I--LETREC) \nr t- el + e2 : lntY3 data T G$ = Ci q l?te:(Tx) r k e; : CT: ui < ((qj[cq := ~1) --+ u)l for all i (~--CASE)r \nk case e of Ci --t e; : u r t e : (VCY 72)u (I--TYAPP) r t- e rl : (~~[a := ~~1) 6.1 Multiple uses \nContexts are treated in an entirely standard way-the rules of weakening and contraction are implicit \nin the presenta- tion. How then do we identify multiple uses? The answer lies in the appropriate use \nof the syntactic oc-currence function defined in Section 5.3. If a variable oc-curs more than once in \nits scope, the variable is certainly used more than once and we annotate it thus in (F-Ass) and (k-LETREC). \nAdditionally, recall Example 2 from Sec- tion 2.1: each time an abstraction is used, any free variables \nof that abstraction may also be used. Thus all free variables of an abstraction have at least the usage \nof the abstrac-tion itself. This appears as the third line of the premise in (k-ABS). The (I=LETREc) \nrule includes a condition on syntactic oc-currences that recognises both recursive and nonrecursive uses \nof variables, and handles them appropriately. Finally, a use of a constructor entails a (possible) use \nof its contents. Thus we require in (J--CON) that an expression placed into a constructor has a usage \nat least that of the constructor application itself (expressed by placing u from the constructor type \nas annotation on the r-type of the con- structor argument from the data type declaration). This corresponds \nto [TWM95a] s global well-formedness condi-tion . If one thinks of the standard functional encoding of \na constructor, then this constraint is similar to that discussed above on the free variables of a lambda. \n 6.2 Type polymorphism The first development we mentioned in Section 1 was the extension to a polymorphic \nlanguage. Handling polymor-phism in some way is clearly essential for realistic applica-tions. This is \nthe r81e of the (I--TYABs) and (k-TYAPP) rules (Figure 5). It is not the case that introducing type polymorphism \nrequires introducing usage polymorphism as well; the two are in fact orthogonal issues. We consider type \npolymorphism in this section and usage polymorphism in the next. There are two design decisions to be \nconsidered in the im-plementation of type polymorphism. The first concerns the (l--TYABs) rule, and the \nrepresentation of a type abstrac-tion. Given that P,cw k e : u, what type has Ra! e in context I? Clearly \nfor consistency it must be a r type; we expect something of the form (t/o .) . In Section 4.2 we argue \nthat for operational reasons it only makes sense to have a single usage annotation on a type abstraction, \nand thus l? I- ha e : (Va 7) for some r and u. The same argument shows u = ]cr], and thus it follows \nlogically that r = u. This gives us the (t-TYABS) rule shown; it can be interpreted as lifting the usage \nannotation of the original expression outside the abstraction. The second design decision concerns the \n(t--TYAPP) rule, and the range of a type variable. Type application should reverse the behaviour of (t--TYABs): \ngiven I k e : (Vcy T)~, we expect to have P k e $ : (~[a := +,I) . But what should II, (and hence cr) \nbe: a r- or a u-type? The answer becomes obvious when we consider that the pur- pose of the usage typing \nof a function is to convey informa- tion about how that function uses its arguments. Consider the functions \ndup : Va a + Pair cy a, which duplicates its argument, making a pair with two identical components, and \nin1 : Vcu . Q -+ L a, which injects its argument into a sum type. We should be able to express that dup \nuses its argument more than once, whereas in1 does not; that is, we should have dup : Va . d -+ but in1 \n: V(Y a1 -+ It is only possible so to annotate 01 if (Y E 7. The alternative is to introduce bounded \nquantification, so that we have dup : Vo : Ial 2 w a -+ . and in1 : dcy: Icx( 2 1 Q + . . , as suggested \nin [TWM95a, $4.51. However this leads to a loss of precision: consider the function twoone=Rcr.Xz:a.Xy:cu.(z,z,y) \n(5) We want here to express that although both arguments have the same underlying type, their usages \ndiffer: the first is used twice, the second once: twoone : Va . CP + a1 -+ . . Bounded quantification \nis insufficient for this; we must instead allow cy to range over unannotated types and add separate annotations \non each occurrence. This is done in the (k-TYAPP) rule shown. 6.3 Subsumption and usage polymorphism \nA distinctive feature of our system compared to more con-ventional linear type systems, including the \n[TWM95a] anal-ysis, is our use of subsumption. In this section we discuss and justify this choice. Consider \nthe expression .let f = XX z + 1 (6) a=2+3 b =5+6 in a + (f a) + (f b) Here it is clear by inspection \nthat a s value will be demanded twice, but b s only once. The implementations we have in mind use self-updating \nthunks [PJ92], so that f s behaviour is independent of whether its argument thunk requires to be updated. \nWe therefore expect b to be given a used-once type. (In an implementation where the consumer of a thunk \nper-forms the update, f could not be applied to both a used-once and a used-many thunk. Exploiting sharing \ninformation is much harder in such implementations.) However, [TWM95a] infers the type I&#38; , denoting \npossible multiple usage, for b. Why? Clearly a s type must be It+ , since a is used more than once. So \na non-subsumptive type system must attribute the type lnt + to f. But since f is applied to b as well, \nb gets f s argument type Intw. We call this the poisoning problem , because one call to f poisons all \nthe others. Poisoning is absolutely unacceptable in practice -for a start, separate compilation means \nthat we may not know what all the calls to f are. There are two solutions to the poisoning problem. One \npos- sibility is to use usage polymorphism. That is, f gets the type Vu Int + , so that it is applicable \nto both values of type lnt (by instantiating u to w), and to values of type lnt (by instantiating u \nto 1). Such a system was sketched by [TWM95a], although as a solution to a different problem. We see \nthree difficulties with this approach: Technically, simple polymorphism turns out to be insuf- ficient; \nbounded polymorphism is required, with types such as (Vu1 VUZ 5 ui Vu3 5 ui VQ (List o) i + ((List c~) \n~ -+ (List a)u2)u3)W. Theoretically, bounded usage polymorphism significantly complicates the (already \nhard) problem of providing a denotational-semantic model for the intermediate lan-guage. Pragmatically, \nwe fear that the compiler will get bogged down in the usage lambda abstractions, applications, and constraints, \nthat must accompany usage polymorphism in a type-based compiler. Even quite modest functions or data \nconstructors can be polymorphic in literally dozens of usage variables, so they quickly become quite \ndominant in a program text. Furthermore, bounded polymorphism seems like a sledge-hammer to crack a \nnut. Suppose f has the type In? -+ . . . We want to interpret this as saying that f evaluates its ar-gument \nat most once, but saying nothing about how f is used. In particular, it should be perfectly OK to apply \nf to an argument either of type lnti or of type lntW without fuss. On the other hand, if g s type is \nlntW -+ . , we interpret that as saying that g may evaluate its argument more than once, and so it should \nbe ill-typed to apply g to an argument of type Inti. This one-way notion of compatibility is just what \nsubtyping was invented for. Using a subtyping judgement based on usages (see Section 5.2), the addition \nof a subsumption rule to the system: Ite:g 0 =$ (T (!--SUB) rt-e:a at one stroke removes the poisoning \nproblem. In terms of the example above, (~--SUB) allows us to conclude that an argument of type lntW \nalso has type lnti and hence can be an argument of f. We have pushed subsumption down into (t--APP), \n(~--CON), (k-LETREC), and (~--CASE) in Figure 5 in order to obtain a syntax-directed rule set. This brings \nthe rules closer to the form of a type inference algorithm (Section 7) and makes the proofs easier. Returning \nto Example 6 at the start of this section, we see that a still gets annotation w, but this does not now \naffect f s argument annotation of 1 and thus does not affect the annotation given to b, which is 1 as \nwe originally expected. The inference of an w usage of a variable will no longer result in the needless \npoisoning of the remainder of the program. All of this is pretty simple. There is no clutter in the term \nlanguage corresponding to subsumption, so the compiler is not burdened with extra administration. The \nsystem is sim- ple enough that all our proofs include subsumption, whereas none of the proofs in [TWM95a] \ncover the system with us-age polymorphism. Lastly, the effectiveness of subsumption seems to be closely \ntied up with our use of self-updating thunks; we wonder whether there may be some deeper prin- ciple \nhiding inside this observation. Does the polymorphic sledgehammer crack any nuts that subsumption does \nnot? Certainly it does. For a start, it en- ables us to recover a conventional principal typing property. \nFurthermore, usage polymorphism is able to express the de- pendency of usage information among several \narguments. For example, consider the apply function: apply=Xfx. f x (7) The most general type our system \ncan infer for apply, as-suming no partial applications of apply are permitted, is apply :: VCY V/3 ((a? \n4 /3 )l + (aw -+ ,P)l) In the application (apply negate z), our system would there- fore infer that x \nmight be evaluated more than once when actually it is evaluated exactly once. (We are assuming that negate \nevaluates its argument just once.) Usage polymor- phism is able to express the connection between the \ntwo arguments to apply by giving it the type apply:: Vu1 Vu2 Va . VP ((au1 + flu2)l -+ (au1 + ,P)l)w \nThe situation is not unlike that for strictness analysis, where abstract values are often widened to \nmake the analysis more tractable, but at the expense of losing information about the interaction of function \narguments. Our present view is that the costs (in terms of complexity and compiler efficiency) of usage \npolymorphism are likely to be great, while the benefits (in terms of more accurate types) are likely \nto be small. It remains an interesting topic for further work.  6.4 Data structures The extension of \nthe system to handle general Haskell-style data types with declarations of the form data T ZZ = Ci 7;, \nis not entirely straightforward. For a start, should the constructor component types spec-ified in the \ndata declaration range over T- or a-types? At first it seems they should range over u-types, as do the \nar- guments of functions. But a little thought reveals that it does not make sense to talk about how \na constructor uses its arguments: their usage depends on the way in which the constructed data is used, \nnot on the constructor itself. Hence, we give no annotation to constructor argument types in the data \ndeclaration; instead we infer the usage of con-structor arguments at a constructor aplication from the \nway in which the constructed value is used. Constructors do not, therefore, have a single functional \ntype; instead there is a special typing rule for construction. The key issue, therefore, is how to ensure \nmultiple use of a component (via a case expression) propagates to the appro-priate constructor site, \nforcing the annotation of the appro- priate constructor argument to be w. The manner of this propagation \ngives rise to a number of alternatives. Consider the code fragment5 data T a! = MkTa (aW --+ au) let \na :: Int? = 1 + 1 (8) f :: (InP -+ Intw) = Xy y + y x :: (T Int) = MIcTa f g :: (VQ . (T CY) + a?) g \n(MkTa f) = f (f a) in gx Here the function g uses its argument (MkTa f) once, but the components of \nthe argument are used respectively once and twice. Clearly this information must propagate to the type \nof x and thence to the types of a and f. We consider four alternative ways of typing this expression \nto propagate this information: We could explicitly specify constructor component usage annotations in \nthe type. This would give g the type (VQ . (T 1 w cx)l -+ CI? )~, where we introduce usage arguments \nto T giving the uses of each component. Now x is given the type (T 1 w Int) , and a and f annotations \n1 and w respectively. We reject this for the same reasons as before (Section 6.3); it amounts to usage \npolymorphism and would signifi-cantly complicate our system technically, theoretically, and pragmatically. \nAn intermediate alternative, more ad hoc but perhaps more practical, would be to follow the example of \nthe Clean type system [BS96] and attach usage annotations to each type argument of the constructor. These \nannota- tions would then be applied uniformly to all occurrences of that type variable, and some other \nrule would be used to provide the remainin annotations. Thus g would have the type (VQ (T CX ) B -+ &#38; \n)l, x the type (T Id) , and a and f annotations 1 and w respectively. The rule would here have to assign \nannotation w to the function argument. We have already argued in Section 6.2 that usage anno-tations \nshould not be linked to specific type variables. Furthermore, the use of a separate rule to assign non-type-variable \nannotations adds extra complexity. We could assume that all constructor components are used more than \nonce. This seems reasonable since one usually places data in a data structure in order that it may be \nused repeatedly. This would give g the type 5Notice that the syntax still forces us to give annotations \nfor the internal types of the constructor arguments. The only reasonable choice seems to be the most-general \nannotation, w, since we do not want to restrict the arguments to which the constructor can be ap-plied, \nand in the presence of separate compilation we cannot see all its uses. (VCX (2 (Y) + &#38;)l, x the \ntype (2 Int) , and both 7.1 Subtyping a and f the annotation w. While this avoids the overhead of usage \npolymorphism, we lose a lot of precision by making this assumption. Consider the common use of pairs \nto return two values from a function (say quotient and remainder from a di- vision). The pair and its \ncomponents are each used only once; if the rules above are used they will be annotated w and thunks will \nbe built for them needlessly. . We could identify the constructor s overall usage annota- tion with \nthe annotations on its components. Since one component (f) of g s argument is used more than once, this \nmeans we annotate the argument itself with w; so g is given the type (Va (T CU)~ + &#38; )I, z the type \n(T Int)w, and both a and f the annotation w. Since we use only a single usage annotation, if any com-ponent \nis used more than once, all are annotated w. This identification preserves good behaviour for, e.g., \npairs used once, while behaving only slightly worse in the gen- eral case. Since different construction \nsites may have dif- ferent annotations, the effect of the approximation is lo- calised. The final alternative \nabove seems a reasonable compromise between simplicity and precision: we avoid adding usage polymorphism \nand its attendant complexity, at the expense of sub-optimal types in some cases. The general type rules \n(~--CON) and (~--CASE) in Figure 5 follow this alternative. The rules are uniform and do not penalise \nusers for defining their own data types: built-in data types such as lists and pairs are handled by the \nsame rules as user-defined ones. Inference In order to use Usage% as a compiler analysis phase, it is \nnecessary to provide an inference algorithm. Such an algo-rithm must be decidable, and should infer annotations \nthat are in some sense optimal. It should also be fast. In general, type inference in System F is undecidable \n[Wel94]. But we are already in possession of a fully-typed term in a language lacking only usage annotations; \nwe need only infer the optimal values of these annotations. This task is much easier. The annotation \nsites are uniquely determined by the structure of the unannotated term, and there is a unique type derivation \nfor our annotated term according the the rules of Figure 5 (this is why we chose to present the rules \nin syntax-directed form). Thus inference proceeds in two steps: Given an unannotated term e and an initial \ncontext J? , we examine its type derivation and collect all the usage constraints in a constraint set, \n0. This is straightforward apart from the computation of the subtyping relation 4, which we discuss in \nSection 7.1. We then obtain the optimal solution of 0 (in a sense defined below), and apply it to e to \nyield an optimally-annotated term. We discuss this in Section 7.2. This two-pass technique, separating \nthe generation and so-lution of the constraints, is possible because all the usage variables are global; \ninference of bounded usage quantifiers (Section 6.3) would probably require local constraint solu-tion \nto occur during type inference. A nai ve definition of the subtyping relation would state that data T \nz = Ci 7ij q[c~, := ~kc] =$ qj[ak := &#38;] for all i, j -TE=$T&#38; (TYCON-NA I vE) However, this definition \nis not well-founded. A recursive data type (e.g., List CY) contains an instance of the head type (List \ncx) as one of the constructor argument types rij, and so the same clause appears in the premise and the \nconclusion of the (+TYCON-NA I vE) rule. It follows that a straight- forward use of this rule to generate \nusage constraints would go into an infinite loop. This behaviour is fairly simple to avoid in the case \nof regular data types [ACSS], but Haskell admits non-regular (also known as nested [BM98, Oka98]) data \ntypes in which a type constructor may appear recur- sively with different arguments from the head occurrence. \nIn order to compute the subtyping relation for general Haskell data types, we use the rule originally \npresented, (=$-TYCON). It turns out that (TYCON-NA I vE) holds, as a reverse impli-cation, in this system; \nthis fact is used in the proof of the substitution lemma (Lemma B.2). The relation cy E fw (o) can be \ndefined straightforwardly by induction on the structure of (T. In practice Vij (cQ E fvE(~iij)) can be \nprecomputed for all T, lc, P very early in the com-piler, since it depends only on the data type definitions \nand not on the code. Membership in the subtyping relation can then be computed in the obvious recursive \nfashion. 7.2 Constraint solution The constraint set 0 characterises all usage variable sub-stitutions \nsatisfying the type rules; in fact there is a non-standard notion of principal type based on 0, formalised \nin [TWM95a]. The constraint set comprises constraints of two forms, either II, 5 IL or u = w, and defines \na partial order on the set of usage variables extended with maximal and min- imal elements w and 1 respectively; \na solution must respect this partial order. We want the optimal solution, in the sense that we want as \nmany l-annotations as possible. This clearly exists: we let every usage variable not equal to w be set \nequal to 1. Finding the solution is straightforward due to the trivial nature of the constraints, and \ncan be done in time linear in the number of constraints. Due to the syntax-directed nature of the rules \nand the ex-plicit typing, the inference generates a constraint set of size approximately linear in the \nsize of the program text (as-suming data type declarations of constant size). Hence the analysis overall \nis approximately linear in the size of the program text. The approximation arises from the (M~AsE) rule; \nthe subtyping of the result type generates constraints proportional to the result type (which is not, \nspecified in the program text). Hencearbitrary nesting of case expressions inside case alternatives can \nyield arbitrarily large constraint sets. We do not expect this situation to arise in practice, but even \nin this case the constraint set is at most quadratic in the size of the program text. Figure 6 Extension \nof type rules to include configurations. 9 Status and further work H = 2i : f7i k-+ ei r, zj : oj t- \ne; : ~7: ui * ci for all i l?,x;:ai,~t-e:a OCCW(X~, e) + cjn=*O&#38;ur(~i ,ej) > 1 * ]fri] = u for all \ni (t-CONF) Syntactic soundness It is important to ensure that our type rules do indeed cor-respond to \nthe operational behaviour of the evaluator. Since our operational semantics deletes l-annotated variables \nfrom the heap after use (Section 4.1), an incorrect annotation of the program will eventually result \nin a stuck expression: a variable will be referenced that is not present in the heap, and reduction will \nnot be able to proceed. Hence, 2f we can demonstrate that a well-typed term never gets stuck, then we \nwill know that OUT annotations are truthful. Our plan for the proof is as follows. We use the technique \nof Wright and Felleisen [WF94]: We introduce a new typing judgement, koonf, that spec- ifies when a configuration \n[Section 4) is well-typed (Fig-ure 6). A configuration is essentially a letrec, and the type rule is \nessentially identical to (I--LETREc). How-ever, notice the additional type context G; these vari- ables \n(representing type lambdas under which we are cur- rently evaluating) scope over the expression but not \nover the heap, since the heap is not permitted to contain un-bound type variables (see Section 4.2). \nWe demonstrate subject reduction: if a well-typed term is reducible, it reduces to a term that is itself \nwell-typed, as a subtype of the original type. In order to do this we first derive from our natural semantics \nV_ (Figure 3) a set of small-step reduction rules +. We present these quite conventional rules in Appendix \nA and prove sound- ness and adequacy with respect to the natural semantics. Given these rules, subject \nreduction takes the form of the following theorem: Theorem 8.1 (Subject reduction) If ; t-cconf (H)e \n: u and (H)e -+ (H ) e , then ; t,,nf (H ) e : u , where 0 =$ u. Finally, we demonstrate progress: a \nwell-typed term is either reducible or a value; it cannot be stuck. Theorem 8.2 (Progress) For all configurations \n(H) e such that ; I--c,,~~ (H) e : u, either there exist H and e such that (H) e + (H ) e , or e is a \nvalue. Since our system possesses subject reduction and progress, it is type-safe: the types are respected \nby the operational semantics, and our annotations are truthful. The proofs follow the proofs of [TWM95b], \nwith a number of modifications and additions (notably progress). We give proof sketches in Appendix B; \nfull proofs may be found in the companion technical report [WPJ98]. We have presented a new usage type \nsystem, Usage% , which handles a language essentially equivalent to the intermediate language of the \nGlasgow Haskell Compiler. We have proved this type system sound with respect to the operational se-mantics \nof the language. In addition, we have presented an efficient and optimal type inference algorithm for \nthis sys- tem, enabling it to be hidden from the programmer. We have yet to prove this algorithm correct, \nbut we expect the proof to be straightforward. Our prototype implementation is implemented in approxi- \nmately 2000 non-comment lines of Haskell code, 800 of which is the inference proper. For comparison, \nthe GHC com-piler [GT98] is approximately 50000 lines of Haskell; the strictness analyser is about 1400 \nlines. It was useful to develop the prototype in parallel with the type system itself; several of the \ndesign decisions made in the evolution of the type system were clarified and motivated by our practical \nexperience with the prototype. There are a number of areas for future work. We expect it to be reasonably \nstraightforward to inte-grate the new analysis into the Glasgow Haskell Com-piler. However, integration \nwith GHC will force us to address a number of issues. GHC permits separate com-pilation. We propose to \nuse a pessimising translation on the inferred types of exported functions, giving them the most generally-applicable \ntypes possible. GHC also has unboxed types [PJLSla]. It is not yet clear whether our analysis should \ntreat these differently from other data types. It is possible to infer better types for multi-argument \nfunctions if we are sure they are never partially applied. Our implementation of UsageSP could possibly \nbe ex-tended to use a version of the worker/wrapper transfor-mation currently used by the strictness \nanalyser [PJLSla], yielding improved usage annotations in common cases. Mogensen [Mog98] augments his \nanalysis with a zero-usage annotation. Intuitively, zero-usages should occur infrequently; programmers \nare unlikely to write such ex- pressions and a syntactic dead-code analysis in the com- piler removes \nany that arise during optimisation. How-ever Mogensen shows that such information is useful when it refers \nto portions of a data structure. The full ramifications of adding a zero annotation to our analysis are \ncurrently unclear. Certain portions of it would require rethinking. For example, a syntactic oc-currence \nfunction is no longer sufficient: a variable may occur syntactically in a subexpression which is used \nzero times, and thus not occur for the purposes of the analy- sis. Additionally, as noted in footnote \n7, some significant modifications would have to be made to our proof tech- nique. Constraint solution \nover the larger class of con-straints generated by such a system is also more complex, possibly asymptotically. \nIt is unclear whether the advan- tages of greater precision would outweigh the disadvan- tages of a significantly \nmore complicated and expensive analysis. Acknowledgements We wish especially to thank David N. Turner \nfor a series of discussions in which key ideas of this paper arose. We also wish to thank Clem Baker-Finch, \nErik Barendsen, Gavin Bierman, Neil Ghani, Simon Marlow, Tom Melham, Tor- ben Mogensen, Chris Okasaki, \nValeria de Paiva, Benjamin Pierce, Andrew Pitts, Eike Ritter, Mark Shields, Sjaak Smet- sers, Philip \nWadler, Limsoon Wong, and the anonymous ref- erees for many other useful discussions and suggestions. \nReferences [AC931 Roberto M. Amadio and Luca Cardelli. Subtyping recursive types. ACM Transactions on \nProgramming Languages and Systems, 15(4):575-631, 1993. [ACM951 22nd ACM SIGPLAN-SIGACT Symposium on \nPrinciples of Programming Languages (POPL 95), San Francisco, California, 1995. ACM Press. [AFM+95] Zena \nM. Ariola, Matthias Felleisen, John Maraist, Martin Odersky, and Philip Wadler. A call-by-need lambda \ncalculus. In ACM [ACMSS]. [BM98] Richard Bird and Lambert Meertens. Nested datatypes. In Johan Jeuring, \neditor, Proc. 4th Mathematics of Program Construction, MPC 98, Marstrand, Sweden, number 1422 in LNCS, \npages 52-67. Springer-Verlag, June 1998. [BSSG] Erik Barendsen and Sjaak Smetsers. Uniqueness typ- ing \nfor functional languages with graph rewriting se-mantics. Mathematical Structures in Computer Sci-ence, \n6:579-612, 1996. [CG94] Pierre-Louis Curien and Giorgio Gheili. Coherence of subsumption, minimum typing \nand type-checking in F<. In Carl A. Gunter and John C. Mitchell, ed-it&#38;, Theoretical Aspects of Object \nOriented Pro-gramming, Foundations of Computing, chapter 8, pages 247-292. MIT Press, 1994. [Gil961 Andrew \nJohn Gill. Cheap Deforestation for Non-Strict Functional Languages. PhD thesis, University of Glasgow \nDepartment of Computing Science, Jan-uary 1996. [Gir95] Jean-Yves Girard. Linear logic: Its syntax and \nse-mantics. In Girard, Lafont, and Regnier, eds, Ad-vances in Linear Logic, no. 222 in London Math. Sot. \nLect. Notes Series. Cambridge University Press, 1995. [Go1871 Benjamin Goldberg. Detecting sharing of \npartial ap-plications in functional programs. In Gilles Kahn, ed- itor, Functional Programming Languages \nand Com-puter Architecture, Portland, Oregon, USA, number 274 in Lecture Notes in Computer Science, pages \n408-425. Springer-Verlag, September 1987. [GT98] The GHC Team. The Glasgow Haskell Compiler user s guide, \nversion 3.02. Distributed with GHC. Available http://www.dcs.gla.ac.uk/fp/software/ ghc/, April 1998. \n[Gus981 Jijrgen Gustavsson. A type based sharing analysis for update avoidance and optimisation. In ACM \nSIG-PLAN International Conference on Functional Pro-gramming (ICFP 98), 1998. [HM95] Robert Harper and \nGreg Morrisett. Compiling poly-morphism using intensional type analysis. In ACM [ACMSB]. [Jac94] Bart \nJacobs. Semantics of weakening and contrac-tion. Annals of Pure and Applied Logic, 69:73-106, 1994. [Lau93] \n[LGH+92] [Lin92] [Mar931    P bTl P Jw 981 [Oka98] [PJ92] [PJSG] [PJLSla] [PJLSlb] [PJL92] [PJPSSG] \n[San951 [SP94] [TWM95a] John Launchbury. A natural semantics for lazy eval- uation. In 20th ACM SIGPLAN-SIGACT \nSym-posium on Principles of Progmmming Languages (POPL 93), Charleston, South Carolina, pages 144- 154. \nACM Press, 1993. John Launchbury, Andy Gill, John Hughes, Simon Marlow, Simon Peyton Jones, and Philip \nWadler. Avoiding unnecessary updates. In Proc. 5th Glasgow Functional Programming Workshop, 1992. Patrick \nLincoln. Linear logic. ACM SIGACT No-tices, 23(2):29-37, Spring 1992. Simon Marlow. Update avoidance \nanalysis by ab-stract interpretation. In Glasgow Workshop on Func- tional Programming, Ayr, Springer \nVerlag Work-shops in Computing Series, July 1993. Torben i@. Mogensen. Types for 0, 1 or many uses. In \nChris Clack, Tony Davie, and Kevin Hammond, eds, Proc. 9th International Workshop on Implementa-tion \nof Functional Languages, St. Andrews, Scotland, September IOth-ldth, 1997, pages 157-165, 1997. Torben \nAX. Mogensen. Types for 0, 1 or many uses. Updated and corrected version of [Mog97]. Obtained from author, \n1998. Chris Okasaki. Purely Functional Data Structures. Cambridge University Press, 1998. Simon L. Peyton \nJones. Implementing lazy func-tional languages on stock hardware: the Spineless Tagless G-machine. Journal \nof Functional Progmm-ming, 2(2):127-202, April 1992. Simon L. Peyton Jones. Compilation by transforma- \ntion: A report from the trenches. In Hanne Riis Niel- son, editor, Programming Languages and Systems- \nESOP 96: 6th European Symposium on Program-ming, Linkiiping, Sweden, April 1996, number 1058 in Lecture \nNotes in Computer Science, pages 18-44. Springer-Verlag, 1996. Simon L. Peyton Jones and John Launchbury. \nUn-boxed values as first-class citizens in a non-strict functional language. In Proceedings of the 1991 \nCon- fernce on Functional Programming Languages and Computer Architecture, September 1991. Simon L. Peyton \nJones and David Lester. A mod-ular fully-lazy lambda lifter in Haskell. Software -Practice and Experience, \n21(5):479-506, May 1991. Simon L. Peyton Jones and David Lester. Implement-ing Functional Languages: \nA Tutorial. Prentice Hall International Series in Computer Science. Prentice Hall, 1992. Simon Peyton \nJones, Will Partain, and Andre San-tos. Let-floating: Moving bindings to give faster pro- grams. In ACM \nSIGPLAN International Conference on Functional Programming (ICFP 96), New York, 1996. ACM Press. Andre \nLuis de Mederios Santos. Compilation by Transformation in Non-Strict Functional Languages. PhD thesis, \nDepartment of Computing Science, Uni-versity of Glasgow, July 1995. Available as Technical Report TR-1995-17. \nMartin Steffen and Benjamin Pierce. Higher-order subtvnine. Technical Renort LFCS-ECS-94-280. Uni-versity \nof Edinburgh, February 1994. Also Univer-sit%t Erlangen-Niirnberg Interner Bericht IMMD7-01/94. Revised \nversion with Fun subtyping. David N. Turner, Philip Wadler, and Christian Mossin. Once upon a type. In \nConf. on Functional Programming Languages and Computer Architecture (FPCA 95), La Jolla, California, \npages l-11, 1995. Fiaure 8 Evaluation contexts. Contexts E ::= [] a case [] of Ci + ei n+[l [TWMSSb] \nDavid N. Turner, Philip Wadler, and Christian Mossin. Once upon a type. Technical Report TR-1995-8, Computing \nScience Department, University of Glasgow, 1995. Extended version of [TWM95a]. Available http://vvw.dcs.gla.ac.uk/people/ \nold-users/dnt/MossinTurnerWadlerTR95.dvi.gz. [Wad931 Philip Wadler. A taste of linear logic. In Mathe-matical \nFoundations of Computer Science, Gdansk, August-September 1993, proceedings, number 711 in Lecture Notes \nin Computer Science. Springer-Verlag, 1993. [We1941 J. B. Wells. Typability and type checking in the \nsecond-order X-calculus are equivalent and undecid-able. In Proc. 9th Ann. IEEE Symp. on Logic in Computer \nScience (LICS), pp. 176-185, Paris, 1994. [WF94] Andrew K. Wright and Matthias Felleisen. A syn-tactic \napproach to type soundness. Information and Computation, 115(1):38-94, 1994. [WPJ98] Keith Wansbrough \nand Simon Peyton Jones. Once upon a polymorphic type. Technical Report TR-1998-19, Department of Computing \nScience, Univer-sity of Glasgow, 1998. A Reduction rules As explained in Section 8, to support our subject \nreduction proof we derive from our natural semantics u (Figure 3) a set of small-step reduction rules \n+. These appear in Fig- ure 7, and are unsurprising. The (+-CTX) rule collapses a number of rules that \ndefine when we may evaluate a sub- term, using evaluation contexts E as defined in Figure 8. These reduction \nrules are provably equivalent to the natural semantics given earlier. Full proofs appear in the companion \ntechnical report [WPJ98]. B Syntactic soundness Subject reduction and progress are discussed in Section \n8. In this appendix we sketch the proofs of these results; full proofs appear in the companion technical \nreport [WPJ98].7 We commence by proving several lemmas dealing with spe- cial cases arising in the proof \nof the main theorem. Note Notice that our proof technique would be inadequate for a sys- tem that tracked \nzero-usages. In such a system, we could infer that z is used only once in the expression (z : lnt ct \nel, y : lntl H ez) fst (z, y) + snd (z, y); this reduces in two steps to an expression (y : lot c) ez) \nel + and (z, y) in which + appears without a binding in the heap, thus breaking our current definition \nof well-typedness. However, this could be remedied by the addition of a type rule that in the following \nwe use the convention that H = (xi : cr; ++ ei) and H = (yi : pi t-+ e ,). Also recall that + abbreviates \ndI1. Lemma B.l (Context pruning) 1ff,y:cro+e:oandoccur(y,e)=O,thenrbe:o. Context pruning allows us to \nremove an unused variable from the context. It is used to handle the (+-VAR-ONCE) rule. Lemma B.2 (Substitution) \nIf F, y : (~0, y : I$ + e : o where ~6 =$ (TO and occu~(y , e) > 0 + lobi = w, then l?, y : oh t- e[y \n:= y ] : CT where a < 0. Substitution is a key result, showing that if we substitute one variable for \nanother, the new a subtype of the old, the expression is still well-typed and has a type that is a subtype \nof the original type. This lemma is used for the (-+-APP) rule. Lemma B.3 (Nondeletion) If OCCW(Z~, e) \n+ Cyzl occw(xi, ej) > 0 * Ic7il = w for some i and (H) e +_ (H ) e then there is some y,&#38; - zi in \nH , and occur(yk, e ) + cjncl occ~r(yr,, e$) > 0 3 lprcl = w as before. The nondeletion lemma guarantees \nthat essential bindings are never dropped while evaluating inside an evaluation con-text (rule (+-CTX)). \nGiven the above lemmas, we show subject reduction. We do this by means of an invariance lemma which strength-ens \nthe inductive hypothesis. This is to handle the case (+-VAR-EVAL) where we must evaluate a binding inside \nthe heap: the binding is temporarily removed from the heap, but since the expression may recursively \nrefer to itself the vari- able must be placed in the context to retain well-typedness. Lemma B.4 (Invariance) \nIf l7; c kc., (H) e : D and (H) e -+_ (H ) e , then lY; c I-c,,f (H ) e : d, where o =$ u. Proof Proof \nis by induction on the depth of the inference of (H) e --+= (H )e , making key use of Lemmas B.l, B.2 \nand B.3. We present here two key cazes from the proof: (+-VAR-ONCE) and (-+TYABs). case (---VAR-ONCE): \n(H, x : D ++ e) x += (H) e where iI71= 1 By assumption we have that I?; z Fc,,~ (H,x : u ++ e) x : u \n(it is easy to show by (I--CONF) and (t--VAR) that it is the same u; we will omit this demonstra- tion \nhere and in the subsequent two cases). Hence by (k-CONF) we have that  r,xj:uj,,2:ukei:~: ui < Ui for \nall i (9) r,xj : uj,x : a,=!- e: CT u =$ u (10) The full ramifications of introducing zero-usages, however, \nare cur-rently unclear. Figure 7 Reduction rules for UsageSI?. ]u] = 1 ]u] = w (+-VAR-ONCE) (+-VAR-MANY)(H,a::atie)z+K((H)e \n(H,Ic:aI-,v)a:_)~(H 2:a ) IffI= w (H) e -kK (H ) e (H) e --+G (H ) e (-+-VAR-EVAL) (+-CTX) (H,2:oHe)z_t~(H \n,2:uHe )a: W) -W 4% W ) Ek l fresh 5 s = (xj) (-+-LETREC) (H) letrec zi : 7i u; = ei in e -kK (H, yi \n: (m . 5-i) +b AZ. ei[S]) e[S] (+-APP) (H) (kc : u . e) a += (H) e[z := o] (H) case Cj 7k al . a, of \nCi + fresh Q (H) ha . e -+ (H ) Aa . e occw(x;, x) + Cj =, occur-(xi, ej) + occur(xi, e) > 1 =+ ]Ui] \n= w for all i (11) OCCW(Z, z) + Cj =, OCCW(X, ej) + occ~r(z, e) > 1 * ]u] = w (12) By (12), the assumption \nthat ]u] = 1, and the fact that occzlr(z,~) = 1, we have that for all i occur(z,ei) = occ~r(z,e) = 0. \nHence, by Lemma B.l, (9) and (10) reduce to P, Xj 1 Uj b ei 1 Ui ~7: =$ ui for all i (13) l?,Xj :Uj,wke:U \nu + u (14) Since occur(xi, x) = 0 for all i, we have from (11) that cjn=r OCC?lT(Xi, ej) + OCCUT(Xi, \ne) > 1 * IUil = W for all i (15) and combining (13), (14), and (15) we have by (t--CONF) the result we \nrequire, I ; z kconf (H) e : u where u 4 u. case (+-TYABS): (H)Ra . e --s_ (H )Aa . e where (I-I) e[cx \n:= a ] --+= ,, (H ) e , a fresh By assumption and (k-TYABS) we have that I?; c kconf (H) Acu . e : (Va \n7) Hence by (k-CONF) we have that r,Xj : uj k ei : ai ~7: =$ ui for all i (16) I ,~,ckAa.e:(Vcr.~)~ (17) \nOCCUT(X~, Ao . e) + C, =, occur(xi, ej) > 1  (18) *(Uil=W foralli (-+-PRIMOP) (H) n1 + 722 +K (If) \nnl@ nz (+-CASE) ei -+= (H) ej al.. a, (+-TYAPP) (H) (ha. w) 7 -kK (H) o[a := T]  By (i--TYABs) from \n(17) we have that r,2j,z,a t-e[cx := CY ]: T~[(Y := a ] (1% Since occur(xi,e[o := a ]) = occ~(x;,Aa . \ne), (16), (19), and (18) imply by (~CONF) that I?; =,a kconf (H) e[cy := cr ] : T [CY := a ]. By the \ninductive hypoth-esis this implies that I? ; z, LY kconf (H ) e : ? where ? 4 r [cr := a ] (20) Hence \nby (~CONF) we have that r,yj : pj k e , : pi pi 4 pi for all i (21) l?,yj : pj,ak,a k e : P (22) OCCU?fyi, \ne ) + cy=n=l OCCUr(yi, ei) > 1 (23) *Jpil=W foralli By (I--TYABs) from (22) we have that I?, yj, k t- \nACY e : (t/a T )~ (24) and clearly (Va T )~ =$ (Va . ~[a := CX ])~ =a (Va . 7) . Since occUr(yi,e ) = \noccur(y;,Vc~ . e ), (21), (24), and (23) imply by (~CONF) that I?; EL kc,,f (H)Acr . e : (VCY T )~ , \nwhere (Va T )~ =$ (Vcy 7) as required. The remaining cases are similar. Subject reduction (Theorem 8.1) \nfollows immediately from invariance: let P = 0. Progress (Theorem 8.2) is clear by inspection of the \nreduction and type rules. 28  \n\t\t\t", "proc_id": "292540", "abstract": "", "authors": [{"name": "Keith Wansbrough", "author_profile_id": "81100225284", "affiliation": "Computing Laboratory, University of Cambridge, Cambridge CB2 3QG, England", "person_id": "PP38023996", "email_address": "", "orcid_id": ""}, {"name": "Simon Peyton Jones", "author_profile_id": "81100271851", "affiliation": "Microsoft Research Ltd., St George House, 1 Guildhall St, Cambridge CB2 3NH, England", "person_id": "PP40033275", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/292540.292545", "year": "1999", "article_id": "292545", "conference": "POPL", "title": "Once upon a polymorphic type", "url": "http://dl.acm.org/citation.cfm?id=292545"}