{"article_publication_date": "01-01-1999", "fulltext": "\n Stochastic Processes as Concurrent Constraint Programs (An Extended Abstract) Vineet Gupta Radha Jagadeesan* \nPrakash Panangadent vgupta@mail.arc.nasa.gov radha@cs.luc.edu prakash@cs.mcgill.ca Caelum Research Corporation \nDept. of Math. and Computer Sciences School of Computer Science NASA Ames Research Center Loyola University-Lake \nShore Campus McGill University Moffett Field CA 94035, USA Chicago IL 60626, USA Montreal, Quebec, Canada \n Abstract This paper describes a stochastic concurrent constraint language for the description and programming \nof concurrent probabilistic sys-tems. The language can be viewed both as a calculus for describing and \nreasoning about stochastic processes and as an executable lan- guage for simulating stochastic processes. \nIn this language pro- grams encode probability distributions over (potentially infinite) sets of objects. \nWe illustrate the subtleties that arise from the in- teraction of constraints, random choice and recursion. \nWe describe operational semantics of these programs (programs are run by sam- pling random choices), \ndenotational semantics of programs (based on labeled transition systems and weak probabilistic bisimulation), \nand prove soundness theorems. We show that Probabilistic CC is conservative over CC, thus justifying \nthe design of Probabilistic CC. We use the semantic study to illustrate a novel use of probability to \nanalyze a problem stated without reference to probability, namely the problem of indeterminacy in synchronous \nprograms. 1 Introduction This paper describes a stochastic concurrent constraint language for the description \nand programming of concurrent probabilistic sys-tems. The language is both a calculus for reasoning about \nMarkov processes and an executable language that can be used to program stochastic processes. 1.1 The \nNeed for Probability The traditional conceptual framework of programming languages is based on the assumption \nthat enough information will be avail- able to model a given system in as accurate detail as is needed \nso that appropriate causal, and maybe even determinate, models can be constructed. However, this assumption \nis violated in many sit- uations -it becomes necessary to reason and program with ap- proximate, incomplete \nor uncertain information. We consider three paradigmatic situations. *Research supported in part by a \nCAREER grant from NSF. +Research supported in part by NSERC. POPL 99 San Antonio Texas ~rsA 1999 I-581 \n13-095-3/99/01 Firstly, probability finds use as an abstraction mechanism to finesse inessential or \nunknown details of the system and/or the en- vironment. For exam le, [3] analyzes a component of the \nLucent Technologies SESS B telephone switching system that is respon- sible for detecting malfunctions \non the hardware connections be-tween switches. This component responds to alarms being gen-erated from \nanother complicated system that is only available as a black-box. A natural model to consider for the \nblack-box is a stochastic one, which represents the timing and duration of the alarm by random variables \nwith a given probability distribution. [3] then shows that the desired properties hold with extremely \nhigh probability. For another instance of modeling a complex environ- ment that is best done statistically, \nsee [24]. Secondly, consider model-based diagnosis settings. Often in- formation about failure models \nand their associated probabilities is obtained from field studies and studies of manufacturing prac-tices. \nFailure models can be incorporated by assigning a variable, called the mode of the component, to represent \nthe physical state of the component, and associating a failure model with each value of the mode variable. \nProbabilistic information can be incorporated by letting the mode vary according to the given probability \ndistri-bution [21]. The diagnostic engine computes the most probable diagnostic hypothesis, given observations \nabout the current state of the system. Thirdly, probability finds use as one of the tools in the design \nof efficient algorithms for communication and computation in dis- tributed unreliable networks. For example, \n[35] describes a class of scalable (probabilistic) protocols based on a probabilistic sys-tem model whose \ncosts grow slowly with the size of the system. A similar class of examples arise in the area of amorphous \ncom-puting [ 1, lo]. Here one assumes vast numbers of programmable elements amassed without precise interconnection \nand addressing; and solve control problems even though the individual entities are unreliable and interconnected \nin unknown and irregular patterns. Probabilistic methods are a basic tool used in the design of al- gorithms \nin this area, e.g. see [19] for probabilistic algorithms to compute maximal independent sets, [48] for \nprobabilistic algo- rithms to build hierarchical abstractions, and [171 for programming an amorphous \ncomputer. One can take the other extreme position and work with inde- terminacy and avoid the subtleties \nof measure theory and proba- bility theory. There are certainly situations where this is the rec- ommended \napproach, in particular when one has no quantitative information at all: using a uniform probability \ndistribution is not the same as expressing complete ignorance about the possible out- comes. However, \none often does have the quantitative information needed for a probabilistic analysis and then one can \nget far more information from a probabilistic treatment than from a nondeter- ministic treatment -[22] \nshows that the logical characterization of a probabilistic system is almost the same as in the deterministic \ncase and very different from the indeterminate case. 1.2 Design Criteria The above examples motivate \nthe following criteria on candidate modeling languages. Concurrency. The need for concurrency arises \nfrom two sources. First, note that concurrency arises inevitably in the first and third examples. The \nsecond example motivates a second source of concurrency -concurrency as a structuring mechanism to aid \nin modular construction of models -as motivated by the analysis underlying synchronous programming languages \n[8, 30, 111. Thus the intended stochastic language should permit the de- scription of concurrency and \ninteraction between concurrent pro-cesses. Furthermore -as earlier work on concurrent constraint programming \n[54, 501 has shown -concurrent composition is precisely modeled by conjunction. In the stochastic case \nwe now have much richer possibilities for interaction, namely interaction between stochastic processes, \nbut we do not want to model this by introducing a plethora of new combinators. Rather, we adopt as a \ndesign principle for our language that we have a single composi- tion primitive -traditional parallel \ncomposition -encompassing both interaction between stochastic processes and determinate pro- cesses. \nExecutability. The motivation for executability in a program- ming context such as the third example \nis evident. Furthermore, even specification contexts, such as the first two examples, ben- efit from \nan executable modeling language. At a foundational level, executability helps to bridge the gap between \na system and its model. Secondly, in the style of abstract-interpretation based program-analysis techniques, \nexecutability provides a systematic way for developing reasoning algorithms, e.g. in the stochastic con- \ntext, [2] uses the simulation engine of the language to develop an algorithm for maximum likelihood estimation, \nand [42] uses the lazy evaluation mechanism of their stochastic functional program-ming language to compute \nconditional probabilities. Constraints. One of the key motivations of the present work is to have a formalism \nthat allows one to work with physical sys- tems. In such systems the real numbers and associated continuous \nmathematics plays a significant role. Inevitably in working with such systems one has to work with approximate \ninformation, quite apart from the uncertainty in the dynamics. A natural way of ex- pressing this uncertainty \nis as partial information, or constraints. In traditional numerical analysis the notion of partial information \nenters implicitly in every discussion of approximation and error es- timation. However there one does \nnot often directly manipulate intervals qua approximate real numbers; a notable exception is, of course, \ninterval analysis [46] which influenced the early work by Scott on domain theory [57]. In this paper, \nwe work directly with a cpo of intervals as computational approximations to real numbers. A major part \nof the philosophy of concurrent constraint pro-gramming was putting partial information in the hands \nof the pro- grammer . In our setting we have captured both a qualitative notion of uncertainty, embodied \nin the use of partial information, as well as the evident quantitative notion associated with the use \nof proba- bilities. 1.3 Our Results We describe Probabilistic CC, an executable stochastic concurrent \nconstraint language, which adds a form of probabilistic choice to the underlying concurrent constraint \nprogramming paradigm, CC. Probabilistic CC is probabilistically determinate [47], rather than nondeterministic. \nProbabilistic cc is expressive, e.g. Probabilistic CC is expressive enough to encode (a variant of) Probabilistic \nPetri nets and probabilistic dataflow networks. We describe an operational semantics given by SOS rules \nand a notion of sampling. We show that a denotational semantics based on input-output relations cannot \nbe compositional; and provide examples to illustrate the impact of the non-monotonic character of the \nlanguage on (semantic) interpretations of recursion. We describe a denotational semantics based on weak \nbisimu- lation on constraint-labeled transition systems. It is necessary to work with weak (rather than \nstrong) bisimulation in a pro- gramming language context. We show that weak bisimulation is a congruence \nand is an adequate semantics for reasoning about the input-output relation of processes. The denotational \nsemantics when specialized to CC is fully abstract for cc. We show that Probabilistic cc is conservative \nover cc- thus our integration of probability is coherent with the underlying concurrent constraint paradigm. \n Our techniques suggest that probability has a role in the suite of techniques at the disposal of concurrency \ntheorists. As evidence, we show that our semantic study permits a new characterization of a problem stated \nwithout any reference to probability -the inde- terminacy issues of synchronous programming. Organization \nof paper. The next section describes a collec- tion of examples that illustrate the design of the language \nand some of its subtleties. We follow with a description of the operational semantics. The next section \ndescribes a sketch of the denotational semantics and contains soundness and conservativity theorems. \nWe conclude the paper with a comparison with related work. In this extended abstract, we content ourselves \nwith an overview of the results, and some key examples, and defer the de- tailed technical development \nto the full paper. 2 The language 2.1 Concurrent constraint programming The concurrent constraint (CC) \nprogramming paradigm [56] re-places the traditional notion of a store as a valuation of variables with \nthe notion of a store as a constraint on the possible val-ues of variables. Computation progresses by \naccumulating con-straints in the store, and by checking whether the store entails constraints. Several \nconcrete general-purpose programming lan-guages have been implemented in this paradigm [39,60], including \ntimed [53] and hybrid extensions [27] of CC that have been used for modeling physical systems [28,29]. \nA salient aspect of the CC computation model is that programs may be thought of as imposing constraints \non the evolution of the system. CC provides four basic constructs: (tell) c (for c a primitive constraint), \nparallel composition (A, B), positive ask (if c then A) and hiding (new X in A). The program c imposes \nthe constraint c. The program (A, B) imposes the constraints of both A and B -logically, this is the \nconjunction of A and B. new X in A imposes the constraints of A, but hides the variable X from the other \npro- grams -logically, this can be thought of as a form of existential quantification. The program if \nc then A imposes the constraints of A provided that the rest of the system imposes the constraint c -logically, \nthis can be thought of as intuitionist implication. This declarative way of looking at programs is complemented \nby an op- erational view. The basic idea in the operational view is that of a network of programs interacting \nwith a shared store of primitive constraints. The program c is viewed as adding c to the store in- stantaneously. \nThe program (A, B) behaves like the simultaneous execution of both A and B. new X in A starts A but creates \na new local variable X which is inaccessible outside A. The program if c then A behaves like A if the \ncurrent store entails c.  2.2 Constraint systems cc languages are described parametrically over a constraint \nsystem[54, 551. For technical convenience, the presentation here is a slight variation on earlier published \npresentations. The information added to the store consists of primitive con-straints which are drawn \nfrom a constraint system. A constraint system D is a system of partial information, consisting of a set \nof primitive constraints (first-order formulas) or tokens D, closed under finite conjunction and existential \nquantification, and an infer- ence relation (logical entailment) !- that relates tokens to tokens. k \nnaturally induces logical equivalence, written x. Formally, Definition 2.1 A constraint system is a structure \n(D, I- , Var, { 3~ 1 X E Var}) such that: . D is closed under conjunction(A); if a, b, c E D, then t-c \nD x D satisfies: - aka;akb andbAckdimpliesthataAct_d - aAbt_aandaAbl-b;at-b andal-cimpliesrhar al-bAc \n. Var is an in.nite set of variables, such that for each variable X E Var, 3x : D + D is an operation \nsatisfying usual laws on existentials: - at-3xa - &#38;(a A 3xb) z 3xa A 3xb - 3x&#38;a M 3y3xa - al-b+ \n3xak3xb A constraint is an entailment closed subset of 2). The set of all constraints, denoted 1DI, \nis ordered by inclusion, and forms an al- gebraic lattice with least upper bounds induced by A. We will \nuse U and n to denote joins and meets of this lattice. From now on we will use a, b, c, . . . to denote \nconstraints, noting that a token a can be represented as the constraint {b E D 1 a I-b}: such constraints \nare the $nite constraints, as they are the finite elements in the lat- tice. 3 lifts to an operation \non constraints. In any implementable language, I- must be decidable, and we also assume the the set V \nis countable. Examples of such systems are the system Herbrand (underlying logic programming) and FD \n[36](finite domains). In this paper we will work with a constraint system that in- cludes interval constraints \n[46]. Primitive constraints are built up (by conjunction and existential quantification) from tokens \nof the form z E [I, U] where 1, u are rationals: z E [1, U] constrains the variable z to lie in the closed \ninterval [1, u]. The entailment rela-tion on interval constraints is induced by inclusion of intervals \n-ZEltzEJifICJ. In addition, our constraint system will also include signal con- straints like On and \nS -these are 0-ary predicates, and telling such a constraint is like emitting a signal. 5 1 A . . . A \nS, b T iff Si = T for some i E l..n. 2.3 Syntax In this paper, we describe the integration of discrete \nrandom vari- ables in CC. This paper extends the results of [26] with recur-sion. We augment the usual \nCC syntax with a choose construct, choose X from Dom in P. Thus the Probabilistic cc syntax is as follows: \nDecl ::= e 1 g(X1,. . . ,X,) :: P 1Decl, Decl P ::= c I g(tl ,... ,tn) IifcthenP I P,PInewXinP I choose \nX from Dom in P Prog ::= Decl, P In this description, a Decl is a set of procedure declarations, with \ng a procedure name and Xl, . . . , X, a list of parameters. tl, . . . , t, is a list of variables or \nvalues. X is a variable and Dom is a finite set of real numbers. 2.4 Constraints on random variables \nRandom variables (RV) must be declared using the choose combi-nator. choose X from Dom in A models a \nfair coin toss that takes values from the set Dom -choose X from Dom in A reduces to A after choosing \na value for X from Dom. Furthermore, this com- binator has the scoping connotations of new X in A combinator-the \nvariable X is a new local variable and no information on it can be communicated outside this scope. Thus \nour RVs satisfy: RV Property 2.2 . RVs are hidden, i.e. not part of the observable results of the program. \n. Each RV is associated with a unique probability distribution. Example 2.3 Consider the following program: \nit has one RX X, with 4 equiprobable possible values. choose X from (0,1,2,3} in [ifX=OVX=lthena,ifX=2thenb] \n On input true , this program will produce output a with proba- bility 0.5, b with probability 0.25 or \ntrue with probability 0.25. Note that the visible outputs do not include X. Random variables, like any \nother variables, may be constrained. In fact this is a fundamental aspect of the entire framework. Each \nRV acquires a value according to some distribution and the choices are all made independently but the \noverall effect of the interaction between these choices is achieved by imposing constraints on the RVs. \nIn particular if a choice is made which conflicts with a con- straint then the inconsistent choices are \ndiscarded by the imple- mentation and the probabilities get renormalized according to the consistent \nchoices. Thus constraints may cause portions of the do- mains of the RVs to be eliminated. In such cases \nthe renormaliza- tion of the result yields the conditional probability of a given output given the valid \nset of outputs. Relatively complex joint distributions Input true to a program in the CC context means \nrunning the pmgmm in the unconstrained store, i.e. without external input. can be emulated by this mechanism. \nThe encoding of Probabilistic Petri nets in Example 2.8 requires constraints on RVs, as does the encoding \nof synchronous programs in Example 2.13. In order to understand how to interpret the probabilities re-ported \nin the formal semantics we think of execution in the follow- ing Monte Carlo style. We consider a program \nas standing for an ensemble of initially identical processes. Each process makes choices according to \nthe indicated distributions as it evolves. As processes be&#38;me inconsistent they are discarded from \nthe ensem- ble. At the end the probability reported is the conditional proba-bility of observing a given \nstore conditioned on seeing a consistent store. This is the sampling view of the process which is what \nthe semantics captures. Example 2.4 choose X from (0,1,2,3} in [X 5 2, ifX = OVX = 1 thena,ifX = 2 thenb] \nOn input true, this program will output a or b; true is not a valid output because of the constraint \nX 5 2. a is associated with 0.5 and b with 0.25; however to compute the probabilities, we must normalize \nthese numbers with the sum of the numbers associated with all outputs. This yields the probability 2/3 \nfor a and l/3 for b. We make the following assumption. RV Property 2.5 The choices of values for different \nRVs are made independently. Correlations between random variables are established by con-straints so \nwe cannot just say that the RVs are independent. Example 2.6 choose X from (0,1) in [X = z], choose Y \nfrom (0,1) in [if z = 1 then Y = l] There are a total of four execution paths. One of these paths (X \n= 1, Y = 0) gets eliminated because of inconsistent con-straints on the random variable Y, leaving three \nvalid execution paths. Normalizing the probabilities, we get the following paths (and associated probabilities): \nX = 0, Y = 0, z = 0(1/3), X = 0,Y = 1,~ = 0(1/3), X = l,Y = 1,~ = 1(1/3). Since the random variables \nare hidden, the visible outputs (and associated probabilities) are: z = 0(2/3), z = 1(1/3) The above \nexamples show how to get any finite distribu-tion with rational probabilities by just using fair coin \ntosses. For the rest of this paper, we will use the derived combinator: choose X from Dom with distribution \nf in P to indicate that the random variable X is chosen from Dom with distribution func- tion given by \nf. Consider the rational version of the probabilistic choice operator of [40]. Example 2.7 Let r be a \nrational. The probabilistic choice opera- tor of 1401, P +T Q, reduces to P with probability r and to \nQ with probability (1 -r). This combinator can be defined in Probabilis-tic cc as: choose X from (0,1) \nwith distribution {r, 1 - r} in [ifX=OthenP,ifX=lthenQ] where {r, 1 - r} represents thefunction f (0) \n= r, f (1) = 1 - r. X is not free in P, Q. Since the random variables are hidden, we get the expected \nlaws: P +r P = P(absorption), P +? Q = Q+c~-~) P (commutativity), (P+,Q)+, R = P+,, (Q+s(l--r) P R)(associativity). \nExample 2.8 We model the Probabilistic Petri nets described in [45, 621. In these nets, places are responsible \nfor the probabilis- tic behavioc while transitions impose constraints to ensure correct behavio,: Nets \nare l-safe, so a place may contain at most one to- ken. Temporal evolution is discrete (modeled here \nby a recursive call). At each time tick, a place with a token chooses randomly a transition which it \nwould like to send its token to, or choose not to send it anywhere. Similarly an empty place chooses \nwhich transi- tion to accept a tokenfrom, or accept no token at all. A transition constraint ensures \nthat either all preconditions and postconditions of a transition choose it, or none of them choose it. \nThen a new marking is computed, and the net starts over again. The program is shown in Figure 1. Note \nthe use of constraints on RV s to ensure that only correct runs are generated. P(S, t) :: /* S E [l..n] \nis the marking, t = time*/ newTl,...T,in{ I* Ti will be the transition chosen by place i to send a token \nto or receive from. */ /* Select a transition for each place */ if (i E S) then choose X from Prei with \ndistribution f; in Ti = X, I* Prei is the set of transitions of which i is a precondition. Also, 0 E \nPrei, signifying no choice. *I if (i $Z S) then choose X from Posti with distribution gi in Ti = X, I* \nAs above, Posti is the set of transitions of which i is a postcondition, and 0 E Posti. *I ... 7 /* Transition \nconstraints -one for each transition. Contraint for transition i. il, . . . , i.+ are its pre and postconditions. \nIf one of its pre or postconditions chose transition i, all must choose it. */ if (Ti, = i V . . . V \nTi, = i) then {Ti, = i, . . . , Ti, = i} ... 7 /* Compute the next marking *I new news in { if ((1 E \nS A TI = 0) V (1 $! S A TI # 0)) then 1 E news, if ((1 $! S A TI = 0) V (1 E S A Tl # 0)) then 1 6 news, \n... , P(newS, t + 1) I* next instant *I ? Figure 1: Probabilistic Petri nets as Probabilistic cc programs. \n2.5 Recursion and limit computations Recursion increases the expressiveness of our language by allowing \nthe programming of continuous distributions. Example 2.9 Consider the program U(1, U, 2) :: z E [I, U], \nchoose X from (0, l} in [ if X = 0 then U(1, (U + 1)/2, z), if X = 1 then U((u + Z)/2, u, z)] This program \ncan be visualized as a full binary branching tree, where the branches correspond to the two possible \nvalues for the random variable of each recursive call. Each internal node of the tree can be associated \nwith the interval that z has been constrained to be in. Thus, in the limit, at the leaves of the tree, \n.Z gets con- strained to be a real number. Furthermore, the induced probability distribution is on infinite \nbinary sequences satisfying the condition that each finite sequence has the same probability of being \nextended by a 0 or by 1. By classical results, this is the uniform random dis- tribution on z over [0,l](see \n[4] Page 19.5, Prob. 8). In the rest of this paper, we will use U to stand for the program defined in \nExample 2.9. The following example conveys the flavor of use of probability in the programming of large \narrays of simple devices. This example satisfies all the properties required of an amorphous computer \n[ 191. Example 2.10 Suppose we have a large array of tiny light emitting diodes (LEDs). Assume that each \nLED can be switched on or ofi We would like to produce light of a certain intensity. One method is to \nswitch on a fraction of the LEDs by telling each LED to switch on or ofl This method requires each LED \nto have a unique iden-tity. A more eficient way is to let each LED light up with a certain probability. \nThe central limit theorem [4] ensures that the inten- sity will be proportional to the probability. The \nprobability can be communicated to the LEDs via an electricpotential, thus replacing individual messages \nby a broadcast communication. new X in [U(O, 1, X), if (X < Potential) then On] A clever implementation \nwould need to unwind the recursion in U onlyfinitely (almost always). Furthermore, this method allows \none to compensate for a broken fraction of LED s by increasing the potential suitably. How do we compute \nprobabilities in the presence of recursion? We use a computationally motivated analogy to the limiting \npro-cess [59] that computes conditional probabilities in measure the-ory. This subtle limiting process \nplays a key role in our theory. The important point here is that we can have situations where the prob- \nability of an inconsistent store is 1, i.e. the conditional probability is undefined according to classical \nmeasure theory. In our theory we can sometimes define these probabilities by taking into account the \nway in which the computational approximation proceeds. We begin with a seductively simple warm-up example \nthat il- lustrates our techniques. Example 2.11 Consider P = [U(O, 1, z), z = 0] Intuitively we would \nexpect its output to be .Z = 0 with probability 1, since after all the only possible output of this program \nis z = 0. Note however, that the probability of z = 0 in the above program is 0, so a naive calculation \nat normalization time leads us to calculate o/o. We handle this problem by computing the required probabil- \nities for a recursive program as a limit of the probabilities of its finite unwindings. Consider the \nprogram P, = [U,, z = 0] got by replacing the recursion in U by a finite unwinding U,. U, can be viewed \nas a finite subtree of the tree associated with U, see ex- ample 2.9. P, is a finite program operating \non a finite discrete probability distribution. Each of these finite programs yield output I = 0 with \nprobability 1. Thus, the result is z = 0 with probability 1. Apropos, this example also illustrates the \nneed for these tech- niques -the above problem arises anytime a random variable is constrained. The limiting \nprocess yields expected answers in nor- mal situations. Example2.12 Consider P = U(O,l,z),if (l/4 < z \n< 314) then S where S is a signal. Let us compute the probability of S being present -we expect the answer \nto be I/2. Consider a finite unwinding U, of U; the re- quired answer for this unwinding is the sum of \nthe sizes of the finite intervals contained completely in the interval (l/4,3/4). Consider the set of \nreal numbers that are answers deduced at the finite un- windings. The only limit point of this set of \nreals numbers is l/2. This can be visualized as follows. Take any directed set of finite subtrees of \nthe tree associated with U (we are thinking of trees un- der the prefix ordering on trees), whose limit \nis U. Then, the limit of the probabilities associated with this directed set is l/2. The above example \ntells us when probability numbers are well defined -any way of unwinding the various recursions in a \nprogram should yield the same answer. Do limits exits al- ways? Unfortunately not! These subtleties are \nillustrated by the next set of examples that deal with program combinators remi-niscent of indeterminacy \nissues in synchronous programming lan-guages [30,8,12,31,25,33,53]. Example 2.13 Consider new X in [U(O, \n1, X), ifathenX=O,ifX>Othenb] This program can be thought of as if a else b, i.e. if a is not present, \nproduce b with probability 1. If a is present, b is not produced. In essence, we use the RV to be certain \nthat b will be produced; however, this expectation can be denied on the production of a. The analysis \nof this program proceeds as follows. On input true, the program produces constraint b with probability \n1 and true with probability 0. On input a the only output of this pro- gram is a. Indeed in this latter \ncase, computation of the probability proceeds similar to Example 2.11 -the probability prior to nor- \nmalization is 0, which seems to lead to a O/O case -however, our analysis tells us that the probability \nis 1. The next example shows how indeterminacy issues in syn- chronous programming languages show up \nas problems with the limit computation of probabilities. For example, the program if a else b, if b else \na has two potential outputs a and b on input true . Example 2.14 Consider P = if a else b, if b else \na. A simple calculation shows that on input true the outputs are a, b or true, with all non-normalized \nprobabilities being 0. However, in this case the limiting process does not produce unique answers. The \nformal calculation proceeds as follows. Consider approximations if a elskb to if a else b. if a elsenb \nstands for the program got by unwinding the recursion in U n times. A simple calculation shows that if \na else,b behaves as fol- lows: if a is not present, produce b with probability (1 -2-n). If a is present, \nb is not produced. Now consider approximations P,,, = if a else,b, if b else,,u to P. On input true the \nnon-normalized probabilities of a and b are 2-m (1 -2- ), 2- (1 -2- ). Now the limits of the nor- malized \nprobabilities (resp. &#38;&#38;, &#38;$&#38;) depend on the relative rates of m, n approaching CQ, i.e. \nthe limits depend on the speed of unwinding of the two recursions involved. In our theory, this program \nwould thus not have a defined result. Hence is rejected by the compilers for synchronous programming \nlanguages. 3 Operational semantics We first describe a SOS style transition system for finite (recursion- \nfree) Probabilistic cc programs (as in [26]), and follow it with a formalization of the limit constructions \n(alluded to earlier) for handling recursion. 3.1 Transition relation of recursion-free programs. We follow \nthe treatment of CC. We assume that the program is operating in isolation -interaction with the environment \ncan be coded as an observation and run in parallel with the program. A configuration is a multiset of \nprograms I . cr(I ) will denote the store in the multiset -it is recoverable as the conjunction of the \n(tell) primitive constraints in r. or ka l?,lfathenB --_) l ,B I?, new X in A + r, A[Y/X] (Y not free \nin I ) I , choose X from Dom in A ---+ I?, Y = T, A[Y/X] r E Dom, Y not free in F Consider the finite \nset of consistent quiescent configurations of A on any input a, i.e. {I i 1A,a +* l?i ++,a(ri) it: false}. \nThe unnormalized probability of ri is determined by the RVs in u(l?i) -the transition sequence(s) to \nl?i do not play a role in the calculation. The unnormalized probability is IIIpl/lDomy ( where ? is the \nset of RVs, Y = ry E l?i, and Domy is the domain of Y. The (finite set of) outputs of a process P on \nan input a, denoted OpsemIO(P, a) is given by hiding the random variables and new variables in the set \nof o(l?i). The probability of an output o, written Pr(P, a, o) is computed as follows: . For each output, \ncompute the unnormalized probability by adding the probabilities of all configurations that yield the \nsame output. . Normalize probabilities of the set of outputs. Define - . PT-(P,a,o) = C{Pr(P,a,o ) 1 \n0 E OpsemIO(P,a),o c o}. E(P, a, o) is a cumulative probability interpreted as the probability that the \noutput of P on a is at most o. Pr can be recovered from x by an inclusion-exclusion principle. 0 Pr(P, \na, 0) = C{Pr(P, a, 0 ) I 0 E OpsemIO(P, a), 0 E 3. P,(P, a, o) is a cumulative probability interpreted \nas the probability that the output of P on a is at least o. Pr can be recovered from fi by an inclusion<xclusion \nprinciple. 3.2 Handling recursion. We first generate all syntactic finite approximations to the given \nprogram. Each of these finite programs is executed using the above transition relation. We then describe \nthe limit calculation for the probabilities. The partial order, (App(P), 5~) describes the syntactic \nap-proximations of a Probabilistic CCprogram P. +, the partial or- dering is intended to capture refinement \nof approximation. App(P) is defined by structural induction on P. For a recursive program, App(P) is \nconstructed by considering the set of approximations of all finite unwindings with ordering induced by \nthe (a) match order- ing on recursive approximations [44], i.e. an expression is refined by replacing \nthe least program (in our case true) by an expres- sion. For a sample of other cases: APP@) = {cl APP(&#38;, \n-42) = {(A;, -412) I A: E APP(&#38;}, ordering induced by the Cartesian product of -&#38; App(P) has \nthe expected properties. Lemma 3.1 App(P) is a directed set. Let c be an input. Then, c induces a function \non App(P) that maps Ai E App(P) to OpsemIO(A;)(c). If Ai 3App(P) A:, then (Vdi E OpsemIO(A:)(c)) (3di \nE OpsemIO(Ai)(c))di E di. The outputs of P on c are determined by a limit computation -d is an output \nof P on c if, there exists a monotone function Fd mapping App(P) to constraints such that Fd(.&#38;) \nE OpsemIO(Ai)(c), and d = ui Fd(Ai). 3 We now turn to computing the probability numbers. Let d be an \noutput of P on c. Then, d maps each Ai E App(P) to the cumulative probability E(Ai, c, d). Since App(P) \nis a directed set, we use the standard definition of convergence (e.g. see [4], page 371). The cumulative \nprobability of the output d equals T if: -(Ve) (3A E App(P)) (VA ) [A 5 A + IPr(A , c, d) -rj < E] V \nmay not converge in general. We say that the output is defined for the program P on a given input c, \nif the cumulative probability is defined for all outputs of P on c. We first explore the relationship \nof the above definition to the two notions of cumulative probabilities discussed in the preceding subsection. \n-Relationship to Pr and pr Example 3.2 Let P be a Jinite process. Then the set of outputs given by the \nabove dejinition on input c is OpsemIO(P, c); the out-put is defined on c and the de$nition yields K( \nP, c, d) for all d E OpsemIO(P, c). The inclusion-exclusion style argument underlying the recovery of \nprobability (Pr) from cumulative probability (Pr) can be used for programs that satisfy the following \ncountability condition -for any input c, any output d satisfies the condition that the cardinality of \nthe subset of outputs e C d is countable. This condition is satisfied by all our earlier examples. Example \n3.3 On Example 2.12 for input true, the de$nition yields cumulative probability 1for output S, 0.5 for \noutput true; thus yielding 0.5 for both probabilities. Let e be a constraint. Then, e induces a function \nthat maps Ai E App(P) to &#38;(A;, c, e). Since App(P) is a directed set, we can once again use the standard \ndefinition of convergence and define &#38;( P, c, e) equals r if:  (VE)(3A E App(P)) (VA ) [A 5 A + \nI&#38;(A , c, e) -rl < E] Again, convergence is not assured. We have the following relation- ships between \n&#38;(P, c, e) and the cumulative probability of out- puts of P on c. We can prove the following fact: \nThere is a monotone function Fd satkfying the properties above, such that d = lJi Fd (Ai)iff there is \na derivation starting from A whose output is d, where the output of an infinite derivation is the lub \nof the outputs of its prefixes. Let P be a finite process. Then, the above definition of &#38;(P, c, \ne) agrees with the definition of fi from the pre- ceding subsection. Let P be a program such that its \noutput is defined on input c. Furthermore, let pT(P, c, e) be defined for all finite e. Then, -the cumulative \nprobability Pr( P, c, d) of an output d can be recovered from thee information by an inclusion-exclusion \nprinciple on the (finite) constraints e such that d 2 e. Interpreting the probability numbers. How are \ntheseprob- ability numbers to be interpreted? Recall that we say that there is an ensemble of processes \neach of which executes a copy of the Probabilistic cc program. The probability numbers are interpreted \nstatistically with respect to this ensemble, conditional on the pro- cess being consistent. The next \nfew examples illustrate what we have in mind. Example 3.4 Let P be a Probabilistic CC program in which \nrun-dom variables are read only , i.e. no constraints are imposed on random variables; they are only \nqueried in asks. (This class of pro- Eums includes all programs from [42].) In this case, the function \nPr( ,c, d) as defined above is u monotone, decreasing, bounded (by -0) function on App(P). Thus, Pr(, \nc, d) converges, and the output of P is defined for all inputs. Recall example 2.14 for examples of non-convergence. \nLet A be a Default cc ( synchronous , [53]) program and A be a Proba-bilistic CC program obtained from \nA via the definition for else from example 2.13. Then, Theorem 3.5 If A has multiple outputs on an input \nc, the output of A is not dejned for c. If A is determinate, the output of A on any input c is the the \noutput of A on c with probability 1. The next couple of examples illustrate the fact that our theory \nagrees with the answers produced by standard probability theory, when the standard theory produces defined \nanswers. Example 3.6 Let f : [a, b] -+ [c, d] be a Riemann integrublefinc-tion. Consider the program \nP :: U(a, b, XL U(c, d, Y), if (Y < f(X)) then A, if (Y > f(X)) then B What is the probability of the \noutput A? Standard probability theory tells us that if R = (b -a) * (d -c), it should be Now consider \nthe set of approximations to(l/R) x J,U -c). P. Each approximation defines a partition on [a, b] and \na par- tition on [c, d], and successive approximations refine these parti- tions. Thus the probability \nnumber for signal A corresponds to a lower Riemann sum, and similarly the probability number for B corresponds \nto an upper Riemann sum. It is now easy to show that P(A) + $c(f -c) and P(B) --+ 1 - $s ,(f -c), thus \nour computations agree with standard probability theory. These results extend to functions of many variables; \nand U can be replaced by any program that produces the uniform distribution, such as V in Example 3.9 \nbelow. Example 3.7 The following program emits the signal notC if z is not an element of the Cantor set. \nC(1, u, 2) :: C(k (U + 2913, z), C((2 LL + 1)/3, 11, z), if ((u + 21)/3 < z < (2~. + 1)/3) then notC \nNow, consider the program: P :: U(0,1,W, C(0,1, X) The probability of the output notC as per our theory \nis 1, in con- formunce with standard probability theory. In the case that a constraint forces the choices \non a random variable to be inconsistent with probability 1 we get a situation in which conventional probability \ntheory has no answer. This is the situation with examples 2.13 and 2.14. Standard probability theory \nwould say that if we condition with respect to a set of mea- sure zero the resulting conditional probability \nis undefined-this is true both for discrete (countable) systems, and for continuous state spaces, where \nthe conditional probabilities are computed based on some notion of derivative of measures, for example \nby use of the Radon-Nikodym theorem [4,59]. We can however ascribe a limit- ing probability based on \nthe structure of the computational process. This additional information allows us to associate sensible \nproba- bilities in situation where conventional theory leaves the numbers undefined. Perhaps a better \nway of saying it is that probability the-ory leaves the possibility of augmenting the information to \ncome up with a sensible conditional probability. The limit formula above is defined to capture exactly \nthe computational information that goes into the definition. Example 3.8 Consider: new X in new Y in \n[U(O, 1, X), U(0, 1, Y), X = Y] This should and does yield a uniform distribution on the diago- nal of \nthe unit X -Y square. This is intuitively plausible but completely at variance with probability theory. \nThat is at it should be. If one were to say that there are two independent uniform dis- tributions on \nX and Y then the question what is the distribution given that X and Y are equal? is not answerable. In \nour case the operational model shows how the uniform distribution is obtained by successive approximation. \nThe calculation based on the limit formula is exactly the embodiment of this idea. It is important to \nnot mistake the intent of the above discussion. The numbers that we compute do not depend on the details \nof the execution. Thus in the example just above we are claiming that even if the recursions in the two \ncalls to U are unwound at very different rates we get the same answer. An explicit calculation verifies \nthis easily. However if we took two d$erent processes both generating the uniform distribution we get \na very different answer. Example 3.9 Consider a variant of the program U: V(l, u, z) :: z E [l, 4, choose \nX from (0,1) with distribution {l/3,2/3} in [ if X = 0 then V(1, (21+ u)/3, z), if X = 1 then V((21+ \nu)/3, u, z)] This program chooses 0 with probability l/3 and 1 with probability 213. It then subdivides \nthe interval into two unequal portions -one twice the size of the other - and recursively calls itself. \nThis also produces the uniform distribution in the limit. Now consider U(X, 0, l), V(Y, 0, l), x = Y \nWe do not get the uniform distribution along the diagonal. It is easy to verify that the distribution \nassigns to the subinterval (0,O.l) a smaller probability than it does to the interval (0.9,l). The actual \ndistribution is quite fascinating (see [14, pp. 4081) but does not concern us further here. What is important \nis the fact that we got two very different answers to the question if X and Y are uni- formly distributed, \nwhat is the distribution given that X and Y are equal? when the programs generating the uniform distributions \nare different. Here we see why the conventional probability the-ory answer is sensible; without further \ninformation one can get an almost arbitrary answer. However our semantics provides exactly this additional \ninformation. 4 Denotational semantics 4.1 CC semantics We begin with a brief review of the model for \nCC programs, refer- ring the reader to [54] for details. In our treatment of cc, we will consider3nite \nprograms first. We describe recursion (in parenthet- ical remarks) by working with the sets of programs \nobtained by unwinding the recursion. An observation of a CC program A is a store u in which it is quiescent, \ni.e. running A in the store u adds no further information to the store. Formally we define the relation \nA$ , read as A is quiescent on IL, with the evident axioms: e (if c tienz) _1 The denotation of a program \nA can be taken to be the set of all 2~ such that A 4 . The semantics is compositional since the axioms \nabove are compositional. The output of A on any given input a is now the least 21 containing a on which \nA quiesces. (If A does not become quiescent on a, which might happen with a recursive program, then we \nhave to take the lub of the stores that are produced by the finite unwindings of A on a.) 4.2 The problems \nWe turn now to Probabilistic cc. The input-output relation is not compositional. The following example \nis inspired by Russell s sim- plified version [52] of the Brock-Ackermann anomaly [16]. Example 4.1 Consider \nthe programs A1 = S1, if RI then S2, if RI A R2 then S3 A2 = ,751,if RI then SZ, if RI A RZ then S4 Ax \n= if RI then S1, if RI A Rz then S2 A S4 A4 = if RI then S1 A SZ, if RI A R2 then S4 As = if RI then \nS1, if RI A R2 then 5 2 A S3 Si, Ri are signals (see example 2.10). De&#38;e: B1=ifX=1thenAl,ifX=2thenAz, \nifX =3thenAa,ifX =4thenA4 Bz=Bl,ifX=5thenAg Pl = choose X from { 1,2,3,4} with distribution {0.2,0.1,0.5,0.2} \nin BI P2 = choose X from {1,2,3,4,5} with distribution {0.1,0.2,0.4,0.2,0.1} in B2 PI and P2 have identical \ninput/output behavior: Key cases are: Let Q = if S1 then RI, if 5 2 then R2. PI, Q on input true out-puts \nSI AS2 A&#38;(0.2), &#38; AS2 A&#38;(0.1), true(0.7); Pz, Q input true outputs S1 A 5 2 A Ss(O.l), SI \nA S2 A &#38;(0.2), true(0.7). Thus PI and PZ can be distinguished by the context Q. Our solution to this \nproblem is to make Probabilistic CC and CC amenable directly to some of the standard techniques of con- \ncurrency theory via a key technical innovation, CLTS (constraint labeled transition systems) with weak \nbisimulation, described in greater detail below. Unfortunately, more problems loom. The mixture of probabil- \nities and constraints violates basic monotonicity properties needed in standard treatments of recursion \n-these problems were indi- cated by the synchronous programming examples. Example 4.2 Consider P :: choose \nX from (0,1) with distribution {0.2,0.8} in [X = z, Trim(l)] Trim(Y) :: Trim(l -Y), ifz=Ythen choose \nX from (0,1) with distribution {0.9375,0.0625} in X = 1 Unwinding Trim 0 times yields the approximation \nPO = (.z = 0) +0.2 (z = 1) to P. Unwinding Trim once yields the approx- imation PI = (z = 0) +0.8 (z \n= 1) to P. Unwinding Trim twice yields the approximation PO again. In a standard monotonic leastjixed \npoint treatment of recursion, the denotation of the n th unwinding is less than the denotation of the \n(n + 1) st unwinding for the purported order C on programs. Thus, PO E PI E PO , forcing PO = PI, an \nequation that is unsound. The subtle interaction of recursion and normalization underlying this example \nis not handled by our study. We however show how to handle the issue of normalization in the case of \nrecursion free programs. 4.3 CLTS We begin with the intuitions underlying our definitions. Let P be a \nprocess. A transition system with transitions given by the oper- ational semantics of CC is informally \ngiven in the following way. Each state of the system intuitively corresponds to a constraint store. There \nis an initial state, SO, intuitively this corresponds to the store (true). The transitions are labeled \nin one of three ways: T, c! and c? where c is a finite constraint. The c! and c? labels represent interactions \nwith the environment and are deterministic, i.e. any state has at most one transition labeled with a \nc! or c?. A transition s c! s means that the system outputs the constraint c to the environment and adds \nit to the store. If c was already in the store, s would be s, as the system can always output any constraint \nalready in the store. A transition s % s means that the system reads the finite constraint c from the \nenvironment and adds it to the store. If the system contains a top-level choose, the actual choice will \nnot be visible to the environment and the transition is labeled with r and the probability. CLTS closure \nconditions make them a probabilistically de-terminate system in the context of CC computation. . Probabilistic \ntransitions: Only T-transitions can be asso- ciated with probabilities, and the associated probability \nmust be strictly positive. Acyclicity: The only cycles in the transition system are l- cycles of non-probabilistic \ntransitions. All states are reach- able from the start state. Determinacy: for every state s and every \nlabel c? or d! there is at most one transition from s with that label. Receptivity: In every state s \nand for every finite constraint c there is a transition labeled c?. Commutativity: If there is a state \n51 and transitions s1 4 s2, ~1 &#38; ss, where at least one of c, d is a non-probabilistic transition, \nthen there is a state sq and transitions s2 3 ~4, s3 a s4. Masking: If there is a transition s -% s then \nany transition of the form s --% s has s = s . Masking is needed for the (forthcoming) definition of \nparallel composition. Splitting: if c and d are constraints and we have a transition s (c Ll d)! } s \nthen there is a state s and transitions s &#38; s and s d! s . Ibansitivity: if s % s and s 2 s then \nthere is a (c U d)? transition s + s . Also, ifs c! s and s d! s (cud)! ,, then there is a transition \ns s . Saturation: if there is a transition s % s , then there are transitions s d! s for every d such \nthat c _> d. Thus every query resolved by the constraint solver is represented by an action in the transition \nsystem. Tau-finiteness condition: There are at most finitely many 7 transitions from any state. The sole \nsource of non-confluence in a CLTS is the (possible) non-commutativity of conflicting probabilistic transitions \n-non-probabilistic transitions commute with all other transtions. It also ensures that adding new information \ncannot disable an already enabled transition, and ensures the content of Lemmas (353.6) of [54]. These \nconditions ensure that the set of output constraints leading out of a state forms a directed set; and \nensure that the set of output constraints in the labels on all the transitions between any two states \nforms a directed set. Path-equivalence: A CLTS comes equipped with a state-indexed equivalence relation \nZ~ on paths starting from the same state s in a CLTS: intuitively two paths are equivalent if they agree \non the random variables. Each equivalence class comes associated with a probability, corresponding to \nthe choice of the random vari- ables. =s satisfies conditions such as: . If t is a non-probabilistic \ntransition and p . t is a path starting from s, then p. t ss p. . If paths pl , p2 both start in s and \nend in the same state s , then p1 =a p2. We say a CLTS is finite if intuitively it encodes only finitely \nmany probabilistic choices. Formally, we say a CLTS is finite if: . the number of equivalence classes \nin Ed,,, where SO is the start state, is finite; and For a finite CLTS, the number of equivalence classes \nin any state is finite, and the probability of any equivalence class is strictly pos- itive. For the \npurposes of this paper, we restrict our attention to finite CLTSs. Example 4.3 The CLTS for true is constructed \nas follows. We will assign labels to the states for notational convenience. For each constraint c, there \nis a state with label c. The start state is the state with label true. A state with label c has self-loops \nlabeled d! for allJinite constraints d such that c > d. There is transition labeled e?fYom a state with \nlabel c to a state with label d if d is equivalent to c A e. For any state s, =s consists of one equivalence \nclass consisting of all paths starting from s. This class has probability 1. Example 4.4 The CLTS for \na program that emits c is obtained from the CLTS for true as follows. For each transition with la-bel \nd? such that c > d, aad a transition labeled d! with the same sounze and target states. The following \nexample extends the ideas of examples 4.4 and 4.4 to all determinate CC programs. Example 4.5 Let (E, \niI) be an algebraic lattice in which false is a compact element. Then, any continuous closure operator \nf4 can be encoded as a CLTS as follows. Start with a copy of the CLTS for true. Close the CLTS under \nthe addition of following transitions: aa d a transition d! from a state with label c to a state with \nlabel c u d if d E f(c). Example 4.6 The CLTSfor the program c +0.5 d (emit one of c or d with probability \n0.5 each) is as follows. The set of states is the disjoint union of the set of states of c, d (as given \nin Example 4.4) and the set of states of a copy of truelfrom Example 4.3). Each component of the disjoint \nunion retains all its transitions. To each state, say with label e coming from the copy of the CLTSfor \ntrue, add two probabilistic r transitions each with probability 0.5, with target the states with the \nsame label in CLTSs for c and d. The states s coming from the CLTS for c (resp. d) retain the associated \nEd. For each state s coming from the copy of true, there are thefollowing three equivalence classes in \nzs. These three equivalence classes correspond to the three possible cases of the probabilistic choice. \nChoice not resolved: this equivalence class is the set of paths starting that only visit states from \nthe copy of true and has probability number 1. Choice resolved in favor of c: this class is the set of \npaths that visit some state from the copy of the CLTS for c and has probability number 0.5. Choice resolved \nin favor of d: this class is the set of paths that visit some state from the copy of the CLTSfor d and \nhas probability number 0.5. Consistent states: A consistent state of a CLTS is intuitively a state which \ndoes not already entail inconsistency. The inconsis- tency of such a state is witnessed by a maximal \npath that does not have a false! labeled transition. We formalize this idea below. Let P be a path from \nstate ~0. Let the transitions in P be to, t1 . . . with transition ti (of label ei) going from state \nwi to state w+l. Then, a path P from wo is a T extension of P if P # P and the following hold: (1) the \ntransitions in P are 7, tb, t:, . . . , with ti going from wi to w:+~, (2) t!, has the same label as \nti, and 4f : E -+ E is a closure operator if f is monotone, 2 &#38; f(z) and f(f(s)) = . there are no \npaths with infinitely many occurrences of 7 f(z). (3) Vi there is a transition labeled r from wi to WI. \nA path P, all for c from example 4.4. These two CLTSs are bisimilar The wit- of whose suffixes have no \nr extension, is called r-maximal. nessing binary relation relates all consistent states with the same \nLet s be a state of a CLTS. We say that s is consistent if there label. is a path from s which satisfies \nthe following: (1) all transitions on Example 4.10 Consider the CLTS for c +0.5 false (based on the path \nare labeled ! or 7, (2) the path is r-maximal (3) if any c! example 4.6, but with false taking the role \nof d) and the CLTS transition is enabled on the path, a d! transition is taken for some for c (example \n4.4). These two CLTSs are bisimilar The states d > c (4) false! does not occur on the path. A state that \nis not consistent is called inconsistent. Note that the closure conditions of false do not need to be \npart of the relation because they are ensure that any state reachable from an inconsistent state remains \ninconsistent. Example 4.7 In the CLTS for true and c, the only inconsistent state is the one with label \nfalse. In the CLTSfor c +O.S d, the only inconsistent states are the three states with labels false. \nA state s of a CLTS is observable if s is the start state, or s is a consistent state with no outgoing \nT transitions. Finite CLTS IO. Let 0 be the set of paths from the start state SO, which satisfy the following: \n(1) the first transition on the path is labeled ?, all others are labeled ! or r, (2) the path is r-maximal \n(3) if any c! is enabled on the path, a d! transition is taken for some d > c (4) false! does not occur \non the path (5) no prefix of the path is in 0. Now for an input a, consider the subset O(a) of 0 whose \npaths start with a?. The output of each path is u c, for all c! on the path. This is the set of outputs \non a. The probability of an output o is C{Prob(Q) 1 Q E =sO, 3p E Q n O(a),output ofp = o} normalized \nby C{Prob(Q) 1 Q E +,, O(a) n Q # 0}. Weak bisimulation. A path P in a CLTS has label (c?)(resp. c!) \nif it is of the form r*(ci?)r*(cz?)r* . . . (G?)T* (resp. r* (ci!)r* (cs!)~ . . . (cn!)r*) and c is equivalent \nto cl A cs A.. . A Cn. We first define the probability of reaching a non-empty count-able set of observable \nstates S from a given state s on paths labeled c?. Let X be the set of all consistent states reachable \nfrom the state s by paths having label c? . Let P be the set of all paths ending in consistent states \nwhose initial state is s and no state other than the final one is in X. Let p = C{Prob(Q) 1 Q E Ed, P \nn Q # 0). Let P be the set of all paths whose initial state is s, the final state is in S, no state other \nthan the final one is in S, and whose label is c?. Then the probability of reaching S from s on c? is \ndefined as 0 if p = 0; if p # 0, it is defined as (l/p) x (C{ Prob(Q) 1 Q E G., P n Q # 0)). A similar \ndefinition holds for the probability of reaching a non- empty countable set of consistent states S from \na given state s on , paths labeled c!. We view CLTSs modulo the equivalence induced by the fol- lowing \ndefinition of probabilistic weak bisimulation (modeled on the definition for strong bisimulation in [43]). \nThis definition relies on tau-finiteness to ensure that a state can reach only countably many states \non r* Definition 4.8 Given two CLTSs Cl and G s, an equivalence rela-tion, R, on the disjoint union of \ntheir observable states is called a bisimulation if (I) their start states are related by R, and (2) \nwhenever two states SI and s2 are R-related, then for any finite constraint c and any R-equivalence class \nof states S the probabil- ity of reaching Sfiom SI on c?/c! is the same as the probability of reaching \nS from s2 on c?/c!. Example 4.9 Consider the CLTS for c +0.5 c (based on exam-ple 4.6, but with c taking \nthe role of d too). Consider also the CLTS all inconsistent. Thus, the witnessing binary relation relates \n(1) all consistent states with the same label coming from the CLTS for c with the same label, and (2) \nthe start state of the CLTS for c +0.5 false and c. 4.4 An algebra of CLTSs Given the definition of a \nCLTS, we now describe a Probabilistic cc algebra of CLTS, i.e. each Probabilistic CC combinator is in- \nterpreted as a function on CLTSs. The construction for true (ex-ample 4.3) and c (example 4.4) have been \nalready described. We describe the basic intuitions in the inductive cases. In each case, the transition \nsystem needs to be closed under the closure conditions to get a CLTS. if c then A. Consider the CLTS \nfor true(example 4.3), and restrict it to the states with labels d such that d Z, c, by removing all \nstates with labels d such that d > c. Add transitions labeled e? from a state with label e (in the altered \ncopy of true) to a state of A reached by a transition d? from the start state of A, if d > c and d is \nequivalent to e A e. The new start state is the start state of the copy of true and the CLTS is restricted \nto its reachable states. Iwo paths in the resulting CLTS are equivalent if their restriction to the states \nof A are equivalent; the probability numbers of an equivalence class of paths are inherited from the \nCLTS for A (the equivalence class(es) of paths contained completely in the copy of true have probability \nnumber 1). Parallel composition. Parallel composition proceeds by a product construction that mixes aspects \nof asynchronous and syn- chronous products. We first form the product of the sets of states. Define transitions \nas follows: si c! i!l 52 -s tz Sl A t1 s2 c! t2 (Sl,SZ) c! (t1, t2) (Sl,S2) c! (t1,tz) Sl -5 t1 sq at2 \n(Sl,S2) -5 (tl,t2) Sl -St1 s2 i t2 (Sl,SZ) ; (h,sz) (Sl,SZ) ; (Sl,t2)  We draw the readers attention \nto two technical points. Firstly, our handling of r transitions means that the probabilities at a state \ncan add up to more than 1. For example, let each parallel component have a state with two tau transitions \nof probability 0.5; the resulting product state has four r transitions each with associated number 0.5. \nThis peculiarity does not affect the results of this paper; in- deed, it can be fixed by a slightly more \ninvolved syntactic construc- tion that we will not describe in this paper. Secondly, unlike in tra- ditional \nprocess algebras, the c! transition and the c? transition do not cancel each other. Rather the broadcast \nstyle captured by the CLTS construction captures the CC intuition that if one process tells c, then this \nconstraint is in the global store and in effect is broad- cast to all other processes. The masking condition \nis necessary to ensure that c! transitions in parallel composition are deterministic, while saturation \nensures that all necessary synchronizations hap-pen. The set of equivalence classes of paths in the product \nCLTS is the product of the sets of equivalence classes of the two component Definition 4.12 L 5 L if \nthe state/transition set of L is a subset CLTSs. l%o paths in the product are equivalent if their projections \non the two component systems are equivalent; their probability is the product of the two respective probabilities \ndue to Property 2.5. New variables. The CLTS for new X in A is constructed in the following stages. From \nthe CLTS for A remove all transitions labeled c? where 3xc # c, and delete any states not connected to \nthe start state. This step prevents the process from receiving any X-information from the environment. \nReplace all (output) labels c! with &#38;.c! to prevent output of X-information. This however may violate \nthe determinacy con-dition on CLTSs, so we collapse certain states. We define an equivalence relation \nS on the remaining states of A as follows: 5 is defined inductively as the smallest equivalence relation \nsat- (3Z.C)? (3r.c)? isfying (a) sSS , 8 ) 81,s b s;, then 81%; (3Z.C)! and (b)sSs , s , s;, then slSsi \nand (c) 81 s ( )!) sSs , s 4 81, s 4 si, the paths from the start state SO of A to ~1,s; are in the same \nequivalence class of paths in z~,,, then ~1%;. We quotient the remaining states of A under the relation \nS. The start state of the resulting CLTS is the (equivalence class 00 the start state of A; and the equivalence \nrelation on paths is inherited from A. choose X from {O,l} in P. This corresponds to new X in (P, X = \n0) -to.5 (P, X = l), so the CLTS for it will be very similar to that in example 4.6. Let PO be the CLTS \ncor- responding to P, X = 0, and PI be the CLTS corresponding to P, X = 1. Construct a CLTS A as follows. \nA contains the disjoint union of the CLTSs of true,PO and PI. To each state coming from true and with \nlabel c, add two 7 transitions each with prob- ability 0.5; one going to the target of the c? transition \nfrom the start state of PO and the other going to the the target of the c? transition from the start \nstate of PI. The start state of true becomes the start state. The set of equivalence classes of paths \nfrom a state s coming from the copy of true are determined by the choice made on X (see example 4.6) \nand are as follows: 1. Choice not resolved: this equivalence class is the set of paths starting that \nonly visit states from the copy of trueand has probability number 1. 2. Choice X = 0: this class is \nthe set of paths that visit some state from PO and has probability number 0.5. 3. Choice X = 1: this \nclass is the set of paths that visit some state from PI and has probability number 0.5.  The required \nCLTS is given by new X in A, i.e. hide the variable X in A. The algebra of CLTSs serves as a suitable \ntarget for the seman- tics of finite Probabilistic CC programs because of the following theorem. Theorem \n4.11 Weak bisimulation is a congruence on CLTS. 4.5 Recursion In analogy with the treatment of recursion \nin the operational seman-tics, we treat an infinite Probabilistic cc process as a countable set of finite \nCLTSs. Intuitively, the set associated with a program P can be viewed as the set of CLTSs corresponding \nto the elements of App(P). We first define an ordering 5 on CLTS. Let L, L be CLTSs. of the state/transition \nset of L and . The start states of L and L are the same. . If t is a r transition in L whose target is \nin L, then the source oft is in L and t is a transition in L. . If pl _-s p2 in L for paths of L starting \nfrom s, then pl ZG~ p2 in L with same probability number: Our idea is to model (potentially infinite) \nProbabilistic CC pro-grams by countable directed (wrt 5) sets of CLTSs. Example 4.13 Recall the program \nof Example 2.9. U(Z,u,z) :: z E [l,u], choose X from (0,1) in [ if X = 0 then U(Z, (U + Z)/2, z), if \nX = 1 then U((u + Q/2, U, z)] Recall that this program can be visualized as a full binary branch- ing \ntree, where the branches correspond to the two possible values for the random variable of each recursive \ncall. The directed set cor- responding to this program is induced by the finite prefixes of this tree \nsatisfying the condition that every non-leaf node has exactly 2 children -such a prefix corresponds to \nsome element of the set of syntactic finite approximants of the operational semantics (sec- tion 3). \nIndeed, every such prefix forms the the skeleton for the associated CLTS where each node is equipped \nwith a (self-loop) transition with label c! where c is the finest associated interval, e.g. the left \n(resp. right) child of the root node has a self loop of the form (Z E [0,0.5])! (resp. (.z E [0.5.1])!). \nExample 4.14 Consider the following modified variant of the pro- gram of Example 2.9. U(l,u, z) :: .z \nE [Z,u], choose X from (0,1,2,3} in [ if X = 0 then U(Z, (u + 31)/4, z), if X = 1 then U((u + 31)/4, \n(u + Z)/2, z), if X = 2 then U((u + Z)/2, (3~ + Z)/4, z), if X = 3 then U((3u + Z)/4, u, z)] The directed \nset has similar intuitions to the one from example 4.13. This program can be visualized as a full quartemary \nbranching tree, where the branches correspond to the four possible values for the random variable of \neach recursive call. Consider the the finite pre- fixes of this tree satisfying the condition that all \nnon-leaf nodes have exactly 4 children. As in example 4.13, each node is equipped with a (self-loop) \ntransition with label c! where c is the finest associated interval, eg. the children of the root node \nhave self loops of the form (.z E [0,0.25])!, (.z E [0.25.0.5])!, (.z E [0.5,0.75])!, (z E [0.75, l])!. \nThe directed set corresponding to this program consists of the CLTSs built out of the skeleton transition \nsystem encoded in these trees. IO relation. The IO relation for directed sets of CLTSs is de- fined as \na limit of the IO relation of the elements of the set following the ideas of Section 3 - the directed \nset of CLTSs takes the place of the directed set of syntactic approximations in the definitions of Section \n3. Weak bisimulation. First some notation. Let D = {Ai} be a directed set of CLTSs. Define the LTS 2) \nwhose states and transitions are the union of the set of states of the CLTSs Ai . Let its state set be \nX. Let S z X. We first define S.+ , the projection of S on Ai. Let s be a state of Ai. Then s E SAG, \nif in V s reaches some state of S by a path of the form 7*, and no other state of Ai is on this path. \nWe now define the probability of reaching a non-empty count-able set of states S 2 X from a given state \ns on paths labeled c? (resp. c!). Let s be a state in Ak and k 5 s. Let PA,, the (unnor- malized) probability \nof reaching SA, from s on a path labeled c? (resp. c!) 6 We define the probability of reaching S from \ns on a path labeled c? (resp. c!) as the limit of the net {pa, }. In the absence of normalization, this \nlimit always exists. Definition 4.15 Let D and E be two directed sets of CLTSs, and let V and E be their \nunion LTSs. A partial equivalence relation, R, on the disjoint union of the states in 2, and &#38; is \ncalled a bisimulation if . Let s be a state such that s is not related to s by R. Then, every r* path \nfrom s can be extended to a r* path ending in a state t such that t R t; every roe path from s includes \na state t such that t R t, where these paths are in the respective LTSs. . Whenever two states SI and \ns2 are R-related, then for any finite constraint c and any R-equivalence class of states S the probability \nof reaching S from SI on c?/c! is the same as the probability of reaching S from s2 on c?/c!. Two states \nare bisimilar if there is a bisimulation relating two states. Two CLTSs are bisimilar if there is a binary \nrelation between their state sets, satisfying the above conditions such that the initial states are bisimilal: \nIn the above definition, the partiality of the equivalence rela-tion captures the id ea that all internal \nconfigurations need not be matched. This idea was captured by the restriction to observable states in \nthe finite case. This issue is illustrated by the following example. Example 4.16 There is a bisimulation \nrelating the trees of exam- ple 4.13 and example 4.14. The witnessing partial equivalence re- lation \nrelates the nodes of example 4.14 to the corresponding nodes of example 4.13. The other nodes of example \n4. I3 (e.g. the children of the root node, and every alternating level of nodes from thereon) are not \nincluded in the equivalence relation. The following non-example illustrates the issues further. Example \n4.17 No two distinct nodes of the tree described in Ex- amples 4.13 can be in the same equivalence class \nof a bisimulation relation, since their possible outputs distinguish them. Similarly, no two distinct \nnodes of Example 4.14 can be in the same equivalence class of a bisimulation relation. The absence of \nnormalization in the computation of probabil- ity numbers of our programs shows up in the following example. \n(recall that for finite recursion free programs A, the semantics of Section 4.3 validates the equational \nlaw A +0.5 false = A.) Example 4.18 The programs A +0.5 falseand A are not bisim- ilar in general. Note \nthat 23 may not he CLTS as it may not satisfy ~-finiteness. 6pai is defined following the definitions \nof section 4.3. Let P lx the set of all pathsin Ai whose initial state is 8, the final state is in SA;, \nnoState other than the final one is in S, and whose label is c? (resp. c!). Then, p.+ = (C{Prob(Q) 1 \nQE =.,P nQ#0)). Algebra of directed sets of CLTSs. The following lemma lifts the Probabilistic cc algebra \nto sets. It allows us to lift the Probabilistic cc combinators to sets of CLTSs by just extending them \npointwise -e.g. the parallel composition of two sets is the set of CLTSs got by performing the defined \ncomposition on all possible pairs from the two sets. Lemma 4.19 All operations in the algebra of CLTS \nare monotone with respect to 5. The earlier theorem that weak bisimulation is a congruence gets lifted \nto directed sets of CLTSs: Lemma 4.20 Weak bisimulation is a congruence on directed sets of CLTS. 4.6 \nCorrespondence and conservativity results Theorem 4.21 (Adequacy) (Directed sets of) CLTS modulo weak \nbisimulation is sound for reasoning about Probabilistic CC with respect to observations of IO relations. \nThe key step in this theorem is to show computational adequacy -the operational and CLTS IO relations \ncoincide. Our proof exploits the set construction in the CLTS semantics for recursive programs to reduce \nthe proof to the case of finite recursion free programs. For this case, the proof is carried out by using \na standard CC style argument to reduce finite recursion free programs to the following normal form -random \nvariable declarations at the outside enclos- ing a body with all local variable declarations outside \na body built out of tells, composition and asks. The directed sets CLTS semantics is not complete, i.e. \nnot fully abstract because it does not handle normalization of probability numbers. The theory is consistent \nwith limit observing semantics of de- terminate CC [54, Pg. 3441 - the output false of [54] corre- sponds \nto absence of output in our treatment of determinate CC. Example 4.5 motivates:. Theorem 4.22 CLTS module \nweak bisimulation is afilly abstract semantics for determinate CC with respect to observations of IO \nrelation. Corollary 4.23 (Conservativity) Probabilistic CC is conservative over CC. 5 Related work Our \nintegrated treatment of probability and the underlying con-current programming paradigm is inspired by \n[9, 23, e.g. condi-tions 2.2 and 2.5 are directly borrowed from these papers. The treatment of recursion \nand associated semantics are not explored in these papers. Our work falls fundamentally into the realm \nof study initiated in these two papers, with our contribution being the inte- gration of programming \nlanguage and concurrency theory methods. The role of probability has been extensively studied in the \ncon- text of several models of concurrency. npically, these studies have involved a marriage of a concurrent \ncomputation model with a model of probability. (1) Probabilistic process algebras add a notion of randomness \nto the underlying process algebra model. This theory is quite com-prehensive and these extensive studies \nhave been carried out in the traditional framework of (different) semantic theories of (different) process \nalgebras (to name but a few, see [32,41,43,34,5,61,18]) e.g. bisimulation, theories of (probabilistic) \ntesting, relationship with (probabilistic) modal logics etc. Recently, these theories have been shown \nto extend nicely to continuous distributions [15, 221. We have imported powerful machinery to analyze \nlabeled transi- tion systems from this area of probabilistic process algebra into our work. (2) The \nwork of Jane Hillston [37] develops a process algebra, PEPA, for compositional performance modeling. \nThe probabilities enter through the fact that each action has a duration chosen ac-cording to an exponential \ndistribution. A natural question for us is to encode her process algebra in Probabilistic cc. This would \nlead us into the integration of explicit continuous time into our model. (3) The verification community \nhas been very active and there has been significant activity in developing model checking tools for probabilistic \nsystems, for example [13, 6, 20, 381. Our work is not directedly related but should be seen as a complementary \ntool. (4) Probabilistic Petri nets [45, 621 add Markov chains to the un- derlying Petri net model. This \narea has a well developed suite of algorithms for performance evaluation. Example 2.8 shows how to represent \nsuch nets in Probabilistic cc. (5) Probabilistic studies have also been carried out in the context of \nIO Automata [58, 631. The reader will have noticed the influence of IO-automata on the definition of \nCLTS.  Our work differs in the following aspects. Formally, our work is based on the CC model. More \nimportantly perhaps, our work fo- cuses on the execution aspect of stochastic mddels, in contrast to \nthe specification focus of the above. Thus, our model remains de- terminately probabilistic and we integrate \nprobability more deeply into the language, e.g. the CC paradigm is exploited to build and specify joint \nprobability distributions of several variables. This forces us to explore the semantic issues associated \nwith the inter- action of constraints, recursion and random variables; issues not treated in the above \ntheories. On the other hand, our work does not currently incorporate the sophisticated reasoning methodolo-gies \nof the above formalisms. Our semantic study based on classi- cal techniques, labeled transition systems \nand bisimulation, makes us hopeful that this technology can be adapted to Probabilistic cc. The development \nof probabilistic frameworks in knowledge representation has been extensive [51]. Our earlier examples \nmoti- vate how to express the joint probability distributions of Bayesian networks within Probabilistic \nCC making Probabilistic cc a sim- ple notation for describing Bayesian networks. However, Prob-abilistic \nCC does not allow the direct manipulation of conditional probability assertions as in the logics of 149, \n231. The work in this genre that is most closely related to our work is the work on (lazy first order) \nstochastic functional programming languages [42]. The key technical contribution of that paper is an \nalgorithm that com- putes the output distribution exactly when all possible execution paths terminate. \nOur paper differs in the choice of the underlying concurrent computing idiom -recall our earlier arguments \nfor the importance of concurrency. [42] also does not handle probability distributions when the underlying \ncomputation does not terminate. However, we hope to be able to adapt the very general (program analysis \nstyle) ideas in the algorithm of that paper to develop rea- soning methods for Probabilistic cc. References \nVI H. Abelson, T. E Knight, and G. .I. Sussman. Amorphous comput- ing manifesto. http://www-swiss.ai.mit.edu/ \nswitzlamorphouslwhite-pap&#38;morph-newiamorph-new.html, 1996. [21 A. Aghasaryan, R. Boubour, E. Fabre, \nC. Jard, and A. Benveniste. A petri net approach to fault detection and diagnosis in distributed systems. \nTechnical Report PI-l 117, IRISA -Institut de recherche en informatique et systemes aleatoires -Rennes, \n1997. [31 Rajcev Alur, Lalita Jategaonkar Jagadeesan, Joseph J. Kott, and James E. Von Olnhausen. Model-checking \nof real-time systems: A telecommunications application. In Proceedings of the 19th Intema- tional conference \non Software Engineering, pages 514-524, 1997. [41 Robert B. Ash. Real Analysis and Probability. Academic \nPress, 1972. [51 J.C.M. Baeten, J.A. Bergstra, and %A. Smolka. Axiomatizing proba- bilistic processes: \nAcp with generative probabilities. Information and Computation, 121(2):234-255, 1995. [61 Christel Baier, \nEd Clark, Vasiliki Hartonas-Garmhausen, Marta Kwiatkowska, and Mark Ryan. Symbolic model checking for \nprob- abilistic processes. In Proceedings of the 24th International Collo- quium On Automata Languages \nAnd Programming, number 1256 in Lecture Notes In Computer Science, pages 430-440, 1997. [71 A. Benveniste \nand G. Berry, editors. Another Look at Real-time Sys- tems, volume 79:9, September 1991. VI A. Benveniste \nand G. Berry. The synchronous approach to reactive and real-time systems. In Proceedings of the IEEE \n[7], pages 1270- 1282. 191 Albert Benveniste, Bernard C. Levy, Eric Fabrc, and Paul Le Guer- nit. A calculus \nof stochastic systems for the specification, simula- tion, and hidden state estimation of mixed stochastic/nonstochastic \nsystems. Theoretical Computer Science, 152(2): 171-217, Dee 1995. UOI A. Berlin, H. Abelson, N. Cohen, \nL. Fogel, C-H. Ho, M. Horowitz, J. How, T. F. Knight, R. Newton, and K. Pistel. Distributed informa- \ntion systems for mems. Technical report, Xerox Palo Alto Research Center, 1995. [Ill G. Berry. Preemption \nin concurrent systems. In R. K. Shyamasundar, editor, Proc. of FSITCS, pages 72-93. Springer-Verlag. \n1993. LNCS 761. 1121 G. Berry and G. Gonthier. The ~~~~~~~programming language: De- sign, semantics and \nimplementation. Science of Computer Program- ming, 19(2):87 - 152, November 1992. 1131 A. Bianco and \nL. de Alfaro. Model checking of probabilistic and nondeterministic systems. In P. S. Thiagarajan, editor, \nProceedings of the 15th Annual Conference on Foundations of Sofrware Technology and Theoretical Computer \nScience, number 1026 in Lecture Notes In Computer Science, pages 499-513, 1995. [I41 Patrick Billingsley. \nProbability and Measure. Wiley-Interscience, 1995. [I51 Richard Blute, JosCe Desharnais, Abbas Edalat, \nand P&#38;ash Panan-gaden. Bisimulation for labelled markov processes. In Proceedings of the Twelfth \nIEEE Symposium On Logic In Computer Science, Warsaw, Poland., 1997. U61 J. Dean Brock and W. B. Ackerman. \nScenarios: A model of non- determinate computation. In J. Diaz and I. Ramos, editors, Interna-tional \nColloquium on Formalization of Programming Concepts, pages 252-259. Springer-Verlag, 1981. Lect. Notes \nin Comp. Sci. 107. [171 Bjom Carlson, Vine&#38; Gupta, and Tadd Hogg. Controlling agents in smart matter \nwith global constraints. In Proceedings of the AAAI Workshop on Constraints and Agents, July 1997. [I81 \nR. Cleaveland, S. A. Smolka, and A. Zwarico. Testing preorders for probabilistic processes. Lecture Notes \nin Computer Science, 623, 1992. [191 D. Coore, R. Nagpal, and R. Weiss. Paradigms for structure in an \namorphous computer. Technical Report AI Memo 1614, MIT, 1997. [201 C. Courcoubetis and M. Yannakakis. \nThe complexity of probabilistic verification. Journal of the ACM, 42(4):857-907, 1995. [211 Johan de \nKIeer and Brian C. Williams. Diagnosis with behavioral modes. In Proceedings of the Eleventh International \nJoint Conference on Artijcial Intelligence, pages 1324-1330, August 1989. 1221Josee Desharnais, Abbas \nEdaIat, and Prakash Panangaden. A logical characterization of bisimulation for labeled markov processes. \nIn Pro-ceedings of the 13th IEEE Symposium On Logic In Computer Science, Indianapolis. IEEE Press, June \n1998. [231 R. Fagin, J.Y.Halpem, and N. Megiddo. A logic for reasoning about probabilities. Information \nand Computation, 87:78-128, 1990. [241 WI P61 v71 m ~291 [301 [311 r321 [331 [341 [351 [361 1371 1381 \n[391 [401 1411 [421 [431 WI Erann Gat. Towards principled experimental study of autonomous mobile robots. \nAutonomous Robots, 2: 179-189, 1995. P Le Guemic, M. Le Borgne, T. Gauthier, and C. Le. Maire. Program- \nming real time applications with SIGNAL. In Proceedings offhe IEEE [7], pages 1321-1336. Vineet Gupta, \nRadha Jagadeesan, and Vijay Saraswat. Probabilis-tic concurrent constraint programming. In Antoni Mazurkiewicz \nand Jozef Winkowski, editors, CONCUR 97: Concurrency Theory, Lec- ture notes in computer science, vol \n1243. Springer Verlag, August 1997. Vineet Gupta, Radha Jagadeesan, and Vijay Saraswat. Computing with \ncontinuous change. Science of Computer Programming, 30(1-2):3-50, 1998. Vine&#38; Gupta, Radha Jagadeesan, \nVijay Saraswat, and Daniel Bobrow. Program&#38;g in hybrid constraint languages. In Panos Antsaklis, \nWolf Kohn, Anil Nerode, and Sankar Sastry, editors, Hybrid Systems II, volume 999 of Lecture notes in \ncornpurer science. Springer Verlag, November 1995. Vineet Gupta, Vijay Saraswat, and Peter Stmss. A model \nof a pho- tocopier paper path. In Proceedings of the 2nd IJCAI Workshop on Engineering Problems for Qualitative \nReasoning, August 1995. N. Halbwachs. Synchronous programming of reactive systems. The Kluwer international \nseries in Engineering and Computer Science. Kluwer Academic publishers, 1993. N. Halbwachs, P. Caspi, \nand D. Pilaud. The synchronous program- ming language LUSTRE. In Proceedings of the IEEE [7], pages 1305- \n1320. H. Hansson and B. Jonsson. A calculus for communicating systems with time and probabilities. In \nProceedings of the 21th IEEE Reul- Time Systems Symposium, pages 278-287. IEEE Computer Society Press, \n1990. D. Harel. Statecharts: A visual approach to complex systems. Science of Computer Programming, 8:231 \n-274, 1987. S. Hart and M. Sharir. Probabilistic propositional temporal logics. Inform&#38;on and Control, \n70197-155, 1986. M. Hayden and K. Birman. Probabilistic broadcast. Technical Report TR96-1606, Cornell \nUniversity, 1996. Pascal Van Hentenryck, Vijay A. Saraswat, and Yves Deville. Con- straint processing \nin cc(fd). Technical report, Computer Science De- partment, Brown University, 1992. Jane Hillston. A \nCompositional Approach io Performance Modelling. PhD thesis, University of Edinburgh, 1994. To be published \nas a Dis- tinguished Dissertation by Cambridge University Press. Michael Hutb and Marta Kwiatkowska. \nQuantitative analysis and model checking. In proceedings of rhe 12 IEEE Symposium On Logic In Computer \nScience, pages 11 I-122. IEEE Press, 1997. Sverker Janson and Seif Haridi. Programming Paradigms of the \nAn- dorra Kernel Language. In Logic Programming: Proceedings of the 1991 International Symposium, pages \n167-186. MIT Press, 1991. C. Jones and G. D. Plotkin. A probabilistic powerdomain of evalua- tions. In \nProceedings, Fourth Annual Symposium on Logic in Com- puter Science, pages 186-195, Asilomru Conference \nCenter, Pacific Grove, California, 1989. Bengt Jonsson and Wang Yi. Compositional testing preorders for \nprobabilistic processes. In Proceedings, Tenth Annual IEEE Sym-posium on Logic in Computer Science, pages \n431-441, San Diego, California, 1995. D. Keller, D. McAllester, and A. Pfeffer. Effective bayesian inference \nfor stochastic programs. In Proceedings of the 14th National Confer- ence on Artijicial Intelligence \n(AAAI), pages 740-747, 1997. Kim G. Larsen and Ame Skou. Bisimulation through probabilistic testing. \nInformation and Computurion, 94(1):1-28, September 1991. J. W. Lloyd. Foundations of Logic Programming. \nSpringer-Verlag, 1984. [451 [&#38;I [471 [481 [491 [501 [511 ~521 [531 [541 [551 [561 1571 [581 1591 \n1601 Wll WI W31 M. Ajmone Marsan. Stochastic petri nets: an elementary introduction. In Advances in Petri \nNets 1989, pages l-29. Springer, June 1989. Ramon E. Moore. Interval Analysis. Prentice-Hall, 1966. Carroll \nMorgan, Annabelle McIver, and Karen Seidel. Probabilis-tic predicate transformers. ACM Transactions on \nProgramming Lun- guages and Systems, 18(3):325-353, May 1996. R. Nagpal and D. Coore. An algorithm for \ngroup formation and find- ing a maximal independent set in an amorphous computer. Technical Report LCS-TR-737, \nMIT, 1998. N. J. Nilsson. Probabilistic logic. Arfi$cial Intelligence, 28:71-87, 1986. P. Panangaden. \nThe expressive power of indeterminate primitives in asynchronous computation. In P. S. Thiagarajan, editor, \nProceedings of the Fifteenth Conference on Foundations of Sofrware Technology and Theoreh cal Computer \nScience, Lecture Notes In Computer Sci- ence, 1995. Invited Lecture. J. Pearl. Probabilistic Reasoning \nin Intelligent Systems. Morgan-Kaufmann Publishers, 1988. J. R. Russell. Full abstraction for nondeterministic \ndataflow networks. In Proceedings of the 30th Annual Symposium of Foundations of Com- puter Science, \npages 170-177, 1989. V. A. Saraswat, R. Jagadeesan, and V. Gupta. Timed Default Con- current Constraint \nProgramming. Journal of Symbolic Computation, 22(5-6):475-520, November/December 1996. Extended abstract \nap- peared in the Proceedings of the 22nd ACM Symposium on Principles of Programming Languages, San Francisco, \nJanuary 1995. V. A. Saraswat, M. Rinard, and P. Panangaden. Semantic foundations of concurrent constraint \nprogramming. In Proceedings of Eighteenth ACM Symposium on Principles of Programming Languages, Orlando, \npages 333-352, January 1991. Vijay A. Saraswat. The Category of Constraint Systems is Cartesian- closed. \nIn Proc. 7th IEEE Symp. on Logic in Computer Science, Santa Cruz, 1992. Vijay A. Saraswat. Concurrent \nconstraint programming. Doctoral Dissertation Award and Logic Programming Series. MIT Press, 1993. Dana \nScott. Lattice theory, data types and semantics. In Randall Rustin, editor, Formal Semantics and Programming \nLanguages, pages 65-106. Prentice Hall, 1972. R. Segala. Modeling and VeriBcarion of Randomized Distributed \nReal-Time Systems. PhD thesis, MIT, Dept. of Electrical Engineer- ing and Computer Science, 1995. Also \nappears as technical report MIT/LCS/TR-676. G. E. Shilov and B. L. Gurevich. Integral, Measure and Derivative: \nA Unified Approach. Prentice-Hall Inc., 1966. Gert Smolka, M. Henz, and J. Werz. Consrraint Programming: \nThe Newport Papers, chapter Object-oriented programming in Oz. MIT Press, 1994. R. van Glabbcek, S.A. \nSmolka, and B.U. Steffen. Reactive, genera- tive, and stratified models of probabilistic processes. Information \nand Computation, 121(1):59-80, 1995. N. Viswanadham and Y. Narahari. Performance Modeling of Auto- mated \nManufacturing Systems. Prentice-Hall Inc., 1992. S.-H. Wu, S.A. Smolka and E. Stark. Compositionality \nand full ab- straction for probabilistic i/o automata. Theoretical Computer Sci- ence, 1996. Preliminary \nversion in CONCUR94.   \n\t\t\t", "proc_id": "292540", "abstract": "", "authors": [{"name": "Vineet Gupta", "author_profile_id": "81100020249", "affiliation": "Caelum Research Corporation, NASA Ames Research Center, Moffett Field CA", "person_id": "PP14020191", "email_address": "", "orcid_id": ""}, {"name": "Radha Jagadeesan", "author_profile_id": "81100214384", "affiliation": "Dept. of Math. and Computer Sciences, Loyola University--Lake Shore Campus, Chicago IL", "person_id": "P237380", "email_address": "", "orcid_id": ""}, {"name": "Prakash Panangaden", "author_profile_id": "81100010719", "affiliation": "School of Computer Science, McGill University, Montreal, Quebec, Canada", "person_id": "PP39023096", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/292540.292558", "year": "1999", "article_id": "292558", "conference": "POPL", "title": "Stochastic processes as concurrent constraint programs", "url": "http://dl.acm.org/citation.cfm?id=292558"}