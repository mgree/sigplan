{"article_publication_date": "01-01-1999", "fulltext": "\n Optimal Code Selection in DAGs M. Anton Ertl Institut fiir Computersprachen Technische Universit&#38; \nWien Argentinierstraae 8, 1040 Wien, Austria antonQmips.complang.tuwien.ac.at http://www.complang.tuwien.ac.at/anton/ \nTel.: (+43-l) 58801 18515 Fax.: (+43-l) 58801 18598 Abstract We extend the tree parsing approach to code \nselection to DAGs. In general, our extension does not produce the opti- mal code selection for all DAGs \n(this problem would be NP- complete), but for certain code selection grammars, it does. We present a \nmethod for checking whether a code selection grammar belongs to this set of DAG-optimal grammars, and \nuse this method to check code selection grammars adapted from ICC: the grammars for the MIPS and SPARC \narchitec-tures are DAG-optimal, and the code selection grammar for the 386 architecture is almost DAG-optimal. \n Introduction The code generator of a compiler transforms programs from the compiler s intermediate \nrepresentation into assembly language or binary machine code. Code generation can be divided into three \nphases: Code selection translates the op- erations in the intermediate representation into instructions \nfor the target machine; instruction scheduling orders the in- structions in a way that keeps register \npressure and/or the number of pipeline stalls low; register allocation replaces the pseudo-registers \nused in the intermediate representation with real registers and spills excess pseudo-registers to mem- \nory. A popular method for code selection is tree parsing. It al- lows very fast code selectors and guarantees \noptimality (with respect to its machine description). Tree parsing works only if the intermediate representation \nis a tree. However, the preferred intermediate representation is often in the form of DAGs (directed \nacyclic graphs). This paper extends tree parsing for dealing with DAGs. Our main contributions are: a \nlinear-time method for pars- ing DAGs (Section 4); and a check, whether this method parses all DAGs optimally \nfor a given grammar (Section 5). We report our (encouraging) experiences with this checking method in \nSection 6. nonterminal+pattern cost 1 start+ reg 0 2 reg-+ Reg 0 3 reg-+ Int 1 Fetch 4 reg-t I addr \n2 Plus 5 reg-+ A 2 reg reg 6 addr+ reg 0 7 addr+ Int 0 Plus 8 addr+ A 0 reg Int Figure 1: A simple tree \ngrammar In Section 2 we introduce code selection by tree parsing and in Section 3 we discuss why we \nwould like to perform code selection on DAGs. Finally, Section 7 presents some related work. 2 Code Selection \nby Tree Parsing The machine description for code selection by tree parsing is a tree grammar. Figure \n1 shows a simple tree grammar (from [Pro95]). Following the conventions in the code selec- tion literature, \nwe show nonterminals in lower case, opera-tors capitalized, and trees with the root at the top (i.e., \nif we view intermediate representation trees as data flow graphs, the data flows upwards). Each rule \nconsists of a production (of the form nonterminal-+pattern), a cost, and some code generation action \n(not shown). The productions work similar to pro-ductions in string grammars: A derivation step is made \nby replacing a nonterminal occurring on the left-hand side of a rule with the pattern on the right-hand \nside of the rule. For a complete derivation, we begin with a start nonter-minal, and perform derivation \nsteps until no nonterminal is left. Figure 3 shows two ways to derive a tree (adapted from  Labeler \ninformation Derivation tree start:cost=4 rule=1 1 F&#38;J-, addr:cost=4 rule=6 I reg: cost=4 rule=4 \nFetch starkcost= rule=1 addr:cost=2 rule=6 reg: cost=2 rule=4 PIUS staft:cost=3 rule=1 addr:cost=O rule=6 \nreg: cost=3 rule=5 8 start:cost=O rule=1 A addr:cost=O rule=6 kg reg: cost=0 rule=2 ktt start: cost=1 \nrule=1 addr:cost=O rule=7 reg: cost=1 rule=3 / 2 Figure 2: The information computed by the labeler and \nthe resulting derivation tree. cost=7 cost=4 Figure 3: Two derivations of the same tree (the circles \nand numbers indicate the rules used). [Pro95]). The cost of a derivation is the sum of the costs of the \napplied rules. For code selection the operators used in the tree grammar are the operators of the intermediate \nrepresentation, and the costs of the rules reflect the costs of the code generated by the rule. The cost \nof a whole derivation re resents the cost of the code generated for the derived tree. P As the example \nshows, code selection grammars are usu- ally ambiguous. The problem in tree parsing for code selec- tion \nis to find a minimum-cost derivation for a given tree. A relatively simple method with linear complexity \nis the dynamic programming approach used by BEG [ESL89] and 1While-in these days of superscalar and deeply \npipelined proces-sors with high cache miss costs-we cannot use an additive cost model for a direct prediction \nof the number of cycles used by the generated code, there are resources like code size and functional \nunit usage that conform to an additive cost model and have an influence on the exe-cution time through \nevents like instruction cache misses, instruction fetch unit contention or functional unit contention. \nMoreover, by reducing the number of instructions the code selector also tends to shorten data dependence \nchains and the associated latencies: e.g., on all MIPS processors the instruction lu $1,4($2) has a shorter \nlatency than the equivalent sequence li $1.4; addiu Sl,S2,$1; lw Sl,O(Sl) that would be produced by a \nnayve code selector. Iburg [FHP93]. It works in two passes: Labeler: The first pass works bottom-up. \nFor every node/nonterminal combination, it determines the min- imal cost for deriving the subtree rooted \nat the node from the nonterminal and the rule used in the first step of this derivation. Because the \nminimal cost for all lower node/nonterminal combinations is already known, this can be performed easily \nby checking all ap- plicable rules, and computing which one is cheapest. Rules of the form nonterminal-tnontermina6 \n(chain rules) have to be checked repeatedly until there are no changes. If there are several optimal \nrules, any of them can be used. Reducer: This pass performs a walk of the derivation tree. It starts \nat start nonterminal at the root node. It looks up the rule recorded for this node/nonterminal combi-nation. \nThe nonterminals in the pattern of this rule determine the nodes and nonterminals where the walk continues. \nAt some time during the processing of a rule (typically after the subtrees have been processed), the \ncode generation action of the rule is executed. Figure 2 shows the information generated by this method. \nThe resulting, optimal derivation is the same that is shown on the right-hand side of Fig. 3. 3 DAGs \nIntermediate representation DAGs arise from language con-structs like C s +=, from common subexpression \nelimination, or from performing code selection on basic blocks (instead of statements or expressions). \nFigure 4 gives an example for these possibilities. We assume in this paper, that it is acceptable to \ngener- ate code for shared subgraphs just once; and that it is also acceptable to generate code that \ncorresponds to a partial or complete replication of shared subgraphs. This assumption depends on the \nsemantics of the intermediate representation, i.e., the intermediate operations must be functions or \n(when scheduled correctly) idempotent. This assumption is usually Source Intermediate representation \nOperators like += p->count += 3; Common subexpression elimination __+ P,US p->count = p->count+3; Code \nselection on basic blocks int *pc = &#38; (p->count) ; ( PC) = (*pc)+3; Figure 4: Three ways of getting \nintermediate representation DAGs optimal lw $2, count($l) nop #load delay addu $2, $2, 3 SW $2, collnt($l) \nDAG-splitting Store addu $2, $1, lw $3, 0($2) nop #load delay addu $3, $3, SW $3, 0($2) Reg Reg count \n3 Plus 4 cycles /\\ Reg(p) Int(count) 5 cycles Figure 5: Code generated from the DAG in Fig. 4 for the \np resides in register $1. valid (e.g., it is valid for lee s intermediate representation [FH91, FH95]). \nNote that the problem of optimal pars-ing becomes easier, if this assumption is not valid, because there \nis no choice between replication and non-replication. Common subexpression elimination also relies on \nthis as-sumption3. One approach for handling DAGs is to split them into trees (by removing edges between \nshared nodes and their parents, see Fig. 5) and to parse each tree separately; the results for each tree \nare forced into a register. No work is done twice, but the resulting code can be suboptimal for the whole \nDAG (see Fig. 5). 4 Parsing DAGs Our general approach is a straightforward extension of the tree-parsing \nalgorithm to DAGs: Labeler: The same information (as in the tree labeler) is computed for each node. \nOnly the component for vis- iting the nodes in bottom-up order may have to be adapted (e.g., a recursive \nwalk would have to be ex-tended with a visited flag). Reducer: The reducer now has to deal with the potential \nexistence of several roots. Moreover, it has to set (and This assumption would not be valid for, e.g., \nan intermediate representation that contains an operator like C s ++ operator. sBut with a different \nperspective: It is ok to have only one shared subgraph instead of two equal ones. MIPS FUOOO with optimal \nand DAG-splitting code selection. check) a visited flag in every node/nonterminal combi-nation it visits, \nthus ensuring that the reducer walks every path of the derivation graph exactly once. Figure 6 shows \nhow this method parses a graph using the grammar of Fig. 1. This DAG-parsing algorithm does not take \nnode sharing into account in the labeling stage. In the reducing stage sharing is only considered for \nnode/nonterminal combina-tions. I.e., if a subgraph is derived several times from the same nonterminal, \nthe reduction of the subgraph is shared. In contrast, if a shared subgraph is derived from different \nnonterminals through base rules, its root node will be re-duced several times (i.e., the operation represented \nby the node will be replicated in the generated code); a child sub- graph of the node will also be derived \nseveral times, with the sharing of reductions depending again on whether the derivations are from the \nsame nonterminal. E.g., in Fig. 6 the left Plus node is reduced twice, because it is derived from addr \nthrough rule 8 (parent node: Fetch), and from reg through rule 5 (parent node: the right Plus); but the \nreduction of its left child (the left Reg) is shared, because it is derived from reg in both derivations. \nThe cost of deriving a DAG is the sum of the costs of the applied derivations (shared derivations are \nonly counted once). The problem in parsing DAGs for code selection is to find the minimum-cost derivation \nfor the DAG. In general, this problem is NP-complete [Pro98]. Our method for parsing DAGs is linear in \ntime and space. The labeler visits every node once and the time it spends on Labeler information starhost= \nrule=1 staWosk5 rule=1 addr:cost=2 rule=6 Fetch PiuS addr:cost=5 rule=6 reg: cost=2 rule=4 reg: cost=5 \nrule=5 starkcost= rule=1 I/\\ starkest-0 rule=1 addr:cost=O rule=6 PiUS Reg addr:co&#38;O rule=6 reg: \ncost=3 rule=5 reg: cost=0 rule=2 starkcost= rule-l starkcost= rule=1 addr:cost=O rule=6 R&#38;g hit addr:cost=O \nrule=7 reg: cost=0 rule=2 reg: cost=1 rule=3  Figure 6: Parsing a DAG. In the DAG cover the dashed each \nnode is constant (independent of the graph size). Simi-larly, the reducer visits every node/nonterminal \ncombination at most once and spends constant time on each combination. Replication of is limited by the \nnumber of nonterminals in the grammar, i.e., it is independent of the grammar size. Our method is linear, \nbut does it produce optimal deriva-tions? In general, it does not. Fortunately, for a certain class of \ngrammars this method parses DAGs optimally, as we will show in Section 5. A note on grammar design for \nparsing DAGs: For gram- mars to be used on trees there is a practice of distributing cost among the rules \ncontributing to an instruction; e.g., to assign some cost to an addressing mode. When pars-ing DAGs this \npractice would lead to some of the cost be-ing counted only once, although in the generated code the \ncost occurs twice. Therefore, grammars intended to be used with DAGs should attribute the full cost to \nrules that actu-ally generate code (in general, rules that actually cause the cost), and no cost to other \nrules. 5 Determining Optimality 5.1 Normal form grammars To simplify the discussion, we assume that \nthe grammar is in normal form [BDBSO], i.e., contains only rules of the form n + n1 (chain rules) or \nn + Op(n1,...,n;) (base rules), where the ns are nonterminals. A tree grammar can be con-verted into \nnormal form easily by introducing nonterminals. Most rules in the example grammar (see Fig. 1) are already \nin normal form, except rule 8, which can be converted to normal form by splitting it into two rules: \nnonterminal-+pattern cost Plus 8a addr+ /\\ 0 reg nl 8b nl+ Int 0 The advantage of normal form is that \nwe don t have to think about rules that parse several nodes at once and thus jump over nodes in the reducer; \ninstead, we know that any derivation of the node has to go through a nonterminal at the node.  Derivation \nDAG DAG cover t I vi _A_ 2 2 3 cost=7 lines represent the rules shown with slanted numbers. 5.2 Basic \nidea Our DAG-parsing algorithm (Section 4) makes its choice of rules as if it was parsing a tree, irrespective \nof the sharing of subgraphs. In other words, in case of a shared node it optimizes locally for each parent. \nThe job of our checker is to determine whether these locally optimal choices are also globally optimal \nfor every DAG that can be derived from the grammar. The basic principle of our checker is: Given a subgraph \nG, we consider that it can be derived from any number other parents, from every combination of nonterminals, \nus-ing every globally optimal derivation; these cases represent all ways of sharing G. Then we check \nthe following condition for every nonterminal n: The locally optimal derivation of G from n must be optimal \nfor all of these cases; if it is, this derivation of G from n is globally optimal. There is just one \nproblem: This method requires knowing globally optimal derivations, but in general we do not know them. \nWe use a conservative approximation: We assume a derivation is globally optimal unless we know that it \nis not; this ensures that the checker reports all grammars for which our parsing method is not optimal \nfor all DAGs, but it also may report some spurious problems4 When checking a derivation, we assume that \nthe shared node/nonterminal combinations are fully paid for by the other parents; i.e., for the derivation \nwe are looking at these node/nonterminal combinations have cost 0. If a rule is op- timal in this case, \nand in the full-cost case, then it is also optimal for all distributions of costs in between. 5.3 The \nchecker We check a grammar for DAG-optimality by constructing an inductive proof over the set of all \npossible DAGs: The base cases are the terminals, the step cases build larger DAGs from smaller DAGs. \nWe implemented such a checker called Dburg6. 4This is similar to, e.g., LL or LALR parser generators, \nthat report conflicts that may be caused by an ambiguous grammar, but also can be artifacts of the parser \ngeneration algorithm. 6This structure makes our checker very similar to a generator for tree parsing \nautomata [Cha87, ProSS], and it should be easy to extend our checker into a generator. Available at http://www.complang.tuwien.ac.at/anton/dburg/. \n 5.3.1 Data structures The computation of the partial-cost itemsets basically An item is a record that \ncontains a cost and a set of rules. An itemset is an array of items, indexed by nonterminals. The costs \nof the items in an itemset can be absolute or relative to a base d (common for the whole itemset). A \nstate is a record consisting of one full-cost itemset and a set of partial-cost itemsets. These data \nstructures have the following meanings: A state represents a class of graphs that have certain commonalities \nwith respect to the rules and costs for deriv- ing them. Each state corresponds to a base case or a step \ncase of the proof. The full-cost itemset is the information computed by the labeling pass of the parsing \nalgorithm for the root node of the represented graphs, with two twists: Relative costs allow representing \ngraphs with different absolute costs; and the items store the set of optimal rules instead of just one \nof these rules. A partial-cost itemset represents the incremental costs (and rules used) for deriving \nthe graphs for a specific sharing pattern. The incremental cost for deriving a partially shared graph \nfrom a nonterminal is the cost incurred for the non- shared rule derivations; it is computed by setting \nthe costs of all shared node/nonterminal combinations to 0 and doing a labeling pass.s The partial-cost \nitemsets also include an itemset for no sharing (i.e., one corresponding to the full- cost itemset). \n 5.3.2 Computing the states Dburg computes the states with a worklist algorithm: Dburg tries to build \na new state by applying every n-ary operator to every tuple of n states, until no new states occur. At \nthe start, there are no states, so only operators without operands (i.e., terminals) can be applied. \nDburg applies an operator o to a tuple of child states (51, . . . . sk) like this: First it computes \nthe full-cost itemset from the full-cost itemsets of the child states just as it is done in labeling: \nfor each base rule r of the form n -+ o(ni, . . ..nk). it computes: r.cost + Si.fUllCOSt[?Ii].COSt c \n For each nonterminal, Dburg builds an item that contains the least cost for the nonterminal, and the \nset of optimal rules for this nonterminal; this results in a preliminary item-set I. Then Dburg repeatedly \napplies the chain rules until there is no change: for a chain rule T of the form n -+ ni it computes \nr.cost + I[ni].cost If the resulting cost c is smaller than I[n].cost, the item I[n] is replaced by \none with cost c and rule set {T}; if the resulting cost is equal to I[n].cost, the rule is added to the \noptimal rules for the item I[n]. States used by tree parsing automaton generators differ by not having \npartial-cost itemsets and having only one rule (instead of a set) in each item. You may wonder about \nthe case where a derivation shares a node/nonterminal combination that is not shared with any other par- \nent (i.e., diamondishaped graphs). This case is equivalent to the case where this aubgraph is replicated \nand one of the copies is paid by the derivation we are looking at, while the other copies have cost 0. \nworks in the same way, with the following differences: For computing the partial-cost itemsets for an \nopera-tor and a tuple of states, Dburg uses every combination of partial-cost itemsets (representing \nvarious sharing variants of the subgraphs). Dburg also produces partial-cost itemsets for every sub- \nset N of the nonterminals, where the members of the subset have (absolute) cost 0; this represents the \nsharing of the whole graph represented by the state through the nonter-minals in N. To produce such a \nzero-cost item, the rules may be applied at cost 0, whatever their normal cost (be-cause they do not \ncontribute to the incremental cost), but the items used for producing the item must also have cost 0 \n(they must not contribute to the incremental cost, either). However, if a rule is not among the optimal \nrules for any partial-cost itemsets where the cost of the rule counts, it is not used for producing a \nzero-cost item, either (this avoids some spurious errors and warnings when checking). Because the absolute \ncost is important for these compu- tations, Dburg does not use relative costs for partial-cost itemsets \nthat contain an item of cost 0. For all other item- sets, Dburg uses relative costs (cost + 6); this \nallows the al- gorithm to terminate (in most cases), because relative costs allow the worklist algorithm \nto determine that a state is not new. Figure 7 shows the states computed for our example grammar. In \nthe rest of the section we give an example for the computation of the partial-cost itemsets: We will \nlook at a specific case, the operator Plus with A as the left child state and B as the right child (i.e., \nstate D): State A represents the graph Reg, state B represents the graph Int, and state D represents \nPlus(Reg, Int). A has one partial-cost itemset (Al) that represents all ways of sharing this graph. B \nhas two partial-cost itemsets: . B2 represents all ways of sharing where reg is not paid (directly or \nindirectly) by some other parent; the non- terminals start and reg cost 1 (because rule 3 costs 1). . \nBl represents all ways of sharing where reg is shared and paid by some other parent; the nonterminals \nstart and reg come for free. There are two combinations of the partial-cost itemsets of the children: \nPlus(Al,Bl) and Plus(Al,BZ). There are 16 subsets of the set of nonterminals, i.e., 16 possible sharing \nvariants, but the only important issue for this example is whether reg is in the subset or not: If yes, \nthen rule 5 (the only one applicable for deriving Plus from reg) must be applied at cost 2; this produces \nD2 from Plus(Al,Bl) and D3 from Plus(Al,B2). If no, then rule 5 must be applied at cost 0; this is only \nlegal for Plus(Al,Bl), giving Dl. In B2 reg has a non- zero cost, so it is not shared, and cannot be \nused for a shared graph Plus(Al,B2). In Dl two rules are optimal for deriving the tree from addr; in \naddition to 8a, rule 6 is optimal in Dl, because reg has cost 0. The difference between D2 and D3 is \nwho pays for deriving the right-hand child (state B, representing the graph Int) from reg: in D3 it is \nthe parent we are looking at (the Plus node), in D2 it is some other parent. . El 0 1 0 5 0 6 E2 0+6 \n1 0+6 5 0+6 6 F Plus(*,Int) full-cost 3+6 1 3+6 5 0+6 8a Fl 0 1 0 5 0 6,Ba F2 2 1 2 5 0 8a F3 3 1 3 5 \n0 8a F4 2+6 1 2+6 5 0+6 8a F5 3+6 1 3+6 5 0+6 8a Figure 7: States for the tree grammar in Fig. 1 and \ntheir itemsets 5.3.3 Checking After computing a state, Dburg performs a check for each nonterminal \nthat can derive the DAGs represented by the state: If the intersection of the sets of rules in the items \nfor this nonterminal is empty, then there is no rule that is op- timal for all sharing variants, and \nour way of parsing DAGs (see Section 4) can result in suboptimal parsing of some DAGs. This result can \nhave two causes: Either the globally optimal derivation is locally suboptimal for this nontermi- nal, \nor this is a spurious conflict resulting from considering a derivation as potentially globally optimal \nfor some sharing pattern that actually is not globally optimal for any sharing pattern. The checker also \nproduces another result: If a rule that is optimal for the full-cost item is not in all partial-cost \nitems (for the LHS nonterminal of the rule), then using this rule in this state would result in suboptimal \nparsing for some DAGs; this rule should therefore not be used for parsing this state. We have to modify \nour parser to avoid this rule; a simple way to achieve this is to generate a tree parsing automaton from \nthe states that Dburg has computed; this parser would use the rules that are optimal in all itemsets \nof the state. If there are no such partially optimal rules, we can use the grammar with any tree parser \ngenerator, e.g., Burg or Iburg (of course, the labeller and the reducer have to be adapted to DAGs as \ndescribed in Section 4). Back to our example (Fig. 7): It is easy to see that all partial-cost items \ncontain supersets of the rulesets of their corresponding full-cost items, so our example grammar can \nbe used to parse DAGs optimally with any tree parser gen- erator. 6 Experiences 6.1 Results We have applied \nDburg to the example grammar of Fig. 1 and four realistic grammars: adaptions of the grammars sup- plied \nwith lee-3.3 [FH95] for the MIPS, SPARC and 386 ar- chitectures and the MIPS grammar of the RAFTS compiler \n[EP97]. As discussed above, Dburg found that the example gram- mar of Fig. 1 can be used for optimal \nparsing of DAGs, with any tree parser generator. 6.2 RAFTS grammar The RAFTS-MIPS grammar is already \nin use for parsing DAGs, using a Burg-generated parsing automaton. It is similar in spirit to ICC S MIPS \ngrammar; the differences are that it does not deal with floating-point instructions (but code selection \nfor the MIPS FP instructions is quite trivial anyway), is more complex in the integer part (e.g., there \nare patterns for selecting instructions like bgsz), and there are some differences due to differences \nin the intermediate representation. At first Dburg did not terminate for the RAFTS-MIPS grammar. This \nnontermination uncovered a bug in the grammar (a rule had the wrong cost), and after fixing that bug, \nDburg did terminate. Then Dburg reported state/nonterminal combinations that have no globally opti-mal \nrule. We fixed all these problems by adding rules . Our improved RAFTS grammar can be used for optimal \ncode se-lection on DAGs; however, Dburg warns that our use of a Burg-generated parsing automaton may \nresult in subopti- ma1 parsing of some DAGs (i.e., we should use a generator that knows about DAG-optiomal \nrules instead of Burg). Note that adding rules cannot decrease the quality of code selec-tion (if the \nresult is DAG-optimal). A typical problem found in the RAFTS-MIPS grammar lows the code selector to generate \nthe addressing mode is this: The grammar contained the following rules: index*scale+displacement) . 0 \n1 nonterminal+ cons+ reg-t pattern Cons cons And cost 0 1 2 reg+ A reg reg And 1 reg+ // 1 reg cons \nAnd 4 reg+ A I cons reg When deriving the tree And(Cons,Cons) from reg, the fol- lowing problem arises: \nIf the left Cons node is derived from reg by another parent, then rule 3 is optimal and rule 4 is not \noptimal; if the right Cons node is derived from reg by another parent, the situation is reversed. So \nwithout know-ing the sharing situation we cannot decide which rule to use. We solved the problem by adding \nanother rule: nonterminal-+ pattern cost And 5 cons+ /Y 0 cons cons Now rule 1 is optimal for the problematic \ncase, indepen- dent of the sharing pattern. Moreover, the addition of this rule also improves the code \nselection for trees, because now constant folding is performed at compiule time instead of computing \nthe constant at run-time. 6.3 ICC Sgrammars ICC S grammars use dynamic costs (i.e., costs determined \nat code selection time) for complex code selection decisions. This technique is incompatible with tree \nparsing automata in general and with Dburg in particular. We have removed the dynamic costs in various \nways: in most cases we replaced them with fixed costs; however, in some cases we also re-moved rules. \nIn particular, the 386 grammar used dynamic costs for selecting read-modify-write instructions, which \nwe eliminated; this is no loss because these instructions cost the same as the equivalent sequence of \nsimple instructions (and on the Pentium, the simpler instructions are easier to pair). Another technique \nused by ICC S grammars is to assign costs to rules corresponding to addressing modes. This al-lows simplifications \nin the grammar and works for trees, but not for DAGs (two instructions cannot share the computa- tion \nof the addressing mode). We assigned these costs to the corresponding rules for instructions. The resulting \ngrammar is equivalent to the old one for tree parsing, but works better with DAGs. Dburg reports that \nthe resulting MIPS ans SPARC grammars can be used for optimal parsing of DAGs with any tree parser generator. \nFor the 386 grammar Dburg reported seven cases without a DAG-optimal rule. Six of these vanished after \nwe added the rule base+acon 0 which also improves the code selection for trees (it al- The last problem \nis harder, but not very important. It can only take effect when a CVDI (convert double to inte-ger) node \nis shared. This is extremely rare, so suboptimal- ity in that case is acceptable. Apart from this problem, \nthe grammar can be used for optimal parsing of DAGs; how-ever, there are many cases where tree-optimal \nrules are not DAG-optimal (i.e., using Burg or Iburg with the resulting grammar may produce suboptimal \nparses even in the ab- sence of shared CVDI nodes). 6.4 Usability Performing all combinations of itemsets \nmay appear to take a huge amount of time, result in a huge number of itemsets, and significantly increase \nthe danger of non-termination. However, in practice we found this to be no problem: For the grammars \nwe looked at, the number of itemsets per state is moderate (typically < lo), the time used for generating \nall states is bearable (very grammar-dependent, < 7min on a 6OOMHz 21164A for the 386 grammar, < lmin \nfor the others), and only for the original RAFTS-MIPS grammar Dburg did not terminate (thus uncovering \na bug). Apart from that, Dburg generates slightly more states than it did before we added the computation \nof partial-cost itemsets. The memory use is also bearable (12MB for the 386 gram- mar). 7 Related Work \nThe Davidson-Fraser approach to code selection (used in, e.g., GCC) [DF84, WenSO] can deal with DAGs, \nbut does not guarantee optimality (not even for trees). However, it is more flexible in the kind of code \nselection patterns al-lowed (not only tree patterns), and can therefore produce better code than tree \nparsing. Its disadvantages are that it is harder to use (e.g., adding a new rule can result in worse \ncode) and that the resulting code selectors are slower. lee s front-end [FHSl, FH95] can produce intermediate \nrepresentation DAGs or trees. The code selectors in [FH95] are,based on tree parsing; they deal with \nthe problem by asking ICC S front end to split the DAGs into trees. In [BE911 an approach for dealing \nwith DAGs in the con- text of tree parsing is discussed that has the same results as DAG-splitting in \ngeneral, but allows replication of zero-cost sub-trees (e.g., constants or effective address expressions). \n[PW96] presents a method for performing single-pass tree parsing code selection (in contrast to the classic \nmethod, which requires a labeler pass and a reducer pass). Like our way of optimal parsing of DAGs, this \nmethod works only with certain grammars, but can be used with realistic code selection grammars. 8 Further \nwork Although nontermination was not a problem in our experi- ments, it would be worthwhile to investigate \ncauses for non- termination, and how they can be avoided; ideally Dburg should terminate whenever Burg \nterminates. Concerning spurious errors, more experimentation is needed to see if this is a practical \nproblem. If yes, meth-ods for improving the accuracy of the optimality test need to be developed. An \nimportant practical issue is how to report errors and warnings about supoptimality in a way that can \nbe under- stood easily. In particular, it should make it easy to discern real suboptimalities from spurious \nerrors that are artifacts of our conservative apporoach. For real errors it should be easy to see what \nchanges in the grammar might help. Users would probably accept suboptimalities in many cases, if they \nknow what cases are still handled opti-mally (e.g., as in the 386 grammar). Providing such near-optimality \nguarantees would be a worthwhile extension. Dynamic costs are often used in practice, but dburg cur-rently \ndoes not handle them. An extension of Dburg that takes this issue into account would have great practical \nim-portance. Another area of investigation is the relation with single- pass tree parsing [PW96]. Both \nDAG-parsing and single- pass tree parsing have to decide comparatively early which derivation to use, \nso there is probably a large overlap be-tween the grammars accepted by dburg and by wburg, lead- ing \nto a class of grammars suited for single-pass DAG-parsing.     Conclusion We extend tree parsing \nfor dealing with DAGs. This exten-sion is simple and parses DAGs in linear time. The same derivations \nare selected as in tree parsing; sharing of nodes by several parents does not influence the selection \nof deriva- tions. In general, this method does not produce the optimal code selection for all DAGs, but \nfor certain code selection grammars, it does. We also present Dburg, a tool to check whether a code selection \ngrammar belongs to this set of DAG-optimal grammars. We used Dburg to check realis- tic code selection \ngrammars for the MIPS, SPARC and 386 architectures; they required a few additions, but can now be used \nfor optimal (or, for the 386, nearly optimal) code selection on DAGs. Acknowledgements Discussions \nwith Franz Puntigam helped me in refining the ideas in this paper. Manfred Brockhaus, Andreas Krall, \nFranz Puntigam and the referees of POPL 98, PLDI 98 and POPL 99 provided valuable feedback on earlier \nversions of this paper. References [BDBSO] A. Balachandran, D. M. Dhamdhere, and S. Biswas. Efficient \nretargetable code generation using bottom-up tree pattern matching. Computer Languages, 15(3):127-140,199O. \n[BE911 John Boyland and Helmut Emmelmann. Dis-cussion: Code generator specification techniques (summary). \nIn Robert Giegerich and Susan L. Graham, editors, Code Generation -Concepts, Tools, Techniques, Workshops \nin Computing, pages 66-69. Springer, 1991. (Cha87] David R. Chase. An improvement to bottom- up tree \npattern matching. In Fourteenth Annual ACM Symposium on Principles of Programming Languages, pages 168-177, \n1987. [DF84] [EP97] [ESL89] [FH91] [FH95] [FHP93] [Pro951 [Pro981 [PW96] [Wen90] Jack W. Davidson and \nChristopher W. Fraser. Code selection through object code optimization. ACM 13-anaactiona on Programming \nLanguages and Systems, 6(4):505-526, October 1984. M. Anton Ertl and Christian Pirker. The structure \nof a Forth native code compiler. In EuroForth 9 7 Conference Proceedings, pages 107-116, Oxford, 1997. \nHelmut Emmelmann, Friedrich-Wilhelm SchrBer, and Rudolf Landwehr. BEG -a generator for effi- cient back \nends. In SIGPLAN 89 Conference on Programming Language Design and Implementa-tion, pages 227-237, 1989. \nChristopher W. Fraser and David R. Hanson. A code generation interface for ANSI C. Software-Practice \nand Experience, 21(9):963-988, Septem-ber 1991. Christopher Fraser and David Hanson. A Retar-getable \nC compiler: Design and Implementation. Benjamin/Cummings Publishing, 1995. Christopher W. Fraser, David \nR. Hanson, and Todd A. Proebsting. Engineering a simple, efficient code generator genera-tor. ACM Letters \non Programming Lan-guages and Systems, 1993. Available from ftp://ftp.cs.princeton.edu/pub/iburg.tar.Z. \nTodd A. Proebsting. BURS automata genera-tion. ACM mansactions on Programming Lan-guages and Systems, \n17(3):461-486, May 1995. Todd Proebsting. Least-cost instruc- tion selection in DAGs is NP-complete. \nhttp://research.microsoft.com/ tod- dpro/papers/proof.htm, 1998. Todd A. Proebsting and Benjamin R. \nWhaley. One-pass, optimal tree parsing -with or with-out trees. In Tibor Gyimbthy, editor, Compiler Construction \n(CCXV), pages 294-308, LinkGping, 1996. Springer LNCS 1060. Alan L. Wendt. Fast code generation using \nautomatically-generated decision trees. In SIG-PLAN 90 Conference on Programming Language Design and \nImplementation, pages 9-15, 1990.  \n\t\t\t", "proc_id": "292540", "abstract": "", "authors": [{"name": "M. Anton Ertl", "author_profile_id": "81100016086", "affiliation": "Institut f&#252;r Computersprachen, Technische Universit&#228;t Wien, Argentinierstra&#946;e 8, 1040 Wien, Austria", "person_id": "PP39071533", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/292540.292562", "year": "1999", "article_id": "292562", "conference": "POPL", "title": "Optimal code selection in DAGs", "url": "http://dl.acm.org/citation.cfm?id=292562"}