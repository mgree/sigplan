{"article_publication_date": "01-01-1999", "fulltext": "\n Improvement in a Lazy Context: An Operational Theory for Call-By-Need Andrew Moran and David Sands Chalmers1 \n Abstract The standard implementation technique for lazy func-tional languages is call-by-need, which \nensures that an argument to a function in any given call is evaluated at most once. A significant problem \nwith call-by-need is that it is difficult -even for compiler writers -to predict the effects of program \ntransformations. The tra-ditional theories for lazy functional languages are based on call-by-name models, \nand offer no help in determining which transformations do indeed optimize a program. In this article \nwe present an operational theory for call- by-need, based upon an improvement ordering on pro-grams: \nA4 is improved by N if in all program-contexts C, when C[M] terminates then @[N] terminates at least \nas cheaply. We show that this improvement relation satisfies a con- text lemma , and supports a rich \ninequational theory, subsuming the call-by-need lambda calculi of Ariola et al. [AFM+95]. The reduction-based \ncall-by-need calculi are inadequate as a theory of lazy-program transformation since they only permit \ntransformations which speed up programs by at most a constant factor (a claim we sub- stantiate); we \ngo beyond the various reduction-based cal-culi for call-by-need by providing powerful proof rules for \nrecursion, including syntactic continuity -the basis of fixed-point-induction style reasoning, and an \nimprove- ment theorem, suitable for arguing the correctness and safety of recursion-based program transformations. \n Introduction Call-by-need optimises call-by-name by ensuring that when evaluating a given function \napplication, arguments are evaluated at most once. All serious compilers for lazy functional languages \nimplement call-by-need evaluation. Department of Computing Science, Cllalrners University of Technology \nand tlua University of Giiteborg, S-412 96 GGteborg, Sweden; [andrew.dave]@cs.chalmers.se. Permission \nto make digital or hard topics ofall or part ofthiswork for personal or classroom use is granted without \nfee provided that copies arc not made or distributed rhr prolit or commercial advantage and that topics \nbear this notice and the full citation on the first page. 1.0 copy othsrwise. to republish, to post on \nsewers or to redistribute to lists. requires prior specilic permission and!or a fee. POPL 99 San Antonio \nTexas L!SA Copyright ACM 1999 l-581 13-0953/99/01...$5.00 Lazy functional languages are believed to be \nwell-suited to high-level program transformations, and some state-of-the-art compilers take advantage \nof this by applying a myriad of transformations and analyses during compi- lation [PJS98]. However, it \nis notoriously difficult, even for those with extremely solid intuitions about call-by-need, to predict \nthe effects of a program transformation on the running time. Since traditional theories for lazy languages \nare based upon call-by-name models, they give no assurance that a given transformation doesn t lead to \nan asymptotic slow-down. Call-by-need Calculi The call-by-need lambda calculi [AFM+95, AF97, MOW981 offer \na solution to some of these problems. By permitting fewer equations than call- by-name, these calculi \nenable term-level reasoning with-out ignoring the key implementation issues underpinning call-by-need. \nHowever, they do have some serious limita- tions. All of the equations in the calculi are, by definition, \nsymmetric. This means that certain useful local transfor- mations cannot be present. In fact, the call-by-need \ncal-culi are limited to transformations which change running- times by at most a constant-factor (see \nsection 4.6), inde- pendent of the context in which the programs are used. Even within the confines of \nconstant-factor transforma-tions there are significant shortcomings, since none of the calculi have proof \nrules for recursion; we believe that, as a consequence, almost no interesting equivalences between recursive \nprograms -such as the fusion of recursive func- tions (e.g. via deforestation) -can be justified in the \ncalculi. Our Approach We aim to go beyond these limitations by refining the notion of observational approa?imation \nbe- tween terms, and by establishing algebraic laws (contain- ing the laws of the call-by-need calculi \nas theorems) and recursion principles for that approximation relation. A key result of [AFM+95] is that \nthe standard observa-tional equivalence and approximation relations, in which one only observes termination, \ncannot distinguish call-by-need evaluation from call-by-name. To obtain an op- erational theory which \nretains the computational distinc-tions between name and need, we also observe the cost of evaluation, \nin terms of a high-level model of compu- tation steps. Our observational approximation relation, improvement, \nis defined with respect to a fixed opera-tional semantics by saying that: M is improved by N if in all \nprogram-contexts C, when C[M] terminates then C[N] terminates at least as fast. Summary of Results We \ndevelop an operational theory for a call-by-need lambda calculus with recursive lets, constructors, and \ncase expressions. The theory is based upon an abstract machine semantics for call-by-need, and is cost-sensitive, \nand therefore reflects the computational distinctions between call-by-name and call-by-need. We show \nthat the improvement relation has a rich inequa- tional theory, validating the reduction rules of the \ncall- by-need calculi. Most importantly, it supports powerful induction principles for recursive programs. \nSome spe- cific original results are: . A context lemma for call-by-need, meaning we can establish improvement \nby considering just computa- tion in a restricted class of contexts, the evaluation contexts; A rich \ninequational theory, the tick algebra, which subsumes the call-by-need calculi; A syntactic continuity \nproperty which characterises improvement of a recursive function in terms of its finite unwindings, and \nforms the basis of fixed-point induction style proofs, and Two powerful proof techniques, the improvement \ntheorem and improvement induction, which are par- ticularly well-suited to inferring the correctness \nand safety of recursion-based program transformations which proceed by local improvements. Overview We \nbegin with a discussion of related work in section 2. Section 3 then presents the operational seman-tics \n(Sestoft s mark 1 abstract machine for laziness). This is used as the basis for a contextual definition \nof im- provement and cost equivalence, and the context lemma is stated. The inequational theory, known \nas the tick al- gebra, is presented in section 4, and the relative power of the algebra and the call-by-need \ncalculi is discussed. Syntactic continuity is presented in section 5 and used to show that an unwinding \nfixed-point combinator is im- proved (up to a constant factor) by a knot-tying fixed-point combinator. \nWe also present a syntactic variant of fixed-point fusion for call-by-need, which can be estab- lished \nvia syntactic continuity. The improvement theo-rem is introduced in section 6, along with improvement \ninduction and examples of their use. Section 7 concludes, and we discuss of future avenues of research. \nThere is an extended version of this paper available for download from http://www.cs.chaImers.se/ andrew/. \nIt includes an appendix that summarises the technical de-velopment and presents the proofs of the main \ntheo-rems. 2 Related Work Improvement theory and the improvement theorem were originally developed in \nthe call-by-name setting [SanSl, San96], and generalised to a variety of call-by-name and call-by-value \nlanguages in [San97]. Whether this pro-gramme could be carried out in a call-by-need setting has long \nbeen an open question. An inspiration which gave us confidence in the possibility of a tractable improve-ment \ntheory for call-by-need is the call-by-need lambda calcuZus presented by Ariola and Felleisen, and Maraist, \nOdersky and Wadler [AFM+95, AF97, MOWSS]. For us, the significance of the call-by-need calculi is that \nthey are based on reduction (and hence equations) between terms in the source language (see figure 7), \nrather than, say, term-graphs, abstract-machine configurations, or terms plus explicit substitutions. \nThe reduction rules are con- fluent, and enjoy a deterministic notion of standard re-duction. Related \nconcepts appear in other approaches, in particular in the study of so-called optimal reductions e.g., \n[FieSO, Mar91, YosSS]. One limitation of the original work by Ariola et al. is in the treatment of recursive \ncycles; naive extension of the calculi to deal with recursive lets leads to a loss of con-fluence [Jef93, \nAK97]. The original call-by-need calculus considers recursive lets only briefly. To recover conflu-ence, \none can simply disallow reductions under cycles, a~ in e.g., [BLR96, Nie96]. Ariola and Blom give a full \nstudy of cyclic recursion in [AB97, AB98], and show that an ap- proximation to confluence can be obtained \nby equating terms with the same infinite normal-form. Their &#38;x~RE calculus can be seen as the natural \nsuccessor to the call- by-need calculi. In general, reduction calculi appear to be a good ve-hicle for \nexploring the language design space with re-gard to call-by-need-like features. Rose s work e.g. [Ros96, \nBLRSG] exemplifies this approach in an elegant combination of explicit substitution and combinatory re-duction \nsystems. Our view is complementary to the rewriting approaches: once a particular operational se-mantics \n(reduction strategy) has been fixed, one can go beyond the confines of the calculi by developing an oper- \national theory. Apart from the rewriting-based approaches, there have been a few attempts to give a high-level \nsemantics to call-by-need e,g. [Jos89, Jef94, SP196, Lau93, Ses97]. Launchbury s natural semantics, and \nSestoft s abstract machine(s) have been adopted by a number of researchers as the formal definition of \ncall-by-need e.g. [TWM95, HM95, SPJ97, Gus98]. Since it appears to be a non-controversial choice, we \nadopt Sestoft s machine -es-sentially a Krivine-machine [Cur911 with updating of the heap -as the operational \nmodel underpinning our the-ory. As others have observed (e.g. [Pit97a]), working with an abstract machine \nrather than an inductive semantics also has benefits in proofs about computations. 3 The Operational \nSemantics Our language is an untyped lambda calculus with recur-sive lets, structured data, and case \nexpressions. We work with a restricted syntax in which arguments to functions (including constructors) \nare always variables: L,M,N::=a:]Xz.M]Mz]cZ 1 let {Z = G} in N ( case M of (c; Zi -+ Ni). The syntactic \nrestriction is now rather standard, follow-ing its use in core language of the Glasgow Haskell com- piler, \ne.g., [PJPSSG, PJS98], and in [Lau93, Ses97]. All constructors have a fixed arity, and are assumed to \nbe saturated. By c 2 we mean c zi . . . zn. The only values are lambda expressions and fully-applied \nconstructors. Throughout, z, y, z, and w will range over variables, c over constructor names, and V and \nW over values. We will write let {Z = A?} in N as a shorthand for let{~cl=M1,...,2,=M,)inN where the \n5 are distinct, the order of bindings is not syntactically significant, and the jt are considered bound \nin N and the A? (so our lets are recursive). Similarly we write case M of {ci ci!i + Ni} for where each \n26 is a vector of distinct variables, and the ci are distinct constructors. In addition, we will sometimes \nwrite alts as an abbreviation for case alternatives {ci &#38; + Ni}. For examples, working with a restricted \nsyntax can be cumbersome, so it is sometimes useful to lift the restric- tion. Where we do this it should \nbe taken that MNzlet {z= N} in Mz, z fresh whenever N is not a variable. Similarly for constructor expressions. \nThe only kind of substitution that we consider is variable for variable, with u ranging over such substitutions. \nThe simultaneous substitution of one vector of variables for another will be written M[g/z], where the \n~2 are assumed to be distinct (but the y need not be). 3.1 The Abstract Machine The semantics presented \nin this section is essentially Ses- toft s mark 1 abstract machine for laziness [Ses97]. In that paper, \nhe proves his abstract machine semantics sound and complete with respect to Launchbury s nat-ural semantics, \nand we will not repeat those proofs here. Transitions are over configurations consisting of a heap, containing \nbindings, the expression currently being eval- uated, and a stack. The heap is a partial function from \nvariables to terms, and denoted in an identical manner to a collection of let-bindings. The stack may \ncontain vari- ables (the arguments to applications), case alternatives, or update markers denoted by \n#x for some variable x. Update markers ensure that a binding to x will be recre- ated in the heap with \nthe result of the current evaluation; this is how sharing is maintained in the semantics. We write (I \n, M, S) for the abstract machine configura- tion with heap P, expression M, and stack S. We denote the \nempty heap by 0, and the addition of a group of bind- ings 2 = J? to a heap I by juxtaposition: I (3 \n= G}. The stack written b : S will denote the a stack S with b pushed on the top. The empty stack is \ndenoted by E, and the concatenation of two stacks S and T by ST (where S is on top of T). We will refer \nto the set of variables bound by P as domr, and to the set of variables marked for update in a stack \nS as domS. Update markers should be thought of as binding occurrences of variables. A configuration is \nwell-formed if dom P and domS are disjoint. We write dom(I ,S) for their union. For a configuration (P, \nM, S) to be closed, any free variables in I , M, and S must be contained in don@, S). For sets of variables \nP and Q we will write P j Q to mean that P and Q are disjoint, i.e., P n Q = 0. The free variables of \na term M will be denoted FV (M); for a vector of terms G, we will write FV (i@). The abstract machine \nsemantics is presented in figure 3.1; we implicitly restrict the definition to well-formed con-figurations. \nThere are seven rules, which can grouped as follows. Rules (Loolcup) and (Update) concern evaluation \nof variables. To begin evaluation of x, we remove the binding x = M from the heap and start evaluating \nM, with x, marked for update, pushed onto the stack. Rule (update) applies when this evaluation is finished, \nand we may update the heap with the new binding for x. Rules (Unwind) and (Subst) concern function applica-tion: \nrule (Unwind) pushes an argument onto the stack while the function is being evaluated; once a lambda \nex-pression has been obtained, rule (Subst) retrieves the ar- gument from the stack and substitutes it \ninto the body of that lambda expression. Rules (Case) and (Branch) govern the evaluation of case expressions. \nRule (Case) initiates evaluation of the case expression, with the case alternatives pushed onto the stack. \nRule (Branch) uses the result of this evaluation to choose one of the branches of the case, performing \nsub-stitution of the constructor s arguments for the branch s pattern variables. Lastly, rule (Letrec) \nadds a set of bindings to the heap. The side condition ensures that no inadvertent name cap- ture occurs, \nand can always be satisfied by a local (Y-conversion. Definition 3.1 (Convergence) FOT closed configurations \n(P, M, S), (P, M, S).&#38; %A,V.(P, M, S)-+ (A, V,e), (I , M, S)U gf 3n.( l-, M, S).lJn, (r, M, s)p Zf \n3m.(r, M, S)&#38; A m <n. Closed configurations which do not converge are of three types: they either \nreduce indefinitely, get stuck because of a type error, or get stuck because of a black-hole (a self-dependent \nexpression as in let x = x in x). All non-converging configurations will be semantically identified. \nWe will also write MU, MUn and MU< , identifying closed M with the initial configuration (0, M, E). \n3.2 Program Contexts The starting point for an operational theory is usually an approximation and an \nequivalence defined in terms of program contexts. Program contexts are usually intro-duced as programs \nwith holes , the intention being that an expression is to be LLplugged into all of the holes in the context. \nThe central idea is that to compare the be- haviour of two terms one should compare their behaviour in \nall program contexts. (r{x=M}, x, S)-i(l-, M, #x:S) (Lookup) (I-, v, #x : S) -+ (r{x = V}, v, S) (Update) \n(r, Mx, s) -+ (r, M, x: s) (Unwind) (r, Ax.M, Y: s) --) (c wqxi,s) (Sub&#38;) (r, case M of alts, S) \n-+ (r, M, alts : S) (Case) (r,cjg, (~~z~--,~~):s)-+ (r, ivjp7&#38;], s) (Branch) (r, let{j:=A?}inN, S)-t(f \n{Z=A?}, N, S) Zjdom(r,S) (Letrec) Figure 1: The abstract machine semantics for call-by-need. We will \nuse contexts of the following form: C, ID ::= [.I 12 1 xz.c 1 csc 1 cc? I let{Z=C} in ID ( case C of \n{ci Zi + Q). Our contexts may contain zero or more occurrences of the hole, and as usual the operation \nof filling a hole with a term can cause variables in the term to become captured. The relationship between \nterms and configurations is characterised by a translation function from configura-tions to terms, defined \ninductively on the stack: trans(0, M, E) = M trans( {Z= &#38;?}, N, E) = let {Z= J?} in N trans( I?, \nM, z : S) = trans( l , M z, S) trans( r, M, #x : S) = trans( I {x = M}, I, S) trans( r, M, alts : S) \n= trans( r, case M of ah, S) The following lemma clarifies the relationship: Lemma 3.1 (Translation) \nFOT all r, C, S, there exists k > 0 such that for any M,  (0, trans(!?, C[M], S), e) _tk (I , @[Ml, \nS). PROOF. Simple induction on the size of S. 0 3.3 Improvement We define observational approximation \nand equivalence via contexts in the standard way [A093]. Definition 3.2 (Observational Approximation) \nWe say that M observationally approximates N, written M L N, if for all C such that C[M] and C[N] are \nclosed, ClMlU =+ @[NW We say that M and N are observationally equivalent, written M EN, when M &#38; \nN and N &#38; M. We know that % coincides with its call-by-name counter-part, so this tells us nothing \nnew. We need to incorporate more intensional information if we are to build an oper- ational theory that \nretains the distinction between name and need. Since call-by-need may be thought of as an optimisation \nof call-by-name, a natural intensional prop-erty to compare is how many reduction steps are required \nfor termination. Definition 3.3 (Improvement) We say that M is im- proved by N, written M k N, if for \nall C such that C[M] and @[N] are closed,  @[M]lj --r. C[N]JJ+. We say that M and N are cost equivalent, \nwritten M 2 N,whenMkNandNkM. This definition suffers from the same problem as any con- textual definition: \nto prove that two terms are related requires one to examine their behaviour in all contexts. For this \nreason, it is common to seek to prove a context lemma [Mi177] for an operational semantics: one tries \nto show that to prove M observationally approximates N, one only need compare their behaviour with respect \nto a much smaller set of contexts. We have established the following context lemma for call- by-need: \n Lemma 3.2 (Context Lemma) FOT all terms M and N, if for all r and S, (r, M, S)U * (r, N, S)UC then \nM k N. It says that we need only consider configuration contexts of the form (r, [.I, S) where the hole \n[.I appears only once. This corresponds exactly to a subset of term con-texts called evaluation contexts, \nin which the hole is the subject of evaluation. We shall make this correspondence precise in the section \n4.2. Note that the context lemma applies to open terms M and N. It is more common to restrict one s attention \nto closed terms, and then show that the preorder in question is closed under (general) substitution. \n 3.4 Strong Improvement The improvement relation, like the notion of operational approximation which \nit refines, also increases the termi- nation of programs, so if M k N then N may also ter-minate more \noften than M. In the context of compiler optimisations it is natural to ask for a stronger notion of \nimprovement which does not permit any change in ter- mination behaviour. Definition 3.4 (Strong Improvement) \nWe say that M is We can define J within the language as an empty let strongly improved by N, written \nM B N, if binding, thus: MENANLM JM dgflet {} in M. M is strongly improved by N if it is improved by \nN, and N has identical termination behaviour (note that we need only have N L M in the definition since \nM k N * M L N). For simplicity of presentation we emphasise improvement rather than strong improvement. \nHowever, almost all the laws and proof rules presented in subsequent sections also hold for strong improvement. \nNotable exceptions being the strictness laws concerning fl, the divergent term. The syntactic continuity \nproof principle is sound for strong improvement, but degenerates to a trivial rule. The following Hasse-diagram \nillustrates the relationships between the various approximations and equivalences in- troduced in this \nsection: AxA \\/\\/ B B The diagram is a r-l-semi-lattice of relations on terms. In other words, the greatest \nlower bound of any two relations in the diagram is equal to their set-intersection. 4 The Tick Algebra \nConsider the following improvement: Clearly, for any I and S: (l-, (Xz.M)y, S) + (r, Xz.M, y : S) + (r, \nM[y/A S), so (*) follows from the context lemma. But we can say more: (X2.M) y always takes exactly two \nmore steps to converge than M[y&#38;]. If we had some syntactic way of slowing the right-hand side down, \n(*) could be written as a cost equivalence, which would be preferable, since it is a more informative \nstatement. This motivates the introduction of the tick , written J, which we will use to add a dummy \nstep to a computation. Now we can write (*) as Clearly, J adds one unit to the cost of evaluating M \nwithout otherwise changing its behaviour. Note that: MU _ Ml,L MJJ u JMJJ. +l We will write kJM to mean \nthat M has been slowed down by k ticks. The following inference rule and axiom, known collectively as \ntick elimination are crucial when establishing improvement or cost equivalence. JMkJN /MkM (J-e&#38;m) \nMEN Their validity follows from the definition of R. We can easily prove a number of improvements and \ncost equivalences modulo tick, and we present a selection of the more useful ones in the following sections. \nThrough-out, we will follow the standard convention that all bound variables in the statement of a law \nare distinct, and that they are disjoint from the free variables. Together with (J-elim), the laws presented \nin figures 2, 3, 4, 5, and figure 6 are known collectively as the tick algebra. 4.1 Beta Laws The first \nset of laws, presented in figure 2, are important in that they allow us to mimic evaluation within the \nalge- bra. We have already seen (,D); (case-p) is the analogous law for case expressions. In (value+), \none may replace occurrences of a variable, which is bound to some value V, with aJV. The ticks reflect \nthe fact that by replacing z with its value, we are short-circuiting one lookup and one update step. \nThe proofs of validity of (v&#38;e-/3), (var-/3), (war-abs), and (uar-subst) rely upon general techniques \nthat are elaborated in an extended version of the paper. There are also two derived beta laws, corresponding \nto unrestricted versions of (/3) and (case-@. We can derive the following cost equivalence (modulo tick): \n(Az.M) N z /let (z = N) in M (P ) where N is not a variable. There is a similar derived law for general \ncase expressions. 4.2 Laws for Evaluation Contexts An evaluation wntext is a context in which the hole \nis the target of evaluation; in other words, evaluation cannot We could introduce a new syntactic construct \ninstead, with the semantics of a dummy step. Since we are working with con-textual approximations and \nequivalences, this would not neces-sarily be a conservative extension: we would have to prove that it \ncould be represented in the original language. By defining J in the language, we neatly sidestep such \nconsiderations. Laws of the Tick Algebra Throughout, we follow the standard convention that all bound \nvariables in the statement of a law are distinct, and that they are disjoint from the free variables. \n  Px.W Y 2 wvxl (PI case cj y of {ci i?i * Mi} z /Mj [@zj] (Use-P) let {Z = V,% = li$]} in C[x] 2 let \n{x = V,a= fjrdVj} in c[z V] (value-P) let {Z = Z, y = @z]} in C[Z] k let {Z = z, y = Qz]} in c[z] (var-@) \nlet {Z = ~,a = 16[a z]} in @[aJ~] R let {ci7 = z,y = Qz]} in C[z] (var-abs) let {x = ,2,d= II?} in N \nk let {x = z, y = A&#38;z/x]) in N[z/x] (var-sub&#38;) Figure 2: Beta laws for call-by-need. iE[case \nM of {pati + Ni)] 2 case M of {pati -) IE[Ni]} (case-E) E[let (3 = A?) in N] 2 let {Z = AZ} in !E[N] \n(let-E) Figure 3: Laws for evaluation contexts. let {a= ~?,a= n?}inN~let(y =1\\;1}inN, ifZf FV(h?,N) (SC) \nlet {Z = J?} in let {y = G} in N 2 /let {Z = i, y = G3 in N (let-flatten) let{x=let(y =t,Z=A?) in N) \nin N 2 let {x = let {,Z = A?) in N, y = I?) in N (let-let) /(Xx.let {y = C, Z= A?) in N) 2 let {y = ?3 \nin Xx.let {Z= A?) in N (let-float-val) let {x =M,y =~3inx~let{x=M,y =Ij3inaJM, ifx#FV(M,i$ (inline) Iet \n{z= iial,y = ?az,z = $3 in N 2 let {Z= ~oz(Ts,~= n?a,)in Nc73, 01 = [$~],c~ = [$4,03 = [$$, (value-copy) \nFigure 4: Laws for dealing with lets. RkM (W Mkt, iffMES3 (imp-O) MSil, iffMk/M (diverge) let{x=Qy = \n6[x]> in C[x] 2 let {x = 0, y = Ii!443 in C[C2] (Q-P) /(Ax.let {y = a, Z = G} in N) 2 let {y = n3 in \nXx.let (2 = $3 in N (let-float-52) C[/M] e /C[M], if C is strict (J-float) Figure 5: Laws for Q and strictness. \nlet {x = M,y = @ Ml) in C[a/M] k let {x = M, y = l&#38;c]> in @[xl (,B-expand) Figure 6: Beta expansion \nconjecture. proceed until the hole is filled. Evaluation contexts have the following form: lE ::= A ( \nlet {? = II?} in A 1 let {y = A?, zs = A&#38;i],zi = Ar[zz], . . , z,, = A,} in A[za] A ::= [+I( Ax 1case \nA of {cizi + A4i). E ranges over evaluation contexts, and A over what we call applicative contexts. Our \nevaluation contexts are strictly contained in those mentioned in the letrec ex-tension of Ariola and \nFelleisen [AF97]: there they allow II to appear anywhere we have an A. Our flattened definition corresponds \nexactly to configuration contexts (with a single hole) of the form (P, [+I, S), as stated by the following \nlemma, where AC is the set of all evaluation contexts. Lemma 4.1 AE = {trans( r, 11, S) ( all I?, S}. \nThe two laws in figure 3 are very useful indeed: they al- low us to move cases and lets in and out of \nevaluation contexts. A common motif in proofs using the tick al- gebra is the use of (case-E) and (let-E) \nto expose the sub-term of interest. Their validity follows easily from a simple lemma. We can derive \na law which allows us to move ticks in and out of evaluation contexts: IE[JM] z JIE[M] (J-Q Since J is \njust a let expression, this is an instance of (let-Q. Another derived law is the following: let {x = \nM} in E[x] 2 3JE[M], x $ FV (M, IE) (inline-IE) allowing us to inline x when used but once in an evalua- \ntion context. It follows by (let-E), (&#38;line), and (gc). 4.3 Concerning Lets Some of the laws that \nallow us to manipulate lets are presented in figure 4. Law (gc) corresponds to garbage collection: it \nallows us to add or remove superfluous bind- ings. A derived garbage collection cost equivalence is the \nfollowing: let{x=M)inN~JN, z$FV(N) It follows from (gc) and the definition of J. Laws (let-flatten) and \n(let-let) allow bindings to move across each other, and law (let-float-val) concerns the movement of \nvalue bindings across Xs (along with (let-float) below, it forms the essence of the full-laziness transformation, \nas noted in [PJPS96]). (inline) is simple inlining. The last law, (value-copy) says that if we have two \ncopies of a strongly-connected component of the heap (composed solely of values), then we may remove \none of them, pro-vided we perform some renaming. Note that in, for example, the (let-let) axiom, the \nvari- All of the let laws except (value-copy) follow via similar arguments to that for (0) above. (value-copy) \nrequires the use of the same general techniques needed to justify the more complex p laws. 4.4 Divergence \nand Strictness Let Q denote any closed term which does not converge. For example, the black-hole term, \nlet x = x in x, would suffice as a definition for a. The first three laws in fig- ure 5 concern 0 and \nits relationship with k. (0-p) and (let-float-O) are similar to (value-p) and (let-float-val) except \nthat fl is used in place of a value. All of these laws follow in a straightforward manner from the con-text \nlemma and the fact that call-by-name termination behaviour is preserved in call-by-need. We say that \na context @ is strict if and only if C[C!] % 0. Given this definition, we can float ticks out of any \nstrict context, as stated by (J-float). The proof follows by the same techniques used to prove (value-,@. \nIt turns out that this tick floating property can be used as a characterisation of strictness: for all \nC, if C[/z] k C[x], x fresh, then C is strict. This follows since, by congruence, let z = 0 in C[/x] \nk let x = fl in C[x] which implies, by (@?), and (gc), that C[/Q] k C[n]. But since /Q 2 52 (by (0) and \n(imp-n)), C[C!] R C[fl]. Therefore, by (diverge), C[s2] % Q.  4.5 Beta Expansion: A Conjecture In analogy \nto (value-p), we have (,&#38;ezpand) where values are replaced by general terms: let {x = M,y = 6[aJM]} \nin C[aJM] R let {x = M, y = Qx]} in C[x] (,B- expand) The intuition here is that the rule undoes a call-by-name \ncomputation step (a beta-reduction). This is an improve- ment providing we can pay for the potential \ngain that the computation step might have made -which is at most two ticks at each occurrence of the \nvariable which is un- folded. Unfortunately we lack a satisfactory proof for (P-expand). The context \nlemma seems inadequate to establish this property. This seems to be linked to the fact that the axiom \nembodies the essential difference between call-by-name and call-by-need evaluation, and thus it may be \npossible to adapt techniques based on redex-marking [MOW98]. The conjecture can also be used to tie the \nknot when deriving cyclic programs. This possible since we allow x to occur free in M. See the last step \nof the proof of proposition 5.4 for an example of the use of (P-ezrpand) in this context. Using the conjecture, \nwe can also establish the following: able convention ensures that the z do not occur free in J(Xx.let \n{y = i;, z = AT?} in N) the i; in (let-float-val), the convention guarantees that k let {y = 2) in Xx.let \n{Z = A?} in N (let-float) x is not free in the Q. which concerns moving non-value bindings out of Xs \n(where the variable convention ensures that z does not occur free in the i). As noted above, this is \nan es-sential part of the full-laziness transformation. Another consequence of the conjecture is standard \ncommon sub-expression elimination: + C[2JM] k let {z = M} in C[z] (=-e) Again, the convention ensures \nthat any free variables of M are not captured by context C. 4.6 Constant Factors and the Calculi We \nreproduce the axioms of the call-by-need calculus of [AFM+95], in figure 73. The laws collected in figures \n2, 3, and 4 subsume the call-bv-need lambda calculi (in both cases minus the symmetry law): each calcului \nrewrite rule of the form L -+ R turns out to be either an outright improvement, i.e. L k R, or, in the \ncsse of (let-A), an improvement module tick : /let y = (let z = L in M) in N k let z = L in let y = M \nin N. This follows by (let-flatten), (let-let), and (J-elim). The extra tick is needed in this case \nis because without it the right-hand side might (in the case when y is not used) perform one extra heap-allocation \nstep. What is more interesting is that in each case we can re- verse the improvement modulo tick. In \nother words, there exists an R , obtained from R by inserting ticks, such that R R L. This fact will \nenable us to prove that any two terms related by these calculi compute within a constant factor of each \nother in any program context. Thus the best (worst) speedup (resp. slowdown) program obtain-able in these \ncalculi is linear. First it is natural to generalise the idea of improvement modulo ticks. Definition \n4.1 (Imp. within a Constant Factor) We say that M is improved by N within a constant factor, writ-ten \nM &#38; N, if there exists a k such that for all @ such that C[M] and C[N] are closed, C[M]JJ a C[N]J,l@@+ \n). So M g N means that N is never more than a constant factor slower than M (but it might still be faster \nby a non-constant factor). Note that the constant factor is independent of the context of use. It can \nbe seen that &#38; is a precongruence relation (to show transitivity requires a small calculation) and \nclearly contains the improvement relation. Now we consider a special case of k, namely programs which \nonly differ by ticks. Let M 3 N mean that N can be obtained from M by removing some ticks (from In the \noriginal paper V ranges over variables as well as values. In addition, Ariola and Felleisen [AF97] restrict \nC in (let-v) to be evaluation contexts. anywhere within the term), and M A N mean that there exists an \nL such that M 4 L and N % L. Clearly 4 is a precongruence and L is a congruence. Lemma 4.2 MAN =+ M $ \nN. PROOF. (Sketch) Clearly $ C 42, so it suffices to show that M 4 N ==+ N g M. First show that the nesting \nof ticks in a configuration never increases as computa- tion proceeds (easy to see since the rules never \nsubstitute terms for variables). Then let k be the maximum nesting of ticks in M, and show by induction \non the length of the computation that C[N]JJn implies C[N]Jj (n+ ) (strength-ening this statement to \nconfigurations). 0 With this lemma we can establish the following: Theorem 4.3 For all terms N and M \n(of our restticted syntax) if M =XEED N then M $ N. PROOF. (Sketch) By induction on the proof of M =SEED \nN. The base case requires us to show that the (oriented) equations are contained in !&#38;. This follows \neasily since they are all either improven&#38;ts or improvements mod-ulo tick. In the inductive cases, \nthe congruence and tran- sitivity rules follow from the inductive hypothesis since $ is a precongruence. \nThe only difficult case is symmetry. It will be sufficient to prove that reversed equations are contained \nin $. For each equation L =SEED R we have from the laws an R such that R -5 R and R R L. By lemma 4.2 \nwe know that R &#38; R , so R $ L follows from the fact that k C 5 and transitivity of &#38;. 0 Corollary \n4.4 The call-by-need calculzls of [AFn/r 95] cannot improve (OT women) a program by more than a constant \nfactor. We are confident that this result can be extended to Ar- iola and Blom s sharing calculus XOSH,kRE \n[AB97] since al- most all the rules are represented more or less directly in the collection of improvement \nlaws. It is interesting to note that we assembled our collection of laws by need , considering what was \nrequired to tackle a number of ex- amples, and it was encouraging to find that we had al-ready covered \nalmost all of Ariola and Blom s rules. As it stands however, our (value-copy) cost equivalence is not \nas expressive as Ariola and Blom s value-copy rule.4 We believe that Ariola and Blom s value-copy rule \nis a cost equivalence, but their formulation of the rule is rather indirect, so it is not obvious to \nus how to prove this. 5 Syntactic Continuity We wish to say something meaningful about recursive functions \nwith this theory, and a natural starting point is to attempt to mimic the fixed-point induction Scott-style \n4Thanks to Stefan Blorn for providing an example, and to Zena Ariola for pointing out an error in the \nuse of an earlier formulation of our value-copy rule. (Xz.M) N =xEED let z = N in A4 (let-Z) let z = \nV in C[cc] =xEED let x = V in C[V] (let-v) (let x = L in M) N =XEED let x = L in MN (let-C) let y = (let \nx = L in M) in N =SEED let z = L in let y = M in N (let-A) Figure 7: Axioms of the call-by-need calculus \n[AFM+95]. denotational semantics. Examples of this kind of oper- ational analogue to Scott induction \nfor other languages may be found in e.g., [Pit97b, Smi91, MST96, San97, Las98]; we present the first \nsuch result for a call-by-need semantics. We will use the following mechanism to describe the syn- tactic \nunwindings of a recursive function. In the defini- tion, the fi are distinct, new variables. Then, for \nan f defined by let {f = V} in f, we define the nth unwinding as let {f g V} in fn. If we expand the \ndefinition off g V, we see that this is really let {fo = a, fi = V[fO/f], . . . ) fm = v[f+ /f]) in \nfn. Note that we have restricted our attention to f whose defining body is a value; this unwinding trick \nwould not work for general cycles (since loss of sharing would render the exercise pointless). To extend \nthe method to cycles would require some extension to the language, but this would lead to the problem \nof showing that the extension is conservative with respect to the improvement relation. The point is \nthat the functions let {f g V} in f,, com-pletely characterise the behaviour of let {f = V} in f. This \nis the essence of Scott induction. The main prop-erty that justifies this is a syntactic notion of continuity, \nwhich says that let {f = V} in f is the least upper bound of chain {let {f 2 V} in fn}+o and that any \nM which uses f preserves this property. We first show that {let {f z V} in M[fn/f]},+O does indeed form \na chain with respect to k, and that let {f = V) in A4 is an upper bound of that chain. Lemma 5.1 Vn.let \n{f 2 V} in M[f+f] k let {f 2 V} in M[fn+l/f] k let {f = V} in M. PROOF. We prove only the second improvement, \nthat for all n, let {f g V) in M[fn/f] k let {f = V} in M. The first follows by a similar argument. We \nproceed by induction on n. The base case follows easily by (gc) and the Cl laws, and the inductive case \nfollows by (J-elim), since /let {f z V} in M[fm/f] k /let {f = V) in M To establish syntactic continuity, \nwe will need the follow- ing lemma. It says that if let {f = V) in A4 converges then there must exist \nsome unwinding that does so with the same cost. Lemma 5.2 (Unwinding) For all r, S, and n, (r, let {f \n= V} in M, S)lJ * 3m.(l?, let {f 2 V} in M[t/f], S)&#38; . Definition 5.1 f z V gf fo = 0, f 2 v !Lf \nf Ai v, fn+l = V[frn/f]. Theorem sound proof 5.3 (Syntactic de: Continuity) The following is a Vdet \n{f g V} in M[fn/f] R N let {f = V) in M k N PROOF. Assume (I , let {f = V} in M, S)Jjn. Then by the \nUnwinding lemma, there exists some m such that (I , let {f E V} in M[fm/f], S)JJ . By the premise, we \nhave that (P, N, S).&#38;l , and the result follows by the context lemma. 0 Syntactic continuity is also \nvalid for mutually recursive functions. This proof rule is sound for strong improve- ment, but note that \nthe base case of the premise requires that N be contextually equivalent to 0. This tends to limit the \napplicability of the strong improvement version of syntactic continuity. As an example of the use of \nsyntactic continuity, we show that an unwinding fixed-point combinator is improved within a constant \nfactor by a knot-tying fixed-point combinator. Proposition 5.4 Zf (P-expand) is valid, then let Tee = \n(Af.let x = ret f in f x) in ret g let fix = (Xf.let 2 = f x in x) in fix. PROOF. Let V = Xf.let x = \n/ret f in f x, and ab- breviate V[ eCn/,-ec]by V,. We will show that for all n, let ret g V in ret, \nk (Xf.let x = f x in x. Then the result will then follow by syntactic continuity, since 3JXf.let x = \nf x in x 2 /let fix = (Af.let x = f x in x) (SC) in Xf.let x = f x in x 2 let jk = (Xf.let x = f x in \nx) in fix (value-p) We proceed via induction on n. The base case follows trivially by (imp-O) and (52) \nsince let reco = Q in reco E Cl, and the inductive case follows by the derivation in figure 9. We have \ng and not k because we use a slightly by the derivation in figure 8. . slower version of rec. /let {f \ng V, fn+l = V[fn/f]} in M[fn+l/f] 2 let {f 2 V} in let {In+1 = V[fnlf]} in M[fn+l/f] (let-let) G let \n{f 2 V} in let {g = V[fm/f]} in M[glf] (rename) k let {f = V} in let {g = V[f/f]} in M[g/f] (I.H.) z \n/let {f = V,g = V} in M[glf] (let-let) 2 /let {f = V} in M (value-cow), (gc) Figure 8: The inductive \ncase for lemma 5.1. let ret &#38;L V, recn+l = V, in recn+l 2 let ret g V, recn+l = V, in /V, (vahe-/3) \n2 let ret 4L V in /Xf.let x = (recn f in a4fx (gc), (defn. of K) 2 /Xf.let ret 2 V,3: = /recn f in /fx \n(let-float-val), (let-float-n) 2 3JXf.let x = (let ret P V in ret,) f in aJfx (let-let), (let-E) !2 3JXf.let \nx = ( /Xg.let y = gy in y) f in 2Jfx (I.H.), (rename)  2 /Xf.let x = /let y = fy in y in 2Jfx (P) 2 \n/Xf.let x = /y,y = fy in 2Jfx (let-let) 3JXf.let x = y,y = fy in fy (J-e&#38;n), (var-subst) R  z 3JXf.let \nx = fx in aJfx (gc), (rename) 3JXf.let x = fx in x (p-expand) !z Figure 9: The inductive case for proposition \n5.4. The converse of the proposition is false, since the knot- tying fixed-point combinator can give \nasymptotically bet-ter programs. We can also use syntactic continuity to establish the fol- lowing proof \nrule, which is a syntactic, call-by-need ver-sion of what is called &#38;e&#38;point fusion in [MFPSl]. \nIn the statement, V and W range over value contexts. Theorem 5.5 (Improvement Fusion) Zf @ is strict, \nand  x w ere x 4 FV(V,W,C)uCV(V,W,C),   wwl R WP[ 11 h then for all ID such that x 4 FV (ID) U CV \n(ID), let {x = V[x]} in ID[C[x]] k let {x = W[x]} in D[x]. PROOF. Assume C is strict, and that @[V[x]] \nR W[C[x]]. By syntactic continuity, it suffices to show, for all n and all ID such that x 4 FV (a)) U \nCV (a>), let {x g V[z]} in D[C[xn]] R let {x = W[x]} in lD[x]. The base case follows by this calculation: \nlet {xc = 03 in ID[C[x0]] 2 let {x0 = a} in lD[C[fI]] (w3) 2 let {x0 = 52) in iD[O] (C strict) 2 lIXfl1 \n(SC) 2 let {x = W[x]) in D[a]  (gc) k let {x = W[x]) in lD[x] (0 k xc), (cow) and for the inductive \ncase: let {x 2 a/[~]} in DIC[xn+l]] z let {x g V[x]} in lDIC[aJV[xn]]] (value-p), (gc) 2 let {x g V[x]} \nin lD[ @[V[x,]]] (C strict) k let {x g V[x]) in lD[aJWIC[~n]]] (assumption) k let {x = W[x]} in lD[aJW[x]] \n(I.H.) 2 let {x = W[x]} in lD[x] (vaZue+) Fixed-point fusion can be used to establish a number of general \nfusion laws. It is also central to Tullsen and Hudak s [TH98] approach to program transformation in Haskell. \n6 The Improvement Theorem In this section we introduce a second key technique for reasoning about recursion, \nthe improvement theorem. In [SanSG] a call-by-name improvement theorem was intro- duced as a means to \nprove the extensional correctness of recursion-based program transformations. In this section we show \nhow these results carry over to the call-by-need setting. 6.1 The Problem of Transformations As a motivation \nfor the improvement theorem, consider the correctness problem for recursion-based program transformations \nsuch as unfold-fold; the correctness of such transformations does not follow from the simple fact that \nthe basic transformation steps are equivalences. To take a simple example to illustrate the problem, \nconsider the following transformation by equivalence-preserving steps . Start with the recursive function \nrepeat which produces the infinite list of its argument: repeat 2 = z : (repeat z) The following property \ncan be easily deduced: repeat z g tail(repeat z). Now suppose that we use this local equiv- alence to \ntransform the body of the function to obtain a new version of the function:  repeat z = 2 : (tail (repeat \n3~)) This definition is not equivalent to the original, since it can never produce more than first element \nin the list. How did equivalence-preserving local steps produce a non-equivalent function? Analysing \nsuch transformations more carefully we see that while it is true that M!ZN =+ let{x=M}inLSflet{x=N}inL \n(6.1) it is no longer the case when the transformation from M to N depends on the recursive definition \nof x itselk let {Z = M} in A4 2 let {x = M} in N a let (x = M} in L g let {x = N} in L. But in order \nto reason about interesting program trans- formations (e.g. unfold-fold, recursion-based deforesta-tion, \npartial evaluation with memoization), inference (6.1) is simply not sufficient. The improvement theorem \ncomes to the rescue: let {x = M) in M k let {x = M} in N  (6.2) let {x = M) in L R let {x = N} in L \n This is sufficient to establish the correctness of recursion- based transformations by requiring -rather \nnaturally - that the local transformation steps are also improve- ments. This was proved for an improvement \ntheory based on call-by-name, so the fact that the theorem gives im- proved programs as well as correctness \nis not considered to be particularly significant. A question left open was whether the improvement the-orem \nholds for a call-by-need improvement theory. We can now supply the answer: Theorem 6.1 (Improvement Theorem) \nThe following proof rule is sound: let {f = V} in V k let (f = V) in W let {f = V} in N k let {f = W} \nin N The inference is also sound when R is replaced throughout with 2 (the cost equivalence theorem). \nThe improvement theorem and the cost equivalence the- orem can also be stated for a set of mutually recursive \ndefinitions. Notation In establishing a premise of the improvement theorem, in the context of some recursive \ndeclarations 9 = a, a derivation of the form let{~=~)inM1~let(g =~}inMz + k let {$ = V} in h/r,. . . \n will be written in the following abbreviated form: when the declarations 9 are clear from the context. \nThe following example illustrates the use of the proof rule, which shows that a representation of the \nstan-dard lambda-calculus fixed-point combinator Y = xf.f ((Xz.f (zz)) Xz.f (XT)) (suitably converted \nto the restricted syntax) is cost equivalent to the non-cyclic ver-sion Tee from proposition 5.4. Proposition \n6.2 let Y = xf.let d = Xy.let z = yy in fz x=dd in fx in Y 2 let 7-m = (Af.let x = 7-w f in f x) in rec. \n PROOF. By a simple B-step derivation, we have that Y k-Xf.let d = Xy.let z = y y in f z x = dd in fx \n2 Xf.let x = Y f in f z Then the result follows by the Cost Equivalence Theo-rem. cl Improvement Theorem \nvs. Syntactic Continuity Sup-pose one wants to establish an improvement of the form let {f = V} in N \nk let {f = W} in N. If the left-hand side is non-recursive (in f) then syntactic continuity is of no \nhelp, since the unwindings (> 0) of the left-hand side will all be identical; conversely, if the right-hand \nside is non recursive (in f) then the improve- ment theorem is not immediately useful, since proving \nthe premise amounts to directly proving the conclusion of the rule. There are, however, many examples \nwhich can be proved by both methods. In these cases the im- provement theorem is often preferable since \nit is more calculational in style. 6.2 Improvement Induction Finally, we mention one last proof rule \nwhich is closely allied to the improvement theorem (in the sense that a closely-related rule can be derived \nfrom the improve- ment theorem); this corresponds to what we called im-provement induction in [San97], \nwhere it was established for any call-by-name or call-by-value language with SOS rules fitting a certain \nsyntactic rule-format. Theorem 6.3 (Improvement Induction) For any M, N, @, and substitution u, the following \nproof rule is sound: M k J@[Mu] N z JCINo] MEN The proof is quite straightforward. Let us take a standard \nexample to illustrate the proof technique: the associativity of append (the list-concatenation function). \nIn order to show that it is an improvement, we need to insert a tick into the recursive branch; this \nis a consequence of the fact that our cost-measure is rather fine-grained. To ease the notation, we will \nmake use of the syntactic identity for general appli- cation from section 3, and we will use an infix \nform of append. Given the recursive declaration (_H) = Xzs.XYs.case xs of nil + YS h : t -+ h : (t-H- \nys), then (St_) l- (zr-ti_y)+-z k z_H(y-#z). To show this, by improvement induction it is sufficient \nto find a context Q: and substitution c such that (-H) t-(x +t-Y) +I- 2 R C[((x +tY) -H-44, (+t) I- z \n+t-(Y +t z) z C[b -MY +I-4bl. The solution is to take Q: to be 5Jcase z of nil h : t ;,f ,st = + b] \nin h : T and o = [t/z]. This context is easily derived by perform- ing a cost equivalence calculation \non the right-hand side until a recurring instance of x -/+(y _H z) is discovered. We omit the derivations. \nBy using suitably slowed versions of append on the right- hand side, we can also show that this is only \na linear speedup in all contexts, i.e. that (St) ~~+l--H(Yst4 $ b-ttY)+tZ. Using the above properties \nwe have been able to prove, with the help of the improvement theorem and an adap- tation of the argument \ngiven in [SanSG], that a mecha- nisable append-elimination transformation described in [Wad881 can never \nslow-down programs by more than a constant factor. What is interesting about this exam-ple is that it \nshows that the improvement theorem can obtain asymptotic speedups using only linear ones, since in particular \nthe transformation is able to turn the naive quadratic-time definition of reverse into a linear-time \nver-sion. 7 Conclusions and Future Work We have presented a rich operational theory for a call- by-need \nbased on an improvement ordering on programs. The theory subsumes the (oriented) call-by-need lambda \ncalculi of Ariola et al. [AFMf95]. The most important extensions are proof techniques for reasoning about \nrecur-sion. Syntactic continuity allows us to prove properties of recursive programs via a kind of fixed-point \ninduc-tion, without sacrificing information about intensional behaviour, like sharing. The improvement \ntheorem and improvement induction are rules for recursion which sup- port more calculational proofs. \nBoth are particularly use-ful in proving the safety of program transformations. Our unit of cost is the \nabstract machine reduction step. This choice simplifies the technical development. A draw- back is that \nit is very fine-grained, so one must carefully track costs of optimisation steps. This bookkeeping can, \nin larger examples, become rather tedious. However, it should be possible to follow our programme with \na much more abstract cost measure. For example, one possi-bility would be to count only the number of \nlookups5. This should significantly simplify the tick algebra, with-out compromising the ability to characterise \nrelative ef-ficiency within a constant factor. An obvious further application of the theory is to for-malise \narguments about the running time of programs, following Sands use of call-by-name cost equivalence for \nthis purpose [San95, San98]. Another direction for future work would be to consider the time-safety of \na larger-scale program transformation, such as deforestation [WadSO]. In such a transformation we must \ninevitably consider conditions under which we can unfold function calls. It is straightforward to define \nsimple syntactic conditions on contexts which guarantee that let {Z= AZ} in C[Z] 52 let {Z= G} in C[n;i], \nbut in the case where holes occur under X-abstractions a more global form of information is required: \none needs to know that the lambda expression in question will not be applied more than once. The type \nsystem of [TWMSS] provides just such global information, so it would be in- teresting to prove that their \nsystem (and generslisations to full recursive lets [Gus98]) does indeed satisfy the de- sired improvement \nproperty above. We saw in section 4.4 that the strictness property of a context can be charac- terised \nexactly by where x is fresh. Could it be the case that the used at most once property might be semantically \ncharacterised by C[x] D @[ z]? Acknowledgements We have benefited from numerous discussions with Jiirgen \nGustavsson on various aspects of this work, and we would thank him in particular for his suggestions \nwhich led to a simplification of the proof of the context lemma. Thanks also to Koen Claessen and the \nreferees for their helpful comments. 50f course, me would need to change the definition of J ac-cordingly. \nReferences [AB97] [AB98] [AF97] [AFM+95] [AK971 [A0931 [BLRSG] [Cur911 [FieSO] [GP98] [Gus981 [HM95] \n[Jef93] Z. M. Ariola and S. Blom, Cyclic lambda calculi, Proc. TACS 97, LNCS, vol. 1281, Springer-Verlag, \nFebruary 1997, pp. 77-106. Z. M. Ariola and S. Blom, Lambda calculi plus letrec, Tech. report, Dept. \nof Computer Sci-ence, University of Oregon, 1998, Extended version of [AB97]; submitted for publication. \nZ. M. Ariola and M. Felleisen, The call-by- need lambda calculus, Journal of Functional Programming 7 \n(1997), no. 3, 265-301. Z. Ariola, M. Felleisen, J. Maraist, M. Oder- sky, and P. Wadler, A call-by-need \nlambda calculus, Proc. POPL 95, the 22 d ACM SIGPLAN-SIGACT Symposium on Princi-ples of Programming Languages, \nACM Press, January 1995, pp. 233-246. Z. M. Ariola and J. W. Klop, Lambda calcu- lus with explicit recursion, \nInformation and Computation 139 (1997), no. 2, 154-233. S. Abramsky and C.-H. L. Ong, Full abstrac-tion \nin the lazy lambda calculus, Information and Computation 105 (1993), 159-267. Z.-E.-A. Benaissa, P. Lescanne, \nand K. H. Rose, Modeling sharing and recursion for weak reduction strategies using explicit substi-tution, \nProc. PLILP 96, the sth International Symposium on Programming Languages, Im- plementations, Logics, \nand Programs, LNCS, vol. 1140, Springer-Verlag, 1996, pp. 393-407. P.-L. Curien, An abstract framework \nfor en-vironment machines, Theoretical Computer Science 82 (1991), no. 2, 389-402. J. Field, On laziness \nand optimality in lambda interpreters: Tools for speciption and analysis, Proc. POPL SO, the 17t ACM \nSIGPLAN-SIGACT Symposium on Princi-ples of Programming Languages, ACM Press, January 1990, pp. 1-15. \nA. D. Gordon and A. M. Pitts (eds.), Higher Order Operational Techniques in Semantics, Publications of \nthe Newton Institute, Cam-bridge University Press, 1998. J. Gustavsson, A type based sharing analysis \nfor update avoidance and optimisation, Proc. ICFP 98, the 3 d ACM SIGPLAN Interna-tional Conference on \nFunctional Program-ming, September 1998, pp. 39-50. J. Hughes and A. K. Moran, Ma/zing choices lazily, \nProc. FPCA 95, ACM Conference on Functional Programming Languages and Computer Architecture, ACM Press, \nJune 1995, pp. 108-119. A. Jeffrey, A fully abstract semantics for con-current graph reduction, Tech. \nReport 93:12, School of Cognitive and Computing Sciences, University of Sussex, 1993. [Jef94] [Jos89] \n[Las981 [Lau93] [Mar911 [MFPSl] [Mi177] [MOW981 [MST961 [Nie96] [Pit97a] [Pit97b] A. Jeffrey, A fully \nabstract semantics for con-current graph reduction, Proc. LICS 94, the gth IEEE Symposium on Logic in \nComputer Science, IEEE Computer Society Press, July 1994, pp. 82-91. M. B. Josephs, The semantics of \nlazy func-tional languages, Theoretical Computer Sci-ence 68 (1989), no. 1, 105-111. S. B. Lassen, Relational \nreasoning about func- tions and nondeterminism, Ph.D. thesis, De- partment of Computer Science, University \nof Aarhus, Majr 1998. J. Launchbury, A natural semantics for lazy evaluation, Proc. POPL 93, the 20th \nACM SIGPLAN-SIGACT Symposium on Princi- ples of Programming Languages, ACM Press, January 1993, pp. 144-154. \nL. Maranget, Optimal derivations in weak lambda-calculi and in orthogonal term rewrit- ing systems, Proc. \nPOPL 91, the 18th ACM SIGPLAN-SIGACT Symposium on Princi-ples of Programming Languages, ACM Press, January \n1991, pp. 255-269. E. Meijer, M. Fokkinga, and R. Pater-son, Functional programming with bananas, lenses, \nenvelopes and barbed wire, Proc. FPCA 91, ACM Conference on Functional Programming Languages and Computer \nAr-chitecture (J. Hughes, ed.), LNCS, vol. 523, Springer-Verlag, August 1991, pp. 124-144. R. Milner, \nFully abstract models of the typed X-calculus, Theoretical Computer Science 4 (1977), l-22. 3. Maraist, \nM. Odersky, and P. Wadler, The cdl by need lambda calculus, Journal of Func- tional Programming 8 (1998), \nno. 3,275-317. I. A. Mason, S. F. Smith, and C. L. Tal-cott, From operational semantics to domain theory, \nInformation and Computation 128 (1996), no. 1, 26-47. J. Niehren, Functional computation as con-current \ncomputation, Proc. POPL 96, the 23&#38; ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, \nACM Press, January 1996, pp. 333-343. A. M. Pitts, Operational semantics for program equivalence, March \n1997, Invited talk at MFPS XIII, the 13th Conference on Mathematical Foundations of Pro-gramming Semantics, \nslides available at http://www.cl.cam.ac.uk/users/ap/talks/ mfps97.ps.gz. A. M. Pitts, Operationally-based \ntheories of program equivalence, Semantics and Logics of Computation (P. Dybjer and A. M. Pitts, eds.), \nPublications of the Newton Institute, Cambridge University Press, 1997, pp. 241- 298. [PJPSSG] [PJS98] \n[Ros96] [San911 [San951 [San961 [San971 [San981 [Ses97] [SmiSl] [SPI96] [SPJ97] S. Peyton Jones, W. Partain, \nand A. San-tos, Let-floating: moving bindings to give faster programs, Proc. ICFP 96, the lat ACM SIGPLAN \nInternational Conference on Func- tional Programming, ACM Press, May 1996, pp. 1-12. S. Peyton Jones \nand A. Santos, A transformation-based optimiser for Haskell, Science of Computer Programming 32 (1998), \nno. 1-3, 3-47. K. H. Rose, Operational reduction models for functional programming languages, Ph.D. thesis, \nDIKU, University of Copenhagen, Denmark, February 1996, available as DIKU report 96/l. D. Sands, Operational \ntheories of improve-ment in functional languages (extended ab-stract), Proc. 1991 Glasgow Functional \nPro-gramming Workshop, Workshops in Com-puting Series, Springer-Verlag, August 1991, pp. 298-311. D. \nSands, A nacve time analysis and its the- ory of cost equivalence, Journal of Logic and Computation 5 \n(1995), no. 4, 495-541. D. Sands, Total correctness by local im-provement in the transformation of functional \nprogram, ACM Transactions on Program-ming Languages and Systems (TOPLAS) 18 (1996), no. 2, 175-234. D. \nSands, From SOS rules to proof princi- ples: An operational metatheory for func-tional languages, Proc. \nPOPL 97, the 24th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, ACM Press, January \n1997. D. Sands, Improvement theory and its appli-cations, In Gordon and Pitts [GPSB], pp. 275- 306. P. \nSestoft, Deriving a lazy abstract machine, Journal of Functional Programming 7 (1997), no. 3, 231-264. \nS. F. Smith, From operational to deno-tational semantics, Proc. MFPS VII, the 7th Conference on Mathematical \nFounda-tions of Programming Semantics (S. Brookes, M. Main, A. Melton, M. Mislove, and D. Schmidt, eds.), \nLNCS, vol. 598, Springer- Verlag, March 1991, pp. 54-76. J. Seaman and S. Purushothaman Iyer, An operational \nsemantics of sharing in lazy eval- uation, Science of Computer Programming 27 (1996), no. 3, 289-322. \nP. Sansom and S. Peyton Jones, Formally-based profiling for higher-order functional languages, ACM Transactions \non Program-ming Languages and Systems (TOPLAS) 19 (1997), no. 1, 334-385. [TH98] M. Tullsen and P. Hudak, \nAn intermedi-ate meta-language for program transforma-tion, YALEU/DCS/RR 1154, Yale Univer-sity, June \n1998. [TWM95] D. N. Turner, P. Wadler, and C. Mossin, Once upon a type, Proc. FPCA 95, ACM Conference \non Functional Programming Lan-guages and Computer Architecture, ACM Press, June 1995, pp. 1-11. [Wad881 \nP. Wadler, The concatenate vanishes, Tech. report, University of Glasgow (UK), 1988, ap- peared as a \nnote on an FP electronic mailing list, December 1987. [WadSO] P. Wadler, Deforestation: Bansforming pro-grams \nto eliminate trees, Theoretical Com-puter Science 73 (1990), 231-248. [Yos93] N. Yoshida, Optimal reduction \nin weak-lambda-calculus with shared environments, Proc. FPCA 93, ACM Conference on Func-tional Programming \nLanguages and Com-puter Architecture, ACM Press, June 1993, pp. 243-254.  \n\t\t\t", "proc_id": "292540", "abstract": "", "authors": [{"name": "Andrew Moran", "author_profile_id": "81100516264", "affiliation": "Department of Computing Science, Chalmers University of Technology and the University of G&#246;teborg, S-412 96 G&#246;teborg, Sweden", "person_id": "PP14179363", "email_address": "", "orcid_id": ""}, {"name": "David Sands", "author_profile_id": "81100313762", "affiliation": "Department of Computing Science, Chalmers University of Technology and the University of G&#246;teborg, S-412 96 G&#246;teborg, Sweden", "person_id": "PP14114603", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/292540.292547", "year": "1999", "article_id": "292547", "conference": "POPL", "title": "Improvement in a lazy context: an operational theory for call-by-need", "url": "http://dl.acm.org/citation.cfm?id=292547"}