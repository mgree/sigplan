{"article_publication_date": "01-01-1999", "fulltext": "\n Continuous Grammars MARTIN RUCKERT ruckertmQacm.org State University of New York at New Paltz Abstract \nAfter defining appropriate metrics on strings and parse trees, the classic definition of continuity is \nadapted and applied to functions from strings to parse trees. Grammars that yield continuous mappings \nare of special interest, because they provide a sound theoretical framework for syntax error correction. \nContinuity justifies the approach, taken by many error correctors, to use the function output (the parse \ntree), and all the additional information it provides, in order to find corrections to the function input \n(the string). We prove that all Bounded Context grammars are continuous and that all continuous grammars \nare Bounded Context Parseable grammars, giving a characterization of continuous grammars in terms of \npossible parsing algorithms. Introduction The problem of recovery horn syntax errors in compilers is \nnot yet satisfactorily solved states Richter[lJ] and the shear amount of literature on syntactic error \nhandling (see [7]) attests to its difficulty. Most practical methods lack a sound theoretical foundation \nand even worse, some thec-retical investigations[20] conclude that automatic tools will never be able \nto generate good error correctors. Today, the best error-correction methods for LR(lc) par-sers, as described \nin [8], are characterized by a twephase approach: After the error is detected, the first phase, called \nthe condensation phase, attempts to parse the unread por- tion of the input, so gathering right context \n(forward moue). Some methods also collect the left context of the error (baclc- ward move). The second \nphase, called the correction phase, uses the previously gathered information to modify the to- ken sequence \nand/or stack configuration to restart the anal- ysis and generates diagnostic messages. To gain a foundation \nfor this method, two questions must be answered: . Is it possible to obtain a reliable analysis of the \nleft or right context of the error as a basis for the correction phase? Permission to make digital or \nhard copies of all or part ofthis work for personal or classroom use is granted without fee provided \nthat copies are not made or distrihutcd for prolit or commercial ad\\,antage and that copies bear this \nnotice and the full citation on the first page. TO copy otherwise. to republish, to post on sewers or \nto redistribute to lists, requires prior specific permission and!or a fee. POPL 99 San Antonio Tesas \nUSA Copyright ACM 1999 I-581 l3-0953/99/01...$5.00 . Is it possible to find the correction, based on \nthe results of the condensation phase, for some reasonably defined class of errors? For LR(lc) 8~ammars, \nthe analysis of the right context might depend on the entire left context and in the presence of errors \nthe complete left context is usually not available. The left context, as captured by the parse stack, \nmight not be reliable; especially SLR or LALR parsers can produce unwanted reductions before shifting \nthe error-token. A pathological example can illustrate how hopeless the situation is: Given a string \nLY and two arbitrary parse trees &#38; and &#38;Z for this string, it is possible to write two LR(lc) \ngrammars G1 and Gs with start symbols S1 and SZ that produce the respective parse trees &#38; and &#38;Z \nfrom (Y. Adding two new terminal symbols a and b, and the productions S -+ a&#38;aa and S + bsabb, we \ncan easily combine both grammars into a new LR(lc) grammar G. Now a single token error can change the \nstring aaaa into baaa. A typical LR(L) parser will detect this error when reading the double a at the \nvery end of the string. With bS2 as stack configuration, the left context does not provide any clue for \nthe correct repair, the parse tree &#38;a is completely wrong, and it takes an arbitrary large amount \nof work to change &#38;Z into &#38;1 before parsing can continue. Unfortunately, similar examples can \nbe found in com-mon programming languages. Just consider the following program-fragments written in C[4]: \nExample 1 figments of C programs if( a, *b. **c=O, d[3]) ; else fi( a, *b, **c=O, d[31); int a, *b, **c=O, \ndC31 ; Here, the comma separated list is either the controlling expression of an if-statement, using \nthe comma operator, or the argument list for the function fi, using the comma as a separator, or a list \nof variable declarations. A small change in the input will send the parser into completely dif-ferent \ndirections-whether this direction is correct or not- producing quite diierent parse trees. Further examples \nare used in [20] to conclude that good automatic error correction is impossible. In contrast, this paper \nhas a more optimistic view. It defines a class of grammars that are reasonably powerful and provably \nwell behaved in the presence of errors. Of the two questions given above, this paper deals al- most exclusively \nwith the second. We assume a parser that extracts as much structure as possible from a given string of \ninput tokens (condensation phase) and ask how difficult it is to repair the resulting parse trees. It \nturns out that the class of continuous grammars not only offers a solution to this problem, but is a \nproper subclass of the Bounded Context Parseable grammars[21], a class of grammars that provides an answer \nto the first question. The core of this paper are the metrics defined in sec-tion 2. In contrast to \nprevious work[20] the metrics given here are derived directly from the parsing process and use the rich \nrepresentation of parse trees. Based on these met- rics, section 3 explains the notion of continuous \ngrammars. Grammars for programming languages are designed to yield a mapping from strings of input tokens \nto parse trees and a parser is a program to compute this mapping. Intuitively, this mapping is called \ncontinuous, if small changes in the token string will cause only small changes in the resulting parse \ntree. Specifically, a continuous grammar ensures that the effect of any single token error is confined \nto a bounded set of nodes of the parse tree. Section 4 investigates the re- lationship of continuous \ngrammars to other classes of gram- mars. The final section 5 discusses the results and provides links \nto related work.  2 Notation and Definitions To avoid a lengthy section of standard definitions, we \nfollow, as closely as possible, the notation system and definitions of [2] chapter 4, pp. 166, and provide \nhere only a short informal summary. Wherever our notations and definitions are new or different, full \ndetails are given. 2.1 Symbols, Grammars, and Derivations a, b, c, I[. areused for terminal symbols; \nS, A, B, C, are used for nonterminal symbols; X, Y, 2 for symbols in general; and (Y, p, y, p, cr, r \nfor strings of symbols. The length n of a string LY = X1 . - * X, is denoted as Ia]. Grammars are described \nwith rules of the form A + (Y. The maximum length of the right hand sides over all rules m = max{ lo] \n: A + cy} is called the branching factor of the grammar. Grammars define languages through the process \nof de- rivation. Given the string EAT and the rule A -+ cy, we can derive the string U(YT by replacing \nthe occurrence of the left hand side of the rule by its right hand side. We write ~AT=SU~YT. Derivation \nis the reflexive and transitive closure of *; it is written 4. Strings that cau be derived from a special \nstart symbol are called sentential forms. Sentences are sentential forms containing only terminal symbols, \nand the set of all sen-tences defined by the grammar is the language. 2.2 Parse Trees Parse trees capture \nthe same information as derivations but filter out the choice regarding the replacement order. We use \np ST] to denote the parse tree that is defined by the derivation p 3-y. When the derivation is clear \nfrom the context or not essential for the current argument, we will write 6 for the parse tree p 4 71. \nA parse tree is an ordered list of nodes, each labeled with a symbol, together with a parent/child relation. \nFor a given derivation, the corresponding parse tree is defined by induc- tion over the length of the \nderivation. For the basis, a string p = YlYZ... Y, with the trivial derivation /I &#38;-p corre-sponds \nto a list of nodes labeled Yi, Yz, . . . , and Y,. For the induction step, 4 with XiXz if p ~AT=CVXYT \n(Y = * . . X,, we add a list of new nodes labeled Xi, X2, . . . , and X,,, that correspond to the symbols \nof (Y, and these nodes are child nodes of the node that corresponds to A. The symbols of u and r on the \nright side of =+ correspond to the same nodes as the same symbols on the left side. Note that a parse \ntree according to our definition is a tree in the traditional sense, with a single root node, only if \n]pI = 1; otherwise a parse tree, here, is a list of several tree structures, one for each symbol of p. \nStrings are actually s ecial cases of parse trees. Using the trivial derivation /I P ,L3 strings are \nembedded in the larger domain of parse trees. This natural embedding sim-plifies the description of parsing \nas a transformation on parse trees. No special consideration needs to be given to input strings . For \nexample, all metrics defined in the next section are metrics on strings a.9 well. Parse trees, however, \ncan contain more information than plain strings-namely the grammatical structure-and this information \nis essential for the definition of metrics appro-priately reflecting the parsing process. 2.3 Metrics \nIn short, the difference between two parse trees is described by sets of nodes, and we will measure the \nsize of these sets by something like the diameter of the set, limiting the distance of two elements of \nthe set. We start by defining the distance dh of two nodes X and Y in a parse tree &#38;: If X and Y \nare two root nodes and CY = a.. XuY .- ., the distance is naturally defined by Q (X, Y) = &#38; (Y, X) \n= ]u] + I. This definition is extended to internal nodes X and Y of [ol 3 r] by defining d&#38;(X, Y) \n= 0, if X is an ances-tor of Y or Y is an ancestor of X; otherwise, db (X, Y) = min{da(X,Y)] a! SD}. \nIn summary, the distance of two nodes X and Y is mea sured by the distance of the corresponding symbols \nX and Y in a derivation. Since, however, a parse tree can have many different derivations and many symbols \nin a derivation cor-respond to the same node, we take the shortest possible distance, or the minimum \ndistance. The ultimate reason to define the measure 4 as given above is however the parsing process. \nde (X, Y) is the short- est distance that the nodes X and Y will have as entries on the parse stack for \nall possible parsers. A bound on dh (X, Y) is therefore of practical importance: for an optimal parser, \nit limits the amount of nodes on the parse stack that need to be considered. Note, that dd is not a topological \nmetric; the inequality dk(Y, 2) < d&#38;(X, 2) + d&#38;(X, Y) does not hold. The reason is simply that \nthe node X might be an ancestor of both Y and 2. The following lemma gives a restricted form of this \ninequality that is needed for the proof of Theorem 1. Lemma 1 Let &#38; be a parse tree, let X, Y, and \nZ be nodes in &#38;, and let Y be the parent node of X. Then dk(Y, 2) < dh (X, 2) PROOF. If dh(Y,Z) = \n0, then there is nothing to prove, otherwise Y is not an ancestor of 2. If d&#38;(X, 2) = 0, then X and \nZ have an ancestor relation, and hence Y and Z are in an ancestor relation, implying db(Y, 2) = 0. Otherwise, \nX is not an ancestor of 2 and dd (X, 2) = mln{dp(X, z)l Q SP], since X, having a parent node Y, can not \nbe a root node. Assume that the minimum defining d&#38;(X, 2) is reached in p. Therefore, our final messure \nof distance between parse trees adds up the diameters of node sets each repairing a I \\ one token in \nthe input string. To normalize this metric, . . . . we measure the size of the node set relative to \nthe size Y . .   ir-I2- /A \\ . . . .x. . . . . . .z. . . 3-T Figure 1: Parse Tree supporting the Proof \nof Lemma 1 The replacement Y --t aXr must be one step in the derivation (Y %p. Without loss of generality, \nwe assume that 2 is to the right of X, then p has the form p = . . . xyg. . . with r gp (see Figure 1). \nThis implies that there is a derivation [Y 3 . . . Y+yZ. . . and therefore we have 4(Y,Z)I(~l+l~lpvl+l=Q(x,z). \n. Given a parse tree &#38; and a set S of nodes of the parse tree, the size of S, written as ISI, is \ndefined as the smallest number n E lNe such that d&#38;(X, Y) < n for all X E S and Y E S. The size ISI \nof S measures not simply the number of nodes in S but is something like the diameter of S, reflecting \nthe fact that local changes are easier to accomplish than global changes. As a measure of the distance \nof two parse trees &#38; and p, we use the minimum amount of change needed to transform one parse tree \ninto the other. To accomplish the transfor- mation, first some nodes in &#38; are deleted, Del(&#38;, \np) is used to denote this set of nodes. Then some nodes are inserted to obtain /3, this set of nodes \nis written as Ins(&#38;,P). Now, we defiue the distance of two parse trees &#38; and fi, written I&#38;-b], \nas the smallest number n such that there are setsDel(h,&#38; andIns with ]Del(h,fi)]+]Ins(h,$)] = n. \nThe distance 1 1is not yet a topological metric on parse trees, because the triangl~inequality I&#38;-b] \n< I&#38;-31 + 13 -fi] does not hold. The problem with this inequality is that it would allow to decompose \nthe change from &#38; to B into a sequence of smaller changes. The smaller changes are reflected by smaller \ndistances which, never the less, will yield an upper bound on the cumulative change. Of course any transformation \nof parse trees can be accomplished by chang-ing one node at a time, and this would reduce the measure \nof distance to merely counting the number of diierent nodes. This model, however, would not be very realistic. \nIn practice, an error correction that affects two nodes far apart in the parse tree is much more difficult \nto find than a correction that affects two nodes close together. Further, error correctors usually try \nto correct the input one error at a time. This is reasonable, since syntax errors in real programs tend \nto be sparse[l4], and efficient. In contrast, the search for a transformation that repairs all errors \ncombined with the globally least cost is prohibitively expensive[l][ll]. of the input string. We use \n)1. 11to denote this normal-ized single token error distance and define it as follows: For two parse \ntrees [c~ 4~1 and p $71 with ]o -r] = 1 we define I]&#38; -,011 = !&#38; -P]/max(]~], Ir]}. Of course \n(18 -/3]] = 0 iff &#38; = /3. If &#38; # p there are sequences [oo 4uo],... , [a, 4 CT,,]with ho = &#38;, \n&#38; = ,$, and loi -ai+i I = 1 that decompose the change from &#38; to b into sin- gle token errors \n. Considering all such seauences we define It is easy to verify that I] . II indeed defines a metric \nand makes the set of parse trees a topological space. The continuous mappings on parse trees as defined \nin the next section are exactly the Lipschitz-continuous1 (see e.g. [S]) functions regarding this topology. \nBecause the measure ) e) is more intuitive to use and closer to the parsing process, we will develop \nan equivalent definition of continuity using this measure and will continue to use it throughout this \npaper. 3 Continuity of a Grammar 3.1 Handle Pruning Since strings are just special cases of our generalized \nparse trees, Grammars define mappings on parse trees. The com-putation of such a mapping is called parsing. \nIt is the re verse process of derivation. Here, we specifically investigate bottom-up parsing by handle \npruning. A handle of a sentential form /3 is defined as a substring (Y of /l and a rule A + a such that \nthe replacement of A by cy is one step in the derivation of 0. Note that our defi-nition does not require \n(Yto be the leftmost substring with this property, a restriction that is often included in the defi-nition \nof handle because LR(k) parsing, called canonical parsing , is the main parsing technique under consideration. \nOur definition is less restrictive allowing non canonical pars-ing techniques[lb, 191. A derivation of \na sentential form /3 can be reconstructed by successively identifying a handle of P = Q(Y~and replacing \nthe substring (Y by A to yield aAr until no more handles are left. This process is called handle pruning. \nGrammars for programming languages are usually de-signed to map sentences to uniquely2 defined parse \ntrees and therefore parsing by handle pruning defines a function mapping sentences to parse trees. In \nthe following, the sym-bol f is used to denote such a function mapping p to a parse tree f UJ>. Since \nthe nature of our investigation makes it necessary to look at arbitrary strings, the above definition \nneeds to be generalized. Our generalization is motivated by the aim of providing a reliable analysis \nof the left and right context of a syntax error. Imagine a string p, which is a correct fragment of a \nprogram between two successive errors, and a substring a! of p together with a rule A + a. The pruning \nof cuwill be valid, regardless of the possible continuation of p, if for all sentential forms p that \nhave p as a substring the replacement of A by cy is one step in the derivation of /?. In general, we \ndefine a handle of an arbitrary string 7 as a substring cy of y and a production A + a such that there \nIt makes no sense to discuss plain continuous mappings relative to this topology since it is discrete \n(all mappings are continuous). All grammars here are unambiguous. is a substring p of 7 containing \nCY such that for all sentential forms p that have p as a substring the replacement of A by cx is one \nstep in the derivation of /3. The definition means that the substring Q! is in a local context p, which \nensures the correctness of handle pruning independent of the global context 7, which might not be a sentential \nform at all. Using the generalized definition of handle pruning, parse trees can be computed for arbitrary \nstrings. With an unam- biguous grammar, the parse tree f(r) is well defined for all sentences 7, and \nif y is a program containing syntax errors, f(r) will be a conservative approximation to the correct \nparse tree. There are usually still several possible ways to extend the function f to arbitrary strings. \nIn the following, we assume that f denotes one such extension. 3.2 Continuity The usual definition of \ncontinuity for a function f is: Va:vE>038>0vJ/]z--y~ <6 + If(z)-f(y)1 <E Intuitively this reads: if \nwe can make the difference be- tween z and y (the error in y) small enough (less than 6) the difference \nafter application of f will also be small (less than any given E). And applied to error correction it \nmeans: if f is continuous, an error corrector capable of correcting parse trees up to an error distance \nE will be sufficient to correct all input errors smaller than 6. This definition cannot be used directly \nbecause strings are finite objects and the natural topology is discrete. Unlike the case of real analysis, \nthere exists a smallest distance between two different strings (Y and 0. One could always choose 6 < \n1 and satisfy the condition trivially. Therefore we replace the requirement ]z -yI < 6 by Icy - /3] = \n1, allowing a single token error (the smallest possible distance). The restriction to single symbol changes \nin (Y is not a serious limitation, since any large change can be decomposed into a sequence of single \nsymbol changes. If we cannot make the bound 6 arbitrarily small, we can- not expect that it is possible \nto satisfy the condition for an arbitrarily small E. But even though 1 is the smallest possible distance \nbetween two d&#38;rent strings, we perceive the change of one symbol in a 10,000 symbol sentence as a \nfairly small change, compared to the change of one symbol in a three symbol sentence. Therefore it is \nreasonable to require that If (a) -f @)I is bounded by a constant. This implies that the difference between \nf(a) and f(p) is %el- atively small for sufficiently large cy, for which, in turn, a one symbol change \nfrom cy to /3 is also relatively small . In the above definition E and 6 may depend on x and one could \nconsider to allow the bound on If (a) - f @)I to depend on (Y as well. The fact, however, that cy and \nf(a) are finite objects gives a trivial upper bound: 2]a] + 1. Therefore, only a uniform bound for all \ncy makes sense-similar to the definition of uniform continuity in real analysis. We finally obtain the \nfollowing definition: f is called continuous if 3n E INvov/3 Icy -PI = 1 + If(a) -f(P)1 <It It is easy \nto prove that this definition is equivalent to   ~~~~~~~~Ilf~~~-ff(P~II~~llQ-PII This is the usual \ndefinition of Lipschitz-continuous (see e.g. [S]) relative to the metric 1 I . I I. A grammar is called \ncontinuous, or a C grammar, if there exists an (extension) f that is continuous. The following example \npresents a grammar that is continuous. Example 2 S+aBA S+cBC A-+x C+X B+b B+bB Language = {abnx, cbnr) \nThis grammar illustrates how a kind of unbounded left context can be used during parsing without, at \nthe same time, propagating errors through large parts of the resulting parse tree and destroying continuity. \nExample 1, on the other hand, illustrates a discontinuity: a small error can propagate and change structure \nand meaning of arbitrarily large parts of a program. Lemma 2 The grammar of example 2 is continuous. \nPROOF. Assume an arbitrary string cy. Any maximal sub-string that contains only b s can be reduced to \na single B. Then, given a substring of the form aBx the x can be reduced to A. The same idea holds for \ncBx. Finally, substrings of the form aBA and cBC can be reduced to S. In case (Y was a sentence of the \nlanguage, this results in a parse tree with a single root. Now consider obtaining p by changing a single \ntoken of CY. Here only two representative cases are presented; all others are mere variations and do \nnot add further insight. For each case, we show that If (a) -f @)I is bounded by a constant. . Deleting/Inserting \na b into a sequence of b s is equiva lent to deleting/inserting the first b of a whole sequence of b \ns The change to f(a) is a missing/extra B, the deletion of the old S, and the insertion of a new S. Therefore \nIf(a) -f(P)/ = 2. . The maximum Yamage to a parse tree is achieved by inserting an a into a sequence \nof b s preceded by an c and followed by a x. As can be seen from Fig- ure 2, IWfb),fWI = 2 and IWfk4,fW)I \n= dfca)(B,A) + 1 = 4. Therefore If(a) -f(P)1 5 6. 0 4 BC and BCP Grammars 4.1 BC Grammars A grammar \nis called a BC(j,L) grammar-parseable with Bounded Context-, if, given any sentential form, it is pos- \nsible to identify all handles of the sentential form looking at j symbols to the left and L symbols to \nthe right of the handle (for some finite j and k). BC(j, k) grammars never gained much importance, since \nshortly after Floyd[S] presented them in 1964, Knuth[ll], in 1965, introduced LR(k) grammars and showed \nhow very efficient parsers can be constructed for this larger class of grammars. All BC grammars, however, \nare continuous and so form an important subclass of continuous grammars. This is proved next. Theorem \n1 BC(j, k) c C PROOF. Assume we are given a BC(j, k) grammar and two strings CY and /3 with ]c~ -PI = \n1. Two cases have to be considered: p is obtained from cy by inserting one symbol or by deleting one \nsymbol. Both cases are completely symmetric, so it is sufficient to look at only one case.  c b,.. . \nbjbi+l. . b,x. . . Figure 2: Parse Bees supporting proof of Lemma 2, Insert- ing an a Assume CY = Xi \n***Xm, and 0 = Xi ***Xi-iXi+l ***Xn is obtained by deleting a single symbol X; from cy. To compute f(o) \nor f(P), all substrings of Xi . . * xi-k-1 and of Xi+j+i * * * X, which can be identified as handles \ncan be pruned first. Since the maximum context needed is only k symbols to the right and j to the left, \nnone of the handle pruning is affected by the existence or deletion of Xi which can neither be part of \nthese handles nor part of the necessary contexts. The handle pruning can continue this way as long as \nthere are identifiable handles and none of the symbols Xi-k * * Xi+j is part of these handles. So far \nthe nodes generated in the computation of f(o) are exactly the same as those generated in the computation \nof f(p). From here on, the computation of f(a) and f(P) might go in different directions, adding further \nnodes. One can choose Del(f(cx),f(P)) to be the set of all nodes further added in the computation of \nf(o) and Ins(f(cr),f(P)) to be the set of all nodes further added in the computation of f(P), It is clear \nthat each of these additional nodes has at least one of the symbols Xi-k,. . . , Xi+j as descendent node. \nUsing Lemma 1, one can conclude that ]Ins(f(o), f(P))] 5 d/(@)(xi-k,xi+j) 5 k + J - 1 ad IDW(4, f(P))1 \n5 df(a)(Xi-k, xi+j) 5 k -l-j. It follows immediately that If(a) -f(P)] < 2k + 2j, proving the continuity \nof f. 0 Theorem 2 C @ BC(j,k) PROOF. Example 2 presents a continuous grammar that is not BC(j,k). . 4.2 \nBCP(j, k) Grammars We call a grammar BCP&#38; k)-Bounded Context Parse-able-, if, given any sentential \nform, it is possible to identify at least one handle of the sentential form looking at j sym- bols to \nthe left and k symbols to the right of the handle (for some finite j and k).  -7 -T- Y Figure 3: Parse \nTree supporting the Proof of Lemma 3 BCP(i, k) Grammars were introduced by Williams[21] and studied \nextensively in the 197O s[17,18,19]. An interest- ing subclass of BCP grammars is the class of Bounded \nRight Context grammers[9]-basically the intersection of LR(k) and BCP(j, k). This class of grammars is \nlarge enough to generate all deterministic languages but does still allow for fast linear parsing algorithms[l0][16]. \nThe class of LR(k) grammars and the class of continuous grammars are incom- mensurate, there is, however, \na general way to transform any LR(k) grammar into a BCP grammar[lO]. While this technique yields BCP \ngrammars automatically, it does not make the grammars continuous. How to write continuous grammars for \npopular programming languages is illustrated in [16]. We will prove that the BCP grammars form a true \nsuper- set of the continuous grammars, thus locating C grammars just between BC and BCP grammars. First, \nwe prove a lemma estimating the damage caused by a missing symbol X that is crucial for the decision \nto reduce (Y to A later in the parser input. The damage will grow at least logarithmically with the distance \nbetween X and a. Lemma 3 If ,d 3 X~A+r=~Xcycy7 then . IP -T-l > h3,(171)/2 > where m is the branching \nfactor of the grammar. PROOF.By definition of db(X,A), there is a string u with p $XaAr and Q $7 such \nthat da(X, A) = 1~1 + 1 (see Figure 3). We prove by induction on ]u] that there is a node Y in 6 with \nY @ 7 such that dxyA(X,Y)+dxyA(Y,A) > log,(]r]) The lemma follows directly from this because X, Y, and \nA are elements of Del(b,Tor), and da is the same as dxTA for the distances of X, Y, and A. . Assume IQ]= \n1. Consider a path from the root node of B down to an element of 7 and a node Y on this path. Let n be \nthe length of the path from the root down to Y. Note, that here and in the following we do not count \nunit productions, that is nodes that have only one child, when computing the length of a path. We prove, \nby induction on n that Basis n = 0 : Wehavea=Y andd-x,JX,Y) + dx-a,(Y, A) = 2. Induction step: Assume \nY has k 1 2 child nodes Yr,. . . ,Yk. We then have for all i with 1 <, i < k +~~(X,yl,) = dxTA(X,Y) \n+ z - 1 and dx_d~(Yi,A) =dxyA(Y,A) +k-i. Therefore + ;; A(X,Y~) + +&#38;%A) = d,6(X,Y)+d,-;;-,(Y,A)+k-lLn+2+1. \n The parse tree 19 has 171 leaves and a maximum branch- ing factor of m. Its height will be at least \nlog,(]7]). Let Y be the second to the last node in a path with maximal length. Then Y $!z 7 and dxa(X,Y) \n+ dx-a,(YA) L log,(l71) -I + 2 > log,(l71). . Assume lo] > 1. Let 2 be a symbol in u that has a parse \ntree with a minimum number of leaves. Now delete 2 from 5 to obtain 6. If we delete from 7 all the descendents \nof 2, the length of 7 is reduced but still has at least half the original length. We can therefore find \na node Y in B with dx~~(X,y) + dx=(Y,A) > log,(lM) 1 log,(]7]) -1. Reinserting 2 into b will increase \neither the distance of X to Y or the distance of Y to A by 1, giving the desired result. 0 Theorem 3 \nC C BCP(j, k) PROOF. Given any grammar with branching factor m, we now construct sequences of sets, and \nuse a superscript k E IN to identif the elements of a sequence. Let F I be the finite set of all strings \ng such that ]uJ < 2k + m and a is a sentential form or an initial or fmal segment of a sentential form, \nor lo] = 2k + m and tr is a substring of some sentential form. Now construct a table Tk, that contains \nfor each string u E Fk and each handle of u that can be identified, an entry giving the string and the \nhandle. If the grammar is BCP(k, k), (or BCP(j,i) for some j _< k and i _< k), then the table can be \nused to parse arbitrary sentences: Given a sentential form p there is a substring (Y (Icy] 5 m) of p \nthat can be identified as a handle using at most k symbols left and right. Therefore, there is a substring \nof p containing Q, which extends Q to the left and to the right by at most k symbols and is sufficient \nto identify cy as a handle. This substring can be extended further until either the end of p is reached \nor the length is 2k + m. This final substring of p, together with the handle, will be in the table Tk \nconstructed above. The table can be used to look up the handle and reduce it. Assume now that the grammar \nis not BCP, then for each k a counter example to the above construction can be found. That is, for every \nk there is a sentential form p such that no substring of p has an entry in Tk. Let p be a minimum length \nsubstring of p that has at least one identifiable handle (Y with rule A + ct. /3 exists because p, being \na complete sentential form, allows the iden- tification of all handles. Also, I/l] > 2k + m because smaller \nsubstrings do not allow the identification of handles, due to the choice of p. Without loss of generality \nwe can assume that /3 = X7cyr with 1x71 1 171 (the handle is right of the middle). We have ]p - 7~~71 \n= 1x7~~7 - ycrr] = 1 and proceed by estimating If(P) -f(7or)]. 7ar has no identifiable handle since p \nwas minimal and hence f(7cur) = 7~~7. Since f(P) contains X7Ar+Xycyr, we can use Lemma 3 to obtain If@) \n-f(74 = IfuJ) -74 > h3,(l7lY2 Using JX7cr7] > 2k + m, IcxI I m, and 1x71 2 171, we have I71 k and finally \nIf(P) -f(74 > log,(kW !%nce this is true for all k, a fmite bound cannot exist, and consequently the \ngrammar is not continuous. c1 Theorem 4 BCP(j,k) c C PROOF. Example 3, below, presents a grammar that \nis BCP but not continuous. Example 3 S+aA S-+cC A+x C-+x A+DA C+EC D+b E+b Language = {ab x, cb x} The \nproper reduction of the trailing L as well as the re-ductions for the preceding b s, depends on the first \nsymbol. Therefore the string (Y = b x has no identifiable handle, and the string /3 = ab x can be reduced \ncompletely. We have la-P] = 1 and If(o)-f@)] = df(p)(a,parent of x) = n+l. Thus, the grammar is not continuous. \n0 5 Summary Parse trees are the product of the parsing process-tree structures found on the parse stack. \nWe defined a metric on the nodes of these trees based on the minimum distance these nodes have as entries \non the parse stack using stan-dard bottom-up parsing techniques[3] and their generaliza- tions[l7, 161. \nThe size of a set of nodes was then defined by the maximum distance of its elements. As usual, the distance \nbetween two parsing situations, as captured by the complete parse stack, is measured considering the \nnodes to be deleted and then inserted to transform one situation into the other. The measure directly \ncorresponds to the compu- tational effort needed for an optimal kepair of the parse state after an error \nis found. There is only one assumption made here that is nottriv- ially true for most parsers used today: \nthe measure of dis- tance assumes that all handles, even after the point of error, are reduced, if possible. \nThis feature is available in some modern parsers[5] that can perform non-corrective syntax analysis[ \n131. Further, [16] presents a suitable parser generator for con- tinuous LR(k) grammars which produces \nparsers as fast as traditional parsers generated by lex and yacc. It proves that the function f cm be \ncomputed efficiently. Continuous grammars can be written for popular pro gramming languages as demonstrated \nin [15] where the par- ser generator described in [16] was used to implement a ro- bust pretty printer \nfor the programming language C using a continuous LR(k) grammar. Unfortunately there is, so far, no known \nalgorithm to decide the continuity of an arbitrary grammar. Practical examples and specific details on \nhow to rewrite ordinary LR(lc) grammars to make them continuous can be fond in [lS]. We de&#38;red the \nclass of continuous grammars, in a man- na quite similar to the classic e-&#38;definition of real analysis. \nAs a main result, we have BC c C c BCP. Further interest- ing results on the hierarchy of grammatical \nclasses can be found in [17]. Continuity is defined independently of a particular pars-ing algorithm. \nWhile we know that a BCP parser is always sufficient for a continuous grammar, this does not preclude \nthe use of ordinary LL or LR(L) parsers. Similar, continuity does not imply any particular mechanism \nof error recovery and repair. All it does is limiting the worst case. Given a discontinuous grammar, \nevery error repair mechanism will fail on some cases. Too long this was taken for granted and even provably \nunavoidable. Given a continuous grammar, there will be no longer any excuse for a bad error recovery \nmechanism. Usually, surprisingly little is known about the relation of parser input and parser output \nin the presence of input errors[20]. Continuity is a very important property in this respect, it provides \nprecise bounds on the effects of all single token errors. To preserve continuity, parsing decisions for \nan unbounded segment of the input must not depend on a bounded segment of the input, as for instance \nin the first and last example. The ideas and results presented here are of threefold use: 1. Since the \nclass of continuous grammars is not defined in terms of a parsing algorithm, the main application of \nthe ideas presented here are expected to be in the area of language design. Language-designers might \nbe come better aware of the kind of features that make errors in programs hard to find and correct. To \nput it somewhat simply, it is a bad idea to have the same syntax for two different entities such that \nsome context far away is necessary to find the proper interpret+ tion (see Example 1). 2. Even if the \nlanguage is already defined, the implemen- tors have some choices left. The grammar used for parsing \ncan still be changed to have a better basis for error diagnosis and recovery (for details see [16]). \nThis paper points the implementor in the direction of BCP grammars and shows why BCP grammars are a good \nbasis for parsers with improved error correction. It is interesting to note that research in HCI confirms \nthat a good human computer interface should have no hid- den state information. 3. Finally, a close \nanalysis of the definitions and proofs given here reveal interesting bounds. In particular, the proof \nof Theorem 1 gives a bound which applies for BC grammars, or for those parts of a grammar that are BC, \nlimiting the choice of nodes that must be examined and changed to find a minimum distance repair for \nsingle token errors.  The work presented here is part of the authors research in syntactic error correction. \nCommonly, parsing is seen as a front-end for code generation and as such a solved prob- lem (lex, yacc). \nWith code generation as the ultimate goal, the correctness of the input string becomes a natural as-sumption \nand syntactic errors are considered an exception. Every programmer, however, can attest to the fact that \ndur- ing program development syntactic errors are the rule rather than the exception. For this special \nsituation, a dedicated parser with improved error diagnosis and interactive error correction is desirable. \nThis is still an unsolved problem, but there is reason to hope that programming languages and grammars, \ndesigned with an eye on the problem of error correction, together with appropriate parsers, can perform \nerror corrections of far better quality than available today. References Alfred V. Aho and Thomas G. \nPeterson. A mini- PI mum distance error-correcting parser for context-free languages. SIAM Journal on \nComputing, 1(4):305-312, December 1972. Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ullman. Com- PI pilers \n-Principles, Techniques and Tools. Addison Wesley, Reading, MA, 1986. Alfred V. Aho, Ravi Sethi, and \nJeffrey D. Ullman. Com- [31 pilers -Principles, Techniques and Tools. Addison Wesley, 1986. American \nNational Standards Institute, New York, PI NY. American National Standard for Information Systems -Programming \nLanguage-C, ANSI X3.159-1989, 1990. Joseph Bates and Alon Lavie. Recognizing substrings [51 of LR(k) \nlanguages in linear time. ACM Tkanaactions on Programming Languages and Systems, 16(3):1051-1077, May \n1994. Robert C. Buck. Advanced Calculus. McGraw-Hill, New PI York, NY, 1978. Definition of Lipschitz-continuous. \nJoachim Ciesinger. A bibliography of error-handling. VI SIGPLAN Notices, 14(1):16-26, January 1979. PierPaolo \nDegano and Corrado Priami. Comparison PI of syntactic error handling in LR parsers. Software-Practice \nand Experience, 25(6):657-679, June 1995. Robert W. Floyd. Bounded context syntactic analysis. PI Communications \nof the ACM, 7(2):62-67, 1964. Susan L. Graham. Precedence Languages and Bounded PO1 Right Context Languages. \nPhD thesis, Stanford Univer- sity, 1971. Susan L. Graham, Michael A. Harrison, and Walter L. WI Ruzzo. \nAn improved context-free recognizer. ACM !lkansactions on Programming Languages and Systems, 2(3):415462, \nJuly 1980. Donald E. Knuth. On the translation of languages from P21 left to right. Information and \nControll, 8:607-639, 1965. Helmut Richter. Noncorrecting syntax error recov- P31 ery. ACM !lkansactions \non Programming Languages and Systems, 7(3):478489, July 1985. David G. Ripley and Frederick C. Druseikis. \nA statis- P4 tical analysis of syntax errors. Computer Languages, 3(4):227-240, June 1978. Martin Ruckert. \nConservative pretty printing. SIG- P51 PLAN Notices, 32(2):45-53, 1997. [16] Martin Ruckert. Generating \nefficient substring parsers for BRC grammars. Technical Report 98-105, SUNY New Paitz, New Paitz, NY, \nJuly 1998. [17] Thomas G. Szymanski. Generalized Bottom-Up Pars-ing. PhD thesis, Cornel University, Ithaca, \nNY, May 1973. [18] Thomas G. Szymanski and John H. Williams. Non-canonical extensions of bottom-up parsing \ntechniques. Technical Report 75-226, Cornel University, Ithaca, NY, January 1975. [19] Kuo-Chung Tai. \nNoncanonical SLR( 1) grammars. ACM !lkansactions on Programming Languages and Systems, 1(2):295-320, \n1979. [20] Charles Wetherell. Why automatic error correctors fail. Computer Languagea, 2:179-186, 1977. \n[21] John H. Williams. Bounded context psrsable gram-mars. Technical Report 72-127, Cornel University, \nIthaca, NY, April 1972.  \n\t\t\t", "proc_id": "292540", "abstract": "", "authors": [{"name": "Martin Ruckert", "author_profile_id": "81100319749", "affiliation": "State University of New York at New Paltz", "person_id": "PP31037049", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/292540.292568", "year": "1999", "article_id": "292568", "conference": "POPL", "title": "Continuous grammars", "url": "http://dl.acm.org/citation.cfm?id=292568"}