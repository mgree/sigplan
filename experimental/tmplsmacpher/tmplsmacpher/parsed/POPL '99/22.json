{"article_publication_date": "01-01-1999", "fulltext": "\n Constraint Systems for Useless Variable Elimination Mitchell Wand and Igor Siveroni* College of Computer \nScience Northeastern University Boston, MA 02115, USA {wand,igor}@ccs.neu.edu Abstract A useless variable \nis one whose value contributes nothing to the final outcome of a computation. Such variables are unlikely \nto occur in human-produced code, but may be in- troduced by various program transformations. We would \nlike to eliminate useless parameters from procedures and eliminate the corresponding actual parameters \nfrom their call sites. This transformation is the extension to higher-order programming of a variety \nof dead-code elimination optimizations that are important in compilers for first-order imperative languages. \nShivers has presented such a transformation. We refor- mulate the transformation and prove its correctness. \nWe believe that this correctness proof can be a model for proofs of other analysis-based transformations. \nWe proceed as fol- lows: We reformulate Shivers analysis as a set of constraints; since the constraints \nare conditional inclusions, they can be solved using standard algorithms. We prove that any solution \nto the constraints is sound: that two computations that differ only on variables marked as useless give \nthe same answer up to useless variables, and hence the same answer when the answer is a constant. We \nobserve that this notion of soundness is too weak to support the transformation of the analyzed program. \nWe add additional closure conditions to the analysis and show that any solution to the new set of con-straints \njustifies the transformation. The proof is by tree surgery: we show that from any computation of the \noriginal program, we can construct a computation of the transformed program that yields an answer re- \nlated to the original, and the same answer when the original answer is a constant. *Work supported by \nthe National Science Foundation under grants numbered CCR-9404646 and CCR-9629801. Permission to mke \ndigital or hard copies ofall or part of this work for personal or classroom use is granted withoutfee \nprovided that copies are not made or distributed for prolit or commercial advantage and that copies bear \nthis notice and the full citation on the first page. To copy otherwise, to republish, to post on sewers \nor to redistribute to lists, requires prior specific permission and!or a fee. POPL 99 San Antonio Texas \nUSA Copyright ACM 1999 I-581 13-095-3/99/01...$5.00 1 Introduction In a series of papers, we have explored \nthe question of how program transformations are justified by program analyses [Wan93, SW97, WC98]. In \nthis paper we consider the ex-ample of useless variable elimination. A useless variable is one whose \nvalue contributes nothing to the final outcome of a computation [ShiSl]. For example, consider, the following \nexample, obtained by translating [Muc97, Figure 18.181 into a functional set-ting: let fun loop(a, bogus, \nj) = if (j > 100) then a else loopff (a, j), bogus+2, j+l) in loop (a. 3. 1) end Here it is clear that \nthe second argument does not contribute to the value of the computation. Our transformation re-places \nloop by the evident two-argument procedure, getting let fun loop (a. j> = if (j > 100) then a else loop \n(f(a,j), j+l) in loop (a. 1) end Our analysis removes all the useless variables identified by Muchnick \ns first-order analysis [Muc97, Section 181. In addition, it identifies useless variables even in higher-order \nprocedures. For example, consider: let fun fl (x, y) = x fun f2 (x, y) = x+x fun f3 (x, y) = y val g \n= if p(a.b) then fi else f2 val h = if q(a,b) then fl else f3 in g(x,h) f 1 and f2 are the only functions \nthat flow to the applica- tion site g(x, h) These functions do not use their second argument and they \ndo not flow to any other calling points. Hence, the second actual parameter can be removed from the application \nsite and both functions can be replaced by their one-argument versions. We observe that h and f3 do not \ncontribute to the value of the computation. We can semantics, and a simple control-flow analysis. In \nSec- remove them from the initial program, getting tion 5 we present our reformulation of Shivers analysis \nas a constraint-generation system and prove its soundness. In let fun fi (x1 = x fun f2 (x) = x+x val \ng = if p(a,b) then fl else f2 in g(x) The decision of removing f 3 from the let expression de- pended \non the elimination of h, which in turn depended on the removal of the second actual parameter from g(x,h). \nThe second formal parameter off 1 and f 2 can be eliminated only if the second actual parameter from \ng (x, h1 is removed, and viceversa. It is essential that these transformations are done at the same time. \nWhile useless variables seem unlikely to occur in high- quality human-produced code, this transformation \nseems worthy of study for several reasons: Such variables may be introduced by various program transformations. \nFor example, arity-raising [PJS98] of- ten creates useless variables. This transformation is the extension \nto higher-order programming of a variety of dead-code elimination op-timizations that are important in \ncompilers for first- order imperative languages. Such variables may be result from program mainte-nance \nor other engineering considerations. For exam- ple, [ST981 finds that an average of 12.5% of the data \nmembers in realistic C++ benchmark programs were useless. We present an analysis and transformation \nto eliminate useless variables from programs. This analysis and transfor- mation is similar to one proposed \nby Shivers [ShiSl]. Our major contribution is a proof of correctness of the transfor- mation. The plan \nof the work is: . We reformulate Shivers analysis as a set of constraints; since the constraints are \nconditional inclusions, they can be solved using standard algorithms [PS94]. We later show that although \nthere may be O(n3) con-straints, the standard algorithm still solves them in O(n3) time. . We prove that \nany solution to the constraints is sound: that two computations that differ only on variables marked \nas useless give the same answer up to useless variables, and hence the same answer when the answer is \na constant. . We observe that this notion of soundness is too weak to support the transformation of the \nanalyzed program. We add additional closure conditions to the analysis and show that any solution to \nthe new set of constraints justifies the transformation: we show that from any computation of the original \nprogram, we can construct a computation of the transformed program that yields an answer related to the \noriginal, and the same answer when the original answer is a constant. We begin in Section 2 with a survey \nof related work. In Sections 3 and 4 we present the source language, its Section 6 we observe (as did \nShivers) that the naive anal- ysis is insufficient; we then show how to strengthen it. In Section 7 we \npresent the transformation and show that it is correct. Last, we close with comments on the complex- \nity of the analysis, possible additions to support effects and separate compilation, and conclusions. \n 2 Related Work Our treatment of useless-variable elimination for higher-order languages is based on \nShivers [ShiSl]. We extend Shiv- ers work by proving the correctness of the transformation. Our transformation \nalso eliminates useless let-bindings and replaces recursive abstractions by non-recursive ones when possible. \nShivers result is an application of control- flow analysis, which has been rediscovered several times \n[Jon81, JM82, Ses88, ShiSl]. The formulation of control- flow analysis as a constraint problem is largely \ndue to [Hei92, PS94]. Our transformation is the extension to higher-order lan-guages of optimizations \nsuch as as dead code elimination, useless variable elimination, and unreachable code elimina- tion, which \nare described in detail for first-order imperative languages by most standard compiler texts [App98, \nASU86, Muc97]. Our definition of dead code comes closest to the one given by Muchnick, who defines dead \ncode as code that is executable but that has no effect on the result of the compu- tation being performed \n[Muc97]. Since a variable is useless when its value contributes nothing to the final outcome of the computation, \nany terms whose values are bound to use- less variables are dead code. Our transformation eliminates \nall such terms. These also include terms that correspond to unreachable code in imperative programs. \nThis is not the same as live-variable analysis: A variable is dead at a program point if its contribution \nto the result of the computation has already been incorporated into some other value; a useless variable \nis one which is dead at its initial binding. Peyton Jones [PJS98] points out that whilst it is un-usual \nfor programmers to write functions with arguments that are completely unused, it is rather common for \nthem to write functions that does not use some parts of their arguments . This fact is exposed by the \nwrapper/worker splitting transformation used in the Glasgow Haskell Com- piler(GHC) [PJHH+93]. The transformation \ncreates a wrap- per function that evaluates the strict arguments of the orig- inal functions and passes \nthe unboxed values to the worker, which implements the original function. Elements of data structures \n(i.e. products) can be passed separately -if some of them are known to be strict -to the worker, exposing \ncomponents that are not used at all by the function. These useless parameters are detected by an absence \nanalyzer and removed from the code. Program slicing [Wei84, HRB84] is a method for decom- posing programs \nby analyzing their data and control flow. A slice of a program with respect to program point p and variable \nx consists of all statements and predicates of the program that might affect the value of x at this point. \nOur dependency sets are much like slices, but we take advantage of the simpler structure of expressions: \nour labels 1 are like program points, but we are concerned only with the value of the expression at that \nlabel, and we collect just the variables upon which that value depends. We use the term dependence analysis \nfor this process, but our concept seems unrelated to the notion of dependence graphs in first-order languages, \nwhich are relations on basic blocks using assignment. Abadi et al [ALL961 define a dependency system \nfor a higher-order language. However, rather than relying on analysis, they instrument the interpreter \nto keep track of which parts of the original term the result depends. They then cache these results to \navoid future recalculation. This scheme enables them to cache pairs of the form (input pat- tern, output), \nrather than just (input, output) as in tradi- tional memoizing systems [Mic68]. They also succinctly \nex-plain the difference between dependence analysis and strict- ness analysis: strictness analysis is \nconcerned with what parts of a program must be evaluated, [whereas dependence analysis is concerned with] \nwhat parts of a program may affect the result. [ALL96, p. 841 Our previous work includes a series of \npapers that de-scribe transformations justified by program analyses [SW97, Wan93, WC98]. [Wan931 proves \nthe correctness of a binding- time analysis and an off-line partial evaluator, using purely syntactic \nnotions much like those in this paper. Steckler and Wand [SW971 use a control-flow analysis with additional \nconstraints to introduce site-specific procedure-calling pro-tocols; their protocols add extra parameters \n(to avoid saving them in closures), whereas we now consider eliminating use-less parameters. [SW971 also \nuse notions of occurrence index and occurrence closures that we refine by using labels, label configurations \nand a universe C of labeled terms that link labels and terms. [WC981 uses a sequence of three analyses \nto identify dead values in a first-order language of recur-sion equations. That result relies entirely \non the soundness properties of the analyses, unlike the current paper. 3 The Source Language The syntax \nof the source language is shown in Figure 1. It is an untyped lambda calculus extended with primitive \nfunctions and explicit operators for conditionals, let, and recursive procedures. 0-ary procedures are \nconsidered by allowing n = 0 in the production for Fun. Labels are used to identify the position of a \nterm in a program; an expression is a labeled term. The semantics is a big-step call-by-value environment \nse-mantics. It deals with configurations consisting of an ex-pression label 1 and an environment p mapping \nvariables to values. Values are either constants c or closures consisting of a pair (I, p), where 1 is \nthe label of an abstraction. Note that a closure (I, p) is both a configuration and a value. C E Config \n::= (I, p) (configurations) u, 21, w E VaZ ::= c 1 (I, p) (values) p E Env ::= [] 1 p[z ++ w] (environments) \n We connect labels to terms by hypothesizing a finite uni- verse C of labeled terms, with the property \nthat no label occurs more than once in C. We write C(Z) = t if the unique occurrence of 1 in C labels \nthe term t. This enables us to connect the semantics (which deals with terms) to the analysis (which \ndeals with labels). It is useful to think of C as a store containing the nodes of the parse trees of \nsome expressions, and labels 1 as L-values pointing to those nodes. In some development environments, \nan object like C is called the storage map. However, since labeled terms are finite, we can use induction \non substruc- tures of a labeled term as an induction principle. The semantics is then given by judgments \nC t- C J,l u defined by the system of Figure 2. Except for the systematic use of labels to refer to terms \nin the universe, this is an ordinary call-by-value big-step environment semantics. 4 Control Flow Analysis \nOur analyses depend on a OCFA specified by the judgment (C,Z) /= (c,p), where c is (following [NN97]) \na map from labels to sets of labels of abstractions and b is a map from variables to sets of labels of \nabstractions. This represents the judgment that &#38; and /j represent a consistent description of label \n1 in universe C. This notation is reversed from that in [NN97]: to us an analysis is the proposition, \nthe set of labeled terms constitutes a model, and a particular label constitutes a point inside the model. \nThe semantics of these propositions is standard and is given by the definition of b in Figure 3. The \ndefinition is given by induction on the size of I in C, unlike the gfp version of [NN97]. In this figure \nwe have used vertical braces to indicate conjunctions. We extend this definition to configurations and \nenviron- ments by: CC, (4 4) I= (6,/j) iff CC, 1) P CC,4 A CC,PI k CC,b) (C,p) k (&#38; ,b) iff Vz E \ncZom(p) : (CT Pb)) I= cc7 #a 1 A Pb) = lb P ) * 1 E b(x) CC, 4 I= CC, B) always The soundness of the \ncontrol flow analysis says that if a configuration evaluates to a closure, then that closure is one that \nis predicted by the flow analysis as a possible value for the original configuration, and furthermore \n(d,b) is a consistent description of the result. The latter property is particularly useful to prove \nthe application case in some of our theorems. Lemma 4.1 (Soundness of Control Flow Analysis) If (E, (Z, \nP)) k (c,b) and E k (Z, P) U (Z , P ), then 1 E c(Z) and (C, (I , p )) k (c, 8). Proof: By induction \non the derivation of C k (I, p) Jj (Z , P ). 5 Dependency Analysis and Useless Variables We next proceed \nto the dependency analysis. The depen-dency analysis computes for each label 1 a superset of the set \nof variables that contribute to the value of C(Z). A variable is deemed useless for label 1 if it does \nnot appear in D(Z). A dependency analysis is a function V D : Lab -+ P( Var) e E Exp ::= t (expressions) \ntE Term (terms) t ::= c [const] [:y I 7 1 (gel...e,) 1 (eo el . . . en) KS 1 (if e0 then er else es) \n[yf] 1 (let x = er in es) WI fEl%n (function terms) f ::= (fnzr...xCn*eO) 1 (furry XI . ..3h * eo) g \nE Prim (primitives) c, d E Const (constants) z,y,z E VW (variables) f$ : Prim + [Const -+ Const] Figure \n1: Syntax of the Source Language - [const] if C(Z) = c c I- (Z, P) u c if C(2) = z A x E dam(p) A p(z) \n= v Iv4 c k (Z, P) u v C k (1, p) Jj (1, p) if C(1) E Fun If4 C(z)=(g 0t: ...t~)ACI-(zi,p)Uci i=l ,. \n, .,* [prim1  ~~(z,P)u[~(g)l(cl...c~) C(Z) = (tf; t: . t:, A C k (lo, P) U (If, PO) A Cb(Zi, p)J,lvi \ni=l,...,n A C(Zf) = (fn x1 . xn + tfp) A c ,- (Zb, pO[xl * vl] . . . [%I ++ Vn]) u 'u [wpfnl c I- (6 \nP) uv  C(Z)=(t$t: ...tk)A C i-- (Zo, P) U Or, PO) A C I- (Zi, p) JJ vi i = 1,. . . , n A C(Zf) = (fun \ny 21.. . xv, =s t?) A c ,- (Zb, ,oO[y I+ (Zf, pO)][zl -vl]. . bZ ++ %]) u v kwfunl c k o, P) u v C(Z)=(ift~thent~elset \n,2)ACk-(Z0,p)~trueAC~(Zr,p)~~ [ifi] c k (Z, P) u v C(Z) = (if t; then t? else t$) A C t- (lo, p) J,l \nfalse A C I- (12, p) U v [ifa] c I- (Z, P) u v C(Z) = (let x = ty in t?) A C t- (11, p) 4 VI A C t- \n(Zz, p[x H VI]) JJ v WI c t- (Z, P) u v Figure 2: Operational Semantics of the Source Language =3 always \n* 2 E V(Z) (C, Zo) I= (6 8, V) C(Z) = (fn 21 . . zn * t$) =S V(Z0) -{Xl,. . , xn) c qq 1 C(Z)=(funyx1...x,=+t$) \n(C,Zi) k (CT 67 V) C(Z) = (910 t:l . . t:, * i=l,...,n: D(li) &#38; D(Z) 1 C(Z) = (t$ tp . tk) (C, Zi) \n+ (C, j?, D) i = 0,.  Wo) 5 W) VZ : (X(1 ) = (fn 21.. . Zn * t?) V X(1 ) = (fun 2 21.. xn +-tfP)) =+ \n ! \\ 1' E C(ZO) A Xi E V(Zb) * V(Zi) 5 V(Z) i = 1,. . ,?I D(l0) u D(h) u D(Z2) c D(Z) Rh) t= (bm C(Z) \n= (let 2 = tp in t$) X E qa2) * Io(ll> 5 V(Z) Figure 4: Dependency Analysis C(Z) = <t$ t:l . . tk) =s-(C,Zi) \nk(C,@,V) i=O,...,n Wo) G w VZ : (C(Z ) = (fn x1 . ..zn *t?)v C(Z )= (funx x1 ...xn *tb lb ))* Z E~(Zo)Ax;EV(Z~)*V(Zi)~V(Z) \ni=l,...,n VI : (C(Z ) = (fn yl . . . yn =3 tk) V C(Z ) = (fun y yl . . . yn * t$)) * z , I E &#38;&#38;I) \nA zi E V(Zb) + yi E v(Z,) i = 1,. , n A I @,I) i= @ ,i%v)-@,I) l= (b) A C(Z) = (fn xl.. . xn * tfp), \nV(Z) C FV(C(Z)) U (~1,. . . , xn} C(Z) = (fun y x1.. .2n%-t~),v(z)~FV(C(z))U{x1,...,xn} otherwise , V(Z) \nC FL @(Z)) Figure 5: Extension to the Dependency Analysis 295 [const] C(Z) = c =+- true (C, 1 ) I= (Ca \nFrill C(Z) = (fn ZI . . . xn * t ) * z E etzl {   c%0 k cc,8 [fun1 C(Z) = (fun y x1 . . .xn =s 8) \n* z g C(Z) i zE B(Y) [prim] C(1) = (g10ty . . tk) =+ (C, Zi) + (C,b) i = 1,. . , n (C,Zi) b((c,js) i=o \n,...) 71 (C(Z ) = (fn z1 . . xn * ty )V bwl C(z)=(tfPt: ...t~) vz,: C(z )=(funyxl...x,*t: ))~ I E &#38;I,) \n+- C(Zi) c 6(x;) i = 1,. , Tl I i 1 E &#38;(Zo) * C(Zf) c C(Z)) { (C,Zi) k (C,b) i = 1,2,3 VI C(Z)= \n(if t$ thent? else tF)+ C(Z1) c C(Z) C (h) c_ C(Z) @,Zl) I= (m (FW I= (Ci9 C(Z)= (let a: = t? in tt)=+[let1 \n m) c_ 8(x) i C(Z2) E W) Figure 3: Control Flow Analysis such that if C(Z) = t then D(Z) C R% (t). The \nseman-the constraints are all monotonic in V, so we can solve the tics of a dependency analysis is specified \nas a judgment constraints using the standard closure algorithms [PS94]. (C, Z) k (&#38;, b, IO). This \nrepresents the judgment that the In order to state a correctness property for this analysis, propositions \n6, b, and V are true at label 1 of universe C. we extend the definition to environments, configurations, \nThe semantics of these propositions is given by the rules of and values as follows: Figure 4. Like the \nrules of Figure 3, these rules generate a finite set of constraints for any label Z of universe C. The \nrules of Figure 4 are fairly intuitive: For (C, 1) b (6, b, V) to hold, (E, 1) k (e,b) must hold, as \ndefined in Figure 3. In addition: . A variable always depends on itself. . An abstraction depends on \nall the variables on which It is then easy to prove that any dependency analysisits body depends, less \nits bound variables. satisfying these constraints is preserved under evaluation: . An application of \na primitive depends on all the vari- ables that any of its arguments depends on. Lemma 5.1 If (C, (1, \np)) /= (C,;, D) and C b (1, p) # . For a procedure application, we consider all of the clo- (I , p ), \nthen I E C(Z) and (C, (Z , p )) t= (C, b, D). sures that can flow to that site. For each i, if the body \nof any such closure depends on its i-th argument, then In order to express why variables not in D(Z) \nare useless the entire expression depends on all the variables on for 1, we define an equivalence relation \non configurations that which the i-th actual parameter depends. says that they are equal except perhaps \nfor useless variables: Notice that this definition is inductive on C(Z), so for any 1 it generates a \nfinite set of constraints. Furthermore, . In each let term: The let term is replaced by its body if \nthe let-variable is not part of the dependency set of the body term. To determine which actual parameters \nshould be re-tained at an application site, we define the set where C(Z) = (t$ t; t$) A I E C?(Zo) where \nin the new analysis, the right-hand side is independent of the choice of 1 . If there is no such Z , \nwe define the set to be empty, since c(Zo) = 0 implies that t$ never returns a closure. Given .4 = ((?,I, \nC, D), the transformation &#38;(-) takes a label and produces a labeled term as described by the rules \nin figure 6,where in the last line for let, the notation 7~(Zz) means an expression the same as 7d(Zz) \nexcept that it is labeled with 1. As usual, in order to express the correctness of the trans- formation, \nwe extend it to operate on configurations, values, and universes: $13 P)) = (Z, ifi( 12 E D(z))) 1 iTA \n) 1 E dam(C)} fi@) The decision about removing a formal parameter from an abstraction is made separately \nfrom decisions about remov-ing occurrences of terms containing that variable. Hence there is a danger \nthat some variable might be removed from a formal parameter list while some occurrence of that vari- \nable still remained in the transformed body, resulting in an unbound variable. It would be easy to add \nmore constraints to the analysis to prevent this, but luckily that is not neces- sary: we show that the \ntransformation creates no new free variables. Lemma 7.1 If (C, 1) /= (c,b, 2)) then Fv(G(Z)) c D(Z) MUi \nFV(7;2(Z)) c Fv(x(Z)). Proof: By induction on the size of 1 in C. We prove first that FV(Td(Z)) c D(Z) \nby showing the abstraction and ap- plication cases: C(Z) = (fn 21 . .xn * t;) Pnl The transformed term \nis Ta(Z) = (fn 2;i . . Xi,,, * fi(Zb)) , ij E DFoTwwJZS(~,~)(Z) The constraints for (C, 1) k (6, jj, \nD) are: (&#38; Zb) k $3 6, D) A D(lb) -{xl,...,&#38;} c 2)(z) Hence we can apply the inductive hypothesis \nto Zb and derive FV(G(lb)) c D(lb) By applying the definition of free variables we get FV(fi(Z)) = Fv(G(Zb)) \n-{Xi1 , , xim} ~~(lb)-(xil,...,xi,} If zi $! DFo~~uZS(~,~)(Z) then xi $22)(zb). This means that D(Zb) \n-{Zii, , xi,} = D(lb) -{xl,. , xn) We finally have FV(fi(Z)) c D(lb) -{xl,. . . ,x,1 c D(z) [app] C(Z) \n= (ttp tp . tk) The transformed term is: G(Z) = (%&#38;O) %&#38;) fi(k,)) ij E DActuaZs(e,p,x,(Z) Let \nI E a and X(1 ) = (fn $1 xn * t?). This means that DActuaZsC~,p,Cj(Z) = DFormaZs(~,c,(Z ) A xij E D(lb) \nBy applying the definition of the dependency analysis we can derive D(Zo) C D(Z) o(Zij) C D(Z) ij E DActuaZs(e,p,c,(Z) \n Hence we can apply the inductive hypothesis and derive FV(%(Zo)) 5 D(Zo) FV( L(Zij)) E D(Zij) i, E DActuaZsce,p,Cj(Z) \nBy applying the definition of free variables we get Fv(fi(z)) = Fv(7$o)) U FV( %&#38;)) u.. u Fv(fi(kn \n1) c V(Z0) u V(Zi1) u . . . u qzi,) c V(Z) u V(Z) u . u D(Z) = V(Z) The second part of the lemma is \ntrivial except for the abstraction case: C(Z) = (fn xi.. .2, * t:) WI The transformed term is: %4(Z) \n= (fn Xi1 . . . xi,,, * G(lb)) , ij E DFormaZsp,~)(Z) Using the definition of DFormaZs(Dyc) and the first \npart of the lemma we have ij E DFormaZs(p,c,(Z) H xij E 2)(zb) fl {XI,. ,xn} FV(c(k)) c D(lb) If xi $ \nDFormaZs(~S~,(Z) then Xi # v(Zb) and X; $! Fk (Td(lb)). Then we can derive FV(%(lb)) -{xi,, ) xi,,,} \n= FV(??t(lb)) -{xl,. . , Zn} By applying the definition of free variables we get FV(fi (I)) = FV(Td(Zb)) \n-{Xily.. . ) Xi,,,} ij E DFormaZsp,c)(Z) = FV(%(lb)) -{xl,. . , Gz} c FV(C(b)) -{a,. . . , X:n} Ind. \nHypothesis = FV(C(Z))  C(1) = c % G(6) = CL C(1) = x * Td(6) = x1 C(Z)=(fnxr...xn*tfP) j T-(Z)= (fnxi, \n. ..xi. ij E DFOTYdS(D,~)(z) *%4(b))', C(Z)= (fun y Zl...Xn +- tfp) =+ 7a(Z) = Xi,,,* %4(b))' ~~~nrkfx~~.. \n. xi,,, + fi(lb)) if y $Y! D(Za) otherwise C(Z) = ($0 tv . . tk, * am = (g10 &#38;(61) . . fi(zd)l \nC(z)=(@t: ...tt:) * Td(l) = (7d(6,,) %(lil) . fi(zd)L, ij E DActuazs(d.,p,C)(z) 7--(Z) = (if Ta(Zo) \nthen 7A(Zl) else %(/e))lC(Z) = (if t$ then t: else tt) + C(Z) = (let x = t? in t ,z) =s (let 2 = fi(Zr) \nin %4(Ze)) c(l) = %4(Z2) Figure 6: The Transformation In the transformed program, the corresponding \nevalua-  [QED1 tion tree has the subtrees corresponding to useless pa- Now we can state our main theorem: \nthe correctness of rameters removed, the remaining subtrees recursively the transformation. The theorem \nsays that given an evalu- transformed, and the environments of the various clo-ation of some configuration \nto some value, we can construct sures trimmed accordingly. an evaluation of the transformed configuration \nto the trans- To make this precise, let C = (I, p) and C = (I, p ) formed value. where 2) k p gr p . \nBy applying the definition of the dependency analysis and lemma 6.1 we can derive Theorem 7.1 (Correctness \nof the Transformation) If(C,C)~(~,8,2)),C~Cuvand~DCCCC ,then3v s.t. Td(c) b C u v and 2) b n 2 Y . Proof: \nBy tree surgery: given a derivation of C l- C 4 v, i=l,...,n (X7 Vi) b (C, li, D) we recursively construct \na derivation of Td(c) k C u v . Vi = (Zq, pi) * 1,; E C(Zi) i { step semantics, is that if (C, C) + \n(6, @, 2)) holds for the The key step, corresponding to subject reduction in a small- Since Zf E C(ls) \nwe know that conclusion C I- C JJ v of some rule, then (C, C ) k (6, b, 2)) holds for any configuration \nC that appears on the left-hand D.4ct74aZs(~,p,C)(Z) = DFormaZs(n,n)(Zf) side of any antecedent of the \nrule.2 so the transformed terms are We show the application case: c(Z) = (%4(lO) G(li,). . . fi(k,)) \nC(1) = (tt tp . ..tk) A fi(lf) = (fn xi1 . . . zi,,, =i fi(lb)) ij E DActuaZs(~,,,,)(Z) = DFomaZsp,~)(Z~) \nc I-00, P) uOf, PO) A cl-(Zi, p)q-vi i=l,...,nA The definition of the analysis says that D(Zo) C D(Z) \nand  C(Zr) = (fn xi . . . xn * tfp) A D(Zi) s D(Z), if xi E D(Zb). Now, SiXICe 2) k p %l p , if c t- \n(lb, pO[zl I-) Vl]...[xn i+ %a]) u 21 D(Zi) C D(Z), then 2) k p %i; p . Hence [awfn 1 c I- (6 P) uv \n This is of course an old idea, going back at least as far as [MW77]. Therefore we can apply the inductive \nhypothesis to (10, p) As usual, since 7d(c) = c, we deduce that if the origi- and to (Zi, p) when zi \nE V(Zb), getting nal program evaluates to a constant, then the transformed Let We know V + po 5, Pb V \n/= Vij 2 Vij ij E DFormaZs(o,n)(Zf) i E DFormaZs(p,n)(Zf) ++ zi E D(lb) f-l (21,. . ,2,} D(lb) -{Xl,. \nI Zn} g D(lf) We need to prove V /= pr ?I, pi. Consider z E V(Zb): . ifz$!{zr,...,z,} Then PI(Z) = PO(X), \np;(X) = p ,(X), and X E Nf), which means 2, k PO(Z) g p;(z). Therefore 2, k Pi(Z) =dk). . if2E{zr,...,zn} \nLet 2 = xi, then i E DFormaZs(o,n)(Zf) and Pi(X) = vi pi(z) = v; V k Vi G V: so v /= Pi(X) 2 Pi(X). \nTherefore 2, k pr %lb p l, as claimed, and V k (lb, PI) E (lb, pi). We know that (C, (Zf, PO)) k (&#38;,8, \nV) and (C, vi) + (6, b, V). Prom the control-flow analysis, we also know that vi = (Zvi, pi), Z,; E c(Z,), \nand c(Zi) 5 ,i?(zi). Prom these we can conclude that (c, (Zb, PI)) 1 (6, b, V). Then we can apply the \ninductive hypothesis at (Zb, pr) to derive %$) t-(lb, d) 4 7~ V/=vVv We have %4@)(Z) = (fi(lO) ??A&#38;,). \n%4(h,)) %4(c) k (kj, p ) 4 Vij -j%(c) k (lo, P ) u (zf, Pb) A %4(c)(lf) = (fn Xi1 .Xi, =b fi(zb)) A c(c) \nt-(lb, I :) u 21 hence we can apply rule [appf,,] and derive fi@) k (1, p ) U v , where 2, k w g v . \n[QED1 If (E, c) k (d',$, then  Lemma 7.2 V) V /= C E ra(C) Corollary 7.1  If(C,C) /Z (&#38;,b,V) ad \nC !- C U v then 3 s.t. 7_(C) I- program evaluates to the same constant: Corollary 7.2 If (C, C) b (&#38;,@, \nV) and C I- C U c, then fi@) t- L(C) U C. Furthermore, the flow analysis still holds for the trans- formed \nprogram. This is the counterpart in the untyped analysis world to the idea that transformations preserve \ntype: Theorem 7.2 (The Transformation preserves the analysis) If @, C) k (d ,b, V,) then (%(c), G(C)) \nk (fi,i4 V). This is a desirable property in a compilation by trans- formation approach, where a series \nof transformations are applied one after the other. Control flow information can be re-used without affecting \nthe correctness of the transforma- tion though each transformation may be working with less precise values. \n 8 Complexity The analyses of Figures 3 and 4 are all straightforward con-straint systems, generating \nconstraints of the forms x1 E x1 A.. A 2, E x, =+ x 2 Y constraining the O(n) sets 6 (Z), b(x), and V(Z). \nThere are only O(n2) of these, since the number of argument con-straints such as 1 E &#38;(Zo) * C(Zi) \n2 c(Xi) i = 1,. . ,TL is bounded by the number of actual parameters 1; in the program. Hence these can \nbe solved by the standard method [PS94] in O(n3) time. The only potential difficulty is the 0(n3) constraints \nof the form 1 11 E I A Xi E V(Zb) 3 yt E V(Z,) But each of these has a consequent of the form y E Y, \nwhich costs only constant time, rather than a consequent of the form X C Y, which would take O(n) time. \nSo the total cost of the analysis is still 0(n3). 9 Adding Effects and Separate Compilation This transformation \ndoes not preserve effects such as non- termination. Consider let fun loop(a, bogus, j) = if (j > 100) \nthen a else loop(f (a, j), infloop(bogus) , j+l) fun infloop x = infloop(x+l) in loop (a, 3, 1) end This \nprogram will not terminate, of course, but the trans- formed version will eliminate the second argument, \nand hence it will terminate. We can extend the analysis to preserve such effects, by using a separate \nanalysis to detect such expressions. The dependency analysis can use the information provided by the \neffect analysis in this way: If the i-th actual parameter could not be proved to be free of the effect \nbeing preserved, then mark that position as useful and for each function that flows to the application, \ninclude its i-th formal parameter in the dependency set of its body. The new constraint will look like: \nVZ : (C(t) = (fn 21 . ..xn =ktf) V X(1 ) = (fun y 21.. xn =3 tfP)) * 1 E I A -$Eflect-Free(k)) * xi E \nZ)(Zb) In the case that external functions need to be considered i.e. references to a function in another \nmodule, we can ex-tend the set of abstract values of the flow analysis to include a value Eztemal. We \ndeclare all the formal parameters of an external function as useful by adding the following con-straint \nto the application case: C(Z) = (t$ ty tk) A External E I *D(Zi)CD(l) i=l,...,n and by extending the \ndefinition of DActuals(e,n,n) to: C(1) = (t$ t? t>) A External E &#38;(lo) * DActuals(c,p,C,(l) = (1,. \n,n} Similarly, if a function escapes the program, we must as- sume that all its formal parameters are \nuseful. Furthermore, its formal parameters could be bound to any function after it has escaped the program, \nand therefore we must declare them as External as well: C(Z) = (fn x1.. xn j i$) A Escapes(Z) {Xl,... \n, %a) c D(lb) * External E l;(xi) i = 1,. , n 1 The rest of the analysis proceeds as before. 10 Conclusions \n We have completed a proof of correctness of useless variable elimination. This transformation is the \nextension to higher- order programming of a variety of dead-code elimination optimizations that are important \nin compilers for first-order imperative languages. Our proof highlights the difference between proving \nthe soundness of an analysis and proving the correctness of a transformation. We were able to formulate \nand prove a very plausible soundness theorem for our analysis, but it turned out that this soundness \nproperty was essentially useless for the transformation: indeed, we had to strengthen the anal- ysis \nitself in order to obtain a transformation. The choice of semantics was crucial: the proof would have \nbeen much more difficult in a small-step framework, because it would have been quite difficult to formalize \njust when a particular reduction step in the original computation was relevant to the reduction of the \ntransformed program. Because the big-step semantics is organized top-down, we can choose just which subtrees \nto explore. The usual advan- tage of small-step semantics in dealing with non-terminating computations \n[NN97] was not relevant because our transfor- mation does not preserve non-termination. The proof also \nintroduces a few technical tricks that may be useful to others who wish to prove the correctness of analysis-based \ntransformations. In particular, the use of a universe C of labeled terms provides a clean way of managing \nthe link between labels and terms, and of re-using labels in different universes. We hope to extend this \nwork to consider transformations that duplicate or otherwise move code, and also to consider how our \nproofs change as we consider finer analyses, such as replacing OCFA by L-CFA.  References [ALL961 Martin \nAbadi, Butler Lampson, and Jean-Jacques Levy. Analysis and caching of depen- dencies. In Proceedings \nof the A CM 96 Interna- tional Conference on Functional Programming, pages 83-91, 1996. Pm-W Andrew W. \nAppel. Modern mentation in ML. Cambridge Cambridge, UK, 1998. Compiler Imple-University Press, [ASUSS] \nAlfred V. Aho, Ravi Sethi, and Jeffrey D. Ull- man. Compilers: Principles, Techniques, and Tools. Addison-Wesley, \nReading, MA, 1986. [Hei92] Nevin Heintze. Set-Based Program Analysis. PhD thesis, Carnegie-Mellon University, \nPitts-burgh, PA, 1992. [HRB84] Susan Horwitz, Thomas Reps, and David Bink- ley. Program slicing. IEEE \nZ&#38;ansactions on Software Engineering, 10(4):352-357, August 1984. [JM82] Neil D. Jones and Steven \nS. Muchnick. A flexi- ble approach to interprocedural data flow anal- ysis and progrms with recursive \ndata structures. In Conf. Rec. 9th ACM Symposium. on Princi-ples of Programming Languages, pages 66674, \n1982. [Jon811 Neil D. Jones. Flow analysis of lambda ex-pressions. In International Colloquium on Au-tomata, \nLanguages, and Programming, 1981. [Mic68] Donald Michie. Memo functions and machine learning. Nature, \n218:19-22, 1968. [Muc97] Steven S. Muchnick. and Implementation. lishers, 1997. Advanced Comiler Design \nMorgan Kaufmann Pub- [MW77] James H. Morris, Jr. and Ben Wegbreit. Sub-goal induction. Communications \nof the ACM, 20:209-222, 1977.  [NN97] [PJHH+93] [PJS98] [PS94] [Ses88] [ShiSl] [ST981 [SW971 [Wan931 \n[WC981 [Wei84] Flemming Nielson and Hanne Riis Nielson. In-finitary control flow analysis: a collecting \nse-mantics for closure analysis. In Proceedings 24th Annual ACM Symposium on Principles of Programming \nLanguages, pages 332-345. ACM, January 1997. Simon Peyton Jones, Cordelia V. Hall, Kevin Hammond, Will \nPartain, and Philip Wadler. The glasgow haskell compiler: a technical overview. In Proc. UK Joint Framework \nfor Information Technology (JFIT) Technical Con- ference, July 93. S Peyton Jones and A Santos. A transformation-based \noptimiser for haskell. Sci- ence of Computer Programming, 32(I-3):3-47, September 1998. Jens Palsberg \nand Michael I. Schwartzbach. Object-Oriented Type Systems. Wiley Profes-sional Computing, Chichester, \n1994. Peter Sestoft. Replacing function parameters by global variables. Master s thesis, DIKU, Uni- versity \nof Copenhagen, October 1988. Olin Shivers. Control-Flow Analysis of Higher- Order Languages. PhD thesis, \nCarnegie-Mellon University, May 1991. Peter F. Sweeney and Frank Tip. A study of dead data members in \nC++ applications. In Proc. ACM SIGPLAN 98 Conference on Pro-gramming Language Design and Implementa-tion, \npages 324-332. ACM, June 1998. Paul A. Steckler and Mitchell Wand. Lightweight closure conversion. ACM \n!i?ansac-tions on Programming Languages and Systems, pages 48-86, January 1997. Original version appeared \nin Proceedings 21st Annual ACM Symposium on Principles of Programming Languages, 1994. Mitchell Wand. \nSpecifying the correctness of binding-time analysis. Journal of Functional Programming, 3(3):365-387, \nJuly 1993. prelim- inary version appeared in Conf. Rec. 20th ACM Symp. on Principles of Prog. Lang. (1993), \n137- 143. Mitchell Wand and William D. Clinger. Set constraints for destructive array update opti-mization. \nIn Proc. IEEE Conf. on Computer Languages 98, pages 184-193. IEEE, April 1998. Mark Weiser. Program slicing. \nIEEE !Pransac-tions on Software Engineering, 10(4):352-357, August 1984.   \n\t\t\t", "proc_id": "292540", "abstract": "", "authors": [{"name": "Mitchell Wand", "author_profile_id": "81100072594", "affiliation": "College of Computer Science, Northeastern University, Boston, MA", "person_id": "PP39025873", "email_address": "", "orcid_id": ""}, {"name": "Igor Siveroni", "author_profile_id": "81351593913", "affiliation": "College of Computer Science, Northeastern University, Boston, MA", "person_id": "P115972", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/292540.292567", "year": "1999", "article_id": "292567", "conference": "POPL", "title": "Constraint systems for useless variable elimination", "url": "http://dl.acm.org/citation.cfm?id=292567"}