{"article_publication_date": "01-01-1999", "fulltext": "\n Quasi-Linear Types Naoki Kobayashi Department of Information Science, University of Tokyo email:koba@is.s.u-tokyo.ac.jp \nAbstract Linear types (types of values that can be used just once) have been drawing a great deal of \nattention because they are useful for memory management, in-place update of data structures, etc.: an \nobvious advantage is that a value of a linear type can be immediately deallocated after being used. However, \nthe linear types have not been applied so widely in practice, probably because linear values (values \nof linear types) in the traditional sense do not so often appear in ac- tual programs. In order to increase \nthe applicability of linear types, we relax the condition of linearity by extending the types with information \non an evaluation order and simple dataflow information. The extended type system, called a quasi-linear \ntype system, is formalized and its correctness is proved. We have implemented a prototype type infer-ence \nsystem for the core-ML that can automatically find out which value is linear in the relaxed sense. Promising \nre-sults were obtained from preliminary experiments with the prototype system. 1 Introduction 1.1 Linear \ntypes A number of type systems based on Girard s linear logic [6] have been proposed [l, 3,9,13,14,22]. \nThey guarantee that certain data structures (called linear values) are accessed just once (or at most \nonce). The distinction between lin-ear values and other values brings us several benefits: im-provement \nof memory management [11, safe inlining [22], etc. Among them, we are here interested in the improvement \nof memory management. If some data structure is statically known to be linear, it can be deallocated \nimmediately af-ter it is accessed. Moreover, if another heap value needs to be created after a linear \nvalue is accessed, we can just replace the linear value with the new value, instead of deal- locating \nthe linear value and allocating space for the new data (provided that the physical size of the new value \nis not greater than that of the linear value). For instance (through- out this paper, we assume a call-by-value \nlanguage), when the function X(z, y).(z, y + 1) is called, if the argument is a Permission to make digital \nor hard copies ofall or part of thiswork for personal or classroon~ use is granted without f ee provided \nthat copies are not made or distributed for prolit or commercial advantage and that topics bear this \nnotice and the full citation on the first page. To copy otherwise. to republish, to post on servers or \nto redistribute to lists, requires prior specific permission and/or a fee. POPL 99 San Antonio Texas \nUSA Copyright ACM 1999 l-581 13-095-3/99/01...$5.00 linear pair, the second element y of the pair can \nbe destruc- tively updated to y + 1. Similarly, the append of linear lists can be performed destructively, \nwithout copying cons cells. This kind of improvement will be especially effective for functional programs, \nbecause many intermediate data structures like cons cells and closures are created in naive functional \nprograms.  1.2 Limitation of linear types In spite of the above-mentioned benefit from linear type sys- \ntems, they do not seem to have been used so widely, even after type inference algorithms [8,9,22] were \nproposed that can automatically find which values are linear. We think one of the major reasons for this \nis that linear type systems are too naive for the above applica-tion: what is actually important for \nautomatic dealloca-tion and in-place update is to identify which is the last access to a heap value; \nthe condition of linearity is too strong for this purpose. For example, consider an expres-sion M E let \n2 = 2.0 in let y = x + 1.0 in y + 2: it is easy to see that y + x is the last access to z, but linear \ntype sys- tems cannot insert a code to deallocate z -just because z is used twice! This limitation also \nforces a programmer to follow a particular programming style. For instance, in Xz.(fst(z),snd(z) + l), \nthe pair z is judged to be accessed twice; so, if one wants the pair to be deallocated, one must write \nX(z, y).(x, y + 1) instead. 1.3 Our proposal -almost linear types (quasi-linear types) In order to remove \nthe above-mentioned limitation of linear types, we relax the condition of linearity. A key idea is to \nexpress information on the evaluation order and dataflow by using types. Recall the above ex-pression \nM: if a type system can take it into account that z + 1.0 is evaluated before y + z, it can ignore the \nuse of z in z + 1.0 and treat the above expression in the same way as let 2 = 2.0 in let y = 1.0 in y \n+ 5. In or-der to obtain such information on the evaluation order, the type system should also be able \nto deal with some dataflow information. Consider, for example, an expression let y = f(x) in g(x) + h(y). \nf(z) is evaluated before g(z) but it does not imply that the access to x in g(z) is the last one: x may \nbe returned by f(z) and accessed again in h(y). In order to deal with this, we distinguish between the \ntype of a value that must be used up in an expression and the type of a value that may be returned as \na part of the evaluation result of an expression. It would be possible to obtain the above kinds of infor- \nmation by using more sophisticated analyses, but we instead obtain it by a very simple method -by introducing \na new use [9,22]. Advantages of that approach include: (1) the whole analysis can be formalized uniformly \nin terms of a type system (this is especially advantageous when we in-troduce polymorphism) and (2) a \nprogram containing free variables can be analyzed as long as their type information is provided, which \nis useful for modular (or incremental) anal-ysis and separate compilation of programs. We f&#38;t review \nthe ideas of previous linear type systems [9,22] and then overview the ideas of our system below. 1.3.1 \nReview of linear type systems A use was introduced to control how often values can be ac- cessed [9,22]: \nit was either 0 (not used at all), 1 (accessed at most once), or w (accessed an arbitrary number of times) \nin [9] and was either 1 or w in [22]. Types are annotated with such uses. For example, real represents \nthe type of real numbers that can be accessed at most once, and int-b int is the type of functions on \nintegers that can be called an arbitrary number of times. A type judgment can be ac-cordingly refined \nso that it gives more useful information than the ordinary one: for example, x : real l- N : real means \nnot only that 2 is used as a real number in N but also that it is used at most once. So, it is invalid \nif N is let y = 2 + 1.0 in x + y. The following type derivation highlights a key point of the linear \ntype system: z : real1 I- 3: + 1.0 : real 2:reaZ ,y:reallt-z+y:real z : real + 2 : real1 (= z : real \n) I-let y = z + 1.0 in z + y : real The premises of the derivation imply that z is used once each in \nx + 1.0 and x + y. In order to know how x is to- tally used in the expression, we combine the type environ- \nments of subexpressions by adding the corresponding uses as above. This is in contrast to the following \nrule of ordinary type systems, where type environments are shared between subexpressions: rl--r :7-i \nr, y : 71 i- iI&#38; : 72 r t-let y = Ml in M2 : 73 By using the linear type system, we can infer how \noften each heap value is accessed, and annotate each allocation with a use. For instance, we can annotate \nthe expression. M as let xy = 2.0 in let y1 = x + 1.0 in z + y, so that y is deallocated after it is \naccessed in x + y while x is not deallocated. 1.3.2 Overview of our type system In order to express \nsimple dataflow information, we intro- duce another use 6. Intuitively, a value with use S (which we \ncall a S-value below) may be accessed many times but only locally -it cannot be returned as (a part of) \nthe com- puted result. So, x can have type real6 in z + 1.0 and (Xy.(y + l.O))z, but not in (2,l.O) or \nXy.s + y. A value with use 1 (hereafter, we call it a quasi-linear value or sim- ply a linear value, \nand call a use-once value in the traditional sense a strictly linear value) can now be accessed in a \nmore relaxed manner: It can be accessed many times as a b-value, and then it can be accessed as a strictly \nlinear value (i.e., as a value that can be accessed once and deallocated). In order to guarantee such \nusage of a value, it is crucial that the type system can take the evaluation order into ac-count. The \ntype derivation shown above is now changed as follows: x : real6 t x + 1.0 : real x:reaP,y:reaPl-z+y:reall \n2:rea16;z:reaZ1(=x:reaZ1) b let y = x + 1.0 in x + y : real1 Here, the new combination (x : reals); \n(z : real ) of type en-vironments captures the fact that z is first accessed as a J-value in z + 1.0, \nand then it is accessed as a linear value in x + y. Thus, 2 can be deallocated after x + y is evaluated. \nBecause a quasi-linear value may be used more than once, in addition to allocation of a heap value, each \naccess to a heap value is also annotated with a use: it is for distin-guishing between the last access \nof the value and the other accesses. For example, the expression M is annotated as let x1 = 2.0 in let \ny1 = x6 + 1.0 in y1 + z1 (actually, we will annotate constructors and destructors of heap values instead \nof variables). Here, we annotate x in y + x with 1 in order to indicate that it is the last access, while \nx in x + 1.0 is annotated with 6 in order to indicate that it is not the last one.  1.4 Main results \nMain contributions of the present work are formalization of the new type system sketched above and a \nproof of its correctness. Since the evaluation order must be taken into account, the proof is non-trivial \nand more involved than that of previous type systems. Another contribution is implementation of a type \ninfer- ence system for the core-ML (i.e., Standard ML without modules [15]) based on the new type system, \nwhich in- puts an unannotated core-ML expression and outputs a use- annotated expression. We have so \nfar tested several fairly small programs and obtained promising results: for pro- grams that use lists \nfrequently (such as sorting programs, the sieve of Eratosthenes and Conway s game of life), the type \ninference system could judge that most heap values (in the case of the sorting programs and the sieve \nof Eratos- thenes, all values except for top-level functions) are linear, which indicates that they can \nbe executed almost without garbage collection. Moreover, the annotated programs out- put by the system \nindicate that most linear cons cells in those programs can be updated in-place (the application of linear \ntypes to the in-place quick sort has been suggested by Baker [2] but a remarkable point here is that \nit can be performed for naive programs with no programmer s anno- tation). 1.5 Structure of the paper \n The rest of this paper is structured as follows. Section 2 in- troduces the syntax of the target language. \nSection 3 gives our type system for judging whether an expression is cor- rectly annotated with uses. \nSection 4 sketches a proof of the correctness of the type system and Section 5 briefly dis- cusses type \ninference issues. Section 6 discusses several ex- tensions of the target language. Section 7 explains \nour pro- totype type inference system and the results of preliminary experiments. Section 8 discusses \nrelated work and Section 9 concludes this paper. For space restriction, we omit some formal definitions, \nproofs, etc.: they are found in the full version of this paper [12]. 2 Target Language 2.1 Uses As explained \nin Section 1, a use is introduced to control how often and in which way each heap value can be accessed. \nDefinition 2.1 [uses]: A use is 0, 6, 1, or w. We use metavariables ~1, ~2,. . . for uses. A new point \nhere is the use 6: basically, we say that a heap value is accessed in an expression as a value of use \n6 if it is accessed locally inside the expression and cannot be returned as a part of the evaluation \nresult,. 0 means that the data cannot be accessed at all, 1 means that after the data is accessed in \na restricted manner as a value of use 6, it can be either (1) accessed at most once and deallocated inside \nthe current expression being evaluated or (2) put into the evaluation result as a value of use 1. w means \nthat the data can be accessed many times in any way. A value of use w can be regarded as a value of any \nother use, and a value of 1 can be regarded as a value of use S or 0. In order to express this kind of \nrelationship between uses, we define a total order 5 on uses by 0 5 6 5 1 5 w. We write ~1 < ~2 if ~1 \n# ~2 and ~1 5 ~2. Remark 2.2: By a heap value being accessed (or used), we mean the contents of the heap \nvalue are indeed read; so, just passing the reference to the heap value does not count as access. For \nexample, we do not say that y is accessed in (Xz.z)y, since the expression can be reduced to y without \nreading the actual contents of y. A pair is considered to be accessed only when its first or second element \nis extracted, a closure is accessed only when it is invoked, and a real number is accessed only when \nit is passed as an argument to a primitive function on real numbers. 2.2 Expressions We consider a simply-typed \nX-calculus with recursion and pairs as a target language. Its extension with polymor-phism and other \ndata structures will be briefly discussed in Section 6. Because our type system is sensitive to an evaluation \norder, we use the K-normal form [5] in order to make the evaluation order explicit and name each interme- \ndiate value. (For readability, we sometimes use expressions that are not in K-normal form when giving \nexamples.) The syntax of expressions is given in Figure 1. Each allocation or read operation of a heap \nvalue is annotated with a use. The annotation can be automatically inferred by a type inference algorithm \nexplained in Section 5. We write &#38; for the set of expressions. The metavariable V represents a value \nthat can be rep- resented in one word and need not be allocated in the heap. Here, it is either an integer \nconstant (denoted by n) or a variable (denoted by 2). A variable can be considered a heap address. (VI, \nVz) and Xny.Ml allocate a pair and a closure re-spectively in the heap, and records that their use is \nIC (which shall be restricted to O,l, or w by the type system given later). The recorded use expresses \nhow the value can be ac- cessed in the rest of the computation: 0 means that the value cannot be accessed \nat all (so, (VI, V2) need not actually al-locate the pair in the heap), 1 means that the value can be \naccessed as a linear value and may be deallocated later, and w means that the value can be accessed in \nan arbitrary manner and can be deallocated only by other mechanisms such as garbage collection. recn \n(f, y, Ml) creates a recursive function f such that f(y) = Ml. There are three operations to access the \nheap: fst (z) (snd (z), resp.) extracts the first (second, resp.) element of the pair stored at 2, and \nappZy (y, V) reads the closure stored at y and applies it to V. R attached to an access operation represents \nas a value of what use the heap value is accessed by this operation. If K. is 6, the heap is just read \nand unchanged; if it is 1, the heap value is deallocated after being read (if the use of the heap value \nis also 1 ). If n is w, then the operation assumes that the accessed heap value is an w-value and does \nnot deallocate the value. (so, w is the same as 6 in effect and is unnecessary; it is included just for \ntechnical convenience). If K. is greater than the actual use recorded at the accessed address, the access \nis invalid and causes an error. A conditional expression ifD V then Ml else Ma is re-duced to Ml if V \n= 0 and otherwise reduced to M2. Example 2.3: let z = (1,2)l in let y = fst (z) in let z = snd (z) in \nz allocates a linear pair (1,2) in the heap, and then reads its first element; at this moment, z is not \ndeallocated since fst is annotated with 6. After that, the second element, is read and the pair is deallocated. \n 2.3 Operational semantics In order to clarify how use annotation is used for allocat- ing/deallocating \nheap values at run-time, we define an op- erational semantics as a rewriting relation on pairs of a heap \nand an expression [17,22]. Definition 2.4 [evaluation contexts]: The set, of evalu- ation contexts is \ngiven by the following syntax: C[ ] ::= [ ] 1 let z = C[ ] in M. We write C[M] for the expression obtained \nfrom C[ ] by replacing the hole [ ] with M. Definition 2.5 [heap]: A heap value, denoted by h, is a term \nof the form (VI, Vz) or Xx.M. A heap is a mapping from a finite set, of variables to pairs consisting \nof a use and a heap value. We write (21 I-+~~hl, . . . , z,, I-+~ h,} for a heap H such that H(zi) = \n(Ki, hi). The use associated with a heap value is used for judging whether the value can be deallocated \nafter it is accessed as a linear value: the value can be deallocated only if the use is 1. The use 0 \nmeans that the value has been already The check of the use of the heap value is necessary, because we \nallow an w-value to be accessed as a linear value. If we forbid such coercion of an w-value into a linear \nvalue as in [22], then the check is unnecessary. However, we prefer not to make such restriction because \nthe check can be implemented efficiently(see Remark 2.6). A4 (expressions) ::= V 1 let z = (VI,VZ)~ in \nM 1let z = fst (y) in M 1let z = snd (y) in M 1let x = X y.Ml in M2 I let z = reC(f,y,Ml) in M2 I let \nz = appZy ((y, V) in M I let z = Ml in M2 1ifII V then MI else M2 V (non-heap values) ::= z 1 n Figure \n1: The Syntax of Expressions deallocated. We often write h for (VI, Vz) or A z.M. We write X for the \nset of heaps. The reduction relation WE (X x E) x ((74 x E) U{Error}) is defined by the rules given in \nFigure 2. Error indicates that invalid access to the heap has occurred (there are other kinds of errors \nsuch as application of a non-function to a value, etc.; however, because the lack of those errors can \nbe guaranteed by a type system in the usual way, this paper focuses on errors caused by invalid heap \naccess). The first two rules (R-HEAP) and (R-REC) are for allo- cating heap values. The use K. is recorded \nat a new address together with the allocated value. The heap is not changed in (R-SVAL) since V is a \none-word value. The rules (R-APP) and (R-APP-ERROR) for function ap-plication deserve attention. Since \napply (2, V) reads a clo- sure stored at z as a value with use K , the currently available use K of \nz should be greater than or equal to PC : otherwise, the application causes an error. If K. = K = 1, \nthe closure is deallocated after the application and it can no longer be called; so, the use of z should \nbe changed to 0. It is ex- pressed by the subtraction n -K , whose definition is given in Figure 3. Similarly, \naccess to a pair fst (x) or sndn (z) succeeds only if K, 2 IE , and the use is decreased by K after \nthe access ((T-FST) -(T-SND-ERROR)). Remark 2.6: One may think that the above operation on uses incurs \na heavy overhead at run-time. However, because the type system guarantees that no invalid access occurs, \nthe check of K > n is always guaranteed to succeed and can be eliminated. Moreover, the use associated \nwith a heap value can change only from 1 to 0; since this change means that the heap value is deallocated \nin the actual implementation, the use need not be updated actually. This further implies that the use \nof a heap value can be recorded not in the heap space itself but in the pointer to it by using a one-bit \ntag, and that the extra run-time cost is only to check this tag (to know whether the use of the accessed \nheap value is 1 or w) when the access operation is annotated with 1. So, we think the actual run-time \noverhead is quite small. Example 2.7: The expression in Example 2.3 is reduced as follows: ({},let z \n= (1,2)l in let y = fst (z) in let z = snd (z) in z) - ({u +bl (1,2)), let y = fst6(u) in let I = snd \n(u) in z) 4 ({u I+) (1,2)},let y = 1 in let z = snd (u) in z u ({u I+ (1,2)),let z = snd (u) in z) Q \n({u * (1,2)}, let z = 2 in z) 9 -({u ++O (112)), 2) The heap value u is considered to be deallocated \nwhen the use changes from 1 to 0. Example 2.8: If the expression in Example 2.3 is wrongly annotated \nas: let z = (1,2)l in let y = fst (z) in let z = snd (z) in z, then it is reduced to Error as follows: \n({},let z = (1,2)l in let y = fst (z) in let z = snd6 (z) in z) - ({u *l (L2)L let y = fst (u) in let \nz = snd6(u) in z) v) ({u I+ (1,2)},let y = 1 in let z = snd6(u) in z) u I+ (1,2)}, let z = snd (u) in \nz) Z Lror 3 Type System As was shown in Example 2.8, if an expression is wrongly annotated with uses, \na run-time error may occur. In order to reject such expressions, we introduce a type system. Our type \nsystem is a conservative extension of the usual type system for the simply typed X-calculus in the sense \nthat any expressions well-typed in the usual type system are well typed also in our type system. There \nare two major differences between our quasi-linear type system and the usual linear type systems. As \nexplained in Section 1, a linear value can be accessed many times as long as the last access is identified \nby the quasi-linear type system. The other difference is the interpretation of a pair type (see below). \n 3.1 Types Definition 3.1 [types]: The set of types, ranged over by 7, is given by the following syntax. \n7 ::= int 171 x6 72 I 71-bnl+Q72 n of 71 xK r2 and ~1 of r1-+K11r;2r2 expresses how a pair and a closure \ncan be accessed. We require n2 of rr-+~L lnars not to be 6. It represents how often (0, 1, or w ) the \nclosure can be invoked in the sense of previous linear type systems [9, 221. The closure itself is allocated \nand deallocated based on nl. The reason why we keep ~2 will become clear when we present a typing rule \nfor closure creations. Note that the meaning of a pair type is different from that in previous linear \ntype systems [8,22]. In the previous linear type systems, int x (int x1 int) corresponds to the linear \nlogic formula !( int @ (int @ int)); so, if a pair (1, (2,3)) has this type, it can be read an arbitrary \nnumber of times, and each time the second element (2,3) is extracted, it can be read at most once. On \nthe other hand, in our quasi-linear type system, the second element (2,3) can be accessed only linearly \nthrougghout aEZ the access to the pair (1, (2,3)) (not each time (2,3) is extracted). (If, C[let x = \nh in M]) +P (H{y r-) ( h}, C[[y/x]M]) (y fresh) (H, C[let z = rec (f, y, MI) in M]) w (H{z eK Xy.[z/fMr), \nC[[z/x]M]) (z fresh) (K CPet z = VP>:!)>; (H, CVI4W) _ (H{x +-FKXy.M}, C[appZy (x, V)]) u (H{x t-k- Xy.M}, \nC[[V/y]M]) (fc < d) v (K = 0) (H{x tie Xy.M}, C[appZy'('(x, V)]) -Error a>> >0 (H{x +-bK 047 vz)}, CwqZ)]) \n-(H{x +b-(v-l, vi)}, C[vl]) (lc < d) v (K = 0) (H{x I-+~ (VI, Vz)), C[fst (z)]) u Error K>IE >O (H{x \nI+= (V,,Vz)},C[snd E (x)]) v) (H{z tiKPK (Vr, V,)},C[Vz]) (K < K ) v (fc = 0) (H{z +-+ (VI, Vz)}, C[snd \n(z)]) u Error (H, C[ifo 0 then Mi else Mz]) ti (H, C[Mi]) n#O (H, C[ifo n then Mr else Mz]) -..+ (H, \nC[M2]) Figure 2: Operational Semantics (R-HEAP) (R-REC) (R-SVAL) (R-APP) (R-APP-ERROR) (R-FST) (R-FST-ERROR) \n(R-SND)  (R-SND-ERROR) (R-IFT) (R-IFF) Example 3.2: (int x6 int)-+ + int is the type of a linear closure \nthat takes a pair of integers as an argument, uses it locally, and returns an integer. The function may \nbe invoked many times (thus its use is w in the strict sense), but it can be invoked only linearly in \nthe relaxed sense (i.e., invoked as a closure of use 6 except for the last invocation). For example, \nX z.(f&#38;(z) + snd (z)) can have this type. Example 3.3: Consider an expression let f = XYz.(fst6(x) \n+ z) in let y = fst (snd (x)) in M and suppose x does not appear in M. Because the second element of \nthe pair x is accessed just once in fst (snd (x)), it can be assigned the type int xw (int x1 int); so, \nthe pair of integers can be deallocated at fst . Consider another expression let f = X z.(fst (snd (z)) \n+ 1) in apply (f, x) + apply (f, x). The second element of x is ac- cessed once (by fst ) each time it \nis extracted by snd (z). Since it is totally accessed more than once, the type int xw (int x w int) is \nassigned to x in our quasi-linear type system. 3.2 Type judgment A type judgment is of the form r !-M \n: 7, where I (called a type environment) is a mapping from a finite set of variables to types. It expresses \nhow each heap value is accessed during evaluation of M. For example, x : int x6 int I- M : int x1 int \nimplies (l)M may access the heap address x, (2)the result is a linear pair of integers, and (3)~ is used \nup in M and does not escape through the result. Notation 3.4: We write VI : ~1,. . . , V, : 7n for the \ntype en-vironment l? such that dom(I ) = {VI,. . . , Vn} f~ V and l?(V;) = Ti for each vi E don@), where \nV denotes the set of variables. For example, (x : int, 2 : id) denotes the type environment that maps \nx to int. If V $Z dom(l?), we write I, V : T for the type environment I such that (1) I =dom(r)u({V}nV)and(2) \nI (x) =rifx E ({V}nV) and r (x) = r(x) if 2 E dam(r). 3.3 Operations on uses, types, and type environments \nIn presenting typing rules, we use several operations on uses, types, and type environments given in \nFigure 3 and 4. As explained in Section 1, the operation ~1 + IE~ is used for computing the total use \nof a value when it is accessed as ~1 in one place and as tcz in another place. When the order of the \nuses is statically known, K~;KZ is used instead. 1~1 is the least non-6 use that is greater than or equal \nto K, and ]K] is the greatest non-6 use that is less than or equal to n. ~1 . ~2 is used for computing \nthe total use of a value when the value is used as a Kz-value ~1 times. Motivations for these operations \nwill become clearer when typing rules are given below. These operations are extended to operations on \ntypes and type environments. For example, if a heap value is accessed as a value of type int x6 int in \none place and as a value of type int x int in another place, it is totally accessed as a value of type \n(int x6 int) + (int x1 id) = int xw int, which means that the value may be accessed more than once. The \ndefinitions of operations on pair types and func-tion types deserve special attention; notice that the \nop-erations act on sub-components of pair type, but not on the arguments and return types of functions. \nIn order to understand the reason for this, suppose x is accessed in two places, each as a value of type \n(int x1 int) x1 int (i.e., the inner pair of integers and the outer pair are both ac-cessed linearly). \nThen, both the inner pair of integers and the outer pair are totally accessed more than once; it is expressed \nby the type (int x1+ int) x1+ int obtained by adding each use attached to the pair types, not by the \ntype (int x1 int) xl+l int (which means that the inner pair is totally accessed only linearly). On the \nother hand, sup-pose z is accessed in two places as a function of type (int x1 int)--+ yl(int x1 int). \nThen, the function is invoked twice, and each time it is invoked, it uses an argument pair linearly and \nreturns a linear pair. So, the total access to the function is expressed by (int x1 int)+ + ~ + (int \nx1 int), not by (int x +l int)-+ +ll +l(int x1+ id). Similarly, the operation r.1 also acts on sub-components \nof pair type but not on the arguments and return types of function types. This is because [rl is intended \nto express the type of an expression whose evaluation result does not contain S-values. Although the \ntype (int x6 int)-+ > (int x1 int) contains 6, it just means that a function of that type uses an argument \npair locally, not that the function must be used locally. so, [(int x6 int)+l* (int x1 int)l = (id x6 \nint)+lJ(int x1 iTId). We often omit . and write ~1~2 for ~1 . ~2 and IEP for K. I?. We give higher precedence \nto ., +, and ; in this order. Example 3.5: Let ri be int x (id x6 int) and 72 be int x1 (id x0 id). Then, \nh + = int xy (id x int) 72 r1; 3-2 = int x1 (id x6 int) = int x1 (int x1 int) w * 71 = id xw (int xw \nhat).  rr11 (z:71,y:71)+(z:72) = 2 : (71 + 72), y :7-l = 2: int xw (int x6 int),y: int x6 (int x6 id). \n 3.4 Typing rules 3.4.1 Variables, constants, and weakening The rule for variables and constants is: \nv:rl-v:7. (T-VAL) If an expression is well typed under some type environ- ment, then it should also \nbe well typed under a type environ- ment that represents more access capabilities. For example, if z \n: int x1 int t- M : int, then 2 : int xw int k M : int, since the former judgment requires z to be used \nas a linear pair, while the latter does not impose such a requirement. The following rule allows such \nweakening of an assumption: r kM:r r 5 r (T-WEAK) rkM:r The relation I 5 P, which means P represents \nmore access capabilities than I (in other words, I allows more liberal access to the heap), is defined \nin Figure 4.  3.4.2 Allocation of heap values Let us consider an expression let z = (y, z)~ in M. In \nM, y and .z can be accessed in two ways: either directly by using addresses y and z, or through the pair \nz (by extracting y and 2). If P, z : ~1 x s 72 I- M : 7, then the access in the former way is represented \nby the types I (y) and I (z), while the access in the latter way is represented by the types ri and ~2. \nTherefore, information on the total access is obtained by adding those types. For example, if I (y) = \nint x1 int and 71 = int x int, then the total access to y can be represented by P(Y) + 71 = int xy int. \nThus, the rule for the allocation of a pair is: r,z:rlx~T22M:r3 K#d (T-PAIR) We require E # 6 because \nthere is no use creating a J-value: since it can never be deallocated, creating a J-value is the same \nas creating an w-value. Remark 3.6: Actually, annotating allocation of a pair with 6 is meaningful and \nmay be rather useful if the type 7s of M does not contain 6: we can modify the operational se-mantics \nso that let x = (VI, V2) in M allocates the pair (VI, V2) in the stack and deallocates it after M has \nbeen evaluated (recall that a value with use 6 cannot be a part of the evaluation result). The rule for \nthe allocation of a closure is: r2,f: 71+*l+*rT22] k M2 : 5 Kl #S l~~[l?ll + IT2 I- let f = X x.M1 in \nMZ : 73 (T-Ass) [7-21 in the premise Pi, x : 71 i- Ml : [-r21 guarantees that when the closure Xx.M \nis called, it does not return a 6-value as a result. PI, x : 71 I- Ml : [7-21 also means that during \neach call of the closure, heap values are accessed as described by Pi. Because the other premise means \nthat the closure will be called ~2 times, the total access to heap values by MI can be represented by \n~2 copies of Pi, which is written as Icsl?i. Since the access happens only later (not when Xx.Ml is evaluated \nand the closure is created), r-1 is applied to it, so that it does not contain &#38;values. The following \nrule for recursive functions is similar to (T-Ass) except for the estimation of the use of the created \nfunction: t-let f = (f, x, Ml) recrK3(1+K1)1 in M2 : 73 (T-REC) The total number of calls of the function \nf is calculated by using the number ~2 of calls in MI and the number ~4 of calls in Ms. Since each invocation \nof f may result in other ~2 invocations of f, the total calls are counted as ~(1 + ~2 + K: + . . .) = \n~4(1+ ~2). The use of function f itself (in the relaxed sense) is similarly calculated by using its uses \n~1 and ~3 in Ml and Ms. The ceiling function r-1 applied to ~r(l + ~3) is just to make sure that the \nuse is not 6. 3.4.3 Let-expressions The following rule for let-expressions best illustrates how the \nevaluation order is taken into account in our type system: rl I- MI : [Tll r2, z : [T11 k M2 : T (T_LET) \nl?l;~21-letx=MIinM2:~ Here, the premise means that heap values are accessed by Ml as described by I i \nand by Ms as described by I s, x : [q]. In ordinary linear type systems, the total use of heap values \nin let x = Ml in M2 was computed by adding Pi and I s. However, since Ms is evaluated only after MI has \nbeen fully evaluated, we estimate the total access of heap values by sequential composition Ii; 12. We \nrequire the type of MI not to contain 6 at top-level (by applying 1-1 to ri), in order to make sure that \nevery &#38;value represented by Ii is really used up during the evaluation of Ml and does not escape \nto the rest of computation.  3.4.4 Access to heap values The following is the rule for closure invocation: \nr, z : ~1 I- M : 7-2 K>O (f : T~-+~*~TI + V : 5); I? I- let x = appZy (f, V) in M : ~2 (T-APP) The access \nto heap values by apply (f, V) is estimated as f : 7y-b+ n+ V : 7-3. Note that the first use K. of the \ntype of f should not be zero, and that the second use is 1, since the function is used once here. The \naccess of heap values except for z by M is estimated as r. Since apply (f, V) is evaluated before M is \nevaluated, the total access can be estimated as their sequential composition (f : T~+~* TI + V : 7-s); \nr. The rule for reading the first element of a pair is: rly : 7:,x : Tl d 72 k M : 73 tc>o r, 2 : CT1 \n+ Q X-K 72 f- let y = f&#38;*(x) in M : 73 (T-FST) The premise implies that the pair is used as a &#38; \n-value in M after being used as a n-value in fst (x). So, the total use of x is estimated as K; K . The \nfirst element may be accessed through either y or CC, hence the total use of the first element is estimated \nas ~1 + 71. Similarly, the rule for extraction of the second element of the pair is: r,y:~~,x:~~x~ 72~M:73 \nIc>o I?, x : 71 xILia (72 + T;) t- let y = snd (x) in M : 73 (T-SND)  3.4.5 Rules for conditionals In \nthe following rule for conditional expressions, we require that the then-part and the else-part has the \nsame typing.  rt-Ml:Tl r 1 M2 : 71 (T-IF)V:int+I kifDVthenMielseMa:ri 4 Type Soundness This section \nsketches a proof of the type soundness property that no invalid heap access occurs during evaluation \nof well- typed expressions. The full proof is given in the technical report [12]. The type soundness \nis formally stated as follows: Theorem 4.1: If 0 l- M : 7, then (0, M) +* Error. We want to show this \nproperty as usual [8,22], by defin-ing a typing system for a run-time state (H, M) and prov- ing that \nthe reduction relation preserves typing and that no well-typed state (H, M) causes an error immediately. \nUn-fortunately, however, the reduction relation -Q defined in Section 2.3 is not suitable for this purpose: \na key feature of our type system is to capture information on the order of heap access (recall (T-LET), \nfor example), whereas the flat representation of a heap loses that information. Therefore, we define \nan alternative operational semantics that repre-sents a heap by using nested letrec-expressions, and \nshow the soundness of the type system with respect to that seman-tics. Although it includes rather strange \nreduction rules, it is easy to see that the alternative semantics is essentially equivalent to the original \nsemantics. Before presenting the alternative semantics, we explain why the reduction relation ?r) fails \nto preserve typing. Let us consider the following configuration: ({w e1 (1,2),x ++l (w,3)},let v = MI \nin M2). where Ml = let y = fst (x) in let z = f&#38;(y) in z. Then, z : (id x6 int) x int k MI : int. \nSo, if x : (id x1 int) x1 int k M2 : id, then by (T-LET), x : (id x1 id) x1 int k let v = MI in M2 : \nint, since ((id x6 int) x int);((int X1 int) X1 int) = (id xl int) x1 int. So it is fine that w is a \nlinear pair. However, if we reduce the configuration by (R-FsT), the resulting configuration: ({w e1 \n(1,2), 3: e1 (w, 3)], let u = (let z = f&#38;(w) in z) in M2) requires w to be an w-pair: w is accessed \nas a linear pair in M (through x) and as a S-pair in fst!(w), and the type system cannot capture the \norder between the two uses (because the access is made through different variables z and w). One way \nto avoid the above problem would be to extend the type system so that the order of access to different \nvari- ables can be expressed, as in [ll]. Since the resulting typing rules would become rather complex, \nwe instead redefme the operational semantics. The idea is, before losing informa-tion on the order of \nheap access, to split the heap space in advance by taking the order into account. For example, the above \nconfiguration is first converted to something like: let v = ({w H (1,2),x ++ (w, 3)), MI) in ({w ++ (1,2),x \ne1 (w, 3)}, M2). Since the heap space has been split for the definition part and for the body part of \nthe let-expression, we can safely throw away information that the definition part is evaluated before \nthe body part. So, it can be reduced to: let v = ({w +k (1,2),x I-+ (w,3)},let z = fst (w) in z) in ({w \n+-+l (1,2),z ++ (w,3)}, Mz). In order to express the above kind of nested heaps and expressions, we \nintroduce a new class of expressions called dynamic expressions and define a reduction relation for dy- \nnamic expressions. Note that this new operational seman-tics is introduced just for proving Theorem 4.1. \nThe actual implementation can be based on the semantics defined in Section 2.3 and no cost for splitting \nheap needs to be paid. 4.1 Dynamic expressions The syntax of expressions given in Section 2 is extended \nto that of dynamic expressions as follows: Definition of IEI -KZ I Idl\\K.2 II0 I 6 L -, ,, , I 0 II 0 \nI ,un def 6 b 6 1 1 1 1 undef undef 0 W undef undef undef Definition of ~1 + ~2 Definition of ~1; ~2 \nIErIE 0 6 1 w w w W W Definition of ~1 * ~2 IE1 Tc2 0 6 1 w0 0 0 0 0 6 0 6 6 W1II0l~1 llw W Definition \nof [/cl R /016111w] 1r111,,,.,., 1 I/cl 11 u 1111 1 w I I Definition of 1~1 /c ~~0~6~1~w] nJllOlOlllwl \nW llolwlwlw Figure 3: Operations on uses Binary operations (op = +, ;) intopint = int (7-20p~i) if \nrropri and r20pri are well defined. = ~~~~~dx *ziLerwise NloPfi: ,fi20P4 h (71 x6 T2)OP(4 xK T;) (n-t \n=l+272)op(T~-+ 44,) = Tl-+ if x E dOm(rI) n d0m(r2) (r10pr2)(x) = lf x E dom(rl)\\do~(r2) if x E d0m(r2)\\d077i(rl) \n Umry operations (op = r-1, fc . _) op(int) = int op(7-1 xX T2) = op(71) X P(R) op(72) op(71+=1'~2T2) \n= T1+~P(41M2)~2 (op(r))(x) = Opmx)) Figure 4: Definitions of a relation and operations on types and type \nenvironments let x = h in A4 -% letrec z = h in [z/x]M (z fresh) (DR.-HEAP) ==C~zlz, (DR-FST) let u \n= fst (x) in M  wI4~ D q D, K>K >O (DR-READ) letrec x = h in D &#38; letrec x = h - in D D .$ D, \n (/c > K) v (d = 0) (DR-ERROR) letrec x = h in D -% Error letrec x = hRlzs2 in let y = D1 in D2 &#38; \nlet y = letrec x = hnl in D1 in letrec z = hsa in D2 @R-SPLIT) _ let x = (letrec Z = Z+ in letrec 23 \n= hfiS3 in V) in (letrec r = Z in M) (DR-VAR) A letrec z = ilLGJilr ain letrec 2u= I? @ in [V/z]M Figure \n5: Main reduction rules for dynamic expressions Definition 4.2 [dynamic expressions]: The set D of dy- \nnamic expressions, ranged over by D, is given by the follow- ing syntax: D ::= M 1 let z = D1 in Dz 1 \nletrec x = h in D Here, we introduced a letrec-expression letrec x = h in D to express a heap binding, \nand extended the syntax of let- expressions so that heap bindings and let-bindings can be nested. By \nusing the new syntax, the nested heap and ex-pression let u = ({z ti (w,3)}, Ml) in ({x til (w,3)}, Mz) \ncan be expressed by the following dynamic expression: let v = (letrec x = (w, 3) in Ml) in letrec x = \n(u, 3)l in MZ The typing rules for expressions can be easily extended for dynamic expressions. The new \nrules are as follows:  I ,x:~~x~~~i-D:r~ 2 e w, v,l I? + VI : 71 + V2 : 72 k letrec 2 = (VI, Vs) in \nD : 73 (DT-PAIR) ~~(1 + ICZ.)[~~~ + r2 t- letrec f = X x.Ml in D : 73 (DT-ABS) 4.2 Operational semantics \nof dynamic expressions We define an operational semantics by using a reduction relation D &#38; E. D \nis a dynamic expression and E is either a dynamic expression or a special constant Error representing \ninvalid heap access. The label I shows which heap value is accessed in the reduction step. It is either \ne, which indicates that an internal heap value is accessed, or x = h , which indicates that the heap \naddress x is accessed as a value with use n, or x, which indicates that the heap of address x is split \nas explained above. We show only the key rules in Figure 5. The rule (DR-HEAP) corresponds to the rule \n(R-HEAP) of the orig- inal semantics. The rules (DR-FsT), (DR-READ) and (DR-ERROR) correspond to the \nrules (R-FST) and (F-FST- ERROR). The rule (DR-SPLIT) is the most important rule: it allows the current \nheap to be split into that for the definition part of a let-expression and that for the body part. The \nrule (DR-VAR) corresponds to the rule (R-VAR), but it allows split heap bindings to be merged after the \ndefinition part of a let-expression has been fully eval-uated. In the rule, letrec z = 2 in D is a shorthand \nfor letrec zi = h; in . . . letrec z,, = hz in D. Example 4.3: The expression in Example 2.3 is reduced \nas follows: let 2 = (1,2)l in let y = fst6(x) in let z = snd (x) in z A letrec x = (1,2)l in let y = \nfst (x) in let z = snd (x) in z -% let y = (letrec 2 = (1,2) in fst6(x)) in (letrec x = (1,2)' in let \nz = snd (x) in z) A let y = (letrec x = (1,2)6 in 1) in (letrec x = (1,2)l in let z = snd (x) in z) &#38; \nletrec x = (1, 2)6 in let z = snd (x) in z &#38; let z = (letrec x = (1,2)l in snd (x)) in (letrec 2 \n= (1,2) in z) -% let z = (letrec 2 = (1,2) in 2) in (letrec x = (1,2)' in z) A letrec x = (1,2) in 2 \n 4.3 Proof sketch of Theorem 4.1 Theorem 4.1 follows immediately from the soundness of the type system \nwith respect to the new operational semantics and the fact that if an expression is reduced to Error \nin the original semantics then it is also the case in the new semantics. The type soundness with respect \nto the new operational semantics is stated as the two lemmas given below. The subject reduction property \n(Lemma 4.5) is a little more complicated than the usual one because even a well-typed dynamic expression \nmay be reduced to an ill-typed expres-sion if a heap value is split in an inappropriate way. The second \nstatement of the lemma says that, if heap splitting (the rule (DR-SPLIT)) can be applied to a well-typed \nex-pression, there is always a good splitting that preserves the well-typeness of the expression. Lemma \n4.4 [Lack of Immediate Error]: If l? F D : 7, then D f, Error. Lemma 4.5 [Subject Reduction]: a IfI l-D:randDiD \n,thenI I-D :r;and . If I I- D : r and D o\\ D , then there exists D such that D % D and I k D : r. We \nwrite -% for the relation % a._ a&#38;. We also write D t if D is reduced to Error no matter how the \nheap bindings in D are split, i.e., if D &#38; Error and there is no D E 2) such that D 4 D . The correspondence \nbetween the new semantics and the original semantics can be stated as follows: Lemma 4.6: There is a \nrelation 7Z(s ( H x E) x D) that satisfies the following conditions: . (0, M) RM; . if (H, M) -A (IS \n, M ) and (H, M)RD, then either D 4 D and (H , M )RD for some D or D t; and . if (H, M) -A Error and \n(H, M)RD, then D t. 5 Type Reconstruction Use annotations can be automatically inferred from unanno- \ntated expressions. Since it is done basically in the same way as that for previous linear type systems \n[9,22], we explain it only informally. The basic idea is to introduce variables ranging over uses and \ntypes, and constraints on them, so that we can express the most general typing (principal typ-ing) of \nan expression. A new type judgment is of the form I , C t- A4 : r, where a set of constraints C specifies \nwhich set of uses/types each use/type variable (in P, M, or 7) can range over. For instance, let z = \n(y, y) in z is typed as: y:a,{a>~+~,i#6,i>j}I-letx=(y,y) inz:/3xj~. This typing is principal% the sense \nthat all the typings derivable from the rules in Section 3 can be obtained by in- stantiating variables \n(Y, p, 7, i and j so that the constraint is satisfied. (We do not give the formal definition of principal \ntypings here; it is basically the same as that in [22].) Given an unannotated expression M, the inference \nof a use annotation proceeds as follows (actually, the second and third steps may overlap): 1. Attach \na fresh use variable to each place where use annotation is required (let the annotated expression be \nM ). 2. Compute a principal typing P, C t- M : r. 3. Solve the constraint C and apply the obtained substi-tution \nto M .  In the following subsections, we explain the second and third steps in a little more detail, \nand then discuss the cost of the analysis. 5.1 Computing a principal typing An algorithm for computing \na principal typing is obtained by constructing syntax-directed typing rules for the new type judgment \nform and by reading them from bottom to up. For example, we can merge the rules (T-PAIR) and (T-WEAK) \nand obtain the following rule for the new type judgment form: r,x: rl xK n,C k M: 7-3 c + r 1 (r + K \n: r1 + v, : rz) C!=K#J (TR-PAIR) P ,C E let x = (Vl,V2jR in M : 5 Here, C + Pi 1 Pz means that, for \nany substitution 0 on type/use variables, if K is satisfied then BI i 1 8 1 s is also satisfied. The \nabove rule says that in order to compute a typing for let x = (VI,VZ)~ in M, we should first compute \na typing for M and then add new constraints entailing I > P + Vl : 71 + Vz : Q and IE # 6. One major \ndifference from the previous linear type in- ference [8,22] is that constraints on type variables appear \nin C (recall the typing for let z = (y, y) in x given above). They are of the form r1 2 7s or 71 - 7s \n(meaning 71 and 72 are compatible in the sense that 7-1 + 72 and 71; 72 are well defined). Here, the \nrighthand type expression 72 of rl > 7-z may contain +, r-1, etc. as constructors of type expressions \n(rather than as functions on types). 5.2 Solving constraints on types and uses An algorithm for solving \nconstraints can be divided into two steps. First, a set of constraints on type/use variables can be simplified \ninto a pair of a set of constraints on use vari- ables of the form i 2 K and a set of trivial constraints \non type variables of the form a -/3 or cr 2 r where r is a Note that a constraint i # 6 can be expressed \nas i 2 [il. type expression (such as p + 7) constructed only from type variables. This simplification \nis performed by partial uni-fication of types, with some uses being kept different. For example, given \na constraint (Y 2 (p Xj real ) + y, we can instantiate a and y with ,B xj real and p xj real for fresh \nvariables ,B , /3 , i , i , j , j and reduce the constraint to{p ~~+~ ,i ~i+i ,j ~j+j }. Next, since \nthe set of trivial constraints on type variables always has a solution (let all the remaining type variables \nbe instantiated with, say, id), we can obtain the least as-signment to use variables by solving the set \nof constraints on use variables. We can use the simple iterative method used in previous linear type \nsystems [8,9]. Of course, the least assignment to some use variables cannot be completely determined \nuntil the whole program is given. For example, given an expression of type real , we cannot fmd the least \nassignment to the use variable i until we know all the places where the evaluation result of the expression \nis used. For these unknown variables, we can either simply assign w or delay constraint solving. One \nimportant difference from the previous linear type inference is that the least solution is not neces-sarily \nthe best solution from the viewpoint of mem-ory management. For example, consider an expres-sion let \ny = (1,2) in (let x = fst(y) in x). The least annotation is let y = (1,2) in (let x = fsts(y) in x). \nIn this expression, y cannot be deallocated since the only access to the pair y (fst(y)) is annotated \nwith 6. Therefore, it is better to annotate the expression as let y = (I,2)l in (let z = fd(y) in x). \nWe can think of two ways for dealing with this. One way is to classify uses into those for annotating \nheap allocation and those for annotating heap access. After the least solution is ob- tained, we can \nmaximize heap access annotations as long as no heap allocation amrotations increase. The other way is \nto extend a target language with an explicit dealloca-tion operation free(P): It deallocates heap values \nas if the heap were accessed as described by I . For example, fiee(z : reall,y : rea!6) deallocates x \nbut not y. The rule (T-WEAK) can be changed as follows (l? -I is the least type environment I such that \nI ; I = I ): r't-M:r r' 5 r (T-WEAK ) r I-M;free(r -r ) : 7 This change avoids forgetting to deallocate \nlinear values. We can optimize the result by moving the operation free leftward and merging it with heap \naccess annotation as far as possible. Although there is no guarantee that these meth- ods give the best \nuse annotation, we expect that both give enough good an annotation in practice. 5.3 Cost of the analysis \nUnfortunately, the computational cost for type reconstruc-tion can be exponential in the size of an input \nin the worst case. The reason is that the number of use variables can be linear in the size of the type \nof a variable appearing in an input expression and that the size of a type can be expo-nential in the \nsize of an input expression. For example, if a variable x is required to have type (int x int) x (id \nx int) by the context, its typing is: 2 : (int x jl id) xi3 (id x i* id), {il 2 i;, i2 > i;, is > ii) \nk 2 : (int xi; int) xi; (int xi: id). So, the number of use variables is linear in the size 3 of the \ntype (int x int) x (int x int). Next, consider an expression let 20 = 1 in let 21 = (20,ze) in let 22 \n= (zi,zi) in . ..let 2, = (2,_1,z,_i) in zn. The size of the type of the expression is exponential in \nn. In spite of the above fact, we expect that the reconstruc- tion can be performed efficiently for \nrealistic programs for the following reasons. First, the above problem occurs only when tuple or record \ntypes are extraordinarily nested; a huge flat tuple type expression int x int x ... x int or an arrow \ntype expression ri+ 72 do not cause such problems. Second, if the computational cost is really problematic \nfor some pro- gram, we can save the cost by sacrificing the accuracy of the analysis. For example, we \ncan restrict the + operator so that (71 xK 72) + (ri x ~4) is defined only if ri = ri and r2 = r;. This \nforces many type expressions to be shared and saves the time and space of the analysis (in fact, the \npre- vious analysis in [8,9], which imposes a similar restriction, is performed in polynomial time for \nthe monomorphic type system). 6 Extensions We have so far considered a simply-typed X-calculus with recursion \nand pairs. It can be extended by introducing poly-morphism on uses and types, and by adding other kinds \nof data such as recursive data structures and reference cells. For lack of space, we only briefly discuss \nhow to introduce polymorphism, constants such as real numbers, and recur-sive data structures. 6.1 Polymorphism \nBecause the type system presented in Section 3 is monomor- phic in both uses and types, the analysis \nof (quasi-)linear values is often rough. Consider the following expression: let f = Az.(z,z) in let z \n= upply(f,2) in let w = apply (f, 3) in M. If either z or w is used as an w-value in M, the other is \nalso forced to be an w-value, because the code for the pair creation (i.e., the body of the function \nf) is shared. In order to avoid the above problem, we can intro-duce polymorphism on uses. Then, f can \nbe given a type Vi # G.(int+ + (int xi id)) and the above expression can be annotated, for example, as \nlet f = Ai.Xz.(z, z)~ in let L = appZy(f[l],  2)in let w = appZy(f[w],3) in M. Here, uses are explicitly \npassed as parameters to polymor-phic functions. In general, we need to introduce a constrained type scheme \nof the form Voi,. . . ,a,, il,. . . ,ik :: C.-r, where C is a set of constraints on type and use variables \nsuch as (~1 2 (~2 + (~3 and ii 1 iz + 1. A new rule for polymorphic let-expressions roughly looks like: \nrl, 4 A C2 k Ml : 1~11 l-2,2 : trd,;:: c2.[Tll,&#38; t-Mz : 72 Ci, a do not affect the values of variables \nin Pi, M rl;rz,C1AC3~letz=M~inM2:r2  (T-LETPOLY) 6.2 Large constants So far, we have considered only \nintegers as constants. In order to deal with large constants (i.e., constants that cannot be represented \nin one word and must be allocated in a heap), we need to annotate each allocation of a large constant. \nFor example, allocation of a linear real number 2.0 is annotated as let 2 = 2.0 in . . . . The type of \na large constant is also annotated with a use. For instance, the type of linear real numbers is represented \nby real . The typing rule for real numbers is given as follows:  P, z : real k M : T l? k let x = rK \nin M : r 6.3 Primitive functions Primitive functions can be treated as constants of polymor- phic types, \nand each occurrence of a primitive function can be annotated with uses. For example, the primitive + \non real numbers can be assigned a type The expression let z = + z in . . . can be annotated like: ? \nlet z=+[l,l,l,l](y,z) in .... 6.4 Recursive data structures Recursive data structures such as lists \nand trees can be treated in the same way as pairs, by annotating their type constructors with uses. The \ntype of a list of values of type r is expressed as r list , where K expresses the use of each cons cell \nof the list. For example, real list is the type of a list whose cons cells may be accessed many times \nin an arbitrary manner, but whose elements are real numbers that can be accessed only linearly. Primitives \nfor lists can be given the following types: :: : VCY,i.((a xi a liSti)+u*wa list ) nil : Va, i.o list \nhd : Vo,i :: {i > S}.(a list + +a) . . . In general, we can generate the types of constructors and \nde- structors from datatype declarations of Standard ML [15]. 7 Preliminary Experiments We have implemented \na prototype type reconstruction sys-tem for the core-ML (i.e., SML [15] without modules) based on our \nquasi-linear type system and a simple profiler that executes the output program and counts the number \nof al- located heap values. We first describe the current status of the system in Section 7.1 and then \nreport the results of simple experiments in Section 7.2. The prototype system willbe availablefrom http://wuv.yl.is.s.u-tokyo.ac. \njp /'koba/research/qlinear/. 7.1 A prototype type reconstruction system Following the extensions discussed \nin Section 6, we have im- plemented a prototype analyzer. It takes an expression of the core-ML as an \ninput, performs type (and use) recon-struction and produces a use-annotated expression. It has been implemented \nby using the ML kit version 1 [4] as a front end, and supports full features of the core-ML: records, \ndatatype declarations, references, and exceptions. Polymor- phism on types and uses is based on the let-polymorphism \ndiscussed in Section 6. Polymorphic recursion on uses would be sound (just as polymorphic recursion on \nregions is sound in the region in- ference [20]), but it is not supported currently. That is be- cause \nthe algorithm would become complex and also because the polymorphic recursion does not seem so crucial \nfor our analysis. The current system has the following limitations. It produces the least use annotation, \nnot the best one (see Section 5); The analysis of the use of a reference cell is rough; Unnecessary uses \nare passed as parameters to use-polymorphic functions (they can be removed in the same manner as unnecessary \nregion parameters are removed in a post-path of the region inference [5]); The current system is very \nslow for several known reasons (it is just because we preferred rapid prototyping and the system will \nbe opti- mized in the future); A compiler that utilizes the analyzed information for in-place update, \netc. has not been imple-mented yet.  7.2 Results of preliminary experiments Table 1 shows the result \nof experiments. The columns zero, linear, and omega respectively show how many heap val- ues of use \n0, 1, and w are allocated dynamically in each program. The rightmost column shows what percentage of \nheap values are zero or linear. For a use-polymorphic func-tion like f : Vi.(realZ+ + real ), the allocation \nwas counted only once; in other words, creations of instances like f[l] and f[w] are not counted as the \nheap allocation (this is because the current analyzer infers not the use of each instance of a use-polymorphic \nfunction but the total use of all the in- stances). sumlist10000 is a program that makes a list of integers \nfrom 0 to 10,000 and computes the sum of them. qsort20 and msort20 sort a list of 20 real numbers by \nusing the quick sort and merge sort algorithms. They were written in a naive way; the main part of the \nquick sort program is: fun quick (Cl> = [I I quick (x::l) = let val (11,12)=divide(x,l) in append(quick(ll), \nx::quick(l2)) end sieve2000 finds prime numbers less than 2,000 by using the sieve of Eratosthenes. life \ncomputes the Iirst 10 gen- erations of lives generated from the initial 44 lives. dangle and reynolds3 \nare programs taken from [5] (with some pa- rameters being changed). reynolds3u is a slightly modified \nversion of reynolds3, in which one function is uncurried so that our analysis works better. In the cases \nof sumlist, merge/quick sorts and the sieve of Eratosthenes, almost all of the heap values are linear: \nnon-linear values were only top-level functions. Moreover, by looking at the produced use-annotated expression, \nwe can find that the cons cells generated in merge/quick sorts and the sieve of Eratosthenes can all \nbe updated in-place. In the case of the life (some curried functions in the orig- inal program were uncurried: \nsee discussions in Section 9), dangle, and reynolds3, not all heap values are linear, but the number \nof linear values is still huge enough to greatly reduce the need for garbage collection. In the Knuth-Bendix \nand boyer programs, the percentage of linear values is relatively smaller. This is probably because unification \nof first-order terms performed in the program causes sharing of many heap values. One should, however, \nnote that the percentages shown in the rightmost column do not necessarily indicate how much memory space \ncan be saved by our analysis: com-pared with memory management using conventional garbage collection, \nextra memory space is required for representing use-polymorphic functions and its instances, and it is \nnot yet clear how soon linear values can indeed be deallocated. Also, the current analysis is performed \non unoptimized programs; other optimizations such as inlining and hoisting may reduce the ratio of linear \nvalues. Therefore, we know for sure the real impact of our analysis only after more serious experi- ments \nare carried out (for that purpose, some of the future work described in Section 9 must be completed). \n8 Related Work Previous linear type systems To our knowledge, among previous linear type systems [3,9,22] \nthat can automati-cally infer the usage of values, only Barendsen and Smet- sers s uniqueness typing \n[3] takes an evaluation order into account. The effect of taking an evaluation order into ac-count sometimes \ndepends on a programming style, but it is evident at least for the following functions: fun map f Cl \n= Cl I map f (x::l) = f x::map f 1 fun diff ( Cl , S2) = Cl I diff (x::Sl, S2) = if member(x, S2) then \ndiff(S1, 52) else x::diff(Sl,SZ) fun move (p, dx) = updatecp, X, p.X+dx) The function f in map and the \nlist S2 in diff (which is a function computing the set difference) are used many times, but are judged \nto be linear in the quasi-linear type system. The function move takes a record representing a point object \nas the first argument and adds dx to its X field. The point p is accessed twice, but it is also judged \nto be linear. The uniqueness typing [3] is different from ours in many ways: the evaluation order is \ncall-by-need,3 their type sys-tem does not have b-type of values that must be used up locally, and instead \nit relies on a separate analysis for an-alyzing the order of memory access (so, the analysis is not integrated \nas a use-type system, and it is rather complex). Guzman and Hudak [7] and Odersky [18] also proposed \na kind of an extension of a linear type system, which can take an evaluation order into account and check \nthat destructive operations on arrays or lists are safely used. Unlike ours, their approach is to let \nprogrammers explicitly declare where destructive operations should be performed (by using special primitives \nfor destructive update of data structures) and check whether they are safe by performing type inference. \nRegion inference There is an alternative approach to static memory management: region inference [5,20]. \nThe region inference abstracts a bunch of memory addresses as a region, and estimates its life-time by \nperforming a kind of type in- ference. Our type system and the region inference have both 3Recently, \nthey dealt also with strict let expressions, but the anal-ysis for them seems to be more naive than our \nanalysis [19]. I zero I linear I omena I (zero+linear)/total I I v I\\ I, I sumlist qsort20 msort20 sieve2000 \nlife Knuth-Bendix boyer mandelbrot 0 40,0011 [ 0 448$ I 0 496 0 256,455 0 2.353.116 0 10;339;410 0 1,163,786 \n0 5,672,170 danale 20.000 20.032 reynolds3 0 1 211515 ] 2 1 100.0% 4 I 99.1% 4 99.2% 8 100.0% 26.600 \n98.9% 2,949;292 77.8% 342,642 77.3% 920,944 86.0% 34 99.9% 2,059 ] 91.3%  reynolds3u 0 I 21,514 1 13 \n] 99.9% Table 1: The number of allocated heap values advantages and disadvantages. Because the life-time \nof data is estimated region-wise, it is difficult to use the region in- ference for in-place update. \nAnother shortcoming of the re- gion inference is that too many data are often merged into the same region \n(especially when recursive data structures, higher-order functions, and reference cells are used), and \nas a result, the analysis of the life-time of data sometimes be-comes rough. For example, consider how \nto represent the type of a list. Because a list may contain an arbitrary num-ber of cons cells, it is \nimpossible to use a distinct region for each cons cell; so, the type of a list must be something like \n(7 list, T) where T is the region in which all the cons cells are stored. This implies that a cons cell \nof a list can be deallo- cated only after all the cons cells of the list become garbage. On the other \nhand, in our analysis, the type of a list is of the form r list ; if K. is 1, each cons cell can be deallocated \nseparately when it is accessed as a value of use 1. Thus, unlike in the region inference, the life-times \nof cons cells are not merged. (We do not intend to say that our analysis is always better for lists than \nthe region inference. Indeed, if some cons cell of a list is accessed as an w-value, our analysis assigns \ntype r list to the list; hence no cons cells of the list can be deallocated automatically. In this case, \nthe region inference would be much better.) On the other hand, the advantages of the region inference \nare that it is completely independent of how often values are accessed, and that the deallocation of \ndata in the same region can be performed in a constant time. So, it may be interesting to use our type \nsystem and the region inference as complementary methods for static memory management. The goal of our \nanalysis is also similar to that of the storage mode analysis [5], which was proposed as a comple- mentary \nto the region inference. It analyzes whether each access to a value is the last access to the region \nof the value, and if so, it inserts a code to deallocate all values in the region. Thus, unlike our analysis, \nwith the storage mode analysis, a value can be deallocated only after all the data in the region become \ngarbage. 9 Conclusion and Future Work We proposed an extended linear type system that can el-egantly \ntake an evaluation order into account, and proved its correctness. Static memory management like the \none proposed here and the region inference is currently less popular than conventional garbage collection. \nHowever, we think it will become very important in the near fu-ture. First, static memory management \nis especially attrac-tive in parallel/distributed environments, since it requires much less communications/synchronizations \nthan conven-tional garbage collection. Second, with the increasing im-portance of cache memory, saving \nthe memory space for a program is also likely to save its execution time. The type system proposed in \nthis paper is for call-by- value languages; it is not yet clear whether a similar type system can be \ndeveloped for lazy functional languages. Much work is left to be done to apply our type system to a compiler \nof ML and know its real impact. We need to work at least on the following issues (although they seem \nto be fairly straightforward except for the last issue): Refinement of the type inference system As discussed \nin Section 5.2, the least use annotation may not be optimal from the viewpoint of memory management; \nso, we need to modify the analyzer so that it can produce a better use annotation. Transformation for \nin-place updates We need to imple-ment a compilation path that finds places where deallocation of a heap \nvalue is immediately followed by allocation of an- other value of the same type and replaces it with \nm-place update. Combination with conventional garbage collection Since the linear-type-based memory management \ncan automati-cally deallocate only quasi-linear values, we need a conven- tional garbage collector as \nwell. There is, however, a subtle problem: since the linear-type-based memory management is not based \non the pointer traversal information, it may create a dangling pointer (just as the region-based mem-ory \nmanagements does [20]). For example, when the closure (Xz.(fst(y)+z), {y = (1.0,2.0)}) is traced at garbage \ncollec- tion time, the second element 2.0 of y may have already been deallocated. There are two ways \nfor dealing with this prob- lem. One way is to exclude the use 0 from the type system so that dangling \npointers cannot be created. The other way, which we are currently exploring (with Atsushi Igarashi), \nis to use the idea of tag-free garbage collection [16,21]: we can utilize use-annotated types for tracing \nheap values at garbage collection time, instead of conventional types. For example, from the type real \nxy real of y in the above clo- sure, it is known that the second element of y will not be accessed by \nthe closure and hence it need not be traced by a garbage collector. The following improvement of the \naccuracy of the anal- ysis is also future work. Taking the order of access to different variables into \nac-count Because the type system cannot deal with the order of access to different variables, the analysis \ncan be rough when variables are aliased. For example, consider an ex-pression let y = z in fst (z) + \nfst (y). Since z and y re-fer to the same heap value, we could regard z as a linear value; however, the \ncurrent type system judges z to be an w-value. We can overcome this problem by representing a type environment \nas a poset of type bindings in order to express the order of access to different variables, as in an \nearlier version [lo] of Kobayashi s type system for deadlock- freedom [ill. Combining our analysis with \nother analyses Our analysis of &#38;values is weak in dealing with curried functions. Con-sider an expression \n(Xr.(fst(z) + 1.0))(2.0,3.0) and its cur- ried version ((Xz.Xy.(z + 1.0))2.0)3.0. For the former ex-pression, \n6 can be assigned to 2.0, while 1 must be assigned to it for the latter one. This problem may not be \nso serious because the ordinary compiler optimization uncurries func- tions as far as possible, but it \nmay be interesting to improve the analysis by using more accurate dataflow information like the one obtained \nby the region inference. Acknowledgment We would like to thank Atsushi Igarashi, Martin Odersky, Kenjiro \nTaura, Mads Tofte, and the anonymous referees for useful discussions and comments. References H. G. Baker. \nLively linear lisp -look ma, no garbage! PI   ACM Sigplan Notices, 27(8):89-98, 1992. H. G. Baker. \nA linear logic quicksort. ACM Sigplan PI Notices, 29(2):13-18, 1994. E. Barendsen and S. Smetsers. Conventional \nand[31 uniqueness typing in graph rewrite systems. Technical Report CSI-R9328, Computer Science Institute, \nUni-versity of Nijmegen, 1993. An extended abstract ap-peared in Proc. of FST&#38;TCS 12, Springer LNCS \n761, pp.41-51. L. Birkedal, N. Rothwell, M. Tofte, and D. N. Turner. PI The ML Kit (Version 1). Technical \nReport 93/14, De-partment of Computer Science, University of Copen-hagen, 1993. L. Birkedal, M. Tofte, \nand M. Vejlstrup. From re- FJI gion inference to von neumann machines via region representation inference. \nIn Proceedings of ACM SIG-PLAN/SIGACT Symposium on Principles of Program- ming Languages, pages 171-183, \n1996. J.-Y. Girard. Linear logic. Theoretical Computer Sci- 161 ence, 50:1-102, 1987. J. G. GuzmBn and \nP. Hudak. Single-threaded polymor- 171 phic lambda calculus. In Proceedings of IEEE Sym-posium on Logic \nin Computer Science, pages 333-343, 1990. A. Igarashi. Type-based analysis of usage of values PI for \nconcurrent programming languages. Master s the-sis, Department of Information Science, University of \nTokyo, 1997. A. Igarashi and N. Kobayashi. Type-based analysis of PI usage of communication channels \nfor concurrent pro-gramming languages. In Proceedings of International Static Analysis Symposium (SAS \ng7), Springer LNCS 1302, pages 187-201, 1997. N. Kobayashi. A Partially Deadlock-free Typed Process Calculus \n(I) -A Simple System -. Technical Report 96-02, Department of Information Science, University of Tokyo, \nSeptember 1996. PO1 N. Kobayashi. A partially deadlock-free typed process calculus. ACM nansuctions on \nProgmmming Lan-guages and Systems, 20(2):436-482, 1998. A prelim- inary summary appeared in Proceedings \nof LICS 97, pages 128-139. Pll P21 N. Kobayashi. Quasi-linear types. Techni-cal Report 98-02, Department \nof Information Sci-ence, University of Tokyo, 1998. Available through http://ava.yl.is.s.u-tokyo.ac.jp/ \nkoba /publications.html. N. Kobayashi, B. C. Pierce, and D. N. Turner. Linear-1131  ity and the pi-calculus. \nIn Proceedings of ACM SIG-PLAN/SIGACT Symposium on Principles of Progmm- ming Languages, pages 358-371, \nJanuary 1996. I. Mackie. Lilac : A functional programming language based on linear logic. Journal of \nFunctional Program-ming, 4(4):1-39, October 1994. [I41 R. Milner, M. Tofte, R. Harper, and D. MacQueen. \nThe Definition of Standard ML (Revised). The MIT Press, 1997. 1151 G. Morrisett. Compiling with Types. \nPhD thesis, School of Computer Science, Carnegie Mellon University, 1995. [I61 G. Morrisett, M. Felleisen, \nand R. Harper. Abstract models of memory management. In Proceeding8 of Functional Programming Languages \nand Computer Ar-chitecture, pages 66-76, 1995. P71 [I81 M. Odersky. Observers for linear types. In Pro-ceedings \nof 4th European Symposium on Programming (ESOP g2), Springer LNCS 582, pages 390-407, 1992. R. Plasmeijer \nand M. van Eekelen. Concurrent Clean ver.l.3 language report, 1997. P91 M. Tofte and J.-P. Talpin. Implementing \nthe call-by- value lambda-calculus using a stack of regions. In Pro-ceedings of ACM SIGPLAN/SIGACT Symposium \non Principles of Programming Languages, pages 188-201, 1994. PO1 A. Tolmach. Tag-free garbage collection \nusing explicit type parameters. In Proceedings of ACM Conference on Lisp and Functional Programming, \npages l-11, 1994. WI D. N. Turner, P. Wadler, and C. Mossin. Once upon a type. In Proceedings of Functional \nProgramming Lan-guages and Computer Architecture, San Diego, Califor-nia, pages l-11, 1995. P21   \n \n\t\t\t", "proc_id": "292540", "abstract": "", "authors": [{"name": "Naoki Kobayashi", "author_profile_id": "81100603931", "affiliation": "Department of Information Science, University of Tokyo", "person_id": "PP39074467", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/292540.292546", "year": "1999", "article_id": "292546", "conference": "POPL", "title": "Quasi-linear types", "url": "http://dl.acm.org/citation.cfm?id=292546"}