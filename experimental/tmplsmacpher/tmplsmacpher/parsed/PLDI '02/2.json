{"article_publication_date": "05-17-2002", "fulltext": "\n Fast Copy Coalescing and Live-Range Identi.cation Zoran Budimli\u00b4 c, Keith D. Cooper, Timothy J. Harvey, \nKen Kennedy, Timothy S. Oberg, and Steven W. Reeves Department of Computer Science Rice University, Houston, \nTexas ABSTRACT This paper presents a fast new algorithm for modeling and rea\u00adsoning about interferences \nfor variables in a program without con\u00adstructing an interference graph. It then describes how to use \nthis in\u00adformation to minimize copy insertion for 4-node instantiation dur\u00ading the conversion of the static \nsingle assignment (SSA) form into the control-.ow graph (CFG), effectively yielding a new, very fast \ncopy coalescing and live-range identi.cation algorithm. This paper proves some properties of the SSA \nform that enable construction of data structures to compute interference informa\u00adtion for variables that \nare considered for folding. The asymptotic complexity of our SSA-to-CFG conversion algorithm is O(na(n)), \nwhere n is the number of instructions in the program. Performing copy folding during the SSA-to-CFG conversion \nelim\u00adinates the need for a separate coalescing phase while simplifying the intermediate code. This may \nmake graph-coloring register allo\u00adcation more practical in just in time (JIT) and other time-critical \ncompilers For example, Sun s Hotspot Server Compiler already employs a graph-coloring register allocator[10]. \nThis paper also presents an improvement to the classical inter\u00adference-graph based coalescing optimization \nthat shows a decrease in memory usage of up to three orders of magnitude and a decrease of a factor of \ntwo in compilation time, while providing the exact same results. We present experimental results that \ndemonstrate that our algo\u00adrithm is almost as precise (within one percent on average) as the improved \ninterference-graph-based coalescing algorithm, while re\u00adquiring three times less compilation time. Categories \nand Subject Descriptors: D.3.4 [Programming Lan\u00adguages]: Processors compilers, optimization General \nTerms: Algorithms, Languages, Theory Additional Keywords and Phrases: Register allocation, code gener\u00adation, \ncopy coalescing, interference graph, live-range identi.cation Permission to make digital or hard copies \nof all or part of this work for personal or classroom use is granted without fee provided that copies \nare not made or distributed for pro.t or commercial advantage and that copies bear this notice and the \nfull citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. PLDI 02, June 17-19, 2002, Berlin, Germany \nCopyright 2002 ACM 1-58113-463-0/02/0006 ...$5.00. 1. INTRODUCTION It has long been known that copies \ncan be folded during the construction of the SSA form [2]. Essentially, each variable that is de.ned \nby a copy is replaced in subsequent operations by the source of that copy. The implementation of this \nstrategy has a cou\u00adple of subtle problems but is otherwise an effective optimization. In effect, copy \nfolding during SSA construction deletes all of the copies in a program, except for those that must be \nused to instanti\u00adate 4-nodes. This simple observation may reduce the number of copies in a program, but \nnaive 4-node instantiation introduces many unneces\u00adsary copies. In our experience, the number of copies \nused to in\u00adstantiate 4-nodes is much higher than the number of copies in the original code. Classically, \nto solve this problem one would coalesce SSA names into a single name using an interference graph, as \ndescribed by Chaitin [5, 4]. The interference graph models names as nodes, and edges between the nodes \nrepresent an interference there is at least one point in the code where the two name are simultaneously \nlive. The intuition for register allocation is that if two variables interfere, we will have to use a \ndifferent register for each variable. The same holds true for copy coalescing: if two names joined by \na copy interfere, the copy is necessary and cannot be coalesced. Unfortunately, this conceptually simple \nmethod requires a data structure that is quadratic in the number of names. As Cooper, et al. showed [6], \nthe cost of building an interference graph is signi.\u00adcant, and for JIT compilers or other systems where \ncompile time is crucial, this cost may be prohibitive. This paper presents a new algorithm that performs \ncopy coalesc\u00ading without building an interference graph. We model interferences using a combination of \nliveness and dominance information. We present a theoretic background that shows that our optimization \nis safe. We present experimental data that shows that, on average, our implementation of this algorithm \nrivals the effectiveness of the interference-graph coalescer in removing dynamic copies while us\u00ading \nonly a fraction of the compilation time required by that algo\u00adrithm.  2. SSA PROPERTIES This section \npresents a brief description of the SSA properties that we use to construct our algorithm. A full formal \ndescription, along with the proofs of all the lemmas and theorems is presented elsewhere [3]. The preliminary \nstep in our algorithm is to ensure that the code is in regular form, which we shall de.ne with the following \ncon\u00adstruction. We start with a standard CFG, (N, E, b0 ), made up of a set of N nodes, E edges, and a \nunique start node, b0 . We impose an added restriction on the code in the incoming CFG, that it be strict. \n DEFINITION 2.1. A strict program is one in which, for every variable, v, and for every use of v, every \npossible path from b0 to the use of v passes through one of the de.nitions of v. Strictness is a requirement \nfor languages such as Java, but it can be imposed on languages like C or Fortran as well by initializing \nevery variable at the start of b0 . The initializations that are unnec\u00adessary can then be removed by \na dead-code elimination pass, or, alternatively, we can restrict the initializations to only those vari\u00adables \nthat are in the live-in set of b0 . Since the live-in set of b0 should be empty, variables that make \nit into this set are those for which there is an upwards-exposed use of the variable, and thus some path \nfrom the use backwards up the CFG on which there is no de.nition of the variable. Given a strict program, \nwe then convert the code into SSA form. It is a natural result of the SSA algorithm that, after the conversion, \nnot only is every use of a variable dominated by a single de.nition, but also every de.nition dominates \nall of its uses.1 We call this a regular program. Given a regular program, we can now de.ne interference: \nDEFINITION 2.2. Two variables interfere if and only if they are simultaneously live at some point in \na regular program. With De.nition 2.2, we can prove the following theorem: THEOREM 2.1. If two variables \nin a regular program interfere, the de.nition point of one will dominate the de.nition point of the other. \nInformally, the proof for this theorem results from the fact that both variable de.nitions must dominate \nthe point in the program where they interfere. We can re.ne Theorem 2.1 to suit our purposes with the \nfollow\u00ading theorem, which results directly from the computation of live\u00adness: THEOREM 2.2. If two variables, \nv1 and v2 , in a regular pro\u00adgram interfere and the de.nition point of v1 dominates the de.ni\u00adtion point \nof v2 , then either v1 is in the live-in set of the block in which v2 is de.ned, or both variables are \nde.ned in the same block. Thus, we can quickly check interference between two variables simply by examining \nthe liveness information associated with the blocks in which the variables are de.ned.  3. COALESCING \nALGORITHM With the code in regular form, we can proceed with the algo\u00adrithm. Essentially, we use union-.nd \nto group together all of the names joined at 4-nodes. We then use the previous reasoning about interference \nto break the union-.nd sets apart when we discover two members of the same set interfere. To break a \nset, we reinsert copies between the member that we want to remove and all of the other members of the \nset. At the end, all SSA names that are in the same set do not interfere, and, therefore, can share the \nsame name. Note that we build pruned SSA [8] to make the reasoning simpler and because parts of the analysis \nnecessary for pruned SSA, such as liveness analysis, are assumed. The algorithm we present should work \nfor minimal or semi-pruned SSA as well, although the addi\u00adtional inexactness of those forms propagates \nitself into our analysis, possibly causing the insertion of extra copies that may not other\u00adwise have \nbeen added. If we had not allowed copy folding during the construction of SSA form, the initial union-.nd \nsets would contain only values that 1 The seeming exception to this is a 4-node parameter, v, whose de.nition \nis in a block, bv , not necessarily dominating the block, bphi, in which v is used as a parameter. However, \nthe move\u00adment of the value from the variable to the 4-node actually takes place along the incoming edge \nto bphi, and this edge is dominated by bv . do not interfere. Indeed, this is precisely the algorithm \nused by a Chaitin/Briggs register allocator [5, 4, 1] to identify live ranges. The allocator joins all \n4-node names into a single set and then builds the aforementioned interference graph. It then coalesces \nlive ranges joined at a copy but that do not otherwise interfere. This is a classic pessimistic algorithm: \nall copies are assumed to be neces\u00adsary until proven otherwise. In contrast, the algorithm presented \nhere is optimistic: we as\u00adsume that every copy is unnecessary, and then we reinsert those that we cannot \nprove are unnecessary. The register allocator does not perform copy folding during SSA construction because \nsome of those copies really are necessary they move a value from one storage location to another, usually \nto free up the .rst location for another value. When we fold copies during SSA construction, we, in effect, \ntransfer the information about the exchange into a 4-node, where we may later recover the move by inserting \na new copy. The algorithm has four steps. First, we union together all 4-node parameters (and the name \nof the 4-node itself). This gives us an initial guess at what are, essentially, live-range names. The \nsec\u00adond step of the algorithm compares the set members against each other, looking for interferences, \nwhich, if found, cause one of the members to be split into a new set, necessitating insertion of copies \nfrom that member to members of the .rst. Then a unique name is given to each set and, .nally, the code \nis rewritten with all neces\u00adsary copies. Notice that copies are not actually inserted until the .nal \nstep, for reasons described in Section 3.6. Instead, we maintain an ar\u00adray, W aiting, indexed by block \nname, where each entry is a list of pending copy insertions for the end of that block. When one of the \nearlier stages discovers a copy that needs to be inserted into some block, b, we add the copy to W aiting[b]. \nAlso, because 4-node pa\u00adrameters .ow along edges, we use the notation F rom(x) to specify the block out \nof which the value in x .ows. 3.1 Building Initial Live Ranges The .rst step to coalescing copies and \nbuilding live ranges is to union together the 4-node parameters. As we explained earlier, some of these \nunions will include names of variables that interfere, and one of each of these pairs will have to be \nremoved from the set. While we were developing the implementation of our algorithm, we found that some \n.ltering during the building of the unions could save time and give us fewer copies. While unioning names, \nwe de\u00adtect interferences between just two names (the 4-node and the cur\u00adrent parameter). Speci.cally, \nwe use liveness information to detect that the SSA construction erroneously folded a copy. The folding \nwas in error because both variables are live at some point in the code. In general, only a single copy \nis needed to break the in\u00adterference. On the other hand, if we wait until later, each of those names \nmay interfere with many of the other names in the union, ne\u00adcessitating many more copies to ensure breaking \nthe interference. Thus, given a 4-node, p, de.ned in block b, with parameters a1 through an, we apply \nthe following simple tests of interference. These .ve are not exhaustive, but they handle the simple \ncases any interference found will cause a copy to be inserted; other\u00adwise, ai is added to the union. \n If ai is in the live-in set of b, add a copy from ai to p in W aiting[F rom(ai)]. Note that our liveness \nanalysis distin\u00adguishes between values that .ow into b s 4-nodes and values that .ow directly to some \nother use in b or b s successors. Only in the latter case will ai be in b s live-in set.  If p is in \nthe live-out set of ai s de.ning block, add a copy to W aiting[F rom(ai)].  If ai is de.ned by a 4-node \nand p is in the live-in set of the block de.ning ai, add a copy to W aiting[F rom(ai)].  CurrentP arent \nis maintained, which holds the reference to the inputs: IN (dominator tree DT ) root of the subtree currently \nin construction. There is an edge be\u00ad (set of variables S) tween CurrentP arent and the current variable \nv if pn(v) is less outputs: OUT (dominance forest DF ) for depth-.rst order over dominator tree nodes \nb than or equal to the the largest preorder number of CurrentP arent, preorder(b)= the next preorder \nname maxpreorder(b)= the largest preorder number of b s descendants Take S in dominator order maxpreorder(V \nirtualRoot)= MAX CurrentP arent = V irtualRoot stack.P ush(V irtualRoot) for all the variables v in S \nin sorted order while preorder(v) > maxpreorder(CurentP arent) stack.P op() CurrentP arent = stack.T \nop() make v a child of the CurrentP arent stack.P ush(v) CurrentP arent = v Remove V irtualRoot from \nDF Figure 1: Constructing the dominance-forest If ai has already been added to another set of names \nbe\u00adcause of another 4-node in the current block, add a copy to W aiting[F rom(ai)].  If ai and aj are \nde.ned in the same block, then add a copy to either W aiting[F rom(ai)] or W aiting[F rom(aj)].   3.2 \nThe Dominance Forest At the end of the .rst step, we have disjoint sets of names that may share the same \nname. We now need to discover members of the set that interfere with other members of the same set. A \ncritical part of our algorithm is the engineering required to perform the second step ef.ciently. It \nwould be prohibitively ex\u00adpensive to do a pairwise comparison of all the members in the set. Instead, \nwe have developed a new data structure, called a domi\u00adnance forest, which allows us to perform a linear \ncomparison of set members by ordering them according to dominance information. DEFINITION 3.1. Let S \nbe a set of SSA variables in a regular SSA program such that no two variables in S are de.ned in the \nsame block. Let > be a strict dominance relation. Let vi be a variable in S, and Bi the block in which \nvi is de.ned. Dominanceforest DF (S) is a graph in which the nodes are the blocks Bi such that vi E S, \nand there is an edge from Bi to Bj if and only if Bi >Bj , and(vk E S),vi = vk vj , such that Bi >Bk \n>Bj . = Succinctly, the dominance forest is a mapping of SSA variables to the basic blocks that contain \ntheir de.nition points, with the edges representing collapsed dominator-tree paths. We use Lemma 3.1 \nto show that we need only check edges in the dominance forest for interferences. We will show in the \nnext section how to use the dominance forest. Figure 1 shows the pseudo code for dominance-forest construc\u00adtion. \nThis algorithm starts by adding a V irtualRoot to the result to simplify the construction process. (We \nremove the V irtualRoot at the end, which may create a forest.) In a depth-.rst traversal of the dominator \ntree, we label all nodes in the tree with their preorder sequence of traversal. On the way up in the \ntraversal, we also com\u00adpute the maximum preorder number of the descendants for each node. This number \nallows the algorithm to identify the antecedent\u00addescendent information from the dominator tree in constant \ntime and is due to Tarjan[11]. This preorder-numbering process is done only once for the whole SSA. The \nalgorithm iterates over the variables in order of increas\u00ading preorder number, or pn. Within the loop, \na variable named which means that CurrentP arent dominates v. Traversing the variables in increasing \norder of pn(v) ensures that no edges are inserted prematurely (if a dominates b, which dominates c, the \nre\u00adlationship will be: pn(a) < pn(b) < pn(c), which will ensure that only the edges (a, b) and (b, c) \nare inserted and not the edge (a, c)). 3.3 Walk the Forests We built the dominance forests to reduce \nthe number of inter\u00adferences for which we have to check. We can do this using the following Lemma: LEMMA \n3.1. Given variable vi de.ned in block bi and vari\u00adable vj de.ned in block bj , if bj is a child of bi \nin the dominance forest and vi does not interfere with vj , then vi cannot interfere with any of the \nvariables represented by bj s descendants in the dominance forest. Informally, the proof of this Lemma \ncomes from the fact that if vi interferes with any of the vj s descendents for example vk, then vi must \ninterfere with vj , since it must be live at the de.nition point of vk , thus it must be live at the \nde.nition of vj . Lemma 3.1, in essence, prunes the pairwise search space of names in a set. That is, \nit shows that when we map the names of some set onto its dominance forest, each variable, v, needs to \nbe compared only to those variables de.ned in blocks immediately descendant of the block de.ning v. Figure \n2 contains the algorithm for .nding and resolving inter\u00adferences within a dominance forest. The dominance \nforest is tra\u00adversed depth .rst. If a variable, v, interferes with any other variable in the dominance \nforest, v must interfere with one of its children. Because variables are checked for interference only \nagainst their children in the dominance forest, any parent variable that interferes with its child forces \nus to insert all of the copies necessary to break any interferences between the two. This has the effect \nof separat\u00ading the variable from any of its more distant descendants it might interfere with. 3.4 Local \nInterferences Up to this point, only liveness information at block boundaries has been considered. However, \nthere are situations where two vari\u00adables do not interfere at any block boundaries but are nonetheless \nlive in the same block. This happens when one of the variables, v1 , is live coming into the block where \nthe second variable, v2 , is de\u00ad.ned. In this case, we need to ensure that v1 s last use occurs be\u00adfore \nv2 is de.ned, which requires a walk through the block to de\u00adtermine. Our algorithm keeps a list of variable \npairs that need to be checked for local interference. After traversing all of the domi\u00adnance forests \nand before inserting actual copies, our algorithm tra\u00adverses each block backwards to .nd and break any \nof these local interferences.  3.5 Renaming Variables Once copies have been inserted, those variables \nthat are still con\u00adnected by 4-nodes need to be given a single unique name. Naive it\u00aderation over 4-nodes \nand renaming the parameters to a single name is not suf.cient. If a variable were a parameter to multiple \n4-nodes, the variable would be renamed multiple times, resulting in incorrect code. The solution is to \niterate over all those variables that are can\u00addidates for renaming (those that were a part of one of \nthe union\u00ad.nd sets and are not on the list of variables to remove from the for depth-.rst traversal \nof DFi if variable p is c s parent and is in the live - out set of c s de.ning block if p can not interfere \nwith any of its other children and c has fewer copies to insert than p Insert copies for c and make c \ns children p s children else insert copies for p else if parent p is in the live - in set of c s de.ning \nblock or p and c have the same de.ning block Add the variable pair (p, c) to the list to check for local \ninterference later Figure 2: Finding and resolving interferences a1 =1) a=1) a1=1) b1 =2) b1 = 2) \nb= 2)  if p then if p then if p then x2=a1) x2=b1) x=a) x=b)  y2=b1) y2=a1) y=b) y=a) )) x2 = f(a1,b1)) \ny2 = f(b1,a1)  return x2/y2 return x/y return x2/y2 a. Original Code b. SSA Form With c. f-nodes Replaced \nWith Copies Copies Folded Figure 3: The virtual swap problem a1=1 a1=1 a1=1 b1 =2 b1 = 2  b1 =2 if \np then if p then if p then  x2=a1 y2=a1 b1=a1 y2=a1 y2=b1 y2=a1  b1 = a1   x2 = f(x2,b1) b1 = f(b1,b1) \nb1 = f(b1,b1) y2 = f(b1,y2) y2 = f(b1,y2) y2 = f(y2,y2) return x2/y2 return b1/y2 return b1/y2 a. Inserted \nCopies for a1 b. x2 Folded Into b1 c. Inserted Copy For y2 Figure 4: Inserting copies for the virtual \nswap problem set by inserting copies) and rename them to a single unique name. The F ind function will \nreturn a single unique name per set, and that name can be used for renaming all members of that set. \n  3.6 Correct Copy Insertion We have to be careful when inserting copies for 4-nodes, be\u00adcause naive \ncopy insertion may produce incorrect code. Two such problems, the lost copy problem, and the swap problem, \nare described in detail by Briggs, et al. [2]. We avoid the lost copy problem by splitting critical edges \nafter we have read in the code. The swap problem is addressed as in Briggs, et al. by careful ordering \nof copies with temporaries inserted to break any cycles. This is precisely the reason that we use the \nW aiting array to store pending copies; as Briggs, et al. show, the ordering of the full set of copies \nto be inserted is critical to correctness. The virtual swap problem is a case similar to the swap prob\u00adlem, \nbut has to be addressed with special attention when attempting to produce the minimal number of copies. \nIn the virtual swap problem, two variables are de.ned by copies on either side of a conditional, and \nthey take opposite values on opposing sides. 3.6.1 The Virtual Swap Problem Figure 4 illustrates an example \nwhere the naive insertion of copies still produces correct code. However, a careful implementation is \nneeded when the copies are inserted into the SSA in our algorithm. The naive algorithm inserts all copies \nfor the 4-nodes, while our algorithm attempts to insert as few copies as possible. When ana\u00adlyzing the \n4-nodes, the algorithm determines that the variables a1 and b1 are simultaneously live at the end of \nthe .rst block and can\u00adnot be folded together. The algorithm then picks one of them and inserts copies \nfor it. On the left of the Figure 4 we show the exam\u00adple from Figure 3 with a1 being picked for copy \ninsertion. After the copies have been inserted, the last pass of the algorithm scans through the SSA \nand renames the variables as needed. This is the point where some additional interferences can be identi.ed \nand some additional copies needed. In Figure 4b, all appearances of x2 have been replaced with b1. This \nexposes an interference between the .rst and the second 4-nodes, which forces insertion of a copy on \nFigure 4 c.  3.7 Algorithmic Complexity The dominance-forest construction algorithm is linear in the \nsize of the join set. It starts with a depth-.rst traversal of the dominator tree, which is linear in \nsize of the dominator tree, but it is done only once for the whole SSA. It then uses the radix sort [7] \nto sort the variables in the set, which is linear as well since the number of variables in the join set \ncannot be greater than the number of basic blocks in the CFG. Each of the variables in the set is visited \nexactly once in the loop. So the complexity of the dominance forest construction algorithm is O(|S|). \nThe copy insertion algorithm begins with constructing join sets for variables in the 4-nodes of the graph. \nThis can be done in O(na(n)) time using the union-.nd algorithm [7], where n is the number of variables \nin 4-nodes, and a is the inverse Ackermann s function. The algorithm then constructs the dominance forests \nfor these sets (which are disjoint), which is linear in the total number of variables in 4-nodes. For \neach dominance forest, the algorithm visits all the edges, which is linear in the number of nodes in \nthe forest. At the end, all 4-nodes are visited and the variables that are the arguments of the 4-nodes \nare renamed into a single variable name. The total complexity of this algorithm is O(na(n)), where n \nis the total number of arguments in all the 4-nodes in the SSA. Since the inverse of Ackermann s function \nis practically a con\u00adstant, one cannot hope to achieve better algorithmic complexity than what is presented \nhere, since all the 4-nodes and all of their arguments have to be visited at least once in the SSA-to-CFG \ncon\u00adversion.  4. EXPERIMENTS In this section, we present experimental evidence to show that this algorithm \nis both effective and ef.cient. We compare our al\u00adgorithm against two versions of the interference-graph \ncoalescer. One is simply a coalescing phase stripped from our implementation of a Chaitin/Briggs register \nallocator. The second is an improved version, made possible by an insight into building the interference \ngraph, that is signi.cantly faster than the original, but equally pre\u00adcise. The test suite we used in \nour experiments is made up of 169 routines that come from Forsythe, et al. s book on numerical meth\u00adods[9], \nas well as the Spec and Spec 95 libraries. We ran these codes on a minimally loaded 300 MHz Sparc Ultra \n10 with 256 megabytes of memory. Due to space constraints, we only report on the ten largest results \nin each experiment. We took the ten programs that took longest to compile using the standard SSA-to-CFG \ncon\u00adversion for Table 2 and Table 3 and the ten programs with the most dynamic copies for Table 4 and \nTable 5. However, we think that these give a reasonable insight into the behavior of the algorithms. \nWe use the following nomenclature to distinguish the algorithms: Briggs the Chaitin/Briggs interference-graph \ncoalescer  Briggs* the improved interference-graph coalescer, below  Standard the Briggs, et al. \n4-node-instantiation algorithm that does not attempt to eliminate any copies  New the algorithm presented \nherein.  4.1 Engineering the Interference-Graph Coalescer During the development of the experiments \nto test this new algo\u00adrithm, we discovered a simple oversight in building the interference graph as described \nby Briggs in his dissertation. Briggs algorithm requires four steps. First, the code is converted to \nSSA form. Names are then joined to form live ranges by unioning 4-node parameters. The third step is \na loop in which the interference graph is built and then copies whose source and destination do not interfere \nhave those live ranges coalesced. Because the interference graph is not exact, it needs to be rebuilt \nafter all of the coalescible copies have been identi.ed and those live ranges have been unioned together. \nThis can expose additional opportunities for coalescing, so these two steps are iterated until all opportunities \nhave been found. For these experiments, the .nal step is to rewrite the code to re.ect the namespace \ndescribed by the live ranges. This algorithm is simple and powerful, but it is very expensive. The interference \ngraph is modeled as a triangular bit matrix, with as many rows or columns as there are live-range names. \nThis data structure requires n 2 /2 bits that have to be cleared. As Cooper, et al. showed [6], this \nis a considerable part of the overall running time of a graph-coloring register allocator. The .aw in \nthis algorithm is that it builds an interference graph that includes the full set of live-range names. \nThe reason for this is that if the coalescing phase does not fold any copies, the inter\u00adference graph \nis correct, and the remaining phases of the allocator then use it. However, if the coalescing phase unions \nany live ranges,  Algorithm First Pass Second Pass Briggs. Briggs* Briggs Briggs* Briggs Briggs \n File Briggs  Memory Usage (in bytes) Briggs* .eldX 3.10 1.55 0.53 2120664 83521 1553762 1139 parmvrX \n2.53 1.26 0.53 1968409 8281 1800293 380 parmovX 2.26 1.11 0.51 1701720 7140 1556880 324 twldrv 2.20 \n 0.49 0.23 598689 24806 426735 715 fpppp 1.28 0.03 0.02 950137 0 radfgX 1.25 0.73 0.58 446224 90300 \n215296 4900 radbgX 1.22 0.67 0.56 405132 80940 200928 5005 parmveX 0.95 0.46 0.51 453939 4556 395012 \n182 jacld 0.57 0.15 0.26 82225 8695 50512 0 smoothX 0.46 0.25 0.52 175561 23562 104976 1640    \n AVERAGE 1.58 0.67 0.42 Table 1: Time (in seconds) and memory (in bytes) comparison for the interference-graph \ncoalescers the interference graph has to be rebuilt, and in our experiments, we rarely saw input that \ndid not require at least two iterations (and few that required more). While the build/coalesce loop, \nas Briggs calls it, is iterating, the interference graph should only be built using live-range names \nthat are involved in copies. To maintain a com\u00adpact namespace, this requires an extra mapping array, \nbut the cost of accessing the array each time a name is examined is offset by the considerable decrease \nin the size of the interference-graph bit matrix. Table 1 shows the difference in memory used to build \nthe interference graph. Clearly, the savings in memory usage is im\u00admense, and the time required to perform \ncoalescing is, on average, less than half the time with our improved algorithm. To our knowledge, this \nis the .rst publication of this insight. Cer\u00adtainly, comparison of our SSA-coalescing algorithm has to \nbe made against the best known coalescer (which, in this case, is the Briggs algorithm improved by using \nthe insights described above), which these experiments do.  4.2 Running Time Table 2 shows the running \ntime of the three algorithms, the orig\u00adinal SSA 4-node replacement algorithm presented in Briggs, et \nal., the algorithm presented in this paper, and the improved coalescer from our register allocator. The \ntimer was started immediately be\u00adfore building SSA form, and its value is recorded immediately after \nthe code is rewritten in its various forms by the different algorithms. Clearly, the additional analysis \nto restrict the number of copies is more expensive than the universal copy-insertion algorithm. How\u00adever, \nour algorithm is considerably faster than the interference\u00adgraph coalescer. Table 3 shows the maximum \namount of memory used by the three algorithms during compilation. Over the full test suite, our algorithm \nuses, on average, 40% more memory than the standard 4-node-replacement algorithm. It uses only 21% more \nmemory than the improved interference-graph coalescer, which itself uses substantially less memory than \nthe previous state of the art. This table and Table 2 show that memory usage alone is not the only determinant \nof the compiler s total running time. 4.3 Ef.cacy Measurements In Table 4, we show the number of copy \noperations that were executed. Our algorithm produces code that executes about 1% fewer copies, on average, \nthan the interference-graph coalescer. The interference-graph coalescer tries to remove copies out of \nin\u00adnermost loops .rst, on the theory that these are the most pro.table to remove. This heuristic sometimes \nfails, as in the case of initX, but it also sometimes wins; on some of the codes in the test suite, our \nalgorithm results in code that executes up to two-thirds again as many copy operations as the code produced \nby the interference\u00adgraph coalescer. In Table 5, we show a static measurement of copies left in the code \nby the three algorithms. On average, our algorithm leaves in approximately three percent more static \ncopies than the interference\u00adgraph coalescer, but as with dynamic copies, the results vary signif\u00adicantly. \nAgain, we believe this re.ects the heuristic nature of both algorithms.  5. CONCLUSIONS AND FUTURE \nWORK In this paper, we give a theoretical background that enables fast computation of interference information \nand present a practical, ef\u00ad.cient algorithm to perform copy folding without building an inter\u00adference \ngraph. The applications of this algorithm are many. It can be used as a standalone pass of an optimizer. \nIt can replace the cur\u00adrent copy-insertion phase of an optimizer s SSA implementation. Finally, it can \nreplace the coalescing phase of a Chaitin/Briggs reg\u00adister allocator. Our experiments show that this \nalgorithm provides signi.cant improvements in running time over an interference-graph-based al\u00adgorithm, \nwhile maintaining comparable precision. These results make this algorithm very attractive for use in \nany system, includ\u00ading, perhaps, systems in which compile time is a critical concern, such as JIT compilers. \nWe have also presented an implementation insight concerning building an interference graph for copy coalescing \nthat can be triv\u00adially inserted into existing graph-coloring register allocators to make them run much \nfaster. Our plan for future research includes design and implementa\u00adtion of a fast register-allocation \nalgorithm that uses the results pre\u00adsented in this paper. We will also consider implementation of sev\u00aderal \nheuristics to improve the precision of this algorithm without sacri.cing the compilation time.  6. ACKNOWLEDGMENTS \nThe authors would like to thank the many people who supported this work, including members of the MSCP, \nespecially Linda Tor\u00adczon, as well as John Mellor-Crummey, both of whom provided many useful suggestions. \nThe authors would like to thank Preston Briggs for his contributions. We are also indebted to Compaq, \nLos Alamos Computer Science Institute, and the GrADS NSF project (grant #9975020) for their support of \nthis work. File Standard New Briggs* New Standard New Briggs.  parmvrX 0.17 0.38 1.26 2.24 0.30 .eldX \n0.17 0.48 1.55 2.82 0.31 parmovX 0.15 0.30 1.11 2.00 0.27 radfgX 0.10 0.24 0.73 2.40 0.33 radbgX 0.10 \n0.22 0.67 2.20 0.33 twldrv 0.07 0.16 0.49 2.29 0.33 parmveX 0.06 0.13 0.46 2.17 0.28 initX 0.06 0.11 \n0.26 1.83 0.42 advbndX 0.04 0.07 0.15 1.75 0.47 deseco 0.03 0.07 0.20 2.33 0.35 AVERAGE 0.10 0.22 0.69 \n2.20 0.32 Table 2: Comparison of compilation times (in seconds) File Standard New Briggs* New Standard \nNew Briggs.  parmvrX 4214880 5365964 5126100 1.27 1.05 .eldX 3144244 5062808 3513180 1.61 1.44 parmovX \n3740196 4785972 4562856 1.28 1.05 radfgX 1594768 2379156 1785964 1.49 1.33 radbgX 1492484 2250356 1695244 \n1.51 1.33 twldrv 1496208 1805984 1613080 1.21 1.12 parmveX 1734400 2204940 2081044 1.27 1.06 initX 1456780 \n1814224 1680640 1.25 1.08 advbndX 1009844 1253616 1130072 1.24 1.11 deseco 614496 954356 718856 1.55 \n1.33 AVERAGE 2049830 2787738 2390704 1.36 1.17 Table 3: Comparison of compiler memory usage (in bytes) \nFile Standard New Briggs* New Standard New Briggs.  tomcatv 23572565 64571 64571 0.00 1.00 blts 17222000 \n 3716100 3666100 0.22 1.01 buts 17111000 4055500 4050500 0.24 1.00 getbX 8197640 63520 63520 0.01 1.00 \ntwldrv 5244414 215761 217777 0.04 0.99 smoothX 4213120 599920 467440 0.14 1.28 rhs 3509310 414324 \n414324 0.12 1.00 parmvrX 2218540 43610 42630 0.02 1.02 saxpy 2000000 20000 20000 0.01 1.00 initX 1755598 \n 92810 390455 0.05 0.24    Table 4: Comparison of dynamic copies executed File Standard New Briggs* \nNew Standard New Briggs.  tomcatv 112 47 47 0.42 1.00 blts 82 36 31 0.44 1.16 buts 72 33 31 0.46 1.06 \ngetbX 176 28 28 0.16 1.00 twldrv 976 164 169 0.17 0.97 smoothX 668 283 215 0.42 1.32 rhs 678 32 27 0.05 \n1.19 parmvrX 352 101 101 0.29 1.00 saxpy 4 1 1 0.25 1.00 initX 458 106 118 0.23 0.90   Table 5: Comparison \nof static number of copies 7 7. REFERENCES [1] Preston Briggs. Register Allocation via Graph Coloring. \nPhD thesis, Rice University, April 1992. [2] Preston Briggs, Keith D. Cooper, Timothy J. Harvey, and \nL. Taylor Simpson. Practical improvements to the construction and destruction of static single assignment \nform. Software Practice and Experience, 28(8):859 881, July 1998. [3] Zoran Budimli\u00b4c. Compiling Java \nfor High Performance and the Internet. PhD thesis, Rice University, January 2001. [4] Gregory J. Chaitin. \nRegister allocation and spilling via graph coloring. SIGPLAN Notices, 17(6):98 105, June 1982. Proceedings \nof the ACM SIGPLAN 82 Symposium on Compiler Construction. [5] Gregory J. Chaitin, Marc A. Auslander, \nAshok K. Chandra, John Cocke, Martin E. Hopkins, and Peter W. Markstein. Register allocation via coloring. \nComputer Languages, 6:47 57, January 1981. [6] Keith D. Cooper, Timothy J. Harvey, and Linda Torczon. \nHow to build an interference graph. Software Practice and Experience, 28(4):425 444, April 1998. [7] \nThomas H. Cormen, Charles E. Leiserson, and Ronald L. Rivest. Introduction to Algorithms. M.I.T. Press, \nCambridge, Massachusetts, U.S.A., 1990. [8] Ron Cytron, Jeanne Ferrante, Barry K. Rosen, Mark N. Wegman, \nand F. Kenneth Zadeck. Ef.ciently computing static single assignment form and the control dependence \ngraph. ACM Transactions on Programming Languages and Systems, 13(4):451 490, October 1991. [9] George \nE. Forsythe, Michael A. Malcolm, and Cleve B. Moler. Computer Methods for Mathematical Computations. \nPrentice-Hall, Englewood Cliffs, New Jersey, 1977. [10] The Java Hotspot Virtual Machine, Technical White \nPaper, April 2001. [11] Robert Endre Tarjan. Testing .ow graph reducibility. Journal of Computer and \nSystem Sciences, 9:355 365, 1974.  \n\t\t\t", "proc_id": "512529", "abstract": "This paper presents a fast new algorithm for modeling and reasoning about interferences for variables in a program without constructing an interference graph. It then describes how to use this information to minimize copy insertion for &fgr;-node instantiation during the conversion of the static single assignment (SSA) form into the control-flow graph (CFG), effectively yielding a new, very fast copy coalescing and live-range identification algorithm.This paper proves some properties of the SSA form that enable construction of data structures to compute interference information for variables that are considered for folding. The asymptotic complexity of our SSA-to-CFG conversion algorithm is where-is the number of instructions in the program.Performing copy folding during the SSA-to-CFG conversion eliminates the need for a separate coalescing phase while simplifying the intermediate code. This may make graph-coloring register allocation more practical in just in time (JIT) and other time-critical compilers For example, Sun's Hotspot Server Compiler already employs a graph-coloring register allocator[10].This paper also presents an improvement to the classical interference-graph based coalescing optimization that shows adecrease in memory usage of up to three orders of magnitude and a decrease of a factor of two in compilation time, while providing the exact same results.We present experimental results that demonstrate that our algorithm is almost as precise (within one percent on average) as the improved interference-graph-based coalescing algorithm, while requiring three times less compilation time.", "authors": [{"name": "Zoran Budimlic", "author_profile_id": "81100227584", "affiliation": "Rice University, Houston, Texas", "person_id": "P348284", "email_address": "", "orcid_id": ""}, {"name": "Keith D. Cooper", "author_profile_id": "81100286556", "affiliation": "Rice University, Houston, Texas", "person_id": "P158954", "email_address": "", "orcid_id": ""}, {"name": "Timothy J. Harvey", "author_profile_id": "81100589538", "affiliation": "Rice University, Houston, Texas", "person_id": "P282656", "email_address": "", "orcid_id": ""}, {"name": "Ken Kennedy", "author_profile_id": "81100453545", "affiliation": "Rice University, Houston, Texas", "person_id": "PP40027435", "email_address": "", "orcid_id": ""}, {"name": "Timothy S. Oberg", "author_profile_id": "81100188765", "affiliation": "Rice University, Houston, Texas", "person_id": "P348280", "email_address": "", "orcid_id": ""}, {"name": "Steven W. Reeves", "author_profile_id": "81100228689", "affiliation": "Rice University, Houston, Texas", "person_id": "P348278", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/512529.512534", "year": "2002", "article_id": "512534", "conference": "PLDI", "title": "Fast copy coalescing and live-range identification", "url": "http://dl.acm.org/citation.cfm?id=512534"}