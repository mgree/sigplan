{"article_publication_date": "05-17-2002", "fulltext": "\n Ef.cient and Precise Datarace Detection for Multithreaded Object-Oriented Programs Jong-Deok Choi \nKeunwoo Lee Alexey Loginov IBM T. J. Watson Research Center Univ. of Washington Univ. of Wisconsin -Madison \njdchoi@us.ibm.com klee@cs.washington.edu alexey@cs.wisc.edu Robert O Callahan Vivek Sarkar Manu Sridharan \nIBM T. J. Watson Research Center IBM T. J. Watson Research Center MIT roca@us.ibm.com vsarkar@us.ibm.com \nmanu@alum.mit.edu ABSTRACT We present a novel approach to dynamic datarace detection for multithreaded \nobject-oriented programs. Past techniques for on\u00adthe-.y datarace detection either sacri.ced precision \nfor performance, leading to many false positive datarace reports, or maintained preci\u00adsion but incurred \nsigni.cant overheads in the range of 3.to 30.. In contrast, our approach results in very few false positives \nand runtime overhead in the 13% to 42% range, making it both ef.\u00adcient and precise. This performance \nimprovement is the result of a unique combination of complementary static and dynamic opti\u00admization techniques. \nCategories and Subject Descriptors D.1.3 [Programming Techniques]: Concurrent Programming; D.1.5 [Programming \nTechniques]: Object-oriented Programming; D.2.5 [Software Engineering]: Testing and Debugging General \nTerms Experimentation, Languages, Measurement, Performance  Keywords dataraces, race conditions, debugging, \nparallel programs, synchro\u00adnization, multithreaded programming, object-oriented programming, static-dynamic \nco-analysis 1. INTRODUCTION A datarace occurs in a multithreaded program when two threads access the \nsame memory location with no ordering constraints en\u00adforced between the accesses, such that at least \none of the accesses is a write. In most cases, a datarace is a programming error. Fur\u00adthermore, programs \ncontaining dataraces are notoriously dif.cult to debug because they can exhibit different functional \nbehaviors even when executed repeatedly with the same set of inputs and the same execution order of synchronization \noperations. Because of Permission to make digital or hard copies of all or part of this work for personal \nor classroom use is granted without fee provided that copies are not made or distributed for pro.t or \ncommercial advantage and that copies bear this notice and the full citation on the .rst page. To copy \notherwise, to republish, to post on servers or to redistribute to lists, requires prior speci.c permission \nand/or a fee. PLDI 02, June 17-19, 2002, Berlin, Germany. Copyright 2002 ACM 1-58113-463-0/02/0006 ...$5.00. \nthe detrimental effects of dataraces on the reliability and compre\u00adhensibility of multithreaded software, \nit is widely recognized that tools for automatic detection of dataraces can be extremely valu\u00adable. As \na result, there has been a substantial amount of past work in building tools for analysis and detection \nof dataraces [1, 13, 15, 17, 18, 21, 24, 27]. Most previous dynamic datarace detection techniques have \nbeen relatively precise, in that most races reported correspond to truly unsynchronized accesses to shared \nmemory. However, these de\u00adtectors incur order-of-magnitude overheads in the range of 3.to 30.[13, 18, \n17, 24]. Recent approaches reduce the overhead of datarace detection, but at the cost of decreased precision. \nFor example, monitoring dataraces at the object level rather than the memory-location level reduced overheads \nfor datarace detection to the range of 16% to 129% [21] but resulted in many spurious race reports (see \nSection 9 for details). This paper presents a novel approach to dynamic datarace de\u00adtection for multithreaded \nobject-oriented programs which is both ef.cient and precise. A key idea in our approach is the weaker\u00adthan \nrelation (Section 3), which is used to identify memory ac\u00adcesses that are provably redundant from the \nviewpoint of datarace detection. Another source of reduction in overhead is that our ap\u00adproach does not \nreport all access pairs that participate in dataraces, but instead guarantees that at least one access \nis reported for each distinct memory location involved in a datarace (see Section 2.5 for details). Our \napproach results in runtime overhead ranging from 13% to 42% which is well below the runtime overhead \nof previ\u00adous approaches with comparable precision. This performance is obtained through a combination \nof static and dynamic optimization techniques which complement each other in reducing the overhead of \nour detector. Furthermore, almost all the dataraces reported by our system correspond to actual bugs, \nand the precise output of our tool allowed us to easily .nd and understand the problematic source code \nlines in our test programs. Figure 1 shows the overall architecture of our approach. The .rst phase is \nan optional static datarace analysis which produces a static datarace set i.e., a (conservative) set \nof statements that are identi.ed as potentially participating in dataraces. Any statement that does not \nbelong to the static datarace set is guaranteed to never cause a datarace during execution. If this phase \nis omitted, then the static datarace set defaults to all statements that contain memory accesses. The \nsecond phase is instrumentation, whose goal is to insert trace statements at program points identi.ed \nin the static datarace set. This insertion process can be optimized, in which case no instru\u00admentation \nis inserted at redundant trace points, i.e., program points instrumented access events static datarace \nset executable dynamic datarace set  Program whose access events can be ignored since other (non-redundant) \ntrace points will provide suf.cient information for datarace detec\u00adtion. The result of the second phase \nis an executable that is ex\u00adtended with code to generate access events during program execu\u00adtion. The \nthird phase is an optional runtime optimizer, which uses a cache to identify and discard redundant access \nevents that do not contain new information. Finally, the runtime detector examines the access events \nand detects dataraces during the program execu\u00adtion. The instrumentation and runtime detector phases \nguarantee the precision of our approach, whereas the optimization phases deliver the ef.ciency that makes \nour approach practical. Our results show that it is necessary to combine all the optimization phases \n(static analysis, optimized instrumentation, and runtime optimizer) to ob\u00adtain maximum performance. Our \napproach contrasts with purely ahead-of-time (static) datarace detection, which attempts to report dataraces \nthat may occur in some possible program execution [1, 15, 27]. Instead, our approach detects dataraces \non-the-.y, usually the most convenient mode for the user. If so desired, our approach could be easily \nmodi.ed to perform post-mortem datarace detec\u00adtion by creating a log of access events during program \nexecution and performing the .nal datarace detection phase off-line. The rest of the paper is organized \nas follows. Section 2 de.nes the conditions under which a datarace may occur and summarizes the problem \nstatement addressed by this work. Section 3 describes the algorithm used by the runtime datarace detector. \nEven though it is the last phase in Figure 1, we describe the runtime datarace detector phase .rst because \nit is a mandatory phase and it provides necessary background for explaining the optimization phases. \nSec\u00adtion 4 presents the caching mechanism and key optimizations that the runtime optimizer uses to identify \nand delete redundant access events. Sections 5 and 6 respectively describe the static analysis and optimized \ninstrumentation phases shown in Figure 1. In Sec\u00adtion 7, we discuss our implementation of the ownership \nmodel [21] and its interaction with the weaker-than relation. Section 8 con\u00adtains our experimental results \nobtained by executing a set of multi\u00adthreaded Java programs on a prototype implementation of our ap\u00adproach. \nFinally, Section 9 describes related work, and Section 10 contains our conclusions.  2. DATARACE CONDITIONS \nAND PROB-LEM STATEMENT In this section, we .rst formalize the notion of dataraces, and give an example. \nWe then formalize the problem of dynamic datarace detection and describe the set of dataraces we guarantee \nto detect and report.   2.1 Datarace Conditions We de.ne a datarace as two memory accesses which satisfy \nthe following four conditions: (1) the two accesses are to the same memory location (i.e., the same .eld \nin the same object1 ) and at least one of the accesses is a write operation 2; (2) the two ac\u00adcesses \nare executed by different threads; (3) the two accesses are not guarded by a common synchronization object \n(lock); and (4) there is no execution ordering enforced between the two accesses, for example by thread \nstart or join operations. We call these conditions the datarace conditions, and observe that they are \ndif\u00adferent from datarace conditions assumed in past work on datarace detection for fork-join programs \n[1, 8]. In general, our approach is applicable to any monitor-style synchronization primitives sup\u00adported \nby the programming language, operating system, or user.  2.2 Example Figure 2 shows an example program \nwith three threads: main, T1, and T2. Statements are labeled with statement numbers such as T01, the \n.rst labeled statement in the main thread. We will also use the notation stmt:expr to denote a .eld access \nexpres\u00adsion within a statement. For convenience, statements that are not relevant to dataraces have been \nelided from this example. Note that thread mainperforms a write access on .eld x.fat statement T01, before \ncreating and starting threads T1 and T2. Thread T1 calls method foo which contains three accesses to \nobject .elds: a write access T11:a.f, a write access T14:b.g, and a read access T14:b.f. Thread T2 calls \nmethod bar which contains a write access, T21:d.f. Let us .rst assume that object references a, b, d, \nand x all point to the same object. All the accesses to the f .eld in the example will be to the same \nmemory location, thus every pair of them except for (T14:b.f, T14:b.f) satis.es the .rst of the datarace \nconditions. In addition, assume that object references T10:this, T13:p, and T20:q all point to different \nobjects during that execution. Then, no two statement instances belonging to different threads are guarded \nby the same synchronization object, satisfying the third of the datarace conditions. T1and T2are different \nthreads without ex\u00adecution ordering between them via start or join, satisfying the second and the fourth \nof the conditions. Accesses T11:a.f and T14:b.f thus exhibit a datarace with access T21:d.f. State\u00adment \nT01does not cause a datarace with the others in the example because there exists an ordering via start \nat T04 and T05, not satisfying the fourth of the conditions. Our de.nition of dataraces identi.es both \nactual and feasible dataraces [20] in a given program execution. This is different from 1We associate \nonly one memory location with all elements of a given array. 2Undercertain memory models, two read accessesmay \nalso gener\u00adate a datarace. The framework presented in this paper can be easily applied to such models \nby dropping the requirement that at least one of the accesses must be a write. // THREAD MAIN // CALLED \nBY THREAD T1 class MainThread { T10: synchronized void foo(...) { ... T11: a.f=50; public static void \nmain(String args[]) { T12: . . . . . . T13: synchronized(p) { T01: x.f = 100; T14: b.g = b.f; ... } \nT02: Thread T1 = new ChildThread(...); } T03: Thread T2 = new ChildThread(...); T04: T1.start(); // CALLED \nBY THREAD T2 T05: T2.start(); void bar(...) { . . . T20: synchronized(q) { } T21: d.f = 10; } // class \nMainThread } } Figure 2: Example Program with Three Threads. datarace de.nitions (as in [13]) that model \nmutual exclusion using the happened-before relation, and exclude feasible dataraces from their de.nition. \nFor example, let us now assume that T13:p and T20:q point to the same object (which is different from \nthe ob\u00adject pointed to by T10:this). Therefore the two synchronized blocks in methods fooand barare protected \nby the same lock. If thread T1 acquires the lock before T2, an approach based on the happened-before \nrelation will record the fact that statement T13 must execute before statement T20. Doing so will lead \nit to con\u00adclude that there is a happened-before relation from T11 to T21 (through T13), and that there \nis no datarace between T11:a.fand T21:d.f. In contrast, our approach reports the feasible datarace between \nT11:a.f and T21:d.f since it could have occurred if thread T2 acquired the lock before thread T1. In \nthis regard, our de.nition of dataraces is similar to that of Eraser [24] (a more de\u00adtailed comparison \nwith the Eraser approach appears later in Sec\u00adtions 8.3 and 9).  2.3 Thread Start and Join Operations \nAs the third and the fourth of the datarace conditions indicate, there are two kinds of inter-thread \nserialization constructs that can be used to avoid dataraces mutual exclusion (synchronized methods \nand blocks) and happened-before relations (thread start and join operations). In this section, we brie.y \ndiscuss how startand joinoperations can be handled by a detector based on mutual exclusion, using some \napproximations. The rest of the pa\u00adper will then present our approach to datarace detection by focusing \non mutual exclusion as the sole inter-thread serialization construct. To precisely model a join operation \nusing mutual exclusion, we introduce a dummy synchronization object Sjfor each thread Tj. The Sjlocks \nare used solely for the purpose of datarace de\u00adtection, and are not visible to the application. A dummy \nmon\u00adenter(Sj)operation is performed at the start of Tj s execution, and a mon-exit(Sj)operation is performed \nat its end. When thread Tj s parent or any other thread performs a join operation on Tj,a dummy mon-enter(Sj)operation \nis performed in that thread af\u00adter the join completes. These dummy synchronizations help the datarace \ndetection system observe that the operations following the join cannot execute concurrently with operations \nin Tj. It is dif.cult to model start constraints the same way, because generally one cannot know in advance \nhow many threads will be started by each thread, or which dummy locks should be held prior to starting \nchild threads. Instead, we use an ownership model to approximate the ordering constraints that arise \nfrom start oper\u00adations. As in [21], we de.ne the owner of a location to be the .rst thread that accesses \nthe location. We only start recording data ac\u00adcesses and checking for dataraces on a location when the \nlocation is accessed by some thread other than its owner. Though approxi\u00admate, this approach is suf.cient \nto capture the ordering constraints that arise in the common case when one thread initializes some data \nthat is later accessed by a child thread without explicit locking. (Further details on our use of the \nownership model are provided in Section 7.) 2.4 Datarace Detection We formally de.ne datarace detection \nas follows. An access event eis a 5-tuple (m,t,L,a,s)where mis the identity of the logical memory location \nbeing ac\u00adcessed,  tis the identity of the thread which performs the access,  Lis the set of locks held \nby tat the time of the access,  ais the access type (one of {WRITE, READ }) and  sis the source location \nof the access instruction.  Note that source location information is used only in reporting and has \nno bearing on our other de.nitions and optimizations. Given access events (or, simply, accesses) eiand \nej, we de.ne IsRae(ei,ej)as follows: IsRae(ei,j) (ei.m=ej.m)/(ei.t e.=ej.t)/ (ei.Lnej.L=0)/(ei.a=WRITE \nVej.a=WRITE). A program execution generates a sequence of access events E. Performing datarace detection \non this execution is equivalent to computing the value of the condition: :ei,ejEElIsRae(ei,ej). This \nde.nition does not capture the ownership model described above. Discussion of the ownership model and \nits effect on our design and implementation is deferred to Section 7. 2.5 Dataraces Reported Let FullRae={(ei,ej)}be \nthe set of all access pairs that form a datarace during an execution. Given an execution with Naccesses, \nany algorithm which attempts to detect all pairs in FullRaemust have worst-case time and space complexity \nO(N2) (since all possible pairs could be in FullRae), costs that could be prohibitive for a large sequence \nof accesses. To avoid these costs, our detection algorithm does not guarantee enumeration of all pairs \nin FullRae, although it still performs datarace detection as pre\u00adviously de.ned. For each memory location \nminvolved in a datarace, our detec\u00adtion algorithm reports at least one access event participating in \na datarace on m. More formally, consider a partitioning of FullRae by memory location into MemRaesets: \nMemRae(mk)={(ei,ej)EFullRaelei.m=ej.m=mk} We use boolean predicate IsRaeOn(ei,m)to indicate whether the \nevent eiis inapairin MemRae(m): IsRaeOn(ei,m) :ej.(ei,ej)EMemRae(m). We now de.ne the set of dataraces \nreported by our approach: De.nition 1. For each mwith non-empty MemRae(m), our dynamic datarace detector \ndetects and reports at least one access event esuch that IsRaeOn(e,m)=true. 2.6 Debugging Support We \nreport a racing access eat the moment it occurs in the pro\u00adgram, and therefore the program can be suspended \nand its current state examined to aid in debugging the race. Our algorithm also re\u00adports, for some previous \naccess fwith IsRae(e,f), f s lockset, and often f s thread3. (see below). Furthermore, our static datarace \nanalyzer discussed in Section 5 can provide a (usually small) set of source locations whose execution \ncould potentially race with e. In our experience this information, combined with study of the source \ncode, has been enough to identify the causes of dataraces. To obtain full information about rarely occurring \ndataraces, a program record and replay tool such as DejaVu [9] can be used, where the dynamic detection \nruns along with DejaVu recording and the expensive reconstruction of FullRaeoccurs during DejaVu replay. \nDejaVu recording incurs approximately 30% time over\u00adhead.  3. RUNTIME DATARACE DETECTION In this section, \nwe describe our algorithm for dynamic datarace detection. Since we do not need to report all races in \na given pro\u00adgram execution, we use two key techniques to decrease the cost of our algorithm. Our use \nof the weaker-than relation allows us to decrease the number of accesses we need to consider and save, \nand our representation of the access event history using tries [16] allows us to ef.ciently represent \nand search past accesses. 3.1 The Weaker-Than Relation Given two past access events eiand ej, if for \nevery future ac\u00adcess ek, IsRae(ej,ek)implies IsRae(ei,ek), ejneed not be considered when performing datarace \ndetection on future accesses. Since eiis more weakly protected from dataraces than ej(or pro\u00adtected equally), \nwe say that eiis weaker than ej(or ejis stronger than ei). Exploiting the weaker-than relationship between \naccesses allows us to greatly reduce the overhead of our datarace detection algorithm. We now outline \na suf.cient condition for dynamically determin\u00ading that event eiis weaker-than event ej, by using the \nmemory location, access type, thread, and lockset information contained in each event. We add the pseudothread \nt.to the possible values 3See Section 3.1 for an explanation of why the speci.c threads cannot always \nbe reported in our approach. of e.tfor a past access event estored by our detector. t.means at least \ntwo distinct threads, and we set ei.tto t.when we en\u00adcounter some later event ejsuch that ei.m=ej.m, \nei.L=ej.L, and ei.t=ej.t. The intuition behind t.is that once two different threads access a memory location \nwith the same lockset, any fu\u00adture access to that memory location with a non-intersecting lockset will \nbe a datarace (unless all accesses are reads), independent of which threads previously accessed the location. \nUtilizing t.is a space optimization that simpli.es our implementation, but it is also the reason why \nwe cannot always report the speci.c thread for the earlier access in a datarace. We de.ne a partial order \ncbetween two threads tiand tj, and between two access types aiand aj, as follows: tictj ti =tjVti =t. \naicaj ai =ajVai =WRITE. Given these orderings, we can now de.ne the weaker-than partial order cfor accesses: \nDe.nition 2. For access events pand q, pcqp.m=q.m/p.L.q.L /p.tcq.t/p.acq.a. THEOREM 1(WEAKER-THAN). For \npast accessespand qand for all future accesses r, pcq=(IsRae(q,r)= IsRae(p,r)). PROOF. First, p.m=q.mand \nq.m=r.mimplies p.m=r.m. Second, p.L.q.Land q.Lnr.L=0implies p.Lnr.L=0. Third, p.tcq.timplies that p.t=t.or \np.t=q.t.In either case, p.t=r.tsince q.t=r.t.(A new access rcannot have r.t=t..) Finally, p.acq.aimplies \np.a=WRITE or p.a=q.a.If p.a= q.a=WRITE,r.amust be WRITE. Our race detector ensures that if we detect \nthat pis weaker than q, we at most store information about the weaker of pand q, de\u00adcreasing our time \nand space overhead 4. In Sections 4 and 6, we show how the weaker-than relation can also be used to .lter \nevents before they reach the detector. 3.2 Trie-Based Algorithm In this section, we describe our runtime \ndatarace detection algo\u00adrithm and its use of tries to represent the event history. 3.2.1 Detection Algorithm \nFor each unique memory location in an access event observed by the datarace detector, we represent the \nhistory of accesses to that location using an edge-labeled trie. The edges of the trie are labeled with \nidenti.ers of lock objects, and the nodes hold thread and access type information for a (possibly empty) \nset of access events. The set of locks for an access is represented by the path from the root of the \ntrie to the node corresponding to that access. Nodes in our tries have a thread .eld tand an access type \n.eld a. Internal nodes which have no corresponding accesses are as\u00adsigned access type READ and a special \nthread value t(meaning T 4In the rare case that our tool reports a spurious datarace, an opti\u00admization \nbased on the weaker-than relation could suppress the re\u00adporting of a real datarace while allowing the \nfalse positive report. Using extra locking inserted by the user to suppress the spurious report overcomes \nthis de.ciency. no threads ). We de.ne the meet operator nfor thread informa\u00adtion tiand tjand access \ninformation aiand aj: Vi.tinti =ti,tntT =ti,anai =ai ii Vi.Vj.tintj=t. if ti =tj Vi.Vj.ainaj=WRITE if \nai =aj When we encounter an access event e, we .rst check if there exists an access epin the history \nsuch that epce. This check is performed through a traversal of the trie corresponding to e.m, following \nonly edges labeled with lock identi.ers in e.L(in depth\u00ad.rst order). During this traversal, we examine \neach encountered node s access type and thread information to see if it represents accesses weaker than \ne, as de.ned in the previous section. (The traversal procedure guarantees that the lockset and memory \nloca\u00adtion weakness conditions are satis.ed.) If we .nd such a node, then we can safely ignore ewhile \nmaintaining the reporting guarantees described in Section 2. In practice the vast majority of accesses \nare .ltered by this check. If the weakness check fails, we check efor dataraces by per\u00adforming another \ndepth-.rst traversal of the trie. For each node n encountered, we have one of three cases: Case I. The \nedge whose destination is nis labeled with lock iden\u00adti.er lnsuch that lnEe.L. In this case, eshares \nat least one lock with all the accesses represented by nand its children. Therefore, there cannot be \na datarace with any access repre\u00adsented by the subtree rooted at n, and we need not search any deeper \nin this branch of the trie. Case II. Case I does not hold, e.tnn.t=t., and e.ann.a= WRITE. In this case \nwe have a datarace, since e.tdiffers from some previous thread which accessed e.m, the inter\u00adsection \nof their lock sets is empty, and at least one access was a write. We report the race immediately and \nterminate the traversal. Case III. Neither case I nor II holds, in which case we traverse all children \nof n. After checking for races, we update the trie with information about e. If there is already a node \nnin the trie whose path to the root is labeled with the locks e.L, we update nwith n.t+n.tne.tand n.a+n.ane.a. \nIf no such nexists then we add nodes and edges to create such an n, setting n.t+e.tand n.a+e.a. Finally, \nwe traverse the trie once more to remove all the stored accesses which are stronger than the newly-added \naccess.  3.3 Implementation We have implemented the algorithm in Java, and the code is straightforward. \nThe algorithm runs online alongside the program. (The interface between the algorithm and the program \nis discussed below.) Our implementation uses memory addresses to identify logical memory locations. Garbage \ncollection can move objects to differ\u00adent addresses and reuse the same addresses for different objects. \nWe could respond to garbage collection by augmenting the ob\u00adject address information stored in our data \nstructures, but for our prototype implementation we simply use enough memory so that garbage collection \ndoes not occur.  4. RUNTIME OPTIMIZATION The algorithm described in the previous section reads an event \nstream generated by the running target program. To reduce the overhead of race detection, we reduce the \nnumber of access events that need to be fed into the detector, using a combination of static and dynamic \ntechniques. This section describes the dynamic tech\u00adnique: caching to detect redundant accesses. 4.1 \nOverview The previous section describes how an access is discarded if we have already seen a weaker access. \nExperiments show that in many benchmarks almost all accesses are discarded this way. Therefore we make \nthe check for a previous weaker access as ef\u00ad.cient as possible, by introducing caches to record previous \nac\u00adcesses. Thereare twocachesperthread, onerecordingreadaccessesand one recording write accesses. Each \ncache is indexed by memory lo\u00adcation. Whenever the program performs an access to location m, we look \nup min the appropriate cache. The cache design guaran\u00adtees that if an entry is found, there must have \nbeen a weaker access already recorded by the algorithm, so no further work is required. If no entry is \nfound, then we send information about the new access to the runtime detector and also add a corresponding \nnew entry to the cache. 4.2 Cache Policy Recall that access pis weaker than access qif p.m=q.m/ p.Loksq.Loks/p.tcq.t/p.acq.a. \nWe require that if entry for access pis found in the cache when new access qis checked, then pis weaker \nthan q. To guarantee that p.tcq.t, we observe that q.tis simply the currently executing thread when qoccurs. \nTherefore we use sepa\u00adrate caches for each thread. Any pfound in thread q.t s cache must have p.t=q.t. \n(This also ensures that cache operations do not require synchronization.) Because we use separate caches \nfor reads and writes, if we .nd entry pwhen we look up the cache then certainly their access type is \nthe same, i.e., p.a=q.a. To ensure that p.Loksq.Loks, we monitor the set of locks currently held by each \nthread. Whenever the program executes monitorexit to release a lock l, we evict from the cache any psuch \nthat lEp.Loks. This ensures that at all times, for every p in the cache, p.Loksis a subset of the currently \nheld locks. Hence when qoccurs we know p.Loksq.Loksfor all pin the cache. Note that because Java synchronization \nblocks are reentrant, a thread might execute monitorexit but not actually release the lock becausethe \nlock had previously been acquired more than once. We ignore these nested locks and unlocks; only the \nlast monito\u00adrexit on a lock object requires cache entries to be evicted. Each cache is indexed by memory \nlocation alone. Because our policy guarantees all entries in the cache are weaker than the access being \nlooked up, we do not actually have to check the thread ID, access type, or lock set, and they are not \nstored in the cache entries. When a thread releases a lock lwe need to quickly evict all the cache entries \nwhose lock sets contain l. We exploit the nested lock\u00ading discipline imposed by the Java language (although \nnot by the bytecode language we rely on the fact that the bytecode was gen\u00aderated by a Java compiler). \nThe discipline ensures that at the time some access generated a cache entry p, if lock lwas the last \nlock in p.Loksto be acquired, then lock lwill be the .rst of p.Loks to be subsequently released ( last \nin, .rst out ). Therefore for each lock lcurrently held by the thread, we keep a linked list of the cache \nentries pwhere lwas the last lock in p.Loksto be acquired. When lis released we evict all the entries \non its list from the cache. The lists are doubly-linked so that individual cache entries can be quickly \nremoved when they are evicted due to cache con.icts. 4.3 Implementation We use two 256-entry direct \nmapped caches, one for reads and one for writes, indexed by memory address. The hash function multiplies \nthe 32-bit memory address by a constant and takes the upper 16 bits of the result. The cache code is \nentirely written in Java and was executed on the Jalape no virtual machine [2]. We ensure that the Jalape \nno optimizing compiler inlines all calls to the cache lookup methods in the user s program. We also used \nJalape no-speci.cmethodcalls to ensurethat thecachelookupcode is compiled into ef.cient machine code \n(e.g., without array bounds checks). A cache lookup which results in a hit requires ten Pow\u00aderPC instructions \nin our implementation.  5. STATIC DATARACE ANALYSIS The static datarace analysis algorithm formulates \ndatarace anal\u00adysis as a conjunction of interthread control .ow analysis and points\u00adto analysis of thread \nobjects, synchronization objects, and access objects. We use this formulation to compute the static datarace \nset, a set of statement pairs that may cause a datarace during some execution. Statements that are not \npart of any statement pair in the static datarace set are non-datarace statements and need not be in\u00adstrumented \nat all. In this section, we give a brief summary of our approach to static datarace analysis. A more \ndetailed description can be found in [11]. We .rst describe a static formulation of the datarace conditions \n(Section 5.1). We then describe the interthread control .ow graph (ICFG) that we use to represent sequential \nand parallel interpro\u00adcedural control .ow (Section 5.2), and the ICFG-based points-to analysis that can \nbe used to compute the static formulation of the datarace conditions (Section 5.3). Finally, we describe \nan exten\u00adsion of escape analysis [10, 4, 5, 23] that can be used to improve the precision of static datarace \nanalysis (Section 5.4). 5.1 Datarace Conditions For two statements xand y, the datarace conditions de.ned \nin Section 2 can be formulated conservatively as follows for static analysis5: IsMayRae(x,y)=AccMayCon.ict(x,y)/ \n(-MustSameThread(x,y))/ (-MustCommonSync(x,y))(1) AccMayCon.ict(x,y)=true if executions of xand ymay \naccess the same memory location, so we use may points-to information for its computation. For example \nin Figure 2, we use may points-to information for object references T11:a and T21:d to statically determine \nwhether they may access the same memory location dur\u00ading some execution. MustSameThread(x,y)=true if \nxand yare always executed by the same thread, so we use must points-to information on thread objects \nfor its computation. In Figure 2, we use must points-to information on the thread objects that can run \nT11 or T21 to stat\u00adically determine whether the two statements may be executed by different threads. \nMustCommonSync(x,y)=true if xand yare always synchro\u00adnized by at least one common lock, so we use must \npoints-to in\u00adformation on synchronization objects for its computation. In Fig\u00adure 2, we use must points-to \ninformation on the synchronization objects pointed to by T10:this and T20:q to statically deter\u00admine \nwhether the two statements may be executed under different synchronization objects. 5For convenience, \nwe ignore the fourth of the datarace conditions in Section 2, and conservatively assume that it always \nholds. It is worth noting that may approximations of MustSameThread and MustCommonSync cannot be correctly \nused in conservative datarace analysis, because the datarace conditions refer to the com\u00adplements of \nthese sets. 5.2 Interthread Control Flow Graph (ICFG) The ICFG is a detailed interprocedural representation \nof a multi\u00adthreaded program in which nodes represent instructions (i.e., state\u00adments) and edges represent \nsequential and parallel control .ow. Each method and each synchronized block has distinguished entry \nand exit nodes in the ICFG. An ICFG contains four types of control .ow edges: intrapro\u00adcedural 6, call, \nreturn, and start. The .rst three types are present in a standard interprocedural control .ow graph. \nStart edges are unique to the ICFG, and represent invocations of the start() method of a Thread object, \nwhich starts the thread and invokes its run() method. All other invocations of a run() method ex\u00adecute \nas part of the calling thread. (Join edges are not included in the ICFG because they are not needed for \nthe conservative static datarace analysis performed in our approach.) Start edges are referred to as \ninterthread edges, while all other edges in the ICFG are called intrathread edges. The entry node that \nis a target of a start edge is called a thread-root node. An ICFG path without any interthread edges \nis an intrathread path, and an ICFG path with one or more interthread edges is an interthread path. We \nuse the interthread call graph (ICG) as the interprocedural abstraction of the ICFG, designed for practical \nand scalable analy\u00adsis of large programs. An ICG node is created for each method and each synchronized \nblock in the ICFG. The inclusion of separate ICG nodes for synchronized blocks is a notable difference \nbetween the ICG and standard call graphs. 5.3 Points-to Analysis The points-to analysis that we employ \nfor a static datarace anal\u00adysis is a .ow-insensitive, whole program analysis. In our analysis, a distinct \nabstract object is created for each allocation site in the program. Each abstract object represents all \nthe concrete objects created at the same site during execution. The points-to analysis computes for each \naccess in the program the set of abstract objects it points to along some path. A precise must points-to \nanalysis is expensive in general. We have devised a simple and conservative must points-to analysis based \non the notion of single-instance statements, each of which executes at most once during an execution. \nAn object created at a single\u00adinstance statement is called a single-instance object. If an access points \nto only one abstract object and that abstract object is a single\u00adinstance object, then the relation between \nthe access and the object is a must points-to relation7. More details on the must points-to analysis \nare given in [11]. Let Must T(x))and May T(x)be the must and may points\u00adto sets of access x. We compute \nAccMayCon.ict(x,y)of Equa\u00adtion (1) as follows using points-to information: AccMayCon.ict(x,y)= (2) (May \nT(x)nMay T(y)=0)/(field(x)=field(y)), where field(x)refers to the accessed .eld of the object (or class). \nFor access u, let TStart(u)be the set of thread-root nodes from whose entry nodes there exists an intrathread \nICFG path to u. We compute MustSameThread(x,y)as follows using points-to 6We assume that the intraprocedural \nedges capture all intraproce\u00addural control .ow, including control .ow arising from exceptions. 7We use \na special null object to represent a null reference. information: MustT read(u)=MustT(v.this) vEThStart(u) \nMustSameThread(x,y)= (3) (MustT read(x)nMustTread(y)=0), where v.this denotes the this pointer of thread-root \nnode v. For node nEIG, let Syn (n)=trueif nis a synchro\u00adnized method or block, and let unbe the access \nof the synchro\u00adnization object if Syn(n)=true.Also, let red(n)be the set of intrathread predecessor nodes \nof non ICG. We compute MustSyn(v)by the following set of data.ow equations: { MustT(un)if Syn(n)Gen(n)= \n0 otherwise nn n p SO.=SOiUGen(n),SOi =SO. pEr (n) n MustSyn(v)=SO.,VvEn. Now, we compute MustCommonSync(x,y)as \nfollows: MustCommonSync(x,y)= (4) (MustSyn(x)nMustSyn(y)=0). Finally, we compute IsMayRaein Equation \n1 by combining Equations 2, 3, and 4. 5.4 Extending Escape Analysis Past work on escape analysis normally \nidenti.es objects as thread\u00adlocal when they are never reachable from threads other than the thread that \ncreated them. A thread-local object can never partici\u00adpate in a datarace. Java code frequently uses objects \nassociated with a thread T which do not follow the above pattern but which are not suscep\u00adtible to data \nraces. In particular, we say an object Ois thread\u00adspeci.c to Tif all accesses to Oare performed while \nTis being constructed (and before Tstarts running), or by Titself. Refer\u00adences to such objects are typically \nstored in .elds of the Tobject and hence escape to the thread creating T, and are not thread-local as \ndescribed above. Because this usage is common, we extended our static analysis to identify some thread-speci.c \nobjects. We have implemented a simple, but effective, approximation al\u00adgorithm to compute the thread-speci.c \nobjects. First, we de.ne the thread-speci.c methods recursively as follows: (1) <init> meth\u00adods of thread \nobjects, and runmethods that are not invoked explic\u00aditly (i.e., invoked only as a result of the thread \nbeing started) and (2) a non-static method all of whose direct callers themselves are thread-speci.c \nnon-static methods passing their this references as the this reference of the callee. Second, we de.ne \nthe thread-speci.c .elds as the .elds of a thread that are only accessed via getfield/putfield oper\u00adations \non the this reference of a thread-speci.c method. Fi\u00adnally, we de.ne an unsafe thread as a thread whose \nexecution may start before its initialization completes. A thread object is conser\u00advatively identi.ed \nas unsafe if its constructor can transitively call Thread.start or if the this reference escapes from \nthe con\u00adstructor. (A thread is safe if it is not unsafe.) Based on these de.nitions, we say an object \nis thread-speci.c to T if T is safe and the object is only reachable from thread-speci.c methods of T \nor through thread-speci.c .elds of T. Accesses to a thread-speci.c object of a safe thread cannot be \ninvolved in a datarace. Moreover, accesses to thread-speci.c .elds cannot be involved in a datarace. \n 6. COMPILE-TIME OPTIMIZATIONS Our static datarace analysis phase improves the performance of our dynamic \ndetector by eliminating from consideration statements that can never participate in a datarace. Another \napproach to compile\u00adtime optimization stems from the weaker-than relation de.ned in Section 3: if the \nexecution of a statement always generates an ac\u00adcess that will be discarded because a previous access \nis weaker, the statement need not be instrumented. In this section, we describe how we use a static form \nof the weaker-than relation and a loop peeling transformation to avoid inserting instrumentation that \nwe can prove will only produce redundant access events. 6.1 Static Weaker-Than Relation Let Events(S)denote \nthe set of access events generated by in\u00adstrumentation statement Sin a given execution. We de.ne the \nstatic weaker-than relation for statements as follows: De.nition 3. Siis weaker than Sj, written as SicSj,iff \nfor all ejEEvents(Sj)in any given execution, there exists eiE Events(Si)in the same execution such that \n(1) eicej, where eicejas de.ned in section 3, and (2) there exists no thread start() or join() between \neiand ej. A sophisticated interprocedural analysis would be required to de\u00adtermine SicSjfor arbitrary \nSiand Sj. However, we have devel\u00adoped a conservative and effective analysis for computing SicSj when \nSiand Sjbelong to the same method. We model the instrumentation which generates access events us\u00ading \na pseudo-instruction trae(o,f,L,a), where ois the object being accessed, fis the .eld of the object being \naccessed, Lis the lock set held during the access, and ais the access type (READ or WRITE). All operands \nare treated as uses of their values. For accesses to static .elds, orepresents the class in which the \n.eld is declared, and for accesses to array elements, frepresents the array index. Thread information \nis not explicitly modelled in the traeinstruction since we do not attempt to optimize across thread boundaries \n(thread information is available to the instrumentation code at runtime). We insert a traepseudo-instruction \nafter ev\u00adery instruction which accesses a .eld of an object, a static .eld, or an array element (optionally \nusing information from static datarace analysis to eliminate consideration of instructions which cannot \nbe involved in dataraces). After insertion, we attempt to eliminate traepseudo-instructions using the \nstatic weaker-than relation. First, we de.ne Exe(Si,Sj) for statements Siand Sjof the same method as \nfollows: De.nition 4. Exe(Si,Sj)is true iff (1) Siis on every intrapro\u00adcedural path that contains Sj, \nand (2) there exists no method invo\u00adcation on any intraprocedural path between Siand Sj. The .rst condition \nindicates that whenever Sjexecutes in an exe\u00adcution instance of the method, Sialso executes. Two well-known \nconcepts can be used for computing Exe(Si,Sj): Sidominates Sj, written dom(Si,Sj), and Sipost-dominates \nSj, written pdom(Si,Sj). In our experiments, we used dom. (It is very dif\u00ad.cult to prove that one statement \npost-dominates another in Java, because almost any statement can throw an exception, and there\u00adfore we \nsuspect that pdomwould not be effective.) The second condition guarantees that no path between Siand \nSjwill contain start() or join(). With Exe, the static weaker-than relation can be decomposed into the \nfollowing easily veri.able conditions (notation to be ex\u00adplained): SicSj =Exe(Si,Sj)/aicaj/outer(Si,Sj) \n/valnum(oi)=valnum(oj)/fi =fj. To show that a statement Si =trae(oi,fi,Li,ai)always gener\u00adates an event \neiweaker than any ejproduced by Sj=trae(oj,fj,Lj,aj), we must show that ei.tcej.t/ei.acej.a/ei.Lej.L/ei.m=ej.m. \nIntraprocedurally, ei.twill always equal ej.t, and we can directly check aicajwhich implies ei.acej.a. \nWe check that ei.L ej.Lusing the nesting of Java s synchronization blocks. Speci.\u00adcally, we verify the \ncondition outer(Si,Sj), which is true if and only if Sjis at the same nesting level in synchronization \nblocks as Sior at a deeper level within Si s block. Finally, to show that ei.m=ej.m, our analysis checks \nthat (valnum(oi)=valnum(oj))/(fi =fj), where valnum(oi)is the value number of the object reference. If \nall of these conditions hold, then SicSj, and therefore we can safely eliminate Sj. 6.2 Implementation \nIn this section, we brie.y describe the implementation infras\u00adtructure that we use for optimized instrumentation. \nThe instru\u00admentation and the analysis of the weaker-than relation is performed during the compilation \nof each method by the Jalape no optimizing compiler [2]. We created a new instruction in the high-level \ninter\u00admediate representation (HIR) of the compiler corresponding to our traepseudo-instruction, and these \ninstructions are inserted as pre\u00adviously described. After the insertion of the traestatements, con\u00adversion \nto static single assignment (SSA) form is performed, during which the dominance relation is computed. \nElimination of redun\u00addant traestatements is then performed based on the static weaker\u00adthan relation, \nutilizing an existing value numbering phase. The re\u00admaining traestatements are marked as having an unknown \nside effect to ensure they are not eliminated as dead code by Jalape no s other optimization phases unless \nthey are truly unreachable. After the completion of some of Jalape no s HIR optimization phases, we expand \neach traestatement into a call to a method of our dynamic detector, and we force Jalape no to inline \nthis call. Jalape nothen optimizes the HIR again. Finally, the HIR represen\u00adtation is converted to lower-level \nrepresentations (and eventually to machine code) by the compiler, without further instrumentation\u00adspeci.c \noptimization. 6.3 Loop Peeling Loops can be a key source of redundant access events. For ex\u00adample, in \nthe loop in Figure 3 consisting of statements S10through S13, statement S13 will produce redundant access \nevents after the .rst iteration of the loop, since the information is the same as that recorded in the \n.rst iteration. However, two issues make these redundant events dif.cult to statically eliminate. Our \nredun\u00addancy elimination based on the static weaker-than relation cannot be applied to remove the instrumentation, \nsince the information produced in the .rst iteration of the loop is not redundant. Further\u00admore, we cannot \nperform standard loop-invariant code motion to hoist the instrumentation outside the loop, because statement \nS11 is a potentially excepting instruction (PEI); it may throw an ex\u00adception and bypass the remaining \ninstructions of the loop. Thus statement S13 is not guaranteed to execute even if the loop condi\u00adtion \nis initially true. PEIs occur frequently in Java because of safety checks such as null-pointer and array \nbounds checks. We reduce the generation of redundant access events in loops using a loop peeling program \ntransformation. This transformation // Before optimization. S00: A a; S10: for(...) { S11: PEI S12: \na.f = ...; S13: trace(a, f, L, W) } // After optimization. S20: if (...) { S21: PEI S22: a.f = \n...; S23: trace(a, f, L, W); S24: for (...) { S25: PEI S26: a.f = ...; } } Figure 3: Example of \nLoop Peeling Optimization creates a new copy of the body of the loop for the .rst iteration and utilizes \nthe original body for the remaining iterations. Statements S20 through S26 show the result of loop peeling \nand our existing redundancy elimination applied to the loop of S00. The if state\u00adment at S20 is needed \nto guard against the possibility of the loop not executing at all. The for statement at S24 is modi.ed \nto en\u00adsure that the loop will not execute the .rst iteration, which is now executed by statements S21 \nthrough S23. After the loop peeling, the traestatement in the loop body can be eliminated since state\u00adment \nS23 is statically weaker. The resulting code traces the write access to a.fat most once, achieving the \ngoal of eliminating the instrumentation from the loop. We are unaware of previous work that performs \nthis type of program transformation to decrease the cost of instrumentation.  7. OWNERSHIP MODEL All \nof the preceding discussion ignores the effects of the own\u00adership model introduced in Section 2.3. Here \nwe brie.y consider how the ownership model interacts with our other machinery. 7.1 Implementation We \nmodi.ed our runtime race detector to record for each mem\u00adory location an owner thread t, the .rst thread \nto access the mem\u00adory location. Every time the location is accessed we check to see if the current thread \nis t, and ignore the access in that case. The .rst time the current thread is not t, we say the memory \nlocation becomes shared; we set tto .and send this access event and all subsequent events on to the rest \nof the detector, as described in sec\u00adtion 3. Essentially the access event stream is .ltered to only include \naccesses to memory locations in the shared state. 7.2 Interactions with Weaker-Than Relation The run-time \nand compile-time optimization phases rely on the concept of one access event e1being weaker-than another \nevent e2, in which case e2 can be suppressed. Unfortunately, in the presence of the ownership model, \nthe de.nitions of IsRaeand weaker-than in section 3.1 are not suf.cient to guarantee that e1 weaker-than \ne2implies e2can be suppressed. The dif.culty arises when an event e1is sent to the detector while e1.mis \nin the owned state, and then e1.mchanges to the shared state before e2occurs. In this situation e2should \nnot be suppressed. For run-time optimization (i.e., the cache), we can avoid this problem by forcibly \nevicting a location mfrom each thread s cache when it becomes shared. It is harder to avoid this problem \nin compile-time optimization. Given two statements S1and S2, it is generally dif.cult to prove that the \naccessed location s state cannot change from owned to shared between S1and S2. Introducing a dynamic \ncheck of the ownership state at S1or S2would eliminate the bene.t of the opti\u00admization. The only truly \nsound compile-time approach would be to use the post-dominance relationship; i.e., when S2post-dominates \nS1and the access at S2is guaranteed to be weaker than S1, re\u00admove the instrumentation at S1. This is \nsafe because if the object is owned at S2, and therefore the access is suppressed, then the ob\u00adject must \nalso have been owned at S1and that access can also be suppressed. Unfortunately, as previously noted, \npost-dominance between S1and S2almost never holds in Java because almost any bytecode instruction can \nthrow an exception. (This might be less of a problem in other languages.) Our actual approach is to simply \nignore the interaction between weaker-than and the ownership model, for both static and dynamic optimizations. \nThis means that in theory our tool may inadvertently suppressaccessesandthusfailtoreportraces. However,wedidnot \nobserve any such problems in practice; in our experiments we ver\u00adi.ed that the same races were reported \nwhether the optimizations using the unsafe weaker-than relation were enabled or disabled.  8. EXPERIMENTAL \nRESULTS Here we present evidence supporting our two major claims: that our de.nition of dataraces captures \ntruly unsynchronized accesses with fewer false alarms than alternative de.nitions, and that those dataraces \ncan be detected with modest overhead especially com\u00adpared to other datarace detection implementations. \n8.1 Program Examples Table 1 lists the programs used in our experiments. We derived sor2 from the original \nsor benchmark by manu\u00adally hoisting loop invariant array subscript expressions out of inner loops. This \noptimization could be performed by a compiler using onlyintraproceduralanalysis,but it isnotimplementedinJalape \nno, and it has signi.cant impact on the effectiveness of our optimiza\u00adtions. We modi.ed elevator slightly \nto force it to terminate when the simulation .nishes (normally it just hangs). 8 The elevator and hedc \nbenchmarks are interactive and not CPU-bound, and therefore we do not report performance results for \nthese benchmarks. 8.2 Performance Table 2 shows the runtime performance of our algorithm and some selected \nvariants to demonstrate the impact of each of our optimizations. Base records the performance of each \nexample without any instrumentation (and without loop peeling). Full is our complete algorithm with all \noptimizations turned on. NoS\u00adtatic is Full but with the static datarace detection turned off, so all \naccess statements are potential dataraces. NoDominators is Full with the static weaker-than check disabled; \nit also disables loop peeling (which is useless without that check). NoPeeling turns off loop peeling \nonly. NoCache disables the cache. In mtrtwithout static datarace detection, we instrument so many accessesthat \nJalape norunsout of memorybeforetheprogramter\u00adminates. 8We obtained all these examples from Praun and \nGross, to whom we owe great thanks. For each con.guration, we ran the program .ve times in one invocation \nof the VM and reported the best-performing run. We enabledfull optimization in Jalape nobut disabledadaptivecompi\u00adlation. \nJalape no was con.gured to use a mark-and-sweep garbage collector, but we set the heap size to 1GB of \nRAM so no GC ac\u00adtually occurred. Our test machine had a single 450MHz POWER3 CPU running AIX. These overheads \nare lower than for any previously reported dy\u00adnamic datarace detection algorithm. The bene.ts of each \nopti\u00admization vary across benchmarks, but each optimization is vital for some benchmark. Programs such \nas tsp, with loops involving many method calls and even recursive method calls, bene.t greatly from the \ncache. Programs such as sor2, which are dominated by loops over arrays, bene.t most from dominator analysis \nand loop peeling. Wedidnotmeasurespaceoverheaddirectly; Jalape nomixespro\u00adgram data with virtual machine \ndata, making space measurements dif.cult. Our instrumentation consumed the most space for tsp, requiring \napproximately 16K of memory per thread (for 3 threads) and 7967 trie nodes holding history for 6562 memory \nlocations. (We have a scheme for packing information for multiple locations into one trie which we cannot \npresent due to space limitations.) We estimate the total amount of memory used by instrumentation for \ntsp to be about 500K.  8.3 Accuracy Table 3 records the number of objects for which we report dataraces \nusing our algorithm and some selected variants. (We normally out\u00adput each object .eld on which a datarace \noccurs; for comparison purposes, here we count only the number of distinct objects men\u00adtioned.) Full \nis our complete, most precise algorithm. Field\u00adsMerged is a variant of our algorithm where we do not \ndistinguish different .elds of the same object, so one thread accessing o.f 1 might appear to datarace \nwith another thread accessing o.f2if they do not hold a common lock. (Static .elds of the same class \nare still distinguished.) NoOwnership is another variant of Full which does not wait for a location to \nbe touched by multiple threads before starting to monitor its accesses. We report two dataraces in mtrt. \nAccesses to the .eld RayTrace.threadCountare not synchronized, causing its value to potentially become \ninvalid; fortunately its value is not actually used. There are also unsynchronized accesses to ValidityCheckOutputStream.startOfLine \nin the SPEC test harness, which could result in incorrect output. tsp has a serious datarace on TspSolver.MinTourLen, \nwhich can lead to incorrect output. We also report dataraces on .elds of TourElement, which cannot in \nfact happen due to higher\u00adlevel synchronization. The dataraces we report in sor2 are not truly unsynchronized \naccesses; the program uses barrier synchronization, which is not captured by our algorithm. The dataraces \nwe report in hedcare all true unsynchronized ac\u00adcesses and have two causes. The size of a thread pool \nis read and written without appropriate locking, which could cause the pool size to become invalid. More \nseriously, there is an unsynchronized assignment of null to .eld Task.thread , which could cause the \nprogram to die with a NullPointerException if the Task completes just as another thread calls Task.cancel. \nThis would be nearly impossible to .nd during normal testing and debugging. In fact, previous work [21] \nmistakenly classi.ed this datarace as benign (possibly because they had to sort through a number of spu\u00adrious \ndatarace reports). If we fail to distinguish .elds, in hedc we produce spurious  Example Lines of Code \nNum. Dynamic Threads Description mtrt tsp sor2 elevator hedc 3751 706 17742 523 29948 3 3 3 5 8 MultiThreaded \nRay Tracer from SPECJVM98 Traveling Salesman Problem solver from ETH [21] Modi.ed Successive Over-Relaxation \nbenchmark from ETH [21] A real-time discrete event simulator A Web-crawler application kernel developed \nat ETH [21], using a concurrent programming library by Doug Lea. Table 1: Benchmark programs and their \ncharacteristics Example Base Full NoStatic NoDominators NoPeeling NoCache mtrt 9.0s 10.9s (20%) Out of \nMemory 10.9s (21%) 10.9s (21%) 11.4s (26%) tsp 10.0s 14.2s (42%) 27.5s (175%) 15.7s (57%) 15.7s (57%) \n381.7s (3722%) sor2 2.4s 2.7s (13%) 2.7s (13%) 9.8s (316%) 7.7s (226%) 3.2s (37%) Table 2: Runtime Performance \nExample Full FieldsMerged NoOwnership mtrt 2 2 12 tsp 5 20 241 sor2 4 4 1009 elevator 0 0 16 hedc 5 10 \n29 Table 3: Number of Objects With Dataraces Reported race reports in the LinkedQueue class where some \n.elds are im\u00admutable and accessed without synchronization and others are not. It also produces spurious \nwarnings for MetaSearchRequest objects where some .elds are thread-local and others are shared and require \nsynchronization. In tsp we report additional spurious dataraces on .elds of TourElement. In all benchmarks, \nNoOwnership reports many spurious dataraces when data is initialized in one thread and passed into a \nchild thread for processing. Previous work such as Eraser [24] and object datarace detec\u00adtion [21] uses \na looser de.nition of dataraces, where a datarace is deemed to have occurred on a location mif there \nis no single common lock held during all accesses to m. This approach pro\u00adduces spurious datarace reports \nin mtrt, where variables holding I/O statistics are accessed by two child threads holding a common lock \nsyncObject, but also by a parent thread after it has called join on the two child threads but without \nany other synchroniza\u00adtion. Our scheme for representing join introduces pseudolocks S1and S2; the three \nthreads access the variables with lock sets {S1,syncObject}, {S2,syncObject}and {S1,S2}. We re\u00adport no \ndatarace because these locksets are mutually intersecting, although they have no single common lock. \nIn summary, for these benchmarks, most of the dataraces we re\u00adport are true unsynchronized accesses, \nand most of those corre\u00adspond to real bugs. Using a less strict de.nition induces signi.\u00adcantly more \nspurious reports. 9. RELATED WORK Past research on datarace detection can be classi.ed as ahead-of\u00adtime, \non-the-.y,or post-mortem. These approaches offer different trade-offs along ease-of-use, precision, ef.ciency, \nand coverage di\u00admensions. Ahead-of-time datarace detection is usually performed in static datarace analysis \ntools which yield high coverage by consider\u00ading the space of all possible program executions and identifying \ndataraces that might occur in any one of them. Flanagan and Fre\u00adund s datarace detection tool is a static \ntool for Java [15] which tracks synchronization using extended type inference and check\u00ading. Guava is \na dialect of Java that statically disallows dataraces by preventing concurrent accesses to shared data \n[3]. Only instances of classes belonging to the class category called monitor can be shared by multiple \nthreads. By serializing all accesses to .elds or methods of the same shared data, Guava can prevent dataraces. \nBoyapati and Rinard propose a system of type annotations for Java that ensures a well-typed program is \ndatarace-free and allows the programmer to write a generic class and subclass it with different protection \nmechanisms. [6]. Warlock is an annotation-based static datarace detection tool for ANSI C programs [27], \nwhich also supports lock-based synchro\u00adnization. Aiken and Gay s work statically detects dataraces in \nSPMD programs [1]. Since SPMD programs employ barrier-style syn\u00adchronizations, they need not track locks \nheld at each statement. The static datarace analysis employed as part of our datarace detection is based \non points-to analysis of reference variables [7, 26]. The primary advantage of a static analysis approach \nis its ef.ciency due to the fact that it incurs no runtime overhead. However, this advan\u00adtage is mitigated \nin practice by severe limitations in precision (due to false positive reports) and ease-of-use (due to \nthe requirement of presenting a whole program to the static analysis tool, sometimes augmented with annotations \nto aid the analysis). The key advantage of dynamic analysis approaches such as on\u00adthe-.y and post-mortem \ndatarace detection is the precision of the results (few or no false positives), but in past work this \nadvantage usually came at a high cost in ef.ciency. A dynamic approach also has more limited coverage \nthan a static approach because it only reports dataraces observed in a single dynamic execution. In some \ncases, dynamic tools can improve coverage by considering alternate orderings of synchronization operations \nthat are consistent with the actual events observed in the original program execution [24]. Dinning and \nSchonberg introduced the idea of detecting dataraces based on a proper locking discipline [14]. Their \nsystem employed a detection approach based on both the happened-before relation and locksets, which they \ncalled lock covers. Their subtraction opti\u00admization uses a notion similar to the weaker-than relation, \nbut they only suggest using the optimization in the detector itself, while we employ the notion in many \nstages of our detection framework. Eraser is similar to our approach in that its datarace detection algorithm \nis based on lock-based synchronization [24]. However, Eraser enforces the constraint that each shared \nmemory location is protected by a unique lock throughout an execution, which we do not, thus reporting \nfewer spurious data races. Our ownership model is based on Eraser s, but Eraser has no comparable handling \nof the join operation (see Section 8). Eraser works independently of the input source language by instrumenting \nbinary code, but its run\u00adtime overhead is in the range of 10.to 30.. Praun and Gross s object race detection \n[21] greatly improves on Eraser s performance by applying escape analysis to .lter out non-datarace statements \nand by detecting dataraces at the object level instead of at the level of each memory location (their \noverhead ranges from 16% to 129% on the same benchmarks we used, with less than 25% space overhead). \nHowever, their coarser granularity of datarace detection (which includes treating a method call on an \nobject as a write) leads to the reporting of many dataraces which are not true dataraces, i.e., the reported \nraces do not indicate unordered concurrent accesses to shared state. For example, on the hedc program, \nwe report dataraces on 5 objects, all of which are true dataraces, while object race detection reports \nover 100 dataraces, almost all of which are not true dataraces. (The race de.nitions for object race \ndetection and Eraser imply they always report a superset of the races we report.) TRaDe is similar to \nobject race detection in that they both apply escape analysis [13], although TRaDe does the analysis \ndynami\u00adcally. TRaDe s datarace detection differs from ours in that it is based on the happens-before \nrelation. TRaDe adds a runtime over\u00adhead ranging from 4.to 15.[13] compared to an interpreter, with approximately \n3.space overhead. AssureJ [18] and JProbe [17] are commercial products that can dynamically detect dataraces \nin Java programs. AssureJ has been observed to have overhead rang\u00ading from 3.to 30., while JProbe s memory \nrequirements make its use practically impossible for any reasonably sized program [13]. Min and Choi \ns hardware-based scheme [19] uses the cache co\u00adherence protocol, and Richards and Larus work [22] uses \nthe Dis\u00adtributed Shared-Memory(DSM) computer s memory coherencepro\u00adtocol, respectively, in collecting \ninformation for on-the-.y datarace detection. Most dynamic datarace detection techniques for SPMD programs \nwork either as post-mortem tools or as on-the-.y tools [25], by collecting information from actual executions \nwith software instru\u00admentation. A post-mortem approach offers the possibility of im\u00adproving on-line ef.ciency \n(by moving the bulk of the work to the post-mortem phase) at the cost of complicating ease-of-use. How\u00adever, \nthe size of the trace structure can grow prohibitively large thus making the post-mortem approach infeasible \nfor long-running pro\u00adgrams. Another dimension that can be used to classify past work on datarace detection \nis the underlying concurrency model. Past work on datarace detection was historically targeted to multithreaded \nfork\u00adjoin programs [1, 8]. However, those results are not applicable to the object-based concurrency \nmodels present in multithreaded object-oriented programming languages such as Java. Netzer and Miller \ncategorize dynamic dataraces into actual, ap\u00adparent, and feasible dataraces [20]. Assuming T10:this and \nT20:q in Figure 2 point to different synchronization objects, T11 and T21 are both an actual and a feasible \ndatarace if T20 occurs before T13. They are, however, only a feasible datarace if T13 occurs before T20, \nwhich introduces a happened-before relation from T11 to T21. Choi and Min describe how to identify and \nreproduce the race frontier, which is the set of dataraces not affected by any other dataraces [12]. \nBy repeatedly reproducing and correcting the dataraces in the race frontier, one can identify all the \ndataraces that occur in executions. 10. CONCLUSIONS AND FUTURE WORK In this paper, we presented a novel \napproach to ef.cient and pre\u00adcise datarace detection for multithreaded object-oriented programs. Our \napproach consists of a unique combination of static datarace analysis, optimized instrumentation, runtime \naccess caching and runtime detection phases. This approach results in a runtime over\u00adhead that is only \nin the 13% to 42% range, well below most past work. Furthermore, our datarace de.nition is precise enough \nthat in our test cases, almost all the dataraces reported were in fact con\u00adcurrent accesses to shared \nmemory locations without any ordering constraints. These results show that it is feasible to perform \nprecise datarace detection in a production setting. In the future, we plan to broaden the static/dynamic \ncoanalysis approach to tackle other problems such as deadlock detection and immutability analysis. We \nalso intend to enhance the static analysis phases with more precise alias analysis algorithms. We plan \nto in\u00adtegrate these new analyses with the record/replay capabilities of our DejaVu debugger [9], providing \na powerful platform for reasoning about the behavior of multithreaded programs. Acknowledgments We would \nlike to thank members of the Jikes RVM runtime group and the Jikes RVM optimization group at IBM T. J. \nWatson Re\u00adsearch Center for their help with the Jikes RVM system. We also thank the referees and the \ncommittee members of PLDI for their insightful comments. We thank Julian Dolby for his GNOSIS interprocedural \nanalysis framework, which forms the basis of our static datarace analysis. 11. REFERENCES [1] A. Aiken \nand D. Gay. Barrier inference. In Proceedings of the 25th Symposium on Principles of Programming Languages \n(POPL), pages 342 354, January 1998. [2] B.Alpern,et.al.TheJalape novirtualmachine. IBM Systems Journal, \n39(1), 2000. [3] D. F. Bacon, R. E. Strom, and A. Tarafdar. Guava: A dialect of java without data races. \nIn ACM Conference on Object-Oriented Programming Systems, Languages, and Applications, 2000. [4] B. Blanchet. \nEscape analysis for object oriented languages: Application to Java. In Proceedings of ACM SIGPLAN Conference \non Object-Oriented Programming Systems, Languages, and Applications, Denver, Colorado, November 1999. \n[5] J. Bodga and U. H\u00a8olzle. Removing unnecessay synchronization in Java. In Proceedings of ACM SIGPLAN \nConference on Object-Oriented Programming Systems, Languages, and Applications, Denver, Colorado, November \n1999. [6] C. Boyapati and M. Rinard. A parameterized type system for race-free java programs. In ACM \nConference on Object-Oriented Programming Systems, Languages, and Applications, 2001. [7] M. Burke, P. \nCarini, J.-D. Choi, and M. Hind. Flow-insensitive interprocedural alias analysis in the presence of pointers. \nIn 7th International Workshop on Languages and Compilers for Parallel Computing, 1994. Extended version \npublished as Research Report RC 19546, IBM T. J. Watson Research Center, September, 1994. [8] G.-I. Cheng, \nM. Feng, C. E. Leiserson, K. H. Randall, and A. F. Stark. Detecting data races in Cilk programs that \nuse locks. Proceedings of the Tenth Annual ACM Symposium on Parallel Algorithms and Architectures, 1998. \n[9] J.-D. Choi, B. Alpern, T. Ngo, M. Sridharan, and J. Vlissides. A perturbation-free replay platform \nfor cross-optimized multithreaded applications. In Proceedings of the 15th IEEE International Parallel \n&#38; Distributed Processing Symposium, April 2001. [10] J.-D. Choi, M. Gupta, M. Serrano, V. C. Sreedhar, \nand S. Midkiff. Escape analysis for Java. In ACM Conference on Object-Oriented Programming Systems, Languages, \nand Applications, pages 1 19, 1999. [11] J.-D. Choi, A. Loginov, and V. Sarkar. Static datarace analysis \nfor multithreaded object-oriented programs. Technical report, IBM Research, 2001. Report RC22146; www.research.ibm.com/jalapeno/dejavu/. \n[12] J.-D. Choi and S. L. Min. Race frontier: Reproducing data races in parallel-program debugging. In \nProceedings of Third ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, April \n1991. [13] M. Christiaens and K. De Bosschere. TRaDe, a topological approach to on-the-.y race detection \nin java programs. Proceedings of the Java Virtual Machine Rsearch and Technology Symposium (JVM 01), \nApril 2001. [14] A. Dinning and E. Schonberg. Detecting access anomalies in programs with critical sections. \nProceedings of the ACM/ONR Workshop on Parallel and Distributed Debugging, published in ACM SIGPLAN Notices, \n26(12):85 96, 1991. [15] C. Flanagan and S. N. Freund. Type-based race detection for java. In Proceedings \nof the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI), pages 219 232, \nJune 2000. [16] E. Fredkin. Trie memory. Communications of the ACM, 3(9):490 499, September 1960. [17] \nKL Group, 260 King Street East, Toronto, Ontario, Canada. Getting Started with JProbe. [18] Kuck &#38; \nAssociates, Inc., 1906 Fox Drive, Champaign, IL 61820-7345, USA. AssureJ User s Manual, 2.0 Edition, \nMarch 1999. [19] S. L. Min and J.-D. Choi. An ef.cient cache-based access anomaly detection scheme. In \nProceedings of 4th International Conference on Architectural Support for Programming Languages and Operating \nSystems (ASPLOS), April 1991. [20] R. H. Netzer and B. P. Miller. What are race conditions? some issues \nand formalizations. ACM Letters on Programming Languages and Systems, 1(1):74 88, Mar. 1992. [21] C. \nv. Praun and T. Gross. Object race detection. In ACM Conference on Object-Oriented Programming Systems, \nLanguages, and Applications, 2001. [22] B. Richards and J. R. Larus. Protocol-based data-race detection. \nIn Proceedings of the ACM SIGMETRICS Symposium on Parallel and Distributed Tools, pages 40 47, August \n1998. [23] E. Ruf. Effective synchronzation removal for Java. In SIGPLAN 2000 Conference on Programming \nLanguage Design and Implementation, pages 208 218, 2000. [24] S. Savage, M. Burrows, G. Nelson, P. Sobalvarro, \nand T. E. Anderson. Eraser: A dynamic data race detector for multi-threaded programs. ACM Transactions \non Computer Systems, 15(4):391 411, 1997. [25] E. Schonberg. On-The-Fly detection of access anomalies. \nIn Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI), \npages 285 297, June 1989. [26] B. Steensgaard. Points-to analysis in almost linear time. In In Proceedings \nof the Twentythird Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages (POPL), \npages 32 41, January 1996. [27] N. Sterling. Warlock: A static data race analysis tool. In USENIX Winter \nTechnical Conference, pages 97 106, 1993.    \n\t\t\t", "proc_id": "512529", "abstract": "We present a novel approach to dynamic datarace detection for multithreaded object-oriented programs. Past techniques for on-the-fly datarace detection either sacrificed precision for performance, leading to many false positive datarace reports, or maintained precision but incurred significant overheads in the range of 3<sc>x</sc> to 30<sc>x</sc>. In contrast, our approach results in very few false positives and runtime overhead in the 13% to 42% range, making it both efficient <i>and</i> precise. This performance improvement is the result of a unique combination of complementary static and dynamic optimization techniques.", "authors": [{"name": "Jong-Deok Choi", "author_profile_id": "81423596242", "affiliation": "IBM T. J. Watson Research Center", "person_id": "PP42052144", "email_address": "", "orcid_id": ""}, {"name": "Keunwoo Lee", "author_profile_id": "81406597623", "affiliation": "Univ. of Washington", "person_id": "P348267", "email_address": "", "orcid_id": ""}, {"name": "Alexey Loginov", "author_profile_id": "81100304647", "affiliation": "Univ. of Wisconsin - Madison", "person_id": "P348255", "email_address": "", "orcid_id": ""}, {"name": "Robert O'Callahan", "author_profile_id": "81100649205", "affiliation": "IBM T. J. Watson Research Center", "person_id": "P245990", "email_address": "", "orcid_id": ""}, {"name": "Vivek Sarkar", "author_profile_id": "81100597290", "affiliation": "IBM T. J. Watson Research Center", "person_id": "PP14206121", "email_address": "", "orcid_id": ""}, {"name": "Manu Sridharan", "author_profile_id": "81100641428", "affiliation": "MIT", "person_id": "P186999", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/512529.512560", "year": "2002", "article_id": "512560", "conference": "PLDI", "title": "Efficient and precise datarace detection for multithreaded object-oriented programs", "url": "http://dl.acm.org/citation.cfm?id=512560"}