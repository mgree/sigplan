{"article_publication_date": "05-17-2002", "fulltext": "\n The Embedded Machine: Predictable, Portable Real-Time Code* Thomas A. Henzinger EECS, University of \nCalifornia, Berkeley tah@eecs.berkeley.edu ABSTRACT The Embedded Machine is a virtual machine that mediates \nin real time the interaction between software processes and physical processes. It separates the compilation \nof embed\u00added programs into two phases. The .rst, platform-indepen\u00addent compiler phase generates E code \n(code executed by the Embedded Machine), which supervises the timing not the scheduling of application \ntasks relative to external events, such as clock ticks and sensor interrupts. E code is portable and \nexhibits, given an input behavior, predictable (i.e., deterministic)timing and output behavior. The sec\u00adond, \nplatform-dependent compiler phase checks the time safety of the E code, that is, whether platform performance \n(determined by the hardware)and platform utilization (de\u00adtermined by the scheduler of the operating system)enable \nits timely execution. We have used the Embedded Machine to compile and execute high-performance control \napplica\u00adtions written in Giotto, such as the .ight control system of an autonomous model helicopter. \n Categories and Subject Descriptors D.4.7 [Operating Systems]: Organization and Design Real-time systems \nand embedded systems  General Terms Languages Keywords Real Time, Virtual Machine 1. INTRODUCTION We \nde.ne a real-time execution model, called the Embedded Machine (E machine, for short), which provides \na portable * This research was supported in part by the DARPA SEC grant F33615-C-98-3614, the MARCO GSRC \ngrant 98-DT\u00ad660, the AFOSR MURI grant F49620-00-1-0327, and the NSF ITR grant CCR-0085949. Permission \nto make digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior speci.c permission and/or a fee. PLDI 02, June 17-19, \n2002, Berlin, Germany. Copyright 2002 ACM 1-58113-463-0/02/0006 ...$5.00. Christoph M. Kirsch EECS, University \nof California, Berkeley  cm@eecs.berkeley.edu target for the compilation of languages with hard real-time \nconstraints, such as Giotto [8]. E code (the code executed by the E machine)has strong theoretical properties, \nin par\u00adticular, its timing and behavior are predictable. These ben\u00ade.ts do not come at undue cost in \nperformance. We have demonstrated this by using E code to reimplement the .ight control system of an \nautonomous model helicopter [10]. From Platform-Centric to Requirements-Centric Real-Time Programming \nIn embedded systems, there are two time lines. The interac\u00adtion of software processes with physical processes \n(sensors, actuators, clocks)happens in environment time. Applica\u00adtion requirements are speci.ed in environment \ntime, e.g., the actuator is set within 2 clock ticks of a sensor inter\u00adrupt. On the other hand, the execution \nof software pro\u00adcesses on a speci.c platform happens in platform time.By platform we mean the combination \nof hardware and real\u00adtime operating system (RTOS). Issues of platform perfor\u00admance, such as worst-case \nexecution times (WCETs), and platform utilization, such as distribution and scheduling, must be addressed \nin terms of CPU time. The art of em\u00adbedded programming is to reconcile the two time lines. The E machine \nproposes a paradigm shift in real-time programming: it permits the programmer to think exclu\u00adsively in \nterms of environment time ( reactivity ), and shifts the burden of reconciliation with platform time \nto the com\u00adpiler ( schedulability ). This paradigm shift is in line with the steady move towards higher-level \nprogramming abstrac\u00adtions. In fact, the E machine treats platform time as a re\u00adsource in the way in which \nmost high-level languages treat memory: the programmer assumes there is enough of it; the compiler makes \nsure there is enough of it (or fails to com\u00adpile); the runtime system throws an exception in case the \ncompiler was wrong (usually due to incorrect assumptions about the platform and possible contingencies). \nProgramming in terms of environment time avoids the two central drawbacks of conventional embedded code: \nnon\u00adportability and unpredictability. The former is immedi\u00adate: conventional embedded code is intrinsically \nplatform\u00addependent, because it directly or indirectly (say, through priorities)refers to platform time; \nE code is platform-inde\u00adpendent, because it refers only to environment time. Pre\u00addictability, both in \ntiming and functionality, is a less im\u00admediate but powerful consequence of programming in terms of environment \ntime. By task we mean a software process (or a fragment thereof)without internal synchronization points \n[13]. Suppose that the inputs of a task t become available at time x (say, through a sensor interrupt), \nand its outputs are required at time y (say, as an actuator setting). The programmer and, in turn, the \nE machine is concerned only with these two times: at environment time x, the inputs are provided to t \nand the task is turned over to the platform, namely, the RTOS; at environment time y, the outputs of \nt are read and given to the actuator. The programmer may assume that the task has indeed completed at \ntime y;other\u00adwise the compiler (or, as last resort, the runtime system)will complain. However, the programmer \ncannot know exactly when in the interval [x, y] the task completes; in fact, she cannot even read the \noutputs of t as soon as they become available, as this would introduce an instant of platform time into \nthe program. The strict adherence to environ\u00adment time allows us to design E code without race condi\u00adtions: \nfor two concurrent tasks, it does not matter which task completes .rst, as long as each task completes \nbefore its outputs are read. A computation of the E machine is time-safe if each task completes before \nits outputs are read. Time safety depends, of course, on the platform (performance, distribution, sched\u00aduling). \nA good compiler ensures time safety; in addition, the runtime system monitors time safety. For E code \nthat refers only to environment time, time safety implies environ\u00adment determinedness. A computation \nof the E machine is environment-determined if the inputs from the environment processes (e.g., the sensor \nreadings)uniquely determine the outputs of the software processes (e.g., the actuator set\u00adtings). While \ntime safety captures timing predictability (the actuators are written at predictable times), environment \nde\u00adterminedness captures, in addition, value predictability (the actuators are given predictable values). \nBased on these principles, we de.ned the language Giotto for high-performance control applications [8]. \nGiotto sup\u00adports high-level structuring principles for modern control systems, such as periodic task \ninvocation and multiple con\u00adtrol modes. In compiling Giotto, we have found it useful to have an intermediate \nlanguage, with none of the high-level concepts but the same platform-independent semantics for mediating \nbetween the physical environment (typically, sen\u00adsors and actuators)and software tasks (typically, control \nlaw computations). This intermediate language, which has evolved into E code, o.ers several bene.ts. \nFirst, it sepa\u00adrates the platform-independent from the platform-dependent parts of the Giotto compiler, \nthus enabling reuse. The platform-independent part of the compiler generates E code from a Giotto program; \nits main purpose is to specify the timing of all interactions among software tasks, and be\u00adtween software \ntasks and the environment. The platform\u00addependent part of the compiler checks the time safety of the \nE code for a given platform with known WCETs and known scheduling scheme (a more ambitious compiler may \nattempt to synthesize a scheduling scheme that guarantees time safety). Second, E code permits the dynamic \nimple\u00admentation of Giotto: code can be patched at runtime, and whenever the controller switches mode, \nnew code can be linked at runtime. While E code has evolved from compiling Giotto, we have found it of \nconsiderable independent interest, as it illustrates the causalities between the underlying semantic \nprinciples, and ways to generalize them. One limitation of Giotto, for some applications, is its time-triggered \nnature: all Giotto time instants are ticks of an external clock, which, in high\u00adperformance control applications, \nminimizes jitter. By con\u00adtrast, E code may refer to environment events that are not clock ticks, such \nas sensor interrupts. Also, Giotto cannot refer to the completion time of tasks. For E code, it is a \nsmall and natural step to consider inputs not only from the environment processes, but inputs from both \nthe environ\u00adment and the software processes. Then the completion of a software task becomes an input \nevent, which may in.u\u00adence the execution of E code. As this generalization in\u00adtroduces E code references \nto platform time, environment determinedness is sacri.ced, and replaced by the weaker but symmetric \nnotion of input determinedness:the in\u00adputs from the environment processes (e.g., the sensor read\u00adings)and \nfrom the software processes (e.g., the scheduling scheme)together uniquely determine the outputs of the \nsoft\u00adware processes (e.g., the actuator settings). It is the study of concepts such as time safety and \nenvironment/input deter\u00adminedness that elevates the E machine from an intermediate language for compiling \nembedded code to a framework for evaluating embedded programming paradigms.  An Overview of the E Machine \nThe E machine is a mediator between physical processes and software processes: it interpretes E code, \nwhich supervises the execution of software processes (written in, say, C)in relation to physical events, \nsuch as clock ticks, and physical states, such as sensor values. The E machine has two input interfaces \nand one output interface. Environment inputs The physical processes communicate information to the E \nmachine through environment ports, such as clocks and sensors. Software inputs The application software \nprocesses, called tasks, communicate information through task ports to the E machine. Outputs The E machine \ncommunicates information to the physical processes and to the tasks by calling system processes, called \ndrivers,which writeto driverports. Logically, the E machine does not need to distinguish be\u00adtween environment \nand task ports; they are both input ports, while driver ports are output ports. A change of value at \nan input port is called an input event. Every input event causes an interrupt that is observed by the \nE machine and may initiate the execution of E code. E code, in turn, supervises the execution of both \ntasks and drivers. Tasks A task is a piece of application-level code which typ\u00adically implements a computation \nactivity. When in\u00advoked with arguments, a task computes and writes the results to task ports. The execution \nof a task requires a positive amount of real time, i.e., the results can\u00adnot be observed until at least \none input event happens (e.g., a clock tick, or a signal that indicates the com\u00adpletion of the task). \nA task can be preempted but has no internal synchronization points. Drivers A driver is a piece of system-level \ncode which typ\u00adically facilitates a communication activity. A driver may provide sensor readings as arguments \nto a task, or may load task results into actuators, or may pro\u00advide task results as arguments to other \ntasks. The execution of a driver satis.es the synchrony assump\u00adtion [6], that it can be performed in \nlogical zero time, i.e., before the next input event can be observed. In other words, interrupts that \nimplement input events are disabled during the execution of a driver. To protect IP, both tasks and drivers \nmay be given as binary executables; E code refers to tasks and drivers only through symbolic references. \nE code is interpreted system-level code that supervises the execution of a given set of tasks and drivers \nrelative to input events. For this purpose, E code has essentially three instructions. Call driver The \ncall instruction initiates the execution of a driver. As the driver is synchronous system-level code, \nthe E machine waits until the driver is .nished before interpreting the next instruction of E code. Schedule \ntask The schedule instruction hands a task to the operating system. Typically, the task is put into a \nready queue, from which the scheduler of the oper\u00adating system chooses tasks for execution according \nto some scheduling scheme. The scheduler is not under control of the E machine; like the physical environ\u00adment \nand the underlying hardware, it is external to the E machine and may or may not be able to sat\u00adisfy the \nreal-time assumptions of E code. Runtime real-time violations are due to a combination of fast physical \nenvironment, slow hardware, and ine.cient scheduling; they cannot be blamed on any single one of these \nfactors. However, they can be ruled out by a compiler that checks time safety. Future E code The future \ninstruction marks a block of E code for execution at some future time. It has two parameters: a trigger, \nwhich is a predicate that is eval\u00aduated with every input event; and the address of a block of E code, \nwhich is executed as soon as the trig\u00adger evaluates to true. In order to handle multiple ac\u00adtive triggers, \nthe future instruction puts the trigger\u00adaddress pair into a trigger queue. With each input event, all \ntriggers in the trigger queue are evaluated, and the .rst one to evaluate to true determines the next \nactions of the E machine. The E machine is a virtual machine. In an actual implemen\u00adtation of the E machine, \nE code need not be interpreted, but may be compiled into, say, C code, or even silicon. The dif\u00adference \nbetween E code and equivalent C code lies in the programming discipline imposed by E code. In particular, \nthe fact that E code relates to time strictly through the trigger queue makes time-safety analysis possible. \nMore\u00adover, the overhead incurred by E code, rather than opti\u00admized C code, is minimal, because for code \nthat supervises the timing and interaction of tasks, correctness (i.e., pre\u00addictability)dwarfs performance \nas the critical design issue, even in high-performance control applications. For example, we have found \nthat in helicopter control, the entire program contains less than 400 instructions of E code. A Simple \nExample with Two Periodic Tasks We present a highly simpli.ed version of the control pro\u00adgram for a model \nhelicopter built at ETH Z\u00a8urich [10]. Con\u00adsider the helicopter in hover mode m. There are two tasks, \nboth given in native code, possibly autogenerated from Mat\u00adlab/Simulink models: the control task t1, \nand the naviga\u00adtion task t2. The navigation task processes GPS input every pc di pa ps Figure 1: An example \nwith two periodic tasks 10 ms and provides the processed data to the control task. The control task reads \nadditional sensor data (not modeled here), computes a control law, and writes the result to ac\u00adtuators \n(reduced here to a single port). The control task is executed every 20 ms. The data communication requires \nthree drivers: a sensor driver ds, which provides the GPS data to the navigation task; a connection driver \ndi,which provides the result of the navigation task to the control task; andanactuatordriver da, which \nloads the result of the con\u00adtrol task into the actuator. The drivers may process the data in simple ways \n(such as type conversion), as long as their WCETs are negligible. There are two environment ports, namely, \na clock pc and the GPS sensor ps;two task ports, one for the result of each task; and three driver ports \nthe destinations of the three drivers including the ac\u00adtuator pa. Figure 1 shows the topology of the \nprogram: we denote ports by bullets, tasks by rectangles, drivers by dia\u00admonds, and triggers by circles. \nHere is a Giotto description of the program timing: mode m() period 20 { actfreq 1 do pa(da); taskfreq \n1 do t1(di); taskfreq 2 do t2(ds); } The actfreq 1 statement causes the actuator to be up\u00addated once \nevery 20 ms; the taskfreq 2 statement causes the navigation task to be invoked twice every 20 ms; etc. \nHere is the E code generated by the Giotto compiler: a1: call(da) a2: call(ds) call(ds) schedule(t2) \ncall(di) future(g, a1) schedule(t1) schedule(t2) future(g, a2) The E code consists of two blocks. The \nblock at address a1 is executed at the beginning of a period, say, at 0 ms: it calls the three drivers, \nwhich provide data for the tasks and the actuator, then hands the two tasks to the scheduler, and .nally \nactivates a trigger g with address a2. When the block .nishes, the trigger queue of the E machine contains \nthe trigger g bound to address a2, and the ready queue of the scheduler contains two tasks, t1 and t2.Now \nthe E machine relinquishes control, only to wake up with the next input event that causes the trigger \ng to evaluate to true. In the meantime, the scheduler takes over and assigns CPU time to the tasks in \nthe ready queue according to some scheduling scheme. When a task completes, the scheduler removes it \nfrom the ready queue. There are two kinds of input events, one for each envi\u00adronment port: clock ticks, \nand changes in the value of the sensor ps. The trigger g: pc . = pc + 10 speci.es that the E code at \naddress a2 will be executed after 10 clock ticks. Logically, the E machine wakes up at every input event \nto evaluate the trigger, .nds it to be false, until at 10 ms, the trigger is true. An e.cient implementation, \nof course, wakes up the E machine only when necessary, in this case at 10 ms. The trigger g is now removed \nfrom the trigger queue, and the associated a2 block is executed. It calls the sensor driver, which updates \na port read by task t2.There are two possible scenarios: the earlier invocation of task t2 may already \nhave completed, and is therefore no longer in the ready queue when the a2 block is executed. In this \ncase, the E code proceeds to put another invocation of t2 into the ready queue, and to trigger the a1 \nblock in another 10 ms, at 20 ms. In this way, the entire process repeats every 20 ms. The other scenario \nat 10 ms has the earlier invocation of task t2 still incomplete, i.e., in the ready queue. In this case, \nthe attempt by the sensor driver to overwrite a port read by t2 causes a runtime exception, called time-safety \nviolation.At 20 ms, when ports read by both tasks t1 and t2 are updated, and ports written by both t1 \nand t2 are read, a time-safety violation occurs unless both tasks have completed, i.e., the ready queue \nmust be empty. In other words, an execution of the program is time-safe if the scheduler ensures the \nfol\u00adlowing: (1)each invocation of task t1 at 20n ms, for n = 0, completes by 20n+ 20 ms; (2)each invocation \nof task t2 at 20nms completes by 20n+10 ms; and (3)each invocation of task t2 at 20n+10 ms completes \nby 20n+20 ms. Therefore, a necessary requirement for time safety is d1 +2d2 < 20, where d1 is the WCET \nof task t1,and d2 is the WCET of t2. If this requirement is satis.ed, then a scheduler that gives priority \nto t2 over t1 guarantees time safety. The E code implements the Giotto program correctly only if it is \ntime-safe: during a time-safe execution, the navigation task is executed every 10 ms, the control task \nevery 20 ms, and the data.ow follows Figure 1. Thus the Giotto compiler needs to ensure time safety when \nproducing E code. In or\u00adder to ensure this, the compiler needs to know the WCETs of all tasks and drivers \n(cf., for example, [5]), as well as the scheduling scheme used by the operating system. With this information, \ntime safety for E code produced from Giotto can be checked. However, for arbitrary E code and plat\u00adforms, \nsuch a check may be di.cult, and the programmer may have to rely on runtime exception handling. The time-safe \nexecutions of the E code example have an important property: assuming the two tasks compute deter\u00administic \nresults, for given sensor values that are read at the input port ps at times 0, 10, 20, ... ms, the actuator \nvalues that are written at the output port pa at these times are de\u00adtermined, i.e., independent of the \nscheduling scheme. This is because each invocation of the control task t1 at 20n ms operates on an argument \nprovided by the invocation of the navigation task t2 at 20n- 10 ms, whether or not the sub\u00adsequent invocation \nof t2,at 20n ms, has completed before the control task obtains the CPU. Time safety, therefore, ensures \nnot only deterministic output timing, but also de\u00adterministic output values; it guarantees predictable, \nrepro\u00adducible real-time code. This is made precise in Section 3. The helicopter may change mode, say, \nfrom hover to de\u00adscend, and in doing so, apply a di.erent control law. In this case, the control task \nt1 needs to be replaced by another task t1. . In Section 4, we show how to implement di.erent modes of \noperation using E code with control .ow instruc\u00adtions, and how E code can be changed dynamically, at \nrun\u00adtime, still guaranteeing determinism if no time-safety viola\u00adtions occur. This capability enables \nthe real-time program\u00adming of embedded devices that upload code on demand, of code that migrates between \nhosts, and of code patches. Let us summarize how the programming of the helicopter using Giotto and the \nimplementation using E code di.ers from conventional real-time software design. All executions of E code \nhappen at prede.ned instants of environment time, as speci.ed by the control model and, therefore, the \nGiotto program: the sensor is read every 10 ms, the result of the control task is written to the actuator \nport every 20 ms, etc. The compiler, by checking time safety, ensures that the program can be executed; \nthat is, the compiler matches en\u00advironment time against platform time. If time safety holds, then deterministic \ntiming and output behavior is guaran\u00adteed. Otherwise, platform performance (WCETs)or plat\u00adform utilization \n(scheduling)must be improved. Recompi\u00adlation supports code reuse on upgraded and di.erent plat\u00adforms. \nConventional real-time software design proceeds in the opposite direction: the programmer s model is \nthe plat\u00adform (e.g., priority-preemptive scheduling). For example, the actuator ports are typically written \nwhenever the con\u00adtrol task completes, which is an instant of platform time. Then, code validation is \nnecessary to gain con.dence that the application requirements are met (e.g., that the actuator port is \nupdated at least every 20 ms), and that the output jitter is acceptable. Code validation, however, is \nusually di.cult, .rst, because the code exhibits nondeterministic timing and output behavior, and second, \nbecause the ap\u00adplication requirements are, unlike time safety, nonuniform. If the application requirements \nare not satis.ed, platform performance or utilization needs to be improved. So pro\u00adgramming the platform \ndoes not necessarily guarantee a better success rate, but at the same time makes platform upgrades and \ncode reuse cumbersome. 2. DEFINITION OF THE E MACHINE The E machine mediates the timing and interaction \nbetween environment and software processes. The software processes fall into three categories: drivers, \ntasks, and triggers. The processes communicate via ports. Given a set P of ports, a P state is a function \nthat maps each port in P to a value. The set P is partitioned into three disjoint sets: a set PE of environment \nports,a set PT of task ports,and a set PD of driver ports. The read/write access of processes to ports \nis as follows: Environment Task Driver Ports Ports Ports Environment RW  R Tasks  RW R Drivers R R \nRW Triggers R R R Input triggers R R Environment triggers R The environment, task, and driver ports \nare updated by the physical environment, by tasks, and by drivers, respectively. All information between \nthe environment and the tasks .ows through drivers: environment ports cannot be read by tasks, and task \nports cannot be read by the environment. For example, a driver may read an environment port, such as \na sensor or a clock, and load the value into a driver port that is read by a task; another driver may \nread a task port and load the value into a driver port, such as an actuator, which is read by the environment. \nAn event is a change of valueat a port,say,at a sensor ps,which is observed by the E machine through \nan interrupt. Such an event interrupt can be characterized by a predicate, namely, p . s = ps,where ps \n. refers to the current sensor reading, and ps refers to the most recent previous sensor reading. De.nition \n1. A program of the embedded machine con\u00adsists of (1)a set P of program ports, (2)a set of drivers, a \nset of tasks, and a set of triggers, and (3)a set of addresses, and for each address, a .nite sequence \nof instructions. The instructions, discussed below, call drivers, apply schedul\u00ading services of the operating \nsystem to tasks, and handle interrupts through triggers. We do not prescribe any spe\u00adci.c control .ow \ninstructions, but rather view part (3) the E code of a program abstractly as a set of blocks, each with \nan address and a .nite sequence of instructions. We use a function Next(a)that returns for an instruction \nat address a the address of the next instruction; if there is no next instruction, then the function \nreturns bottom. This convention is consistent with any control .ow instructions, structured or unstructured, \nwhose choice is of practical im\u00adportance but entirely orthogonal to the issues discussed here. The instructions \nof E code do not manipulate data; all data is handled by drivers, tasks, and triggers, which can be im\u00adplemented \nin an arbitrary programming language, such as C. Abstractly, drivers and tasks are functions from ports \nto ports, and triggers are boolean functions (i.e., predicates) on ports. De.nition 2. A driver d consists \nof (1)a set P [d] . PD of driver ports, and a set I[d] . (PE .PT )of read environment and task ports, \nand (2)a function f [d]from the P [d] . I[d] states to the P [d] states. A task t consists of (1)a set \nP [t] . PT of task ports, and a set I[t] . PD of read driver ports, and (2)a function f[t]fromthe P [t] \n. I[t] states to the P [t] states. A driver computes on driver ports and may read from envi\u00adronment and \ntask ports; a task computes on task ports and may read from driver ports. Communication to and from a \ntask, like communication to and from the environment, is only possible through drivers. There is a fundamental \ndif\u00adference between drivers and tasks. A driver is nonpreempt\u00adable, atomic, single-threaded code; a task \nis single-threaded code that is operationally preemptable but logically atomic, without internal synchronization \npoints. Logically, a driver is assumed to execute instantaneously in zero time whereas the execution \nof a task takes time. Computation in zero time is called synchronous computation; computation that takes \ntime is called scheduled computation. Operationally, syn\u00adchronous computation is performed in kernel \ncontext with event interrupts disabled. The WCET of synchronous com\u00adputation (i.e., drivers)must be included \nin the administra\u00adtive overhead for an accurate schedulability analysis. Sched\u00aduled computation happens \nin user context with event inter\u00adrupts enabled. In order to validate the real-time behavior of E code \nthrough schedulability analysis, it is necessary to know the WCETs of scheduled computation (i.e., tasks). \nDe.nition 3. A trigger g consists of (1)a set P [g] . P of monitored ports, and (2)a predicate p[g] on \npairs of P [g] states, which evaluates to true or false over each pair (s, s )of P [g] states s and s \n.We require that p[g] evaluates to false if s = s . . The trigger g is an input trigger if P [g] . (PE \n.PT ); an environment trigger,if P [g] . PE . The state s is the state of the ports at the time instant \nwhen the trigger is activated.The state s . is the state of the ports at the time instant when the trigger \nis evalu\u00adated. We assume that all active triggers are evaluated at least at the rate of observed events. \nAn active trigger that evaluates to true may cause a reaction of the E machine. As drivers are executed \nin logical zero time, a trigger that reads driver ports can evaluate to true at the same logical time \ninstant at which the trigger is activated; such trig\u00adgers make possible synchronous reactive communication \nbe\u00adtween drivers [6]. Synchronous self-triggering is not possible with input triggers, which read only \nenvironment and task ports, nor with environment triggers, which read only envi\u00adronment ports. A program \nis input-triggered if all triggers are input triggers; environment-triggered, if all triggers are environment \ntriggers. While input-triggered programs can react to software events such as the completion of tasks, \nenvironment-triggered programs can react only to environ\u00adment events. An important special case of environment\u00adtriggered \nprograms are the time-triggered programs, whose triggers read only an external clock. For example, every \nprogram obtained from a Giotto source is time-triggered. A program con.guration tracks, besides the values \nof all ports, also the active triggers, and the tasks in the ready queue of the operating system. The \nactive triggers are kept in a FIFO queue, called trigger queue, accordingtotheir activation order. An \nactive trigger stays in the trigger queue until it evaluates to true, at which point it is removed from \nthe queue. The tasks under OS control are kept in a set, called task set, as the organization of the \nready queue (e.g., as a priority queue)is unknown to the E machine. A task enters the task set when it \nis released (i.e., handed over to the OS), and leaves the task set when it completes. De.nition 4. A \nprogram con.guration consists of (1)a P state s , called program state; (2)a queue of trigger bindings \n(g, a, s), called trigger queue,where g is a trigger, a is an address, and s is a P [g] state; and (3)a \nset of pairs (t, s), called task set,where t is a task and s is a P [t] . I[t] state. A trigger binding \n(g, a, s)is enabled if the trigger predicate p[g] evaluates to true over the pair (s, s )of P [g] states. \nThe con.guration c is input-enabling if the trigger queue contains no enabled trigger bindings; otherwise, \nc is input-disabling. An E machine instruction is similar to a machine code in\u00adstruction. It has a unique \nopcode and a .nite number of arguments, all of which can be represented by integers. Us\u00ading integers \nas arguments supports the portability of E code. There are only three fundamental instructions of the \nE ma\u00adchine. The control .ow e.ect of these three instructions is trivial: after executing any of the \nthree instructions, the E machine proceeds to the next instruction in the program. Similar to the execution \nof drivers, E code interpretation happens logically in zero time: E code interpretation is syn\u00adchronous \ncomputation, which takes place in kernel context. De.nition 5. An E machine instruction is one of the \nfol\u00adlowing: call(d),for a driver d; schedule(t), for a task t;or future(g, a), for a trigger g and an \naddress a.  0 0 4 8 10 10 1415 171820 20 Figure 2: Earliest-deadline-.rst (EDF) vs. time-slice scheduling \nof the tasks from Figure 1 The call(d)instruction invokes the binary code of the driv\u00ader d. The E machine \nwaits until the execution of d is .nished, and then proceeds to the next instruction. The schedule(t) \ninstruction marks the binary code of the task t for execution by inserting it into the task set. Then \nthe E machine imme\u00addiately proceeds to the next instruction. The future(g, a) instruction marks the E \ncode at address a for (possible)ex\u00adecution at the future time when the trigger g next evaluates to true. \nOperationally, the E machine appends the trigger binding (g, a, s)to the trigger queue, where s is the \ncurrent program state (this is necessary for evaluating the trigger in the future), and then proceeds \nto the next instruction. If there is no next instruction, then the E machine gives up control of the \nCPU and wakes up with each event, to eval\u00aduate all triggers in the trigger queue. If the trigger binding \n(g, a, s)is enabled, then it is removed from the trigger queue and the E code at address a is executed. \nIf several trig\u00adger bindings are enabled, then the corresponding blocks of E code are executed consistent \nwith the order of the trigger queue. In other words, the future(g, a)instruction is sim\u00adilar to binding \nan interrupt handler to an interrupt, where the trigger g de.nes the interrupt, and the E code at address \na de.nes the interrupt handler.   The Two-Task Example Revisited Consider Figure 2, which shows the \nexecution of the E code from the example in Section 1. We assume that the ini\u00adtial con.guration consists \nof a state s that sets the clock pc to -10 ms, a trigger queue with the single trigger binding (g, a1,s), \nand an empty task set. When the clock pc reaches 0 ms, the trigger g evaluates to true, the enabled trigger \nbinding is removed from the trigger queue, and the E ma\u00adchine starts executing the E code at a1. The \n.rst three call instructions execute drivers. The schedule(t1[20])instruc\u00adtion schedules task t1 by inserting \nit into the task set. The term [20] is an example of an E machine annotation,which is handed to the scheduler \nalong with the task. E machine annotations describe information that may be provided by the programmer, \nor by the compiler of a high-level language such as Giotto, to the system scheduler. For the E machine, \nannotations have no meaning, but the scheduler may in\u00adterpret annotations. In particular, an earliest-deadline-.rst \n(EDF)scheduler interpretes the term [20] as the relative deadline of task t1 with respect to the clock \npc. The sub\u00adsequent schedule(t2[10])instruction inserts task t2 into the task set. The EDF scheduler \nassigns a priority to t2 which is higher than the priority of t1,because t2 has the earlier rel\u00adative \ndeadline of 10 ms. The last instruction, future(g, a2), ensures that the E code at a2 will be executed \nat 10 ms. The execution of the E code at a1 is now .nished. The result is a new con.guration, which consists \nof a state s . with new values for task and driver ports, the single trigger binding (g, a2,s )in the \ntrigger queue, and tasks t1 and t2 in the task set. Finally, the E machine enables all interrupts and \nleaves the kernel context. Now the scheduler, which is independent of the E ma\u00adchine, takes over. The \nupper half of Figure 2 shows the resulting timeline. The scheduler invokes task t2,which is the highest-priority \ntask. Task t2 executes in user context and completes after, say, 4 ms, when it is removed from the task \nset. Then task t1 executes. If task t1 executes for, say, 10 ms, it is preempted after 6 ms by the trigger \ng.At 10 ms, the E machine disables event interrupts and executes the E code at a2 by .rst calling driver \nds to provide a new value from the sensor port ps to task t2. Calling the drivers da or di at this time \nwould result in a time-safety violation, because both drivers access ports that are read or written by \ntask t1, which has not yet completed; see Section 3. The schedule(t2[10])instruction inserts task t2 \ninto the task set. Since the deadlines of both tasks in the task set are equal, the EDF scheduler may \nassign a priority to t2 that is lower or higher than the priority of t1. In the .gure, we assume the \npriority of t2 to be lower than the priority of t1.The .nal future(g, a1)instruction marks the E code \nat a1 for execution at 20 ms. The new E machine con.guration has the single trigger binding (g, a1,s \n),where s . is the new state, and tasks t1 and t2 in the task set. Now the sched\u00aduler resumes the execution \nof task t1, because it has a higher priority than t2.Task t1 executes for another 4 ms and com\u00adpletes. \nThen task t2 executes, say, this time only for 3 ms. Then the system is idle for the next 3 ms. At 20 \nms, the E machine executes again the E code at a1,thus closing an in.nite loop of periodic invocations \nof both tasks. Now consider the timeline of the scheduled computation in the lower half of Figure 2. \nThis shows the task execu\u00adtion according to a time-slice scheduler instead of an EDF scheduler. The purpose \nof this example is to demonstrate that the E machine is independent of the scheduling scheme. The E code \nis executed exactly the same way as before. The only di.erence is how the tasks are executed between \nthe synchronous blocks of E code. The time-slice scheduler ig\u00adnores the annotations. Each task gets a \ntime slice of 4 ms, and tasks are executed in the order of their insertion into the task set in a round-robin \nfashion. Note that the second slice of task t1 is preempted by a trigger. Also note that for given sensor \nreadings at 0 ms and 10 ms, the value loaded by the actuator driver into pa at 20 ms is the same no mat\u00adter \nwhich scheduler is used; that is, the output behavior of the program is deterministic. This is the main \nproperty of E code; see Section 3. Operational Semantics We de.ne the semantics of E code operationally \nusing a pseudo-code description of the E machine. Algorithm 1 shows the main loop of the machine as it \nexecutes a given program. Initially, the trigger queue contains a single trigger binding, and the task \nset is empty. After entering the main loop, the machine waits for environment and task ports to change \ntheir values, i.e., for the occurrence of one or more events that may enable input triggers. While the \nmachine waits for input events, scheduled computation may be per\u00adformed, i.e., the scheduler has control \nof the CPU. Waiting for input events is implemented using event interrupts. The occurrence of an input \nevent wakes up the machine, which immediately disables all event interrupts (thus it is still pos\u00adsible \nfor low-level interrupts to preempt the machine, as long as they do not interfere with the triggering \nmechanism of the machine). After the while loop, before the machine loops back to wait for new input \nevents, all event interrupts are enabled again and the scheduler is invoked. The main loop is executed \nad in.nitum. Algorithm 1 The Embedded Machine loop wait for input event disable trigger-related interrupts \nwhile there is an enabled trigger in TriggerQueue do (g, a, s):= GetFirstEnabledTriggerBinding(TriggerQueue) \nTriggerQueue := RemoveFirstEnabledTriggerBinding(TriggerQueue) ProgramCounter := a invoke Algorithm 2 \nend while enable trigger-related interrupts invoke system scheduler on TaskSet end loop The E machine \nruns through the while loop of Algorithm 1 as long as there are enabled trigger bindings in the trigger \nqueue, each time executing a block of E code that is bound to an enabled trigger. The termination of \nthe while loop is guaranteed for input-triggered programs such as E code gen\u00aderated from Giotto. In an \ninput-triggered program, newly activated triggers are initially disabled and can only be en\u00adabled by \nenvironment or task activity. In more general pro\u00adgrams with arbitrary triggers, it is possible that \ncalling a driver enables a trigger. In this case an explicit termina\u00adtion proof for the while loop is \nnecessary. Synchronous self\u00adtriggering among E code blocks corresponds to the signal\u00ading mechanism of \nEsterel [2], or undelayed data dependency in Lustre [7]. For synchronous reactive languages such as Esterel \nand Lustre, an explicit termination proof of syn\u00adchronous computation is necessary (typically by ensuring \nthe existence of .nite .xed points). Each block of E code is interpreted by Algorithm 2. In the while \nloop of Algorithm 2, the machine fetches the cur\u00adrent instruction from the program, decodes and executes \nthe instruction, and then determines the address of the next instruction. This loop terminates, because \nevery block of E code is required to have only a .nite number of instruc\u00adtions with sequential control \n.ow. This is true despite the fact that it is often convenient to use control .ow instruc\u00adtions such \nas absolute or conditional jumps in E code, for example, in the generation of E code from Giotto programs \nwith mode switching; see Section 4. Later, we will also in\u00adtroduce additional instructions for manipulating \nthe task set (such as terminating a task)and trigger queue (such as re\u00admoving a trigger binding), but \nthese have no fundamental impact on the operation of the E machine. Algorithm 2 The E Code Interpreter \nwhile ProgramCounter .do = i := GetInstruction(ProgramCounter) if call(d)= i then ProgramState(P [d]):= \nf[d](ProgramState(P [d] .I[d])) else if schedule(t)= i then TaskSet := TaskSet .{(t, ProgramState(P [t] \n.I[t]))}else if future(g, a)= i then TriggerQueue := TriggerQueue .(g, a, ProgramState(P [g])) end if \nProgramCounter := Next(ProgramCounter) end while The execution of an E machine program yields a trace. \nFrom one con.guration of a trace to the next, there is either an environment event, or a software event \n(i.e., the completion of a task), or the E machine executes one block of E code. The third possibility \nE code execution has precedence as long as there are blocks of E code with enabled triggers. De.nition \n6. A program trace is an in.nite sequence of con.gurations such that for any two adjacent con.gurations \nc and c , one of the following holds: Environment event c is input-enabling, and c . di.ers from c in \nthe values of environment ports. Task completion c is input-enabling, and c . di.ers from c in the values \nof the task ports P [t] for some task t, and in the task set: the task set of c contains a pair (t, s), \nfor some state s, the task set of c . results from c by removing the pair (t, s), and the values of the \ntask ports P[t]in c . result from applying the function f[t] to state s. Ecodeexecution c is input-disabling, \nand c . di.ers from c in the values of driver ports, in the trigger queue, and in the task set: if (g, \na, s)is the .rst enabled trig\u00adger binding in the trigger queue of c,then c . is ob\u00adtained from c by .rst \nremoving (g, a, s)from the trigger queue, and then executing Algorithm 2 with program counter a. In this \ncase, we say that the E code at address a is executed at con.guration c. The input part of a program \ntrace is the projection of the trace to values for environment and task ports; the output part is the \nprojection to values for driver ports; the environ\u00adment part is the projection to values for environment \nports. 3. PROPERTIES OF E CODE The bene.ts of using E code are due to its strong theoret\u00adical properties: \ntime safety is a condition which is satis.ed if all real-time requirements are met on a platform, and \nde\u00adterminism is a consequence which ensures that the output behavior of E code is predictable.  Time \nSafety A compiler that generates E code is not satis.ed with ev\u00adery trace that may result from interpreting \nthe code using Algorithm 2. Rather, the compiler expects su.cient perfor\u00admance from the platform so that \nthe computation of a task always completes before drivers access (read or write)ports that are also accessed \nby the task, and before another invo\u00adcation of the task is scheduled. A trace that satis.es these conditions \nis called time-safe, because it meets all timing requirements of the source (e.g., Giotto)program. De.nition \n7. A program trace is time-safe if for every con.guration c, if the task set of c contains the pair (t, \ns), then E code that is executed at con.guration c must obey the following three conditions. (1)For each \ncall(d) in\u00adstruction, P [d] n I[t]= \u00d8; that is, no driver updates the read driver ports of t. (2)For \neach call(d) instruction, I[d] n P [t]= \u00d8; that is, no driver reads the task ports of t. (3)For each \nschedule(t) instruction, P [t] n P [t]= \u00d8;that is, no scheduled task accesses the task ports of t. The \nE machine throws a runtime exception if any of the above conditions is violated; we will discuss the \nexception handling below. In order to avoid runtime exceptions, it must be shown that the program is \ntime-safe for the envi\u00adronment and the platform, i.e., that all program traces that can occur on the \ntarget platform in the target environment are time-safe. Proving time safety requires a schedulability \nanalysis based on the WCETs of all drivers, tasks, and trig\u00adgers, and if the program is not time-triggered, \nalso requires assumptions about the frequency of input events. As an example, recall the Giotto program \nfrom Section 1, which, like all Giotto programs, is time-triggered. For single-CPU platforms with EDF \nscheduling, the Giotto compiler shows time safety in two steps. First, given the WCETs of all drivers \nand triggers, the compiler computes the WCETs of all E code blocks. Suppose that the a1 and a2 blocks \nhave a WCET of 1 ms each, including the overhead for context switching. This leaves 18 ms CPU time per \n20 ms real time for scheduled computation. Second, given the WCETs of all tasks, and having derived the \nrelative deadlines for all task invocations from the Giotto source (relative deadline 20 for each schedule(t1)instruction, \nand relative deadline 10 for each schedule(t2)), the compiler uses an EDF schedulabil\u00adity test to show \nthat all task invocations complete on time. For instance, assuming a WCET of 10 ms for t1 and of 4 ms \nfor t2, the EDF schedule is feasible and achieves a theoretical CPU utilization of 100%. In addition, \nas WCET assumptions may be wrong, the Giotto compiler generates E code for handling time-safety Algorithm \n3 The E Code Interpreter with Exceptions while ProgramCounter . do = i := GetInstruction(ProgramCounter); \nE := \u00d8 if call(d)= i then E := {e | (t, ,e) . TaskSet: P [d] n I[t] . = \u00d8. I[d] n P [t]= \u00d8} if E = \u00d8 \nthen ProgramState(P [d]):= f [d](ProgramState(P [d] . I[d])) else if schedule(t, e)= i then E := {e . \n| (t. , ,e ) . TaskSet: P [t] n P [t] . = \u00d8}if E = \u00d8 then TaskSet := TaskSet .{(t, ProgramState(P [t] \n. I[t]),e)}else if future(g, a)= i then TriggerQueue := TriggerQueue . (g, a, ProgramState(P [g])) end \nif while E . = \u00d8 do (ProgramCounter, E):= ChooseException(E) invoke Algorithm 3 end while ProgramCounter \n:= Next(ProgramCounter) end while violations. Algorithm 3 shows the E code interpreter of Al\u00adgorithm \n2 enhanced with exception handling. For excep\u00adtion handling, a second argument is added to the schedule \ninstruction: suppose that the task t is scheduled by the in\u00adstruction schedule(t, e),where e is an address. \nThe block of E code at address e is the exception handler, and its address is recorded in the task set. \nIf, before t completes, the ports read by t are updated or the ports written by t are read by a driver \nor another task, then the E machine discards the in\u00adstruction that causes the exception and jumps to \naddress e. The task t is not terminated, but for the case that termina\u00adtion is desired, we add an instruction \nterminate(t)to the instruction set of the E machine, which removes t from the task set. This may be the \n.rst instruction of the exception handler. When the exception handler .nishes, control .ow returns to \nthe instruction that follows the instruction that caused the exception. A single call or schedule instruction \nmay cause multiple exceptions, e.g., because a driver may read ports from multiple tasks in the task \nset. We do not specify how simultaneous exceptions are prioritized; this is done by the function ChooseException. \nAs runtime excep\u00adtions can occur inside an exception handler, Algorithm 3 is invoked recursively, so \nas to implicitly maintain a stack of return addresses. In the case of Giotto, the compiler generates \nan exception handler for each task. If a runtime exception is caused by an instruction that has a con.ict \nwith task t in the task set, then the associated exception handler terminates t and restores the most \nrecent valid values to the task ports of t, that is, the values that preceded the terminated invocation \nof the task. This requires additional ports and drivers. In our example, the new driver dj,for j =1, \n2, stores the re\u00adsult of task tj in a new driver port pj .In case of a runtime exception involving task \ntj , the exception handler, at ad\u00address ej ,terminates tj and calls the new driver d. j ,which restores \nthe value of pj to the task port of tj before the program proceeds. The generated E code below produces \nexactly the same output behavior for time-safe traces as the original E code from Section 1: a1: call(d1) \na2: call(d2) call(d2) call(ds) call(da) schedule(t2,e2) call(ds) future(g,a1) call(di) schedule(t1,e1) \nschedule(t2,e2) future(g,a2) e1: terminate(t1) e2: terminate(t2) call(d. 1) call(d2. ) However, if \na time-safety violation occurs, then the new E code handles the associated exception. Suppose that the \na1 block is executed when task t1 has not yet completed. The attempt to execute d1 throws a runtime exception, \nwhich invokes the E code at address e1.After t1 is terminated, the driver d. 1 restores the task ports \nof t1, and execution pro\u00adceeds with the call(d2)of the a1 block. Determinism Figure 2 gave an example \nwhere di.erent scheduling schemes (EDF and time slicing)lead to the same output behavior. For either \nscheduling scheme, the program interacts with the environment at the same constant rate of 10 ms and \nproduces the same actuator settings. We now show that this is a general property of E machine programs, \nas long as the platform maintains time safety (i.e., there are no runtime exceptions). Recall that by \ninput of the E machine, we refer to both environment and task ports (e.g., input-triggered means triggered \nby events on these ports), by environment we refer to the environment portion of the input, and by output, \nto the driver ports (which includes the actuators). De.nition 8. A program is input-determined if, whenever \ntwo time-safe program traces agree on the initial con.gura\u00adtions and input parts, then they agree on \nthe output parts as well. A program is environment-determined if, whenever two time-safe traces agree \non the initial con.gurations and environment parts, then they agree on the output parts. It is easy to \nsee that all E machine programs are input\u00addetermined. However, input determinedness is a rather weak \nproperty. Input-determined programs are deterministic only with respect to a given behavior of all environment \nand soft\u00adware processes. In order to decouple environment and soft\u00adware, a program must be environment-determined, \ni.e., inde\u00adpendent of the real-time behavior of the software processes. Fact. Every program is input-determined. \nEvery environ\u00adment-triggered program is environment-determined. For example, being environment-triggered, \nthe program of Figure 2 is environment-determined, i.e., it produces the same actuator settings independently \nof the execution order of the tasks, as long as the platform maintains time safety. In fact, as Giotto \nis time-triggered, all E code generated from Giotto sources is environment-determined, and so is E code \ngenerated from more general sources, whose triggers monitor arbitrary environment events. On the other \nhand, an input\u00adtriggered program is in general not environment-determined, because it may trigger on \nthe completion of a task execution. Thus, depending on the detailed performance (WCETs)and scheduling \nof the platform, such a program may exhibit very di.erent output behaviors. Environment determinedness \ncrucially depends on the task model of the E machine. Suppose that tasks were allowed to communicate \nwith each other directly, without going through drivers. In our example, this would mean that task t1 \nreads the task port of t2, instead of calling the connection driver di. Now consider the resulting timelines. \nIn the case of EDF, the .rst invocation of t1 wouldreadthe result of theinvo\u00adcation of t2 that .nishes \nat 4 ms. By contrast, in the case of time slicing, t1 would start .rst and thus read the initial value \nof the task port of t2. As a consequence, the result of the .rst invocation of t1, which determines the \nactuator setting at 20 ms, would be di.erent for the two scheduling schemes. Environment determinedness \nalso depends on the instruction set of the E machine. For example, instead of having a compiler ensure \ntime safety based on schedulabil\u00adity analysis, we could obtain time safety trivially by using the terminate(t)instruction \nto terminate any task t before we interact with t. All traces of a program written in this way are time \nsafe, but the program, even if environment\u00adtriggered, would not be environment-determined, because the \nresult of a terminated task depends on how long the task has executed. Time Liveness Time safety means \nthat all .nite trace pre.xes satisfy the intended real-time constraints of E code. There still may be \nundesirable in.nite traces, however, where the E machine is in.nitely faster than the physical environment. \nThis is because, even in the absence of in.nite loops inside individ\u00adual blocks of E code, two di.erent \nblocks of E code may activate and enable each other without intervening input events. A trace without \nsuch in.nite zero-time behavior is called time-live. De.nition 9. A program trace is time-live if for \nevery con\u00ad.guration c,if c is input-disabling, then it is followed by some later con.guration c . that \nis input-enabling. Fact. Every trace of an input-triggered program is time\u00adlive. Time liveness, therefore, \nis not an issue for E code generated from Giotto, whose triggers monitor only clocks, nor for E code \ngenerated from any other source whose triggers do not look at driver ports. However, E code generation \nfrom a synchronous reactive language such as Esterel may, like any Esterel compilation, require nontrivial \nproofs of time liveness. The execution of a time-live program, i.e., a program whose traces are all time-live, \nmay require a trigger queue of un\u00adbounded size, which is not desirable in practice. (By con\u00adtrast, the \nsize of the task set is always bounded by the num\u00adber of tasks.)For example, two consecutive future instruc\u00adtions \nto the same block of E code may set o. a process that doubles the size of the trigger queue with each \ninput event. A similar problem may arise in the presence of control .ow instructions such as for loops: \nwhile always .nite, the exe\u00adcution of a single block of E code may take more time with each new input \nevent, because the block may consist of a ever increasing number of instructions. De.nition 10. A program \ntrace is bounded time-live if there exists an integer k such that for every con.guration c, if c is input-disabling, \nthen (1) c is followed by at most k input-disabling con.gurations, and (2)the trigger queue of c contains \nat most k entries. Note that bounded time liveness implies time liveness. From Giotto programs, one can \nalways generate E code that is bounded time-live. For E code generation from, say, Esterel, proving bounded \ntime liveness is necessary to ensure that the delay of executing any synchronous reaction is shorter \nthan the time between any two events that can trigger a synchronous reaction. For a .xed bound k, bounded \ntime liveness can be enforced at runtime by exception handling, similar to the handling of time-safety \nviolations. For exam\u00adple, a future instruction may cause a runtime exception if it attempts to create \na new trigger binding when the trigger The E code generation from multi-mode Giotto programs illustrates \nthe use of conditional jumps. Consider the fol\u00adlowing timing part of a Giotto program with two modes, \nma (representing the helicopter in hover mode)and mb (de\u00adscend mode): start ma { mode ma() period 20 \n{ actfreq 1 do pa(da); exitfreq 1 do mb(cb); taskfreq 1 do t1(di); taskfreq 2 do t2(ds); } mode mb() \nperiod 20 { actfreq 1 do pa(da); exitfreq 1 do ma(ca); queue already contains k entries. 4. E CODE GENERATION \nAND LINKING Generating code from standard high-level programming lan\u00adguages is an optimization problem \nto reduce the time and space requirements of the code. Generating E code is dif\u00adferent in the sense that \nthe time and space requirements of E code are usually negligible compared to the e.ciency requirements \nof the task code, even on complex systems (d );i1 taskfreq 1 do t taskfreq 2 do t2(ds); }} The program \nbegins by executing mode ma, which is equiv\u00adalent to the (single)mode m of the Giotto program from Section \n1 except for the mode switch to mode mb.A mode switch in Giotto has a frequency that determines at which \nrate an exit condition is evaluated. The exit condition cb of mode ma is evaluated once every 20 ms. \nIf cb evaluates to true, then the program switches to mode mb,which is ma except that task t1 replaces \ntask t1. Task t similar to mode such as a helicopter .ight-control system. For example, we need less \nthan 400 instructions of E code for the ETH heli\u00adcomputes a di.erent control law on the same ports 1 \ncopter. However, it is essential to guarantee the correct in\u00adtegration of synchronous and scheduled computation \ni.e., time safety and to ensure bounded E code execution i.e., bounded time liveness. When generating \nE code from Giotto, bounded time liveness can be guaranteed by con\u00adstruction, but ensuring time safety \nrequires explicit proof. By contrast, when generating E code from synchronous re\u00adactive languages such \nas Esterel or Lustre, we need only the call and future instructions, but not the schedule in\u00adstruction, \nbecause these languages do not explicitly support scheduled computation. Therefore, time safety is trivial, \nbut achieving bounded time liveness may require proof [6]. Also, for code generation from synchronous \nreactive languages, more .exibility on the manipulation of the trigger queue may be necessary than is \no.ered by the presented, minimal de.nition of the E machine. Useful additions to the instruc\u00adas t1. The \nmode switch back to ma evaluates the exit con\u00addition ca also once every 20 ms. In order to express mode \nswitching in E code, we use a conditional branch instruction if(c,a). The .rst argument cis a condition, \nwhich is a pred\u00adicate on some ports. The second argument a is an E code address. The if(c,a)instruction \nevaluates the condition c synchronously (i.e., in logical zero time), similar to driver calls, and then \neither jumps to the E code at address a (if c evaluates to true), or proceeds to the next instruction \n(if c evaluates to false). Here is the E code that implements the above Giotto program: a1: call(da) \na3: call(da) call(ds) call(ds) call(di) call(di) schedule(t2[10],e2) schedule(t2[10],e2) 3) schedule(t1[20],e1) \n1) schedule(t if(cb,a if(c a,a tion set of the E machine include the cancel(a)instruction, a1: 3: 1[20],e1) \nfuture(g,a2) future(g,a4) a which removes all trigger bindings with the address a from the trigger queue, \nand the future(true,a)instruction, which a2: call(ds) a4: call(ds) marks the E code at address a for \nexecution after all cur\u00ad schedule(t2[10],e2) schedule(t2[10],e2) rently enabled trigger bindings have \nbeen processed. future(g,a1) future(g,a3)  Compiling Giotto The two E code blocks in the left column \nimplement A Giotto program consists of a functionality part and a mode ma; the two blocks on the right \nimplement mb.The timing part. The functionality part contains port, driver, and task declarations, which \ninterface the Giotto program to a functionality implementation, typically written in C. The Giotto compiler \ngenerates so-called functionality wrap\u00adpers parameter-less procedures for each driver and task implementation, \nand stores the wrappers in a table simi\u00adlar to a symbol table. A wrapper calls the corresponding implementation \nwith the proper arguments. The E code, however, refers to the wrapper using only its table index, a portable \ninteger value. From the timing part of the Giotto program the compiler generates annotated E code, where \neach schedule instruction is annotated with the relative deadline of the scheduled task. exception handlers \nfor the three tasks (e1, e1,and e2)are omitted. Note that, no matter which conditional branches are taken, \nthe execution of any block terminates within a .\u00adnite number of E code instructions. This concludes the \n.rst, platform-independent phase of the Giotto compiler. The second, platform-dependent phase of the \nGiotto com\u00adpiler performs a time-safety check for the generated E code and a given platform. For single-CPU \nplatforms with WCET information and an EDF-based scheduling scheme, and for the simple code generation \nstrategy illustrated in the ex\u00adample, the time-safety check is straightforward. For dis\u00adtributed platforms, \ncomplex scheduling schemes, or complex code generation strategies, this, of course, may not be the case. \nThe code generation strategy has to .nd the right with an empty trigger queue and an empty task set. \nThus balance between E code and E machine annotations. An the initial cancel(a3)instruction has no e.ect. \nThe .nal extreme choice is to generate E code that at all times main-future(hb,a3)instruction activates \na new trigger hb and tains a singleton task set, which makes the scheduler s job binds it to the still \nunknown address a3. The trigger predi\u00ad trivial but E code generation di.cult. The other extreme is to \nschedule tasks as early as possible, with precedence an\u00adnotations that allow the scheduler to order task \nexecution correctly. This moves all control over the timing of soft\u00adware events from the code generator \nto the scheduler. In other words, the compiler faces a trade-o. between static (E machine)scheduling \nand dynamic (RTOS)scheduling. Our strategy, which schedules tasks and computes deadlines according to \nthe logical semantics of the Giotto source, chooses a compromise that suggests itself for control appli\u00adcations. \nTo achieve controller stability and maximal perfor\u00admance, it is often necessary to minimize the jitter \non sensor readings and actuator updates. This is accomplished by gen\u00aderating separate, time-triggered \nblocks of E code for calling drivers that interact with the physical environment. In this way, the time-sensitive \nparts of a program are executed sep\u00adarately [12], and for these parts, platform time is statically at \n20 ms if the condition cb evaluates to true (ha is de.ned analogously using condition ca). At 10 ms, \nthe E machine executes the a2 block. The future(g, a1)instruction ap\u00adpends the trigger g to the trigger \nqueue after the trigger hb, which is still in the queue. At 20 ms, if cb is not true, then hb is not \nenabled and thus skipped, but g is enabled, which causes the a1 block to be executed again. Now the .rst \ncancel(a3)instruction removes the hb trigger from the queue. On the other hand, if cb is true, then the \nE machine attempts to execute instead the a3 block. As the a3 block is not available, the E machine starts \nthe loader and linker to retrieve it. This overhead needs to be taken into account is(cateof hpb by time-safety \nanalysis. Once the E code is available, the E machine begins by executing the cancel(a1)instruction, \nwhich removes the g trigger from the queue. The rest of the E code at a3 and a4 is analogous to the E \ncode at a1 + 20) . cb,thatis, hb becomes enabled c = pc matched, at the E code level, to environment \ntime as closely and a2,except thattask t 1 (whose code can also be loaded as possible. On the other hand, \nfor the time-insensitive parts of a program, the scheduler is given maximal .exibility.  Dynamic Linking \nSoftware modularization is an important concept in the non\u00adreal-time world for improving software reusability \nand reduc\u00ading software complexity. Software modularization requires the use of symbolic references in \nexecutable code rather than direct references. Resolving symbolic references at com\u00adpile time and runtime \nis called linking and dynamic linking, respectively. Since E code uses only symbolic references, E code \n(and tasks, drivers)can be linked statically as well as dynamically. For an example of static linking, \nthe two columns of E code that are generated for the two modes of the Giotto program above can be compiled \nseparately and then linked to a com\u00adplete E code executable. For dynamic linking, we leverage the dynamic \nnature of the trigger queue by maintaining trig\u00adger bindings to unloaded E code in the queue. This approach \nrequires control over the queue as provided by the cancel(a) instruction, which removes all trigger bindings \nwith the ad\u00address a from the trigger queue. Hence a cancel instruc\u00adtion may negate the e.ect of several \nfuture instructions. Consider the following program (annotations and exception handling are omitted), \nwhich implements the dynamic load\u00ading and linking of the two Giotto modes ma and mb during helicopter \n.ight: a1: cancel(a3) a3: cancel(a1) call(da) call(da) call(ds) call(ds) call(di) call(di) schedule(t1) \nand linked dynamically)replaces task t1.  Current E Machine Implementations We have three implementations \nof the E machine. The sim\u00adplest one is written in C for Lego Mindstorm robots using the open-source LegOS \noperating system. The interesting aspect of this implementation is that it is a kernel patch, which makes \nthe E machine part of the kernel, rather than the highest-priority thread outside the kernel. The E ma\u00adchine \nimplementation for the ETH helicopter [10] is written in Oberon using a custom-designed RTOS on a StrongARM \nembedded processor. The interesting feature of this imple\u00admentation, besides high performance, is that \ntasks are imple\u00admented as subroutines rather than as coroutines [13]. In this case, the E machine is \nan interrupt handler bound to a real\u00adtime clock. Preemption works through reentrant interrupts. The third \nimplementation is in C under Linux using POSIX threads and semaphores. The E machine and each task runs \nin its own thread. Each task thread runs at a lower priority than the E machine thread and uses a unique \nsemaphore on which it waits until the E machine signals the semaphore. For example, upon executing a \nschedule(t)instruction, the E machine signals the semaphore of the thread that imple\u00adments task t.When \nt completes, the thread loops back and waits on the semaphore for the next schedule(t)instruc\u00adtion. The \ngoal of the Linux implementation is to port it to the RTOS VxWorks, which also features POSIX threads, \nhowever, with real-time guarantees. The Linux version also includes a dynamic loader and linker for the \nbinary format of E code. Moreover, it features a distributed E machine im\u00adplementation that runs interacting \nE machines on each host of a distributed system. The hosts communicate using the UDP protocol on BSD \nsockets. The distributed version is schedule(t1) schedule(t2) schedule(t2) future(g, a2) future(g, a4) \nfuture(hb,a3) future(ha,a1) also supported by the Giotto compiler, which can generate E code separately \nfor each E machine. The goal is again to port the implementation to a network with real-time guar\u00ad a2: \ncall(ds) a4: call(ds) antees, such as a time-triggered bus. schedule(t2) schedule(t2) future(g, a1) future(g, \na3) 5. CONCLUSION Suppose that only the E code in the left column is currently E code is predictable, \nportable, hard real-time code. E code loaded in the E machine. We begin by executing the a1 block is \nhard real-time, because it relates to environment (physi\u00adcal)time, rather than platform (CPU)time; this \nsimpli.es code validation at the expense of code generation. E code is predictable, because the timing \nand behavior of a program depends only on the external inputs; there are no internal race conditions. \nE code is portable, because it is indepen\u00addent of the platform, in particular, of the scheduler. The \ncombination of these attributes is made possible through the notion of time safety: E code is time-safe \nif its tim\u00ading requirements can be met on the chosen platform. In some cases, such as Giotto source programs, \ntime safety can be checked statically, by the compiler; in other cases, time\u00adsafety violations are handled \ndynamically, by the runtime system. The problem of real-time programming is to de.ne ab\u00adstractions that \ncapture the interaction between physical pro\u00adcesses and software processes. Two major research commu\u00adnities \nhave approached this problem from di.erent direc\u00adtions: the synchronous reactive language community has \nstudied zero-delay synchronous computation [6], and the real-time systems community has focused on the \ntheory of scheduled computation [4] and corresponding programming languages [3]. Both .elds have had \na big impact on the de\u00adsign of the E machine, which attempts to bring together the concepts of synchronous \nand scheduled computation. Syn\u00adchronous computation relates software processes to physical processes \nby modeling software processes as instantaneous reactions to physical stimuli. The main challenge is \nto prove the analogue of time liveness, i.e., the existence of .nite re\u00adactions [2]. However, large blocks \nof synchronous computa\u00adtion naturally exhibit delayed reactivity, which may deteri\u00adorate the determinism \nof the synchronous model. Real-time scheduling theory, on the other hand, relates software pro\u00adcesses \nto physical processes by imposing a constraint system of release times and deadlines on the software \nprocesses. The main challenge is to prove the analogue of time safety, i.e., the existence of a feasible \nschedule. However, the arrange\u00adment of computation according to a schedule that depends on the behavior \nof both physical and software processes leads to an inherently nondeterministic model. From the E machine \nperspective, the synchronous reac\u00adtive language community deals with organizing a time-live trigger queue, \nand the real-time systems community wor\u00adries about scheduling a time-safe task set. This can be seen \nmore clearly from various intermediate languages that have been proposed for synchronous and for scheduled \ncompu\u00adtation. Examples from the synchronous realm include the automata-based portable object code [9] \nfor Esterel and Lus\u00adtre, and halt point graphs [11] for Esterel. Examples from the scheduled realm include \nthe abstract machines JAM and BEAM for code generation from the functional real-time language Erlang \n[1]. Each of these formalisms is richer than theE machineinsomerespects, as they permit more gen\u00aderal \nmanipulations of the trigger queue, or of the task set, or more general control .ow and data handling. \nThe E ma\u00adchine attempts to exploit and, at the same time, restrict the possibilities in both the synchronous \nand the scheduled realm by identifying a set of primitives that (1)guarantee strong determinism properties \nof the code and (2)are su.\u00adciently rich to be useful in practice. An important practical question raised \nby the E machine asks which parts of em\u00adbedded software are best modeled by synchronous compu\u00adtation, \nand which by scheduled computation. For example, E code generated from Giotto uses synchronous computa\u00adtion \nto implement the data transport (I/O)from and to the physical system, and between software processes, \nwhile time-consuming data computation is implemented as sched\u00aduled computation. (In a distributed Giotto \nsystem, data communication across networks with nonnegligible latency is also implemented as scheduled \ncomputation.) It should be noted that Wirth already emphasized in 1977 the value of separating logical \nprograms from physical plat\u00adforms to obtain a discipline of real-time programming [12]. He suggested \nthat the time-dependent program parts, i.e., the blocks of synchronous computation, are few, simply structured, \nand without loops with an unknown number of repetitions in order to maintain correctness independently \nfrom the execution time of the non-real-time code, i.e., the units of scheduled computation. In particular, \nhe proposed a ban on the notion of interrupt unless interrupts can be ignored in considerations about \na system s computational state and can be con.ned to timing considerations only. Acknowledgments. We \nthank Marco Sanvido for help with the helicopter implementation, Arkadeb Ghosal and Slobodan Matic for \nhelp with the Giotto compiler, and Ed\u00adward Lee and Jie Liu for many valuable discussions.  6. REFERENCES \n[1] J. Armstrong. The development of Erlang. Int. Conf. Functional Programming, pp. 196 203. ACM, 1997. \n[2] G. Berry. The foundations of Esterel. In G. Plotkin and M. Tofte, eds., Proof, Language, and Interaction: \nEssays in Honour of Robin Milner. MIT Press, 2000. [3] A. Burns and A. Wellings. Real-Time Systems and \nProgramming Languages. Addison-Wesley, 1997. [4] G. Buttazzo. Hard Real-Time Computing Systems: Predictable \nScheduling Algorithms and Applications. Kluwer, 1997. [5] C. Ferdinand, R. Heckmann, M. Langenbach, \nF. Martin, M. Schmidt, H. Theiling, S. Thesing, R. Wilhelm. Reliable and precise WCET determination for \na real-life processor. EMSOFT, LNCS 2211, pp. 469 485. Springer, 2001. [6] N. Halbwachs. Synchronous \nProgramming of Reactive Systems. Kluwer, 1993. [7] N. Halbwachs, P. Caspi, P. Raymond, D. Pilaud. The \nsynchronous data.ow programming language Lustre. Proc. IEEE, 79:1305 1320, 1991. [8] T. Henzinger, B. \nHorowitz, C. Kirsch. Giotto: A time-triggered language for embedded programming. EMSOFT, LNCS 2211, pp. \n166 184. Springer, 2001. [9] J. Plaice and J.-B. Saint. The Lustre-Esterel Portable Format. Tech. Rep., \nINRIA Sophia-Antipolis, 1998. [10] M. Sanvido. A Computer System for Model Helicopter Flight Control, \nPart 3: The Software Core. Tech. Rep. 317, Inst. Computer Systems, ETH Z\u00a8urich, 1999. [11] D. Weil, V. \nBertin, E. Closse, M. Poize, P. Venier, J. Pulou. E.cient compilation of Esterel for real-time embedded \nsystems. Int. Conf. Compilers, Architectures, and Synthesis for Embedded Systems, pp. 2 8. ACM, 2000. \n[12] N. Wirth. Toward a discipline of real-time programming. Comm. ACM, 20:577 583, 1977. [13] N. Wirth. \nTasks versus threads: An alternative multiprocessing paradigm, Software: Concepts and Tools, vol. 17, \npp. 6 12. Springer, 1996.  \n\t\t\t", "proc_id": "512529", "abstract": "The Embedded Machine is a virtual machine that mediates in real time the interaction between software processes and physical processes. It separates the compilation of embedded programs into two phases. The first, platform-independent compiler phase generates E code (code executed by the Embedded Machine), which supervises the timing ---not the scheduling--- of application tasks relative to external events, such as clock ticks and sensor interrupts. E~code is portable and exhibits, given an input behavior, predictable (i.e., deterministic) timing and output behavior. The second, platform-dependent compiler phase checks the <i>time safety</i> of the E code, that is, whether platform performance (determined by the hardware) and platform utilization (determined by the scheduler of the operating system) enable its timely execution. We have used the Embedded Machine to compile and execute high-performance control applications written in Giotto, such as the flight control system of an autonomous model helicopter.", "authors": [{"name": "Thomas A. Henzinger", "author_profile_id": "81100034124", "affiliation": "EECS, University of California, Berkeley", "person_id": "PP14024447", "email_address": "", "orcid_id": ""}, {"name": "Christoph M. Kirsch", "author_profile_id": "81100151701", "affiliation": "EECS, University of California, Berkeley", "person_id": "P348258", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/512529.512567", "year": "2002", "article_id": "512567", "conference": "PLDI", "title": "The embedded machine: predictable, portable real-time code", "url": "http://dl.acm.org/citation.cfm?id=512567"}