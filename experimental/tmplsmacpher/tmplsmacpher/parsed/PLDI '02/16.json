{"article_publication_date": "05-17-2002", "fulltext": "\n Effective Sign Extension Elimination Motohiro Kawahito Hideaki Komatsu Toshio Nakatani IBM Tokyo Research \nLaboratory 1623-14, Shimotsuruma, Yamato, Kanagawa, 242-8502, Japan { jl25131, komatsu, nakatani }@jp.ibm.com \nABSTRACT Computer designs are shifting from 32-bit architectures to 64-bit architectures, while most \nof the programs available today are still designed for 32-bit architectures. Java , for example, specifies \nthe frequently used int as a 32-bit data type. If such Java programs are executed on a 64-bit architecture, \nmany 32-bit values must be sign-extended to 64-bit values for integer operations. This causes serious \nperformance overhead. In this paper, we present a fast and effective algorithm for eliminating sign extensions. \nWe implemented this algorithm in the IBM Java Just-in-Time (JIT) compiler for IA-64 . Our experimental \nresults show that our algorithm effectively eliminates the majority of sign extensions. They also show \nthat it significantly improves performance, while it increases JIT compilation time by only 0.11%. We \nimplemented our algorithm for programs in Java, but it can be applied to any language requiring sign \nextensions.  Categories and Subject Descriptors D.3.4 [Programming Languages]: Processors compilers, \nopti\u00admization. General Terms Algorithms, Performance, Design, Experimentation  Keywords Sign extension, \n64-bit architectures, Java, JIT compilers, IA-64 1. INTRODUCTION When a program is compiled, values \nwhose size is defined to be smaller than the architectural register size must be adjusted to the register \nsize. For example, on a 64-bit architecture, values defined as signed 8, 16, and 32-bit values in a program \nmust be sign\u00adextended to make them 64-bit values (Figure 1). Today, many systems and applications are \nstill designed for 32-bit architectures. For example, Java specifies the frequently used int as a 32-bit \ndata type [5]. If such programs are executed on a 64-bit architec\u00adture, 32-bit values must be sign-extended \nto 64-bit values for many integer instructions. This will cause serious performance degradation. Permission \nto make digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for profit or commercial advantage and that copies \nbear this notice and the full citation on the first page. To copy otherwise, or republish, to post on \nservers or to redistribute to lists, requires prior specific permission and/or a fee. PLDI 02, June 17-19, \n2002, Berlin, Germany. Copyright 2002 ACM 1-58113-463-0/02/0006 $5.00. 0 63 3231 063 3231 0 1 63 3231 \n0 63 3231 Figure 1. Sign extension of a 32-bit value to a 64-bit value 1) Implicit (automatic) sign extension \n(PPC64: lwa) s s  31 063 3231 0 lwa (Load Word Algebraic Instruction)  2) Explicit sign extension (PPC64: \nexts, IA64: sxt) i = mem;  - (1) i = i + 1;  - (2) i = extend(i);  - (3) // explicit sign extension \nis required t = (double) i;  - (4) // i must be sign-exetended (extend( ) denotes a sign extension instruction \nfrom 32-bit to 64-bit) Figure 2. Two types of sign extension Some 64-bit architectures have an instruction \nthat reads from memory and extends the sign in the same instruction automati\u00adcally. We call this implicit \nsign extension. For example, the PowerPC architecture [8] has such an instruction, called the load word \nalgebraic (lwa) instruction (Figure 2(1)). Other 64-bit ar\u00adchitectures have no such instruction. For \nexample, on IA-64, val\u00adues are zero-extended during memory reads, and thus sign exten\u00adsion instructions \nare required. Sign extension elimination is im\u00adportant for these architectures. Even for those architectures \nthat have an implicit sign extension instruction, when the register size and the size of the values de\u00adfined \nin a program are different, sign extensions are required dur\u00ading calculations. For example, in Figure \n2(2), since i must be sign\u00adextended on statement (4), a sign extension instruction (statement (3)) is \nrequired in addition to the implicit sign extension instruc\u00adtion (statement (1)). We call this an explicit \nsign extension and denote it as extend( ). For example, the PowerPC architecture has such an instruction, \ncalled the extend sign (exts) instruction. The IA-64 architecture [9] also has such an instruction, called \nthe sign extend (sxt) instruction. In principle, a sign extension instruction can be eliminated if its \nsource operand is already sign-extended or if the upper 32 bits of its destination operand do not affect \nthe correct execution of the succeeding instructions [3]. We implemented the first algorithm for sign \nextension elimination by using backward dataflow analysis. This algorithm first generates a sign extension \ninstruction immediately following every instruction I with a 32-bit destination operand unless the destination \noperand of the instruction I is guaranteed to be sign-extended. Next, it eliminates a sign extension \ninstruction if the backward dataflow analysis int j; // j is a 32-bit variable. int t = 0; // t is a \n32-bit variable. int i = mem; // i is a 32-bit variable. int C = 0x0fffffff; // C is a 32-bit variable. \ni = extend(i); - (1) (can be eliminated) do { i = i - 1; - (2) i = extend(i); - (3) j = a[i]; - (4) \nj = extend(j); - (5) (can be eliminated) j = j &#38; C; - (6) j = extend(j); - (7) (can be eliminated) \nt += j; - (8) t = extend(t); - (9) } while(i > start); // need sign extension for t d = (double) t; \n- (10) Figure 3. Limitations of the first algorithm proves that the upper 32 bits of the destination \noperand do not affect the correct execution of the following instructions. When we applied this algorithm \nto the example shown in Figure 3, we can only eliminate the sign extensions (1), (5), and (7)1. We found \nthe following four limitations of this algorithm: The first limitation is that a sign extension for an \narray index (e.g. (3) in Figure 3) cannot be eliminated using this algorithm. This is because an effective \naddress computation for an array access requires a sign extension. Figure 4 shows examples of effective \naddress computations on IA-64 and PPC64. For IA-64 (Figure 4 (b)), if the sign extension can be eliminated, \nan effective address can be computed using one instruction, called the shift left and add (shladd) instruction. \nArray accesses often appear inside a loop, and thus leaving these sign extension instructions in the \nloop causes major performance degradation. Additionally, when an array index in a loop is not loop invariant, \nloop invariant code motion techniques cannot move a sign extension for that array index out of the loop. \nOn the other hand, we can utilize an instruction that computes an effective address without an explicit \n(a) Original program base[index] = 0; # array access (b) IA-64 architecture sxt4 index = index # sign \nextension shladd rEA = index, 2, base # shift and add : (index << 2) + base st4 [rEA] = 0 # memory \nwrite (c) PowerPC64 architecture addi rS, 0, 0  # rS = 0 rldic rT, index, 2, 30 # shift and clear index \n32 bits 00 32 bits 00 stwx rS, base, rT # memory write : [base+rT] = rS rT00 Figure 4. Examples of \neffective address computations 1 Statement (10) requires a sign extension, and thus (9) cannot be eliminated. \nStatements (8) and (6) do not require any sign ex\u00adtension because the upper 32 bits of their source operands \ndo not affect their correct execution, and thus (7) and (5) can be eliminated, respectively. Statement \n(4) again requires a sign extension, and thus (3) cannot be eliminated. Finally, since (2) does not require \na sign extension, (1) can be eliminated. sign extension on some architectures. For example, PPC64 has \nsuch an instruction, called the rotate left doubleword immediate then clear (rldic) instruction. If the \nindex never has a negative value according to the language specification, we can utilize this instruction \nto compute an effective address. We will discuss this assumption in Section 3. For another example, on \nAMD s SledgeHammer [2], values are zero-extended during 32-bit operations, and thus we can simply use \nthe 64-bit shift-left (shl) instruction to compute an effective address without an explicit sign extension. \nThe second limitation is that elimination using only backward dataflow analysis may miss some opportunities \nfor eliminating sign extensions. In Figure 3, when (8) t += j is replaced by an instruction requiring \na sign extension for j, such as d += (double) j , (7) is no longer eliminated by this algorithm. The \nthird limitation is that a sign extension inside a loop may fail to be eliminated using this algorithm \nif there are sign extension instructions both inside and outside of the loop for the same vari\u00adable. \nThis is because elimination using backward dataflow analy\u00adsis leaves the last sign extension of the affected \nvariable in the flow graph. In Figure 3, for example, there are two sign exten\u00adsions for the variable \ni, (1) and (3). It is better to eliminate (3) since it is in the loop, but this algorithm results in \neliminating (1). The fourth limitation is that a sign extension inside a loop may fail to be eliminated \nusing this algorithm even when that sign extension instruction can be moved out of the loop. For example, \nin Figure 3, the sign extension (9) is not required inside the loop, but only before (10) outside of \nthe loop. We present a new algorithm solving these problems. Our ap\u00adproach has the following characteristics: \nz It eliminates sign extension for the effective address compu\u00ad tation of an array access based on our \nassumption that a nega\u00ad tive array index is not allowed by the language specification. z It eliminates \nsign extensions selectively, starting with the most frequently executed region. z It utilizes UD/DU chains \n[1] for the above two goals. z It inserts sign extensions before elimination. A combination of insertion \nand elimination can effectively move sign exten\u00adsions to less frequently executed regions, and particularly \nout of loops. We implemented our algorithm in our production-level Java Just\u00adin-Time (JIT) compiler for \nIA-64. In order to improve portability of our JIT compiler, many optimizations [10, 11, 12, 16, 19, 20] \nare performed at the intermediate language level. By porting these optimizations to the IA-64 JIT compiler, \nwe achieved high per\u00adformance quickly. We measured the effectiveness of our algorithm using jBYTEmark \nand SPECjvm98, both on an IBM IntelliStation Z Pro with two Intel Itanium processors. Our experimental \nre\u00adsults show our algorithm can effectively eliminate the majority of sign extensions. They also show \nthat it significantly improves performance, while increasing the JIT compilation time by only 0.11%. \nJava is a trademark of Sun Microsystems. IA-64 and Itanium are trademarks of Intel Corporation. The \nrest of the paper is organized as follows. Section 2 gives an overview of our approach. Section 3 describes \nthe sign extension elimination for array indices. Section 4 shows the performance results obtained in \nour experiments. Section 5 summarizes previ\u00adous work. Section 6 offers some concluding remarks.  2. \nOUR APPROACH Conversion for 64-bit architectures Step (1) (Generating Sign Extension Instructions) Common \nOptimizations (Constant Propagation, Common Subexpression Elimination, Step (2) Value Range Analysis) \n Elimination and Movement of Sign Extensions 1. Sign Extension Insertion  Step (3) 2. Order Determination \nof Elimination 3. Sign Extension Elimination Figure 5. Flow diagram of our algorithm Figure 5 shows \na flow diagram of our algorithm for sign exten\u00adsion elimination, which consists of three steps. Step \n(1) translates the intermediate representation of the target program from a 32-bit architecture form \nto a 64-bit architecture form. There are two approaches to generate sign extension instructions. One \nis to gen\u00aderate a sign extension instruction at a definition point, that is im\u00admediately following every \ninstruction I with a 32-bit destination operand, unless the destination operand of the instruction I \nis guaranteed to be sign-extended. The other is to generate a sign extension instruction at a use point, \nthat is immediately before every instruction I that requires a sign extension, unless the source operand \nof the instruction I is guaranteed to be sign\u00adextended. We use the first approach in order to most effectively \noptimize sign extensions. Figure 6 is an example to show these two approaches. In this example, if the \ncompiler generates a sign extension at a use point as in (c), no sign extension can be elimi\u00adnated. In \ncontrast, if the compiler generates a sign extension at a definition point as in (b), one sign extension \ncan be eliminated. (See Section 3 for the theorems that justify this elimination.) Step (2) also optimizes \nsign extensions. For example, when a constant is propagated as the source operand of a sign extension, \n(a) Original program t = i + j; a[t + 1] = 0; d = (double) t; (b) Generate a sign extension after definition \n(before elimination)(after elimination) t = i + j;t = i + j; t = extend( t ); t = extend( t ); t1 = \nt + 1;t1 = t + 1; t1 = extend( t1 ); a[ t1 ] = 0; a[ t1 ] = 0; d = (double) t; d = (double) t; (c) Generate \na sign extension before use (before elimination)(after elimination) t = i + j;t = i + j; t1 = t + 1; \n t1 = t + 1; t1 = extend( t1 );t1 = extend( t1 ); a[ t1 ] = 0; a[ t1 ] = 0; t = extend( t );t = extend( \nt ); d = (double) t; d = (double) t; Figure 6. Two approaches to generate sign extensions the sign extension \nwill be changed to a copy instruction by con\u00adstant folding. Common sub-expression elimination is also \napplied to sign extensions. Here we employ a variant of the partial redun\u00addancy elimination algorithm \n[12, 13, 14] for common sub\u00adexpression elimination. This optimization moves an expression backward in \nthe control flow graph, and thus loop-invariant sign extensions can be moved out of the loop. A value \nrange analysis [4, 7], used for some optimizations such as array bounds check elimination, also helps \nsign extension elimination. Step (3) is the main part of our algorithm, and it has three phases. In the \nfirst phase ((3)-1), a sign extension is inserted immediately before every instruction that requires \na sign extension. In the sec\u00adond phase ((3)-2), order determination is performed to eliminate sign extensions \nselectively beginning from the most frequently executed region. Finally, in the third phase ((3)-3), \nredundant sign extensions are eliminated using UD/DU chains. The following three sections describe each \nphase. 2.1 Sign Extension Insertion In the first phase, we insert two kinds of sign extensions. To eliminate \nsign extensions effectively from loops, we insert sign extension instructions. In the example of Figure \n7, (10) is the only instruction that requires a sign extension for t. If sign exten\u00adsion elimination \nis applied here without insertion, the sign exten\u00adsion (9) will still remain in the loop as shown in \nFigure 8(a). To avoid this inefficiency, we insert a sign extension instruction im\u00admediately before every \ninstruction where sign extension is neces\u00adsary unless its variable is obviously sign-extended. To balance \n(a) Before insertion (b) After insertion int j; // j is a 32-bit variable int j; // j is a 32-bit variable \nint t = 0; // t is a 32-bit variable int t = 0; // t is a 32-bit variable int i = mem; // i is a 32-bit \nvariable int i = mem; // i is a 32-bit variable i = extend(i); - (1) i = extend(i); - (1) do { do { i \n= i - 1; - (2) i = i - 1; - (2) i = extend(i); - (3) i = extend(i); - (3) j = a[i]; - (4) j = a[i]; - \n(4) j = extend(j); - (5)i = just_extended(i) - (12) j = j &#38; 0x0fffffff; - (6) j = extend(j); - (5) \nj = extend(j); - (7) j = j &#38; 0x0fffffff; - (6) t += j; - (8) j = extend(j); - (7) t = extend(t); \n- (9) t += j; - (8) } while(i > start); t = extend(t); - (9) // need a sign extension for t } while(i \n> start); d = (double)t; - (10) // need a sign extension for t  t = extend(t); - (11) d = (double)t; \n- (10) Figure 7. Example of inserting a sign extension (a) Optimized result without (b) Optimized result \nwith insertion insertion int j; // j is a 32-bit variable int j; // j is a 32-bit variable int t = 0; \n// t is a 32-bit variable int t = 0; // t is a 32-bit variable int i = mem; // i is a 32-bit variable \nint i = mem; // i is a 32-bit variable do { do { i = i - 1; - (2) i = i - 1; - (2) j = a[i]; - (4)j = \na[i]; - (4) j = j &#38; 0x0fffffff; - (6) j = j &#38; 0x0fffffff; - (6) t += j; - (8) t += j; - (8) t \n= extend(t); - (9) } while(i > start); } while(i > start); // need a sign extension for t // need a sign \nextension for t d = (double)t; - (10) t = extend(t); - (11) d = (double)t; -(10) Figure 8. The optimized \nresult of Figure 7 compilation time and effectiveness, we apply this insertion only to those methods \nthat include a loop. We tried another insertion algorithm that is a variant of the partial dead code \nelimination (PDE) algorithm [15]. This algorithm in\u00adserts a sign extension at the latest point on every \npossible path where each sign extension can be reached when it is moved for\u00adward in the control flow \ngraph. However, the simple insertion algorithm turned out to work better than this algorithm, as shown \nin Figure 12 and Figure 13 ( all, using PDE vs. new algorithm (all) ), and therefore we decided to use \nthe simple insertion algo\u00adrithm. We also insert a dummy sign extension instruction just after every array \naccess to indicate that it is guaranteed to be sign-extended, unless an array index is overwritten immediately, \nas in the case of i = a[i] . Dummy sign extension instructions are used to elimi\u00adnate other sign extension \ninstructions, and they will then be elimi\u00adnated after the elimination in Section 2.3 is performed. In \nFigure 7(b), this transformation inserts both a sign extension at (11) and a dummy sign extension (denoted \nas just_extended( )) at (12). After the sign extension elimination (Figure 8), all the sign extensions \nexcept for (11) can be successfully eliminated as shown in Figure 8(b).  2.2 Order Determination for \nElimination It is best to eliminate sign extensions starting from the most fre\u00adquently executed region. \nWhen there are sign extensions both inside and outside of a loop, some sign extensions inside the loop \nmay not be eliminated if the sign extensions outside the loop are eliminated first. In Figure 7(b), if \nwe eliminate the sign extension (11) first, (9) will still be required. To take another example, there \nare two candidates for elimination in Figure 9(a). In this example, while only one of them can be eliminated, \nResult 1 in (b) is obvi\u00adously better than Result 2 in (c). To achieve the results of Figure 8(b) and \nFigure 9(b), we eliminate sign extensions starting from the most frequently executed regions. We sort \nbasic blocks in the order of their execution frequency. For each basic block B, this can be estimated \nfrom both the loop nesting level of B and the execution frequency of B within its acyclic region based \non the probability of each conditional branch [21]. Additionally, we use profile information collected \nfor conditional branches by our combined interpreter and dynamic compiler [20] in order to en\u00adhance the \naccuracy of branch probabilities. The interpreter gathers statistical data on conditional branches. When \nthe interpreter finds that a method is executed frequently, the dynamic compiler is called. At that time, \nthe interpreter provides the statistical data to the dynamic compiler. (a) Before elimination(b) Result \n1(c) Result 2 i = j + k; i = j + k; i = j + k; i = extend(i);i = extend(i); do { do { do { i = i + \n1;  i = i + 1;  i = i + 1; i = extend(i);i = extend(i); a[i] = 0;  a[i] = 0;  a[i] = 0; } while(i \n< end); } while(i < end); } while(i < end); Figure 9. Example requiring order determination  2.3 Sign \nExtension Elimination The goal of this optimization is to analyze and eliminate each sign extension starting \nfrom the most frequently executed region, de\u00adtermined as described in Section 2.2. As mentioned in Section \n1, our algorithm is very powerful for eliminating sign extensions for array subscripts. We will discuss \nthis distinguished feature in the next section, while here we focus on basic cases of eliminations. The \nalgorithm EliminateOneExtend analyzes and eliminates one sign extension by using UD/DU chains. We assume \nthat each instruction has three flags, USE, DEF, and ARRAY, to indicate that the instruction has been \ntraversed for each check. We note here that 8-bit and 16-bit sign extensions are also eliminated based \non the same algorithm, although we describe the algorithm only for eliminating 32-bit sign extensions \nin this section. EliminateOneExtend(EXT) { initialize all flags (USE,DEF,ARRAY) for all instructions; \nrequired = FALSE; /* use DU-chain */ for (I c all instructions that use the destination operand of EXT){ \nrequired = AnalyzeUSE(EXT, I, TRUE); if (required) break; } if (required){ /* use UD-chain */ for (I \nc all instructions that define the source operand of EXT){ required = AnalyzeDEF(I); if (required) break; \n} } if (!required) eliminate EXT; } In principle, a sign extension can be eliminated if its source oper\u00adand \nis already sign-extended or if the upper 32 bits of its destina\u00adtion operand do not affect the correct \nexecution of the following instructions. The algorithm AnalyzeDEF checks the first condition. An example \nof Case 1 is a bit-wise AND instruction where ei\u00adther operand is known to have a positive value. An example \nof Case 2 is a copy operation. /* if it returns FALSE, sign extension is unnecessary for I if it returns \nTRUE, sign extension is necessary for I */ AnalyzeDEF(I) { if (a flag of DEF for I has been already set) \nreturn FALSE; set a flag of DEF for I; switch(I){ case The destination operand of I is known to be sign-extended: \n/* Case 1 */ return FALSE; case The destination operand of I can be determined to be sign-extended if \nthe source operand of I is sign-extended: /* Case 2 */ /* use UD-chain */ for (J c all instructions that \ndefine the source operand of I){ if ( AnalyzeDEF(J) ) return TRUE; } return FALSE; } return TRUE; } The \nfollowing algorithm AnalyzeUSE checks the second condi\u00adtion. An example of Case 1 is a 32-bit memory \nwrite operation. An example of Case 2 is an addition. We will explain the algo\u00adrithm AnalyzeARRAY in \nthe next section. This phase of sign extension elimination ends with one trivial operation that eliminates \nall the dummy sign extensions. /* if it returns FALSE, sign extension is unnecessary for I if it returns \nTRUE, sign extension is necessary for I */ AnalyzeUSE(EXT, I, ANALYZE_ARRAY) { if (a flag of USE for \nI has already been set) return FALSE; set a flag of USE for I; switch(I){ case The upper 32 bits of the \nsource operand do not affect I: /* Case 1 */ return FALSE; case I computes an effective address of an \narray: if (ANALYZE_ARRAY) return AnalyzeARRAY(EXT, I); break; case The source operand of I can be determined \nto be unnecessary  if the destination operand of I is determined to be unnecessary: /* Case 2 */ if \n(it is impossible to analyze array's address computation via I){ ANALYZE_ARRAY = FALSE; } /* use DU-chain \n*/ for (J c all instructions that use the destination operand of I){ if ( AnalyzeUSE(EXT, J, ANALYZE_ARRAY) \n) return TRUE; } return FALSE; } return TRUE; }  3. HANDLING OF ARRAY SUBSCRIPTS The most serious problem \nwith our first algorithm is that a sign extension for an effective address computation of an array access \ncannot be eliminated. Here, we observed that these sign exten\u00adsions could be eliminated if we know that \nan array cannot be ac\u00adcessed with a negative array index. This is true for Java since Java programs throw \nan ArrayIndexOutOfBoundsException if an array is accessed using a negative array index [5]. Note that \nthe imple\u00admentation of array bounds checking may require a sign extension. If the target architecture \nhas 32-bit compare (including trap) in\u00adstructions to compare only the lower 32 bits of registers, we \ncan implement array bounds checking without any sign extension. This is because 32-bit compare instructions \nignore the upper 32 bits of registers. For example, SledgeHammer [2], PPC64 [8], and IA-64 [9] have such \nan instruction. Based on this language speci\u00adfication (LS), the following predicate will hold for a subscript \nexpression e. LS(e) = 0 \u00ef lower 32 bits of e \u00ef 0x7fffffff In this section, we discuss preconditions for \ne to satisfy 0 \u00ef e \u00ef 0x7fffffff by using LS(e). In general, if the language speci\u00adfication rules out \nany array access with a negative array index, then the following four theorems can be derived. Note that \nthe theorems assume 64-bit wrap-around on overflow. These theo\u00adrems depend on the knowledge of the value \nrange, which can be determined at compile time using one of the value range analysis techniques (Figure \n5(2)). We use these theorems to effectively eliminate sign extensions for array indices. Theorem 1 If \na variable i satisfies the following two conditions, i does not need a 32-bit sign extension for the \neffective address computation of an array access whose subscript expression is i. 1. The upper 32 bits \nof i are initialized to zero. 2. LS( i ) holds.  Proof. Because of the two conditions, 0 \u00ef i \u00ef 0x7fffffff \nwill al\u00adways hold. Thus, i does not need a 32-bit sign extension. . (a) Original program index = obj.field; \n# memory read base[index] = 0; # array access (b) IA-64 Instructions ld4 index = [obj.field] # upper \n32 bits are cleared sxt4 index = index # can be eliminated cmp4.ltu.unc p0, p6 = index, base.Len # array \nbounds check (p6) br.spnt.many ExceptionHandler # LS( index ) holds shladd rEA = index, 2, base # shift \nand add st4 [rEA] = 0 # memory writes Figure 10. An example of Theorem 1 Figure 10 shows an example \nof Theorem 1. On IA-64, since a value is zero-extended during any memory read, the upper 32 bits of index \nare cleared. Next, the array bounds check guarantees LS( index ). As a result, the sign extension in \nFigure 10(b) can be eliminated.  Theorem 2 If variables i, j satisfy the following three conditions, \ni+j does not need a 32-bit sign extension for the effective address computation of an array access whose \nsubscript expression is i+j. 1. Both i and j have already been sign-extended from 32 bits. 2. Either \ni or j satisfies the following inequality: 0 \u00ef i or j \u00ef 0x7fffffff 3. LS( i+j ) holds.  Proof. Since \ni and j are commutable for i+j, it is sufficient to prove only the case in which i satisfies the second \ncondition. Case 0 \u00ef i \u00ef 0x7fffffff: Case 1 (0 \u00ef j \u00ef 0x7fffffff): Because the upper 32 bits of i+j must \nbe zero, Theorem 2 holds using Theorem 1. Case 2 (0xffffffff80000000 \u00ef j \u00ef 0xffffffffffffffff): The range \nof i+j will be 0xffffffff80000000 \u00ef i+j \u00ef 0xffffffffffffffff or 0 \u00ef i+j \u00ef 0x7ffffffe . Because of the \nthird condition, 0 \u00ef i+j \u00ef 0x7ffffffe will hold. Thus, i+j does not need a 32-bit sign extension. . By \nusing Theorem 2, we can eliminate the sign extension in the loop of Figure 9(a). Theorem 3 If variables \ni, j satisfy the following three conditions, i-j does not need a 32-bit sign extension for the effective \naddress computation of an array access whose subscript expression is i-j. 1. The upper 32 bits of i are \ninitialized to zero. 2. j satisfies the following inequality: 0 \u00ef j \u00ef 0x7fffffff 3. LS( i-j ) holds. \n This condition always holds for Java. Proof. The range of i is 0 \u00ef i \u00ef 0xffffffff. The range of i-j \nwill be 0xffffffff80000001 \u00ef i-j \u00ef 0xffffffffffffffff or 0 \u00ef i-j \u00ef 0xffffffff. Because of the third condition, \n0 \u00ef i-j \u00ef 0x7fffffff will hold. Thus, i-j does not need a 32-bit sign extension. . Theorem 3 is useful \non IA-64 since zero extension is performed for every memory read. We can enhance sign extension elimina\u00adtion \nfor subtraction by using Theorem 3. When Theorem 3 is ap\u00adplied to Figure 7, the sign extension at (1) \ncan be eliminated. If the maximum array size can be limited to a certain size or if an array size is \nknown at compile time, the following theorem can be derived from Theorem 2. Theorem 4 If variables i, \nj satisfy the following four conditions, i+j does not need a 32-bit sign extension for the effective \naddress computation of an array access whose subscript expression is i+j. 1. The array size can be limited \nto maxlen, and the following inequality holds: 0 \u00ef maxlen \u00ef 0x7fffffff 2. Both i and j have already \nbeen sign-extended from 32 bits. 3. Either i or j satisfies the following inequality: (maxlen-1)-0x7fffffff \n\u00ef i or j \u00ef 0x7fffffff 4. The lower 32 bits of the array index i+j satisfy the following  inequality \nfor the language specification: 0 \u00ef lower 32 bits of (i+j) < maxlen \u00ef 0x7fffffff Proof. Since i and \nj are commutable for i+j, it is sufficient to prove only the case in which i satisfies the third condition. \nCase 0 \u00ef i \u00ef 0x7fffffff: Theorem 4 holds using Theorem 2. Case (maxlen-1)-0x7fffffff \u00ef i \u00ef 0xffffffffffffffff: \nCase 1 (0 \u00ef j \u00ef 0x7fffffff): Theorem 4 holds using Theorem 2. Case 2 (0xffffffff80000000 \u00ef j \u00ef 0xffffffffffffffff): \ni+j will be 0xffffffff00000000+maxlen \u00ef i+j \u00ef 0xfffffffffffffffe; that is, maxlen \u00ef lower 32 bits of \n(i+j) \u00ef 0xfffffffe. The fourth con\u00addition excludes this case. . Note that Theorems 2 and 4 can be applied \nto subtractions like i-k by computing the range of k, which can be computed by assigning (-k) to j. We \ncan eliminate more sign extensions by using Theorem 4. For example, the Java language specification defines \nthe maximum array size as 0x7fffffff. Therefore, we can eliminate a sign exten\u00adsion for the effective \naddress computation of an array access whose subscript expression is i+j, if both i and j have been sign\u00adextended \nand if -1 \u00ef i or j \u00ef 0x7fffffff holds. This will cover count down loops. In Figure 7, we can eliminate \nthe sign extension (3) using Theorem 4. Figure 11 shows an example where a sign ex\u00adtension can be removed \ndepending on the array size. This example is the same as the one in Figure 7(a) except that statement \n(2) is replaced by i = i 2 . In this example, if mem is assumed to be 0x80000000 (the minimum value \nof a signed 32-bit representa\u00adtion), the lower 32 bits of i at the time of the first array access are \n0x7ffffffe. If this array size is 0x7fffffff, the array element a[i] can be accessed. In this case, the \nsign extension (3) cannot be elimi\u00adnated, because the upper 32 bits must be set correctly. On the int \nj; // j is a 32-bit variable int t = 0; // t is a 32-bit variable. int i = mem; // i is a 32-bit variable. \n// assuming mem = 0x80000000. i = extend(i); - (1) // i == 0xffffffff80000000. do { i = i -2; - (2) \n// i == 0xffffffff7ffffffe i = extend(i); - (3) // i == 0x7ffffffe. a[i] can be accessed depending on \narray size. j = a[i]; - (4) j = extend(j); - (5) j = j &#38; 0x0fffffff - (6) j = extend(j); - (7) t \n+= j; - (8) t = extend(t); - (9) } while(i > start); // need sign extension for t d = (double)t; - (10) \nFigure 11. A removable sign extension depending on array size other hand, if the size of this array is \nknown to be smaller than 0x7fffffff, the access of element a[i] must be invalid. This is be\u00adcause the \ncondition \"the lower 32 bits of i (0x7ffffffe) < the array's size < 0x7fffffff\" is never satisfied. Therefore, \nif it is known that the size of this array is always smaller than 0x7fffffff at compile time, the sign \nextension (3) can be eliminated. As another example, when we can limit the maximum array size to 0x7fff0001 \nbased on limitation of the configurable memory re\u00adsources, we can eliminate the sign extension if either \ni or j is lar\u00adger than -65536 and the other conditions (2 and 4) are met. /* if it returns FALSE, sign \nextension is unnecessary for AOP if it returns TRUE, sign extension is necessary for AOP */AnalyzeARRAY(EXT, \nAOP) { if (all instructions that use the destination operand of AOP are array accesses){ for (D c all \ninstructions that define the source operand of EXT){ if (a flag of ARRAY for D has not been set yet){ \nset a flag of ARRAY for D; required = TRUE; switch(D){ case The upper 32 bits of the destination operand \nof D are always zero: /* Theorem 1 */ required = FALSE; break; case subtraction: case addition: required \n= AnalyzeTheorem2_3_4(D); break; case copy: /* use UD-chain */ for (J c all instructions that define \nthe source operand of D){ if ( AnalyzeARRAY(J, AOP) ) return TRUE; } required = FALSE; break; } if (required) \nreturn TRUE; } } return FALSE; } return TRUE; } The algorithm AnalyzeARRAY analyzes the effective address \ncomputation of an array access to see whether the sign extension can be eliminated by checking if any \nof Theorems 1, 2, 3, or 4 can be satisfied for all the instructions that define the source operand of \nthe given sign extension. The algorithm AnalyzeTheorem2_3_4 analyzes if any of Theo\u00adrems 2, 3, or 4 can \nbe satisfied for D. Here, upper32(e) and lower32(e) denote the upper and lower 32 bits of e, respectively. \n/* if it returns FALSE, sign extension is unnecessary for D if it returns TRUE, sign extension is necessary \nfor D */AnalyzeTheorem2_3_4(D){ if (the second operand of D is sign-extended &#38;&#38; /* use UD-chain \n*/  upper32(the first operand of D) are zero &#38;&#38; /* use UD-chain */ ((D is subtraction &#38;&#38; \n0 \u00ef lower32(second operand of D) \u00ef 0x7fffffff) || (D is addition &#38;&#38; -0x7fffffff \u00ef lower32(second \noperand of D) \u00ef 0))) {  return FALSE; /* Theorem 3 */ } else { arraylen = the maximum array size; if \n(the array size is known) arraylen = the array size; min_theorem4 = (arraylen - 1) - 0x7fffffff; if (both \noperands of D are sign-extended &#38;&#38; /* use UD-chain */ ((D is addition &#38;&#38;   (min_theorem4 \n\u00ef lower32(first operand of D) \u00ef 0x7fffffff ||    min_theorem4 \u00ef lower32(second operand of D) \u00ef 0x7fffffff)) \n|| (D is subtraction &#38;&#38;   (-0x7fffffff \u00ef lower32(first operand of D) \u00ef -min_theorem4 ||  \n  -0x7fffffff \u00ef lower32(second operand of D) \u00ef -min_theorem4)))) {   return FALSE; /* Theorem 4 */ \n} } return TRUE; } We use UD chains and the results of value range analysis to see if either Theorem \n3 or 4 is satisfied. For Java, the conditions 1 and 4 of Theorem 4 always hold, and thus Theorem 2 always \nholds when Theorem 4 holds (min_theorem4 is always zero for Theorem 2). Therefore, we can skip Theorem \n2 here. We analyze the value range of the lower 32 bits for the int type in Figure 5 Step (2). Even if \na value range analysis is not performed, our algo\u00adrithm is still effective because one of the two operands \nis often a constant value (e.g. Figure 3(2)).  4. EXPERIMENTAL RESULTS We used jBYTEmark and SPECjvm98 \n[18] benchmarks for the evaluation of our optimizations. To measure every result under the same environment, \nwe ran each benchmark program from the command line. For jBYTEmark, we specified the benchmark size to \nmeasure them consistently. For SPECjvm98, we ran each benchmark program with the count set to 100 from \nthe command line, instead of running all of the benchmarks continuously as suggested by the official \nSPEC run rules. We implemented our algorithm in the IBM Java Just-in-Time (JIT) compiler for IA-64. All \nthe experiments were conducted on an IBM IntelliStation Z Pro model 689412X (two Intel Itanium 800 MHz \nprocessors with 2 GB of RAM) and Windows. The architectural characteristics related to sign extension \noptimizations for this machine are [9]: z IA-64 has a 32-bit compare instruction to compare only the \nlower 32 bits of registers, so array bounds checking can be implemented with no sign extension. z Since \na value is zero-extended during any memory read on IA-64, there are many opportunities to utilize Theorems \n1 and 3 as described in Section 3. 4.1 Performance Improvement We instrumented the compiled code to \ncount the remaining sign extension instructions to see the effectiveness of sign extension elimination. \nTable 1 and Table 2 show the dynamic counts of 32\u00adbit sign extensions and the percentages during a sample \nrun of each benchmark program of jBYTEmark and SPECjvm98 for IA\u00ad64, respectively. In these tables, white \ncircles denote improved results and black circles denote worsened results. The shaded cells denote important \ncells, and others denote breakdowns of our new algorithm. We measured the following algorithm variants: \nDisable opt.: Disable all the sign extension optimizations in Figure 5(3). Baseline: Generate a sign \nextension instruction at the code gen\u00ad eration phase immediately before every instruction that requires \na sign extension. First algorithm (bwd flow): Our first algorithm. It eliminates sign extensions using \nbackward dataflow analysis. Basic ud/du: New algorithm, but disable sign extension insertion, order determination, \nand elimination for array indices. Insert: Enable sign extension insertion, but disable both order determination \nand elimination for array indices. Order: Enable order determination, but disable both sign exten\u00adsion \ninsertion and elimination for array indices. Insert, order: Enable both insert and order , but disable \nelimination for array indices. Array: Enable elimination for array indices, but disable both sign extension \ninsertion and order determination. Array, insert: Enable both array and insert , but disable sign extension \ninsertion. Array, order: Enable both array and order , but disable order determination. All, using PDE \n(reference): All optimizations in this paper are applied, but the insertion algorithm is a variant of \nthe par\u00ad tial dead code elimination algorithm. New algorithm (all): All optimizations in this paper are \napplied. For our baseline, we generate a sign extension instruction at the code generation phase immediately \nbefore every instruction that requires a sign extension. We use this algorithm as our baseline because \nit is a simple and effective approach without performing any optimization. Any version that disables \nthe order determina\u00adtion performs the sign extension elimination in the reverse depth first search order, \nthe same order in which backward dataflow analysis is performed. Figure 12 and Figure 13 plot the percent\u00adages \nof dynamic counts of remaining sign extensions over our baseline. Overall, our algorithm (denoted as \nnew algorithm (all) ) elimi\u00adnates between 60.96% and 99.99% of sign extensions over our baseline. The \ndifference between first algorithm (bwd flow) and basic ud/du shows how often the second problem, described \nin Section 1, occurs, and how often other optimizations increase the opportunity for eliminating sign \nextensions. Sign extension elimi\u00adnation for array indices is most effective for all the benchmark programs. \nRegarding the order determination and the sign exten\u00adsion insertion, we can make the following observations: \n Table 1. Dynamic counts of remaining 32-bit sign extensions for jBYTEmark (.: improved result, .: worsened \nresult) Numeric Sort String Sort Bitfield FP Emu. Fourier Assignment IDEA Huffman Neural Net LU Decom. \naverage 3758195644 998928087 1449826010 1714076540 14430930 1531993239 1458959514 1143451491 324884747 \n738877762  disable opt. (203.53%) (244.20%) (175.51%) (829.37%) (100.04%) (103.25%) (210.26%) (155.66%) \n(93.63%) (34.87%) (215.03%) baseline 1846480139 409059333 826057022 206672979 14424738 1483839474 693886034 \n734563912 346982678 2118828282 (100.00%) (100.00%) (100.00%) (100.00%) (100.00%) (100.00%) (100.00%) \n(100.00%) (100.00%) (100.00%) (100.00%) first algorithm 1113353493 366540417 413060614 229190109 98150 \n1064297572 709112140 653693676 321123167 738398682  (bwd flow) . (60.30%) . (89.61%) . (50.00%) \n. (110.90%) . (0.68%) . (71.73%) . (102.19%) . (88.99%) . (92.55%) . (34.85%) .(70.18%)  basic ud/du \n 1004325701 366876481 413057597 1129233 95474 861401297 341860302 444498974 320996644 738370555 . (54.39%) \n(89.69%) (50.00%) .  (0.55%) (0.66%) . (58.05%) . (49.27%) . (60.51%) (92.51%) (34.85%) .(49.05%) \n1004329535 367129292 413039165 1128654 94863 861403261 341859947 444498283 320996048 738568088  insert \n(54.39%) . (89.75%) (50.00%) (0.55%) (0.66%) (58.05%) (49.27%) (60.51%) (92.51%) . (34.86%) (49.05%) \n1004325701 366876481 413057597 1129232 95474 861401297 341860299 444498974 320996644 738370555  order \n(54.39%) (89.69%) (50.00%) (0.55%) (0.66%) (58.05%) (49.27%) (60.51%) (92.51%) (34.85%) (49.05%) 1004320308 \n366867867 413052165 1123692 90139 860117895 341854867 444477957 320991237 738365142insert, order (54.39%) \n(89.69%) (50.00%)  (0.54%) . . (0.62%) . (57.97%) (49.27%) (60.51%) (92.51%) (34.85%) .(49.03%) 466217696 \n32825823 412965502 37083 58188 1433880 20496734 114906 814294 53711 array . (25.25%) .  (8.02%) . \n (49.99%) .  (0.02%) . (0.40%) .  (0.10%) .  (2.95%) .  (0.02%) . (0.23%) .  (0.00%) . (8.70%) \n466217905 31422376 412997440 1082443 54158 1432224 20492756 110281 810069 247626array, insert (25.25%) \n.  (7.68%) . (50.00%) .  (0.52%) . (0.38%) (0.10%) (2.95%) (0.02%) (0.23%) .  (0.01%) . (8.71%) \n466213149 32821279 412960937 32095 53855 1429335 20492173 109923 809749 49152array, order (25.25%) (8.02%) \n(49.99%) (0.02%) . (0.37%) (0.10%) (2.95%) .  (0.01%) (0.23%) (0.00%) (8.70%) all, using PDE 466212816 \n31167734 412960608 31756 17543 151010 20491806 104414 808814 48826 (reference) (25.25%) .  (7.62%) \n(49.99%) (0.02%) . (0.12%) .  (0.01%) (2.95%) (0.01%) (0.23%) (0.00%) . (8.62%) new algorithm 466205975 \n31158251 412953766 24770 10759 144163 20484964 87114 802576 41980 (all) (25.25%) .  (7.62%) (49.99%) \n.  (0.01%) . (0.07%) .  (0.01%) (2.95%) (0.01%) (0.23%) (0.00%) . (8.62%)  first algorithm (bwd flow)120% \nbasic ud/duinsert order  insert, order 100% array  array, insertarray, order  all, using PDE (reference) \n80% new algorithm (all)  60%  40%  20%  0% Numeric Sort String Sort Bitfield FP Emulation Fourier \nAssignment IDEA Huffman Neural Net LU Decom. average Figure 12. Dynamic counts of remaining 32-bit sign \nextensions for jBYTEmark (baseline=100%) 1. Combining sign extension insertion or elimination for array \neffective for any of the programs except for the mtrt and javac indices with order determination enhances \nthe effectiveness benchmarks. Moreover, many sign extensions that cannot be of elimination. eliminated \nwithout a combination of array , order , and insert (denoted as all ) are found in most of the benchmark \nprograms. 2. Sign extension insertion is ineffective without order determi\u00ad nation. Regarding the second \nobservation, when order determination is disabled, the possibility of sign extensions remaining in frequently \nRegarding the first observation, when either sign extension inser\u00adexecuted regions is increased. The \ncombination of the order de\u00adtion or elimination for array indices is done, there are often sev\u00adtermination \nand the sign extension insertion is particularly effec\u00aderal sign extensions that are potential candidates \nfor elimination. tive for FP Emulation in jBYTEmark (1,082,443 vs. 24,770). As Therefore, using order \ndetermination enhances their effectiveness. we mentioned in Section 2.1, we also tried another insertion \nalgo-Order determination alone (denoted as order in Table 2) is not Table 2. Dynamic counts of remaining \n32-bit sign extensions for SPECjvm98 (.: improved result, .: worsened result) mtrt jess compress db \nmpegaudio jack javac average 19162530 165578180disable opt. (140.70%) (193.42%)13619419 85606229baseline \n(100.00%) (100.00%) first algorithm 8800008 65389295 (bwd flow) . (64.61%) . (76.38%) 8042506 64268121basic \nud/du . (59.05%) . (75.07%) 8091775 64282490insert . (59.41%) . (75.09%) 7962959 64268123order . \n (58.47%) (75.07%) 7702613 58537108insert, order . (56.56%) . (68.38%) 1961456 21544766 array . (14.40%) \n. (25.17%) 1997325 21330983array, insert . (14.67%) . (24.92%) 1815980 21451357array, order . (13.33%) \n. (25.06%) all, using PDE 1811531 21249333 (reference) . (13.30%) . (24.82%) 816153 12104429new algorithm \n(all) .  (5.99%) . (14.14%) 100% 90% 80% 70% 60% 50% 40% 30% 20% 10% 0% mtrt jess compress 1831002215 \n 298338613 807174869 95753436 234753067 (284.42%) (140.73%) ... . (74.26%) ..... 643756102 (100.00%) \n632370673 . (98.23%) 572297310 . (88.90%) 582159879  .  (90.43%) 572297310 (88.90%) 571386557 . (88.76%) \n262105810 . (40.72%) 262095499 . (40.71%) 252222150 . (39.18%) 251323510 . (39.04%) 251303625 . \n(39.04%)  db 211989358 (100.00%) 162360645  (76.59%) 157713624 (74.40%) 180226907  (85.02%) 157713624 \n(74.40%) 157417853 (189.30%) (175.38%) (174.85%) (185.54%) 426398800 54596337 134261024 (100.00%) (100.00%) \n(100.00%) 358862336 43946795 105245327 . (84.16%)  . (80.49%) . (78.39%) 237743297 38839843 93920998 \n. (55.76%) . (71.14%) . (69.95%) 238661074 39919110 93400183 . (55.97%) . (73.12%) . (69.57%) 237743297 \n38839851 90060780 (55.76%) (71.14%) . (67.08%) 237099838 37781407 86758551 . (55.61%) . (69.20%) . \n (64.62%) 33105544 56354143 27999218 47560306  (15.62%) . (13.22%) . (51.28%) . (35.42%) 31514492 \n56287323 29245657 46826978  (14.87%) . (13.20%) . (53.57%) . (34.88%) 10557513 54729584 21714796 \n35732666  (4.98%) . (12.84%) . (39.77%) . (26.61%) 7794544 53638690 21533665 33544130  (3.68%) \n. (12.58%) . (39.44%) . (24.98%) 7472849 53079482 18844058 29942890  (3.53%) . (12.45%) . (34.52%) \n. (22.30%)        mpegaudio jack javac average (100.00%) . (79.84%) . (70.61%)  .  (72.66%) \n. (70.12%) . (68.20%) . (27.98%)  .  (28.12%) . (23.11%) . (22.55%) . (18.85%)  first algorithm \n(bwd flow) basic ud/du insert order insert, order array array, insert array, order all, using PDE (reference) \nnew algorithm (all) Figure 13. Dynamic counts of remaining 32-bit sign extensions for SPECjvm98 (baseline=100%) \nrithm that is a variant of partial dead code elimination algorithm. Our experiments ( all, using PDE \nvs. new algorithm (all) ) show that the simple insertion algorithm is slightly better for all the benchmarks. \nFigure 14 and Figure 15 show the performance improvement over our baseline for jBYTEmark and SPECjvm98, \nrespectively. Our sign extension elimination is effective for all benchmarks, particularly for the Huffman \nand compress benchmarks.  4.2 JIT Compilation Time This section describes how our approach affects the \nJIT compila\u00adtion time. We measured the breakdown of the JIT compilation time for IA-64, as shown in Table \n3, by using a trace tool. In summary, both sign extension optimizations and UD/DU chain creation increased \nthe total compilation time by 3.03% on average. Since we used the UD/DU chains for other optimizations, \nthese chain creations are still necessary even if sign extension optimiza\u00adtions are not performed. Excluding \nthe time for UD/DU chain 14% 12% 10% 8% 6% 4% 2% 0% -2% Num. Str. Sort Bitfield FP Emu. Fourier Assign \nIDEA Huffman Neural LU Sort Net Decom. Baseline: generate a sign extension instruction just before each \ninstruction requiring it Table 3. Breakdown of JIT compilation time Sign extension optimizations (all) \nUD/DU chain creation Others mtrt 0.20% 2.31% 97.49% jess 0.20% 2.44% 97.36% compress 0.13% 3.05% 96.82% \ndb 0.08% 2.56% 97.35% mpegaudio 0.10% 2.41% 97.49% jack 0.10% 2.53% 97.37% javac 0.13% 2.52% 97.35% num. \nsort 0.09% 3.14% 96.77% str. sort 0.09% 2.63% 97.28% Bitfield 0.10% 3.56% 96.34% FP emu. 0.07% 3.19% \n96.73% fourier 0.11% 3.51% 96.38% assignment 0.10% 3.30% 96.61% IDEA 0.13% 3.23% 96.64% huffman 0.10% \n3.32% 96.58% neural net 0.08% 2.97% 96.95% lu decom. 0.09% 2.92% 96.99% average 0.11% 2.92% 96.97% creation, \nthe sign extension optimizations increased the total compilation time only by 0.11% on average, while \nthey achieved significant performance improvements as shown in Figure 14 and Figure 15.  5. PREVIOUS \nWORK PentiumGCC [17], a version of the GNU C compiler customized for Pentium, performs sign extension \nelimination, but there is no description of it. The AS/400 Optimizing Translator [3] also per\u00adforms sign \nextension elimination. It describes two concepts of sign extension elimination, but unfortunately no \ndetailed algo\u00adrithms are presented. A partial redundancy elimination approach [13, 14] is also effec\u00adtive \nfor eliminating sign extensions. In fact, our PRE phase (Figure 5(2)) eliminated some sign extensions. \nA partial dead code elimination (PDE) approach [15] turns out to be less effec\u00adtive than the simple insertion \nalgorithm. Figure 16 shows a draw\u00ad 8% 7% 6% 5% 4% 3% 2% 1% 0% mtrt jess compress db mpegaudio jack javac \nBaseline: generate a sign extension instruction just before each instruction requiring it (a) PDE approach \n(b) Our approach (3) One of sign extensions is eliminated    (5) d = (double)t t = extend(t) (frequency \nbased) (5) d = (double)t It does not move the sign extension (3) to (5) Figure 16. Drawback of the PDE \napproach back of the PDE approach. In this example, PDE does not move the sign extension (3) to (5). \nIn contrast, our approach first inserts a sign extension (5) and next eliminates sign extensions selec\u00adtively \nstarting with the most frequently executed region. Therefore, if the compiler judges that (5) is more \nfrequently executed than (3), the sign extension (5) will be eliminated. If the compiler judges that \n(3) is more frequently executed than (5), the sign ex\u00adtension (3) will be eliminated. A path-profile-guided \npartial dead code elimination approach [6] could be more effective than the simple insertion algorithm \nat the cost of a much longer compila\u00adtion time, but we believe it is not practical for dynamic compilers. \n 6. CONCLUSIONS In this paper, we have presented a new algorithm for sign exten\u00adsion elimination. To \nthe best of our knowledge, this is the first algorithm to provide fast and effective sign extension elimination. \nSign extensions are eliminated selectively based on the order of the most to the least frequently executed \ncode. Our approach can eliminate sign extensions for effective address computation of array accesses \nbased on our assumption that a negative index is not allowed by the language specification. Our experiments \nshow that the majority of sign extensions can be eliminated at a small cost in compilation time. Although \nwe implemented our algorithm for Java, it should also be applicable for other languages requiring sign \nextensions.  7. ACKNOWLEDGMENTS We would like to thank all the members of the IBM JIT Compiler Research \nteam in Tokyo for helpful discussions and analysis of possible performance improvements. We also thank \nStephen Fink and Arvin Shepherd of IBM T.J. Watson Research Center for their helpful comments on this \npaper. Finally, we thank the anonymous reviewers for their helpful comments.  8. REFERENCES [1] A.V. \nAho, R. Sethi, and J. Ullman. Compilers: Principles, Techniques, and Tools, Addison-Wesley Publishing \nCo., Reading, MA, 1986. [2] AMD x86-64 Architecture Programmers Overview, http://www.amd.com/us-en/assets/content_type/ \nwhite_papers_and_tech_docs/x86-64_overview.pdf [3] K. V. Besaw, R. J. Donovan, E. C. Prosser, R. R. Roediger, \nW. J. Schmidt, and P. J. Steinmetz. The Optimizing Translator. http://www-1.ibm.com/servers/eserver/iseries/ \nbeyondtech/translator.htm [4] W. Blume and R. Eigenmann. Symbolic range propagation In Proceedings of \nthe 9th International Parallel Processing Symposium, pp.357-363, 1995. [5] J. Gosling, B. Joy, and G. \nSteele. The Java Language Specifi\u00adcation, Addison-Wesley Publishing Co., Reading, 1996. [6] R. Gupta, \nD.A. Berson, and J.Z. Fang, \"Path Profile Guided Partial Dead Code Elimination Using Predication,\" In \nPro\u00adceedings of the Conference on Parallel Architectures and Compilation Techniques, pp.102-115, San \nFrancisco, Cali\u00adfornia, November 1997. [7] W. Harrison. Compiler Analysis of the Value Ranges for Variables \nIn IEEE Transactions on Software Engineering, Vol. 3, No. 3, pp.243-250, 1977. [8] IBM Corp.: PowerPC \nHomepage  http://www.chips.ibm.com/products/powerpc/ [9] Intel Corp.: Itanium Architecture - Manuals. \n  http://www.intel.com/design/itanium/manuals/ [10] K. Ishizaki, M. Kawahito, T. Yasue, M. Takeuchi, \nT. Oga\u00adsawara, T. Suganuma, T. Onodera, H. Komatsu, and T. Nakatani. \"Design, Implementation, and Evaluation \nof Optimizations in a Just-In-Time Compiler.\" In Proceedings of the ACM SIGPLAN Java Grande Conference, \nJune 1999. [11] K. Ishizaki, M. Kawahito, T. Yasue, H. Komatsu, and T. Nakatani. \"A Study of Devirtualization \nTechniques for a Java Just-In-Time Compiler.\" In Conference on Object Oriented Programming Systems, Languages \n&#38; Applications OOP-SLA, 2000. [12] M. Kawahito, H. Komatsu, and T. Nakatani. Effective Null Pointer \nCheck Elimination Utilizing Hardware Trap, In Pro\u00ad ceedings of the International Conference on Architectural \nSupport for Programming Language and Operating Systems, pp. 139-149, 2000. [13] J. Knoop, O. R\u00fcthing, \nand B. Steffen. Lazy code motion. In Proceedings of the ACM SIGPLAN Conference on Pro\u00adgramming Language \nDesign and Implementation, ACM SIGPLAN Notices, Vol. 27, No. 7, pp. 224-234, San Fran\u00adcisco, CA, June \n1992. [14] J. Knoop, O. R\u00fcthing, and B. Steffen. Optimal code motion: Theory and practice. ACM Transactions \non Programming Languages and Systems, Vol. 17, No. 5, pp. 777-802, 1995. [15] J. Knoop, O. R\u00fcthing, and \nB. Steffen. Partial dead code elimination. In Proceedings of the ACM SIGPLAN '94 Con\u00adference on Programming \nLanguage Design and Implementa\u00adtion, pp. 147-158, Orlando, Florida, June 1994. [16] T. Ogasawara, H. \nKomatsu, and T. Nakatani. \"A Study of Exception Handling and Its Dynamic Optimization in Java\", In Proceedings \nof the International Conference on Object-Oriented Programming, Systems, Languages, and Applica\u00adtions, \nOctober 2001 [17] PentiumGCC Frequently asked Questions  http://www.goof.com/pcg/pgcc-faq.html [18] \nStandard Performance Evaluation Corp. \"SPEC JVM98 Benchmarks,\" http://www.spec.org/osg/jvm98/ [19] T. \nSuganuma, T. Ogasawara, M. Takeuchi, T. Yasue, M. Kawahito, K. Ishizaki, H. Komatsu, T. Nakatani. Overview \nof the IBM Java Just-in-Time Compiler, IBM Systems Jour\u00adnal, Vol. 39, No. 1, 2000. [20] T. Suganuma, \nT. Yasue, M. Kawahito, H. Komatsu, and T. Nakatani. A Dynamic Optimization Framework for a Java Just-In-Time \nCompiler , In Proceedings of the International Conference on Object-Oriented Programming, Systems, Languages, \nand Applications, October 2001 [21] Tim A. Wagner, Vance Maverick, Susan L. Graham, and Michael A. Harrison. \nAccurate Static Estimators for Pro\u00adgram Optimization , In Proceedings of the ACM SIGPLAN Conference on \nProgramming Language Design and Imple\u00admentation, ACM SIGPLAN Notices, pp. 85-96, 1994. APPENDIX Table \n4 and Table 5 show the effectiveness of each theorem. In these tables, Theorem 1 corresponds to the case \nin which only Theorem 1 is used for elimination of array indices for new algo\u00adrithm (all) . In summary, \nTheorem 4 is particularly effective among all, and combining several theorems is most effective for many \nbenchmarks. Table 4. Effectiveness of each theorem (dynamic counts of remaining 32-bit sign extensions \nfor jBYTEmark) Table 5. Effectiveness of each theorem (dynamic counts of remaining 32-bit sign extensions \nfor SPECjvm98) Numeric Sort String Sort Bitfield FP Emu. Fourier Assignment IDEA Huffman Neural Net \nLU Decom. insert, order 1004320308 366867867 413052165 1123692 90139 860117895 341854867 444477957 320991237 \n738365142 20104474 437009 20300531 240368 new algorithm (all) 466205975 31158251 412953766 24770 10759 \n144163 20484964 87114 802576 41980  mtrt jess compress db mpegaudio jack javac insert, order 7702613 \n58537108 571386557 157417853 237099838 37781407 86758551 50888050 47888369 64421260 45041715 new algorithm \n(all) 816153 12104429 251303625 7472849 53079482 18844058 29942890  \n\t\t\t", "proc_id": "512529", "abstract": "Computer designs are shifting from 32-bit architectures to 64-bit architectures, while most of the programs available today are still designed for 32-bit architectures. Java#8482;, for example, specifies the frequently used int\" as a 32-bit data type. If such Java programs are executed on a 64-bit architecture, many 32-bit values must be sign-extended to 64-bit values for integer operations. This causes serious performance overhead. In this paper, we present a fast and effective algorithm for eliminating sign extensions. We implemented this algorithm in the IBM Java Just-in-Time (JIT) compiler for IA-64#8482;. Our experimental results show that our algorithm effectively eliminates the majority of sign extensions. They also show that it significantly improves performance, while it increases JIT compilation time by only 0.11%. We implemented our algorithm for programs in Java, but it can be applied to any language requiring sign extensions.", "authors": [{"name": "Motohiro Kawahito", "author_profile_id": "81100231881", "affiliation": "IBM Tokyo Research Laboratory, Kanagawa, 242-8502, Japan", "person_id": "P203163", "email_address": "", "orcid_id": ""}, {"name": "Hideaki Komatsu", "author_profile_id": "81100557247", "affiliation": "IBM Tokyo Research Laboratory, Kanagawa, 242-8502, Japan", "person_id": "PP39048455", "email_address": "", "orcid_id": ""}, {"name": "Toshio Nakatani", "author_profile_id": "81100311827", "affiliation": "IBM Tokyo Research Laboratory, Kanagawa, 242-8502, Japan", "person_id": "PP14113792", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/512529.512552", "year": "2002", "article_id": "512552", "conference": "PLDI", "title": "Effective sign extension elimination", "url": "http://dl.acm.org/citation.cfm?id=512552"}