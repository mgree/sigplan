{"article_publication_date": "05-17-2002", "fulltext": "\n A Sparse Algorithm for Predicated Global Value Numbering Karthik Gargi Hewlett-Packard India Software \nOperation 29 Cunningham Road, Bangalore, Karnataka, India kg@india.hp.com ABSTRACT This paper presents \na new algorithm for performing global value numbering on a routine in static single assignment form. \nOur algorithm has all the strengths of the most pow\u00aderful existing practical methods of global value \nnumbering; it uni.es optimistic value numbering with constant folding, al\u00adgebraic simpli.cation and unreachable \ncode elimination. It goes beyond existing methods by unifying optimistic value numbering with further \nanalyses: it canonicalizes the struc\u00adture of expressions in order to expose more congruences by performing \nglobal reassociation, it exploits the congruences induced by the predicates of conditional jumps (predicate \ninference and value inference), and it associates the argu\u00adments of acyclic a-functions with the predicates \ncontrolling their arrival (a-predication), thus enabling congruence .nd\u00ading on conditional control structures. \nFinally, it implements an e.cient sparse formulation and o.ers a range of tradeo.s between compilation \ntime and optimization strength. We describe an implementation of the algorithm and present measurements \nof its strength and e.ciency collected when optimizing the SPEC CINT2000 C benchmarks. Categories and \nSubject Descriptors D.3.4 [Programming Languages]: Processors optimiza\u00adtion; F.3.2 [Logics and Meanings \nof Programs]: Seman\u00adtics of Programming Languages program analysis General Terms Algorithms, Performance, \nMeasurement, Experimentation Keywords Global value numbering, static single assignment 1. INTRODUCTION \nThis paper presents a new algorithm for performing global value numbering on a routine in static single \nassignment form. Permission to make digital or hard copies of all or part of this work for personal or \nclassroom use is granted without fee provided that copies are not made or distributed for profit or commercial \nadvantage and that copies bear this notice and the full citation on the first page. To copy otherwise, \nto republish, to post on servers or to redistribute to lists, requires prior specific permission and/or \na fee. PLDI 02, June 17-19, 2002, Berlin, Germany. Copyright 2002 ACM 1-58113-463-0/02/0006 ...$5.00. \n 1.1 Background Global value numbering (GVN) [1, 7, 8, 13, 4] partitions the values computed by a routine \ninto congruence classes. Congruence is a compile-time approximation to run-time equivalence. Congruent \nvalues are guaranteed to be iden\u00adtical for any possible execution of the routine. They are po\u00adtential \ncandidates for optimizations such as constant propa\u00adgation, copy propagation and redundancy elimination. \nIt is safe to place identical values in di.erent congruence classes but every missed congruence is a \npotentially missed oppor\u00adtunity for optimization. Nothing is guaranteed about non\u00adcongruent values, not \neven that they di.er from each other. Static single assignment (SSA) [9] is a property of a pro\u00adgram \nrepresentation. SSA enables stronger and faster opti\u00admizations as well as a uniform treatment of local \nand global optimization. It improves the precision, compactness and ef\u00ad.ciency of use-def chains by factoring \nthem over control .ow edges through a-functions. A routine is said to be in SSA form when every variable \nhas a single de.nition and every use of a variable is reached by a single de.nition. As a con\u00adsequence \nof this property, SSA de.nitions cannot be killed. The SSA property is violated by multiply de.ned variables; \nconversion to SSA form splits multiple de.nitions of a vari\u00adable into single de.nitions of multiple new \nvariables. Where multiple de.nitions of a variable reach a use, a a-function is introduced to merge them \ninto a single de.nition of a new variable. Congruence .nding can be performed e.ciently in SSA form. \nTwo values are congruent if they are identical, or if they are de.ned by identical functions on congruent \nargu\u00adments. In SSA form the arguments have single de.nitions, so their congruence can be e.ciently determined. \nExisting algorithms for global value numbering may be classi.ed as optimistic or pessimistic [1, 16, \n7]. Optimistic algorithms start with the optimistic assump\u00adtion: the start block of the control .ow graph \n(CFG) of the routine is reachable, the other basic blocks and edges of the CFG are unreachable, and the \nvalues of the routine are con\u00adgruent to each other. Applying this assumption uncovers new facts about \nthe reachability of blocks and edges, and the congruences between values. These facts may be incon\u00adsistent \nwith the assumption: unreachable blocks and edges may prove to be reachable and congruent values may \nprove to be non-congruent. These inconsistencies are resolved by assuming that inconsistent blocks and \nedges are reachable and inconsistent values are non-congruent. The re.ned as\u00adsumption is now reapplied \nand the process repeats until the assumption becomes consistent. Pessimistic algorithms start with the \npessimistic assump\u00adtion: the blocks and edges of the CFG are reachable, and the values of the routine \nare congruent only to themselves. It is safe to assume that an unreachable block or edge is reachable \nor that congruent values are non-congruent, so this assumption cannot result in inconsistencies and does \nnot need to be re.ned or reapplied. Optimistic algorithms are stronger than pessimistic al\u00adgorithms. \nThey can detect unreachable blocks and edges, ignore values carried by unreachable edges, and handle \nthe cyclic values which are carried across loop iterations. Cyclic values are de.ned by cyclic a-functions \nwhich have at least one argument carried by a back edge. Optimistic algorithms initially ignore the values \ncarried by back edges, i.e. they as\u00adsume that cyclic values are congruent to their initial values at \nloop entry. When this assumption is consistent, cyclic values are loop invariant. When this assumption \nis incon\u00adsistent, optimistic algorithms can determine the values car\u00adried by back edges and .nd the cyclic \ncongruences between cyclic values. Pessimistic algorithms cannot detect unreach\u00adable blocks and edges, \nignore values carried by unreachable edges, or ignore or determine values carried by back edges. So they \ncannot detect loop invariant cyclic values or cyclic congruences. Pessimistic algorithms are faster than \noptimistic algo\u00adrithms. The pessimistic assumption does not need to be re.ned or reapplied, but the optimistic \nassumption must be repeatedly re.ned and reapplied until it becomes consistent. Global value numbering \ncan be made stronger by unify\u00ading it with additional analyses [1, 16, 7]. These analyses can use the \ncongruences found by global value numbering to discover new facts about the routine, and global value \nnum\u00adbering can use these facts to .nd new congruences. This synergy makes uni.ed algorithms stronger \nthan repeated in\u00advocations of their component analyses. 1.2 Balanced Algorithms In addition to optimistic \nand pessimistic algorithms, we introduce the class of balanced algorithms. Balanced algo\u00adrithms start \nwith the balanced assumption: the start block of the CFG is reachable, the other blocks and edges of \nthe CFG are unreachable, and the values of the routine are congruent only to themselves. The balanced \nassumption is equivalent to the optimistic assumption for the reachability of blocks and edges and the \npessimistic assumption for the congruence of values. Balanced algorithms lie between optimistic and pessimis\u00adtic \nalgorithms in strength. They can detect unreachable blocks and edges, and ignore values carried by unreachable \nnon-back edges. But they cannot ignore values carried by unreachable back edges, or ignore or determine \nvalues car\u00adried by back edges. So they cannot detect loop invariant cyclic values or cyclic congruences. \nBalanced algorithms lie between optimistic and pessimis\u00adtic algorithms in speed. The balanced assumption \nmust also be repeatedly re.ned and reapplied until it is consistent, but not as much as the optimistic \nassumption. Our measurements indicate that in practice balanced al\u00adgorithms are almost as strong as optimistic \nalgorithms and almost as fast as pessimistic algorithms. Unreachable code is much more common in practice \nthan unreachable back edges, loop invariant cyclic values and cyclic congruences. Re.ning and reapplying \nthe balanced assumption is very fast in practice because cyclic values are not involved. 1.3 The New \nAlgorithm Our new algorithm has all the strengths of the most pow\u00aderful existing practical methods of \nglobal value numbering [7, 13]; it uni.es optimistic value numbering with constant folding, algebraic \nsimpli.cation and unreachable code elim\u00adination, thereby subsuming Wegman and Zadeck s sparse conditional \nconstant propagation algorithm [16]. Our algorithm goes beyond existing methods by unify\u00ading optimistic \nvalue numbering with further analyses. It canonicalizes the structure of expressions in order to ex\u00adpose \nmore congruences by performing forward propagation and applying the commutative, associative and distributive \nlaws (global reassociation [3]). It exploits the congruences induced by the predicates of conditional \njumps (predicate inference and value inference). And it associates the argu\u00adments of acyclic a-functions \nwith the predicates controlling their arrival (a-predication), thus enabling congruence .nd\u00ading on conditional \ncontrol structures. Our measurements in\u00addicate that the new algorithm obtains modest improvements over \nthe strongest existing practical global value numbering algorithm due to Click [7], and Wegman and Zadeck \ns sparse conditional constant propagation algorithm [16]. The example in Figure 1 illustrates the strength \nof our algorithm which is currently unique in being able to deter\u00admine that routine R is guaranteed to \nalways return 1. This is achieved through a chain of inferences based on the SSA form and CFG of the \nroutine R given in Figure 2. Opti\u00admistic value numbering ignores I17 because it is carried by a back \nedge, so I2 must be 1. Unreachable code elimina\u00adtion ignores the de.nition of I4, so I5 must be 1. Value \ninference concludes that Y0 has the value X0 in blocks 6 16 because they are dominated by the true outgoing \nedge from the conditional jump with the predicate Y0 = X0 (in\u00adstruction 5.2 ). Unreachable code elimination \nignores the de.nition of P8. .-predication enables congruence .nding to determine that Q14 is congruent \nto P11. Predicate infer\u00adence concludes that the predicate Z0 < 1 is false in block 15 because it is dominated \nby the true outgoing edge from the conditional jump with the predicate Z0 >I5 (instruction 14.2 ). Constant \nfolding, algebraic simpli.cation and global reassociation determine that I15 is 1, so both I16 and I17 \nmust be 1. Optimistic value numbering con.rms the initial assumption that I2 is 1, so I18 must be 1. \nTherefore routine R is guaranteed to always return 1. This result can only be achieved by an algorithm \nthat exploits the congruences induced by the predicates of con\u00additional jumps. If predicate inference, \nvalue inference or a-predication are not performed, it will break the chain of inferences by which the \nresult is achieved. This result can also only be achieved by a uni.ed algorithm. If any of the analyses \nare performed separately from the rest, it will break the chain of inferences by which the result is \nachieved. Our algorithm implements an e.cient sparse formulation. When an optimistic assumption is found \nto be inconsistent, it is re.ned and reapplied. A brute force approach would reapply it to the entire \nroutine. Our algorithm reapplies it only to those blocks, edges and instructions which may be a.ected \nby the re.nement. Our measurements indicate that the algorithm takes less than 4% of the total optimization \ntime and the speedup due to sparseness is 1.23 1.57. 01 routine R(integer X, integer Y , integer Z) 02 \nreturns integer 03 I + 1 04 J + 1 05 while (true) 06 if (J > 9) break 07 J + J + 1 08 if (I = 1) I + \n2 09 if (Y = X) 10 P + 0 11 if (X . 1) 12 if (I = 1) P + 2 else if (X . 9) P + I 13 Q + 0 14 if (I . \nY ) 15 if (9 . Y ) Q + 1 16 if (Z > I) 17 I + P + (X + 2) + (Z < 1) -(I + Y ) -Q 18 return (I)  Figure \n1: An example illustrating the strength of our algorithm which is currently unique in being able to determine \nthat the routine R is guaranteed to always return 1. Our algorithm o.ers a range of tradeo.s between \ncompila\u00adtion time and optimization strength. It can perform bal\u00adanced instead of optimistic value numbering \non a routine, and it allows the other analyses to be selectively disabled. This .exibility can be exploited \nto achieve scalability by dis\u00adabling optimistic value numbering and expensive analyses for the infrequently \nexecuted routines of a program (typi\u00adcally determined from pro.le information). Our measure\u00adments indicate \nthat the algorithm performs balanced value numbering as fast as pessimistic value numbering and 1.39 \n1.90 times faster than optimistic value numbering; and it runs 1.15 1.32 times faster when global reassociation, \npred\u00adicate inference, value inference and a-predication are dis\u00adabled. Existing algorithms do not o.er \nthis .exibility, so they require the compiler writer to choose between mini\u00admizing compile time, maximizing \noptimization strength or implementing multiple algorithms. We describe two versions of our algorithm: \nthe complete algorithm and the practical algorithm. The complete al\u00adgorithm requires the incremental \ncomputation of the dom\u00adinator tree [14]. It completely uni.es predicate and value inference with unreachable \ncode elimination. The practical algorithm does not require the incremental computation of the dominator \ntree. It does not completely unify predicate and value inference with unreachable code elimination, but \nit is simpler to implement. When performing optimistic value numbering on a cyclic CFG, both the complete \nand the practical algorithm have a worst case time complexity of O(CE2(E + I)), where C is the loop connectedness1 \nof the SSA def-use graph, E is the number of edges in the CFG, and I is the number of in\u00adstructions in \nthe routine. When performing optimistic value numbering on an acyclic CFG or balanced value numbering \non any CFG, the worst case time complexity of these al\u00adgorithms reduces to O(E2(E + I)). In spite of \nthis worst case time complexity, our measurements indicate that the practical algorithm is quite e.cient \nin practice. Key: block-number.instruction-number optimistic value(s) instruction [, pessimistic value(s)] \n Unreachable edge \u00ad >  Reachable edge Reachable edge initially assumed to be unreachable - Blocks are \nnumbered in reverse post order. SSA variables are labelled with their de.ning block numbers.  Figure \n2: The results of applying our algorithm to the SSA form and CFG of routine R from Figure 1. We have \nimplemented the practical algorithm in an in\u00adof back edges in any acyclic path of the graph. ternal version \nof the PA-RISC C compiler on HP-UX, and 1The loop connectedness of a graph is the maximum number measured \nits strength and e.ciency when optimizing the SPEC CINT2000 C benchmarks. The rest of this paper is organized \nas follows. Section 2 describes our algorithm. Section 3 brie.y describes how to implement the algorithm \ne.ciently and e.ectively. Section 4 analyzes the complexity of the algorithm. Section 5 de\u00adscribes our \nimplementation of the practical algorithm and presents measurements of its strength and e.ciency. Sec\u00adtion \n6 discusses related work. Section 7 concludes the paper. 2. GLOBAL VALUE NUMBERING This section describes \nour algorithm. We simplify the de\u00adscription by leaving issues of practicality and e.ciency to section \n3. We start with an incomplete and oversimpli.ed description of the algorithm, and supply the missing \ndetails and correct the oversimpli.cations in stages. We also give complete pseudocode for the algorithm \n(this contains for\u00adward references to material that is described later). The algorithm starts with a \nroutine in SSA form. It re\u00adquires the CFG and the SSA def-use and use-def chains of the routine. The \npractical algorithm also requires the dom\u00adinator tree of the routine. The goal of the algorithm is to \ndetermine the reachability of every block and edge, the values, leader and de.ning ex\u00adpression of every \ncongruence class, and the congruence class of every value of the routine (see sections 2.1 and 2.2). \nWhen the algorithm has .nished, every reachable block and edge is in the REACHABLE set, every congruence \nclass is repre\u00adsented by a set of values, the leader and de.ning expression of every congruence class \nare given by the LEADER and EXPRESSION mappings respectively, and the congruence class of every value \nis given by the CLASS mapping (see sections 2.1 and 2.2). The results of global value numbering can now \nbe used to perform optimizations such as unreachable code elimi\u00adnation, constant propagation, copy propagation \nand redun\u00addancy elimination. 2.1 Optimistic Value Numbering The algorithm starts with a routine in SSA \nform. The start block is made reachable, the other blocks and edges are made unreachable, and the instructions \nof the start block are touched to indicate that they must be processed. The algorithm then makes a reverse \npost order (RPO) pass over the blocks of the routine. Each instruction of every block is examined. If \nan instruction is untouched, it is left alone. Otherwise, it is wiped to indicate that it has been processed. \nIf it is in an unreachable block, it is left alone. Otherwise, the expression that it computes is subjected \nto symbolic evaluation and congruence .nding. If the value produced by the instruction has not changed, \nit is left alone. Otherwise, the instructions that consume its result are touched. These are determined \nfrom the def-use chains of the routine. This pass is repeated until there are no touched instruc\u00adtions \nleft. Touched instructions are kept in the TOUCHED set,2 so the algorithm terminates when TOUCHED becomes \nempty. Reachable blocks and edges are kept in the REACH-ABLE set. When the algorithm has .nished, blocks \nand edges not in REACHABLE are unreachable. 2Blocks are also kept in TOUCHED (see section 2.8). Figure \n3 gives the algorithm for global value numbering.3 01 algorithm Global Value Numbering 02 Assign ranks \nto values() 03 BACKWARD + { edge E' 04 | E'.destination block.RPO number 05 . E'.originating block.RPO \nnumber } 06 TOUCHED + routine.start block.instructions 07 REACHABLE + { routine.start block } 08 CHANGED \n+ . 09 for each B = routine.blocks 10 PREDICATE[B] + \u00d8= 11 for each E = B.outgoing edges 12 PREDICATE[E] \n+ \u00d8= 13 INITIAL + routine.values 14 LEADER[INITIAL] + . 15 EXPRESSION [INITIAL] + . 16 for each V = INITIAL \n17 CLASS[V ] + INITIAL 18 Clear TABLE 19 if (use complete algorithm) 20 Initialize the reachable dominator \ntree 21 while (TOUCHED = .) 22 for each B = routine.blocks in RPO 23 if (B = TOUCHED) 24 TOUCHED + TOUCHED \n\\ { B } 25 if (B = REACHABLE) 26 Compute predicate of block(B) 27 for each I = B.instructions in RPO \n28 if (I = TOUCHED) 29 TOUCHED + TOUCHED \\ { I } 30 if (B = REACHABLE) 31 if (I = \u00d8V + E=) 32 E' + Perform \nsymbolic evaluation(E, B) 33 Perform congruence .nding(V, E ') 34 else if (I is a jump) 35 Process outgoing \nedges of block(B) 36 if (\u00ac perform optimistic value numbering) break  Figure 3: The algorithm for global \nvalue numbering. 2.2 Symbolic Evaluation A constant symbolically evaluates to itself, and a variable \nsymbolically evaluates to the leader of its congruence class. A congruence class is represented by a \nset of values, and the congruence class of a value is given by the CLASS mapping. The leader of a congruence \nclass is its representative value, and is given by the LEADER mapping. It may be a constant or a value \nof the congruence class. The algorithm initially places the values of the routine in the INITIAL congruence \nclass. The leader of INITIAL is the undetermined value .. When the algorithm has .nished, values still \nin INITIAL are unreachable. An expression is symbolically evaluated after symbolically evaluating its \noperands. Constant folding and algebraic simpli.cation are performed on it. Global reassociation is performed \nby replacing its operands with their de.ning ex\u00adpressions (forward propagation) and applying the commuta\u00adtive, \nassociative and distributive laws to restructure it into a canonical form. Forward propagation requires \nthe de.n\u00ading expression of a congruence class, which is given by the EXPRESSION mapping. The canonical \nform of an arithmetic expression is a sum of products of values, where sums and products are represented \nby ordered lists. An arithmetic expression is canonicalized 3We use the notation a = ( to mean a has \nthe form (. by combining the de.ning expressions of its operands (which are already in canonical form \nbecause they are processed be\u00adfore their consumers in RPO) and applying the commuta\u00adtive, associative \nand distributive laws. Values and products of values that di.er only in sign are treated as equal when \nordering lists. Thus X + Y is canonicalized by merging the ordered lists of products of values that de.ne \nX and Y into an ordered list, and X * Y is canonicalized by distributing multiplication over addition, \ncombining the ordered lists of products of values that de.ne X and Y into an ordered list. Other kinds \nof expressions are handled in a similar fashion.4 The order of operands in commutative and associative \nex\u00adpressions is canonicalized by arranging operands in order of increasing rank. The rank of a value \nis given by the RANK mapping. Our de.nition of ranks is more .ne grained than the de.nitions of [11, \n3]. Constants are assigned the rank 0. Values are assigned ranks starting from 1 such that lower ranks \ncorrespond to earlier de.nitions in an RPO traver\u00adsal of the CFG. Thus constants are arranged before \nvalues and values with earlier de.nitions before values with later de.nitions. A a-function is subjected \nto special transformations. Ar\u00adguments carried by unreachable edges are ignored (back edges are initially \nunreachable, so this transformation will initially optimistically ignore values carried by back edges). \nIf all the arguments are congruent to the same value, the a-function is reduced to that value. Finally, \nits containing block is made an argument of the a-function. This pre\u00advents congruences between a-functions \nin di.erent blocks. Such congruences are incorrect because the a-functions of classical SSA do not associate \nreaching de.nitions with the conditions under which they arrive. Figure 4 gives the algorithm for symbolic \nevaluation.  2.3 Congruence Finding The congruence class of an expression is given by the TA-BLE mapping. \nIn the special case when the expression is a variable, its congruence class is given by the CLASS map\u00adping. \nIf TABLE does not contain the congruence class of an expression, it is updated to map the expression \nonto a new congruence class. The result of the instruction com\u00adputing the expression becomes the leader \nand sole member of the new congruence class. In the special case when the expression is a constant, it \nbecomes the leader of the new congruence class. Finally, EXPRESSION is updated to map the new congruence \nclass onto the expression. A value is directly changed by a change in its leader or congruence class. \nA change in its leader causes a change in its symbolic evaluation. A change in its congruence class is \ncaused by a change in its de.ning expression, which causes a change in its forward propagation.5 The \nvalues of a congruence class are indirectly changed when their leader moves to a new congruence class. \nThis 4Forward propagation is cancelled when the number of operands in an expression exceeds a .xed limit. \nThis en\u00adsures that global reassociation processes an expression in bounded time, and prevents the exponential \nbehaviour that can occur with the unrestricted application of forward prop\u00adagation and the associative \nand distributive laws. 5As a special case, the leader of a singleton congruence class need not be moved \nto a new congruence class unless it is subject to forward propagation. 01 algorithm Assign ranks to values \n02 rank + 0 03 for each B = routine.blocks in RPO 04 for each I = B.instructions in RPO 05 if (I =\u00d8V \n+ E=) 06 rank + rank +1 07 RANK [V ] + rank 08 algorithm Perform symbolic evaluation(expression E, 09 \nblock B) returns expression 10 if (E =\u00d8\u00a2V1 ...=) 11 if (\u00ac perform optimistic value numbering 12 . (B.incoming \nedges n BACKWARD)= .) 13 E + V 14 else 15 Let value Vi be carried by edge Ei 16 Remove all Vi | Ei = \nREACHABLE 17 X + PREDICATE[B] 18 if (X = \u00d8=) X + B 19 else 20 Reorder the Vi so that the order of the \nEi 21 matches CANONICAL[B] 22 E +\u00d8\u00a2 X Infer value at edge(V1, E1) ...= 23 if (E =\u00d8\u00a2 X V1 ... V1 =) E \n+ V1 24 else if (E =\u00d8op V1 ...=) 25 E +\u00d8op Infer value at block(V1, B) ...= 26 Perform constant folding, \nalgebraic simpli.cation 27 and global reassociation on E 28 if (E is a predicate) 29 E + Infer value \nof predicate(E, B) 30 return (E) 31 algorithm Perform congruence .nding(value V, 32 expression E) 33 \nC0 + CLASS[V ] 34 if (E is a variable) C + CLASS[E] 35 else 36 C + TABLE[E] 37 if (C = .) 38 C +{ V } \n39 LEADER[C ] + if (E is a constant) E else V 40 EXPRESSION [C ] + E 41 TABLE[E] + C 42 if (C = C0 * \nV = CHANGED) 43 CHANGED + CHANGED \\{ V } 44 if (C = C0) 45 C0 + C0 \\{ V } 46 C + C U{ V } 47 CLASS[V \n] + C 48 if (C0 = .) 49 TABLE[EXPRESSION [C0]] +. 50 LEADER[C0] + \u00d8= 51 EXPRESSION [C0] + \u00d8= 52 else \nif (LEADER[C0]= V ) '' 53 LEADER[C0] + V | V = C0 54 TOUCHED + TOUCHED '' '' 55 U{ instruction I | I \nde.nes V . V = C0 } 56 CHANGED + CHANGED U C0 57 TOUCHED + TOUCHED '' 58 U{ instruction I | I uses V \n} Figure 4: The algorithm for symbolic evaluation and congruence .nding. is indicated by adding them \nto the CHANGED set. Their de.ning instructions are touched, and one of them becomes the new leader of \nthe congruence class. In the special case when the congruence class becomes empty, its associated expression \nis removed from TABLE. Figure 4 gives the algorithm for congruence .nding.  2.4 Edges A jump instruction \nrepresents the outgoing edges of its block, so they must be processed when it is touched. An edge X-Y \nis processed by evaluating its reachability. If there is no change in its reachability, it is left alone. \nOtherwise, if Y is unreachable, it is made reachable and its instructions are touched. Otherwise, the \na-instructions of Y are touched. Figure 5 gives the algorithm for processing the outgoing edges of a \nblock. 01 algorithm Process outgoing edges of block(block B) 02 for each E = B.outgoing edges 03 reachable \n+ Evaluate the reachability of E 04 if (reachable . E = REACHABLE) 05 REACHABLE + REACHABLE U{ E } 06 \nD + E.destination block 07 if (D = REACHABLE) 08 REACHABLE + REACHABLE U{ D } 09 TOUCHED + TOUCHED 10 \nU D.instructions U{ D } 11 else 12 TOUCHED + TOUCHED U D.\u00a2-instructions 13 Propagate change in edge(E) \n14 if (use complete algorithm) 15 Update the reachable dominator tree 16 P + Perform symbolic evaluation( \n17 PREDICATE[E], B) 18 if (P is a constant) P + \u00d8= 19 if (PREDICATE[E]= P) 20 PREDICATE[E] + P 21 Propagate \nchange in edge(E) 22 algorithm Propagate change in edge(edge E) 23 D + E.destination block 24 if (use \ncomplete algorithm) 25 TOUCHED + TOUCHED '' 26 U{ instruction I | D dominates I .block } '' 27 U{ block \nB | B postdominates D } 28 else 29 TOUCHED + TOUCHED ' 30 U{ instruction I ' 31 | I .block.RPO number \n. D.RPO number } '' 32 U{ block B | B .RPO number . D.RPO number } Figure 5: The algorithm for processing \nthe outgoing edges of a block. 2.5 Back Edges Back edges are approximated by RPO back edges. The destination \nblock of an RPO back edge precedes its origi\u00adnating block in RPO. The RPO back edges of the routine are \nkept in the BACKWARD set. 2.6 Balanced Value Numbering When performing balanced instead of optimistic \nvalue numbering, there are two changes in the behaviour of the al\u00adgorithm. It treats every cyclic a-function \nas a unique value, and it terminates immediately after the .rst pass. 2.7 Predicate and Value Inference \nThe value of a predicate can be inferred if it is computed in a block dominated by an edge and it is \nrelated to the predicate of the edge. An edge dominates a block if it is the only reachable incoming \nedge of a dominator of the block. The dominators of a block are found by starting from the block and \ngoing up the dominator tree. The predicate of an edge is X op Y if it is the true outgoing edge from \nthe conditional jump if (X op Y ) . . . . The predicate of an edge is given by the PREDICATE mapping.6 \nThus the value of X< 0 is false in a block which is dominated by an edge with the predicate X> 0. The \nvalue of a variable can be inferred if it is used in a block dominated by an edge and it is related to \nthe predicate of the edge. An argument of a a-function is considered to be used at the edge which carries \nit. Thus the value of X is 0 in a block which is dominated by an edge with the pred\u00adicate X = 0. When \nthe predicate compares two variables, the higher ranking variable is replaced by the lower ranking variable; \nthis bias towards de.nitions that dominate larger regions increases the number of congruences found. \nWhen one variable is replaced by another, value inference is re\u00adpeated on the new variable. This second \ninference stops at the edge inducing the .rst inference. In Figure 6, the value of K3 is J2 at line 07 \nbecause it is dominated by the true outgoing edge from the conditional jump with the predicate K3 = J2 \nat line 05. The value of J2 is I1 at line 07 because it is dominated by the true outgoing edge from the \ncondi\u00adtional jump with the predicate J2 = I1 at line 06. Thus X1 is congruent to I1 + 1. 01 I1 + ... \n02 J2 + ... 03 K3 + ... Figure 6: An example 04 ... + ' of value inference: X1 is 05 if (K3 = J2) \ncongruent to I1 +1. 06 if (J2 = I1) 07 X1 + K3 +1 Figure 7 gives the algorithm for predicate and value \ninfer\u00adence. Value inference usually .nds more congruences in practice, but this cannot be guaranteed. \nIn Figure 6, value inference makes X1 congruent to I1 + 1 instead of K3 +1. If\\ is I1 + 1 it has found \nan additional congruence; if \\ is K3 +1 it has lost an existing congruence. The complete algorithm determines \ndominance relation\u00adships from the reachable dominator tree, so it completely uni.es predicate and value \ninference with unreachable code elimination. The reachable dominator tree is the dominator tree of the \ncurrently reachable portion of the CFG; it is built incrementally as blocks and edges become reachable \nusing the algorithm of [14]. The practical algorithm determines dominance relation\u00adships from the dominator \ntree of the routine. It improves upon this by checking for the special case when a block is dominated \nby a single reachable incoming edge. This is a subset of the dominance relationships determined from \nthe reachable dominator tree, so it does not completely unify predicate and value inference with unreachable \ncode elimi\u00adnation, but it is simpler to implement. When the reachability or predicate of an edge X-Y \nchanges, the complete algorithm touches the instructions of the blocks dominated by Y . The practical \nalgorithm con\u00adservatively approximates this by touching the instructions of the blocks downstream of \nY in RPO,7 so it must disallow predicate and value inference along any path containing a 6The predicate \nof a block is also given by the PREDICATE mapping (see section 2.8). 7This is necessary only when Y becomes \na con.uence node. 01 algorithm Infer value of predicate(expression P, 02 block B) returns expression \n03 while (B = \u00d8=) 04 if ((\u00ac perform optimistic value numbering 05 . (B.incoming edges n BACKWARD) = .) \n06 * n edge E | { E } = (B.incoming edges 07 n REACHABLE)) 08 B + B.immediate dominator 09 continue 10 \nif (\u00ac use complete algorithm . E = BACKWARD) 11 break 12 if (P is related to PREDICATE[E]) 13 P + Infer \nvalue of P from PREDICATE[E] 14 break 15 B + E.originating block 16 return (P) 17 algorithm Infer value \nat block(value V, block B) 18 returns value 19 if (V is a variable) 20 V + LEADER[CLASS[V ]] 21 .rst \nblock + B 22 last block + \u00d8= 23 while (V is a variable . B = last block) 24 B + .rst block 25 while (B \n= last block) 26 if ((\u00ac perform optimistic value numbering 27 . (B.incoming edges n BACKWARD) = .) 28 \n* n edge E | { E } = (B.incoming edges 29 n REACHABLE)) 30 B + B.immediate dominator 31 continue 32 if \n(\u00ac use complete algorithm 33 . E = BACKWARD) 34 B + last block 35 break 36 if (PREDICATE[E] = \u00d8X = Y \n= 37 . RANK [X ] < RANK [Y ] 38 . CLASS[Y ] = CLASS[V ]) 39 V + X 40 last block + B 41 B + \u00d8= 42 break \n43 B + E.originating block 44 return (V ) 45 algorithm Infer value at edge(value V, edge E) 46 returns \nvalue 47 if (V is a variable) 48 V + LEADER[CLASS[V ]] 49 if (PREDICATE[E] = \u00d8X = Y = 50 . RANK [X ] \n< RANK [Y ] 51 . CLASS[Y ] = CLASS[V ]) 52 V + X 53 else V + Infer value at block(V, E.originating block) \n54 return (V )  Figure 7: The algorithm for predicate and value in\u00adference. back edge. The special case \nof value inference induced by a back edge on an argument of a a-function carried by it is allowed because \nthis dependency is captured by def-use chains. Thus in Figure 6, a change in the value of I1 will change \nthe predicate J2 = I1; this will cause the instruction in line 07 to be touched. 2.8 <-Predication We \nde.ne two a-functions to be congruent if their argu\u00adments are congruent and either their blocks are identical \nor the predicates of their blocks are congruent. The predicate of a block B with reachable incoming edges \nE1, E2, ... has the form P1 * P2 * ..., where the predicate Pi is true when and only when control .ow \nreaches B through a path from its immediate dominator D to Ei . The predicate of a block is given by \nthe PREDICATE mapping. The last part of this de.nition can be understood by con\u00adsidering a a-function \na(D1,D2,...) of B, where the de.ni\u00adtion Di reaches B through a path from the start block to Ei. This \ncan be written as if (R1) D1 else if (R2) D2 ..., where Ri is true when and only when control .ow reaches \nB through a path from the start block to Ei. All paths from the start block to B must pass through D, \nso the Ri can be replaced by the Pi. Then the a-function becomes if (P1) D1 else if (P2) D2 .... Thus \ntwo a-functions are congruent if their arguments are congruent and the predicates of their blocks are \ncongruent. The predicate of a block B is computed by traversing all the reachable paths from its immediate \ndominator D to B. B must postdominate D8 and traversed edges must not be back edges. The partial predicate \nof a block is computed after its incoming edges are traversed and before its outgo\u00ading edges are traversed. \nThe partial predicate 1(B ' ) of a block B ' with reachable incoming edges E1' , E2' , ... and B '' \nassociated predecessor blocks B1 '' , 2 , ... has the form (1(B ''' ' 1 ) 1 PREDICATE[E1]) * (1(B2 '' \n) 1 PREDICATE[E2]) * . . . . By de.nition 1(D) is empty and 1(B) is also the predicate of B. The partial \npredicate of a block is given by the PARTIAL PREDICATE mapping. In the special case when a block X dominates \nits imme\u00addiate postdominator Y and Y is not B, the outgoing edges of Y are traversed instead of the outgoing \nedges of X (all paths from D to X must go through Y before reaching B, so paths from X to Y cannot a.ect \nthe predicate of B). Congruences between a-functions are increased by canon\u00adicalizing the predicates \nof edges and traversing outgoing edges in a canonical order. The predicates of edges are canonicalized \nby arranging their operands in order of in\u00adcreasing rank. This may change the operator of the pred\u00adicate \n(thus reversing the operand order of Y >X gives X <Y ). For a block ending with a conditional jump, the \noutgoing edges are arranged so that the predicate of the .rst outgoing edge has the operator =, < or \n.. The CANON-ICAL mapping gives a canonical ordering of the reachable incoming edges of a block. This \nis computed along with the predicate of a block. There is a one-to-one correspondence between the reach\u00adable \nincoming edges of a block, the sub-predicates of the predicate of the block and the reaching de.nitions \nof a a\u00ad function of the block. If the reachable incoming edges of a block are reordered, the reaching \nde.nitions of the a-func\u00adtions of the block must also be reordered to maintain the correspondence. Symbolic \nevaluation replaces a block argument of a a\u00adfunction by its non-empty predicate in order to capture con\u00adgruences \nbetweeen a-functions in di.erent blocks. A block is touched to indicate that its predicate must be computed. \nWhen there is a change in the reachabil\u00adity or predicate of an edge X-Y , the complete algorithm 8This \ne.ciently excludes paths that may not reach B. Pre\u00adcomputing the set of blocks reachable from every block \nwould allow excluding paths that cannot reach B. touches the blocks that postdominate Y . 9 The practical \nal\u00adgorithm conservatively approximates this by touching the blocks downstream of Y in RPO. Finally, when \nthe predi\u00adcate of a block changes, its a-functions are touched. Figure 8 gives the algorithm for a-predication. \nSee sec\u00adtion 2.10 for an example of a-predication. 01 algorithm Compute predicate of block(block B0) \n02 D0 + B0.immediate dominator 03 if (B0 postdominates D0) 04 INITIALIZED + . 05 CANONICAL[B0] + \u00d8= 06 \nCompute partial predicate of block(D0, \u00d8=, true) 07 if (PREDICATE[B0] = PARTIAL PREDICATE[B0]) 08 PREDICATE[B0] \n+ PARTIAL PREDICATE[B0] 09 TOUCHED + TOUCHED U B0.\u00a2-instructions 10 algorithm Compute partial predicate \nof block(block B, 11 expression PP, .ag ignore incoming edges) 12 if (ignore incoming edges 13 * |B.incoming \nedges n REACHABLE| < 2) 14 PARTIAL PREDICATE[B] + PP 15 else 16 if (B = INITIALIZED) 17 INITIALIZED + \nINITIALIZED U { B } 18 PARTIAL PREDICATE[B] + \u00d8OR= 19 Make PP the next operand of 20 PARTIAL PREDICATE[B] \n21 if (PARTIAL PREDICATE[B].operand count 22 < |B.incoming edges n REACHABLE|) 23 return 24 if (B = B0) \nreturn 25 if (n D | D = B.immediate postdominator 26 . D = B0 . B dominates D) 27 Compute partial predicate \nof block(D, 28 PARTIAL PREDICATE[B], true) 29 return 30 for each E = B.outgoing edges in canonical order \n31 if (E = REACHABLE) 32 if (E = BACKWARD) 33 PARTIAL PREDICATE[B0] + \u00d8= 34 exit Compute partial predicate \nof block 35 if (|B.outgoing edges n REACHABLE| = 1) 36 EP + PARTIAL PREDICATE[B] 37 else if (PARTIAL \nPREDICATE[B] = \u00d8=) 38 EP + PREDICATE[E] 39 else 40 EP + \u00d8AND PARTIAL PREDICATE[B] 41 PREDICATE[E]= 42 \nCompute partial predicate of block( 43 E.destination block, EP, false) 44 if (E.destination block = B0) \n45 Make E the next member of CANONICAL[B0] 46 if (B = D0 47 . PARTIAL PREDICATE[B0].operand count 48 \n= |B0.incoming edges n REACHABLE|) 49 PARTIAL PREDICATE[B0] + \u00d8= Figure 8: The algorithm for a-predication. \n 2.9 Emulating Other Algorithms By enabling only selected analyses, our algorithm can be made to achieve \nthe same result as other algorithms. If only optimistic value numbering is enabled, our algorithm will \nemulate Simpson s RPO and SCC algorithms [13] and Alpern, Wegman and Zadeck s algorithm [1]. Additionally, \n9This can be re.ned by excluding blocks that do not post\u00addominate their immediate dominators. if constant \nfolding, algebraic simpli.cation and unreachable code elimination are also enabled, our algorithm will \nemulate Click s strongest algorithm [7]. Finally, if symbolic evalua\u00adtion is also modi.ed to replace \na non-constant expression by the result of the instruction computing the expression, our algorithm will \nemulate Wegman and Zadeck s sparse condi\u00adtional constant propagation algorithm [16]. 2.10 The Example \nWe will now brie.y work through parts of the example in Figure 2 using the practical algorithm. The algorithm \ninitializes REACHABLE to { 1 } and TOUCHED to { 1.1, 1.2, 1.3, 1.4 }, and performs 3 passes over the \nroutine. Pass 1 examines the instructions of blocks 1 18 in RPO. Instruction 1.2 is in TOUCHED and REACHABLE, \nso it is removed from TOUCHED and processed. TABLE does not contain the expression 1, so it is updated \nto map it onto a new congruence class { I1 } whose leader is 1. I1 has changed, so it is removed from \nCHANGED and INITIAL, and added to its new congruence class. CLASS is updated to map I1 onto its new congruence \nclass, and the consumers of I1 (instructions 2.1 and 18.1 ) are added to TOUCHED. Instruction 1.4 is \na jump, so the outgoing edges of block 1 are processed. Edge 1 -18 remains unreachable. Edge 1 -2 becomes \nreachable, so it is added to REACHABLE. Its destination block 2 becomes reachable, so it is added to \nREACHABLE, and it and its instructions are added to TOUCHED. The predicate of edge 1 -2 remains empty. \nWhen pass 1 examines instruction 2.1, I1 evaluates to 1 and I17 is ignored because it is carried by edge \n17 -2 which is not in REACHABLE, so I2 evaluates to a(2, 1) and becomes 1. The predicate of instruction \n5.2 evaluates to X0 = Y0 (assuming that X0 ranks lower than Y0). The predicate of edge 5 -6 becomes X0 \n= Y0, so blocks 5 18 and their in\u00adstructions are added to TOUCHED. Edge 5 -17 is handled similarly. Block \n6 is in TOUCHED and REACHABLE, so it is re\u00admoved from TOUCHED and its predicate is computed. It does \nnot postdominate its immediate dominator 5, so its predicate remains empty. The predicate of block 11 \nis computed by traversing all the reachable paths from block 6 to 11. The predicates of edges 6 -7 and \n9 -11 evaluate to 1 . X0 and 9 < X0 respectively, so the reachable paths from block 6 to 11 are in canonical \norder: 6 -7 -9 -11 , 6 -7 -9 -10 -11 , and 6 -11 . Thus CANONICAL[11 ] evaluates to =9 -11 10 -11 6 -11 \n=, and the partial predicates of blocks 6, 7 (and 9 ) and 10, and PREDICATE[11 ] evaluate to ==, (1 . \nX0), =AND (1 . X0) (9 . X0)=, and =OR =AND (1 . X0) (9 < X0) ==AND (1 . X0) (9 . X0) = (1 >X0) = respectively. \nInstruction 11.1 evaluates P11 to the expression a(11, 0, 1, 0) (the argument order matches CANONICAL[11 \n]). The predicates of instructions 11.3 and 12.1 evaluate to 1 . X0 and 9 . X0 respectively (value inference \nevaluates Y0 to X0). PREDICATE[14 ] evaluates to the same expression as PREDICATE[11 ]. Instruction 14.1 \ncomputes the expres\u00adsion a(14, 0, 1, 0), so Q14 evaluates to P11. The expression computed by instruction \n15.1 evaluates to P11 + (2 + X0) + 0 -(1 + X0)-P11 (predicate inference evaluates the predicate Z0 < \n1 to 0), which reassociation reduces to 1. When pass 2 examines instruction 2.1, edge 17 -2 is in REACHABLE \nand I17 evaluates to 1, so I2 evaluates to a(2, 1, 1) and remains 1. When pass 2 examines instruction \n2.2, J2 evaluates to a(2, 1, 2) and becomes J2. When pass 3 examines instruction 2.2, J2 evaluates to \na(2, 1, J3) and remains J2, so the algorithm terminates.  3. IMPLEMENTATION ISSUES This section brie.y \ndescribes how to implement our algo\u00adrithm e.ciently and e.ectively. It is not necessary to use sets at \nall. A congruence class can be e.ciently implemented as a doubly linked list of val\u00adues. Values, instructions \nand blocks can contain bit masks which specify the sets they belong to. Touches of instruc\u00adtions and \nblocks can be recorded in order to avoid subse\u00adquent redundant touches. A count of the touched instruc\u00adtions \nand blocks can be maintained to allow the algorithm to terminate in the middle of a pass. Value inference \nis only applicable to the variable operands of equality or inequality predicates of conditional jumps. \nIf these are initially marked as inferenceable, and a count of the inferenceable values is maintained \nfor every congru\u00adence class, then value inference can be restricted to members of congruence classes \nwith at least one inferenceable value. Predicate inference can be optimized in the same way. Mul\u00adtiple \nuses of an upward exposed inferenceable value in a block must have the same value. The result of the \n.rst value inference can be cached to avoid the rest. The predicate of a block can be permanently nulli.ed \nafter an abnormal ter\u00admination of a-predication; this usually improves e.ciency at a small cost in strength. \nWhen an expression reduces or hashes to a variable, value inference can be reapplied to it. Value inference \nappears to give slightly better results when restricted to congruences with constants (see section 2.7). \n.-predication can be ex\u00adtended to handle switch instructions, even when their de\u00adfault case does not \nhave an explicit predicate. Finally, pruned SSA [6] and converting while to until loops can reduce the \ne.ectiveness of global value numbering and predicate and value inference respectively. 4. COMPLEXITY \nThis section analyzes the worst case time complexity of our algorithm. We assume that the number of operands \nof an expression and the time taken by hashing are bounded, and the CFG is connected. So the number of \nnodes N in the CFG is O(E), where E is the number of edges in the CFG. Processing an instruction is dominated \nby value infer\u00adence. Algorithm Infer value at block in Figure 7 can take N +(N - 1) + \u00b7\u00b7\u00b7 or O(E2) time \nto repeatedly go up the dominator tree (see Figure 9). Processing the instructions can take O(E2I) time, \nwhere I is the number of instructions in the routine. Touching the consumers of the results of the instructions \ncan take O(I) time. Processing the predicates of the edges is dominated by value inference and can take \nO(E3) time. Touching the in\u00adstructions and blocks a.ected by the edges can take O(EI + E2 ) time. Processing \nthe predicate of a block is dominated by CFG traversal. Algorithm Compute partial predicate of block \nin Figure 8 can take O(E) time to visit every block and edge Figure 9: An example of the worst case of \nvalue inference: capturing the congruence of J and I\u00b5 takes O(\u00b5 2) time. 01 I1 + ... 02 ... 03 + ... \nI\u00b5 04 if (I1 = I2) 05 ... 06 if (I\u00b5-1 = I\u00b5) 07 J + I1 in the CFG. Processing the predicates of the blocks \ncan take O(E2) time. Touching the instructions a.ected by the blocks can take O(EI) time. The complete \nalgorithm can also take O(E2) time to in\u00ad crementally build the reachable dominator tree [14]. The algorithm \nterminates when, at the end of a pass, there is no touched instruction. Such an instruction must be \nup\u00ad stream in RPO of the change that caused it to be touched, so it must de.ne a cyclic value. Hence \nthe algorithm termi\u00ad nates when all the cyclic values reach a .xed point. A cyclic value can change only \nfrom an optimistic to a pessimistic value (a a-function can merge an optimistic with a di.ering value \nto symbolically evaluate to a pessimistic value, but there is no inverse function which, given a pessimistic \nvalue, can symbolically evaluate to an optimistic value). So the al\u00ad gorithm terminates when every value \ncarried by a back edge has reached its consuming cyclic a-function. The number of passes required for \nthis is of the order of the maximum number of back edges in any acyclic path in the SSA def-use graph. \nTherefore the number of passes is O(C),10 where C is the loop connectedness of the SSA def-use graph. \nCombining these results, we see that both the complete and the practical algorithm have a worst case \ntime complex\u00ad ity of O(CE2(E + I)). When performing optimistic value numbering on an acyclic CFG or balanced \nvalue numbering on any CFG, both the algorithms .nish in one pass and their worst case time complexity \nreduces to O(E2(E+I)). In spite of this worst case time complexity, our measurements indi\u00ad cate that \nthe practical algorithm is quite e.cient in practice when it is implemented as described in section 3; \nthe average routine takes 1.98 passes, and the average instruction visits 0.91, 0.38 and 0.16 blocks \nwhen performing value inference, predicate inference and a-predication respectively. 5. MEASUREMENTS \nWe have implemented the practical algorithm in the high level optimizer (HLO) [2] component of an internal \nversion of the PA-RISC C compiler on HP-UX. HLO performs whole program, inter-procedural, parallelizing \nand scalar optimiza\u00ad tions on a high level SSA form intermediate representation. We have implemented \nthe practical algorithm as described in section 3. Our implementation does not exploit the dis\u00ad tributive \nlaw in global reassociation, but it is strong enough to arrive at the result given in Figure 1. We now \npresent measurements of the strength and e.\u00ad ciency of the practical algorithm collected when optimizing \nthe SPEC CINT2000 C benchmarks. The benchmarks were compiled with the SPEC peak options on a lightly \nloaded HP 9000 Model N4000 server with 1 GB memory running the HP-UX B.11.0 operating system on a single \n440 Mhz PA- RISC 8500 processor. The measurements were taken from an early invocation of global value \nnumbering performed be\u00ad 10Simpson gives a formal demonstration of this result for the complexity of his \nRPO algorithm [13]. fore any other optimization. We enabled debugging features 100 + + Unreachable values \n\u00d7 0x 6103 1 2 5 10 20 50100  in order to obtain the measurements, and we ran the bench\u00ad 50 Constant \nvalues + 0x 5853 + Total number of routines marks once for every combination of enabled features. The \n20 \u00d7 + + measurements exclude 256.bzip2 because of an unrelated + 10 + bug in the compiler. 5 We compare \nthe number of unreachable values (more is Number of routines \u00d7 ++ + better), constant values (more is \nbetter) and congruence \u00d7\u00d7++ ++ \u00d7 + 2 classes (less is better) discovered per routine by the practical \n\u00d7\u00d7 \u00d7\u00d7+ + + + +\u00d7\u00d7+++\u00d7 + \u00d7 1 algorithm with various features enabled. When a constant value is found to \nbe unreachable, it improves the number of unreachable values but worsens the number of constant values. \nWe correct for this by counting unreachable values as constant values too. We also measure the time spent \nin HLO and global value numbering by the practical algorithm with various features enabled. Our measurements \nlead to a number of observations. The practical algorithm is strong. It obtains modest im\u00adprovements \nwhen performing optimistic value numbering over emulating the strongest existing practical global value \nnumbering algorithm due to Click [7] (Figure 10), and Weg\u00adman and Zadeck s sparse conditional constant \npropagation algorithm [16] (Figure 11). It discovers fewer congruences than Click s algorithm in 6 routines \ndue to value inference (see section 2.7). Improvement Figure 11: The distribution of the improvements \nin the number of unreachable and constant values obtained by the practical algorithm performing op\u00adtimistic \nvalue numbering over emulating Wegman and Zadeck s sparse conditional constant propaga\u00adtion algorithm. \nvisits 0.91, 0.38 and 0.16 blocks when performing value in\u00adference, predicate inference and a-predication \nrespectively. Table 2: The time taken by the practical algorithm Improvement gorithm performing optimistic \nvalue numbering over emulating Click s strongest algorithm.11 The practical algorithm is e.cient. It \ntakes less than 4% of HLO s total optimization time (column C of Table 1). Sparseness makes the practical \nalgorithm run much faster. It runs 1.23 1.57 times faster when sparseness is enabled (column D of Table \n2). The overhead of performing global re\u00adassociation, predicate inference, value inference and a-predi\u00adcation \nin the practical algorithm is more than compensated for by sparseness. It runs 1.15 1.32 times faster \nwhen these analyses are disabled (column E of Table 2). Finally, the av\u00aderage routine takes 1.98 passes, \nand the average instruction 11The x-axis in Figure 10 represents the absolute improve\u00adment in the number \nof unreachable values, constant values or congruence classes. The y-axis represents the absolute num\u00adber \nof routines. Thus the practical algorithm discovered 100 more unreachable values when performing optimistic \nvalue numbering over emulating Click s strongest algorithm in 1 routine. The legend in the top right \ncorner indicates that the practical algorithm discovered an identical number of unreachable values when \nperforming optimistic value num\u00adbering over emulating Click s strongest algorithm in 6103 routines (out \nof a total of 6138), etc. when performing optimistic value numbering on the SPEC CINT2000 C benchmarks \nwith various fea\u00adtures disabled. Figure 10: The distribution of the improvements in the number of unreachable \nvalues, constant values and congruence classes obtained by the practical al\u00ad Benchmark A B C D E Dense \n. Sparse. Basic . A/B B/C 164.gzip 175.vpr 176.gcc 181.mcf 186.crafty 197.parser 253.perlbmk 254.gap \n255.vortex 300.twolf 3804 6992 126019 752 16443 8079 46177 52708 21880 17537 2653 5119 91848 577 10445 \n6001 35416 36422 17777 12425 2231 4300 73610 477 7908 4909 29897 29848 15426 9639 1.43 1.37 1.37 1.30 \n1.57 1.35 1.30 1.45 1.23 1.41 1.19 1.19 1.25 1.21 1.32 1.22 1.18 1.22 1.15 1.29 All 300391 218683 178245 \n1.37 1.23 Time in milliseconds . Sparseness disabled Global reassociation, predicate inference, value \ninference and \u00a2-predication disabled  Balanced value numbering is almost as strong as opti\u00admistic value \nnumbering (Figure 12). It runs as fast as pessi\u00admistic value numbering (column K of Table 1), and much \n(1.39 1.90 times) faster than optimistic value numbering (column G of Table 1). 6. RELATED WORK Existing \nmethods of global value numbering may be classi\u00ad.ed as partition based or hash based [13, 4]. Partition \nbased approaches place the values of a routine in an initial congru\u00adence class, and repeatedly split \nthe congruence classes until they reach a .xed point. Hash based approaches make the values of a routine \nidentical to an initial value, and repeat\u00adedly perform symbolic evaluation on the instructions of the \nroutine, using a hash table to .nd congruences, until the values reach a .xed point. Our algorithm is \na uni.cation of the partition and hash based approaches; it follows the hash based approach, but it also \nuses congruence classes to implement an e.cient sparse formulation. Table 1: The time taken by HLO and \nthe practical algorithm when performing optimistic, balanced and pessimistic value numbering on the SPEC \nCINT2000 C benchmarks. Benchmark Optimistic Balanced Pessimistic A B C D E F G H I J K HLO. GVN. B/A \nHLO. GVN. E/D B/E HLO. GVN. I/H E/I 164.gzip 114705 2653 2.3% 110059 1606 1.5% 1.65 111603 1611 1.4% \n1.00 175.vpr 263796 5119 1.9% 253871 3083 1.2% 1.66 248768 3077 1.2% 1.00 176.gcc 42744539 91848 0.2% \n41173950 54871 0.1% 1.67 40982587 55101 0.1% 1.00 181.mcf 18074 577 3.2% 17233 334 1.9% 1.73 17306 333 \n1.9% 1.00 186.crafty 892060 10445 1.2% 869240 5607 0.6% 1.86 870161 5572 0.6% 1.01 197.parser 316705 \n6001 1.9% 310103 3514 1.1% 1.71 316052 3500 1.1% 1.00 253.perlbmk 5601513 35416 0.6% 6149634 22765 0.4% \n1.56 6092225 22768 0.4% 1.00 254.gap 1694424 36422 2.1% 1631911 21256 1.3% 1.71 1634120 21227 1.3% 1.00 \n255.vortex 1451522 17777 1.2% 1396981 12807 0.9% 1.39 1389433 12393 0.9% 1.03 300.twolf 1521209 12425 \n0.8% 1487306 6535 0.4% 1.90 1479012 6540 0.4% 1.00 All 54618547 218683 0.4% 53400288 132378 0.2% 1.65 \n53141267 132122 0.2% 1.00 Time in milliseconds 1 2 5 10 20 50 100200 Improvement Figure 12: The distribution \nof the improvements in the number of unreachable values, constant values and congruence classes obtained \nby the practical al\u00adgorithm performing optimistic over balanced value numbering. Alpern, Wegman and Zadeck \ndescribe the .rst optimistic partition based algorithm for global value numbering [1]. This algorithm \ntakes O(I log I) time, where I is the number of instructions in the routine. They extend it to .nd congru\u00adences \nin the presence of conditional and loop control struc\u00adtures by using structured forms of the SSA a-function: \naif , aenter and aexit (this is equivalent to performing a-predica\u00adtion on acyclic and cylic a-functions). \nThey explain how to unify this algorithm with constant folding and the algebraic simpli.cation of commutative \nand aif expressions. However this algorithm does not perform unreachable code elimina\u00adtion, global reassociation, \npredicate inference or value infer\u00adence, and it restricts itself to two special cases of algebraic simpli.cation. \nClick describes the strongest existing practical algorithm for global value numbering [7]. This optimistic \npartition based algorithm uni.es Alpern, Wegman and Zadeck s algo\u00adrithm [1], constant folding, algebraic \nsimpli.cation and un\u00adreachable code elimination. It takes O(I2) time. Click also describes a faster but \noccasionally weaker algorithm which takes O(I log I) time [7]. However these algorithms do not perform \nglobal reassociation, predicate inference, value in\u00adference or a-predication. Click discusses a pessimistic \nhash based algorithm that uni.es global value numbering, constant folding, and alge\u00adbraic simpli.cation \n[8]. This algorithm takes O(I) time. However it does not perform optimistic value numbering, unreachable \ncode elimination, global reassociation, predi\u00adcate inference, value inference or a-predication. Simpson \ndescribes the .rst optimistic hash based algo\u00adrithm for global value numbering. His RPO algorithm [13] \nachieves the same result as Alpern, Wegman and Zadeck s algorithm [1]. It performs repeated symbolic \nevaluation on the instructions of a routine in RPO until the values reach a .xed point. Simpson also \ndescribes a more e.cient version of his RPO algorithm. His SCC algorithm [13] processes the strongly \nconnected components (SCCs) of the SSA use-def graph in a single RPO pass, iterating to a .xed point \nonly on cyclic code. His measurements indicate that the SCC algorithm runs 0.88 2.28 times faster than \nAlpern, Weg\u00adman and Zadeck s algorithm [1]. Both the RPO and SCC algorithms take O(CI) time, where C \nis the loop connect\u00adedness of the SSA def-use graph. Simpson extends these algorithms to perform constant \nfolding and algebraic sim\u00adpli.cation. However these algorithms do not perform un\u00adreachable code elimination, \nglobal reassociation, predicate inference, value inference or a-predication, and they do not implement \na sparse formulation. Our algorithm handles hashing much more e.ciently than Simpson s RPO and SCC algorithms. \nThe RPO algorithm uses a single hash table which is cleared after every pass, so it must process every \ninstruction in every pass. The SCC algorithm uses di.erent hash tables for cyclic and acyclic code, so \nit must perform an extra pass over every SCC to merge the hash tables. Our algorithm uses a single hash \ntable, so it does not need to merge hash tables. Expressions are removed from this hash table only when \ntheir associated congruence classes become empty, so it does not need to process every instruction in \nevery pass. Wegman and Zadeck mention that their constant propa\u00adgation algorithm can be improved by value \ninference [15]. Briggs, Torczon and Cooper [5] perform value inference by means of a pre-pass that inserts \nassignments into a routine. In Figure 13, their pre-pass would convert the example in (a) to (b), after \nwhich global value numbering can discover that I1 is congruent to 0. However the pre-pass is not uni.ed \nwith global value numbering, so it operates not on values but on SSA variables. Hence this approach cannot \ndiscover that J1 is also congruent to 0. Our algorithm uni.es value inference with global value numbering, \nso it can discover that both I1 and J1 are congruent to 0. 01 L1 + K1 + 0 01 L1 + K1 + 0 02 if (K1 = \n0) 02 if (K1 = 0) 03 I1 + K1 03 X1 + one of (K1, 0) 04 J1 + L1 04 I1 + X1 05 J1 + L1 (a) (b) Figure \n13: Briggs, Torczon and Cooper s pre-pass converts the code in (a) to (b), after which global value numbering \ncan discover that I1 (but not J1) is congruent to 0. Our uni.ed algorithm can discover that both I1 and \nJ1 are congruent to 0. R\u00a8uthing, Knoop and Ste.en describe an algorithm that al\u00adternately applies Alpern, \nWegman and Zadeck s algorithm, and the transformations a(X, X) -X and a(X1 op Y1, X2 op Y2) -a(X1,X2) \nop a(Y1,Y2), until the congruence classes reach a .xed point [12]. With unpruned SSA [6], this algorithm \nis as strong as Kildall s non-SSA algorithm [10] for acyclic code. It is expected to take O(I2logI) time, \nbut can take O(I4logI) in the worst case. This algorithm (like Kildall s) will capture the congruence \nof K3 and L3 in case (a) (but not (b)) of Figure 14. If the alternative transfor\u00admation a(X1,X2) op a(Y1,Y2) \n-a(X1 op Y1,X2 op Y2) is incorporated into the global reassociation performed by our algorithm, it should \ncapture the congruence of K3 and L3 in both case (a) and (b) of Figure 14. However it remains to be seen \nwhether this is practical. 01 if (. . .) 01 if (. . .) 02 I1 + . . . 02 I1 + 1 03 K1 + I1 + 1 03 J1 + \n2 04 else 04 else 05 I2 + . . . 05 I2 + 2 06 K2 + I2 + 1 06 J2 + 1 07 I3 + \u00a2(I1, I2) 07 I3 + \u00a2(I1, I2) \n08 K3 + \u00a2(K1, K2) 08 J3 + \u00a2(J1, J2) 09 L3 + I3 + 1 09 K3 + I3 + J3 10 L3 + 3 Figure 14: R\u00a8uthing, Knoop \nand Ste.en s algorithm (like Kildall s) will capture the congruence of K3 and L3 in case (a) (but not \n(b)).  (a) (b)  7. CONCLUSIONS Global value numbering can be made stronger by exploit\u00ading the congruences \ninduced by the predicates of conditional jumps, and it can be implemented in an e.cient sparse for\u00admulation \nthat o.ers a range of tradeo.s between compila\u00adtion time and optimization strength. Further extensions \nto our algorithm are possible. The practical algorithm could be extended to allow predicate and value \ninference along paths containing back edges, predicate and value inference could be extended to handle \njoint domination by multiple congruent predicates (which would enable the practical algo\u00adrithm to completely \nunify predicate and value inference with unreachable code elimination), and a-predication could be extended \nto handle cyclic a-functions. 8. ACKNOWLEDGEMENTS We thank everyone who contributed to this work, in \npar\u00adticular Chris Ruemmler for the requirement which suggested this e.ort, Suresh Shanmugam for help \nwith the SPEC CINT2000 benchmarks, Amitav Mishra for help with Linux, LATEX and MetaPost, Kenneth Zadeck \nand Jens Knoop for extracts and preprints of their papers, Jon DeBord for a number of useful papers, \nand the anonymous PLDI 02 re\u00adviewers for their suggestions which signi.cantly improved this paper. 9. \nREFERENCES [1] B. Alpern, M. N. Wegman, and F. K. Zadeck. Detecting equality of variables in programs. \nIn Proceedings of the 15 th ACM Symposium on Principles of Programming Languages, pages 1 11, January \n1988. [2] A. Ayers, S. de Jong, J. Peyton, and R. Schooler. Scalable cross-module optimization. In Proceedings \nof the ACM SIGPLAN 98 Conference on Programming Language Design and Implementation, pages 301 312, June \n1998. [3] P. Briggs and K. D. Cooper. E.ective partial redundancy elimination. In Proceedings of the \nACM SIGPLAN 94 Conference on Programming Language Design and Implementation, pages 159 170, June 1994. \n[4] P. Briggs, K. D. Cooper, and L. T. Simpson. Value numbering. Software Practice and Experience, 27(6):701 \n724, June 1997. [5] P. Briggs, L. Torczon, and K. D. Cooper. Using Conditional Branches to Improve Constant \nPropagation. Technical Report CRPC-TR95533, Center for Research on Parallel Computation, Rice University, \nApril 1995. [6] J.-D. Choi, R. Cytron, and J. Ferrante. Automatic construction of sparse data .ow evaluation \ngraphs. In Proceedings of the 18 th ACM Symposium on Principles of Programming Languages, pages 55 66, \nJanuary 1991. [7] C. Click. Combining Analyses, Combining Optimizations. PhD thesis, Rice University, \nFebruary 1995. [8] C. Click. Global code motion: Global value numbering. In Proceedings of the ACM SIGPLAN \n95 Conference on Programming Language Design and Implementation, pages 246 257, June 1995. [9] R. Cytron, \nJ. Ferrante, B. K. Rosen, M. N. Wegman, and F. K. Zadeck. E.ciently computing static single assignment \nform and the control dependence graph. ACM Transactions on Programming Languages and Systems, 13(4):451 \n490, October 1991. [10] G. A. Kildall. A uni.ed approach to global program optimization. In Proceedings \nof the 1 st ACM Symposium on Principles of Programming Languages, pages 194 206, October 1973. [11] B. \nK. Rosen, M. N. Wegman, and F. K. Zadeck. Global value numbers and redundant computations. In Proceedings \nof the 15 th ACM Symposium on Principles of Programming Languages, pages 12 27, January 1988. [12] O. \nR\u00a8uthing, J. Knoop, and B. Ste.en. Detecting equalities of variables: Combining e.ciency with precision. \nIn Proceedings of the 6 th International Symposium on Static Analysis, pages 232 247, September 1999. \n [13] L. T. Simpson. Value-Driven Redundancy Elimination. PhD thesis, Rice University, April 1996. [14] \nV. C. Sreedhar, G. R. Gao, and Y.-F. Lee. Incremental computation of dominator trees. ACM Transactions \non Programming Languages and Systems, 19(2):239 252, March 1997. [15] M. N. Wegman and F. K. Zadeck. \nConstant propagation with conditional branches. In Proceedings of the 12 th ACM Symposium on Principles \nof Programming Languages, pages 291 299, January 1985. [16] M. N. Wegman and F. K. Zadeck. Constant propagation \nwith conditional branches. ACM Transactions on Programming Languages and Systems, 13(2):181 210, April \n1991.   \n\t\t\t", "proc_id": "512529", "abstract": "This paper presents a new algorithm for performing global value numbering on a routine in static single assignment form. Our algorithm has all the strengths of the most powerful existing practical methods of global value numbering; it unifies optimistic value numbering with constant folding, algebraic simplification and unreachable code elimination. It goes beyond existing methods by unifying optimistic value numbering with further analyses: it canonicalizes the structure of expressions in order to expose more congruences by performing <i>global reassociation</i>, it exploits the congruences induced by the predicates of conditional jumps (<i>predicate inference</i> and <i>value inference</i>), and it associates the arguments of acyclic <i>&#248;</i> functions with the predicates controlling their arrival (<i>&#248; predication</i>), thus enabling congruence finding on conditional control structures. Finally, it implements an efficient <i>sparse</i> formulation and offers a range of tradeoffs between compilation time and optimization strength. We describe an implementation of the algorithm and present measurements of its strength and efficiency collected when optimizing the SPEC CINT2000 C benchmarks.", "authors": [{"name": "Karthik Gargi", "author_profile_id": "81100152848", "affiliation": "Hewlett-Packard India Software Operation, Bangalore, Karnataka, India", "person_id": "P348265", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/512529.512536", "year": "2002", "article_id": "512536", "conference": "PLDI", "title": "A sparse algorithm for predicated global value numbering", "url": "http://dl.acm.org/citation.cfm?id=512536"}