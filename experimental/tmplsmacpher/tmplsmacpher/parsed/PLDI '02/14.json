{"article_publication_date": "05-17-2002", "fulltext": "\n A Compiler Approach to Fast Hardware Design Space Exploration in FPGA-based Systems Byoungro So, Mary \nW. Hall and Pedro C. Diniz Information Sciences Institute University of Southern California 4676 Admiralty \nWay, Suite 1001 Marina del Rey, California 90292 {bso,mhall,pedro}@isi.edu ABSTRACT This paper describes \nan automated approach to hardware design space exploration, through a collaboration between parallelizing \ncompiler technology and high-level synthesis tools. We present a compiler algorithm that automatically \nexplores the large design spaces resulting from the applica\u00adtion of several program transformations commonly \nused in application-speci.c hardware designs. Our approach uses synthesis estimation techniques to quantitatively \nevaluate alternate designs for a loop nest computation. We have im\u00adplemented this design space exploration \nalgorithm in the context of a compilation and synthesis system called DE-FACTO, and present results of \nthis implementation on .ve multimedia kernels. Our algorithm derives an implementa\u00adtion that closely \nmatches the performance of the fastest de\u00adsign in the design space, and among implementations with comparable \nperformance, selects the smallest design. We search on average only 0.3% of the design space. This tech\u00adnology \nthus signi.cantly raises the level of abstraction for hardware design and explores a design space much \nlarger than is feasible for a human designer. Categories and Subject Descriptors D.3.4 [Compilers]: Parallelizing \nCompilers; B.5.2 [Designs Aids]: Automatic Synthesis; D.7.1 [Types and Design Styles]: Algorithms implemented \nin hardware, Gate Arrays General Terms Algorithms, Performance, Design, Experimentation Keywords Design \nSpace Exploration, Data Dependence Analysis, Reuse Analysis, Loop Transformations Permission to make \ndigital or hard copies of all or part of this work for personal or classroom use is granted without fee \nprovided that copies are not made or distributed for pro.t or commercial advantage and that copies bear \nthis notice and the full citation on the .rst page. To copy otherwise, to republish, to post on servers \nor to redistribute to lists, requires prior speci.c permission and/or a fee. PLDI 02, June 17-19, 2002, \nBerlin, Germany. Copyright 2002 ACM 1-58113-463-0/02/0006 ...$5.00. 1. INTRODUCTION The extreme .exibility \nof Field Programmable Gate Ar\u00adrays (FPGAs) has made them the medium of choice for fast hardware prototyping \nand a popular vehicle for the real\u00adization of custom computing machines. FPGAs are com\u00adposed of thousands \nof small programmable logic cells dy\u00adnamically interconnected to allow the implementation of any logic \nfunction. Tremendous growth in device capacity has made possible implementation of complex functions \nin FP-GAs. For example, FPGA implementations can sometimes yield even faster solutions than conventional \nhardware, up to 2 orders of magnitude on encryption [18]. In addition FPGAs o.er a much faster time to \nmarket for time-critical applications and allow post-silicon in-.eld modi.cation to prototypical or low-volume \ndesigns where an Application Speci.c Integrated Circuit (ASIC) is not justi.ed. Despite growing importance \nof application-speci.c FPGA designs, these devices are still di.cult to program making them inaccessible \nto the average developer. The standard practice requires developers to express the application in a hardware-oriented \nlanguage such as Verilog or VHDL, and synthesize the design to hardware using a wide va\u00adriety of synthesis \ntools. As optimizations performed by synthesis tools are very limited, developers must perform high-level \nand global optimizations by hand. For example, no commercially-available high-level synthesis tool handles \nmulti-dimensional array variables1 nor automatic selection of loop unroll factors. Because of the complexity \nof synthesis, it is di.cult to predict a priori the performance and space characteristics of the resulting \ndesign. For this reason, developers engage in an iterative re.nement cycle, at each step manually apply\u00ading \ntransformations, synthesizing the design, examining the results, and modifying the design to trade o. \nperformance and space. Throughout this process, called design space exploration, the developer carries \nthe responsibility for the correctness of the application mapping. We believe the way to make programming \nof FPGA-based systems more accessible is to o.er a high-level imperative programming paradigm, such as \nC, coupled with compiler technology oriented towards FPGA designs. In this way, developers retain the \nadvantages of a simple programming 1Several claim the support of this feature but only for sim\u00adulation \npurposes, not actual hardware synthesis. model via the high-level language but rely on powerful com\u00adpiler \nanalyses and transformations to optimize the design as well as automate most of the tedious and error-prone \nmap\u00adping tasks. We make the observation that, for a class of FPGA applications characterized as highly \nparallel array\u00adbased computations (e.g., multimedia codes), many hand optimizations performed by developers \nare similar to trans\u00adformations used in parallelizing compilers. For example, de\u00advelopers parallelize \ncomputations, optimize external mem\u00adory accesses, explicitly manage storage and perform loop transformations. \nFor this reason, we argue that parallelizing compiler technology can be used to optimize FPGA designs. \nIn this paper, we describe an automated approach to de\u00adsign space exploration, based on a collaboration \nbetween a parallelizing compiler and high-level synthesis tools. Com\u00adpletely synthesizing a design is \nprohibitively slow (hours to days) and further, the compiler must try several designs to arrive at a \ngood solution. For these reasons, we exploit esti\u00admation from behavioral synthesis to determine speci.c \nhard\u00adware parameters (e.g., size and speed) with which the com\u00adpiler can quantitatively evaluate the \napplication of a trans\u00adformation to derive an optimized and feasible implementa\u00adtion of the loop nest \ncomputation. Since the hardware im\u00adplementation is bounded in terms of capacity, the compiler transformations \nmust also consider space constraints. This compiler algorithm e.ectively enables developers to explore \na potentially large design space, which without automation would not be feasible. In previous work, we \npresented an overview of DEFACTO, the system upon which this work is based, which combines parallelizing \ncompiler technology in the Stanford SUIF com\u00adpiler with hardware synthesis tools [9]. In this paper, \nwe present a detailed algorithm for design space exploration and results demonstrating its e.ectiveness. \nWhile there are a few systems that automatically synthesize hardware de\u00adsigns from C speci.cations [24], \nto our knowledge there is no other system that automatically explores the design space in collaboration \nwith behavioral synthesis estimation features. Our current infrastructure largely supports the direct \nmap\u00adping of computations to multiple FPGAs [26]. However, the work in this paper describes an implementation \nand experi\u00admental results for designs that are mapped to a single FPGA and multiple memories. We thus \nfocus on the algorithmic aspects of design space exploration under simpler data and computation partitioning \nstrategies. This paper makes the following speci.c contributions. Describes the integration of behavioral \nsynthesis tools and parallelizing compiler technology to map compu\u00adtations to FPGA-based architectures. \nWe present a compiler algorithm for design space exploration that relies on behavioral synthesis estimates. \nThe algo\u00adrithm applies loop transformations to explore a space\u00adtime trade-o. in the realization of hardware \ndesigns.  De.nes a balance metric for guiding design space explo\u00adration, which suggests when it is pro.table \nto devote more resources to storage or computation. The de\u00adsign space exploration algorithm exploits \nmonotonic\u00adity properties of the balance metric to e.ectively prune large regions of the search space, \nthereby allowing the compiler to consider a wider range of transformations that otherwise would not be \nfeasible.  Presents experimental results for .ve multimedia ker\u00adnels. Our algorithm derives an implementation \nthat  closely matches the performance of the fastest design in the design space, and among implementations \nwith comparable performance, selects the smallest design. We search on average only 0.3% of the design \nspace. As technology advances increase density of FPGA devices, tracking Moore s law for conventional \nlogic of doubling ev\u00adery 18 months, devices will be able to support more sophisti\u00adcated functions. With \nthe future trend towards on-chip inte\u00adgration of internal memories, FPGAs with special-purpose functional \nunits are becoming attractive as a replacement for ASICs and for custom embedded computing architec\u00adtures. \nWe foresee a growing need to combine the strengths of high-level program analysis techniques, to complement \nthe capabilities of current and future synthesis tools. De\u00advices and consequently designs will become \nmore complex, demanding an e.cient solution to exploring even larger de\u00adsign spaces. The remainder of \nthe paper is organized as follows. In the next section we present some background on FPGAs and behavioral \nsynthesis. Section 3 describes the optimization goal of our design space exploration algorithm in mapping \nloop nest computations to hardware. In Section 4 we discuss the analyses and transformation our algorithm \nuses. In Sec\u00adtion 5 we present the design space exploration algorithm. In Section 6 we present experimental \nresults for the application of this algorithm to 5 image processing computations. We survey related work \nin Section 7 and conclude in Section 8. 2. BACKGROUND We now describe FPGAs and synthesis, and compare \nwith optimizations performed in parallelizing compilers. We also discuss features of our target application \ndomain. 2.1 Field-Programmable-Gate-Arrays FPGAs are a popular vehicle for rapid prototyping or as a \nway to implement simple logic interfaces. FPGAs are im\u00adplemented as (re)programmable seas-of-gates with \ndistinct internal architectures. For example, the Xilinx Virtex family of devices consists of 12, 288 \ndevice slices where each slice in turn is composed of 2 look-up tables (LUTs) each of which can implement \nan arbitrary logic function of 11 boolean in\u00adputs and 6 outputs [15]. Two slices form a con.gurable logic \nblock (CLBs) and these blocks are interconnected in a 2\u00addimensional mesh via programmable static routing \nswitches. To con.gure an FPGA, designers have to download a bit\u00adstream .le with the con.guration of all \nslices in the FPGA as well as the routing. Other programmable devices, for example the APEX II devices \nfrom Altera, have a more hi\u00aderarchical routing approach to connecting the CLBs in their FPGAs, but the \noverall functionality is similar [6]. As with traditional architectures, bandwidth to external memory \nis a key performance bottleneck in FPGAs, since it is possible to compute orders of magnitude more data \nin a cycle than can be fetched from or stored to memory. However, unlike traditional architectures, an \nFPGA has the .exibility to devote its internal con.gurable resources either to storage or to computation. \n 2.2 FPGA Synthesis Flow Synthesis .ow for FPGAs is the term given to the process of translating functional \nlogic speci.cations to a bitstream description that con.gures the device. This functional spec\u00adi.cation \ncan be done at multiple levels. Using hardware description languages such as VHDL or Verilog, designers \ncan specify the functionality of their datapath circuits (e.g., adders, multipliers, etc.) as a diagram \nof design blocks. This structural speci.cation de.nes the input/output interface for each block and allows \nthe designers to describe .nite state machines (FSMs) to control the temporal behavior of each of the \nblocks. Using this approach designers can control every single aspect of operations in their datapaths. \nThis is the preferred approach when maximum performance is sought, but requires extremely high design \ntimes. The process that takes a structural speci.cation and tar\u00adgets a particular architecture s programmable \nunits (LUTs in the case of Xilinx devices) is called RTL-level synthesis. The RTL-level synthesis generates \na netlist representation of the intended design, used as the input of low-level synthesis steps such \nas the mapping and place-and-route (P&#38;R) to ultimately generate the device bitstream con.guration \n.le. 2.3 Behavioral Synthesis vs. Compilers Behavioral speci.cations in VHDL or Verilog, as opposed \nto lower level structural speci.cations, express computations without committing to a particular hardware \nimplementa\u00adtion structure. The process of taking a behavioral speci.\u00adcation and generating a hardware \nimplementation is called behavioral synthesis. Behavioral synthesis performs three core functions: binding \noperators and registers in the speci.cation to hardware implementations (e.g., selecting a ripple\u00adcarry \nadder to implement an addition);  resource allocation (e.g., deciding how many ripple\u00adcarry adders are \nneeded); and,  scheduling operations in particular clock cycles.  To generate a particular implementation, \nbehavioral syn\u00adthesis requires the programmer to specify the target design requirements in terms of area, \nclock rate, number of clock cycles, number of operators, or some combination. For ex\u00adample, the designer \nmight request a design that uses two multipliers and takes at most 10 clock cycles. Behavioral synthesis \ntools use this information to generate a particular implementation that satis.es these constraints. In \naddition, behavioral synthesis supports some optimiza\u00adtions, but relies heavily on the developer to direct \nsome of the mapping steps. For example, current behavioral syn\u00adthesis tools allow the speci.cation of \nwhich loops to unroll. After loop unrolling, the tool will perform extensive opti\u00admizations on the resulting \ninner loop body, such as paral\u00adlelizing and pipelining operations and minimizing registers and operators \nto save space. However, deciding the unroll factor is left up to the programmer. Behavioral Synthesis \nParallelizing Compilers Optimizations only on scalar variables Optimizations on scalars and arrays Optimizations \nonly inside loop body Optimizations inside loop body and across loop iterations Supports user-controlled \nAnalyses guide automatic loop unrolling loop transformations Manages registers and Optimizes memory accesses \ninter-operator communication Evaluates trade-offs of different storage on-and off-chip Considers only \nsingle FPGA System-level view: multiple FPGAs multiple memories Performs allocation, binding and No knowledge \nof hardware scheduling of hardware resources implementation of computation Table 1: Comparison of Behavioral \nSynthesis and Parallelizing Compiler Technologies. While there are some similarities between the optimiza\u00adtions \nperformed by synthesis tools and parallelizing com\u00adpilers, in many ways they o.er complementary capabilities, \nas shown in Table 1. The key advantage of parallelizing compiler technology over behavioral synthesis \nis the abil\u00adity to perform data dependence analysis on array variables, used as a basis for parallelization, \nloop transformations and optimizing memory accesses. This technology permits opti\u00admization of designs \nwith array variables, where some data resides in o.-chip memories. Further, it enables reasoning about \nthe bene.ts of code transformations (such as loop unrolling) without explicitly applying them. In addition, \nparallelizing compilers are capable of performing global pro\u00adgram analysis, which permits optimization \nacross the entire system. 2.4 Target Application Domain Because of their customizability, FPGAs are \ncommonly used for applications that have signi.cant amounts of .ne\u00adgrain parallelism and possibly can \nbene.t from non-standard numeric formats (e.g., reduced data widths). Speci.cally, multimedia applications, \nincluding image and signal pro\u00adcessing on 8-bit and 16-bit data, respectively, o.er a wide variety of \npopular applications that map well to FPGAs. For example, a typical image processing algorithm scans \na multi-dimensional image and operates on a given pixel value and all its neighbors. Images are often \nrepresented as multi-dimensional array variables, and the computation is expressed as a loop nest. Such \napplications exhibit abun\u00addant concurrency as well as temporal reuse of data. Exam\u00adples of computations \nthat fall into this category include im\u00adage correlation, Laplacian image operators, erosion/dilation \noperators and edge detection. Fortunately, such applications are a good match for the capabilities of \ncurrent parallelizing compiler analyses, which are most e.ective in the a.ne domain, where array subscript \nexpressions are linear functions of the loop index variables and constants [25]. In this paper, we restrict \ninput programs to loop nest computations on array and scalar variables (no pointers), where all subscript \nexpressions are a.ne with a .xed stride. The loop bounds must be constant.2 We sup\u00adport loops with control \n.ow, but to simplify control and scheduling, the generated code always performs conditional memory accesses. \n 3. OPTIMIZATION GOAL AND BALANCE Simply stated, the optimization criteria for mapping a sin\u00adgle loop \nnest to FPGA-based systems are as follows: (1) the design must not exceed the capacity constraints of \nthe sys\u00adtem; (2) the execution time should be minimized; and, (3) for a given level of performance, FPGA \nspace usage should be minimized. The motivation for the .rst two criteria should be obvious, but the \nthird criterion is also needed for several reasons. First, if two designs have equivalent perfor\u00admance, \nthe smaller design is more desirable, in that it frees up space for other uses of the FPGA logic, such \nas to map other loop nests. In addition, a smaller design usually has less routing complexity, and as \na result, may achieve a faster 2Non-constant bounds could potentially be supported by the algorithm, \nbut the generated code and resulting FPGA de\u00adsigns would be much more complex. For example, behavioral \nsynthesis would transform a for loop with a non-constant bound to a while loop in the hardware implementation. \ntarget clock rate. Moreover, the third criterion suggests a strategy for selecting among a set of candidate \ndesigns that meet the .rst two criteria. With respect to a particular set of transformations, which are \ndescribed in the next section, our algorithm attempts to select the best design that meets the above \ncriteria. The algorithm uses two metrics to guide the selection of a design. First, results of estimation \nprovide space usage of the design, related to criterion 1 above. Another important metric used to guide \nthe selection of a design, related to criteria 2 and 3, is Balance, de.ned by the equation. Balance = \nF/C, where F refers to the data fetch rate, the total data bits that memory can provide per cycle, and \nC refers to the data consumption rate, total data bits the computation can con\u00adsume during the computational \ndelay. If balance is close to one, both memories and FPGAs are busy. If balance is less than one, the \ndesign is memory bound; if greater than one, it is compute bound. When a design is not balanced, this \nmetric suggests whether more resources should be devoted to improving computation time or memory time. \nWe borrow the notion of balance from previous work for mapping array variables to scalar registers to \nbalance the .oating point operations and memory accesses [5]. Because we have the .exibility in FPGAs \nto adjust time spent in either computation or memory accesses, we use the data fetch rate and data consumption \nrate, and compare them under di.erent optimization assumptions. 4. ANALYSES AND TRANSFORMATIONS This \nsection describes at a high level the code transfor\u00admations performed by our system, as illustrated by \nthe FIR .lter example in Figure 1. Unroll-and-Jam. The .rst code transformation, unroll\u00adand-jam, involves \nunrolling one or more loops in the itera\u00adtion space and fusing inner loop bodies together, as shown in \nFigure 1(b). Unrolling exposes operator parallelism to high\u00adlevel synthesis. In the example, all of the \nmultiplies can be performed in parallel. Two additions can subsequently be performed in parallel, followed \nby two more additions. Unroll-and-jam can also decrease the dependence distances for reused data accesses, \nwhich, when combined with scalar replacement discussed below, can be used to expose oppor\u00adtunities for \nparallel memory accesses. Scalar Replacement. Scalar replacement replaces ar\u00adray references by accesses \nto temporary scalar variables, so that high-level synthesis will exploit reuse in registers [5]. Our \napproach to scalar replacement closely matches previ\u00adous work, which eliminates true dependences when \nreuse is carried by the innermost loop, for accesses in the a.ne domain with consistent dependences (i.e., \nconstant depen\u00addence distances) [5]. There are, however, two di.erences: (1) we also eliminate unnecessary \nmemory writes on out\u00adput dependences; and, (2) we exploit reuse across all loops in the nest, not just \nthe innermost loop. The latter di.er\u00adence stems from the observation that many, though not all, algorithms \nmapped to FPGAs have su.ciently small loop bounds or small reuse distances, and the number of regis\u00adters \nthat can be con.gured on an FPGA is su.ciently large. A more detailed description of our scalar replacement \nand register reuse analysis can be found in [9]. In the example in Figure 1(c), we see the results of \nscalar replacement, which illustrates some of the above di.erences int S[96]; int C[32]; int D[64]; for \n(j=0; j<64; j++) for(i = 0; i<32; i++) D[j] = D[j] + (S[i+j] * C[i]); (a) Original code. for (j=0; \nj<64; j+=2) for(i = 0; i<32; i+=2){ D[j] = D[j] + (S[i+j] * C[i]); D[j] = D[j] + (S[i+j+1] * C[i+1]); \nD[j+1] = D[j+1] + (S[i+j+1] * C[i]); D[j+1] = D[j+1] + (S[i+j+2] * C[i+1]); } (b) After unrolling j \nloop and i loop by 1 (unroll factor 2) and jamming copies of i loop together. for (j=0; j<64; j+=2) { \n/* initialize D registers */ d 0=D[j]; d 1=D[j+1]; for (i=0; i<32; i+=2) { if (j==0) { /* initialize \nC registers */ c 0 0 = C[i]; c 1 0 = C[i+1]; } S 0 = S[i+j+1]; d 0=d 0+S[i+j] *c 0 0; /* unroll(0,0) \n*/ d 0=d 0+S 0*c 1 0; /* unroll(0,1) */ d 1=d 1+S 0*c 0 0; /* unroll(1,0) */ d 1=d 1+S[i+j+2] *c 1 0; \n/* unroll(1,1) */ rotate registers(c 0 0, ... ,c 0 15); rotate registers(c 1 0, ... ,c 1 15); } D[j] \n= d 0; D[j+1] = d 1; } (c) After scalar replacement of accesses to C and D across both i and j loop. \nfor (j=0; j<32; j++) { /* initialize D registers */ d 0 = D2[j]; d 1 = D3[j]; for (i=0; i<16; i++) { \nif (j==0) { /* initialize C registers */ c 0 0 = C0[i]; c 1 0 = C1[i]; } S 0 = S1[i+j]; d 0=d 0+S0[i+j] \n*c 0 0; /* unroll(0,0) */ d 0=d 0+S 0*c 1 0; /* unroll(0,1) */ d 1=d 1+S 0*c 0 0; /* unroll(1,0) */ d \n1=d 1+S0[i+j+1] *c 1 0; /* unroll(1,1) */ rotate registers(c 0 0, ... ,c 0 15); rotate registers(c 1 \n0, ... ,c 1 15); } D3[j] = d 1; D2[j] = d 0; } (d) Final code generated for FIR, including loop normalization \nand data layout optimization. Figure 1: Optimization Example: FIR. from previous work. Accesses to arrays \nC and D can all be replaced. The D array is written back to memory at the end of the iteration of the \nj loop, but redundant writes are eliminated. Only loop-independent accesses to array S are replaced because \nthe other accesses to array S do not have a consistent dependence distance. Because reuse on array C \nis carried by the outer loop, to exploit full reuse of data from C involves introducing extra registers \nthat hold values of C across all iterations of the inner loop. The rotate operation shifts the registers \nand rotates the last one into the .rst position; this operation can be performed in parallel in hardware. \nLoop Peeling and Loop-Invariant Code Motion. We see in Figure 1(c) and (d) that values for the c registers \nare loaded on the .rst iteration of the j loop. For clarity it is not shown here, but the code generated \nby our com\u00adpiler actually peels the .rst iteration of the j loop instead of including these conditional \nloads so that other iterations of the j loop have the same number of memory loads and can be optimized \nand scheduled by high-level synthesis ac\u00adcordingly. Although at .rst glance the code size appears to \nbe doubled by peeling, high-level synthesis will usually reuse the operators between the peeled and original \nloop body, so that the code growth does not correspond to a growth in the design. Memory accesses to \narray D are invariant with respect to the i loop, so they are moved outside the loop using loop-invariant \ncode motion. Within the main unrolled loop body, only memory accesses to array S remain. Data Layout \nand Array Renaming. Another code transformation lays out the data in the FPGA s external memory so as \nto maximize memory parallelism. Custom data layout is separated into two distinct phases. In the .rst \nphase, which we call array renaming,performs a 1\u00adto-1 mapping between array access expressions and virtual \nmemory ids, to customize accesses to each array according to their access patterns. Array renaming can \nonly be per\u00adformed if all accesses to the array within the loop nest are uniformly generated. Two a.ne \narray references A(a1i1 + b1,... ,anin + bn)and A(c1i1 + d1,... ,cnin + dn), where a1,... ,an,b1,... \n,bn, c1,...cn, d1,... ,dn are constants and i1,... ,in are loop index variables, are uniformly generated \nif .i=1,nai = ci. If an array s accesses are not uniformly generated, then it is mapped to a single memory. \nThe result of array renaming is an even distribution of data across the virtual memories. The second \nphase, called memory mapping, binds virtual memory ids to physical ids, taking into consideration ac\u00adcesses \nby other arrays in the loop nest to avoid scheduling con.icts. As shown in Figure 1(d), the e.ect of \ndata layout is that even elements of S and C are mapped to memory 0, and odd elements are mapped to memory \n1, with accesses renamed to re.ect this layout. D is similarly distributed to memories 2 and 3. This \napproach is similar in spirit to the modulo unrolling used in the RAW compiler [3]. However, as compared \nto modulo unrolling, which is a loop transformation that as\u00adsumes a .xed data layout, our approach is \na data trans\u00adformation. Further, our current implementation supports a more varied set of custom data \nlayouts. A typical lay\u00adout is cyclic in at least one dimension of an array, possibly more, but more customized \ndata layouts arise from packing small data types, strided accesses, and subscript expressions with multiple \ninduction variables (i.e., subscripts of the form a0i0 +a1i1 +...+anin +b,where n> 0 and more than one \nai is non-zero). A full discussion of the data layout algorithm is beyond the scope of this paper, but \nfurther discussion can be found in [9]. Summary. To summarize, the algorithm evaluates a fo\u00adcused set \nof possible unroll factors for multiple loops in the loop nest. Data reuse is exploited within and across \nthe loops in the nest, as a result of scalar replacement by the compiler, eliminating unnecessary memory \naccesses. Oper\u00adator parallelism is exposed to high-level synthesis through the unrolling of one or more \nloops in the nest; any indepen\u00addent operations will be performed in parallel if high-level synthesis \ndeems this bene.cial. Thus, we have de.ned a set of transformations, widely used in conventional computing, \nthat permit us to adjust parallelism and data reuse in FPGA-based systems through a collaboration between \nparallelizing compiler technology and high-level synthesis. To meet the optimization crite\u00adria set forth \nin the previous section, we have reduced the optimization process to a tractable problem, that of select\u00ading \nthe unroll factors for each loop in the nest that leads to a high-performance, balanced, e.cient design. \nIn the next section, we present the algorithm in detail. Although our algorithm focuses on a .xed set \nof compiler transformations, the notion of using balance to guide the performance-space tradeo. in design \nspace exploration can be used for other optimizations as well. 5. OPTIMIZATION ALGORITHM The discussion \nin this section de.nes terms and uses these to describe the design space exploration algorithm. The algorithm \nis presented assuming that scalar replacement will exploit all reuse in the loop nest on a.ne array accesses. \nThe resulting design will store all reused data internally on the FPGA, which is feasible for many applications \nwith short reuse distances, but may require too many on-chip registers in the general case. We address \nthis problem by limiting the number of registers in Section 5.4. 5.1 De.nitions We de.ne a saturation \npoint as a vector of unroll factors where the memory parallelism reaches the bandwidth of the architecture, \nsuch that the following property holds for the resulting unrolled loop body: .. widthi = C1 * widthl. \ni.Reads l.NumMemories .. widthj = C2 * widthl. j.Writes l.NumMemories Here, C1 and C2 are integer constants. \nTo simplify this discussion, let us assume that the access widths match the memory width, so that we \nare simply looking for an unroll factor that results in a multiple of NumMemories read and write accesses \nfor the smallest values of C1 and C2.The saturation set, Sat, can then be determined as a function of \nthe number of read and write accesses, R and W,in a single iteration of the loop nest and the unroll \nfactor for each loop in the nest. We consider reads and writes separately because they will be scheduled \nseparately. We are interested in determining the saturation point after scalar replacement and redundant \nwrite elimination. For the purposes of this discussion, we assume that for each array accessed in the \nmain loop body, all accesses are uniformly generated, and thus a customized data layout will be ob\u00adtained; \nmodi.cations to the algorithm when this does not hold are straightforward, but complicate the calculation \nof the saturation point. R is de.ned as the number of uni\u00adformly generated read sets. W is the number \nof uniformly generated write sets. That is, there is a single memory read and single write access for \neach set of uniformly generated references because all others will be removed by scalar re\u00adplacement \nor redundant write elimination. We de.ne an unroll factor vector as U = (u1,... ,un), where ui corresponds \nto the unroll factor for loop i,and a . function P(U)= ui, which is the product of all the 1=i=n unroll \nfactors. Let Psat = lcm(gcd(R,W),NumMemories). The saturation set Satcan then be de.ned as a vector whose \nproduct is Psat,where .ui = 1, array subscript expressions for memory accesses are varying with respect \nto loop i.That is, the saturation point considers unrolling only those loops that will introduce additional \nmemory parallelism. Since loop peeling and loop-invariant code motion have eliminated memory accesses \nin the main loop body that are invariant with respect to any loop in the nest, from the perspective of \nmemory parallelism, all such unroll factor vectors are equiv\u00adalent. A particular saturation point Sati \nrefers to unrolling the i loop by the factor Psat, and using an unroll factor of 1 for all other loops. \n 5.2 Search Space Properties The optimization involves selecting unroll factors for the loops in the \nnest. Our search is guided by the following observations about the impact of unrolling a single loop \nin the nest, which depend upon the assumptions about target applications in Section 2.4. Observation \n1. The data fetch rate is monotonically non\u00addecreasing as the unroll factor increases by multiples of \nPsat, but it is also nonincreasing beyond the saturation point. Intuitively, the data fetch rate increases \nas there are more memory accesses available in the loop body for scheduling in parallel. This observation \nrequires that the data is laid out in memory and the accesses are scheduled such that the number of independent \nmemory accesses on each memory cycle is monotonically nondecreasing as the unroll factor increases. Here, \nthe unroll factor must increase by multi\u00adples of Psat, so that each time a memory operation is per\u00adformed, \nthere are NumMemories accesses in the main loop body that are available to schedule in parallel. This \nis true whenever data layout has successfully mapped each array to multiple memories. (If data layout \nis not successful, as is the case when not all accesses to the same array are uni\u00adformly generated, a \nsteady state mapping of data to mem\u00adories can guarantee monotonicity even when there are less than NumMemories \nparallel accesses, but we will ignore this possibility in the subsequent discussion.) Data layout and \nmapping data to speci.c memories is controlled by the compiler. Given the property of array re\u00adnaming \nfrom Section 4, that the accessed data is evenly dis\u00adtributed across virtual memory ids, this mapping \nderives a solution that, in the absence of con.icting accesses to other arrays, exposes fully parallelizable \naccesses. To prevent con\u00ad.icting read or write accesses in mapping virtual memory ids to physical ones, \nwe must .rst consider how accesses will be scheduled. The compiler component of our system is not directly \nresponsible for scheduling; scheduling memory ac\u00adcesses as well as computation is performed by behavioral \nsynthesis tools such as Monet. The scheduling algorithm used by Monet, called As Soon As Possible, .rst \nconsiders which memory accesses can oc\u00adcur in parallel based on comparing subscript expressions and physical \nmemory ids, and then rules out writes whose results are not yet available due to dependences [10]. In \nperforming the physical memory id mapping, we .rst consider read ac\u00adcesses, so that we maximize the number \nof read operations that can occur in parallel. The physical id mapping matches the read access order, \nso that the total number of memory reads in the loop is evenly distributed across the memories for all \narrays. As an added bene.t, operands for individual writes are fetched in parallel. Then the physical \nmapping for write operations is also performed in the same order, evenly distributing write operations \nacross the memories. With these properties of data layout and scheduling, at the saturation point, we \nhave guaranteed through choice of unroll factor that the data fetch rate increases up to the saturation \npoint, but not beyond it. Observation 2. The consumption rate is monotonically non-decreasing as the \nunroll factor increases by multiples of Psat, even beyond the saturation point. Intuitively, as the unroll \nfactor increases, more operator parallelism is enabled, thus reducing the computation time and increasing \nthe frequency at which data can be con\u00adsumed. Further, based on Observation 1, as we increase the data \nfetch rate, we eliminate idle cycles waiting on memory and thus increase the consumption rate. Although \nthe par\u00adallelism exploited as a result of unrolling a loop may reach a threshold, performance continues \nto improve slightly due to simpler loop control. Observation 3. Balance is monotonically nondecreasing \nbefore the saturation point and monotonically nonincreasing beyond the saturation point as the unroll \nfactor increases by multiples of Psat. That balance is nondecreasing before the saturation point relies \non Observation 1. The data fetch rate is increasing as fast as or faster than the data consumption rate \nbecause memory accesses are completely independent, whereas op\u00aderator parallelism may be restricted. \nBeyond the saturation point, the data fetch rate is not increasing further, and the consumption rate \nis increasing at least slightly. 5.3 Algorithm Description The algorithm is presented in Figure 2. Given \nthe above described monotonicity of the search space for each loop in the nest, we start with a design \nat the saturation point, and we search larger unroll factors that are multiples of Psat, looking for \nthe two points between which balance crosses over from compute bound to memory bound, or vice versa. \nIn fact, ignoring space constraints, we could search each loop in the nest independently, but to converge \nto a near-optimal design more rapidly, we select unroll factors based on the data dependences, as described \nbelow. The algorithm .rst selects Uinit, the starting point for the search, which is in Sat. We select \nthe most promising unroll factors based on the dependence distance vectors. A depen\u00addence distance vector \nis a vector d = (d1,d2,... ,dn) which represents the vector di.erence between two accesses to the same \narray, in terms of the loop indices in the nest [25]. Since we are starting with a design that maximizes \nmemory parallelism, then either the design is memory bound and we stop the search, or it is compute bound \nand we continue. If it is compute bound, then we consider unroll factors that provide increased operator \nparallelism, in addition to mem\u00adory parallelism. Thus, we .rst look for a loop that carries no dependence \n(i.e., .d.Ddi = 0). All unrolled iterations of such a loop can be executed in parallel. If such a loop \ni is found, then we set the unroll factor to Sati. assuming this unroll factor is in Sat. If no such \nloop exists, then we instead select an unroll fac\u00adtor that favors loops with the largest dependence distances, \nbecause such loops can perform in parallel computations be\u00adtween dependences. The details of how our \nalgorithm selects the initial unroll factor in this case is beyond the scope of this paper, but the key \ninsight is that we unroll all loops in the nest, with larger unroll factors for the loops carrying larger \nminimum nonzero dependence distances. The mono\u00adtonicity property also applies when considering simultaneous \nunrolling for multiple loops as long as unroll factors for all loops are either increasing or decreasing. \nIf the initial design is space constrained, we must re\u00adduce the unroll factor until the design size is \nless than the size constraint Capacity, resulting in a suboptimal design. The function FindLargestFit \nsimply selects the largest un\u00adroll factor between the baseline design corresponding to no unrolling (called \nUbase), and Uinit, regardless of balance, be\u00adcause this will maximize available parallelism. Assuming \nthe initial design is compute bound, the algo\u00adrithm increases the unroll factors until it reaches a design \nthat is (1) memory bound; (2) larger than Capacity;or, (3) represents full unrolling of all loops in \nthe nest (i.e., Ucurr = Umax), as follows. The function Increase(Uin ) returns unroll factor vector Uout \nsuch that (1)P(Uout)=2 * P(Uin); and, in out max (2).iui = ui = ui . If there are no such remaining \nunroll factor vectors, then Increase returns Uin. If either a space-constrained or memory bound design \nis found, then the algorithm will select an unroll factor vector between the last compute bound design \nthat .t, and the current design, approximating binary search, as follows. The function SelectBetween(Usmall \n,Ularge) returns the un\u00adroll factor vector Uout such that (1)P(Uout)=(P(Usmall)+ P(Ularge)/2; small out \nlarge (2).iui = ui = ui ; and, (3)P(Uout)= C* P(Uinit), for some constant C. If there are no such remaining \nunroll factor vectors, then SelectBetween returns Usmall, a compute bound design. 5.4 Adjusting Number \nof On-Chip Registers For designs where the reuse distance is large and many registers are required, it \nmay become necessary to reduce the number of data items that are stored on the FPGA. Using fewer on-chip \nregisters means that less reuse is exploited, which in turn slows down the fetch rate and, to a lesser \nextent, the consumption rate. The net e.ect is that, in the Search Algorithm: Input: Code /* An n-deep \nloop nest */ Output: (u1,... ,un) /* a vector of unroll factors */ Ucurr = Uinit Umb = Umax ok = False \nwhile (!ok) do Code = Generate(Ucurr) Estimate = Synthesize(Code) B = Balance(Code,Estimate.Performance) \n/* .rst deal with space-constrained designs */ if (Estimate.Space > Capacity) then if (Ucurr = Uinit)then \nUcurr = FindLargestFit(Ubase,Ucurr) ok = True else Ucurr = SelectBetween(Ucb,Ucurr) else if (B = 1) \nthen ok = True /* Balanced, so DONE! */ else if (B< 1) then /* memory bound */ Umb = Ucurr if (Ucurr \n= Uinit) then ok = True else /* Balanced solution is between earlier size and this */ Ucurr = SelectBetween(Ucb,Umb) \nelse if (B> 1) then /* compute bound */ Ucb = Ucurr if (Umb = Umax)then /* Have only seen compute bound \nso far */ Ucurr = Increase(Ucb) else /* Balanced solution is between earlier size and this */ Ucurr \n= SelectBetween(Ucb,Umb) /* Check if no more points to search */ if (Ucurr = Ucb)ok= True end return \nUcurr Figure 2: Algorithm for Design Space Exploration. .rst place, the design will be smaller and more \nlikely to .t on chip, and secondly, space is freed up so that it can be used to increase the operator \nparallelism for designs that are compute bound. To adjust the number of on-chip registers, we can use \nloop tiling to tile the loop nest so that the localized iteration space within a tile matches the desired \nnumber of registers, and exploit full register reuse within the tile.  6. EXPERIMENTAL RESULTS This \nsection presents experimental results that character\u00adize the e.ectiveness of the previously described \ndesign space exploration algorithm for a set of kernel applications. We de\u00adscribe the applications, the \nexperimental methodology and discuss the results. 6.1 Application Kernels We demonstrate our design exploration \nalgorithm on .ve multimedia kernels, namely: Finite Impulse Response (FIR) .lter, integer multiply\u00adaccumulate \nover 32 consecutive elements of a 64 ele\u00adment array.  Matrix Multiply (MM), integer dense matrix multipli\u00adcation \nof a 32-by-16 matrix by a 16-by-4 matrix.  String Pattern Matching (PAT), character matching operator \nof a string of length 16 over an input string of length 64.  C Application  Compiler Analyses scalar \nreplacement data layout array renaming data reuse unroll &#38; jam tiling Unroll Factor Determination \n    Figure 3: Compilation and Synthesis Flow. Jacobi Iteration (JAC), 4-point stencil averaging com\u00adputation \nover the elements of an array.  Sobel (SOBEL) Edge Detection (see e.g., SOBEL [22]), 3-by-3 window Laplacian \noperator over an integer im\u00adage.  Each application is written as a standard C program where the computation \nis a single loop nest. There are no pragmas, annotations or language extensions describing the hardware \nimplementation. 6.2 Methodology We applied our prototype compilation and synthesis sys\u00adtem to analyze \nand determine the best unrolling factor for a balanced hardware implementation. Figure 3 depicts the \ndesign .ow used for these experiments. First, the code is compiled into the SUIF format along with the \napplication of standard compiler optimizations. Next, our design space exploration algorithm iteratively \ndetermines which loops in the loop nest should be unrolled and by how much. To make this determination \nthe compiler starts with a given unrolling factor and applies a sequence of transformations as described \nin Sections 4 and 5. Next, the compiler trans\u00adlates the SUIF code resulting from the application of the \nselected set of transformations to behavioral VHDL using a tool called SUIF2VHDL. The compiler next invokes \nthe Mentor Graphics MonetTM behavioral synthesis tool to ob\u00adtain space and performance estimates for \nthe implementa\u00adtion of the behavioral speci.cation. In this process, the com\u00adpiler currently .xes the \nclock period to be 40ns.The MonetTM synthesis estimation yields the amount of area used by the implementation \nand the number of clock cycles required to execute to completion the computation in the behavioral speci.cation. \nGiven this data, the compiler next computes the balance metric. This system is fully automated. The implementation \nof the compiler passes speci.c to this experiment, namely data reuse analysis, scalar replacement, unroll&#38;jam, \nloop peel\u00ading, and customized data layout, constitutes approximately 14, 500 lines of C++ source code. \nThe algorithm executed in less than 5 minutes for each application, but to fully syn\u00adthesize each design \nwould require an additional couple of hours.  6.3 Results In this section, we present results for the \n.ve previously described kernels in Figures 4 through 10. The graphs show a large number of points in \nthe design space, substantially more than are searched by our algorithm, to highlight the relationship \nbetween unroll factors and metrics of interest. The .rst set of results in Figures 4 through 7 plots \nbal\u00adance, execution cycles and design area in the target FPGA as a function of unroll factors for the \ninner and outer loops of FIR and MM. Although MM is a 3-deep loop nest, we only consider unroll factors \nfor the two outermost loops, since through loop-invariant code motion the compiler has eliminated all \nmemory accesses in the innermost loop. The graphs in the .rst two columns have as their x-axis unroll \nfactors for the inner loop, and each curve represents a spe\u00adci.c unroll factor for the outer loop. For \nFIR and MM, we have plotted the results for pipelined and non-pipelined memory accesses to observe the \nimpact of memory access costs on the balance metric and consequently in the selected designs. In all \nplots, a squared box indicates the design selected by our search algorithm. For pipelined memory accesses, \nwe assume a read and write latency of 1 cycle. For non-pipelined memory accesses, we assume a read latency \nof 7 cycles and a write latency of 3 cycles, which are the latencies for the Annapolis WildStarTM [13] \nboard, a target platform for this work. In practice, memory latency is somewhere in between these two \nas some but not all memory accesses can be fully pipelined. In all results we are assuming 4 memories, \nwhich is the number of external memories that are connected to each of the FPGAs in the Annapolis WildStarTM \nboard. In these plots, a design is balanced for an unrolling factor when the y-axis value is 1.0. Data \npoints above the y-axis value of 1.0 indicate compute-bound designs whereas points with the y-axis value \nbelow 1.0 indicate memory-bound de\u00adsigns. A compute-bound design suggests that more resources should \nbe devoted to speeding up the computation compo\u00adnent of the design, typically by unrolling and consuming \nmore resources for computation. A memory-bound design suggests that less resources should be devoted \nto computa\u00adtion as the functional units that implement the computation are idle waiting for data. The \ndesign area graphs represent space consumed (using a log scale) on the target Xilinx Vir\u00adtex 1000 FPGAs \nfor each of the unrolling factors. A vertical line indicates the maximum device capacity. All designs \nto the right side of this line are therefore unrealizable. With pipelined memory accesses, there is a \ntrend towards compute-bound designs due to low memory latency. With\u00adout pipelining, memory latency becomes \nmore of a bottle\u00adneck leading, in the case of FIR, to designs that are al\u00adways memory bound, while the \nnon-pipelined MM exhibits compute-bound and balanced designs. The second set of results, in Figures 8 \nthrough 10, show performance of the remaining three applications, JAC, PAT and SOBEL. In these .gures, \nwe present, as before, balance, cycles and area as a function of unroll factors, but only for pipelined \nmemory accesses due to space limitations. We make several observations about the full results. First, \nwe see that Balance follows the monotonicity properties de\u00adscribed in Observation 3, increasing until \nit reaches a sat\u00ad 16000  4 10 14000 Execution Cycles (log-scaled) 0.2 4000 0.15 2000 Execution Cycles \n 12000 10000 8000 3 10 6000 2 10 0.1 0 (a) Balance (b) Execution Time (c) Area Figure 4: Balance, Execution \nTime and Area for Non-pipelined FIR. 4 3.2 10  2 10 1.2 1000 0 0.8 (a) Balance (b) Execution Time (c) \nArea Figure 5: Balance, Execution Cycles and Area for Pipelined FIR. Execution Cycles (log-scaled) 4 \n7000 10 Execution Cycles 1.6 2000 6000 5000 4000 3 10 3000   0.7 0.6 6000 5000 4000 3000 2000 10 \n3 1000 0.4 0.3 0 (a) Balance (b) Execution Time (c) Area Figure 6: Balance, Execution Cycles and Area \nfor Non-pipelined MM. 4 5000 10   4000 3000 2000 3 10 1000 1 2 10 0.5 0 (a) Balance (b) Execution \nTime (c) Area Figure 7: Balance, Execution Cycles and Area for Pipelined MM. Balance Balance Balance \nBalance 2.2 1600 1400 2 3 10 12001.8 1.6 1000 800 1.4   Balance Balance Balance 600 1.2 400 1 200 2 \n10 0.8 (a) Balance (b) Execution Time (c) Area Figure 8: Balance, Execution Time and Area for Pipelined \nJAC. 4 2500 10 500 1  0 10 0 (a) Balance (b) Execution Time (c) Area Figure 9: Balance, Execution \nCycles and Area for Pipelined PAT. 4 2.1 10 Execution Cycles (log-scaled) 3 2000 3 10 2.5 1500 2 2 10 \n1000 1.5 1 10  Execution Cycles 1.6 3000 1.5 2000 8000 2 7000 1.9 6000 1.8 5000 1.7 4000 3 10 1000 \n1.4 (a) Balance (b) Execution Time (c) Area uration point, and then decreasing. The execution time is \nalso monotonically nonincreasing, related to Observation 2. In all programs, our algorithm selects a \ndesign that is close to best in terms of performance, but uses relatively small unroll factors. Among \nthe designs with comparable perfor\u00admance, in all cases our algorithm selected the design that consumes \nthe smallest amount of space. As a result, we have shown that our approach meets the optimization goals \nset forth in Section 3. In most cases, the most balanced design is selected by the algorithm. When a \nless balanced design is selected, it is either because the more balanced de\u00adsign is before a saturation \npoint (as for non-pipelined FIR), or is too large to .t on the FPGA (as for pipelined MM). Table 2 presents \nthe speedup results of the selected de\u00adsign for each kernel as compared to the baseline, for both pipelined \nand non-pipelined designs. The baseline is the loop nest with no unrolling (unroll factor is 1 for all \nloops) but including all other applicable code transformations as describedinSection 4. Although in these \ngraphs we present a very large number of design points, the algorithm searches only a tiny fraction of \nthose displayed. Instead, the algorithm uses the prun- Program Non-Pipelined Pipelined FIR MM JAC PAT \nSOBEL 7.67 17.26 4.55 13.36 3.87 5.56 7.53 34.61 4.01 3.90 Table 2: Speedup on a single FPGA. ing heuristics \nbased on the saturation point and balance, as described in section 5. This reveals the e.ectiveness of \nthe algorithm as it .nds the best design point having only explored a small fraction, only 0.3% of the \ndesign space con\u00adsisting of all possible unroll factors for each loop. For larger design spaces, we expect \nthe number of points searched rel\u00adative to the size to be even smaller.  6.4 Accuracy of Estimates To \nspeed up design space exploration, our approach relies on estimates from behavioral synthesis rather \nthan going through the lengthy process of fully synthesizing the design, which can be anywhere from 10 \nto 10, 000 times slower for this set of designs. To determine the gap between the be\u00adhavioral synthesis \nestimates and fully synthesized designs, we ran logic synthesis and place-and-route to derive imple\u00admentations \nfor a few selected design points in the design space for each of the applications. We synthesized the \nbase\u00adline design, the selected designs for both pipelined and non\u00adpipelined versions, and a few additional \nunroll factors be\u00adyond the selected design. In all cases, the number of clock cycles remains the same \nfrom behavioral synthesis to implemented design. However, the target clock rate can degrade for larger \nunroll factors due to increased routing complexity. Similarly, space can also increase, slightly more \nthan linearly with the unroll factors. These factors, while present in the output of logic synthesis \nand place-and-route, were negligible for most of the designs selected by our algorithm. Clock rates degraded \nby less than 10% for almost all the selected designs as compared with the baseline, and the speedups \nin terms of reduction in clock cycles more than made up for this. In the case of FIR with pipelining, \nthe clock degraded by 30%, but it met the target clock of 40ns, and because the speedup was 17X,the performance \nimprovement was still signi.cant. The space increases were sublinear as compared to the unroll factors, \nbut tended to be more space constrained for large designs than suggested by the output of behavioral \nsynthesis. The very large designs that appear to have the high\u00adest performance according to behavioral \nsynthesis estimates show much more signi.cant degradations in clock and in\u00adcreases in space. In these \ncases, performance would be worse than designs with smaller unroll factors. Our approach does not su.er \nfrom this potential problem because we favor small unroll factors, and only increase the unrolling factor \nwhen there is a signi.cant reduction in execution cycles due to memory parallelism or instruction-level \nparallelism. For this set of applications, these estimation discrepan\u00adcies, while not negligible, never \nin.uenced the selected de\u00adsign. While this accuracy issue is clearly orthogonal to the design space algorithm \ndescribed in this paper, we believe that estimation tools will improve their ability to deliver ac\u00adcurate \nestimates given the growing pressures for accuracy in simulation for increasingly larger designs.  7. \nRELATED WORK In this section we discuss related work in the areas of au\u00adtomatic synthesis of hardware \ncircuits from high-level lan\u00adguage constructs and design space exploration using high\u00adlevel loop transformations. \n7.1 Synthesizing High-Level Constructs The gap between hardware description languages such as VHDL or \nVerilog and applications in high-level imper\u00adative programming languages prompted researchers to de\u00advelop \nhardware-oriented high-level languages. These new languages would allow programmers to migrate to con.g\u00adurable \narchitectures without having to learn a radically new programming paradigm while retaining some level \nof control about the hardware mapping and synthesis process. One of the .rst e.orts in this direction \nwas the Handel\u00adC[21] parallel programming language. Handel-C is heavily in.uenced by the OCCAM CSP-like \nparallel language but has a C-like syntax. The mapping from Handel-C to hard\u00adware is compositional where \nconstructs, such as for and while loops, are directly mapped to prede.ned template hardware structures \n[20]. Other researchers have developed approaches to mapping applications to their own recon.gurable \narchitectures that are not FPGAs. These e.orts, e.g., the RaPiD [7] recon\u00ad.gurable architecture and the \nPipeRench [12], have devel\u00adoped an explicitly parallel programming language and/or developed a compilation \nand synthesis .ow tailored to the features of their architecture. The Cameron research project is a system \nthat compiles programs written in a single-assignment subset of C called SA-C into data.ow graphs and \nthen synthesizable VHDL [23]. The SA-C language includes reduction and windowing op\u00aderators for two-dimensional \narray variables which can be combined with doall constructs to explicitly expose paral\u00adlel operations \nin the computation. Like in our approach, the SA-C compiler includes loop-level transformations such \nas loop unrolling and tiling, particularly when windowing operators are present in a loop. However, the \napplication of these transformations is controlled by pragmas,and is not automatic. Cameron s estimation \napproach builds on their own internal data-.ow representation using curve .t\u00adting techniques [17]. Several \nother researchers have developed tools that map computations expressed in a sequential imperative program\u00adming \nlanguage such as C to recon.gurable custom comput\u00ading architectures. Weinhardt [24] describes a set of \npro\u00adgram transformations for the pipelined execution of loops with loop-carried dependences onto custom \nmachines using a pipeline control unit and an approach similar to ours. He also recognizes the bene.t \nof data reuse but does not present a compiler algorithm. The two projects most closely related to ours, \nthe Nimble compiler [19] and work by Babb et. al. [2], map applications in C to FPGAs, but do not perform \ndesign space exploration. They also do not rely on behavioral synthesis, but in fact the compiler replaces \nmost of the function of synthesis tools. 7.2 Design Space Exploration In this discussion, we focus only \non related work that has attempted to use loop transformations to explore a wide design space. Other \nwork has addressed more general issues such as .nding a suitable architecture (either recon.gurable or \nnot) for a particular set of applications [1]. In the context of behavioral VHDL [16] current tools such \nas MonetTM [14] allow the programmer to control the ap\u00adplication of loop unrolling for loops with constant \nbounds. The programmer must .rst specify an application behavioral VHDL, linearize all multi-dimensional \narrays, and then se\u00adlect the order in which the loops must execute. Next the programmer must manually \ndetermine the exact unroll fac\u00adtor for each of the loops and determine how the unrolling is going to \na.ect the required bandwidth and the computation. Given the e.ort and interaction between the transformations \nand the data layout options available this approach to design space exploration is extremely awkward \nand error-prone. Other researchers have also recognized the value of ex\u00adploiting loop-level transformations \nin the mapping of regu\u00adlar loop computations to FPGA-based architectures. Der\u00adrien/Rajopadhye [8] describe \na tiling strategy for doubly nested loops. They model performance analytically and se\u00adlect a tile size \nthat minimizes the iteration s execution time. 7.3 Discussion The research presented in this paper di.ers \nfrom the ef\u00adforts mentioned above in several respects. First the focus of this research is in developing \nan algorithm that can explore a wide number of design points, rather than selecting a sin\u00adgle implementation. \nSecond, the proposed algorithm takes as input a sequential application description and does not require \nthe programmer to control the compiler s transfor\u00admations. Third, the proposed algorithm uses high-level \ncom\u00adpiler analysis and estimation techniques to guide the appli\u00adcation of the transformations as well \nas evaluate the various design points. Our algorithm supports multi-dimensional array variables absent \nin previous analyses for the mapping of loop computations to FPGAs. Finally, we use a com\u00admercially available \nbehavioral synthesis tool to complement the parallelizing compiler techniques rather than creating an \narchitecture-speci.c synthesis .ow that partially replicates the functionality of existing commercial \ntools. Behavioral synthesis allows the design space exploration to extract more accurate performance \nmetrics (time and area used) rather than relying on a compiler-derived performance model. Our approach \ngreatly expands the capability of behavioral syn\u00adthesis tools through more precise program analysis. \n 8. CONCLUSION We have described a compiler algorithm that balances computation and memory access rates \nto guide hardware design space exploration for FPGA-based systems. The ex\u00adperimental results for .ve \nmultimedia kernels reveal the al\u00adgorithm quickly (in less than .ve minutes, searching less than 0.3% \nof the search space) derives a design that closely matches the best performance within the design space \nand is smaller than other designs with comparable performance. This work addresses the growing need for \nraising the level of abstraction in hardware design to simplify the design pro\u00adcess. Through combining \nstrengths of parallelizing compiler and behavioral synthesis, our system automatically performs transformations \ntypically applied manually by hardware de\u00adsigners, and rapidly explores a very large design space. As \ntechnology increases the complexity of devices, consequently designs will become more complex, and furthering \nautoma\u00adtion of the design process will become crucial. Acknowledgements. This research has been supported \nby DARPA contract # F30602-98-2-0113. The authors wish to thank contributors to the DEFACTO project, \nupon which this work is based, in particular Joonseok Park, Heidi Ziegler, Yoon-Ju Lee, and Brian Richards. \n 9. REFERENCES [1] S. Abraham,B.Rau, R.Schreiber,G. Snider, and M. Schlansker. E.cient design space exploration \nin PICO. Tech. report, HP Labs, 1999. [2] J. Babb, M. Rinard, A. Moritz, W. Lee, M. Frank, R. Barua \nand S. Amarasinghe. Parallelizing Applications into Silicon. In Proc. of the IEEE Symp. on FPGA for Custom \nComputing Machines(FCCM 99), 1999. [3] R. Barua, W. Lee, S. Amarasinghe, and A. Agarwal. Maps: A compiler-managed \nmemory system for raw machines. In Proc. of the 26th Intl. Symp. on Computer Architecture (ISCA 99), \n1999. [4] D. Callahan, S. Carr, and K. Kennedy. Improvingregister allocation for subscripted variables. \nIn Proc. of the ACM Conference on Program Language Design and Implementation (PLDI 90), pages 53 65, \n1990. [5] S. Carr and K. Kennedy. Improvingthe ratio of memory operations to .oating-point operations \nin loops. ACM Transactions on Programming Languages and Systems, 15(3):400 462, July 1994. [6] Altera \nCorp. APEX II programmable logic device data sheets. 2001. [7] D. Cronquist, P. Franklin, and C. Ebeling. \nSpecifying and compilingapplications for RaPiD. In Proc. of the IEEE Symp. on FPGA for Custom Computing \nMachines (FCCM 98), pages 116 125, 1998. [8] S. Derrien and S. Rajopadhye. Loop tilingfor recon.gurable \naccelerators. In Proc. of the Eleventh Intl. Symp. on Field Programmable Logic (FPL 2001), 2001. [9] \nP. Diniz, M. Hall, J. Park, B. So, and H. Ziegler. Bridging the gap between compilation and synthesis \nin the DEFACTO system. In Proc. of the Forteenth Workshop on Languages and Compilers for Parallel Computing \n(LCPC 2001), August 2001. To be published as Lecture Notes in Computer Science. [10] J. P. Elliott. UnderStanding \nBehavioral Synthesis: A Practical Guide to High-Level Design. 1999. [11] J. Frigo, M. Gokhale, and D. \nLavenier. Evaluation of the Streams-C C-to-FPGA compiler: an applications perspective. In Proc. of the \nACM Symp. on Field Programmable GateArrays(FPGA 2002), 2001. [12] S. Goldstein, H. Schmit, M. Moe, M. \nBudiu, S. Cadambi, R. Taylor, and R. Laufer. PipeRench: A coprocessor for streamingmultimedia acceleration. \nIn Proc. of the 26th Intl. Symp. on Computer Architecture (ISCA 99), 1999. [13] Annapolis MicroSystems \nWildStartm manual, 4.0. 1999. [14] Mentor Graphics MonetTM user s manual (release r42). 1999. [15] XILINX \nVirtex-II 1.5V FPGA data sheet. ds031(v1.7). 2001. [16] D. Knapp. Behavioral Synthesis. Prentice-Hall, \n1996. [17] D. Kulkarni,W.Najjar, R. Rinker,and F. Kurdahi. Fast area estimation to support compiler optimizations \nin FPGA-based recon.gurable systems. In Proc. of the IEEE Symp. on FPGAs for Custom Computing Machines \n(FCCM 2002), 2002. [18] M. Leong, O. Cheung, K. Tsoi, and P. Leong. A bit-serial implementation of the \ninternational data encryption algorithm IDEA. In Proc. of the IEEE Symp. on FPGA for Custom Computing \nMachines(FCCM 98), pages 122 131, 1998. [19] Y. Li, T. Callahan, E. Darnell, R.E. Harr, U. Kurkure, and \nJ. Stockwood. Hardware-software co-design of embedded recon.gurable architectures. In Proc. of the Design \nAutomation Conference (DAC 00), June, 2000. [20] W. Luk, D. Ferguson, and I. Page. Structured hardware \ncompilation of parallel programs. Abingdon EE&#38;CS Books, 1994. [21] I. Page and W. Luk. Compiling \nOCCAM into FPGAs. In Proc. of the First Intl. Symp. on Field Programmable Logic (FPL 91), 1991. [22] \nJ. Proakis and D. G. Manolakis. Digital Signal Processing: Principles, Algorithms and Applications. Prentice-Hall, \n1995. [23] R. Rinker, M. Carter, A. Patel, M.Chawathe, C. Ross, J. Hammes, W. Najjar, and W. Bohm. An \nautomated process for compilingdata.ow graphs into recon.gurable hardware. IEEE Trans. on VLSI Systems, \n9(1):130 139, 2001. [24] M. Weinhardt. Compilation and pipeline synthesis for recon.gurable architectures. \nIn Proc. of the 1997 Recon.gurable Architectures Workshop RAW 97. Springer-Verlag, 1997. [25] M. Wolfe. \nOptimizing Supercompilers for Supercomputers. Addison-Wesley, 1996. [26] H. Ziegler, B. So, M. Hall, \nand P. Diniz. Coarse-Grain Pipeliningfor Multiple FPGA Architectures. In Proc. of the IEEE Symp. on FPGA \nfor Custom Computing Machines(FCCM 02), 2002.  \n\t\t\t", "proc_id": "512529", "abstract": "The current practice of mapping computations to custom hardware implementations requires programmers to assume the role of hardware designers. In tuning the performance of their hardware implementation, designers manually apply loop transformations such as loop unrolling. designers manually apply loop transformations. For example, loop unrolling is used to expose instruction-level parallelism at the expense of more hardware resources for concurrent operator evaluation. Because unrolling also increases the amount of data a computation requires, too much unrolling can lead to a memory bound implementation where resources are idle. To negotiate inherent hardware space-time trade-offs, designers must engage in an iterative refinement cycle, at each step manually applying transformations and evaluating their impact. This process is not only error-prone and tedious but also prohibitively expensive given the large search spaces and with long synthesis times. This paper describes an automated approach to hardware design space exploration, through a collaboration between parallelizing compiler technology and high-level synthesis tools. We present a compiler algorithm that automatically explores the large design spaces resulting from the application of several program transformations commonly used in application-specific hardware designs. Our approach uses synthesis estimation techniques to quantitatively evaluate alternate designs for a loop nest computation. We have implemented this design space exploration algorithm in the context of a compilation and synthesis system called DEFACTO, and present results of this implementation on five multimedia kernels. Our algorithm derives an implementation that closely matches the performance of the fastest design in the design space, and among implementations with comparable performance, selects the smallest design. We search on average only 0.3% of the design space. This technology thus significantly raises the level of abstraction for hardware design and explores a design space much larger than is feasible for a human designer.", "authors": [{"name": "Byoungro So", "author_profile_id": "81100362924", "affiliation": "University of Southern California, Marina del Rey, California", "person_id": "P34269", "email_address": "", "orcid_id": ""}, {"name": "Mary W. Hall", "author_profile_id": "81407592676", "affiliation": "University of Southern California, Marina del Rey, California", "person_id": "P193111", "email_address": "", "orcid_id": ""}, {"name": "Pedro C. Diniz", "author_profile_id": "81377592829", "affiliation": "University of Southern California, Marina del Rey, California", "person_id": "PP43121098", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/512529.512550", "year": "2002", "article_id": "512550", "conference": "PLDI", "title": "A compiler approach to fast hardware design space exploration in FPGA-based systems", "url": "http://dl.acm.org/citation.cfm?id=512550"}