{"article_publication_date": "05-17-2002", "fulltext": "\n Efficient Discovery of Regular Stride Patterns in Irregular Programs and Its Use in Compiler Prefetching \n Youfeng Wu Programming Systems Research Intel Labs 2200 Mission College Blvd Santa Clara, CA 95052 youfeng.wu@intel.com \n ABSTRACT Irregular data references are difficult to prefetch, as the future memory address of a load \ninstruction is hard to anticipate by a compiler. However, recent studies as well as our experience indicate \nthat some important load instructions in irregular programs contain stride access patterns. Although \nthe load instructions with stride patterns are difficult to identify with static compiler techniques, \nwe developed an efficient profiling method to discover these load instructions. The new profiling method \nintegrates the profiling for stride information and the traditional profiling for edge frequency into \na single profiling pass. The integrated profiling pass runs only 17% slower than the frequency profiling \nalone. The collected stride information helps the compiler to identify load instructions with stride \npatterns that can be prefetched efficiently and beneficially. We implemented the new profiling and prefetching \ntechniques in a research compiler for Itanium Processor Family (IPF), and obtained significant performance \nimprovement for the SPECINT2000 programs running on Itanium machines. For example, we achieved a 1.59x \nspeedup for 181.mcf, 1.14x for 254.gap, and 1.08x for 197.parser. We also showed that the performance \ngain is stable across input data sets. These benefits make the new profiling and prefetching techniques \nsuitable for production compilers. Categories and Subject Descriptors D.3.4 [Programming Languages]: \nProcessors Compiler, Code generation, Memory management, Optimization General Terms Languages, Performance, \nDesign, Experimentation, Algorithms Keywords: Integrated stride and frequency profiling, Data prefetching, \nStrongly single-strided loads, Phased multi-strided loads, Performance evaluation Permission to make \ndigital or hard copies of all or part of this work forpersonal or classroom use is granted without fee \nprovided that copies arenot made or distributed for profit or commercial advantage and that copiesbear \nthis notice and the full citation on the first page. To copy otherwise,or republish, to post on servers \nor to redistribute to lists, requires priorspecific permission and/or a fee. PLDI 02, June 17-19, 2002, \nBerlin, Germany. Copyright 2002 ACM 1-58113-463-0/02/0006 $5.00.  1. INTRODUCTION  Modern computer \nsystems spend significant amount of time in processing memory references. For example, the Itanium systems \nconsume nearly 40% of execution cycles [24] stalling on data cache and DTLB misses while running the \nSPECINT2000 benchmarks. Irregular programs such as the SPECINT2000 benchmarks contain many irregular \ndata references caused by pointer-chasing code. Irregular data references are difficult to prefetch, \nas the future address of a memory location is hard to anticipate by a compiler or hardware [15][21]. \nHowever, recent studies suggest that some pointer chasing references exhibit stride patterns. Namely, \nthe difference between two successive data addresses changes only infrequently at runtime. Stoutchinin \net al. [26] and Collins et al. [5] notice that several important loads in 181.mcf benchmark of SPECINT2000 \nsuite have stride reference patterns. Our experience indicates that many other SPECINT2000 programs, \nin addition to 181.mcf, contain important references with stride patterns. For example, the SPECINT2000 \n197.parser benchmark has code segments as shown in Figure 1. The first load at S1 chases a linked list \nand the second load at S2 references the string pointed to by the current list element. The program maintains \nits own memory allocation. The linked elements and the strings are allocated in the order that is referenced. \nConsequently, the address strides for both loads remain the same 94% of the time. for (; string_list \n!= NULL; string_list = sn) { S1: sn = string_list->next; S2: use string_list->string; other operations; \n}  Figure 1. Pointer-chasing code with stride patterns SPECINT2000 benchmark 254.gap also contains pointer \nreference loads with stride patterns. An important loop in the benchmark performs garbage collection. \nA simplified version of the loop is shown in Figure 2. The variable s is a handle. The first load at \nthe statement S1 accesses *s and it has four dominant strides, which remain the same for 29%, 28%, 21%, \nand 5% of the time, respectively. One of the dominant stride occurs because the increment at S4. The \nother three stride values depend on the values in (*s&#38;~3)->size added to s at S3. The second load \nat the statement S2 accesses (*s &#38; ~3)->ptr. This access has two dominant strides, which remain constant \nfor 48% and 47% of the time, respectively. These strides are mostly affected by the values in (*s&#38;~3)->size \nand by the allocation of the memory pointed to by *s. while ( s < bound ) { S1: if ((*s &#38; 3 == 0) \n{ // 71% time is true S2: access (*s &#38; ~3)->ptr S3: s = s + ((*s &#38; ~3)->size)+values; other \noperations; } else if ((*s &#38; 3 == 2) { // 29% time is true S4: s = s + *s; } else { // never come \nhere } }  Figure 2. Irregular code with multiple stride patterns Static compiler techniques [3][14][15][18][23][26], \nhowever, cannot easily discover the stride patterns in irregular programs. Pointer references make it \nhard for a compiler to understand the stride patterns of the load addresses. Also, the stride patterns \nin many cases are the results of memory allocation and compiler has limited ability to analyze memory \nallocation patterns. Without knowing that a load has stride patterns, it would be futile to insert stride \nprefetching, as doing so will penalize the references not exhibiting the prescribed strides. Profiling \nhas been used successfully to discover regularity among irregular behaviors. For example, edge profiling \nmay identify highly biased branches to form large traces in control-intensive-programs [4]. Value profiling \ncan reveal invariance in data variables to guide specialization optimizations [19]. Similarly, we will \nuse profiling to identify regular stride patterns in irregular programs. We will call this profiling \nthe stride profiling. Stride profiling is a potentially very slow process when every reference is profiled. \nAn important observation to reduce the profiling overhead is that stride patterns often exhibits in loads \nexecuted inside loops with relatively high trip counts. Thus selectively profiling these loads leads \nto significant reduction in profiling overhead. Furthermore, stride patterns of load is often statistically \nstable and can be discovered with sampling of the references. In this paper, we develop an efficient \nprofiling method to discover loads with stride patterns. We integrate the stride profiling into the traditional \nfrequency-profiling pass and the combined profiling pass is only slightly slower than the frequency profiling \nalone. The resulting stride profile is used to guide compiler prefetching during the profile feedback \npass. The example in Figure 3 illustrates our profiling and prefetching techniques. Figure 3 (a) shows \na typical pointer-chasing loop. For simplicity, we assume that the data address of the load reference \nP->data at L is P. The compiler instruments the loop for both block frequency and stride profiling as \nshown in Figure 3 (b). The operations freq[b1]++ and freq[b2]++ collect block frequency information for \nthe loop pre-head block b1 and the loop entry block b2. The conditional assignment \"pr = (r2/r1) > 128\" \nsets the predicate pr to true when the loop trip count is greater than 128, and false otherwise. The \nprofiling runtime routine strideProf is guarded by the predicate pr and it is actually invoked only when \nthe predicate pr is true. After the instrumented program is run, the stride profile is fed back and analyzed. \nThe profile could indicate that the load at L has the same stride, e.g. 60 bytes, 80% of the time. In \nthis case, the compiler can insert prefetching instructions as shown in Figure 3 (c), where the inserted \ninstruction prefetches the load two strides ahead (120=2*60). The compiler decides the number of iterations \nahead using heuristics to be described in this paper. In case the profile indicates that the load has \nmultiple dominant strides, e.g. 30 bytes 40% of the time and 120 bytes 50% of the time, the compiler \nmay insert prefetching instructions as shown in Figure 3 (d) to compute the strides before prefetching. \nFurthermore, the profile may suggest that a load has a constant stride, e.g. 60, sometimes and no stride \nbehavior in the rest of the execution, the compiler may insert a conditional prefetch as shown in Figure \n3 (e). The conditional prefetch can be implemented on Itanium using predication [11]. Figure 3. Example \nof stride profile guided prefetching This paper makes the following contributions. We develop an efficient \nmethod to collect both stride profile and the traditional frequency profile in the same profiling pass. \nThe integrated profiling pass runs only 17% slower than the frequency profiling alone, and significantly \nfaster than the na\u00efve stride profiling methods. Using the stride profile to guide compiler prefetching, \nwe obtain significant performance improvement for the SPECINT2000 programs running on the Itanium machines. \nFor example, we achieve a 1.59x speedup for \"181.mcf\", 1.14x for \"254.gap\", 1.08x for \"197.parser\". These \nperformance improvements, with an average of 7% for the entire benchmark suite, are significant for highly \noptimized SPECINT2000 programs running on real machines. We show that the performance gain is stable \nacross input data sets. We collect the stride profiles with both the reference-input data set and the \ntrain-input data set. The performance difference between the binaries compiled using the two profiles \nrunning with the reference input data set is small.  The rest of the paper is organized as follows. \nSection 2 overviews the prefetching algorithm. Section 3 presents the methods for efficient stride profiling. \nSection 4 provides the experimental results. Section 5 discusses the related work. Section 6 concludes \nthe paper and points out the future research directions. 2. OVERVIEW OF PREFETCHING ALGORITHM  For easy \nof discussions, we will refer a load that is selected for stride profiling as a profiled load, and a \nload that is selected for prefetching as a prefetched load. Also, we will refer to a load inside a loop \nas an in-loop load and a load not inside any loop an out-loop load. For a load inside an irreducible \nloop, we will treat it as an out-loop load. In the following subsections, we outline the prefetching \nalgorithm using the generation and feedback of the stride profiles. We first focus on in-loop loads, \nand then explain how to handle out-loop loads. 2.1. Stride Profile Generation  The compiler identifies \nthe profiled loads for stride profiling with heuristics. The naive method is to select all loads as profiled \nloads. Another heuristic is to select loads inside loops with high trip counts, if the frequency profile \nis available. The set of profiled loads can be refined to reduce profiling overhead. We call a set of \nloads equivalent if they are inside the same loop, in control equivalent blocks, and their addresses \nare different only by compile-time constants. They will have the same stride values or their strides \ncan be derived from the stride for another load. The compiler selects only one of them as the representative \nto be profiled. For each profiled load, the compiler inserts profiling code to collect its stride profile. \nWhen the instrumented program is run, the profiling runtime routine collects two types of information \nfor the profiled load: stride value profile and stride difference profile. The stride value profile collects \nthe top N most frequently occurred stride values and their frequencies. An example for N = 2 is shown \nin Figure 4 (a). For the 10 stride values from a profiled load, the profiling runtime routine identifies \nthat the most frequently occurred stride is 2 with a frequency of 5, and the second mostly occurred stride \nis 100 with a frequency of 4. The stride difference profile collects the top M most frequently occurred \ndifferences between successive strides and their frequencies. An example for M = 1 is shown in Figure \n4 (b). For the nine stride differences, the profiling runtime routine identifies that the most frequently \noccurred stride difference is 0 with a frequency of 7. The stride difference profile is used to distinguish \na phased stride sequence from an alternated stride sequence when they have the same stride value profile. \nThe stride sequence shown in Figure 4 (a) is a phased stride sequence. A phased stride sequence is characterized \nby the fact that its top stride difference is zero. An alternated stride sequence is shown in Figure \n4 (c), which has the same stride value profile as the phased stride sequence in Figure 4 (a). However, \nits most frequently occurred stride difference is not zero. A phased stride sequence is better for prefetching \nthan an alternated stride sequence as the stride values in phased stride sequence remain a constant over \na longer period, while the strides in an alternated stride sequence frequently change. We use the value-profiling \nalgorithm proposed in [2] to collect the stride profile. The detailed profiling algorithm will be given \nin Section 3. Figure 4. Stride value and stride difference profile example 2.2. Stride Profile Feedback \n During the profile feedback pass, the compiler reads both the stride profile and the frequency profile \nto guide its prefetching decision. It first uses the frequency profile to filter out loads that obviously \nwill not benefit from stride prefetching, such as not inside a loop with high trip count. It then identifies \nloads with stride patterns according to the following criteria and filters out all loads that do not \nsatisfy at least one of the criteria: Strong single stride (SSST) load: A load with one non-zero stride \nthat occurs with a very high probability (e.g. at least 70% of the time). We will refer to the threshold \n70% as SSST_threshold. Phased multi-stride (PMST) load: A load with multiple non-zero strides that together \noccur frequently (e.g. at least 30% of the time) and the differences between the strides are frequently \nzeroes (e.g. at least 20% of the time). For example, a phased multi-stride load may have the stride values \n32, 60, 1024 together occur more than 60% of time and 40% of the stride differences are zeros. We will \nrefer to the threshold 60% as PMST_threshold, and the threshold 40% PMST_diff_threshold. Weak single \nstride (WSST) load: A load with only one of the non-zero stride values that occurs somewhat frequently \n(e.g. at least 20% of the time) and the stride differences are sometime zeros (e.g. at least 10% of the \ntime). For example, a weak single stride load may have a stride 32 in 25% of time, and the stride differences \nare zeroes 10% of the time. We will refer to the threshold 25% as WSST_threshold, and the threshold 10% \nWSST_diff_threshold.  Any of the remaining loads may represent an equivalent set of loads with the same \nstride patterns. For each set of equivalent loads, although only one of them may be profiled, multiple \nof them may need to be prefetched. To decide which ones to prefetch, the compiler analyzes the range \nof cache area accessed by the loads in the set. Enough loads will be prefetched to cover the cache lines \nin that range. The selected loads are the prefetched loads. The process to identify prefetched loads \nis outlined in Figure 5, where FT is the frequency threshold, e.g. 2000, and TT is the trip count threshold, \ne.g. 128. Each profiled load uses a data structure prof_data to store its stride profile information. \nThe fields freq[1] to freq[4] stores the frequencies of the top 4 non-zero strides. The field total_freq \ncounts the number of total strides profiled. The field num_zero_diff remembers the number of zero stride \ndifferences. For each prefetched load, the compiler inserts prefetching instructions according to the \ntype of the load. Assume a prefetched load has a load address P. If this is a strong single stride load \nand the dominant stride value is S, the compiler inserts one prefetch instruction \"prefetch (P+K*S)\" \nin the same basic block before the load instruction, where K*S is a compile-time constant. The constant \nK is the prefetch distance determined by compiler analysis. SSST_loads = {} PMST_loads = {} WSST_loads \n= {} for each profiled load { if ((load-> freq <= FT) continue; // load is filtered out if (load is \nan in-loop load &#38;&#38; load->loop->trip_count<=TT) continue; // load is filtered out pdata = load->prof_data \ntop1freq = pdata->freq[1] top4freq = pdata->freq[1] + pdata->freq[2] + pdata->freq[3] + pdata->freq[4] \nnum0diff = pdata->num_zero_diff totalfreq = pdata->total_freq cover_loads = loads that cover the cache \nlines accessed by the equivalent set represented by load if (top1freq / totalfreq > SSST_threshold) SSST_loads \n.= { cover_loads } elseif (top4freq / totalfreq > PMST_threshold &#38;&#38; num0diff / totalfreq > PMST_diff_threshold) \nPMST_loads .= { cover_loads } elseif (top1freq / totalfreq > WSST_threshold&#38;&#38; &#38;&#38; num0diff \n/ totalfreq > PMST_diff_threshold) WSST_loads .= { cover_loads } }  Figure 5. Identify prefetched loads \nduring profile feedback For an in-loop load with a trip count trip_count and a dominant stride stride, \nthe size of the referenced data area is (trip_count * stride). If this size is larger than the size of \na cache level with L cycle miss latency, we can assume that it takes at least L cycles. If the loop body \ntakes B cycles without taking the miss latency of prefetched loads into account, then K can be computed \nas min(.L/B., C), where C is the maximum prefetch distance (e.g. 8). The value of K may also be determined \nusing loop trip count value as follows: K = min (.trip_count / TT., C) where TT is the trip count threshold \n(e.g. 128). If this is a phased multi-stride in-loop load, the compiler inserts the prefetching instructions \nas follows: 1. Insert a move instruction before the load operation to save its address in a scratch register. \n 2. Insert a subtract instruction before the move instruction to subtract the value in the scratch register \nfrom the current address of the load. Place the difference in a register called stride. 3. Insert a \nprefetch (P+K*stride) instruction before the load, where K is determined as described previously, but \nrounded to a power of two to avoid the multiplication operation.  If this is a weak single stride in-loop \nload, the compiler inserts prefetching instructions in the same way as the steps 1 and 2 described for \na phased multi-stride load. Step 3 is modified as follows: 3'. Insert the following conditional prefetching \nbefore the load.  p = (stride == profiled stride) p? prefetch (P+K*stride) The reason for a conditional \nprefetching is to reduce the number of useless prefetches, when the load exhibits only occasional stride \npatterns. 2.3. Handling Out-Loop Loads  Although the subsections above consider only in-loop loads for \nprofiling and prefetching, we can also profile and prefetch out-loop loads. Our experiments show that \nout-loop loads are much more expensive to profile and also more difficult to prefetch. An out-loop load \nis hard to prefetch primarily because the prefetching code sequences for a phased multi-stride and a \nweak single stride load do not work for an out-loop load. The successive executions of the out-loop load \nwill be from different invocations of the current function. The address value saved in the scratch register \nwill be lost when the function returns. We would need a static memory location for each such load to \nstore the address value for computing its stride value. The load and store to the static memory location \nincreases prefetching overhead. For this reason, we will not prefetch out-loop loads when they are classified \nas phased multi-stride or weak single stride loads. Another reason why an out-loop load is not easy to \nprefetch is that the heuristic for determining the prefetching distance for an in-loop load no longer \nworks for an out-loop load. The values used to estimate prefetching distance, such as trip counts and \nthe loop body latencies, are unavailable for an out-loop load. As a result, we will prefetch only SSST \nout-loop loads and use a fixed constant, such as 4, as the prefetching distance. 3. EFFICIENT STRIDE \nPROFILING  Stride profile is collected by instrumenting each profiled load to invoke a profiling runtime \nroutine. The profiling overhead can be reduced in two directions. First, the profiling routine should \nbe made as efficient as possible. Second, the invocation to the runtime routine can be made less frequent. \n We develop an efficient profiling method along both directions. 3.1. Efficient Profiling Runtime Routine \n The profiling runtime routine is outlined in Figure 6, where address is the address for the load being \nprofiled, and prof_data is the data area allocated for the load to collect profiling result. The profiling \nruntime routine calls the Least Frequently Used (LFU) replacement algorithm described in [2] to collect \nstride profiles. LFU manages two buffers, a temp buffer and a final buffer, to keep track of the most \nfrequently recurrent strides. For each stride passed to LFU, if the stride is already in an entry in \nthe temp buffer, the frequency count of the entry is incremented. If no entry is found for the stride, \nthe least frequently used entry in the temp buffer is replaced. The temp buffer is periodically merged \nwith the final buffer by selecting the strides with the highest frequencies in both buffers into the \nfinal buffer. At that time, the temp buffer is cleared. strideProf(address, prof_data) { stride = address \n prof_data->prev_address if (stride == 0) prof_data->num_zero_stride ++ return diff = stride prof_data->prev_stride \nif (diff == 0) prof_data->num_zero_diff ++ else prof_data->prev_stride = stride prof_data->prev_address \n= address LFU(stride, prof_data) }  Figure 6. Profiling runtime routine int is_same_value (a1, a2) { \nif ((a1>>4) == (a2>>4)) return 1 return 0 } strideProf(address, prof_data) { if (is_same_value(address, \nprof_data->prev_address) prof_data->num_zero_stride ++ return stride = address prof_data->prev_address \ndiff = stride prof_data->prev_stride if (diff == 0) prof_data->num_zero_diff ++ else prof_data->prev_stride \n= stride prof_data->prev_address = address LFU(stride, prof_data) }  Figure 7. Enhanced profiling runtime \n Figure 8. Example of fine sampling We could also use the LFU algorithm to collect the stride difference \nprofile. However, we can simply count the number of zero differences between successive strides. If the \npercentage of the zero differences is high, we know that the stride sequence is phased. Since LFU is \na heavy-duty operation, the number of zero strides are also checked and counted without going through \nthe LFU operation. The execution speed of the LFU algorithm depends on the number of different values \nit tracks in its buffers. The less number of different values it tracks, the faster it runs. Notice that \nstride prefetching often remains effective when the stride value changes slightly. For example, the prefetching \nat address and the prefetching at address+8 should not have much performance difference, if the cache \nline is large enough to accommodate the data at both addresses. Thus, the profiling runtime routine may \nreduce the number of different strides it tracks by treating the strides that are different by a small \nvalue, such as half the size of a cache line or less, as the same. The enhanced profiling routine is \nshown in Figure 7. The subroutine is_same_value considers two values the same if they differ only in \nthe last 4 bits. Is_same_value is used inside the LFU routine whenever it compares any two strides for \nequality. strideProf(address, prof_data) { // chunk sampling static int number_skipped = 0 static int \nnumber_profiled = 0 if (number_skipped < N1) number_skipped ++; return; else if (number_profiled == N2) \nnumber_profiled = 0; number_skipped = 0; return; number_profiled ++; // fine sampling if (prof_data->number_to_skip \n> 0) prof_data->number_to_skip --; return; prof_data->number_to_skip = F-1; if (is_same_value(address, \nprof_data->prev_address)) prof_data->num_zero_stride ++ return stride = address prof_data->prev_address \ndiff = stride prof_data->prev_stride if (diff == 0) prof_data->num_zero_diff ++ else prof_data->prev_stride \n= stride prof_data->prev_address = address LFU(stride, prof_data) }  Figure 9. Profiling runtime routine \nwith sampling We may further reduce profiling overhead by introducing sampling into the profiling runtime \nroutine. The stride profile can be sampled in two ways. First, for a sequence of load addresses, we may \ntake one sample after every F references, where F is constant such as 4. We call this the fine sampling \nmethod. A stride value S1 collected with fine sampling relates to the original stride S2 by the equation \nS1 = F*S2. Thus the compiler can derive the original stride value as S2 = S1/F. An example of fine sampling \nis shown in Figure 8. The second sampling method is for chunks of references. In this method, after every \nN1 references are skipped, the profiling runtime routine profiles the next N2 references. Example values \nfor N1, and N2 are 8 millions and 2 millions, respectively. The profiling runtime routine with sampling \nis shown in Figure 9. 3.2. Integrated Profiling  To reduce the frequency that the profiling runtime \nroutine is called, we may select only the loads inside a loop with a high trip count, e.g. > 128, for \nprofiling. This trip count condition indicates that the load is likely to touch a large range of memory \nand will benefit from stride prefetching. For example, the range [x, x+stride * trip_count] of memory \narea will be touched. Our experiment indeed shows that the majority of loads that can benefit from stride \nprefetching are inside loops with high trip counts (see Section 4). Therefore, restricting profiling \nand prefetching to in-loop loads with high trip counts does not impact the performance. Furthermore, \nthe number of dynamic loads inside loops with high trip counts is small. For example, our data indicates \nthat about 7.5% of dynamic loads are inside loops with trip counts > 128 for the SPECINT2000 benchmarks. \nAs a result, restricting prefetching to in-loop loads with high trip counts can significantly reduce \nprofiling overhead.   Figure 10. Example for determining trip count The trip count of a loop is calculated \nas the ratio of the entry block frequency over the entering frequency from outside of the loop to the \nentry block of the loop. For the loop with edge frequencies as shown in Figure 10, the trip count is \ncalculated as follows. . Checking the trip count condition needs the frequency profile information. \nAlthough the frequency profile is usually available for compiler optimizations [4], we would need a separate \npass to collect the stride profile by using the frequency profile generated in an early pass. This two-pass \nmethod poses a usability problem. The additional pass to collect stride profiles could place a significant \nburden on software development. This is especially painful for a cross compilation environment, in which \nthe compilation and execution are on different machines, and a lot of manual work is involved to execute \nthe instrumented program to obtain the profiles. In the following, we develop a number of one-pass methods \nto collect both frequency and stride profiles in the same profiling pass. All the methods select profiled \nloads without using frequency profile information and rely on the frequency profile during the profile \nfeedback pass to filter out unwanted loads (such as using the routine shown in Figure 5). The first method \nsimply collects the stride profile for every load while collecting the frequency profile. The method \ncan be used to profile in-loop loads as well as out-loop loads. If it is used only to profile in-loop \nloads, we will call it the na\u00efve-loop method. When it is used to collect stride profiles for both in-loop \nand out-loop loads, we will call it the na\u00efve-all method. Notice that after the unwanted loads are filtered \nout, the stride profile collected by the na\u00efve-loop method is the same as that collected by the two-pass \nmethod. The na\u00efve-loop method can be improved in two ways. First, loads with loop-invariant memory addresses \nshould not be profiled, as their stride values are usually zero and cannot benefit from stride prefetching. \nSecond, the partially collected frequency profile information may be used to check for the trip count \ncondition, and only when the trip count condition is satisfied, should the calls to the profiling runtime \nroutine inside the loop be activated. This process is efficient as the check code is inserted outside \nthe loop and the profiling runtime routine is not invoked inside the loop unless the condition is satisfied. \nThe following methods include both of the improvements. The stride profiling can be integrated with either \nthe block frequency profiling or the edge frequency profiling, depending on which frequency profiling \nthe compiler uses. If the compiler uses the block frequency profiling already, the stride profiling can \nbe integrated to use the partially collected block frequency information to check for the trip count \ncondition. We will call this method the block-check method. In Figure 11 (a), two blocks b1 and b2 are \ninstrumented for block frequency profiling. For the load in block b2, the naive method would instrument \nblock b2 as shown in Figure 11 (b). If the loop turns out to have a low trip count, the stride-profiling \noverhead could be wasted. The block-check method instrument blocks b1 and b2 as shown in Figure 11 (c), \nwhere the register r1 stores the frequency of the loop pre-head block and r2 contains the frequency of \nthe loop entry block. It computes the trip count by r2/r1 and checks the trip count condition in block \nb1, and if this condition is satisfied it sets a predicate register to true. Block b2 uses the predicate \nto guard the invocation to the profiling runtime routine. To avoid division or multiplication overhead, \nwe use r1<(r2 >>W) to compute r2/r1 > TT, where TT is the trip count threshold and W = . log2TT. is a \ncompile-time constant. Similarly, if the compiler uses the edge frequency profiling already, the stride \nprofiling can be integrated to use the partially collected edge frequency information to check for the \ntrip count condition. We will call this method the edge-check method. For the example in Figure 12 (a), \nthe edges e1=(b1.b2), e2=(b2.b2), and e3=(b2.b3) are instrumented for edge frequency profiling. Although \nthere is no block frequency information for the loop entry block b2, the frequency of block b2 can be \ncomputed as the sum of frequencies of its outgoing edges (freq[e2] + freq[e3]). The instrumentation for \nboth edge frequency and stride profiling is shown in Figure 12 (b).    (a) (b) (c)  Figure 11. Example \nof block-check method In general, the loop entry block may have any number of outgoing edges. To find \nthe entry block frequency, we need to sum up all frequencies on the outgoing edges. Similarly, the entry \nblock may have any number of the incoming edges from outside of the loop. The pre-head frequency is the \nsum of all the frequencies on the incoming edges. Figure 13 shows a general loop and the instrumentation \nfor collecting both the edge frequency and the stride profiles. The instrumentation process for the edge-check \nmethod is shown in Figure 14.   (a) (b)  Figure 12. Example of edge-check method   Figure 13. Example \nof general edge-check method After the instrumented program is executed, both frequency and stride profiles \nare generated. The frequency profile is exactly the same as that would be collected in a separate pass. \nHowever, the stride profile collected by the edge-check or block-check method, after the unwanted loads \nare filtered, may be different from what it would be collected by the two-pass method. This is because \nthe invocation to profiling runtime routine may be turned on and off during the execution. In particular, \nthe two-pass method may profile loads inside a high-trip count loop whose loop nest is executed only \nonce. The block-check and edge-check methods, on the other hand, will not have stride profile information \nfor loads inside loop whose loop nest is executed only once. Instrument_edge_check() { // don t profile \nloads whose addresses are loop invariant for each loop for each load if (load->addr is not a loop invariant) \nmark load as a profiled load identify equivalent loads and reduce profiled loads for each loop that contains \na profiled load // reserve a predicate register for the loop loop->predicate = create a new predicate \nfor each edge // collect edge frequency insert the following instructions on the edge r1 = load edge \ns counter r1++ store r1 to edge s counter // compute trip count predicate if the edge is an incoming \nedge from outside to the loop for every other incoming edge e of the loop insert on the edge r1 += load \ne s counter insert r2 = 0 on the edge for every outgoing edge e of the loop entry block insert on the \nedge r2 += load e s counter insert on the edge r2 = r2 >> W loop->predicate = r2 > r1 // guard strideProf \nwith trip count predicate for each loop for each profiled load inside the loop pr = loop->predicate address \n= load s data address pdata = load->prof_data if the load is a predicated instruction insert the following \ninstruction before load pr1 = pr &#38;&#38; load->predicate pr1 ? strideProf(address, pdata) else insert \nthe following instruction before load pr ? strideProf(address, pdata) }  Figure 14. Instrumentation \nfor edge-check method 4. EXPERIMENTAL RESULTS  We implemented the new profiling and prefetching algorithms \nin a research compiler for the Itanium Processor Family (IPF). The compiler is based on a production \ncompiler with additional components to make compiler and architectural exploration easier. The generated \ncodes are highly optimized with all of the inter-procedural, intra-procedural, architectural specific, \nand profile-guided optimizations, assisted with aggressive memory disambiguation and whole program knowledge \n[8][10][13]. Our compiler automatically performs the profiling and stride-profile guided prefetching \ntransformation without any hand coding involvement. The prefetching for weak single strided load (WSST) \nis not enabled for this paper, as it does not show noticeable performance contribution. All our experiments \nare performed on a 733 MHz Itanium machine [11] running the SPECINT2000 benchmarks ([9] see Figure 15). \nThe Itanium machine used in our experiment have a 16KB 4-way set associative L1 data cache, a 96KB 6-way \nset associative unified L2 cache, 2MB 4-way set associative unified L3 cache, and 1 GB memory. Programs \nLang Description 164.gzip C Compression/Decompression 175.vpr C FPGA circuit placement and routing \n 176.gcc C C programming language compiler 181.mcf C Combinatorial Optimization 186.crafty C Game Playing: \nChess 197.parser C Word Processing 252.eon C++ Computer Visualization 253.perlbmk C PERL programming \nlanguage 254.gap C Group theory, interpreter 255.vortex C Object-oriented database 256.bzip2 C Compression \n 300.twolf C Place and route simulator  Figure 15. SPECINT2000 benchmarks We measured the performance \ngains and profiling overhead for the edge-check, na\u00efve-loop, and na\u00efve-all profiling methods without \nsampling, as well as these methods with sampling. The sampling versions will be called sample-edge-check, \nsample-na\u00efve-loop, and sample-na\u00efve-all. These methods profile only in-loop loads, except the na\u00efve-all \nand sample-na\u00efve-all methods, which profile also out-loop loads. Notice that the block-check and the \nedge-check method generate the same stride profile. Since our compiler only supports edge frequency profiling, \nwe will not evaluate the block-check method. The performance gain with the stride profile guided prefetching \nis measured in terms of speedup, namely the ratio of the execution time of the binaries built with feedback \nof only the edge frequency profile over the execution time of the binaries built with feedback of both \nthe edge frequency and stride profiles which is used to guide stride prefetching. For example, a speedup \nof 1.2 indicates that the stride prefetching improves the performance by 20%. The profiling overhead \nis reported as the following ratio.  For example, a ratio of 0.2 indicates that the new integrated profiling \nmethod is about 20% slower than the edge profiling alone. We also measured the sensitivity of the performance \ngains with respect to the input data sets. 4.1. Performance Gains  For this experiment, the profile \ngeneration runs use the train input data set and the performance runs with profile feedback and prefetching \nuse the reference input data set. Figure 16 shows the speedup from prefetching guided by the stride profiles \ncollected using the six different profiling methods.  Figure 16. Speedup of stride prefetching Overall, \nall the profiling methods result in very similar performance gain as the edge-check method: about 1.59x \nspeedup for \"181.mcf\", 1.14x for \"254.gap\", 1.08x for \"197.parser\" and smaller gains in more benchmarks. \nThese performance improvements, with an average of 7%, are significant for highly optimized SPECINT2000 \nprograms running on real machines. There are some minor performance differences for the na\u00efve methods. \nThe na\u00efve-loop (as well as sample-na\u00efve-loop) method performs slightly worse than the edge-check method \nfor 197.parser (and 181.mcf). The differences are due to the slight differences in the stride profiles \nthey collected. Specifically, the na\u00efve-loop method will generate stride profiles for loads inside a \nloop whose loop nest is executed only once, while the edge-check method will not. It seems that prefetching \nthe loads inside a loop whose loop nest is executed once will degrade performance.  Figure 17. Percentage \nof in-loop and out-loop load references The performance of the na\u00efve-all (as well as sample-na\u00efve-all) \nmethod is slightly better than the other methods mainly because it profiles and prefetches additionally \nthe out-loop loads. It improves the speedup for 197.parser from 1.08x to 1.10x and that for 254.gap from \n1.14x to 1.16x. However, the difference is small considering the large number of out-loop loads profiled. \n Figure 17 shows about 40% of load references (weighted by frequencies) are from out-loop loads and are \nprofiled by the na\u00efve-all method. The reasons for the insignificant performance gain from prefetching \nout-loop loads are two folds. First, only a small portion of out-loop-loads is prefetched. Figure 18 \nshows the distribution of out-loop loads by stride properties, collected with the na\u00efve-all method. Majority \nof the out-loop loads with stride properties are classified into PMST and WSST, which cannot be prefetched. \nOnly 1.7% of the load references are from out-loop loads and is prefetched as SSST loads. In contrast, \nFigure 19 shows that nearly all of in-loop loads with stride patterns are prefetched as SSST and PMST. \nSecond, even when an out-loop load is prefetched, the prefetching and using of the prefetched data may \nbe separated by many other memory references, unlike an in-loop load where the loop body is usually small. \nThe prefetched value could be evicted before it is used.  Figure 18. Distribution of out-loop loads \nby stride properties  Figure 19. Distribution of in-loop loads by stride properties Since all the profiling \nmethods result in similar performance gain, we should select the profiling method basing on their easy \nof use and profiling overhead for production compilers. In particular, we do not need to use the two-pass \nprofiling method that complicates the program development process, as the two-pass method prefetches \nthe same set of loads as the na\u00efve-loop method. 4.2. Profiling Overhead  For this experiment, all profile \ngeneration run uses the train input data set. Figure 20 shows the overhead of the integrated profiling \nmethods over the edge frequency profiling alone. On the average, the edge-check method is about 58% slower \nthan the edge profiling alone, while the na\u00efve-loop method is 272% slower and the na\u00efve-all method is \n436% slower. The edge-check method achieves the low overhead by guarding the calls to the profiling runtime \nroutine with the trip count condition. Figure 21 shows the percentages of load references processed by \nthe profiling runtime routine strideProf for all the six profiling methods. Although there is about \n60% of load references inside loops (see the bar marked with naive-loop in the average area), after checking \nthe trip count condition, only about 11% load references is processed by the strideProf routine (see \nthe bars marked with edge-check). The na\u00efve-loop method on the other hand has to profile all in-loop \nloads (60%). The na\u00efve-all method has to profile all in-loop and out-loop loads (100%). Figure 20 also \nshows that sampling further reduces the overhead: the sample-edge-check method is only 17% slower than \nthe edge profiling alone, while the sample-na\u00efve-loop method is 67% slower and the sample-na\u00efve-all method \nis 122% slower. This is because that only a portion of the total number of load references is sampled. \n Figure 21 shows that less than 1% load references is processed in the strideProf routine (after the \nsampling code) with the sample-edge-check method. The sample-na\u00efve-loop method profiles 3% of the load \nreferences, and the sample-na\u00efve-all method profiles about 5% of load references. The strideProf routine \nis capable of collecting the number of zero strides directly and bypassing the LFU routine. This reduces \nthe profiling overhead for all the profiling routines. Figure 22 shows that the percentage of load references \nthat are processed by the LFU routine is significantly lower than those processed by the strideProf as \nshown in Figure 21. The difference between the corresponding bars on the two graphs shows the percentage \nof zero strides directly handled by strideProf without going through the LFU routine. For example, for \nthe na\u00efve-all method, 100% load references are processed by strideProf while only 68% of them are processed \nby the LFU routine. This indicates that about 32% of load references have zero strides.  Figure 20. \nProfiling overhead  Considering both the performance gain and profiling overhead, we suggest using the \nsample-edge-check method in production compilers, since it has the lowest profiling overhead and similar \nperformance to the other methods.  Figure 21. Percentage of load references processed in strideProf \nroutine (after sampling)  Figure 22. Percentage of load references processed by LFU  Figure 23. Performance \nof train and ref 4.3. Sensitivity to Input Data Sets  In this experiment, we examine the sensitivity \nof the performance gain to input data sets with sample-edge-check profiling and prefetching. We collect \ntwo sets of profiles, one collected from running with the train-input data set and the other with the \nreference-input data set. We compare the performance gains for the binaries compiled with the two profiles, \nboth running with the reference input data set. We will refer to the binaries compiled with the profiles \ncollected with the train input and running on the reference input as \"train\". Similarly, we will refer \nto the binaries compiled with the profiles collected with the reference input and running on the reference \ninput as \"ref\". Figure 23 shows that the performance gain of train is lower than that of the ref. For \nexample, the ref improves over the train for 197.parser from a speedup of 1.08x to 1.09x, and for 254.gap \nfrom 1.14x to 1.20x. Notice that the sample-edge-check profiling method collects both edge profile and \nstride profile. The ref not only uses stride profile collected with the reference input data set, it \nalso uses the edge profile collected with the reference input data set. The performance improvement of \nref over train could be the result of the better edge profile. To isolate the effects of stride profile \nand edge profile with the reference input data sets, we further introduce the following two sets of binaries. \nThese binaries are collected with two profiling passes. edge.ref-stride.train: the binaries compiled \nwith the edge profile collected with the reference input data set and the stride profile collected with \nthe train input data set. edge.train-stride.ref: the binaries compiled with the edge profile collected \nwith the train input data set and stride profile collected with reference input data set.  Figure 24 \nshows that the improvement of edge.ref-stride.train over the train. This improvement is similar to the \nimprovement of ref to train. In other words, the performance improvement of ref over train is the result \nof the better edge profile.  Figure 24. Performance of train and edge.ref-stride.train  Figure 25. \nPerformance of train and edge.train-stride.ref Figure 25 further shows that the performance gain of edge.train-stride.ref \nis very similar to that of the train, although 197.parser shows slightly lower performance gain with \nedge.train-stride.ref. The reason for the lower performance gain of edge.train-stride.ref for 197.parser \nis that stride profile-guided prefetching uses edge profile to filter out unwanted loads. When the edge \nprofile and the stride profile do not match (with edge profile collected from train input data set and \nstride profile collected from reference input data set), a few loads good for stride prefetching is unfortunately \nfiltered out. Comparing the results in Figure 24 and Figure 25, we can see that the sensitivity of performance \ngain to stride profiles collected with different input data sets is less than that to edge profiles. \nSince it is well accepted that edge profile is reasonably stable across input data sets [7], we conclude \nthat stride profile is stable across input data sets as well. 5. RELATED WORK  There is extensive research \non static compiler prefetching. The earlier work focused on prefetching array references [3][18][23]. \nData reuse analysis is done to reduce the amount of redundant prefetching to the same cache line. These \ntechniques have been incorporated in the production compiler with significant performance gains for regular \nprograms (i.e. SPECFP2000 [10]). However, we have yet to see them bringing positive overall performance \ngain for the SPECINT2000 benchmarks. For example, in [27] we established that the static prefetching \nmethod results in a negative overall performance improvement. Enhancing the static prefetching with frequency \nprofile information improve the performance noticeably, but still much lower than the stride profile \nguided prefetching collected with a . Several recent studies focus on prefetching for recursive data \nstructures. Lipasti et al [14] use heuristics to de-reference pointers passed into procedures. They analyze \nexecution trace and inserting prefetching instructions during cache simulation. An overall improvement \nof 1.7% is reported. Luk and Mowry [15] examine several software techniques, including compiler-direct \ngreedy prefetching, software full jumping and data linearization. Roth and Sohi describe a framework \nfor jump-pointer prefetching [22]. Although these techniques have shown promising results in small benchmarks \nusing hand-optimized code, designing compiler algorithms to automatically and beneficially perform the \ntransformations could pose a serious challenge. The most relevant work to ours is the compile-time stride \nprefetching method proposed by Stoutchinin et al [26]. It uses compiler analysis to detect induction \npointers and insert instructions into user programs to compute strides and perform stride prefetching \nfor the induction pointers. However, the compiler analysis cannot determine whether an induction pointer \nhas stride patterns and the prefetching instructions have to be inserted conservatively, e.g. only when \nmachine resource allows the prefetching instructions. Still, this technique can slow a program down when \nthe stride prefetching is applied to loads without stride patterns. They showed 20% performance gain \nfor 181.mcf, and either very small (< 1%) or negative performance gain for the remaining SPECINT2000 \nbenchmarks. Dedicated hardware is also proposed to prefetch for irregular references. The stream buffer \nbased prefetching [12][20][25] uses history information to prefetch data into a stream buffer. When a \nload accesses the data cache, it also searches the stream buffer entries in parallel. If the data requested \nby the load is in the stream buffer, that data is transferred to the cache. The stride prefetching hardware \n[6] uses a reference prediction table, RPT, to monitor cache misses and record the difference between \nsuccessive misses as the stride for prefetching. For a program with many loads that miss cache, the hardware \ntables may overflow and cause useful strides to be thrown away, and thus reduce the effectiveness of \nthe prefetching. We believe that our software techniques can be a viable alternative to reduce hardware \nbudget and power consumption. Pre-computation [5] [16] [28] uses speculative threads to run portions \nof the program in a separate thread to prefetch for the main program. The major challenge is to identify \nsmall and precise backward slices that lead to the loads missing caches. Pointer references make the \nanalysis difficult. All of the studies so far use hand coding to identify the backward slices. 6. CONCLUSIONS \nAND FUTURE WORK  In this paper, we present novel profiling and prefetching techniques for guiding the \ncompiler to perform stride prefetching. We integrate the stride profiling into the traditional frequency-profiling \npass and the integrated profiling pass can be only 17% slower than the frequency profiling alone. The \ncompiler prefetching decision guided by the profile is highly selective and beneficial. We show significant \nperformance improvement for the SPECINT2000 programs running on the Itanium machines. For example, we \nachieve a 1.59x speedup for \"181.mcf\", 1.14x for \"254.gap\", and 1.08x for \"197.parser\". These performance \nimprovements, with an average of 7% for the entire benchmark suite, are significant for highly optimized \nSPECINT2000 programs running on real machines. We also demonstrate that the performance gain is stable \nacross profiling data sets. These benefits make the new techniques suitable for production compilers. \nWe are currently pursuing the study in the following directions. The current prefetching algorithm is \nineffective for out-loop loads, because the prefetch operations and the use of the prefetched data are \nsometimes separated by a large number of memory references. We may profile the number of memory references \nbetween the successive references at a load site. If this number is large, we should not prefetch for \nthe load. This information can also be beneficial for in-loop loads as some loops can be very large or \ncontain function calls. Extend our method to prefetch for loads without stride patterns. There are cases \nwhere a load itself does not have stride patterns, but its address depends on another load with stride \npatterns. We may extend our method to prefetch loads that depend on the results of the prefetching instructions. \n Investigate the possibility of using customized memory allocation to produce more strides that can be \nprefetched.  7. ACKNOWLEDGEMENTS  We would like to thank Jesse Fang, John Shen, Sun Chan, Dong-Yuan \nChen, Li-Ling Chen, Roy Ju, Rakesh Krishnaiyer, Yong-Fong Lee, Hsien-Hsin Lee, Wei Li, Tin-fook Ngai, \nand Mauricio Serrano for their support and valuable comments. We appreciate the comments from the anonymous \nreviewers that helped improve the quality of the paper. REFERENCES [1] Ball, T. and J. Larus, Optimally \nprofiling and tracing programs, ACM Transactions on Programming Languages and Systems, 16(3): 1319-1360, \nJuly 1994. [2] Calder, B., P. Feller, and A. Eustance, Value Profiling, MICRO30, Dec. 1997. [3] Callahan, \nD., K. Kennedy, and A. Porterfield, Software Prefetching , in Proceedings of the Fourth International \nConference on Architecture Support for Programming Languages and Operating Systems, 1991, 40-52. [4] \nChang, P. P, S. Mahlke, and W.M. Hwu, Using profile information to assist classic code optimizations, \nSoftware-practice and Experience, 1991. [5] Collins, J., H. Wang, H. Christopher, D. Tullsen, C. J. \nHughes, Y. F. Lee, D. Lavery and J. Shen, \"Speculative Pre-computation: Long-range Prefetching of Delinquent \nLoads,\" ISCA28, 2001. [6] Dahlgren, F., Stenstrom, P., Evaluation of Hardware-Based Stride and Sequential \nPrefetching in Shared-Memory Multiprocessors , IEEE Transactions on Parallel and Distributed Systems, \nVol. 7, No. 4, April 1996. [7] Fisher, J. and S. Freudenberger, \"Predicting Conditional Branch Directions \nFrom Previous Runs of a Program,\" Proc. 5th Annual Intl. Conf. on Architecture Support for Prog. Lang. \nand Operating Systems, Oct. 1992. [8] Ghiya, R., D. Lavery, D. Sehr, On the Importance of Points-to \nAnalysis and Other Memory Disambiguation Methods for C Programs, PLDI2001, May 2001. [9] Henning, J.L., \nSPEC CPU2000: Measuring CPU Performance in the New Millennium, IEEE Computer, July 2000. [10] Intel \nCorp, \"Benchmarks: Intel\u00ae Itanium based systems,\" http://www.intel.com/eBusiness/products/ia64/overview/bm012101.htm. \n [11] Intel Corp, Intel\u00ae Itanium Processor Hardware Developer s Manual, 2000. http://developer.intel.com/design/ia-64/manuals.htm. \n [12] Jouppi, N., \"Improving direct-mapped cache performance by the addition of a small fully associative \ncache and prefetch buffers,\" ISCA17, May 1990 [13] Krishnaiyer, R., D. Kulkarni, D. Lavery, W. Li, C. \nLim, J. Ng, and D. Sehr, \"An Advanced Optimizer for the IA64 Architecture, IEEE Micro, Vol 20, No 6, \nNov 2000, 60-68 [14] Lipasti, M.H., W.J. Schmidt, S.R. Kunkel, and R.R. Roediger, SPAID: Software Prefetching \nin Pointer and Call Intensive Environments , MICRO28, Nov 1995, 231-236. [15] Luk, C.K. and T.C. Mowry, \nCompiler-Based Prefetching for Recursive Data Structures, in Proceedings of the Seventh International \nConference on Architectural Support for Programming Languages and Operating Systems, September 1996, \n222-233. [16] Luk, C.K., \"Tolerating Memory Latency through Software-Controlled Pre-Execution in Simultaneous \nMultithreading Processors, ISCA28, 2001. [17] Mahlke, S.A., D.C. Lin, W.Y. Chen, R.E. Hank, and R.A. \nBringmann, Effective Compiler Support for Predicated Execution Using Hyperblock, MICRO25, Dec. 1992. \n [18] Mowry, T.C., M.S. Lam, and A. Gupta, Design and Evaluation of a Compiler Algorithm for Prefetching, \nin Proceedings of the Fifth International Conference on Architectural Support for Programming Languages \nand Operating Systems, October 1992, 62-73. [19] Muth, R., S. Watterson, S. Debray, Code Specialization \nbased on Value Profiling, SAS2000. [20] Palacharla, S. and R. Kessler, \"Evaluating stream buffers as \nsecondary cache replacement,\" ISCA21, April 1994. [21] Roth, A., A. Moshovos, and G. Sohi, Dependence \nBased Prefetching for Linked Data Structures, Proc. 8th ASPLOS, pages 115-126. Oct. 1998. [22] Roth, \nA., and G. Sohi. Effective Jump-Pointer Prefetching for linked data structures, ISCA26, June 1999, 111-121. \n [23] Santhanam, V., E. Gornish, and W. Hsu, Data Prefetching on the HP PA-8000,\" ISCA24, June 1997, \n264 273. [24] Serrano, M. J. and Y. Wu, Memory Performance Analysis of SPEC2000C for the Intel ItaniumTM \nProcessor, IEEE 4th Annual Workshop on Workload Characterization, in Conjunction with MICRO34, Austin, \nTexas, December 2, 2001. [25] Sherwood T., S. Sair, B. Calder, \"Predictor-Directed Stream Buffers,\" \nMICRO33, Dec. 2000. [26] Stoutchinin, A., J. N. Amaral, G. Gao, J. Dehnert, S. Jain, and A. Douillet \nSpeculative Prefetching of Induction Pointers, in Proceedings of CC 2001, Geneva, Italy, 2 - 6 April, \n2001. Also in LNCS 2207, pp 289-303, 2001. [27] Wu, Y., M. Serrano, R. Krishnaiyer, W. Li, J. Fang, \nValue-Profile Guided Stride Prefetching for Irregular Code, in Proceedings of CC 2002, April 6 - 14, \n2002, Grenoble, France. [28] Zilles, C. and G. Sohi, \"Execution-based Prediction Using Speculative Slices,\" \nISCA28, 2001.   \n\t\t\t", "proc_id": "512529", "abstract": "Irregular data references are difficult to prefetch, as the future memory address of a load instruction is hard to anticipate by a compiler. However, recent studies as well as our experience indicate that some important load instructions in irregular programs contain stride access patterns. Although the load instructions with stride patterns are difficult to identify with static compiler techniques, we developed an efficient profiling method to discover these load instructions. The new profiling method integrates the profiling for stride information and the traditional profiling for edge frequency into a single profiling pass. The integrated profiling pass runs only 17% slower than the frequency profiling alone. The collected stride information helps the compiler to identify load instructions with stride patterns that can be prefetched efficiently and beneficially. We implemented the new profiling and prefetching techniques in a research compiler for Itanium Processor Family (IPF), and obtained significant performance improvement for the SPECINT2000 programs running on Itanium machines. For example, we achieved a 1.59x speedup for 181.mcf, 1.14x for 254.gap, and 1.08x for 197.parser. We also showed that the performance gain is stable across input data sets. These benefits make the new profiling and prefetching techniques suitable for production compilers.", "authors": [{"name": "Youfeng Wu", "author_profile_id": "81330500621", "affiliation": "Intel Labs, Santa Clara, CA", "person_id": "PP39052915", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/512529.512555", "year": "2002", "article_id": "512555", "conference": "PLDI", "title": "Efficient discovery of regular stride patterns in irregular programs and its use in compiler prefetching", "url": "http://dl.acm.org/citation.cfm?id=512555"}