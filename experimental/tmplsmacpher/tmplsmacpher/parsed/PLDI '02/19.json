{"article_publication_date": "05-17-2002", "fulltext": "\n Static Load Classi.cation for Improving the Value Predictability of Data-Cache Misses * Martin Burtscher \nAmer Diwan Matthias Hauswirth Cornell University University of Colorado University of Colorado burtscher@csl.cornell.edu \ndiwan@cs.colorado.edu hauswirt@cs.colorado.edu ABSTRACT While caches are e.ective at avoiding most main-memory \naccesses, the few remaining memory references are still ex\u00adpensive. Even one cache miss per one hundred \naccesses can double a program s execution time. To better toler\u00adate the data-cache miss latency, architects \nhave proposed various speculation mechanisms, including load-value pre\u00addiction. A load-value predictor \nguesses the result of a load so that the dependent instructions can immediately proceed without having \nto wait for the memory access to complete. To use the prediction resources most e.ectively, specula\u00adtion \nshould be restricted to loads that are likely to miss in the cache and that are likely to be predicted \ncorrectly. Prior work has considered hardware-and pro.le-based methods to make these decisions. Our work \nfocuses on making these decisions at compile time. We show that a simple compiler classi.cation is e.ective \nat separating the loads that should be speculated from the loads that should not. We present results \nfor a number of C and Java programs and demon\u00adstrate that our results are consistent across programming \nlanguages and across program inputs.  Categories and Subject Descriptors D.3.4 [Programming Languages]: \nProcessors Compilers, Optimization; C.4 [Computer Systems Organization]: Performance of Systems General \nTerms Experimentation, Languages, Measurement, Performance  Keywords Load-value prediction, type-based \nanalysis * This work is supported by NSF ITR grant CCR-0085792, an NSF CAREER award, and an equipment \ngift from Intel. Any opinions, .ndings and conclusions or recommendations expressed in this material \nare the authors and do not nec\u00adessarily re.ect those of the sponsors. Permission to make digital or hard \ncopies of all or part of this work for personal or classroom use is granted without fee provided that \ncopies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. PLDI 02, June 17-19, 2002, Berlin, Germany. \nCopyright 2002 ACM 1-58113-463-0/02/0006 ...$5.00. 1. INTRODUCTION Caches address the widening gap between \nprocessor and memory speeds by satisfying most memory requests quickly. However, because of the high \ncost of main-memory refer\u00adences, even a few cache misses can signi.cantly degrade pro\u00adgram performance. \nFor this reason, prior work has proposed several speculation techniques, including load-value predic\u00adtion \n[20], for hiding the latency of cache misses. Load-value predictors guess the result of a load as soon \nas the load starts executing. Instructions that need the loaded value can pro\u00adceed using the guessed \nvalue without waiting for the memory access to complete (reduced latency) and are thus able to execute \nin parallel with the load (increased instruction-level parallelism). This paper focuses on the latency-reduction \naspect of load-value prediction. To make the most of speculation hardware for latency reduction, we should \nideally restrict its use to the loads that miss in the cache and that are predictable. Apply\u00ading speculation \nto loads that hit in the cache can destruc\u00adtively interfere with the speculation of loads that miss in \nthe cache. Moreover, if speculation is done in software, specu\u00adlating loads that hit in the cache unnecessarily \nincreases the code size. Applying speculation to loads that are unlikely to be predicted correctly will \nincur a misspeculation penalty and slow down program execution. The recent load-value prediction literature \nproposes com\u00adplex predictors that combine multiple basic predictors (hy\u00adbrids) [8, 25, 31] and incorporate \ncon.dence estimators to dynamically decide which loads are worth predicting and with which predictor \n[5, 7, 9, 20, 24]. The con.dence estima\u00adtors try to .lter out loads that would be mispredicted since \nmispredictions lower program performance. While modern load-value predictors are quite e.ective in simulations, \ntheir complexity degrades various performance parameters such as critical path length (i.e., cycle time), \nenergy consumption, heat dissipation, and chip area in a real hardware implemen\u00adtation. The goal of our \nresearch is to better understand how various static program characteristics relate to cache perfor\u00admance \nand value predictability, thus providing a foundation for making speculation decisions in compilers rather \nthan in hardware. Our approach is to statically partition all load instruc\u00adtions into 20 classes based \non factors such as the type of the loaded value (e.g., pointer), the region of memory the load references \n(e.g., heap), and the kind of the load (e.g., ar\u00adray reference) and to measure the cache behavior and \nvalue predictability of these classes. We conduct our experiments using three two-way set-associative \ncache sizes (16K, 64K, and 256K) and simulate .ve load-value predictors described in prior work: (i) \nlv, which predicts the last value for every load, (ii) l4v, which predicts one of the last four values \nfor every load, (iii) st2d, which uses strides to predict loads, (iv) fcm, which uses a representation \nof the context of pre\u00adceding loads to predict a load, and (v) dfcm, which enhances fcm with strides. \nWe simulate two sizes of load-value pre\u00addictors: realistic (2048 entries) and in.nite. Our experiments \nuse programs drawn from the SPECint95, SPECint00 and SPECjvm98, benchmark suites and demon\u00adstrate the \nfollowing. First, the same classes make up the majority of cache misses across our benchmark programs. \nSix classes (representing about half of all references) ac\u00adcount for most of the cache misses. Second, \nthe load-value predictability of classes is consistent, i.e., some classes are highly predictable across \nbenchmarks while others are highly unpredictable. We show how to exploit this class behavior to improve \nthe load-value predictability of loads that miss in the cache. Third, we demonstrate that predictors \nthat perform the best in prior work, fcm and dfcm, do not per\u00adform the best for cache misses. In other \nwords, fcm and dfcm are most e.ective on the loads that are unimportant for performance. Finally, we \ndemonstrate that our results are consistent across languages (C and Java) and across pro\u00adgram inputs. \nThe remainder of this paper is organized as follows. Sec\u00adtion 2 presents background information on load-value \npre\u00addiction. Section 3 describes our simulation framework and the benchmark programs. Section 4 presents \nour experimen\u00adtal results and discusses their implications for hardware and compilers. Section 5 reviews \nrelated work, and Section 6 concludes the paper. 2. BACKGROUND At the outset, load-value prediction \nseems like a hopelessly hard problem: after all, a 32-bit word can hold over four billion distinct values \nand a 64-bit word over 1019 values. Fortunately, load values tend to cluster, repeat, occur in sequences, \nexhibit patterns, and correlate with one another. Prior work has proposed di.erent predictors, each tailored \nto some kind of load-value locality. These predictors di.er in the kind of information they retain and \nin the computations they perform on this information to produce a prediction. The last value predictor \nlv [14, 20] simply predicts that a load instruction will load the same value that it did the previous \ntime it executed. lv can only predict sequences of repeating values (e.g., 3, 3, 3, 3, ...). Such sequences \nare sur\u00adprisingly frequent [14, 20]. All load instructions that load run-time constants such as starting \naddresses of data struc\u00adtures and .oating-point constants fall into this category. The stride 2-delta \npredictor st2d [27] remembers the last value for each load (like lv) but also maintains a stride, which \nis the di.erence between the last two loaded values. To make a prediction, st2d adds the stride to the \nlast value of the load. When a load .nishes, st2d updates the last value. st2d updates the stride only \nif it encounters the same stride twice in a row. Doing so eliminates the problem of making two consecutive \nmispredictions at every transition from one predictable sequence to another [5, 27]. Like lv, st2d can \nalso predict sequences of repeating val\u00adues; the stride is simply zero in this case. In addition, st2d \ncan predict sequences that exhibit genuine stride behavior (e.g., -4, -2, 0, 2, 4, ...), i.e., sequences \nwhere the stride is a non-zero constant. Such sequences are not very frequent [14, 27] because register \nallocation assigns most induction variables to registers, but they do occur, for example, when a program \nuses global variables as counters. The last four value predictor l4v [6, 19, 31] is similar to the last \nvalue predictor except that it retains the four most recently loaded values. At each load, l4v selects \nfrom its four possibilities the entry (not the value) that made the most recent correct prediction. Like \nthe st2d and the lv predictors, l4v can also predict repeating values. In addi\u00adtion, l4v can predict \nalternating values (e.g., -1, 0, -1, 0, -1, ...) or, more generally, any short repeating sequence that \nspans no more than four values (e.g., 1, 2, 3, 1, 2, 3, 1, ...). Such sequences are more frequent than \ntrue stride behavior [6, 19]. In particular, alternating sequences occur relatively often when variables \ntoggle between two values. The .nite context method predictor fcm [26, 27] computes a hash value out \nof the last four values of a load using a select-fold-shift-xor function [24, 25, 26] to index the pre\u00addictor \ns second level table. This table stores the values that follow every seen sequence of four values (modulo \nthe ta\u00adble size). Since the table is shared, load instructions can communicate information to one another \nin this predictor. Hence, after observing a sequence of load values, fcm can predict any load that loads \nthe same sequence. fcm can predict long sequences of arbitrary reoccurring values (e.g., 3, 7, 4, 9, \n2, ..., 3, 7, 4, 9, 2, ...). These se\u00adquences occur, for instance, during the repeated traversal of dynamic \ndata structures. Note that this predictor can also predict alternating sequences and sequences exhibiting \nstride behavior as long as the sequence repeats and its length does not exceed the table size. The di.erential \n.nite context method predictor dfcm [16] improves on fcm by retaining strides instead of absolute values. \nThis approach reduces the chance of detrimental aliasing in the second-level table, often increases the \npredic\u00adtor s capacity, and enables it to predict values it has never before seen. Thus, dfcm combines \nthe strengths of fcm and st2d at the cost of some additional complexity.  3. METHODOLOGY Our approach \nis to use compiler and binary instrumenta\u00adtion to generate a detailed trace. For each load, the trace \ngive the class of the load. The cache and load-value predic\u00adtor simulators consume these traces and for \neach load they determine if the predictor would have predicted the load correctly, update the state, \nand attribute the prediction or misprediction to the class of the load. The cache simulators determine \nif the load hits or misses in the cache and also at\u00adtributes the hit or miss to the class of the load. \nAt the end of the run, we output statistical information for each class, including their cache and load-value \npredictor behavior. Sections 3.1 and 3.2 present the classes and our classi\u00ad.cation technique, respectively. \nSection 3.3 describes how we use the class information in our simulations. Section 3.4 describes our \nbenchmark programs. 3.1 Classes We distinguish between two kinds of references: high-level references \nthat are visible at the source level and low-level references that are only visible in the assembly or \nsome other low-level representation of programs. We consider two kinds of low-level references for C \nprograms: loads of return ad\u00addresses (ra) and restores of callee-saved registers (cs), and one kind of \nlow-level load for Java programs: memory copies by the run-time system (mc). We consider three dimensions \nwhen classifying the high-level references: The region of memory the reference accesses: is the reference \nloading from a location in the stack, the heap, or the global space?  The kind of reference: is the \nreference loading an ob\u00adject .eld, an array element, or a scalar variable?  The type of the reference: \nis the reference loading a value of type pointer or non-pointer?  We picked these dimensions based on \nour intuition and the description of load-value predictors in prior work. More speci.cally, one of the \nstrengths of fcm and dfcm (given in prior work) is that they can successfully predict repeated traversals \nof linked data structures. Thus fcm and dfcm will probably be the most successful at predicting pointer\u00adtyped \nloads from object .elds. In subsequent work we are studying other classi.cations also, such as ones based \non simple program analyses. We use three-letter abbreviations for high-level references. The .rst letter \nstands for the region of memory (Stack, Heap, or Global). The second letter denotes the kind of ref\u00aderence \n(Array, Field, or Scalar). The third letter indicates the type of the reference (Pointer or Non-pointer). \nFor ex\u00adample, an hfp reference loads the value of a pointer-typed .eld in a heap-allocated object. 3.2 \nLoad Classi.cation Figure 1 shows our data-collection setup for C programs. We .rst translate the benchmark \nprograms to the SUIF v.1 representation [17]. Next, we add instrumentation to loads that are visible \nat the SUIF level (high-level loads). The in\u00adstrumentation communicates the type, kind, address, and \nvirtual program counter1 of each load to the VP library (Section 3.3). Then, we compile the instrumented \nprograms on an Alpha/OSF workstation and instrument the low-level loads in the resulting binary using \nATOM [29]. Finally, we link the instrumented binary with the VP library and run it to collect our data. \nThere are two sources of imprecision in our methodology for C programs. First, we assume that all references \n(ex\u00adcept references to local scalar variables whose address is not taken) result in loads. This is potentially \nimprecise since a compiler may be able to eliminate some references to non\u00adlocal or non-scalar variables \n[11] or may be unable to as\u00adsign some local scalar variables to registers. Since this work is primarily \nconcerned with understanding the behavior of variables and not so much with evaluating the performance \nimpact of a load-value predictor or cache, we feel that this simpli.cation is acceptable. Second, our \ninstrumentation of high-level loads may perturb later compiler optimizations. We have made careful choices \nin our instrumentation to mit\u00adigate this problem. For example, since passing parameters consumes registers \nand thus a.ects register allocation, we communicate all information between the instrumentation and the \nVP library using a set of scalar global variables. 1The program counter values are not available in SUIF \nso we sequentially number all the loads of the program and use that as the program counter for our simulations. \nThe organization for Java programs is similar to that for C programs except that we instrument using \nthe Jikes RVM [4] from IBM Research instead of SUIF and ATOM. We use the two-generational copying garbage \ncollector for Java. For our experiments, Java programs di.er from C programs in four ways. First, Java \nprograms have only scalar local variables, which are usually allocated in registers. Thus, the classes \nS are empty. Second, unlike in C programs, only objects and arrays are allocated in the heap. Hence, \nthe HSN and HSP classes are empty for Java programs. Third, there are no global arrays and global scalars \nin Java programs, and thus classes GS and GA are empty. Fourth, we do not yet have a convenient mechanism \nfor measuring low-level loads except for memory copies and therefore do not report data for classes RA \nand CS. 3.3 The VP Library For C programs, the VP library simulates the caches and load-value predictors \nand determines the region of mem\u00adory a load touches (i.e., stack, heap, or global space) by examining \nthe address of the load. While we can easily determine an approximation to the region of loads in the \ncompiler [10], we opted to use a precise run-time classi.ca\u00adtion in order to avoid polluting our data \nwith artifacts of an imperfect points-to analysis. Our experience indicates that the region of most loads \nstays constant across executions of the load and thus a compile-time analysis should be e.ec\u00adtive at \ndetermining the region of loads. For Java programs, the bytecode (such as aload) directly tells us the \nregion of memory a load accesses. We simulate caches with a write-no-allocate policy, two\u00adway associativity, \n64-bit word size and 32-byte block size. We consider cache sizes of 16K, 64K, and 256K. We picked these \nsizes because they are representative of L1 data caches for modern processors (e.g., 64K in the Alpha \n21264, the Athlon XP, and the UltraSPARC III, and 16K in the Pen\u00adtium III). We simulate value predictors \nof two sizes. (i) The 2048-entry predictors have 2048 entries in their tables. fcm and dfcm have 2048 \nentries in both the .rst and second\u00adlevel tables. (ii) The in.nite predictors have a su.ciently large \nsize to eliminate any con.icts.  3.4 Benchmarks For C programs, we use programs from the SPECint95 [3] \nand SPECint00 [1] integer benchmark suites compiled on an Alpha/OSF workstation for our measurements \n(Table 1). For Java programs, we use programs from the SPECjvm98 [2] suite compiled on a PowerPC running \nLinux.2 These programs are well understood, non-synthetic, and compute\u00adintensive. Table 2 shows the percentages \nof the measured loads that fall into each of the 20 classes using the reference inputs for the SPECint95 \nprograms and the train inputs for the SPECint00 programs. If a particular class makes up 2% or more of \nthe total references in a program, we highlight it in bold. Note that all but four of the twenty classes \nmake up at least 2% of the total loads in at least one benchmark program. Also, loads in some classes, \nfor example GSN and CS, occur frequently in the majority of the programs. Table 3 presents similar data \nfor the Java benchmarks except that 2Unfortunately, since parts of our infrastructure are plat\u00adform speci.c, \nwe could not run the C and Java programs on the same architecture.  Figure 1: Experimental setup Program \nname Source Description compress SPECint95 Compresses and decompresses a .le in memory gcc SPECint95 \nC compiler that builds SPARC code go SPECint95 Plays the game of GO ijpeg SPECint95 Compression and decompression \nof graphics li SPECint95 Lisp interpreter m88ksim SPECint95 Motorola 88000 chip simulator, runs a test \nprogram perl SPECint95 Manipulates strings (anagrams) and prime numbers in Perl vortex SPECint95 An object \noriented database program bzip2 SPECint00 Compression of an image gzip SPECint00 Compression utility \nusing LZ77 mcf SPECint00 Combinatorial optimizations compress SPECjvm98 Utility to compress/uncompress \nlarge .les based on Lempel-Ziv method jess SPECjvm98 Java expert system shell based on NASA s CLIPS expert \nsystem raytrace SPECjvm98 Single-threaded raytracer db SPECjvm98 Small data-management program on memory-resident \ndatabases javac SPECjvm98 The JDK 1.0.2 Java compiler mpegaudio SPECjvm98 MPEG-3 audio stream decoder \nmtrt SPECjvm98 Multi-threaded raytracer (calls raytrace) jack SPECjvm98 Parser generator with lexical \nanalysis, early version of JavaCC Table 1: Benchmark programs some of the classes do not exist (Section \n3.2). We use input size10 for Java benchmarks.  4. RESULTS Section 4.1 presents our results for C programs \nand Sec\u00adtion 4.2 for Java programs. Section 4.3 validates our ob\u00adservations by comparing to results from \na di.erent set of program inputs. When presenting results we omit data for benchmark/class combinations \nif the class comprises less than 2% of the references in the benchmark program. 4.1 Results for C Programs \nSection 4.1.1 examines the behavior of our classes with respect to data-cache performance. Section 4.1.2 \npresents results for load-value predictor performance. Section 4.1.3 combines the cache and value-prediction \nresults to explore how value predictors perform on cache misses. 4.1.1 Cache Performance Since cache \nmisses bene.t the most from latency tolerance techniques, we start by examining the cache performance \nof our classes. Figure 2 shows the average percentage of to\u00adtal cache misses incurred by each class. \nThe error bars present the highest and lowest percentage of cache misses for each class. We picked this \nmetric instead of the more tradi\u00adtional cache miss or hit rates because it emphasizes classes that contribute \nmost to cache misses rather than classes that contribute few misses but have high miss rates. Later on \nwe also present cache hit rates. For each class we only consider those benchmarks in which the class \nmakes up at least 2% of the references. Thus, the sum of the bars of a given cache size often adds up \nto more than 100%. To see how this happens, imagine that there are two programs and two classes of loads \nsuch that each class occurs in only one program and for that program makes up 100% of the cache misses. \nThen, both classes will have their bars at 100%, and the sum of the two bars will add up to more than \n100% (namely 200%). Figure 2 has three bars for each class for cache sizes of 16K, 64K, and 256K. For \nexample, looking at the 16K GAN bar we see that GAN loads account (on average) for 43% of all the cache \nmisses in programs that have a non-trivial number of GAN loads. We also see that the range for GAN reaches \nfrom contributing nearly 0% to almost 100% of the cache misses, depending on the program. The numbers \nnext to the class names along the horizontal axis give the number of programs for which that class makes \nup at least 2% of the total references. To put these numbers in perspective, Table 4 gives the data-cache \nmiss rates for our benchmark programs. From Figure 2 we see that the classes have fairly consistent cache \nbehavior across benchmark programs. In particular, the vast majority of cache misses are in six classes: \nGAN, HSN, HFN, HAN, HFP, and HAP. The other classes (e.g., the low-level classes) contribute little to \nthe number of cache misses. Table 5 gives the percentage of cache misses that come from the six classes \nmentioned above. Figure 3 presents the cache hit rates for each class using cache sizes of 16K, 64K, \nand 256K. The error bars present the range of hit rates for a class. The y-axis starts at 40% for improved \nreadability. We present cache hit rates rather than miss rates to make the graph easily comparable to \nFigure 4. From Figure 3 we see that the classes that account for the Benchmark 16K 64K 256K compress \n8.5 6.2 3.3 gcc 3.0 1.1 0.3 go 5.0 1.1 0.0 ijpeg 1.5 0.6 0.4 li 3.1 2.5 1.4 m88ksim 0.2 0.0 0.0 perl \n0.9 0.0 0.0 vortex 1.6 0.7 0.3 bzip 2.0 1.9 1.6 gzip 5.8 2.6 0.1 mcf 27.2 25.1 21.5 Table 4: Load miss \nrates for data caches Figure 3: Cache hit rates for all loads (average over all benchmarks, minimum, \nand maximum) Benchmark 16K 64K 256K compress 98 98 97 gcc 78 83 85 go 86 88 94 ijpeg 95 98 98 li 69 74 \n77 m88ksim 41 77 100 perl 50 96 96 vortex 86 96 99 bzip 100 100 100 gzip 96 96 89 mcf 68 68 67 Table \n5: Percentage of cache misses that come from classes GAN, HSN, HFN, HAN, HFP, and HAP Class compress \ngcc go ijpeg li m88ksim perl vortex bzip gzip mcf mean SSN 0 1.28 3.50 0.42 4.40 12.10 6.23 7.26 0.12 \n0.15 0.15 2.97 SAN 0 0.63 1.01 16.61 0 0.45 2.58 0.00 12.73 0.01 0 2.84 SFN 0 0.67 0 3.62 0.00 0.30 0 \n2.60 0 0 0 0.60 SSP 0 0.37 0 0.17 1.40 0.00 0.00 0.33 0 0.02 0 0.19 SAP 0 0.25 0 0.17 0 0 0 0 0 0.00 \n0 0.04 SFP 0 0.29 0 0.25 0.01 0.24 2.15 0.05 0 0 0 0.25 HSN 0 0.88 0 14.75 3.51 0.00 8.07 7.32 0.27 0.01 \n0.20 2.92 HAN 0 7.39 0 48.55 0.00 0.00 4.30 5.39 31.83 0.00 2.75 8.35 HFN 0 16.37 0 0.76 8.80 6.11 8.42 \n0.85 0 3.54 27.35 6.02 HSP 0 0.33 0 0.00 1.82 0.00 20.01 7.64 0 0 0 2.48 HAP 0 9.42 0 1.33 0.56 0 3.02 \n4.97 0 0 0.88 1.68 HFP 0 1.82 0 0.11 24.44 0.57 6.29 0.16 0 0.01 17.47 4.24 GSN 43.46 11.10 14.23 0.45 \n12.76 17.49 16.81 27.79 43.71 43.75 3.12 19.56 GAN 19.27 6.51 52.03 3.00 0.00 21.86 0.00 0.03 3.63 26.24 \n0 11.05 GFN 0 0.81 0 0.41 0.00 10.96 0.00 0.16 0 0.00 2.79 1.26 GSP 0 0.68 0 0.04 0.00 0.00 0.00 0.00 \n0 0 0.48 0.10 GAP 0 2.17 0.00 0.00 0.00 0.86 0.00 0.60 0.41 0.00 4.72 0.73 GFP 0 0.77 0 0.20 0.00 0.07 \n0.00 0.00 0 0.00 0.26 0.11 RA 7.65 5.16 3.68 0.91 8.84 4.58 4.11 4.60 0.76 2.52 7.29 4.17 CS 29.62 33.10 \n25.55 8.27 33.46 24.40 18.01 30.24 6.54 23.75 32.55 22.12 Table 2: Dynamic distribution of total references \nin C benchmarks runs (ref inputs for SPECint95, train inputs for SPECint00) Class compress jess raytrace \ndb javac mpegaudio mtrt jack mean GFN 0.14 3.20 0.87 1.73 14.43 0.39 0.36 3.65 3.10 GFP 1.53 0.76 0.40 \n0.42 1.57 2.00 0.42 0.82 0.99 HAN 14.68 2.36 3.38 15.66 11.28 32.42 4.49 2.43 10.84 HAP 0.07 18.01 13.38 \n9.69 1.88 11.36 11.68 11.37 9.68 HFN 49.01 57.90 54.51 48.65 48.30 47.07 54.05 65.08 53.07 HFP 34.25 \n17.63 27.27 23.37 15.56 6.74 28.69 15.23 21.09 MC 0.31 0.13 0.19 0.46 6.97 0.02 0.29 1.42 1.23 Table \n3: Dynamic distribution of total references in Java benchmarks runs (size 10 inputs for SPECjvm98) Figure \n2: Contribution to cache misses by class (eleven programs) vast majority of the loads (e.g., HFN) have \nlow cache hit rates compared to the other classes. The above results indicate that we can use readily-available \ncompiler information to focus the mechanism for tolerating load latency on only a few of the classes. \nThe six classes that contribute the most to the cache misses make up be\u00adtween 38% and 73% of the loads \nexecuted in the benchmarks (arithmetic mean 55%). Moreover, in a 64K cache, these classes account for \n68% to 100% of the cache misses (arith\u00admetic mean 89%). In other words, it su.ces to use mecha\u00adnisms \nsuch as load-value prediction on only about half the loads, thus reducing con.icts in the predictor s \ntables. From the bars for the three cache sizes in Figure 2, we see that as the cache size increases \nthe contribution of a particular class may decrease (e.g., GAN) or increase (e.g., HAN). This happens \nbecause increasing the cache size will not necessarily remove the same percentage of misses from all \nclasses. It is not surprising that the heap classes have poor cache behavior, though to our knowledge, \nno one has demonstrated this empirically. Class GAN performs poorly in the cache because global arrays \noften hold hash tables that are ac\u00adcessed throughout the lifetime of the program.  4.1.2 Class Predictability \nTables 6 (a) and (b) show which predictors performs best for each class. The Class column gives the name \nof the class. The number in parentheses is the number of programs (out of a total of eleven programs) \nfor which this class makes up at least 2% of the references. The tables omit classes that make up less \nthan 2% of the references in all the benchmarks. The lv, l4v, st2d, fcm, and dfcm columns give the number \nof benchmarks for which the predictor is predictability-wise within 5% of the best predictor for the \nclass. An empty en\u00adtry means that the corresponding predictor does not perform within 5% of the best \npredictor for any benchmark program. A bold entry indicates that the corresponding predictor is one of \nthe most consistent predictors for the class. For ex\u00adample, from Table 6 (a) we see that SSN makes up \nat least 2% of the references in .ve benchmark programs and dfcm performs the best (or within 5% of the \nbest) in these pro\u00adgrams. Table 6 (a) presents data for 2048-entry predictors and Table 6 (b) for in.nite-size \npredictors. Table 6 (b) shows that dfcm is the best predictor if the predictor size is unlimited. For \nrealistic predictor sizes, dfcm (and to a lesser extent fcm) signi.cantly outperforms the other predictors \nparticularly for pointer loads as well as non-pointer loads from the stack. Since register allocators \neliminate the easily predictable loads from the stack (such as references to induction variables), it \nis not surprising that the simpler predictors (lv, st2d, and l4v) perform poorly for non-pointer loads \nfrom the stack. Since register alloca\u00adtors are less e.ective at eliminating loads from the heap and global \nspace, enough simple load-value locality remains for the simpler predictors to perform well. For classes \nHAN, GSN, GFN, RA, and CS, the simpler predictors (particularly l4v and st2d) are comparable or sometimes \neven better than the more complex predictors (dfcm and fcm). Even when l4v and st2d perform as well but \nno better than fcm they may be preferable because they require much simpler and smaller hardware. For \nclass RA, l4v is the most consistent realistic predictor (Table 6(a)). Since RA represents the loads \nof return PC values, it is not Class lv l4v st2d fcm dfcm SSN (5) 1 2 2 4 5 SAN (3) 1 1 1 2 SFN (2) 1 \n2 2 SFP (1) 1 HSN (4) 1 2 1 3 4 HAN (6) 2 2 4 4 5 HFN (6) 2 3 2 4 6 HSP (2) 1 1 1 2 2 HAP (3) 1 2 2 HFP \n(3) 1 2 3 GSN (10) 2 2 8 2 7 GAN (7) 3 3 4 5 5 GFN (2) 1 1 1 1 1 GAP (2) 1 2 2 RA (9) 5 8 5 4 4 CS (11) \n2 3 7 1 9 (a) 2048 (b) in.nite  Class lv l4v st2d fcm dfcm SSN (5) 1 1 1 5 5 SAN (3) 1 3 SFN (2) 1 \n1 2 SFP (1) 1 HSN (4) 2 4 HAN (6) 1 5 6 HFN (6) 5 6 HSP (2) 1 1 1 2 2 HAP (3) 1 2 3 HFP (3) 3 3 GSN (10) \n1 1 4 6 10 GAN (7) 1 1 1 6 6 GFN (2) 1 1 1 2 2 GAP (2) 2 2 RA (9) 2 4 2 8 9 CS (11) 2 7 11 Table 6: \nBest predictor for predictor sizes 2048 and in.nite. Inputs: ref for SPECint95 and train for SPECint00 \n(eleven programs) Class Number of benchmarks SSN (5) 4 SAN (3) 1 SFN (2) 1 SFP (1) 1 HSN (4) 2 HAN (6) \n3 HFN (6) 4 HSP (2) 2 HAP (3) 2 HFP (3) 2 GSN (10) 9 GAN (7) 2 GFN (2) 1 GAP (2) 0 RA (9) 6 CS (11) 7 \n Table 7: Number of benchmarks for which the best 2048-entry predictor for the class predicts more than \n60% of the loads (eleven programs) surprising that RA loads take on one of the last few values of the \nload. If a procedure is always called from the same site, then even lv or st2d will be e.ective at predicting \nRA loads (and indeed for some of the programs, st2d and lv work quite well). dfcm and st2d are the best \npredictor for CS (Table 6 (a)). Note, however, that fcm performs relatively poorly on CS (it is the best \npredictor for CS for only one benchmark program). CS is an unusual class because members of this class \nhave mixed types. For example, imagine a procedure that uses register r1 and is called from two call \nsites. When it is called from the .rst call site r1 may contain a pointer and when it is called from \nthe second call site r1 may con\u00adtain an integer. The integer may be an induction variable and thus favor \nst2d since compilers eagerly allocate induc\u00adtion variables to registers. The pointer, on the other hand, \nwill most likely favor fcm or dfcm. Thus, dfcm performs well on CS because it can predict both strides \nand repeated traversals through linked data structures. Figure 4 presents the average percentage of correct \npre\u00addictions for each class using the .ve load-value predictors. Taller bars in Figure 4 are better. \nFor some classes, such as SSP, there are no bars since these classes make up less than 2% of the total \nreferences in all benchmark programs. Table 7 presents for each class the number of benchmarks where \nthe best value predictor for that class can correctly predict at least 60% of the references in that \nclass. Figure 4 and Table 7 together suggest that some classes are more pre\u00addictable than others. For \nexample, GAN appears in seven benchmark programs, but in only two of them is a predictor able to predict \nmore than 60% of the GAN references. On the other hand, GSN appears in ten benchmark programs and in \nnine of them a predictor is able to correctly predict more than 60% of the GSN references. Comparing \nFigures 4 and 3 we see that classes that suf\u00adfer from low hit rates in caches (e.g., classes HFN, HFP, \nand GAN) also often su.er from low predictability in value predictors. To summarize, we observe that \nsome classes are much more predictable than others. Moreover, for the predictable classes, there is usually \na realistic predictor that performs best for most of the benchmarks. Given this consistency of predictor \nperformance, it should be possible to build an ef\u00adfective hybrid predictor that uses static instead of \ndynamic predictor selection. After all, the classes for which the sim\u00adpler predictors perform almost \nas well or even better than fcm and dfcm represent over a quarter of all loads. We further note that \nclasses that perform poorly for caches also perform poorly for load-value prediction.  4.1.3 Improved \nLatency Tolerance Through Compiler Analysis We now consider how to use our results from Section 4.1.1 \nand 4.1.2 to improve the performance of programs. The full bene.t will be greater once we consider more \nuses of the results, such as for prefetching. Figure 5 gives the performance of load-value prediction \non loads that miss in a 64K cache. A predictor with a taller bar performs better than one with a shorter \nbar. We use the 2048-entry con.gurations for all load-value predictors. To speed up simulations, we ignored \nthe low-level loads in these experiments since they rarely miss in the cache. Fig\u00adure 5 presents results \nonly for the classes that cause the Figure 5: Prediction rates for loads missing in the cache (average \nover all benchmarks, minimum, and maximum) Figure 6: Prediction rates for loads missing in cache and \ndesignated by compiler to be predicted (average over all benchmarks, minimum and maximum) majority of \nthe cache misses. The error bars give the min\u00adimum and maximum correct predictions across our bench\u00admark \nprograms. On inspecting the data we noted that the low and high points of the predictors matched up. \nFor ex\u00adample, when one predictor performed its worst for a class in a benchmark, it was usually the case \nthat the other pre\u00addictors also performed their worst for that same class and benchmark. Figure 5 shows \nthat fcm and dfcm perform about the same or slightly worse than the simpler predictors on the loads that \nmiss in the cache. This is surprising because in Section 4.1.2 we saw that dfcm is one of the strongest \npredictors when we consider all loads. For example, fcm and dfcm perform much better than the other predictors \non class HAP (Figure 4), but when we consider only cache misses, the simpler predictors perform slightly \nbetter than fcm and dfcm. In other words, fcm and dfcm, despite their relative complexity, are outperformed \nby the simpler predictors on the loads that matter the most. One explanation for the relatively poor \nperformance of  Figure 4: Prediction rates for all loads (average over all eleven benchmarks, minimum, \nand maximum) fcm and dfcm is that their tables are not large enough. To determine if this is the case, \nwe increased the size of the fcm and dfcm from 2048 entries to practically in.nite tables. With in.nite \ntables, dfcm and fcm perform better than the simpler predictors. In other words, if it is possible to \nbuild a really large predictor, fcm and dfcm may be preferable to the simpler predictors. Otherwise, \nthe simpler predictors perform just as well (and sometimes even better) than fcm and dfcm. It is also \nworth keeping in mind that a 2048-entry fcm or dfcm will be larger and more complex than a 2048-entry \nst2d. Figure 6 is similar to Figure 5 except that we use compiler information to predict only loads in \nthe classes that account for most of the misses in the cache. Comparing Figure 6 to Figure 5, we see \nthat there is a modest bene.t to .ltering loads, i.e., preventing them from accessing the predictor. \nFor example, lv correctly predicts up to 3% more cache misses if only classes HAN, HFN, HAP, HFP, and \nGAN access the predictor. Reducing predictor accesses eliminates con.icts and thus allows predictors \nto be more e.ective on the remaining accesses. To understand how the cache size a.ects our results, we \nrepeated the above experiments with a 256K cache instead of a 64K cache. A 256K cache should have fewer \nmisses than a 64K cache and thus use the load-value predictor for fewer loads. Interestingly, we found \nthat the relative performance of the predictors did not change. However, the percentage of correct predictions \nfor the predictors improved by several percent over Figure 6. The above results use .ltering based on \nwhich loads are important with respect to the cache. However, another kind of .ltering is also possible: \n.ltering out loads that are poorly predicted with value predictors because they provide little potential \n(and possibly signi.cant harm from the mispredic\u00adtion penalties) for speculation. When we stopped predicting \nclass GAN because it is by far the least predictable of the classes in Figure 6, our results improved: \nmost of the pre\u00addictors performed better (by up to 7%) than in Figure 6 because there were fewer con.icts \nin the predictors tables.  4.2 Results for Java programs In Section 4.1 we demonstrated for C programs \nthat our load classes are useful in separating loads that frequently hit in the cache from loads that \nfrequently miss in the cache. We cannot, however, meaningfully use the same classi.cation for Java programs \nfor two reasons. First, the vast majority of loads in Java programs are from the heap3 (Table 3) and \nthus the distinction between heap and non-heap loads is not interesting. Second, our current framework \nfor Java pro\u00adgrams misses loads in some low-level classes, which causes the heap loads to dominate the \nbehavior even more. For these reasons, we only report a partial set of results for Java programs. When \nwe consider the value predictability of all loads, the relative performance of the predictors is similar \nto that of C programs. dfcm usually has the best predictability and fcm the second-best predictability. \nHowever the di.erence 3 Our traces do not contain all the low-level loads in Java programs. The balance \nbetween heap and non-heap loads may be di.erent in a full memory trace. between the context-based predictors \n(dfcm and fcm) and the other predictors (e.g., lv) are not as dramatic as with C programs. The only class \nfor which fcm and dfcm are much better is HAP, where dfcm predicts 80% of the loads correctly, fcm predicts \n60% of the loads correctly, and the other predictors are at least 10% worse than fcm. When we consider \nthe value predictability of loads that miss in the cache, we see a similar behavior to C programs: dfcm \nand fcm o.er little bene.t over the simpler predictors and for all classes except HAP and HFP, one of \nthe simpler predictors outperforms both dfcm and fcm. We also conducted an experiment using a di.erent \nin\u00adfrastructure that provides a trace of all loads for Java pro\u00adgrams (including loads that are missing \nin the above re\u00adsults). By instrumenting at the very end of optimizing compilation, even after register \nallocation, we are able to trace all loads including loads belonging to the classes CS and RA. However, \nat this late phase of compilation, we do not have enough information to reliably partition loads into \nclasses and thus, we report only overall performance. Our results using these traces are consistent with \nour other re\u00adsults. In particular, we found that when we consider only cache misses, the simpler predictors \nare close in performance to dfcm and fcm. More speci.cally, the simpler predic\u00adtors perform much better \n(by at least 10%) than dfcm and fcm for one benchmark (mpegaudio) and slightly better (by less than 5%) \nfor one benchmark (compress). dfcm or fcm perform much better than the simpler predictors for two benchmarks \n(db and mtrt) and only slightly better for the remaining four benchmarks. 4.3 Validation To ensure wide \napplicability of our results, we conducted our experiments on a variety of programs written in C and \nJava. As noted above, our results are consistent across the two programming languages; for example dfcm \nis the best predictor for pointer loads in both Java and C programs. We also repeated many of our experiments \nwith C pro\u00adgrams using another set of inputs and computed tables sim\u00adilar to Table 6. We found that while \nthe absolute numbers di.ered, our main conclusions were the same: a predictor that performs well (poorly) \nwith one set of inputs also per\u00adforms well (poorly) with a di.erent set of inputs.  5. RELATED WORK \nWe .rst describe related work in load-value prediction and then related work in cache performance. 5.1 \nLoad-Value Prediction Two independent research e.orts [14, 20] .rst recognized that load instructions \nexhibit value locality and concluded that there is potential for prediction. Lipasti et al. [20] investigated \nwhy load values are often predictable and studied the predictability of di.erent kinds of load instructions. \nThey found that while all loads exhibit signi.cant value predictability, address loads have slightly \nbetter value locality than data loads, instruction address loads hold an edge over data address loads, \nand integer data values are more predictable than .oating-point data values. Our approach separates loads \ninto many more classes that exhibit greatly varying predictability behavior. Gabbay and Mendelson [15] \nexplore the possibility of us\u00ading program pro.les to enhance the e.ciency of value pre\u00addiction. They \nuse pro.ling to insert opcode directives to .lter out highly unpredictable values from being allocated \nin the load-value predictor, which considerably reduces the amount of aliasing. Our approach of .ltering \nloads based on how important and predictable they are achieves the same goal without the need for pro.ling. \nFurthermore, Gabbay and Mendelson found that training runs generally correlate with test runs, indicating \nthat a program s input values do not signi.cantly a.ect the value locality. A more detailed study about \npredictability by Sazeides and Smith [28] illus\u00adtrates that most of the locality originates in the program \ncontrol structure and immediate values, which explains the observed independence of program input. Our \nmethodology can also be used with pro.les. Pro.ling may, however, result in insu.cient data to classify \nloads that are never or hardly ever executed during the pro.le run. Our static approach does not su.er \nfrom this problem. Rychlik et al. [25] address the problem of useless predic\u00adtions. They introduce a \nsimple hardware mechanism that inhibits predictions that were never used (because the true load value \nbecame available before the predicted value was consumed) from updating the predictor, which results \nin improved performance due to reduced predictor pollution. This .ltering is complementary to the .ltering \nwith our compiler-based approach. Fu et al. [13] propose a mixed hardware-and software\u00adbased approach \nto value speculation that leverages advan\u00adtages of both hardware schemes for value prediction and compiler \nschemes for exposing instruction-level parallelism. They propose adding new instructions to explicitly \nload val\u00adues from the predictor and to update the predictor. Our ap\u00adproach does not require any new instructions \nand is geared towards hybrid predictors. While the approach of Fu et al. supports hybrids, they defer \nthe problem of which com\u00adponent to select to the hardware. Static classi.cation, as presented in this \npaper, can be used to accomplish this in software. Calder et al. [9] examine selection techniques to \nminimize predictor capacity con.icts by prohibiting unimportant in\u00adstructions from using the predictor. \nAt the same time, they classify instructions depending on their latency so that the con.dence threshold \ncan be adapted to the potential gain of predicting a given instruction. Hence, operations with small \ngains are only predicted if the predictor s con.dence is very high, whereas operations with potentially \nlarge gains are predicted even if the con.dence is rather low. Interest\u00adingly, they found that loads \nare responsible for most of the latency in the critical path and hence predicting only loads represents \na good .ltering criterion. We implicitly use this criterion because we only predict load values. Note \nthat Calder et al. classify loads by latency while we classify by type of memory access. Morancho et \nal. [21] propose separating the con.dence estimator from the predictor so that only the con.dence es\u00adtimator \nhas to be large enough to handle all load instruc\u00adtions, whereas the predictor itself can be designed \nsmaller because it only has to hold the predictable loads. By per\u00adforming con.dence pre-estimation at \ncompile time, our ap\u00adproach allows both the value predictor and the con.dence estimator to be designed \nsmaller because they only have to be large enough to hold the predictable loads. The literature describes \nseveral hybrid value predictors. For example, Wang and Franklin propose a hybrid between a last-distinct-four-value \nand a stride predictor [31], Rychlik et al. use a .nite context method and stride 2-delta hybrid [25], \nand Burtscher and Zorn propose a three-component hybrid that includes a register .le, a stride, and a \nlast three value component [8]. The data in this paper suggests that the best predictor for a load can \noften be picked at compile time rather than at run time in hardware. 5.2 Cache Performance While there \nhas been signi.cant prior work in understand\u00ading the cache behavior of programs, we are not aware of \nany study that correlates cache behavior to high-level proper\u00adties such as types. Some prior work tries \nto understand and improve the cache behavior of heap loads by measuring the cache impact of garbage collection \n[12, 18, 23, 30, 32, 33]. Mowry and Luk [22] also attempt to improve the e.ective\u00adness of latency-tolerance \ntechniques by applying them only to cache misses. They identify instructions that are likely to miss \nin the cache using correlation pro.ling, which, for in\u00adstance, predicts whether a load will hit or miss \nin the cache based on whether previous loads hit or miss in the cache. Rather than using pro.ling (particularly \non-line pro.ling), our approach uses static properties to predict whether or not a load will miss. The \nadvantage of Mowry and Luk s ap\u00adproach when compared to ours is that it can adapt dynami\u00adcally if a load \ns behavior changes during a run. They present numbers in their paper only for the loads that they use \ncor\u00adrelation pro.ling on (the top 15 loads in terms of misses) and thus we cannot directly compare our \naccuracy to theirs. However, the disadvantage of their approach is that it incurs run-time overhead and \nrequires additional hardware to col\u00adlect and act on the correlation pro.les. Our approach has no run-time \noverhead besides the cost of incorrect predictions. We believe that the two techniques are complementary \nand it would be interesting to compare and combine the two.  6. CONCLUSIONS We show that a compiler \ncan divide a program s load instructions into classes that exhibit consistent cache and value-predictability \nbehavior. For example, most (arith\u00admetic mean 89%) of the cache misses stem from six classes that represent \nan average of 55% of the loads in our bench\u00admark programs. Moreover, our classes are largely consistent \nwith respect to load-value predictability: the best predictor for each class seems to be independent \nof the program. Interestingly, we found that load-value predictors behave quite di.erently on loads that \nmiss in the cache than on loads that hit. In particular, predictors such as the fcm and dfcm, which are \nbelieved to be the best predictors in the literature, actually perform well only on loads that hit in \nthe cache. For loads that miss, these more complex predictors are no better than the much simpler ones. \nIn other words, for the loads that need speculation the most, the simpler, smaller, and faster predictors \nperform as well as the more complex predictors. We use our results to determine at compile time which \nloads to speculate. More speci.cally, we only speculate loads from classes that miss frequently in the \ncache and that are predictable. This approach improves the predictor accuracy by up to 8% on the loads \nthat cause cache misses, i.e., on the loads that matter the most. 7. REFERENCES [1] SPECcpu2000 benchmarks. \nhttp://www.spec.org/osg/cpu2000/CINT2000. [2] SPECjvm98 benchmarks. http://www.spec.org/osg/jvm98/. \n [3] In SPECcpu95, 1995. [4] M. Burke, J.-D. Choi, S. Fink, D. Grove, M. Hind, V. Sarkar, M. Serrano, \nV. C. Sreedhar, and H. Srinivasan. The jalapeno dynamic optimizing compiler for Java. In ACM Java Grande \nConference, San Francisco, CA, June 1999. [5] M. Burtscher. Improving Context-Based Load Value Prediction. \nPhD thesis, University of Colorado, Boulder, 2000. [6] M. Burtscher and B. G. Zorn. Exploring Last n \nValue Prediction. In International Conference on Parallel Architectures and Compilation Techniques, pages \n66 76, 1999. [7] M. Burtscher and B. G. Zorn. Prediction Outcome History-based Con.dence Estimation for \nLoad Value Prediction. Journal of Instruction-Level Parallelism, 1999. [8] M. Burtscher and B. G. Zorn. \nHybridizing and Coalescing Load Value Predictors. In International Conference on Computer Design, pages \n81 92, 2000. [9] B. Calder, G. Reinman, and D. M. Tullsen. Selective value prediction. In Proceedings \nof the 26th annual international symposium on computer architecture, pages 64 74. ACM, 1999. [10] S. \nCho, P. Yew, and G. Lee. A high-bandwidth memory pipeline for wide issue processors. IEEE Transactions \non Computers, 50(7):709 723, July 2001. [11] A. Diwan, K. S. McKinley, and J. E. B. Moss. Type-based \nalias analysis. In ACM conference on programming language design and implementation, pages 106 117, Montr\u00b4eal, \nCanada, May 1998. [12] A. Diwan, D. Tarditi, and E. Moss. Memory subsystem performance of programs with \nintensive heap allocation. ACM Transactions on Computer Systems, 1995. [13] C. Y. Fu, M. D. Jennings, \nS. Y. Larin, and T. M. Conte. Value speculation scheduling for high performance processors. In Eighth \nInternational Conference on Architectural Support for Programming Languages and Operating Systems, 1998. \n[14] F. Gabbay. Speculative execution based on value prediction. Technical Report 1080, Department of \nElectrical Engineering, Technion Israel Institute of Technology, 1996. [15] F. Gabbay and A. Mendelson. \nCan program pro.ling support value prediction? In 30th Annual ACM/IEEE International Symposium on Microarchitecture, \n1997. [16] B. Goeman, H. V. Dierendonck, and K. DeBosschere. Di.erential fcm: Increasing value prediction \naccuracy by improving table usage e.ciency. In HPCA-7, Jan. 2001. [17] S. U. S. R. Group. Suif compiler \nsystem version 1.x. suif.stanford.edu/suif/suif1/index.html. [18] P. J. Koopman, Jr., P. Lee, and D. \nP. Siewiorek. Cache behavior of combinator graph reduction. Transactions on Programming Languages and \nSystems, 14(2):265 277, Apr. 1992. [19] M. H. Lipasti and J. P. Shen. Exceeding the data.ow limit via \nvalue prediction. In Proceedings of the 19th IEEE/ACM international symposium on microarchitecture, pages \n226 237, 1996. [20] M. H. Lipasti, C. B. Wilkerson, and J. P. Shen. Value Locality and Load Value Prediction. \nIn Proceedings of the second international conference on architectural support for programming languages \nand operating systems, pages 138 147, 1996. [21] E. Morancho, J. M. Llaberia, and A. Olive. Split last-address \npredictor. In 1998 International Conference on Parallel Architectures and Compilation Techniques, 1998. \n[22] T. C. Mowry and C.-K. Luk. Predicting data cache misses in non-numeric applications through correlation \npro.ling. In International Symposium on Microarchitecture. [23] M. B. Reinhold. Cache Performance of \nGarbage-Collected Programming Languages. PhD thesis, Laboratory for Computer Science, MIT, Sept. 1993. \n[24] G. Reinman and B. Calder. Predictive techniques for aggressive load speculation. In Proceedings \nof the 31st IEEE/ACM international symposium on microarchitecture, pages 127 137, 1998. [25] B. Rychlik, \nJ. Faistl, B. Krug, and J. P. Shen. E.cacy and performance impact of value prediction. In Proceedings \nof the international conference on parallel architectures and compilation techniques, 1998. [26] Y. Sazeides \nand J. E. Smith. Implementations of context based value predictors. Technical Report ECE-97-8, University \nof Wisconsin, Madison, Wisconsin, 1997. [27] Y. Sazeides and J. E. Smith. The predictability of data \nvalues. In Proceedings of the thirteenth IEEE/ACM international symposium on microarchitecture, pages \n248 258, 1997. [28] Y. Sazeides and J. E. Smith. Modeling program predictability. In 25th International \nSymposium on Computer Architecture, 1998. [29] A. Srivastava and A. Eustace. ATOM: A system for building \ncustomized program analysis tools. In ACM conference on programming language design and implementation, \npages 196 205, Orlando, FL, June 1994. [30] D. Tarditi and A. Diwan. Measuring the cost of memory management. \nLisp and Symbolic Computation, 1996. [31] K. Wang and M. Franklin. Highly accurate data value prediction \nusing hybrid predictors. In 30th Annual ACM/IEEE International Symposium on Microarchitecture, 1997. \n[32] P. R. Wilson, M. S. Lam, and T. G. Moher. Caching considerations for generational garbage collection. \nIn 1992 ACM Conference on Lisp and Functional Programming, pages 32 42, San Francisco, California, June \n1992. [33] B. Zorn. The e.ect of garbage collection on cache performance. Technical Report CU-CS-528-91, \nUniversity of Colorado at Boulder, May 1991.  \n\t\t\t", "proc_id": "512529", "abstract": "While caches are effective at avoiding most main-memory accesses, the few remaining memory references are still expensive. Even one cache miss per one hundred accesses can double a program's execution time. To better tolerate the data-cache miss latency, architects have proposed various speculation mechanisms, including load-value prediction. A load-value predictor guesses the result of a load so that the dependent instructions can immediately proceed without having to wait for the memory access to complete. To use the prediction resources most effectively, speculation should be restricted to loads that are likely to miss in the cache and that are likely to be predicted correctly. Prior work has considered hardware- and profile-based methods to make these decisions. Our work focuses on making these decisions at compile time. We show that a simple compiler classification is effective at separating the loads that should be speculated from the loads that should not. We present results for a number of C and Java programs and demonstrate that our results are consistent across programming languages and across program inputs.", "authors": [{"name": "Martin Burtscher", "author_profile_id": "81100246199", "affiliation": "Cornell University", "person_id": "P348271", "email_address": "", "orcid_id": ""}, {"name": "Amer Diwan", "author_profile_id": "81100202872", "affiliation": "University of Colorado", "person_id": "PP15025608", "email_address": "", "orcid_id": ""}, {"name": "Matthias Hauswirth", "author_profile_id": "81332503330", "affiliation": "University of Colorado", "person_id": "PP43117899", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/512529.512556", "year": "2002", "article_id": "512556", "conference": "PLDI", "title": "Static load classification for improving the value predictability of data-cache misses", "url": "http://dl.acm.org/citation.cfm?id=512556"}