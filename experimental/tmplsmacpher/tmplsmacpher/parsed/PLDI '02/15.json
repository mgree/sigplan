{"article_publication_date": "05-17-2002", "fulltext": "\n Space-Time Trade-Off Optimization for a Class of Electronic Structure Calculations Daniel Cociorva1 \nGerald Baumgartner1 J. Ramanujam2 Marcel Nooijen3 1 Dept. of Computer and Information Science The Ohio \nState University {cociorva,gb,clam,saday} @cis.ohio-state.edu 2Dept. of Electrical and Computer Engineering \nLouisiana State University jxr@ece.lsu.edu  ABSTRACT The accurate modeling of the electronic structure \nof atoms and molecules is very computationally intensive. Many models of elec\u00adtronic structure, such \nas the Coupled Cluster approach, involve col\u00adlections of tensor contractions. There are usually a large \nnumber of alternative ways of implementing the tensor contractions, rep\u00adresenting different trade-offs \nbetween the space required for tem\u00adporary intermediates and the total number of arithmetic operations. \nIn this paper, we present an algorithm that starts with an operation\u00adminimal form of the computation \nand systematically explores the possible space-time trade-offs to identify the form with lowest cost \nthat .ts within a speci.ed memory limit. Its utility is demonstrated by applying it to a computation \nrepresentative of a component in the CCSD(T) formulation in the NWChem quantum chemistry suite from Paci.c \nNorthwest National Laboratory. Categories and Subject Descriptors F.2.1 [Numerical Algorithms and Problems]: \nComputations on matrices; D.3.2 [Language Classi.cations]: Specialized applica\u00adtion languages; D.3.4 \n[Processors]: Compilers and Optimization; F.2.3 [Tradeoffs between Complexity Measures]; J.2[Physical \nSciences and Engineering]: Chemistry  General Terms Algorithms Keywords Loop fusion, loop transformation, \ntile size selection Permission to make digital or hard copies of all or part of this work for personal \nor classroom use is granted without fee provided that copies are not made or distributed for pro.t or \ncommercial advantage and that copies bear this notice and the full citation on the .rst page. To copy \notherwise, to republish, to post on servers or to redistribute to lists, requires prior speci.c permission \nand/or a fee. PLDI 02, June 17 19, 2002, Berlin, Germany. Copyright 2002 ACM 1-58113-463-0/02/0006 ...$5.00. \n Chi-Chung Lam1 P. Sadayappan1 David E. Bernholdt4 Robert Harrison5 3Department of Chemistry Princeton \nUniversity Nooijen@Princeton.edu 4Oak Ridge National Laboratory bernholdtde@ornl.gov 5 Paci.c Northwest \nNational Laboratory Robert.Harrison@pnl.gov  1. INTRODUCTION The development of high-performance parallel \nprograms for sci\u00adenti.c applications is usually very time consuming. The time to de\u00advelop an ef.cient \nparallel program for a computational model can be a primary limiting factor in the rate of progress of \nthe science. Our long term goal is to develop a program synthesis system to fa\u00adcilitate the development \nof high-performance parallel programs for a class of scienti.c computations encountered in quantum chem\u00adistry. \nThe domain of our focus is electronic structure calculations, as exempli.ed by coupled cluster methods, \nwhere many compu\u00adtationally intensive components are expressible as a set of tensor contractions. We \nplan to develop a synthesis system that can gener\u00adate ef.cient parallel code for a number of target architectures \nfrom an input speci.cation expressed in a high-level notation. A critical issue in implementing electronic \nstructure models, e.g. using coupled cluster methods, is the management of storage re\u00adquirements for \nintermediates. Signi.cant savings in computational cost can be achieved by computing and storing various \nintermedi\u00adate array quantities, that are reused several times in the process of generating the .nal results. \nHowever, the space requirements for these intermediates is often extremely large, making it infeasible \nto store even on disk. Indeed, multi-dimensional intermediate arrays as large as 100 to 1000TB arise \nfrequently in these computations. In this case, there is no choice but to discard and recompute some \nof the intermediates. Therefore the following optimization problem is of great interest: given a set \nof computations expressed as a se\u00adquence of tensor contractions (explained later on), and a speci.ed \nlimit on the amount of available storage, re-structure the compu\u00adtation so as to minimize the amount \nof redundant recomputation required. In this paper, we present a framework that we have devel\u00adoped to \naddress this problem. The space-time trade-off optimiza\u00adtion we consider here is part of a planned synthesis \nsystem that incorporates a number of optimization modules. The computational structures that we address \nin this paper arise in scienti.c application domains that are extremely intensive and consume signi.cant \ncomputer resources at national supercomputer centers. They are present in computational physics codes \nmodel\u00ading electronic properties of semiconductors and metals [1, 11, 21], and in computational chemistry \ncodes such as ACES II, GAMESS, S=0  T1=0; T2=0; S=0 forb, c T.bdJ.BbcJlXDdcl forb, c,d, e,f, l T1f \n=0; T2f= 0 cl [T1bcdf += Bbefl Dcdel for d, f [  forb, c,d, f,j, k [for e,l T.bjk.T.bdJXCdJjk[ \n[T2bcjk += T1bcdf Cdfjk [[[T1f += Bbefl Dcdel dJ fora, b,c, i,j, k [ [for j,k [Sabij += T2bcjk Aacik \n.[  abijT.bjkXAaik [T2fjk += T1f Cdfjk k for a,i, j,k (b) Direct implementation [Sabij += T2fjk Aacik \n (a) Formula sequence (unfused code) (c) Memory-reduced implementation (fused) Figure 1: Example illustrating \nuse of loop fusion for memory reduction. Gaussian, NWChem, PSI, and MOLPRO. In particular, they com\u00adprise \nthe bulk of the computation with the coupled cluster approach to the accurate description of the electronic \nstructure of atoms and molecules [19, 22]. Computational approaches to modeling the structure and interactions \nof molecules, the electronic and optical properties of molecules, the heats and rates of chemical reactions, \netc., are crucial to the understanding of chemical processes in real\u00adworld systems. The paper is organized \nas follows. In the next section, we elab\u00adorate on the computational context of interest, the pertinent \nopti\u00admization issues and an overview of the overall synthesis system that is under development. Section \n3 elaborates on the problem us\u00ading a concrete example that is abstracted from a computationally intensive \ncalculation in the NWChem [10] system. Section 4 pro\u00advides a high-level description of the solution approach. \nSections 5 and 6 present details of the approach to solve the space-time trade\u00adoff problem. Section 7 \npresents results from the application of the new algorithm to the example abstracted from NWChem. Conclu\u00adsions \nare provided in Section 9.  2. THE COMPUTATIONAL CONTEXT In the class of computations considered, the \n.nal result to be computed can be expressed in terms of tensor contractions, essen\u00adtially a collection \nof multi-dimensional summations of the product of several input arrays. Due to commutativity, associativity, \nand distributivity, there are many different ways to compute the .nal result, and they could differ widely \nin the number of .oating point operations required. Consider the following expression: . abijAaikXBbcJlXCdJjkXDdcl \ndcJkl If this expression is directly translated to code (with ten nested loops, for indices a-l), the \ntotal number of arithmetic operations required will be 4XN10if the range of each index a-lis N. Instead, \nthe same expression can be rewritten by use of associative and distributive laws as the following:  \n abij.BbcJlXDdclXCdJjk)XAaik kdJcl This corresponds to the formula sequence shown in Fig. 1(a) and can \nbe directly translated into code as shown in Fig. 1(b). This form only requires 6XN6operations. However, \nadditional space is required to store temporary arrays Tland T2. Often, the space requirements for the \ntemporary arrays poses a serious problem. For this example, abstracted from a quantum chemistry model, \nthe ar\u00adray extents along indices a-dare the largest, while the extents along indices i-lare the smallest. \nTherefore, the size of tempo\u00adrary array Tlwould dominate the total memory requirement. The operation \nminimization problem encountered here is a gen\u00aderalization of the well known matrix-chain multiplication \nproblem, where a linear chain of matrices to be multiplied is given, e.g. ABCD, and the optimal order \nof pair-wise multiplications is sought, i.e. ((AB)C)D versus (AB)(CD) etc. In contrast to this, for compu\u00adtations \nexpressed as sets of matrix contractions, although the .nal realization of the computation is in terms \nof a sequence of matrix\u00admatrix products, there is additional freedom in choosing the pair\u00adwise products. \nFor the above example, instead of forcing a sin\u00adgle chain order, e.g. ABCD, other orders are possible, \nsuch as the BCDA order shown for the operation-reduced form above. We have previously shown that the \nproblem of determining the operator tree with minimal operation count is NP-complete, and have developed \na pruning search procedure [17, 18] that is very ef.cient in practice. For the above example, although \nthe latter form is far more economical in terms of the number of arithmetic operations, its implementation \nwill require the use of temporary intermediate arrays to hold the partial results of the parenthesized \narray subexpressions. Sometimes, the sizes of intermediate arrays needed for the operation-minimal form \nare too large to even .t on disk. A systematic way to explore ways of reducing the memory re\u00adquirement \nfor the computation is to view it in terms of potential loop fusions. Loop fusion merges loop nests with \ncommon outer loops into larger imperfectly nested loops. When one loop nest produces an intermediate \narray which is consumed by another loop nest, fusing the two loop nests allows the dimension correspond\u00ading \nto the fused loop to be eliminated in the array. This results in a smaller intermediate array and thus \nreduces the memory require\u00adments. For the example considered, the application of fusion is illustrated \nin Fig. 1(c). By use of loop fusion, for this example it can be seen that Tlcan actually be reduced to \na scalar and T2to a 2-dimensional array, without changing the number of arithmetic operations. For a \ncomputation comprising of a number of nested loops, there will generally be a number of fusion choices, \nthat are not all mu\u00adtually compatible. This is because different fusion choices could require different \nloops to be made the outermost. In prior work, we addressed the problem of .nding the choice of fusions \nfor a given operator tree that minimized the total space required for all arrays after fusion [14, 16, \n15]. However, for many of the computational structures within the coupled cluster component of the NWChem \nsoftware suite, we .nd instances where the minimal memory required after optimal loop fusion is still \ntoo large. In such situations, in order to create an executable implementation, it is essential to trade \nspace for time, by only storing lower dimensional slices of the largest arrays, and recomputing the slices \nas needed. This is the compiler optimization problem we address in this paper. We extend the use of a \npreviously proposed concept of a fusion graph and develop an algorithm that explores a space of alternative \nspace-time trade-offs to determine the best set of lower-dimensional arrays that .t within a speci.ed \nspace limit, so that the additional recomputation cost is minimized. The problem addressed in this paper \nis one of several optimiza\u00adtion issues in the context of a larger effort to develop a tool for the automatic \nsynthesis of high-performance parallel code from a high-level speci.cation for a class of quantum chemistry \ncalcula\u00adtions. The system being developed has several components: Algebraic Transformations: It takes \nhigh-level input from the user in the form of tensor expressions (essentially sum-of-products array expressions) \nand synthesizes an output computation sequence. The Algebraic Transformations module uses the properties \nof com\u00admutativity and associativity of addition and multiplication and the distributivity of multiplication \nover addition. It searches for all pos\u00adsible ways of applying these properties to an input sum-of-products \nexpression, and determines a combination that results in an equiv\u00adalent form of the computation with \nminimal operation cost. Memory Minimization: The operation-minimal computation se\u00adquence synthesized \nby the Algebraic Transformation module might require an excessive amount of memory due to the large temporary \nintermediate arrays involved. The Memory Minimization module attempts to perform loop fusion transformations \nto reduce the mem\u00adory requirements. This is done without any change to the number of arithmetic operations. \nSpace-Time Transformation: If the Memory Minimization mod\u00adule is unable to reduce memory requirements \nof the computation sequence below the available disk capacity on the system, the com\u00adputation will be \ninfeasible unless a successful space-time trade-off is performed. This is the issue we address in this \npaper. If no satis\u00adfactory transformation is found, feedback is provided to the Mem\u00adory Minimization \nmodule, causing it to seek a different solution. If the Space-Time Transformation module is successful \nin bringing down the memory requirement below the disk capacity, the Data Locality Optimization module \nis invoked. Data Locality Optimization: If the space requirement exceeds physical memory capacity, portions \nof the arrays must be moved between disk and main memory as needed, in a way that maximizes reuse of \nelements in memory. The same considerations are involved in effectively minimizing cache misses blocks \nof data must be moved between physical memory and the limited space available in the cache. These issues \nhave been addressed elsewhere [4, 3]. Data Distribution and Partitioning: This module determines how \nto best partition the arrays among the processors of a parallel sys\u00adtem. We assume a data-parallel model, \nwhere each operation in the operation sequence is distributed across the entire parallel ma\u00adchine. The \narrays are to be disjointly partitioned between the phys\u00adical memories of the processors. The goal is \nto determine the array distribution that minimizes inter-processor communication cost. In practice, we \n.nd that the parallelization considerations are closely coupled with the memory minimization considerations. \nIn the next section we use an example from quantum chemistry to further elaborate on the space-time trade-off \noptimization ad\u00addressed in this paper. 3. ELABORATION OF THE PROBLEM One of the most computationally \nintensive components of many quantum chemistry packages is the CCSD(T) scheme. It is a cou\u00adpled cluster \napproximation that includes all single and double ex\u00adcitations from the Hartree-Fock wave function plus \na perturbative estimate for the connected triple excitations. For molecules well described by a Hartree-Fock \nwave function, the CCSD(T) method predicts bond energies, ionization potentials, and electron af.nities \nto an accuracy of approximately \u00b10.5kcal/mol, bond lengths accu\u00ad fora, e,c, f [ fori, j [ Xaecf += \nTijae Tijcf forc, e,b, k array space time [ T1cebk =f1(c,e,b,k) X V4 V402 fora, f,b, k V30 [T1 CJV30 \nT2afbk =f2(a,f,b,k) T2 V30CJV30 forc,e,a,f Y V4 V50 [ forb,k E 1 V4 [ Yceaf += T1cebk T2afbk forc, e,a, \nf [ E+= Xaecf Yceaf Figure 2: Unfused operation-minimal form. rate to \u00b10.0005A,iand vibrational frequencies \naccurate to \u00b15m.1 . This level of accuracy is adequate to answer many of the questions that arise in \nstudies of complex chemical systems. As a motivating example for the problem addressed, we discuss a \ncomponent of the CCSD(T) calculation. The following represen\u00adtative equation arises in the Laplace factorized \nexpression for linear triples perturbation correction. ( A3A=lXc,aJrac,J+XafJc,af+XafJrfc,aJ 2c,frfJc,ff \n) +Xff+Xfrf+Xff ac,JfaJc,faffJfarc,fac,faJfrfc,f Jc,J =tacJ where the Xand rintermediates are of the \nform Xac,Jijtij and rc,aJ=(bIck)(abIfk), respectively. Integrals with two vertical bars have been antisymmetrized \nand may be expressed as: (( qIrs)=( qIrs)-( qIsr)), where integrals with one vertical bar are of the \nform (fVIw)= dr3ds3cI(r)cv(s)Ir-sI.1cw(r)c)(s)and are quite expensive to compute (requiring on the order \nof 1000 arithmetic operations). Electrons may have either up or down (or alpha/beta) spin. Down spin \nis denoted here with an over bar. The indices i,j,k,l,m,n refer to occupied orbitals), of number O between \n30 and 100. The indices a,b,,d,c,frefer to unoccupied orbitals of number V be\u00adtween 1000 and 5000. The \nintegrals are written in the molecular or\u00adbital basis, but must be computed in the underlying atom-centered \nGaussian basis, and transformed to the molecular orbital basis. We omit these details in our discussion \nhere. A3A is one of many contributions to the energy, and among the most expensive, scaling as O(O5).Here, \nwe assume that we have already computed the amplitudes tac, and they must be read as nec\u00ad ij essary, \nand contracted to form a block of X.The integrals (bIck) must be recomputed as necessary, contracted \nto form a block of r corresponding to X,and the two contracted to form the scalar con\u00adtribution to the \nenergy. Figure 2 shows pseudo-code for the computation of one of the energy components Efor A3A. Temporary \narrays Tland T2are used to store the integrals of form (abIck), where the functions fland f2represent \nthe integral calculations. The intermediate quantities XacJare computed by contracting over (i.e., summing \nover products of) input array T, while the intermediate quantities rcaJare obtained by contracting over \nTland T2. The .nal result is a single scalar quantity E, that is obtained by adding together the O(O3)pair-wise \nproducts XacJrcaJ. The cost of computing each integral fl, f2is represented by CJ, and in practice is \nof the order of hundreds or a few thousand arithmetic operations. The pseudo-code form shown in Fig. \n2 is computationally very ef.cient in minimizing the number of expen\u00adsive integral function evaluations \nfland f2, and maximizing the for a,e, c,f fora, e,c, f fori, j [fori, j [ [ X+= Tijae TijcfXaecf += \nTijae Tijcf forb, k fora, f [ T1=f1(c,e,b,k) forc, e,b, k [[ [T2=f2(a,f,b,k)T1cebk =f1(c,e,b,k) [ Y+=T1 \nT2 forc, e E+=X Y fora, f,b, k [ T2afbk =f2(a,f,b,k) array space time forc, e,a, f 402 [ X 1 V forb, \nk [ T1 1 CJV50 Yceaf += T1cebk T2afbk T2 1 CJV50 forc, e,a, f 50 [ Y 1 V E+= Xaecf Yceaf E 1 V4 Figure \n3: Use of redundant computation to allow full fusion. t for at,e,ct,ft for a,e, c,f [ for i,j [ Xaecf \n+= Tijae Tijcf array space time for b,k X B4 V402 forc, e [ B2 52 T1 CJV0B T1ce =f1(c,e,b,k) B2 52 T2 \nCJV0B fora, f [ Y B4 V50 T2af =f2(a,f,b,k) [ E 1 V4 forc, e,a, f [ Yceaf += T1ce T2af [ for c,e, a,f \n [ E+=Xaecf Yceaf Figure 4: Use of tiling and partial fusion to reduce recomputa\u00adtion cost. reuse of \nthe stored integrals in Tland T2(each element of Tland T2is used O(2)times). However, it is impractical \ndue to the huge memory requirement. With O=l00and =5000, the size of Tl, T2is O(l014)bytes and the size \nof X, ris O(l015)bytes. By fusing together pairs of producer-consumer loops in the compu\u00adtation, reductions \nin the needed array sizes may be sought, since the fusion of a loop with common index in the pair of \nloops allows the elimination of that dimension of the intermediate array. It can be seen that the loop \nthat produces X(with indices a,c,,f), the loop that produces r(with indices ,c,a,f) and the loop that \nconsumes Xand rto produce E(with indices ,c,a,f) can all be fully fused together, permitting the elimination \nof all explicit indices in Xand rto reduce them to scalars. However, the loops producing Tl (with indices \n,c,b,k) and T2(with indices a,f,b,k) cannot also be directly fused with the other three loops because \ntheir indices do not match. Figure 3 shows how reduction of space for Tland T2can be achieved by introduction \nof redundant loops around their producer loops add loops with the missing indices a,ffor Tland ,cfor \nT2. Now all .ve of the loops have common indices a,c,,fthat can be fused, permitting elimination of those \nindices from all tem\u00adporaries. Further, by fusing together the producer loops for Tland T2with their \nconsumer loop that produces r, the b,kindices can also be eliminated from Tland T2. Dramatic reduction \nof mem\u00adory space is achieved, reducing all temporaries Tl,T2,Xand rto scalars. However, the space savings \ncome at the price of signi.cant increase in computation. Now, no reuse is achieved of the quantities \nderived from the expensive integral calculations fland f2. Since CJis of the order of 1000 in practice, \nthe integral calculations now dominate the total compute time, increasing the operation count by three \norders of magnitude over the unfused form in Fig. 2. A desirable solution would be somewhere in between \nthe un\u00adfused structure of Fig. 2 (with maximal memory requirement and maximal reuse) and the fully fused \nstructure of Fig. 3 (with min\u00adimal memory requirement and minimal reuse). This is shown in Fig. 4, where \ntiling and partial fusion of the loops is employed. The loops with indices a,c,,fare tiled by splitting \neach of those indices into a pair of indices. The indices with a superscript trepre\u00adsent the tiling loops \nand the unsuperscripted indices now stand for intra-tile loops with a range of B, the block size used \nfor tiling. For tt tt each tile (a,c,,f), blocks of Tland T2of size B2are com\u00ad puted and used to form \nB4product contributions to the appropriate 4 components of r, which are stored in an array of size B. \nAs the tile size Bis increased, the cost of function computation for fl,f2decreases by factor B2, due \nto the reuse enabled. How\u00ad 4 ever, the size of the needed temporary array for rincreases as B(the space \nneeded for Xcan actually be reduced back to a scalar by fusing its producer loop with the loop producing \nE, but r s space requirement cannot be decreased). When B4becomes larger than the size of physical memory, \nexpensive paging in and out of disk will be required for r. Further, there are diminishing returns on \nreuse of Tland T2after B2becomes comparable to CJ, since the loop producing rnow becomes the dominant \none. So we can expect that as Bis increased, performance will improve and then level off and then deteriorate. \nThe optimum value of Bwill clearly depend on the cost of access at the various levels of the memory hierarchy. \nThe computation considered here is just one component of the A3Aterm, which in turn is only one of very \nmany terms that must be computed. Although developers of quantum chemistry codes naturally recognize \nand perform some of these optimizations, a col\u00adlective analysis of all these computations to determine \ntheir optimal implementation is beyond the scope of manual effort. While recent developments in optimizing \ncompiler research have resulted in sig\u00adni.cant strides in data locality optimization, we are unaware \nof any existing work that addresses the kind of space-time trade-off opti\u00admization required in the context \nwe consider.  4. SOLUTION APPROACH: THE FUSION GRAPH The operation-minimization procedure discussed \nabove usually results in the creation of intermediate temporary arrays. Sometimes these intermediate \narrays that help in reducing the number of arith\u00admetic operations create a problem with the memory capacity \nre\u00adquired. For a computation comprising of a number of nested loops, there will generally be a number \nof fusion choices, that are not all mu\u00adtually compatible. This is because different fusion choices could \nrequire different loops to be made the outermost. A data structure that we call a fusiongra hcan be used \nto facilitate enumeration of all possible compatible fusion con.gurations for a given compu\u00adtation tree. \nFigure 5 shows the fusion graph for the unfused form of the com\u00adputation from Fig. 2. Corresponding to \neach node in a computation tree, the fusion graph has a set of vertices corresponding to the loop indices \nof the node of the computation tree. In Fig. 5, we do not show the operator tree corresponding to the \ncomputation, but directly illustrate the fusion graph. The potential for fusion of a common loop among \na producer-consumer pair of loop nests is indicated in the fusion graph through a dashed otcntialfusion \nedge connecting the corresponding vertices. Leaf nodes in the fu\u00adsion graph correspond to input arrays \nor primitive function evalua\u00adtions and do not represent a loop nest. The edges from the leaves ceaf T \nT2 f2 ceb afbk Figure 5: Fusion graph for unfused operation-minimal form of loop in Figure 2. to their \nparents are shown as dotted edges and do not affect the fu\u00adsion possibilities. If a pair of loop nests \nis fused using one or more common loops, it is captured in the fusion graph by changing the dashed potential-fusion \nedges to continuous fusion edges. If more than two loop nests are fused together, a chain of fusion edges \nre\u00adsults, called a fusion chain. The scope of a fusion chain is the set of nodes it spans. The fusion \ngraph allows us to characterize the condition for feasibility of a particular combination of fusions: \nthe scope of any two fusion chains in a fusion graph must either be disjoint or a subset/superset of \neach other. Scopes of fusion chains do not partially overlap because loops do not (i.e., loops must be \neither separate or nested). The fusion graph in Fig. 5 can be used to determine the fusion possibilities. \nOn the left side of the graph, the edges corresponding to (a,c,,f)can all be made fusion edges, suggesting \nthat com\u00adplete fusion is possible for the loop nests producing and consum\u00ading X, reducing it to a scalar. \nSimilarly, on the right side of the graph, the edges corresponding to (,c,a,f)can also be made fu\u00adsion \nedges, reducing rto a scalar. Further, by creating fusion edges for indices (,c), the producer loop for \nTlcan be fully fused with the rloop that consumes it. However, now the producer loop for T2cannot be \nfused since the addition of any fusion edge (say for index a) will result in partially overlapping fusion \nchains for aand (,c). The fully fused version from Fig. 3 can be represented graphi\u00adcally as shown in \nFig. 6(a). Additional vertices have been added for indices (,c)and (a,f)respectively at the nodes correspond\u00ading \nto the producer loops for Tland T2. Now, complete fusion chains can be created without any partial overlap \nin the scopes of the fusion chains. From the .gure, it can be seen that in fact the redundant computation \nneed only be added to one of Tlor T2to achieve complete fusion for example, removing the additional \nvertices for (a,f)at T2does not violate the non-partial-overlap condition for fusion. The fusion graph \nwas used to develop an algorithm [16, 14] to determine the combination of fusions that minimizes the \ntotal stor\u00adage required for all the temporary intermediate arrays. A bottom-up dynamic programming approach \nwas used, that maintains a set of pareto-optimal fusion con.gurations at each node, merging solu\u00adtions \nfrom children nodes to generate the optimal con.gurations at a parent. The two metrics used are the total \nmemory required un\u00adder the subtree rooted at the node, and the constraints imposed by a con.guration \non fusion further up the tree. A con.guration is infe\u00adrior to another if it is more or equally constraining \nwith respect to further fusions than the other, and uses no less memory. At the root of the tree, the \ncon.guration with lowest memory requirement is chosen. Although the complexity of the algorithm is exponential \nin the number of index variables and the number of solutions could in theory grow exponentially with \nthe size of the expression tree, the number of index variables in practical applications is small enough \nand there is indication that the pruning is effective in keeping the size of the solution set at each \nnode small. The fusion graph framework addresses a memory minimization problem, without changing the \noperation count. If we applied it to the fusion graph of Fig. 2, the bottom-up dynamic programming al\u00adgorithm \nwould evaluate a number of potential fusion combinations and .nd that fusion could be used to reduce \nthe sizes of arrays X and rand convert them to scalars. It would also be able to reduce the size of one \nof the two temporary arrays Tlor T2, but would be unable to reduce the other at all. Although three of \nfour temporary arrays would be dramatically reduced in size, the size of the sin\u00adgle remaining temporary \narray (of size O(3O)) would make the problem unexecutable on most systems due to disk storage limits. \nAn enhancement of the model to capture a wider range of space\u00adtime trade-offs was already seen in Fig. \n6(a), where additional ver\u00adtices were added to the fusion graph to introduce redundant recom\u00adputation \nto the producer loops for Tland T2and thereby enable a greater degree of fusion. As discussed earlier, \nthe fully fused version of the loops results in excellent memory savings but adds excessive recomputation \ncosts. A combination of fusion and tiling is needed to achieve a good balance between recomputation and \nmemory usage. Figure 6(b) shows how the possibility of tiling can be introduced into the fusion graph. \nFor each loop of a loop nest that is to be tiled, the corresponding vertex in the fusion graph is replaced \nby a pair of vertices one to represent the outer tiling loop and another to denote the intra-tile loop. \nBy a choice of fusion con.guration that only involves the tiling loops, a combination of fusion and tiling \ncan be represented. This framework can be used to explore a range of space-time trade-offs. However, \nthe search space is signi.cantly larger than that for the memory minimization prob\u00adlem discussed in the \nprevious sub-section, requiring that selective search strategies be developed. In this paper, we develop \na two-step search strategy for explo\u00adration of the space-time trade-off: Search among all possible ways \nof introducing redundant loop indices in the fusion graph to reduce memory require\u00adments, and determine \nthe optimal set of lower dimensional intermediate arrays for various total memory limits. In this step, \nthe use of tiling for partial reduction of array extents is not considered. However, among all possible \ncombinations of lower dimensional arrays for intermediates, the combina\u00adtion that minimizes recomputation \ncost is determined, for a speci.ed memory limit. The range from zero to the actual memory limit is split \ninto subranges within which the op\u00adtimal combination of lower dimensional arrays remains the same. Because \nthe .rst step only considers complete fusion of loops, each array dimension is either fully eliminated \nor left intact, i.e. partial reduction of array extents is not performed. The objective of the second \nstep is to allow for such arrays. Start\u00ading from each of the optimal combinations of lower dimen\u00adsional \nintermediate arrays derived in the .rst step, possible ways of using tiling to partially expand arrays \nalong previ\u00adously compressed dimensions are explored. The goal is to further reduce recomputation cost \nby partially expanding ar\u00adrays to fully utilize the available memory ceaf tttt ceafceaf  T  5. DIMENSION \nREDUCTION FOR INTER-MEDIATE ARRAYS In the .rst step of the space-time trade-off algorithm we search among \nall possible combinations of redundant computations and loop fusions. The search is structured as a dynamic \nprogramming algorithm with pruning. The input to this algorithm is an expression tree representing the \noperation-minimal computational structure of the input formula. Expression tree nodes are of four types: \n array references a[i]with index vector i,  function calls f(i)with argument vector i,  summation quanti.ers \nsum(i,t) with summation indices i and subtree t, and  binary operators bin(o,l,r) with operator o(+, \n-,or *) and subtrees land r.  For each tree node v, let indices(v) be the set of loop indices needed \nfor evaluating v, and let fusible(v) be the set of indices that can be fused with the parent (indices \nother than summation indices). An index iis a redundant index for node vif iis not an index of v but \nof some ancestor node of v. E.g., in Fig. 5 indices aand fare redundant indices for Tl. Let redundant(v) \nbe the set of redundant indices for v. Introducing a redundant loop index ito a node vcan allow ad\u00additional \nfusion between vand its parent, which reduces the dimen\u00adsion of the intermediate array holding the result \nof v, in exchange for recomputing vin every iteration of the iloop. The space-time trade-off algorithm \ncomputes for every combination of redundant indices the loop fusion structure that results in the least \namount of total memory. In a bottom-up traversal, we compute a set of solutions for each node v. Each \nsolution consists of a nesting of the loops at v, the memory cost m, the recomputation cost r, and pointers \nto the solutions of the subtrees from which this solution was obtained. A nesting is a sequence of index \nsets that represents constraints on the loop structure for computing v. E.g., the nesting (ij,k) indicates \nthat the loops iand jcan be arbitrarily permuted, while kmust be nested inside of iand j. A solution \ns .is inferior to solution sif its nesting is more constraining than that of s(e.g., (i,j,k)is more constraining \nthan (ij,k)), and if its memory cost and recomputation cost are both higher than those of s. The set \nof solutions for a node is recursively computed as follows: Suppose vis an array reference of the form \na[i]. The set T2 f2 loops. For the purpose of space-time trade-offs, we do not model the cost of reading \narrays from disk. Therefore, we form a solution for each of these nestings with zero memory and recomputation \ncosts. Suppose vis of the form f(i). Similar as for array refer\u00adences, we form a set of solutions for \nall possible nestings. For each nesting h, we initialize the memory cost to the stor\u00adage needed for holding \nthe result of f(i)if all the indices in hare fused with the parent. The recomputation cost is ini\u00adtialized \nto the number of times fmust be recomputed for all redundant indices in htimes the cost of a function \ncall.  Suppose vis of the form sum(i,t). For each solution sfor subtree t, we initialize a solution \ns .for the summation node by adding one to the memory cost (for the scalar holding the result of the \nsummation assuming full fusion with the par\u00adent) and by adding the recomputation cost for the summation \nnode to that of the subtree. We then remove the summation indices ifrom the nesting in s . . All indices \nthat are con\u00adstrained to be nested inside the summation indices must be removed as well since they cannot \nbe fused with the parent anymore. Removing a non-summation index jfrom the nest\u00ading results in an increase \nin memory since the jdimension of the resulting array must be stored. Finally, inferior solutions are \npruned from the set of solutions for v.  Suppose vis of the form bin(o,l,r). Since the subtrees land \nrmight not have all the indices of v(indices(v) is the union of indices(l) and indices(r)), we .rst need \nto compute all the possible ways in which the solutions for land rmight be fused with v. For each solution \nsfor a subtree, we com\u00adpute the set of all pre.xes of the nesting of s(e.g., for the nesting (ij,k), \nthe pre.x (i)represents the loop structure in which only iis fused with v). For all the nestings obtained \nin this way we construct new solutions for the subtrees by in\u00adcreasing the memory cost by the array dimensions \nthat now need to be stored. Then, for all pairs of solutions sland srfor land r, respectively, we merge \nthe constraints on the loop structure from the nestings of sland sr.If sland sr have compatible nestings, \nwe obtain a merged nesting for v. E.g., for the nestings (ij,kl)and (i,jk)for the subtrees, we would \nobtain the nesting (i,j,k,l)for v. Finally, we con\u00adstruct solutions for vout of the merged nestings by \nadding the memory and recomputation costs for vto the costs for the subtrees and then prune inferior \nsolutions.  of possible loops around the array node is fusible(t) Upow-The result of the above algorithm \nis a set of solution trees for erset(redundant(t)) with no constraints on the order of the the original \nexpression tree. A solution tree contains a nesting and E=0 for c for b,e,k T1[b,e,k] = f1(c,e,b,k) for \na,f for e Y[e]= 0 for b,k T2 = f2(a,f,b,k) for e Y[e] += T2 * T1[b,e,k] for e X=0 for i,j X += \nT[i,j,a,e] * T[i,j,c,f] E+= Y[e]* X return E Figure 7: Pseudo-code for the solution with the lowest \nrecom\u00adputation cost after the .rst step of the algorithm, subject to a memory limit of l012words. The \narray sizes are Ni =Nj= Nk =O=l00and Na =Nb =N =Nc =NJ ==3000. The redundant evaluation of f2(a,f,b,k)is \nperformed N = =3000times. memory and recomputation costs for each tree node of the expres\u00adsion tree. \nFor each node v, the nesting for vonly re.ects con\u00adstraints on the loop structure for the subtree rooted \nat v. From a solution tree we compute a fusion tree by propagating constraints on loop nestings from \nthe top of the tree down to the leaves. The resulting fusion tree is then translated into an abstract \nsyntax tree by constructing a computation order for the tree nodes. A node v is computed after its subtrees. \nFor a binary node, the subtree with the most loops fused is computed just before the parent. After the \ncomputation order is determined, the loops are inserted to form an abstract syntax tree representation \nof the code. For example, for the expression tree corresponding to the formula sequence in Fig. 1(a) \nthis algorithm constructs the pseudo-code in Fig. 7 as the solution with the minimal recomputation cost \nthat stays below l012words.  6. PARTIAL EXPANSION OF REDUCED IN-TERMEDIATES Once a set of optimal solutions \nis determined by the .rst step of the space-time trade-off algorithm, we resort to array expansion for \nthe second step. The second step operates on the abstract syn\u00adtax tree generated by the .rst step of \nthe algorithm. In this tree, an interior node represents a loop nest, while a leaf represents the computation \nof a node from the expression tree. A parent-child pair of nodes denotes an outer-inner loop pair, whereas \nnodes with the same parent represent adjacent loops. For an example, the abstract syntax tree corresponding \nto the pseudo-code in Fig. 7 is shown in Fig. 8. The total number of operations needed to compute the \n.nal re\u00adsult is the sum over the number of operations for the leaves of the abstract syntax tree. For \neach leaf, the number of operations is obtained by multiplying the cost of the operation (one for multipli\u00adcations \nor additions, a higher cost for function evaluations) by the loop ranges of all its ancestors in the \nabstract syntax tree. For ex\u00adample, the number of operations required to compute Xin Fig. 8 is 2NNaNJNcNiNj=2O24operations \n(the factor of 2 comes from one multiplication and one addition). Likewise, the number of operations \nnecessary to compute T2is l000NNaNJNbNk = l000O4operations, assuming 1000 .oating point operations are \nneeded for each evaluation of f2. In the case of Xthe number of operations cannot be further reduced. \nThere is no redundant cost c b e k e E += X*Y Figure 8: Abstract syntax tree for the fused loop structure \nshown in Fig. 7. A node in the tree represents a loop nest; a parent-child pair represents an outer loop \n(parent node), and an inner loop (child node). The leaves of the tree are multipli\u00adcation and addition \ncomputations or function evaluations. in computing X. In contrast, T2is repeatedly computed N = times, \nsince is a redundant loop for T2. For the pseudo-code presented in Fig. 7 and the corresponding abstract \nsyntax tree in Fig. 8, the recomputation cost is l000(N -l)NaNJNbNk = l000O3(-l), coming entirely from \nthe evaluation of T2. In practice, the intermediate arrays do not have to be fully down\u00adsized to a lower \nnumber of dimensions. For example, the solution in Fig. 7 uses only 9.0Xl08words, much less than the \nmemory limit of l012words. We can therefore increase the sizes of some intermediate arrays in order to \nreduce the recomputation cost. In our algorithm, each redundant node in the tree is split into a parent\u00adchild \npair, corresponding to a tiling loop node, and an intra-tile loop node. Figures 9 and 10 present the \npseudo-code and abstract syn\u00adtax tree for the same computation, this time performed with loop tiling. \nIn this example, the root of the abstract syntax tree is the only redundant loop, but in general the \nnumber of redundant loops could be as large as the number of nodes in the abstract syntax tree. Here \nthe loop is split into the tiling and intra-tile loops tand i. The ranges of these loops are NB(the number \nof blocks) and B (the block size), respectively, such that their product is equal to the original range: \nBXNB=N ==3000. The arrays Tl, r and Xare partially expanded from size one to size Balong the di\u00admension. \nThe redundant computation of T2is now only performed NBtimes instead of Ntimes, resulting in a lower \nrecomputation cost. The maximum value for the block size Bis determined by the total amount of memory \navailable in the system. Our algorithm for determining the best choice for array expan\u00adsion (the one \nthat minimizes recomputation cost, and still stays within the total amount of memory available) proceeds \nas follows: for a given untiled abstract syntax tree generated in the .rst step (Fig. 8), all its redundant \nnodes are .rst split into tiling/intra-tile pairs. Subsequently, the resulting abstract syntax tree is \ntransformed by intra-tile loop permutation and .ssion into an equivalent ab\u00adstract syntax tree with the \nproperty that each intra-tile loop is either redundant or non-redundant with respect to all of its descendant \nleaves. At this point those intra-tile loops which are redundant with respect to their descendant leaves \nare removed. Figures 9 and 10 show the pseudo-code and abstract syntax tree after such a transformation. \nThe iloop is split into three loops along different branches of the tree. It is present as an ancestor \nof all the leaves except for the one that produces T2, where it has been removed to reduce the recomputation \ncost. After this tree E=0 for c_t for c_i c =c_i +c_t * NB for b,e,k T1[c_i,b,e,k] = f1(c,e,b,k) \n for a,f for e,c_i Y[c_i,e] = 0 for b,k T2 = f2(a,f,b,k) for e,c_i Y[c_i,e] += T2 * T1[c_i,b,e,k] \n for c_i c= c_i+ c_t *NB for e X[c_i] = 0 for i,j X[c_i] += T[i,j,a,e] * T[i,j,c,f] E += Y[c_i,e] \n* X[c_i] return E Figure 9: Pseudo-code for the solution with the lowest recom\u00adputation cost after the \nsecond step of the algorithm, subject to a memory limit of l012words. The loop is split into the tiling \nand intra-tile loops tand i. The ranges of these loops are NBand B, respectively. Bis the block size, \nand NBis the number of blocks. Their product is equal to the original range N ==3000of the loop. The \narrays Tl, rand Xare par\u00adtially expanded from size one to size Balong the dimension. The evaluation of \nf2(a,f,b,k)is performed NBtimes. transformation the algorithm proceeds by choosing numerical val\u00adues \nfor the tile sizes, thus .xing the loop ranges for all the nodes in the abstract syntax tree. If the \noriginal range of a loop is N, choosing a block size Bfor the intra-tile loop also .xes the range NB \n=N/Bof the tiling loop. We thus obtain a new abstract syntax tree with well-de.ned loop ranges. Using \nthe loop ranges, we can determine the recomputa\u00adtion cost for the entire abstract syntax tree by adding \nthe number of redundant operations for each leaf of the tree. With this approach, we arrive at a total \nrecomputation cost for the abstract syntax tree for given tile sizes. We repeat the calculation of the \nrecomputation cost for different sets of tile sizes. We de.ne our tile size search space in the following \nway: if Niis the loop range of a recompu\u00adtation loop, we use a tile size starting from B=l(no tiling), \nand successively increasing Bby doubling it until it reaches Ni. This ensures a slow (logarithmic) growth \nof the search space for increas\u00ading values of Ni.If Niis small enough, an exhaustive search can instead \nbe performed. This tiling procedure and search for the optimal tile sizes is re\u00adpeated for all solutions \nproduced by the .rst step of the algorithm. We .nally choose the solution with the minimal recomputation \ncost. 7. RESULTS In this section we present the results of our two step space-time trade-off algorithm \nfor the NWChem example introduced in Sec\u00adtion 3. We choose input parameters relevant to the addressed \nprob\u00adlem: Ni =Nj=Nk =O=l00, Na =Nb =N =Nc = NJ ==3000, function evaluation cost CJ =l000.oating point \noperations and available memory of J=l012words. Figure 4 shows the pseudo-code for the solution that \nwas man\u00adually optimized by a domain expert.1 The a, , c, and floops are 1The NWChem code also contains \ncode for transforming integrals from the atomic basis into the molecular basis. This transformation \nc_t c_i b e c_i k e E += X*Y Figure 10: Abstract syntax tree for the fused and tiled loop structure shown \nin Fig. 9. The loop is redundant for the leaf evaluating f2(a,f,b,k), resulting in a large recomputation \ncost. To improve upon that, the redundant loop is split into a tiling/intra-tile pair of loops (tand \ni, respectively). The intra-tile loops iare then moved by .ssion and permutation operations toward the \nbottom of the tree. The iloop is .nally discarded for the leaf computing f2(a,f,b,k). The remaining iloops \nare indicated by empty circles. split into tiling and intra-tile loops of size NBand B, respectively. \nThey obey the constraint BXNB =. The largest intermedi\u00adate array is r, which is a four-dimensional block \nof size B4 . The recomputation cost of this solution is 2CJO3(2/B2 -l). Re\u00adquiring that the total memory \nusage is less than J=l012words, and using the values for O, and CJprovided in the previous paragraph, \nwe arrive at a recomputation cost of .5.lXl016oper\u00adations. The recomputation cost is due to the redundant \nevaluation of the functions fland f2N2times. B The optimal solution is obtained using the two step space-time \ntrade-off algorithm presented in Sections 5 and 6. The .rst step pro\u00adduces six solutions. All other possible \nloop fusion structures have both higher memory usage and higher recomputation cost than one or more of \nthese solutions. Figure 11 shows the six solutions rang\u00ading in memory usage from three words to 2.7Xl015words, \nand in recomputation cost from zero operations to 4.9Xl022operations. The memory limit in our example \nis marked by the solid horizon\u00adtal line. Solution number 1 is trivial, and represents the memory optimal \nsolution with no redundant computation. Such a solution always exists for any operator tree. If its memory \nusage is below the memory limit, then the second step of the algorithm is no longer necessary, and this \nbecomes the optimal .nal solution. Otherwise, it is discarded, along with all the other solutions that \nare above the memory limit (in this case, only number 1). The rest of the so\u00adlutions (2 through 6 in \nthis example) are then passed through the second step of the algorithm. Figure 7 shows the pseudo-code \nfor solution 2, which has the lowest recomputation cost (.8.lXl018 .oating point operations) after the \n.rst step of the algorithm. The array expansion step brings signi.cant further reduction of the recomputation \ncost for all the remaining 5 solutions. Their re\u00adcomputation costs, ranging from 8.lXl018to 4.9Xl022opera\u00adtions \nafter step 1, are reduced to between 5.4Xl015and 5.lXl016 operations. The pseudo-code for the .nal optimal \nsolution is pre\u00adsented in Fig. 9. It happens to be the tiled form of solution 2, which was the best solution \nbefore the array expansion step. However, this is just a coincidence, due in part to the very small operator \ntree is encapsulated in the function calls. Memory usage (words) 1015 1010 105 100 Figure 11: Relationship \nbetween memory usage and recompu\u00adtation cost. Solid triangles represent the 6 different solutions produced \nby the .rst step of the space-time trade-off algorithm. The horizontal line shows the hard memory limit \nof J=l012 words used for this example. Except for solution 1, which uses more memory than the l012words \nlimit, all the other solutions are analyzed by the second step of the algorithm. considered for this \nexample, which in turn generates a very limited number of solutions. In general, any of the solutions \nobtained in step one could become the optimal solution after tiling. We note that the .nal solution is \nnot trivial, in fact it has a rather complex structure. We also observe that, although their cost is \nsimi\u00adlar, all the solutions (the tiled versions of 2 through 6) have abstract syntax trees that are quite \ndifferent. Indeed, even for a relatively simple formula, like the one used in this example, the collection \nof solutions is rather rich and non-trivial. Manual optimization is unlikely to .nd and test all possibilities, \nespecially for larger trees. It is also interesting to note that one of the solutions produced by the \nalgorithm (the tiled version of 6) is identical to the manually optimized pseudo-code presented in Fig. \n4. Its recomputation cost of 5.lXl016operations is roughly one order of magnitude higher than the cost \nof the optimal solution. We investigate the recomputation cost of the optimal code in comparison with \nthat of the manually generated code for various values of the input parameters O, , and J, consistent \nwith their physical meaning. We .nd, as expected, that the structure of the op\u00adtimal code may change \nfrom one set of input parameters to another. The improvement factor over the manual code presented in \nFig. 4 ranges from 1 (when the manual code is optimal) to 20, depending on O, , and J. 8. RELATED WORK \nMuch work has been done on improving locality and parallelism by loop fusion. Kennedy and McKinley [13] \npresented an algo\u00adrithm for fusing a collection of loops to minimize parallel loop synchronization and \nmaximize parallelism. They proved that .nd\u00ading loop fusions that maximizes locality is NP-hard. Darte \n[5] dis\u00adcusses the complexity of maximal fusion of parallel loops. A fast algorithm was presented by \nKennedy in [12] that allows accurate modeling of data sharing as well as the use of fusion enabling trans\u00adformations. \nDing [6] illustrates the use of loop fusion in reducing storage requirements through an example, but \ndoes not provide a general solution. Gao et al. [8] studied the contraction of arrays into scalars through \nloop fusion as a means to reduce array access overhead. They partitioned a collection of loop nests into \nfusible clusters using a max-.ow min-cut algorithm, taking into account the data dependencies. Loop fusion \nin the context of delayed evaluation of array expres\u00adsions in compiling APL programs has been discussed \nby Guibas and Wyatt [9]. As part of their algorithm, a general buffering mech\u00adanism was devised to save \nportions of a sub-expression that will be repeatedly needed, to avoid re-computation. They considered \nloop fusion without any loop reordering; and their work is not aimed at minimizing array sizes. Lewis \net al. [20] discusses the applica\u00adtion of fusion directly to array statements in languages such as F90 \nand ZPL. Callahan et al. [2] present a technique to convert array references to scalar accesses in innermost \nloops. There has been some recent work on using loop fusion for mem\u00adory reduction for sequential execution. \nFraboulet et al. [7] use loop alignment to reduce memory requirement between adjacent loops by formulating \nthe one-dimensional version of the problem as a net\u00adwork .ow problem. Song [23] and Song et al. [25, \n24] present a dif\u00adferent network .ow formulation of the memory reduction problem and they include a simple \nmodel of cache misses as well. However, they do not consider the issue of trading off memory for recompu\u00adtation. \n9. CONCLUSION This paper addressed a space-time trade-off problem that arises in the context of a larger \nproject on developing a program synthe\u00adsis system, targeted at the development of high-performance par\u00adallel \nprograms for a class of computations encountered in quan\u00adtum chemistry. A two step algorithm was developed \nfor the space\u00adtime trade-off optimization problem. Results were presented of its application to a test \ncase abstracted from the quantum chemistry code NWChem. The solution derived using our implementation \nof the algorithm reduces the recomputation cost of the calculation by about an order of magnitude for \ntypical problem sizes. 10. ACKNOWLEDGMENTS We are grateful to the National Science Foundation for support \nof this work through the Information Technology Research Program (CHE-0121676). 11. REFERENCES [1] W. \nAulbur. Parallel Implementation of Quasiparticle Calculations of Semiconductors and Insulators. PhD thesis, \nThe Ohio State University, Oct. 1996. [2] D. Callahan, S.Carr, and K. Kennedy. Improving register allocation \nfor subscripted variables. In SIGPLAN Conference on Programming Language Design and Implementation, White \nPlains, NY, June 1990. [3] D. Cociorva, J. Wilkins, G. Baumgartner, P. Sadayappan, M. Nooijen, D. Bernholdt, \nand R. Harrison. Towards automatic synthesis of high-performance codes for electronic structure calculations: \nData locality optimization. In International Conference on High Performance Computing, Dec. 2001. [4] \nD. Cociorva, J. Wilkins, C.-C. Lam, G. Baumgartner, P. Sadayappan, and J. Ramanujam. Loop optimization \nfor a class of memory-constrained computations. In 15th ACM International Conference on Supercomputing, \npages 500 509, Sorrento, Italy, June 2001. [5] A. Darte. On the complexity of loop fusion. In International \nConference on Parallel Architectures and Compilation Techniques, Newport Beach, CA, Oct. 1999. [6] C. \nDing. Improving effective bandwidth through compiler enhancement of global and dynamic cache reuse. PhD \nthesis, Rice University, Jan. 2000. [7] A. Fraboulet, G. Huard, and A. Mignotte. Loop alignment for memory \naccess optimization. In 12th International Symposium on System Synthesis, pages 71 77, San Jose, CA, \nNov. 1999. [8] G. Gao, R. Olsen, V. Sarkar, and R. Thekkath. Collective loop fusion for array contraction. \nIn Languages and Compilers for Parallel Processing, New Haven, CT, Aug. 1992. [9] L. Guibas and D. Wyatt. \nCompilation and delayed evaluation in APL. In 5th Annual ACM Symposium on Principles of Programming Languages, \npages 1 8, Tucson, AZ, Jan. 1978. [10] High Performance Computational Chemistry Group. NWChem, a computational \nchemistry package for parallel computers, version 3.3, 1999. Paci.c Northwest National Laboratory, Richland, \nWA 99352. [11] M. S. Hybertsen and S. G. Louie. Electronic correlation in semiconductors and insulators: \nBand gaps and quasiparticle energies. Phys. Rev. B, 34:5390, 1986. [12] K. Kennedy. Fast greedy weighted \nfusion. In ACM International Conference on Supercomputing, Santa Fe, NM, May 2000. [13] K. Kennedy and \nK. McKinley. Maximizing loop parallelism and improving data locality via loop fusion and distribution. \nIn Languages and Compilers for Parallel Computing, pages 301 320, Portland, OR, Aug. 1993. [14] C.-C. \nLam. Performance Optimization of a Class of Loops Implementing Multi-Dimensional Integrals. PhD thesis, \nThe Ohio State University, Aug. 1999. Also available as Technical Report No. OSU-CISRC-8/99-TR22, Dept. \nof Computer and Information Science, The Ohio State University, August 1999. [15] C.-C. Lam, D. Cociorva, \nG. Baumgartner, and P. Sadayappan. Memory-optimal evaluation of expression trees involving large objects. \nIn International Conference on High Performance Computing, Calcutta, India, Dec. 1999. [16] C.-C. Lam, \nD. Cociorva, G. Baumgartner, and P. Sadayappan. Optimization of memory usage and communication requirements \nfor a class of loops implementing multi-dimensional integrals. In 12th International Workshop on Languages \nand Compilers for Parallel Computing, San Diego, CA, Aug. 1999. [17] C.-C. Lam, P. Sadayappan, and R. \nWenger. On optimizing a class of multi-dimensional loops with reductions for parallel execution. Parall. \nProcess. Lett., 7(2):157 168, 1997. [18] C.-C. Lam, P. Sadayappan, and R. Wenger. Optimization of a class \nof multi-dimensional integrals on parallel machines. In Eighth SIAM Conference on Parallel Processing \nfor Scienti.c Computing,, Minneapolis, MN, Mar. 1997. Society for Industrial and Applied Mathematics. \n[19] T. J. Lee and G. E. Scuseria. Achieving chemical accuracy with coupled cluster theory. In S. R. \nLanghoff, editor, Quantum Mechanical Electronic Structure Calculations with Chemical Accuracy, pages \n47 109. Kluwer Academic, 1997. [20] E. Lewis, C. Lin, and L. Snyder. The implementation and evaluation \nof fusion and contraction in array languages. In ACM SIGPLAN Conference on Programming Language Design \nand Implementation, June 1998. [21] H. N. Rojas, R. W. Godby, and R. J. Needs. Space-time method for \nAb-initio calculations of self-energies and dielectric response functions of solids. Phys. Rev. Lett., \n74:1827, 1995. [22] P. R. Schreiner, N. L. Allinger, T. Clark, J. Gasteiger, P. Kollman, and H. F. Schaefer, \neditors. Encyclopedia of Computational Chemistry, chapter 1, pages 115 128. Wiley &#38; Sons, Berne, \nSwitzerland, 1998. [23] Y. Song. Compiler algorithms for ef.cient use of memory systems. PhD thesis, \nPurdue University, Nov. 2000. [24] Y. Song, C. Wang, and Z. Li. Locality enhancement by array contraction. \nIn 14th International Workshop on Languages and Compilers for Parallel Computing, Aug. 2001. [25] Y. \nSong, R. Xu, C. Wang, and Z. Li. Data locality enhancement by memory reduction. In 15th ACM International \nConference on Supercomputing, pages 50 64, Sorrento, Italy, June 2001.  \n\t\t\t", "proc_id": "512529", "abstract": "The accurate modeling of the electronic structure of atoms and molecules is very computationally intensive. Many models of electronic structure, such as the Coupled Cluster approach, involve collections of tensor contractions. There are usually a large number of alternative ways of implementing the tensor contractions, representing different trade-offs between the space required for temporary intermediates and the total number of arithmetic operations. In this paper, we present an algorithm that starts with an operation-minimal form of the computation and systematically explores the possible space-time trade-offs to identify the form with lowest cost that fits within a specified memory limit. Its utility is demonstrated by applying it to a computation representative of a component in the CCSD(T) formulation in the NWChem quantum chemistry suite from Pacific Northwest National Laboratory.", "authors": [{"name": "Daniel Cociorva", "author_profile_id": "81100229313", "affiliation": "Ohio State University", "person_id": "P348259", "email_address": "", "orcid_id": ""}, {"name": "Gerald Baumgartner", "author_profile_id": "81100581581", "affiliation": "Ohio State University", "person_id": "P96302", "email_address": "", "orcid_id": ""}, {"name": "Chi-Chung Lam", "author_profile_id": "81100237377", "affiliation": "Ohio State University", "person_id": "P348257", "email_address": "", "orcid_id": ""}, {"name": "P. Sadayappan", "author_profile_id": "81100364545", "affiliation": "Ohio State University", "person_id": "PP43119433", "email_address": "", "orcid_id": ""}, {"name": "J. Ramanujam", "author_profile_id": "81100351630", "affiliation": "Louisiana State University", "person_id": "PP43119236", "email_address": "", "orcid_id": ""}, {"name": "Marcel Nooijen", "author_profile_id": "81100143619", "affiliation": "Princeton University", "person_id": "P348269", "email_address": "", "orcid_id": ""}, {"name": "David E. Bernholdt", "author_profile_id": "81100126351", "affiliation": "Oak Ridge National Laboratory", "person_id": "P60613", "email_address": "", "orcid_id": ""}, {"name": "Robert Harrison", "author_profile_id": "81100010409", "affiliation": "Pacific Northwest National Laboratory", "person_id": "PP31023052", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/512529.512551", "year": "2002", "article_id": "512551", "conference": "PLDI", "title": "Space-time trade-off optimization for a class of electronic structure calculations", "url": "http://dl.acm.org/citation.cfm?id=512551"}