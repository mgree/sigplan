{"article_publication_date": "05-17-2002", "fulltext": "\n Pro.le-Guided Code Compression . Saumya Debray William Evans Department of Computer Science Department \nof Computer Science University of Arizona University of British Columbia Tucson, AZ 85721. Vancouver \nB.C. Canada, V6T 1Z4. debray@cs.arizona.edu will@cs.ubc.ca ABSTRACT As computers are increasingly used \nin contexts where the amount of available memory is limited, it becomes important to devise techniques \nthat reduce the memory footprint of application pro\u00adgrams while leaving them in an executable form. This \npaper de\u00adscribes an approach to applying data compression techniques to reduce the size of infrequently \nexecuted portions of a program. The compressed code is decompressed dynamically (via software) if needed, \nprior to execution. The use of data compression tech\u00adniques increases the amount of code size reduction \nthat can be achieved; their application to infrequently executed code limits the runtime overhead due \nto dynamic decompression; and the use of software decompression renders the approach generally applicable, \nwithout requiring specialized hardware. The code size reductions obtained depend on the threshold used \nto determine what code is infrequently executed and hence should be compressed: for low thresholds, we \nsee size reductions of 13.7% to 18.8%, on average, for a set of embedded applications, without excessive \nruntime over\u00adhead. Categories and Subject Descriptors D.3.4 [Programming Languages]: Processors code \ngeneration; compilers; optimization; E.4 [Coding and Information Theory]: Data Compaction and Compression \nprogram representation General Terms Experimentation, Performance Keywords Code compaction, code compression, \ncode size reduction, dynamic decompression .This work was supported in part by the National Science Foun\u00addation \nunder grants CCR-0073394, EIA-0080123, and CCR\u00ad0113633, the Natural Sciences and Engineering Research \nCouncil of Canada under grant NSERC-238828-01, and by a loan of equip\u00adment from the Alpha Development \nGroup of Compaq Computer Corp. Permission to make digital or hard copies of all or part of this work \nfor personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior \nspeci.c permission and/or a fee. PLDI 02, June 17-19, 2002, Berlin, Germany. Copyright 2002 ACM 1-58113-463-0/02/0006 \n...$5.00. 1. INTRODUCTION In recent years there has been an increasing trend towards the incorporation \nof computers into a wide variety of devices, such as palm-tops, telephones, embedded controllers, etc. \nIn many of these devices, the amount of memory available is limited, due to consid\u00aderations such as space, \nweight, power consumption, or price. For example, the widely used TMS320-C5x DSP processor from Texas \nInstruments has only 64 Kwords of program memory for executable code [23]. At the same time, there is \nan increasing desire to use more and more sophisticated software in such devices, such as en\u00adcryption \nsoftware in telephones, speech/image processing software in palm-tops, fault diagnosis software in embedded \nprocessors, etc. Since these devices typically have no secondary storage, an appli\u00adcation that requires \nmore memory than is available will not be able to run. This makes it desirable to reduce the application \ns runtime memory requirements for both instructions and data its memory footprint where possible. We \nfocus in this work on reducing the overall memory footprint by reducing the space required for in\u00adstructions. \nThe intuition underlying our work is very simple. Most pro\u00adgrams obey the so-called 80-20 rule, which \nstates, in essence, that most of a program s execution time is spent in a small portion of its code (see \n[17]); a corollary is that the bulk of a program s code is generally executed infrequently. Our work \naims at exploit\u00ading this aspect of programs by using compression techniques that yield smaller compressed \nrepresentations, but may require greater decompression effort at runtime, on infrequently executed portions \nof programs. The expectation is that the increased compression for the infrequently executed code will \ncontribute to a signi.cant im\u00adprovement in the overall size reduction achieved, but that the con\u00adcomitant \nincrease in decompression effort will not lead to a signi.\u00adcant runtime penalty because the code affected \nby it is infrequently executed. This apparently simple idea poses some interesting implemen\u00adtation challenges \nand requires non-trivial design decisions. These include the management of memory used to hold decompressed \nfunctions (discussed in Section 2); the design of an effective com\u00adpression/decompression scheme so that \nthe decompressor code is small and quick (Section 3); identi.cation of appropriate units for compression \nand decompression (Section 4); as well as optimiza\u00adtions that improve the overall performance of the \nsystem (Section 6). Our work combines aspects of pro.le-directed optimization, runtime code generation/modi.cation, \nand program compression. We discuss other related work in Section 8. call sites call sites     (a) \nOriginal  function offset table 0 1   Decompressor compressed code runtime buffer  f g h  (b) Compressed \n Figure 1: Code Organization: Before and After Compression 2. THE BASIC APPROACH 2.1 Overview Figure \n1 shows The basic organization of code in our system. Consider a program with three infrequently executed \nfunctions,1 f, g and h, as shown in Figure 1(a). The structure of the code after compression is shown \nin Figure 1(b). The code for each of these functions is replaced by a stub (a very short sequence of \ninstruc\u00adtions) that invokes a decompressor whose job is to decompress the code for a function into the \nruntime buffer and then to transfer con\u00adtrol to this decompressed code. A function offset table speci.es \nthe location within the compressed code where the code for a given function starts. The stub for each \ncompressed function passes an argument to the decompressor that is an index into this table; this argument \nis indicated in Figure 1(b) by the label ([0], [1], etc.) on the edge from each stub to the decompressor. \nThe decompressor uses this argument to index into the function offset table, retrieve the start address \nof the compressed code for the appropriate func\u00adtion, and start generating uncompressed executable code \ninto the runtime buffer. Decompression stops when the decompressor en\u00adcounters a sentinel (an illegal \ninstruction) that is inserted at the end of the code for each function. The decompressor then (.ushes \nthe instruction cache, then) transfers control to the code it has gener\u00adated in the runtime buffer. When \nthis decompressed function .n\u00adishes its execution, it returns to its caller in the usual way. Since the \ncontrol transfers from the stubs to the decompressor, and from the decompressor to the runtime buffer \ndo not alter the return address transmitted from the original call site, no special action is necessary \nto return from a decompressed function to its call site. This method partitions the original program \ncode into two parts. Infrequently executed functions (such as f, g, and h) are placed in a compressed \ncode part, while frequently executed functions re\u00admain in a never-compressed part. The stub code that \nmanages con\u00adtrol transfers to compressed functions must also lie in the never\u00adcompressed part. It is \nimportant to note that when comparing the space usage of the original and compressed programs, the latter \nmust take into account the space occupied by the stubs, the decompressor, the function offset table, \nthe compressed code, the runtime buffer, and the never\u00adcompressed original program code. lOur implementation \nuses a notion of function that is somewhat more general than the usual connotation of this term in source \nlan\u00adguage programs. We discuss exactly what constitutes such a func\u00adtion in Section 4.  2.2 Buffer Management \nThe scheme described above is conceptually fairly straightfor\u00adward but fails to mention several issues \nwhose resolution deter\u00admines its performance. The most important of these is the issue of function calls \nin the compressed code. Suppose that in Figure 1, the code for fcontains a call to g. Since fis compressed, \nthe call site is in the runtime buffer when the call is executed. As described above, this call will \nbe to the stub for g, and the code for g will be decompressed and executed as expected. What happens \nwhen g returns? The return address points to the instruction following the call in f. This is a problem: \nthe instructions for f were over\u00adwritten when g was decompressed. The return address points to a location \nin the runtime buffer that now contains g s code. The question that we have to address, therefore, is: \nIf a function call is executed from the runtime buffer, how can we guarantee that the correct code will \nbe executed when the call returns? The an\u00adswer to this question is inextricably linked with the way we \nchoose to manage the runtime buffer. We have the following options for buffer management: 1. We may simply \navoid the problem by refusing to compress any function whose body contains any function calls, since \nthese may result in a function call from within the runtime buffer. We reject this option because it \nseverely limits the amount of code that can be subjected to compression. 2. We may choose to ensure \nthat the decompressed code for a function is never overwritten until after all function calls within \nits body have returned. The simplest way to do this is never to discard the decompressed code for a function. \nIn this case, the compressed code for a function is decompressed at most once the .rst time it is called \nwith subsequent calls bypassing the decompressor and entering the decompressed code directly. This conceptually \nresembles the behavior of just-in-time compilers that translate interpretable code to na\u00adtive code [1, \n22].  An alternative is to discard the decompressed code for a func\u00adtion when it is no longer on the \ncall stack, since at this point we can be certain that any function called by it has returned to it already. \nThis is the approach taken by Lucco [19], though rather than immediately discarding a function after \nexecution, he caches the function in the hope that it might be re-executed. The Smalltalk-80 system also \nextracts an exe\u00adcutable version of a function from an intermediate represen\u00adtation when the procedure \nis .rst invoked [8]. It caches the f: entry 0 offset instruction EntryStub: bsr r, Decompress <index(f), \n0> cs0 bsr $ra, g ... 96 97 return never-compressed RestoreStub(f,98): bsr $ra, Decompress <index(f), \n98> <count> runtime stub list instruction f: offset entry 0 cs0 bsr $ra, CreateStub 96 br g 97 ... 98 \nreturn runtime buffer (a) Original (b) Transformed, during runtime after CreateStub has created Re-storeStub(f,98) \nFigure 2: Managing Function Calls Out of the Runtime Buffer. executable code, and only discards it to \nprevent the system from running out of memory. The main drawback with this approach is that the runtime \nbuffer must be made large enough to hold all of the decom\u00adpressed functions that can possibly coexist \non the call stack. In the worst case, this is the entire program. The resulting memory footprint which \nincludes the space needed for the runtime buffer as well as the stubs, the decompressor, and the function \noffset table will therefore be bigger than that of the original program. This approach is therefore \nnot suitable for limited-memory devices. 3. When a decompressed function f calls a function g from within \nthe runtime buffer, we may choose to allow the de\u00adcompressor to overwrite f s code within the buffer. \nThis is the approach used in our implementation. This has the ben\u00ade.t that we only need a runtime buffer \nlarge enough to hold the code for the largest compressed function. As pointed out above, however, this \nmeans that when the call from greturns, the runtime buffer may no longer hold the correct instruc\u00adtions \nfor it to return to. This problem can be solved if we can ensure that the code for f is restored into \nthe runtime buffer between the point where the callee g returns and the point where control is transferred \nto the caller f. We discuss below how this can be done. Suppose that a function f within the runtime \nbuffer calls a com\u00adpressed function g. In our scheme, this causes the decompressor to overwrite f s code \nin the buffer with g s code. For correctness, we have to restore f s code to the buffer after the call \nto g returns but before control is transferred to the appropriate instruction within f. Since we don \nt have any additional storage area where f s code could be cached, restoring f s code to the runtime \nbuffer requires that it be decompressed again. This means that when control re\u00adturns from g, it must \n.rst be diverted to the decompressor, which can then decompress fand transfer control to it. The decompressor \nmust also be given an additional argument specifying to where con\u00adtrol should be transferred in the decompressed \nfunction, since the program may (re-)enter f at some instruction other than its entry point. One option \nis to create a stub at compile time that contains the function call to g followed by code to call the \ndecompressor to restore fto the runtime buffer and transfer control to the instruction after f s call \nto g. This stub obviously cannot be placed in the runtime buffer, since it may be overwritten there; \nit must be placed in the never-compressed portion of the program. Since every call from a compressed \nfunction requires its own stub, these restore stubs amount to a large fraction of the .nal executable \ns size (e.g., if we only compress code that is never executed during pro.ling, we create restore stubs \nthat occupy 13%, on average, and for some programs 20% of the never-compressed code; if we compress code \nthat accounts for at most 1% of the instructions executed during pro.ling, the average percentage rises \nto 27%). Rather than creating all restore stubs at compile time, we instead create at runtime, when gis \ncalled, a temporary restore stub that ex\u00adists only until greturns. The transfer to gis prefaced with \ncode that generates the restore stub and makes the return address of the orig\u00adinal call point to this \nstub. Then an unconditional jump or branch is made to g. If every control transfer from compressed code \ncreated a restore stub, we would, in effect, be maintaining a call stack of calls from compressed code. \nIf the compressed code is recursive, this could require an arbitrarily large amount of additional space. \nInstead, we create only one restore stub for a particular call site in compressed code and maintain a \nusage count for that restore stub to determine when the stub is no longer needed. When asked to create \na restore stub, we .rst check to see if a stub for that call site already exists and, if it does, increase \nits usage count and use its address for the return address; otherwise we create a new restore stub with \nusage count equal to 1. In effect, this implements a simple reference\u00adcount-based garbage collection \nscheme for restore stubs. The text area of memory for a program now conceptually consists of three parts: \nthe never-compressed code; the runtime stub list; and the runtime decompression buffer (Figure 2(b)). \nOn return from g, the restore stub invokes the decompressor which recognizes that it has been called \nby a restore stub, decre\u00adments the stub s usage count, restores f to the runtime buffer, and transfers \ncontrol to the appropriate instruction. This runtime scheme never creates more restore stubs than the \ncompile-time scheme, though it does require an additional 8 bytes per stub in order to maintain the count. \nIn fact, the maximum num\u00adber of restore stubs that exist at one time in our test suite is 9 for a very \naggressive pro.le threshold of (0.01, i.e., where the code considered for compression accounts for 1% \nof the total dynamic instruction count of the pro.led program (see Section 5). Figure 2 illustrates how \nthis is done. Figure 2(a) shows a func\u00adtion f whose body contains a function call, at callsite cs0, that \ncalls g. The instruction bsr ., Label puts the address of the next instruction (the return address) into \nregister .and branches to Label. Callsite cs0 is at offset 96 within the body of f (relative to the beginning \nof f s code), and the return address it passes to its callee is that of the following instruction, which \nis at offset 97. Figure 2(b) shows the result of transforming this code so that the decompressor is called \nwhen the call to greturns. The function call to g at cs0 is replaced by a function call to CreateStub \nusing the same return address register $ra. CreateStubcreates a re\u00adstore stub for this call site (or \nuses the existing restore stub for this call site if it exists) and changes $rato contain the stub s \naddress. It then transfers control to an unconditional branch at offset 97 that transfers control to \ng. Note that the single original instruction bsr $ra, g becomes two instructions in the runtime buffer. \nTo save space in the compressed code, these two instructions are created by the decompressor from the \nsingle bsr $ra, g when .lling the runtime buffer. When greturns, the instructions in the restore stub \nare executed. This causes the decompressor to be invoked with the argument pair <index(f), 98>, where \nindex(f) is f s index within the function offset table, and 98 is the offset within f s code where control \nshould be transferred after decompression. The overall ef\u00adfect is that when control returns from the \nfunction call, f s code is decompressed, after which control is transferred to the instruction following \nthe function call in the original code. It is important to note that, in the scheme described above, \nthe call stack of the original and compressed program are exactly the same size at any point in the program \ns execution. In fact, there is no need to modify the return sequence of any function. A function gmay \nbe called from either the runtime buffer or never-compressed code and, in general, may have call sites \nin both. If the call site is in a never-compressed function, CreateStub is not invoked and greturns to \nthe instruction following the call instruction in the usual way. If the call site is in compressed code, \nthen the return address passed to g is that of the corresponding restore stub, and control transfers \nto this stub when greturns. It is not hard to see, in fact, that the control transfers happen correctly \nregardless of how g uses the return address passed to it: for example, g may save this address in its \nenvironment at entry and restore it on exit; or keep it in a register, if it is a leaf function; or pass \nthe return address to some other function, if tail-call optimization is carried out. In some cases, such \nas control transfers through longjmp,a function may be returned from without a corresponding call. This \nmeans that the usage count for the callsite s restore stub may be inaccurate or, even worse, the restore \nstub may no longer exist. For this reason, functions that call setjmpare not compressed.  2.3 Decompressor \nInterface The decompressor is invoked with two arguments: an index in the function offset table, indicating \nthe function to be decompressed; and an offset in the runtime buffer, indicating the location in the \nruntime buffer where control should be transferred after decom\u00adpression. Rather than pass these arguments \nto the decompressor in a register, we put them in a dummy instruction, called a tag, that follows the \ncall to the decompressor: the low 16 bits contain the offset and the high 16 bits the function index. \nSince the decom\u00adpressor never returns to its caller (instead it transfers control to the function it \ndecompresses into the runtime buffer), this instruction is never executed. We can, however, access it \nvia the return address set by the call to the decompressor. Various registers may be used as the return \naddress register on a call to the decompressor. For a restore stub, the register that was used in the \noriginal call instruction can be used; it is guaranteed to be free. For an entry stub, any free register \nwill do. (If no register is free, we push the value of a register $ra, use $ra, and then restore it at \nthe end of the decompressor.) The decompressor, however, must know which register contains the return \naddress when it is called. We accomplish this by giving the decompressor multiple entry points, one per \npossible return address register. The entry point for register .pushes .onto the stack and then jumps \nto the body of the decompressor. The decompressor now knows that the return address is at the top of \nthe stack. The decompressor then 1. saves all registers that it will use on the stack, 2. places an \ninstruction at the start of the runtime buffer that unconditionally jumps to the offset provided by the \ntag, 3. .lls the rest of the runtime buffer by decompressing the func\u00adtion indicated by the tag, 4. \nrestores all saved registers, and 5. unconditionally jumps to the start of the runtime buffer (which \nimmediately jumps to the appropriate offset).  By creating the unconditional jump instruction in the \nruntime buffer, we avoid the need for a register to do the control transfer from the end of the decompressor \nto the offset within the runtime buffer. We insert one other instruction before this jump instruction \nthat sets the return register to the address of a restore stub (when creating a stub) or restores $ra \n(when an entry stub has no free register). We note that CreateStuband Decompressare con\u00adtained in the \nsame function. This saves having multiple entry points (one per possible return address register) in \ntwo functions, and it is easy to determine from the return address whether the function was called from \ninside the runtime buffer (when it should act as Cre\u00adateStub) or outside (when it should act as Decompress). \n 3. COMPRESSION &#38; DECOMPRESSION Our primary consideration in choosing a compression scheme is minimizing \nthe size of the compressed functions. We would like to achieve good compression even on very short sequences \nof in\u00adstructions since the functions we may want to compress can be very small. A second consideration \nis the size of the decompressor itself since it becomes part of the memory footprint of the program. \nFi\u00adnally, the decompressor must be fast since it is invoked every time control transfers to a compressed \nfunction that is not already in the runtime buffer. Since the functions that we choose to compress have \na low execution count, we don t expect to invoke the decom\u00adpressor too often during execution. A faster \ndecompressor, how\u00adever, means we can tolerate the compression of more frequently executed code which, \nin turn, leads to greater compression oppor\u00adtunities. The compression technique that we use is a simpli.ed \nversion of the splitting streams approach [9]. The data to be compressed consists of a sequence of machine \ncode instructions. Each instruc\u00adtion contains an opcode .eld and several operand .elds, classi.ed by \ntype. For example, in our test platform, a branch instruction consists of a 6-bit opcode .eld, a 5-bit \nregister .eld, and a 21-bit displacement .eld [2]. In order to compress a sequence of instruc\u00adtions, \nwe .rst split the sequence into separate streams of values, one per .eld type, by extracting, for each \n.eld type, the sequence of .eld values of that type from successive instructions. We then compress each \nstream separately. For our test platform, we split the instructions into 15 streams. Note that no instruction \ncontains all 15 .eld types. To reconstruct the instruction sequence, we decompress an op\u00adcode from the \nopcode stream. This tells us the .eld types of the instruction, and we obtain the .eld values from the \ncorresponding streams. We repeat this process until the opcode stream is empty. We compress each stream \nby encoding each .eld value in the stream using a Huffman code that is optimal for the stream. This is \na two-pass process. The .rst pass calculates the frequency of the .eld values and constructs the Huffman \ncode. The second pass encodes the values using the code. Since the Huffman code is de\u00adsigned for each \nstream, it must be stored along with the encoded stream in order to permit decompression. We use a variant \nof Huffman encoding called canonical Huffman encoding that permits fast decompression yet uses little \nmemory [5]. Like a Huffman code, a canonical Huffman code is an optimal character-based code (the characters \nin this case are the .eld val\u00adues). In fact, the length of the canonical Huffman codeword for a character \nis the same as the length of the Huffman codeword for that character. Thus the number N[i]of codewords \nof length iin both encodings is the same. The codewords of length iin the canoni\u00adcal Huffman code are \nthe N[i], i-bit numbers bi,bi+1,...,bi+ N[i]-1where bl0and bi2(bi-l+N[i-1])for i?2. For example, if N[2] \n3, N[3] 1, and N[5] 4(and N[i] 0otherwise) then bl 0 ,b2 0 , b 3 , b4 1 4 ,b5 2 and the codewords are \n 00,01,10,110,11100,11101,11110,11111. Notice that the codewords are completely determined given the \nnumber of codewords of each length, i.e., the N[i] s. We store the ncharacters to be encoded in an array \nD[0...n-1] ordered by their codeword value. The advantage of the canonical Huffman code is that a codeword \ncan be rapidly decoded using the arrays N[i]and D[j]. DECODE() v.0, b.0, j.0, i.0 do v.2v+NEXTBIT() b.2(b+N[i]) \nj.j+N[i] i.i+1 while (v?b+N[i]) return D[j+v-b] The compressed program consists of the codeword sequence, \ncode representation (the array N[i]), and value list (the array D[j]) for each stream. In fact, since \nevery instruction begins with an opcode that completely speci.es the remaining .elds of the in\u00adstruction, \nwe can merge the codeword sequences of the individual streams into one sequence. We simply interpret \nthe .rst bits of the codeword sequence using the Huffman code for the opcode stream, and use the decoded \nopcode to specify the appropriate Huffman codes to use for the remaining .elds. For example, when decoding \na branch instruction, we would read a codeword from the sequence using .rst the opcode code, then the \nregister code, and .nally the displacement code. The total space required by the compressed program is \napproximately 66% of its original size. We can achieve somewhat better compression for some streams using \nmove-to-front coding prior to Huffman coding. This has the undesirable affect of increasing the code \nsize and running time of the decompression algorithm. Other approaches that decompress larger parts of \nan instruction, or multiple instructions, in one de\u00adcompression operation may result in better and faster \ndecompres\u00adsion, but these approaches typically require a more complex de\u00adcompression algorithm, or one \nthat requires more space for data structures.  4. COMPRESSIBLE REGIONS The functions that we use as \na unit of compression and decom\u00adpression may not agree with the functions speci.ed by the program. It \nis often the case that a program-speci.ed function will contain some frequently-executed code that should \nnot be compressed, and some infrequently-executed (cold) code that should be compressed. If the unit \nof compression is the program-speci.ed function then the entire function cannot be compressed if it contains \nany code that cannot be considered for compression. As a result, the amount of code available for compression \nmay be signi.cantly less than the total amount of cold code in the program. In addition, the runtime \nbuffer must be large enough to hold the largest decompressed function. A single large function may often \naccount for a signi.cant fraction of the cold code in a program. Having a runtime buffer large enough \nto contain this function can offset most of the space-savings due to compression. To address this issue, \nwe create functions from arbitrary code regions and allow these regions to be compressed and decompressed. \nThis means that control transfers into and out of a compressed re\u00adgion of code may no longer follow the \ncall/return model for func\u00adtions. For example, we may have to contend with a conditional branch that \ngoes from one compressed region of code to another, different, compressed region. Since the runtime buffer \nholds the code of at most one such region at any time, a branch from one re\u00adgion to another must now \ngo through a stub that invokes the decom\u00adpressor. This is not a terrible complication. A compressed region \nmight have multiple entry points, each of which requires an entry stub, but in all other ways it is the \nsame as an original function. For instance, function calls from within a compressed region are still \nhandled as discussed in Section 2. We now face the problem of how to choose regions to com\u00adpress. We \nwant these regions to be reasonably small so that the runtime buffer can be small, yet we want few control \ntransfers be\u00adtween different regions so that the number of entry stubs is small. This is an optimization \nproblem. The input is a control .ow graph G(, )for a program in which a vertex brepresents a basic block \nand has size IbIequal to the number of instructions in the block, and an edge (a,b)represents a control \ntransfer from ato b. In addition, the input speci.es a subset Uof the vertices that can be compressed. \nThe output is a partition of a subset Sof the compressible vertices Uinto regions Rl,R2,...,Rkso that \nthe following cost is minimized: . IbI never-compressed code bEV\\S k. + s(Ri) compressed code i=l +k \nfunction offset table +2I.Ientry stubs . + a{i+IbI}runtime buffer ibER. where s(Ri)is the size of the \nregion Riafter compression, .is the set of blocks requiring an entry stub, i.e., .{b(a,b)E,bERi,and a \nfor some i}, ERi the constant 2is the number of words required for an entry stub, and 1.20 1.20 d 1.10 \n1.10  Normalized code size Normalized code size Normalized code size  1.00 0.90 1.00 0.90 0.800.80 \nj jj j jj j j jj jj 0.70 0.70 32 64 128 256 512 1024 2048 4096 32 64 128 256 512 1024 2048 4096 32 64 \n128 256 512 1024 2048 4096 Buffer size bound Buffer size bound Buffer size bound (a) (0.0 (b) (0.00001 \n(c) (0.00005 Key:  0.0 0.00001 0.00005  (d) mean Figure 3: Effect of Buffer Size Bound on Code Size \na: adpcm b: epic c: g721 dec d: g721 enc e: gsm f: jpeg dec g: jpeg enc h: mpeg2dec i: mpeg2enc j: pgp \nk: rasta   iis the number of external function calls within Ri(the decom\u00adpressor creates an additional \ninstruction for each such call). Note that we have not included the size of the restore stub list (calculat\u00ading \nits size, even given a partition, is an NP-hard problem). In practice, we cannot afford to calculate \ns(R)for all possible regions R, so we assume that a .xed compression factor of I1 . applies to all regions \n(i.e., s(R)IbERIbI). Unfortunately, the resulting simpli.ed problem is NP-hard (PARTITION reduces to \nit). We resort to a simple heuristic to choose the compressible regions. We .rst decide which basic blocks \ncan be compressed. Our cri\u00adteria for this decision are discussed in more detail in Section 5. We also \n.x an upper bound Kon the size of the runtime buffer (our cur\u00adrent implementation uses an empirically \nchosen value of K512 bytes; this is determined as described below). We create an initial set of regions \nby performing depth-.rst search in the control .ow graph. We limit the depth-.rst search so that it produces \na tree that contains at most Kinstructions and is composed of compressible blocks from a single function. \nIf it is pro.table to compress the set of blocks in the tree, we make this tree a compressible region; \notherwise, we mark the root of the tree so that we never re-initiate a depth-.rst search from it (though \nit might be visited in a subsequent depth-.rst search starting from a different block). We continue the \ndepth-.rst search until all compressible blocks have been visited. To decide if a region containing 1instructions \nis pro.table to compress, we compare (1-I)1, the number of instructions saved by compressing the region, \nwith the number of instructions added for entry stubs. If (1-I)1, the region is pro.table to com\u00adpress. \nAs mentioned above, we use an empirically determined upper bound Kon the size of the runtime buffer to \nguide the partition\u00ading of functions into compressible regions. If we choose too small a value for K, \nwe get a large number of small compressible re\u00adgions, with a correspondingly large number of entry stubs \nand func\u00adtion offset table entries. These tend to offset the space bene.ts of having a small runtime \nbuffer, resulting in a large overall memory footprint. If the value of Kis too large, we get a smaller \nnumber of distinct compressible regions and function offset table entries, but the savings there are \noffset by the space required for the run\u00adtime buffer. Our empirical observations of the variation of \noverall code size, as Kis varied, are shown in Figure 3, for three differ\u00adent thresholds (of cold code \nas well as the mean for each of these thresholds (other values of (yield similar curves). It can be seen \nthat, for these benchmarks at least, the smallest overall code size is obtained at K25and K512. We prefer \nthe latter value be\u00adcause the larger runtime buffer means that we get somewhat larger regions and correspondingly \nfewer inter-region control transfers; this results in fewer calls to the decompressor at runtime and \nyields somewhat better performance. The partition obtained by depth-.rst search, in practice, typically \ncontains many small regions. This is partly due to the presence of small functions in user and library \ncode, and partly due to frag\u00admentation. This incurs overheads from two sources: .rst, each compressible \nregion requires a word in the function offset table; and second, inter-region control transfers require \nadditional code in the form of entry or restore stubs to invoke the decompressor. These overheads can \nbe reduced by packing several small regions into a single larger one that still contains at most Kinstructions. \nTo pack regions, we start with the set of regions created by the depth-.rst search and repeatedly merge \nthe pair that yields the most savings (without exceeding the instruction bound K) until no such ' pairs \nexist. For the pair of regions {R,R}(and for Rswapped ' with Rin the following), we save an entry stub \nfor every basic Fraction of Code 1.00 0.90 0.80 0.70 0.60 0.50 0.40 0.30 0.20 0.10 0.00 Figure 4: Amount \nof Cold and Compressible Code (Normal\u00adized) block in region Rthat has incoming edges from R '(and possibly \nfrom R) but from no other region. For every call from region Rto R ', we save a restore stub. We may \nalso save a jump instruction for every fall-through edge from region Rto R ' . In principle, the packing \nof regions in this way involves a space\u00adtime tradeoff: packing saves space, but since each region is \nde\u00adcompressed in its entirety before execution, the resulting larger re\u00adgions incur greater decompression \ncost at runtime. However, given that only infrequently-executed code is subjected to runtime de\u00adcompression, \nthe actual increase in runtime cost is not signi.cant.  5. IDENTIFYING COLD CODE The discussion so far \nhas implicitly assumed that we have iden\u00adti.ed portions of the program as cold and, therefore, candidates \nfor compression. The determination of which portions of the pro\u00adgram are cold is carried out as follows. \nWe start with a threshold (, 0.0<(<1.0, that speci.es the maximum fraction of the total number of instructions \nexecuted at runtime (according to the execu\u00adtion pro.le for the program) that cold code can account for. \nThus, (0.25means that all of the code identi.ed as cold should ac\u00adcount for at most 25% of the total \nnumber of instructions executed by the program at runtime. Let the weight of a basic block be the number \nof instructions in the block multiplied by its execution frequency, i.e., the block s contribution to \nthe total number of instructions executed at runtime. Let tot instr ct be the total number of instructions \nexecuted by the program, as given by its execution pro.le. Given a value of (, we consider all basic \nblocks bin the program in increasing order of execution frequency, and determine the largest execution \nfrequency Nsuch that . weight(b)<(.totinstrt. b:freq(b).N Any basic block whose execution frequency is \nat most Nis consid\u00adered to be cold. Figure 4 shows (the geometric mean of) the relative amount of cold \nand compressible code in our programs at different thresholds. It can be seen, from Figure 4, that the \namount of cold code varies from about 73% of the total code, on average, when the threshold (0.0(where \nonly code that is never executed is considered cold) to about 94% at (0.01(the cold code accounts for \n1% of the total number of instructions executed by the program at runtime), to 100% at (1.0. However, \nnot all of this cold code can be compressed: the amount of compressible code varies from about 69% of \nthe program at (0.0to about 90% at (0.01,to about 96% at (1.0. The reason not all of the cold code is \ncompressible, at any given threshold, is that, as discussed in Section 4, a region of code may not be \nconsidered for compression even if it is cold, because it is not pro.table to do so.  6. OPTIMIZATIONS \n6.1 Buffer-Safe Functions As discussed earlier, function calls within compressed code cause the creation, \nduring execution, of a restore stub and an additional instruction in the runtime buffer. This overhead \ncan be avoided if the callee is buffer-safe, i.e., if it and any code it might call will not invoke the \ndecompressor. If the callee is buffer-safe, then the run\u00adtime buffer will not be overwritten during the \ncallee s execution, so the return address passed to the callee can be simply the address of the instruction \nfollowing the call instruction in the runtime buffer: there is no need to create a stub for the call \nor to decompress the caller when the call returns. In other words, a call from within a compressed region \nto a buffer-safe function can be left unchanged. This has two bene.ts: the space cost associated with \nthe restore stub and the additional runtime buffer instruction is eliminated, and the time cost for decompressing \nthe caller on return from the call is avoided. We use a straightforward iterative analysis to identify \nbuffer-safe functions. We .rst mark all regions that are clearly not buffer-safe: i.e., those that have \nbeen identi.ed as compressible, and those that contain indirect function calls whose possible targets \nmay include non-buffer-safe regions. This information is then propagated itera\u00adtively to other regions: \nif Ris a region marked as non-buffer-safe, and R 'is a region from which control can enter R either through \na function call or via a branch operation then R 'is also marked as being non-buffer-safe. This is repeated \nuntil no new region can be marked in this way. Any region that is left unmarked at the end of this process \nis buffer-safe. For the benchmarks we tested, this analysis identi.es on the average, about 12.5% of \nthe compressible regions as buffer-safe; the gsm and g721 enc benchmarks have the largest proportion \nof buffer-safe regions, with a little over 20% and 19%, respectively, of their compressible regions inferred \nto be buffer-safe. 6.2 Unswitching If a code region contains indirect jumps through a jump table, it \nis necessary to process any such code to ensure that runtime con\u00adtrol transfers within the decompressed \ncode in the runtime buffer are carried out correctly. We have two choices: we can either up\u00addate the \naddresses in the jump table to point into the runtime buffer, at the locations where the corresponding \ntargets would reside when the region is decompressed; or we can unswitch the region to use a series of \nconditional branches instead of an indirect jump through a table. Note that in either case, we have to \nknow the size of the jump table: in the context of a binary rewriting implementation such as ours, this \nmay not always be possible. If we are unable to determine the extent of the jump table, the block containing \nthe in\u00addirect jump through the table and the set of possible targets of this jump must be excluded from \ncompression. For the sake of sim\u00adplicity, our current implementation uses unswitching to eliminate the \nindirect jump, after which the space for the jump table can be reclaimed.  7. EXPERIMENTAL RESULTS \nOur ideas have been implemented in the form of a binary-rewriting tool called squash that is based on \nsqueeze, a compactor of Compaq Alpha binaries [7]. Squeeze is based on alto, a post-link-time code optimizer \n[20]. Squeeze alone compacts binaries that have already  Code Size reduction (%) Figure 5: Inputs used \nfor pro.ling and timing runs 30 20 10 0  abcdefghijk M abcdefghijk M abcdefghijk M abcdefghijk M abcdefghijk \nM abcdefghijk M abcdefghijk M 0.0 0.00001 0.0001 0.001 0.01 0.1 1.0 Thresholds Key: a: adpcm d: g721 \nenc g: jpeg enc j: pgp b: epic e: gsm h: mpeg2dec k: rasta c: g721 dec f: jpeg dec i: mpeg2enc M: GEOM.MEAN \nFigure 6: Code Size Reduction due to Pro.le-Guided Code Compression at Different Thresholds   been \nspace optimized by about 30% on average. Squash, using the runtime decompression scheme outlined in this \npaper, compacts squeezed binaries by about another 14 19% on average. To evaluate our work we used eleven \nembedded applications from the MediaBench benchmark suite (available at www.cs.ucla. edu/ leec/mediabench): \nadpcm, which does speech com\u00adpression and decompression; epic, an image data compression util\u00adity; g721 \ndec and g721 enc, which are reference implementations from Sun Microsystems of the CCITT G.721 voice \ncompression decoder and encoder; gsm, an implementation of the European GSM 06.10 provisional standard \nfor full-rate speech transcoding; jpeg dec and jpeg enc, which implement JPEG image decompres\u00adsion and \ncompression; mpeg2dec and mpeg2enc, which implement MPEG-2 decoding and encoding respectively; pgp, a \npopular cryp\u00adtographic encryption/decryption program; and rasta, a speech-analysis program. The inputs \nused to obtain the execution pro.les used to guide code compression, as well as those used to evaluate \nexecu\u00adtion speed (Figure 7(b)), are described in Figure 5: the pro.ling inputs refer to those used to \nobtain the execution pro.les that were used to carry out compression, while the timing inputs refer to \nthe inputs used to generate execution time data for the uncompressed and compressed code. Details of \nthese benchmarks are given in the Appendix.   These programs were compiled using the vendor-supplied \nC com\u00adpiler cc V5.2-036, invoked as cc -O1, with additional .ags in\u00adstructing the linker to retain relocation \ninformation and to produce statically linked executables.2 The vendor-supplied compiler cc produces the \nmost compact code at optimization level -O1: it car\u00adries out local optimizations and recognition of common \nsubexpres\u00adsions; global optimizations including code motion, strength reduc\u00adtion, and test replacement; \nsplit lifetime analysis; and code schedul\u00ad 2The requirement for statically linked executables is a result \nof the fact that alto relies on the presence of relocation information to dis\u00adtinguish addresses from \ndata. The Tru64 Unix linker ldrefuses to retain relocation information for executables that are not statically \nlinked. ing; but not size-increasing optimizations such as inlining; integer multiplication and division \nexpansion using shifts; loop unrolling; and code replication to eliminate branches. The programs were \nthen compacted using squeeze. Squeeze elim\u00adinates redundant, unreachable, and dead code; performs interpro\u00adcedural \nstrength reduction and constant propagation; and replaces multiple similar program fragments with function \ncalls to a single representative function (i.e., it performs procedural abstraction). Squeeze is very \neffective at compacting code. If we start with an ex\u00adecutable produced by cc -O1 and remove unreachable \ncode and no-opinstructions, squeeze will reduce the number of instructions that remain by approximately \n30% on average. The remaining instructions were given to squash along with pro\u00ad.le information obtained \nby running the original executable on sam\u00adple inputs to obtain execution counts for the program s basic \nblocks. Squash produces an executable that contains never-compressed code, entry stubs, the function \noffset table, the runtime decompressor, the compressed code, the buffer used to hold dynamically generated \nstubs, and the runtime buffer. All of this space is included in the code size measurement of squashed \nexecutables. Figure 6 shows how the amount of code size reduction obtained using pro.le-guided compression \nvaries with the cold code thresh\u00adold (. With (0.0, only code that is never executed is con\u00adsidered to \nbe cold; in this case, we see size reductions ranging from 9.0% (g721 enc) to 22.1% (pgp), with a mean \nreduction of 13.7%. The size reductions obtained increase as we increase (, which makes more and more \ncode available for compression. Thus, at (0.00001we have size reductions ranging from 12.1% (ad\u00adpcm) \nto 23.7% (pgp), with a mean reduction of 16.8%. At the extreme, with (1.0, i.e., all code considered \ncold, the code size reductions range from 21.5% (adpcm) to 31.8% (pgp), with a mean of 26.5%. It is noteworthy \nthat much of the size reductions are obtained using quite low thresholds, and that the rate at which \nthe reduction in code size increases with (is quite small. For ex\u00adample, increasing (by .ve orders of \nmagnitude, from 0.00001to 1.0, yields only an additional 10% bene.t in code size reduction. However, \nas (is increased, the runtime overhead associated with repeated dynamic decompression of code quickly \nbegins to make itself felt. Our experience with this set of programs (and others) indicates that beyond \n(0.0001the runtime overhead becomes quite noticeable. To obtain a reasonable balance between code size \nimprovements and execution speed, we focus on values of (up to 0.00005. Execution time data were obtained \non a workstation with a 667 MHz Compaq Alpha 21264 EV67 processor with a split two-way set-associative \nprimary cache (64 Kbytes each of instruction and data cache) and 512 MB of main memory running Tru64 \nUnix. In each case, the execution time was obtained as the smallest of 10 runs of an executable on an \notherwise unloaded system. Figure 7 examines the performance of our programs, both in terms of size and \nspeed, for (ranging from 0.0 to 0.00005. The .\u00adnal set of bars in this .gure shows the mean values for \ncode size re\u00ad (b) Execution Time Figure 7: Effect of Pro.le-Guided Compression on Code Size and Execution \nTime adpcm adpcm epic epic g721_dec g721_dec g721_enc g721_enc gsm gsm jpeg_dec jpeg_enc mpeg2dec mpeg2enc \npgp rasta Geom. Mean jpeg_dec (a) Code Size jpeg_enc mpeg2dec mpeg2enc pgp rasta Geom. Mean  duction \nand execution time, respectively, relative to squeezed code; the number at the top of each bar gives \nthe actual value of the ge\u00adometric mean for that case. It can be seen that at low cold-code thresholds, \nthe runtime overhead incurred by pro.le-guided code compression is small: at (0.0the compressed code \nis about the same speed, on average, as the code without compression; at (0.00001we incur an average \nexecution time overhead of 4%; and at (0.00005the average overhead is 24%. Given the corresponding size \nreductions obtained ranging from 13.7% to 18.8% these overheads do not seem unreasonably high. (Note \nthat these reductions in size are on top of the roughly 30% code size reduction we obtain using our prior \nwork on code compaction [7].) It is important to note, in this context, that the execution speed of compressed \ncode can suffer dramatically if the timing inputs, i.e., inputs used to measure actual execution speed, \ncause a large number of calls to the decompressor. This can happen for two rea\u00adsons. First, a code fragment \nthat is cold in the pro.le may occur in a cycle, which can be either a loop within a procedure, or an \ninter-procedural cycle arising out of recursion. Second, the region partitioning algorithm described \nin Section 4 may split a loop into multiple regions. In either case, if the loop or cycle is executed \nrepeatedly in the timing inputs, the repeated code decompression can have a signi.cant adverse effect \non execution speed. An ex\u00adample of the .rst situation occurs in the SPECint-95 benchmark li, where an \ninterprocedural cycle, that is never executed in the pro\u00ad.le, is executed many times with the timing \ninput. An example of the second situation occurs in the benchmark mpeg2dec when the runtime buffer size \nbound Kis small (e.g., K12).  8. RELATED WORK Our work combines aspects of pro.le-directed optimization, \nrun\u00adtime code generation/modi.cation, and program compression. Dy\u00adnamic optimization systems, such as \nDynamo [4], collect pro.le information and use it to generate or modify code at runtime. These systems \nare not designed to minimize the memory footprint of the executable, but rather to decrease execution \ntime. They tend to focus optimization effort on hot code, whereas our compression ef\u00adforts are most aggressive \non cold code. More closely related is the work of Hoogerbrugge et al., who compile cold code into interpreted \nbyte code for a stack-based ma\u00adchine [14]. By contrast, we use Huffman coding to compress cold code, \nand dynamically uncompress the compressed code at runtime as needed. Thus, our system does not incur \nthe memory cost of a byte-code interpreter. There has been a signi.cant amount of work on architectural \nex\u00adtensions for the execution of compressed code: examples include Thumb for ARM processors [3], CodePack \nfor PowerPC proces\u00adsors [15], and MIPS16, for MIPS processors [16]. Special hard\u00adware support is used \nto expand each compressed instruction to its executable form prior to execution. While such an approach \nhas the advantage of not incurring the space overheads for control stubs and time overheads for software \ndecompression, the requirement for special hardware limits its general applicability. Lefurgy et al. \ndescribe a hybrid system where decompression is carried out mostly in software, but with the assistance \nof special hardware instructions to allow direct manipulation of the instruction cache [18]; decompression \nis carried out at the granularity of individual cache lines. Previous work in program compression has \nexplored the com\u00adpressibility of a wide range of program representations: source programs, intermediate \nrepresentations, machine codes, etc. [24]. The resulting compressed form either must be decompressed \n(and perhaps compiled) before execution [9, 10, 11] or it can be exe\u00adcuted (or interpreted [13, 21]) \nwithout decompression [6, 12]. The .rst method results in a smaller compressed representation than the \nsecond, but requires the time and space overhead of decompres\u00adsion before execution. We avoid requiring \na large amount of ad\u00additional space to place the decompressed code by choosing to de\u00adcompress small pieces \nof the code on demand, using a single, small runtime buffer. Similar techniques of partial decompression \nand decompression-on-the-.y have been used under similar situations [9, 19], but these techniques require \naltering the runtime operation or the hardware of the computer. Most of the earlier work on code compression \nto yield smaller executables treated an executable program as a simple linear se\u00adquence of instructions, \nand used a suf.x tree construction to iden\u00adtify repeated code fragments that could be abstracted out \ninto func\u00adtions [6, 12]. We have recently shown that it is possible to obtain re\u00adsults that are as good, \nor better, by using aggressive inter-procedural size-reducing compiler optimizations applied to the control \n.ow graph of the program, instead of using a suf.x-tree construction over a linear sequence of instructions \n[7]. 9. CONCLUSIONS AND FUTURE WORK We have described an approach to use execution pro.les to guide code \ncompression. Infrequently executed code is compressed using data compression techniques that produce \ncompact representations, and is decompressed dynamically prior to execution if needed. This has several \nbene.ts: the use of powerful compression techniques allows signi.cant improvements in the amount of code \nsize reduc\u00adtion achieved; for low execution frequency thresholds the runtime overheads are small; and \n.nally, no special hardware support is needed for runtime decompression of compressed code. Experi\u00admental \nresults indicate that, with the proper choice of cold code thresholds, this approach can be effective \nin reducing the mem\u00adory footprint of programs without signi.cantly compromising ex\u00adecution speed: we \nsee code size reductions of 13.7% ((0.0) to 18.8% ((0.00005), on average, for a set of embedded ap\u00adplications, \nrelative to the code size obtained using our prior work on code compaction [7]; the concomitant effect \non execution time ranges from a very slight speedup for (0.0to a 27% slowdown, on average, for (0.00005. \nWe are currently looking into a number of ways to enhance this work further. These include other algorithms \nfor compression and decompression, as well as other algorithms for constructing com\u00adpressible regions \nwithin a program. Acknowledgements We gratefully acknowledge the loan of equipment by Karen Flat\u00adtery, \nRichard Flower, and Robert Muth of Compaq Corp. 10. REFERENCES [1] A.-R. Adl-Tabatabai,M. Cierniak, G.-Y. \nLueh, V. M. Parikh, and J. M. Stichnoth. Fast, Effective Code Generation in a Just-in-Time Java Compiler. \nProc. SIGPLAN 98 Conf. on Programming Language Design and Implementation (PLDI), June 1998, pp. 280 290. \n[2] Alpha Architecture Handbook, version 4. Compaq, October 1998. Available at http://www.support.compaq. \ncom/alpha-tools/documentation/current/ alpha-archt/alpha-architecture.pdf [3] ARM. An Introduction to \nThumb. Advanced RISC Machines Ltd., March 1995. Available at http://www.win.tue. nl/cs/pa/rikvdw/papers/ARM95.pdf \n[4] V. Bala, E. Duesterwald, and S. Banerjia. Dynamo: A Transparent Runtime Optimization System. In Proc. \n SIGPLAN 00 Conf. on Programming Language Design and Implementation, pages 1 12, June 2000. [5] I.H. \nWitten, A. Moffat, and T.C. Bell, Managing Gigabytes: Compressing and Indexing Documents and Images, \nVan Nostrand Reinhold, 1994. [6] K. D. Cooper and N. McIntosh. Enhanced code compression for embedded \nRISC processors. In Proc. SIGPLAN 99 Conf. on Programming Language Design and Implementation, pages 139 \n149, May 1999. [7] S. K. Debray, W. Evans, R. Muth, and B. de Sutter. Compiler Techniques for Code Compaction. \nACM Transactions on Programming Languages and Systems vol. 22 no. 2, March 2000, pp. 378 415. [8] P. \nDeutsch and A. Schiffman. Ef.cient implementation of the Smalltalk-80 system. In Proc. Symp. on Principles \nof Programming Languages, pp. 297 302, January 1984. [9] J. Ernst, W. Evans, C. Fraser, S. Lucco, and \nT. Proebsting. Code compression. In SIGPLAN 97 Conference on Programming Language Design and Implementation, \n1997, pp. 358 365. [10] M. Franz. Adaptive compression of syntax trees and iterative dynamic code optimization: \nTwo basic technologies for mobile-object systems. In J. Vitek and C. Tschudin, editors, Mobile Object \nSystems: Towards the Programmable Internet, LNCS vol. 1222, pp. 263 276. Springer, Feb. 1997. [11] M. \nFranz and T. Kistler. Slim binaries. Commun. ACM 40, 12 (Dec.), 87 94. [12] C. Fraser, E. Myers, and \nA. Wendt. Analyzing and compressing assembly code. In Proc. of the ACM SIGPLAN Symposium on Compiler \nConstruction, volume 19, pages 117 121, June 1984. [13] C.W. Fraser and T.A. Proebsting. Custom instruction \nsets for code compression. Unpublished manuscript. http://research.microsoft.com/ toddpro/ papers/pldi2.ps, \nOct. 1995. [14] J. Hoogerbrugge, L. Augusteijn, J. Trum, and R. Van De Wiel. A Code Compression System \nBased on Pipelined Interpreters. Software Practice and Experience 29(1), 1005 1023 (1999). [15] T. M. \nKemp, R. M. Montoye, J. D. Harper, J. D. Palmer, and D. J. Auerbach. A Decompression Core for PowerPC. \nIBM Journal of Research and Development vol. 42 no. 6, Nov. 1998. [16] K. D. Kissell. MIPS16: High-density \nMIPS for the Embedded Market. Proc. Real Time Systems 97 (RTS97), 1997. [17] D. E. Knuth. An Empirical \nStudy of FORTRAN Programs. Software Practice and Experience vol. 1, 105 133 (1971). [18] C. Lefurgy, \nE. Piccininni, and T. Mudge. Reducing Code Size with Run-Time Decompression. Proc. HPCA 2000, Jan 2000, \npp.218 227. [19] S. Lucco. Split-stream dictionary program compression. In SIGPLAN 00 Conference on Programming \nLanguage Design and Implementation, 2000, pp. 27 34. [20] R. Muth, S. K. Debray, S. Watterson, and K. \nDe Bosschere. alto: A Link-Time Optimizer for the DEC Alpha. Software Practice and Experience 31:67 101, \nJan. 2001. [21] T.A. Proebsting. Optimizing an ANSI C interpreter with superoperators. In Proc. Symp. \non Principles of Programming Languages, pages 322 332, Jan. 1995. [22] T. Suganuma, T. Ogasawara, M. \nTakeuchi, T. Yasue, M. Kawahito, K. Ishizaki, K. Komatsu, and T. Nakatani. Overview of the IBM Java Just-in-Time \nCompiler. IBM Program Code size (instrs) Input Squeeze adpcm 18228 11690 epic 33880 24769 g721 dec 15089 \n12008 g721 enc 15065 11771 gsm 29789 21597 jpeg dec 44094 37042 jpeg enc 38701 32168 mpeg2dec 37833 27942 \nmpeg2enc 47152 36062 pgp 83726 60003 rasta 91359 65273    Table 1: Code size data for the benchmarks \nSystems Journal vol. 39 no. 1, 2000, pp. 175 193. [23] Texas Instruments Inc. TMS320C5x User s Guide. \nLiterature No. SPRU056D, June 1998. [24] R. van de Wiel. The Code Compaction Bibliography. http://www.win.tue.nl/cs/pa/rikvdw/bibl. \nhtml. APPENDIX. BENCHMARK DATA Our benchmarks are taken from the MediaBench benchmark suite, available \nat http://www.cs.ucla.edu/ leec/mediabench. We used the following programs: adpcm, which does speech \ncom\u00adpression and decompression; epic, an image data compression util\u00adity; g721 dec and g721 enc, which \nare reference implementations from Sun Microsystems of the CCITT G.721 voice compression decoder and \nencoder; gsm, an implementation of the European GSM 06.10 provisional standard for full-rate speech transcoding; \njpeg dec and jpeg enc, which implement JPEG image decompres\u00adsion and compression; mpeg2dec and mpeg2enc, \nwhich implement MPEG-2 decoding and encoding respectively; pgp, a popular cryp\u00adtographic encryption/decryption \nprogram; and rasta, a speech-analysis program. Table 1 gives the number of instructions in each program: \nthe second column, labeled Input, gives the number of instruc\u00adtions in the input program after the initial \nelimination of unreach\u00adable code and noops; the third column, labeled Squeeze ,gives the number of instructions \nafter the application of our earlier code compaction tool, squeeze. The performance data given in this \npaper are relative to the third column of this table. The inputs used to obtain the execution pro.les \nused to guide code compression, as well as those used to evaluate execution speed, are described in Figure \n5: the pro.ling inputs refer to those used to obtain the execution pro.les that were used to carry out \ncompres\u00adsion, while the timing inputs refer to the inputs used to generate execution time data for the \nuncompressed and compressed code. These input .les are as follows. The various mlk IHaveADream.* .les \nwere derived from the .le oblakhs011u1.wav, a 728.6 KB audio .le of a speech by Martin Luther King Jr., \nobtained from www.britannica.com/blackhistory/audiov.html. The MPEG-2 .les sarnoff2.m2vand tceh v2.m2v, \nused for the benchmarks mpeg2dec and mpeg2enc, were obtained from http://bmrc.berkeley.edu/ftp/pub/mpeg/movies/ \nbitstreams/video/. The .le compression.psis PostScript for the paper [7], obtained using latex2e and \ndvi2ps, while the .le TI-320-user-manual.psis PostScript for a user manual for the TI-320 processor, \nobtained from the Texas Instruments web site. The various clinton.*.les, as well as the .le ex5 c1.wav, \nwere obtained as part of the Mediabench distribution.      \n\t\t\t", "proc_id": "512529", "abstract": "As computers are increasingly used in contexts where the amount of available memory is limited, it becomes important to devise techniques that reduce the memory footprint of application programs while leaving them in an executable form. This paper describes an approach to applying data compression techniques to reduce the size of infrequently executed portions of a program. The compressed code is decompressed dynamically (via software) if needed, prior to execution. The use of data compression techniques increases the amount of code size reduction that can be achieved; their application to infrequently executed code limits the runtime overhead due to dynamic decompression; and the use of software decompression renders the approach generally applicable, without requiring specialized hardware. The code size reductions obtained depend on the threshold used to determine what code is \"infrequently executed\" and hence should be compressed: for low thresholds, we see size reductions of 13.7% to 18.8%, on average, for a set of embedded applications, without excessive runtime overhead.", "authors": [{"name": "Saumya Debray", "author_profile_id": "81100148240", "affiliation": "University of Arizona, Tucson, AZ", "person_id": "P260626", "email_address": "", "orcid_id": ""}, {"name": "William Evans", "author_profile_id": "81100485414", "affiliation": "University of British Columbia, Vancouver B.C. Canada", "person_id": "PP31044304", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/512529.512542", "year": "2002", "article_id": "512542", "conference": "PLDI", "title": "Profile-guided code compression", "url": "http://dl.acm.org/citation.cfm?id=512542"}