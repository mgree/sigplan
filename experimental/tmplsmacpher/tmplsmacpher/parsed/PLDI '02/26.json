{"article_publication_date": "05-17-2002", "fulltext": "\n Denali: A Goal-directed Superoptimizer Rajeev Joshi Greg Nelson Keith Randall * Compaq Systems Research \nCenter 130 Lytton Ave, Palo Alto, CA ABSTRACT This paper provides a preliminary report on a new research \nproject that aims to construct a code generator that uses an automatic theorem prover to produce very \nhigh-quality (in fact, nearly mathematically optimal) machine code for modern architectures. The code \ngenerator is not intended for use in an ordinary compiler, but is intended to be used for inner loops \nand critical subroutines in those cases where peak performance is required, no available compiler gener\u00adates \nadequately e.cient code, and where current engineering practice is to use hand-coded machine language. \nThe pa\u00adper describes the design of the superoptimizer, and presents some encouraging preliminary results. \n Categories and Subject Descriptors C.0 [Computer Systems Organization]: General; C.4 [Computer Systems \nOrganization]: Performance of Sys\u00adtems General Terms Performance, Theory  Keywords superoptimizer, \noptimizing compiler 1. INTRODUCTION 1.1 Goals Automatic code generation is not a young subject, but \nafter all these decades it still happens in many program\u00adming projects that for some portion of the program, \nthe code generated by the best compiler available is not ade\u00adquately e.cient. When this happens, current \nengineering practice is to .nd a senior engineer with intimate knowledge * Randall s current a.liation \nis Google Inc., Mountain View, CA Permission to make digital or hard copies of all or part of this work \nfor personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior \nspeci.c permission and/or a fee. PLDI 02, June 17-19, 2002, Berlin, Germany. Copyright 2002 ACM 1-58113-463-0/02/0006 \n...$5.00. of the relevant processor architecture and assign this un\u00adlucky individual the task of coding \nthe relevant portions of the program in machine language by hand. This generally does produce the required \ne.cient code, but since senior en\u00adgineers have many pressing demands on their time, it is an expensive \nway to get the job done, and software productiv\u00adity would be increased if automatic code generation could \nmatch or beat the best code of the machine language guru. The performance improvement to be expected \nby hand\u00adgenerating machine code is more a matter of folklore and rule of thumb than of documented engineering \nexperience. One report in the literature (on the performance of the Fire.y RPC system) states that a \nfactor of three is typical [19]. In any case, the performance improvement is su.cient to cause many implementers \nto use hand-generated code in parts of their systems, and the cost of this practice only begins with \nthe time required to write the machine code in the .rst place. The additional cost is that the process \nof porting the system to a new architecture is no longer automatic. If the Denali approach ful.lls its \npotential, both costs will be eliminated since the code fragments will be generated automatically. This \nproblem is not one of automating the invention of algorithms or the design of loops, which even we shy \naway from, but the much easier problem of automating the tedious backtracking search to .nd a straight-line \nmachine code se\u00adquence that computes a given vector of expressions in the minimum number of cycles, achieving \nmultiple issue when\u00adever possible, respecting the latency constraints of memory and the various functional \nunits, doing an optimal job of common subexpression elimination, and so on. This is not a busy research \narea, perhaps for the following reason: Most programmers spend most of their day execut\u00ading an edit-compile-debug \nloop; and most automatic code generators (even so-called optimizers ) are designed to run as part of \na compiler that is used in this manner, and there\u00adfore are constrained by the requirement that they generate \nhundreds, thousands, or millions of instructions per second. Such a code generator has little hope of \ngenerating code that will be good enough for our purpose. Consequently a great deal is known about quickly \ngenerating indi.erent code, and very little is known about generating optimal code, which is our goal. \nIndeed, the label optimization has been given to a .eld that does not aspire to optimize but only to \nimprove. This misnomer presented a di.culty to Henry Massalin, who invented the only other code generation \ntechnique we know of that aimed at our goal [14]: the di.culty was that if Mas\u00adsalin called his system \nan optimizer, people would assume that it was only a code improver. So Massalin called his system a superoptimizer. \nIn our title, we have adopted his nomenclature. Massalin s approach was bold and creative, and a strik\u00ading \nexample of Ken Thompson s principle When in doubt, use brute force . His superoptimizer performed an \nexhaus\u00adtive enumeration of all possible code sequences in order of increasing length. For each sequence, \nthe superoptimizer ex\u00adecuted the sequence against a suite of tests, and a sequence that passed all tests \nwas printed as a candidate. Massalin s original implementation was for the 68000 only, but his method \nhas been employed by Granlund and others to produce superoptimizers for other machines[7]. Massalin s \nwork was path-breaking, but the problem is important enough that we decided to explore an alternative \nsearch strategy that we hope will scale better than exhaus\u00adtive enumeration. As yet, we have only preliminary \nresults for our system, but they are consistent with our expecta\u00adtion that Denali will improve on Massalin \ns superoptimizer in several ways: Massalin s approach .nds the shortest program. On Massalin s 68000, \nthe shortest would also be the fastest, but on multiple-issue architectures this need not be so.  To \nrequire the user to prepare a bank of tests for each fragment of code to be generated is painfully onerous. \nBy contrast, the input to Denali is similar to the input to a conventional code generator.  Passing \ntests is not the same as being correct, so the output of Massalin s superoptimizer must be studied carefully \nto check that it is correct. By contrast, the output of Denali is correct by design.  Since executing \nrandom code could have undesirable e.ects, the enumeration of candidate sequences must be limited to \nsome repertoire of su.ciently safe in\u00adstructions, so that executing the candidates doesn t crash the \nprogram or interfere with the code generator itself. It would appear from his paper that Massalin generated \nonly register-to-register computations that performed no accesses to memory. Denali has no such limitation. \n Brute-force enumeration of all code sequences is gla\u00adcially slow. Massalin succeeded in .nding impressive \nshort code sequences, but his method seems to be lim\u00adited to sequences of around half-a-dozen instructions. \nDenali substitutes goal-directed search for brute-force enumeration, for an enormous gain in e.ciency. \nOur prototype is able to generate a near-optimal sequence of thirty-one instructions in around four hours. \n We have been experimenting with the idea behind Denali for a little over a year. Our current prototype \ngenerates code for the Alpha EV6, the latest publicly available imple\u00admentation of the Compaq Alpha processor. \nThe prototype consists of some 15,000 lines of C and Java and some 700 lines of axioms. We are currently \nmaking the changes nec\u00adessary to target the Intel Itanium architecture. It appears that this shift will \nnot require any radical changes (and the changes will mostly be to the axioms), but this preliminary \nnote will describe the Alpha version of Denali only.  1.2 The search principle It may seem to some readers \nthat an automatic theorem prover is an unlikely engine to use as a code generator. In an e.ort to correct \nthis misperception, we would like to em\u00adphasize an important principle. The search principle: A refutation-based \nautomatic theo\u00adrem prover is in fact a general-purpose goal-directed search engine, which can perform \na goal-directed search for any\u00adthing that can be speci.ed in its declarative input language. Successful \nproofs correspond to unsuccessful searches, and vice-versa. A refutation-based prover is a prover that \nattempts to prove a conjecture C by establishing the unsatis.ability of its negation \u00acC. The search principle \nis not true of all refutation-based provers, but it is true of an important kind of prover with which \nour research laboratory has much ex\u00adperience [15, 2]. As an example of the search principle, to search \nfor er\u00adrors in a computer program, we express in formal logic the conjecture that there are no errors, \nand give this conjec\u00adture to a refutation-based automatic theorem prover. If the proof succeeds, the \nsearch for errors has failed. If the proof fails, embedded within the failed proof is the error (or er\u00adrors) \nthat caused the proof to fail, which can be extracted and presented to the user of the program checker. \nThis approach has been used by the Extended Static Checking research project [3, 13, 6]. A second example \nof the principle is Tracy Larrabee s hardware test vector generator, which .nds a test vector for a given \nfault by refuting the conjecture that no test vector for the fault exists, using a satis.ability solver \n[11].  1.3 The obvious approach The search principle suggests an obvious way to build the code generator \nwe desire. To generate optimal code for a program fragment P, we express in formal logic a conjecture \nof the following form: conjecture No program of the target architecture com\u00adputes P in at most 8 cycles. \nWe then submit the conjecture to an appropriate automatic theorem prover. If the proof succeeds, then \n8 cycles are not enough, and we try again, with, say, 16 cycles. On the other hand, if the proof of the \nconjecture fails, then embedded in the failed proof is an (at most) 8-cycle program that com\u00adputes P. \nWe extract that program, and try again with 4 cycles. Continuing with binary search, we eventually .nd, \nfor some K,a K-cycle program that computes P, together with a proof that K - 1 cycles are insu.cient: \nthat is, an optimal program to compute P on the given architecture. (Since the costs of the probes are \nfar from constant, binary search might not be the best strategy, but we have not ex\u00adplored alternatives.) \nIt is easier to describe the obvious approach than to make it work. If carried out naively, the conjectures \nsubmitted to the prover become unwieldy. Suppose, for example, that we proceeded by de.ning in formal \nlogic the two functions exec(M, i) The machine state produced by executing the machine code sequence \nM on the input state i. meaning(P) The meaning of a program (or program frag\u00ad ment) P as a function from \ninput states to output states. Then the conjecture that no machine code sequence M com\u00adputes a given \nprogram fragment P in K cycles becomes: \u00ac(. M : M is a K-cycle program : (. i : i is an input state : \nexec(M, i)= meaning(P)(i))) Conjectures of this form are daunting for two reasons: First, the universal \nquanti.er nested within the existential quanti\u00ad.er is di.cult for automatic theorem provers to handle. \nSec\u00adond, the many cases in the de.nitions of exec and meaning tend to lead automatic theorem provers \ninto a morass of case analyses.  1.4 The Denali Approach Luckily, the alternating quanti.ers and the \nfull de.ni\u00adtions of exec and meaning are unnecessary. For a suf\u00ad.ciently simple program fragment P, the \nequivalence of M and P for all inputs is essentially the universal validity of an equality between two \nvectors of terms, the vector of terms that M computes and the vector of terms that P speci.es must be \ncomputed. Such equivalences can be proved us\u00ading matching, a well-understood automatic theorem-proving \ntechnique. For example, suppose that we want to prove that the program fragment reg6 := 2*reg7 is equivalent, \nfor all inputs, to the one-instruction machine program leftshift reg7,1,reg6 (We use a three-operand \nassembly language with the desti\u00adnation given in the third argument.) Denali s matcher will prove this \nequivalence by instantiating the algebraic identity (. x :2 * x = x<< 1) with the instantiation x := \nreg7. The algebraic identity shown above is an example of a collection of identities used by Denali and \nexpressed in declarative symbolic form (as a Denali axiom). We will describe axioms in more detail in \nsection 4. So, instead of introducing an explicit quanti.er over all in\u00adputs, we accept the limitation \nthat the only proofs of equiv\u00adalence for all inputs that we will consider between a program fragment \nand a machine code sequence are proofs by match\u00ading. If this limitation caused a valid proof of equivalence \nto be missed, then Denali might miss the most e.cient way of computing some term, and its output might \nfail to be optimal, but its output would still be correct. For the kinds of conjectures that we encounter \nin code generation, it turns out, perhaps surprisingly, that, once the proof of equivalence for all inputs \nis handled by matching, all that remains of the proof can be handled by purely proposi\u00adtional reasoning, \nwhich boils down to boolean satis.ability solving (SAT solving). The matcher .nds all possible ways of \ncomputing the result, and the SAT solver selects from these the fastest, considering common subexpressions, \ndelay constraints of the architecture, multiple issue constraints, and so forth. Roughly speaking, the \nmatcher solves the un\u00addecidable part of the optimal code generation problem, and the satis.ability solver \nsolves the NP-complete part. It is an e.ective division of labor. Our current (very limited) experi\u00adence \nsuggests that in practice, the most expensive step is the satis.ability solver. But the architecture \nof Denali separates this solver so e.ectively from the rest of the code generator that we can easily \nsubstitute the current champion satis.\u00adability solver and use it instead of its predecessor. Indeed, \nas short as the project s history is, we have already made several substitutions of this sort. The solver \nused by default by our current prototype is the CHAFF SAT solver [1]. The essential novelty of Denali \nis the combination of the two phases and their application to the superoptimization problem. We do not \nclaim novelty of either phase by itself: the E-graph matching phase applies techniques that have been \nused in automatic theorem-proving for ten or .fteen years [2] (although the earliest accounts of the \ntechnique failed to publish the matching algorithms [4, 15, 17]), and the satis.ability search phase \ncan be viewed as an applica\u00adtion to code generation of the recent ideas of Henry Kautz and Bart Selman \nabout the AI planning problem [9, 8, 10]. The remaining sections of this paper describe in order the \ninput to Denali,  the translation strategy,  the axioms used by Denali,  how Denali s matcher works, \n how the propositional constraints are generated,  some additional issues whose solutions are beyond \nthe scope of this short paper, and .nally  some preliminary results.   2. THE INPUT TO DENALI The \ninput to Denali is a program in a language with a low-level machine model, similar to C or assembly language. \nThe language includes higher-level control constructs, such as conditionals and loops. In addition, the \nlanguage includes features by which the programmer can indicate that certain loops are to be unrolled \nor that certain memory references are likely to miss in the cache, or that the code generator should \ntrust the programmer that certain conditions hold at certain control points in the program. The language \nis not intended for writing programs of any size directly; it is intended to be used for writing the \nbody of an inner loop, for example, or for writing short subroutines. Figures 3 and 5 contain examples. \n 3. THE TRANSLATION STRATEGY The Denali prototype translates its input into an equiva\u00adlent assembly \nlanguage source .le. The translation strategy is as follows: Each procedure in the input is converted \ninto a set of guarded multi-assignments, which are the inputs to the crucial inner subroutine of the \ncode generator. A guarded multi-assignment (or GMA) is determined by a sequence targets of designators \n(also called L-values), an equally long sequence newvals of expressions (also called R\u00advalues), a boolean \nexpression G called the guard, and an exit label L. The meaning of such a GMA is: if G then (targets):=(newvals) \nelse goto L end We generally write G . (targets):= (newvals) to denote this GMA, leaving the exit label \nto be determined by the GMA axioms Matcher E-graph architectural description SAT problem  SAT solver \n assembly program Figure 1: Generating code for a GMA consists of matching followed by satis.ability \nsolving. context. For example, before unrolling, the GMA for the inner loop of a copy routine might be: \np < r . (*p, p, q):=(*q, p +8, q +8) with an exit label appropriate for an exit from the copy loop. Denali \ntranslates the pointer references in this GMA into accesses to the memory M: p < r . (M[p], p, q):=(M[q], \np +8, q +8) Next, because our automatic theorem prover treats entire arrays as values, the update to \nM[p] is transformed by Denali into an update to M: p < r . (M, p, q):=(store(M, p, M[q]), p +8, q +8) \nThe Denali prototype converts each procedure in its input into a set of GMAs, and then uses the crucial \ninner subrou\u00adtine to convert each GMA into near-optimal machine code, using the search principle as modi.ed \nto rely on matching and satis.ability search. Our e.orts have been concentrated on improving this inner \nsubroutine rather than on improv\u00ading the factorization of a procedure body into a collection of GMAs, \nwhere many conventional techniques could usefully be applied (including register allocation, which the \ncurrent prototype ignores). Figure 1 illustrates the crucial inner subroutine that translates a single \nGMA into optimal code in two phases: matching and satis.ability search. The guarded multi-assignment \nis a special case of the Reg\u00adister Transfer List (RTL) [18]. The extra generality of the RTL is the possibility \nof di.erent guards for the di.erent up\u00addates of the multi-assignment. This extra generality seems to \nbe useful when RTLs are used in machine descriptions, but for our purpose of describing the input to \nthe code gen\u00aderator, we have not encountered any examples where the extra generality of the RTL was wanted. \nIn the overall .ow of .gure 1, the matcher converts the GMA into an E-graph, which is a data structure \nthat com\u00adpactly represents all possible ways of computing the goal terms. In addition to the GMA itself, \nthe matcher takes as input a set of axioms about the operators that are com\u00adputable by the target architecture. \nIt remains to be deter\u00admined whether any of the ways of computing the goal terms can be computed by the \ntarget architecture within the cycle budget K. The constraint generator formulates this remain\u00ading question \nas a boolean satis.ability problem. In addition to the E-graph, an important input to the constraint \ngen\u00aderator is an architectural description, which includes tables specifying which functional units can \nexecute which instruc\u00adtions, and a table of latencies of the various ALU operations, and, in the case \nof multiple register banks, of the latencies of the data paths connecting di.erent banks. Finally, a \ncon\u00adventional boolean satis.ability solver is used to .nd a so\u00adlution or determine that no solution exists. \nThe matching step is performed only once per GMA; the constraint genera\u00adtion and satis.ability solution \nsteps are repeated for various cycle budgets until an optimal machine program is found. The matcher and \nthe constraint generator are the subjects of sections 5 and 6 of this report.  4. AXIOMS As mentioned \nabove, axiom .les record in declarative form facts about the operations relevant to e.cient code gener\u00adation. \nWe .nd that we use two kinds of built-in axioms: mathematical axioms, which provide facts about functions \nand relations that would be useful in describing many dif\u00adferent target architectures, and architectural \naxioms, which de.ne or describe operations relevant to a particular target architecture. In addition \nto built-in axioms, Denali allows program-speci.c axioms. We next give some examples of mathematical \naxioms, fol\u00adlowed by examples of architectural axioms for the Alpha instruction set architecture. We \nhave taken these exam\u00adples from Denali s standard axiom .les. For expository pur\u00adposes, we have made \ntwo changes: converting from LISP\u00adlike parenthesized expressions into traditional mathemati\u00adcal notation, \nand suppressing patterns, which determine the instances of universally quanti.ed axioms that will be \nintro\u00adduced by the matcher. Readers who are interested in the details will .nd examples of axioms expressed \nin Denali s LISP-like input syntax in section 8. The mathematical function add64 denotes integer addi\u00adtion \nmodulo 264 . Three representative mathematical axioms postulate that add64 is commutative, associative, \nand has identity 0: (. x, y :: add64(x, y)= add64(y, x)) (. x, y, z :: add64(x, add64(y, z)) = add64(add64(x, \ny),z)) (. x :: add64(x, 0) = x) Denali s mathematical axioms include fundamental proper\u00adties of the \nfunctions select and store that represent reads and writes of arrays. One of these is the select-store \nax\u00adiom, which says that writing element i of an array a doesn t change any element with an index j di.erent \nfrom i: (. a, i, j, x :: i = j . select(store(a, i, x),j)= select(a, j)) Denali s mathematical axioms \nalso de.ne the functions selectb and storeb, which are like select and store, ex\u00adcept that they treat \nintegers as arrays of bytes: selectb(w, i) denotes byte i of word w. A typical architectural axiom is \nsimply an equality that de.nes some operation of the target architecture in terms of mathematical functions. \nFor example, the Alpha has the assembly instructions extbl, insbl and mskbl (extbl(w, i) extracts byte \ni of longword w; insbl(w, i) creates a long\u00adword with byte i equal to the least signi.cant byte of w \nand other bytes zero; mskbl(w, i) creates a copy of longword w with byte i set to zero). These are de.ned \nto Denali by: (. w, i :: extbl(w, i)= selectb(w, i)) (. w, i :: insbl(w, i)= selectb(w, 0) << 8 * i) \n(. w, i :: mskbl(w, i)= storeb(w, i, 0)) As these examples illustrate, we usually use the same name \nfor an instruction and for the function that it computes. The Denali prototype automatically loads a \n.le of math\u00adematical axioms and a .le of architectural axioms for the Alpha EV6. These .les have grown \nover the course of our project, and will need to grow further before they are satis\u00adfactory. Currently, \nthere are 44 mathematical axioms, com\u00adprising 127 source lines, and 275 Alpha axioms, comprising 637 \nlines. Together, these are the built-in axioms of our prototype. In addition, a Denali source program \nmay include axioms that are not important enough to include in the built-in ax\u00adiom .les, but are useful \nto the compilation of that particular program. Such axioms can be used as a powerful substitute for conventional \nmacros. For example, one of our tests re\u00adquires computing the ones complement checksum of an array of \n16-bit integers (see section 8). For convenience, this pro\u00adgram de.nes its own addition operator add \nby means of the following axiom: (. x, y :: add(x, y)= add64(add64(x, y), carry(x, y))) The carry operation \nused in the de.nition of add is also de.ned locally in the program by the following two axioms: (. x, \ny :: carry(x, y)= cmpult(add64(x, y),x)) (. x, y :: carry(x, y)= cmpult(add64(x, y),y)) The de.nition \nby two axioms instead of one gives the code generator the freedom to compute carry(a, b) by comparing \nadd64(a, b) with either a or b.  5. MATCHING The purpose of the matching phase of Denali is to use the \naxioms to identify all of the possible ways in which the ex\u00adpressions in the GMA can be computed. Since \nthe number of ways may be enormous (exponentially larger than the size of the expressions) it is important \nto choose a data structure carefully. The matching phase of Denali uses a data struc\u00adture called the \nE-graph. Early descriptions of the E-graph include the PhD thesis of one of the authors (Nelson)[15], \na journal paper by Nelson and Oppen [17], and the cru\u00adcial Downey-Sethi-Tarjan congruence closure algorithm \n[4]. None of these papers say much about the matching algo\u00adrithm. Denali uses the same matching algorithm \nas the au\u00adtomatic theorem prover Simplify. This matching algorithm is described in an upcoming research \nreport [2]. (a) (b) *1 *1  reg6 4 reg6 4  **  (c) (d) << << + +   *21 *21  reg6 4  ** reg6 \n4  **  22 22 Figure 2: Solid arrows represent term DAG edges and dashed arcs represent equivalences \nin this illus\u00adtration of matching in the E-graph An E-graph is a conventional term DAG augmented with \nan equivalence relation on the nodes of the DAG; two nodes are equivalent if the terms they represent \nare identical in value. Hence the value of an equivalence class can be com\u00adputed by computing any term \nin the class; having selected a term in the class, the values of each argument of the term likewise can \nbe computed by selecting any term equivalent to the argument term, and so forth. Thus an E-graph of size \nO(n) can represent T(2n) distinct ways of computing a term of size n. The machine code for a GMA must \nevaluate the boolean expression that is the guard of the GMA, and must also evaluate the expressions \non the right side of the assign\u00adment statement, as well as the addresses of any targets that are not \nregisters (that is, address arguments to select and store). Let us call all these expressions the goal \nexpressions, since the essential goal of the required machine code is to evaluate them. Typical GMAs \nhave several goal terms, but Figure 2 illus\u00adtrates Denali s matcher for the arti.cially simpli.ed situa\u00adtion \nof a single goal term, namely the term reg6*4+1, which we have chosen to illustrate several points about \nmatching. The .rst step in the matching phase is to construct an E\u00adgraph that represents all the goal \nterms. Figure 2(a) shows the initial E-graph of our simple example. It is a conven\u00adtional term DAG: that \nis, a term of the form f(t1,t2,... ,tn) is represented by a node labelled f with an outgoing se\u00adquence \nof edges pointing to the nodes that represent the t s. If no matching were performed at all, so that \nFigure 2(a) were the .nal E-graph, then the only way to compute the goal term would be by a multiply \nfollowed by an add. The matcher repeatedly transforms the E-graph by in\u00adstantiating a relevant axiom \nand asserting the instance in the E-graph. This is repeated until a quiescent state is reached in which \nthe E-graph records all relevant instances of ax\u00adioms. In the case of our example, the .rst relevant \nfact that 22 we will add to the graph is the fact 4 = . When this fact is added, the E-graph is changed \nby adding a new node to represent the term 22 (or 2 ** 2) and adding this new node to the equivalence \nclass of the existing node for 4 . Figure 2(b) shows the result of this transformation. (We use dashed \nedges to connect nodes that are equivalent.) Of course, the Alpha does not have an instruction for comput\u00ading \n**, so this match has not directly introduced any new ways of computing the goal term: if matching terminated \nwith the E-graph of Figure 2(b), the only way to compute the goal term would be by the same multiply \nand add se\u00adquence available already in the initial graph. But this does not mean the change to the E-graph \nwas useless, because it enables new matches. Speci.cally, matching now continues by .nding a relevant \ninstance of the fact ( . k,n :: k* 2n = k<< n ) namely the instance with (k,n):= (reg6,2). (An ordinary \nmatcher would fail to match the pattern k * 2 ** n against the term-DAG node reg6 *4 because the node \nlabelled 4 is not of the form 2n, but an E-graph matcher will search the equivalence class and .nd the \nnode 2 ** 2 and the match will succeed.) The resulting E-graph is shown in Figure 2(c). If matching were \nterminated at this point, then in addition to the multiply-add sequence there would be a shift-and-add \nsequence (which is faster and therefore would probably be selected). Finally, the Alpha contains an instruction \ncalled s4addl which scales by four and adds. The background facts for Denali therefore include the architectural \naxiom ( . k,n :: k* 4+ n = s4addl(k,n)) . When the matcher instantiates this with (k,n):=(reg6,1) and \nupdates the E-graph, the result is the graph shown in Figure 2d. This adds a new possibility for computing \nthe goal term (superior to both of the other possibilities) using a single s4addl instruction. Here are \nthree comments about this example. First, the order in which the matches would occur in prac\u00adtice might \nvery well be di.erent than the order described: s4addl could have been introduced immediately. However, \nthe << node could not be introduced until the equality of 4 with 2 ** 2 was introduced. Second, we contrast \nE-graph matching with conventional matching. Many conventional matchers are rewriting en\u00adgines, in the \nsense that they directly rewrite a term into a new form, recursively rewriting subexpressions before \nrewrit\u00ading a root expression. For example, they might rewrite n* 2 into n<<1. Such a rewriting engine \nwould be unlikely to rewrite 4 as 2**2, since the latter term is not an e.cient way to compute the former. \nSimilarly, a rewriting engine that produced the fairly e.cient reg6<<2 might miss the most e.cient version \nwith s4addl, since the pattern for the fact involving s4addl most naturally involves multiplication by \nfour, not left-shifting by two. In general, to reach the optimal version by a sequence of elementary \nrewrites may require rewriting some subterms in ways that reduce e.\u00adciency rather than improve it, and, \nin general, a transforma\u00adtion that improves e.ciency may cause the failure of subse\u00adquent matches that \nwould have produced even greater gains. These are well-known and thorny problems for rewriting engines. \nThe E-graph doesn t su.er from these problems, since, instead of rewriting A as B, it records A = B in \nits data structure, leaving both A and B around, where they can be used both for future matching and \nas candidates for the .nal selection of instructions. A third comment is that the attractive features \nof the E\u00adgraph approach mentioned in the second comment are not without their price. Matching in an E-graph \nis more expen\u00adsive than matching a pattern against a simple term DAG. Also, many matches are required \nto reach quiescence, and the quiescent state may be quite a large E-graph. For ex\u00adample, Denali s matcher \nuses the commutativity and asso\u00adciativity of addition to .nd more than a hundred di.erent ways of computing \na+ b+ c+ d+ e. Nevertheless, Denali seems to be e.cient enough to be useful. In our description of Denali \ns matcher, we have so far considered only facts that are (quanti.ed or unquanti.ed) equalities between \nterms (that is, facts of the form T = U). Two other kinds of facts that the matcher uses are (quanti.ed \nor unquanti.ed) distinctions and clauses.As with equalities, quanti.ed distinctions and clauses are trans\u00adformed \ninto the corresponding unquanti.ed kind of fact by .nding heuristically relevant instances, so it su.ces \nto ex\u00adplain how Denali uses unquanti.ed distinctions and clauses. A (binary) distinction is a fact of \nthe form T = U for two terms T and U. Binary distinctions are the only kind of distinction that we will \nconsider in this preliminary report. A distinction T = U is asserted in the E-graph by recording the \nconstraint that the equivalence classes of T and of U are uncombinable. Equalities and Distinctions are \ncollectively called literals. The third kind of fact that Denali uses is a clause, which is a disjunction \n( or ) of literals, that is, a fact of the form L1 . L2 . .... Ln where the L s are literals. An unquanti.ed \nclause is used by recording it in a data structure and then continuing as follows. Whenever any of its \nliterals becomes untenable, the untenable literal is deleted from the recorded clause. Furthermore, if \nthe deletion of the untenable literal from a recorded clause leaves the clause with a single literal, \nthen that lone literal is asserted. An equality T = U is unten\u00adable if the equivalence classes of T and \nof U have been con\u00adstrained to be uncombinable. A distinction T = U is unten\u00adable if T and U are in the \nsame equivalence class. We conclude this section with an example that illustrates how the matcher uses \nclauses and distinctions. If a GMA involved storing some value (say x) to address p and then loading \nfrom address p+ 8, then the E-graph would include the term select(store(M,p,x),p+8) The presence of this \nterm would cause the body of the select\u00adstore axiom (see section 4) to be instantiated by (a,i,j):= (M,p,p \n+ 8), causing the matcher to make a record of the unquanti.ed clause p= p+8 . select(store(M,p,x),p+8) \n= select(M,p+8) By mechanisms that we will not describe in this preliminary report, the literal p= p+8 \nwill be discovered to be untenable and deleted, leading to the assertion of the equality select(store(M,p, \nx),p +8) = select(M,p +8) The presence of this equality in the E-graph gives the code generator the option \nof doing the load and store in either order. The matching phase uses identities and reasons about equalities, \ndistinctions, and clauses. When the matching phase is complete, the .nal equivalence relation of the \nE\u00adgraph is all that matters: the distinctions and clauses used on the way are not used in any way by \nthe satis.ability search phase that follows.  6. SATISFIABILITY SOLVING After the matcher has reached \na quiescent state, the E\u00adgraph represents all possible ways of computing the terms that it represents. \n(More precisely, this is true if the axioms include a complete axiomatization of the .rst order theory \nof the relevant operations and if the matching phase is allowed to run long enough, and if the heuristics \nthat are designed to keep the matcher from running forever don t mistakenly stop it from running long \nenough. These caveats about the matcher are the .rst reason that we call Denali s output near-optimal \ninstead of optimal .) In order to obtain optimal code, it remains to formulate a conjecture of the form \nNo program of the target architecture computes the values of the goal terms within K cycles where K is \na speci.ed literal integer constant. Happily, this can be formulated in propositional calculus, so that \nit can be tested with a satis.ability solver. The exact details are somewhat architecture-dependent, \nbut the basic idea is sim\u00adple. To illustrate the basic idea we assume a machine with\u00adout multiple issue, \nso that at most one instruction is issued per cycle. We de.ne a function to be a machine operation if \nsome instruction of the target architecture is capable of computing the function. In addition to machine \noperations, the E-graph also includes non-machine operations that are allowed in the input (and in the \naxioms) but that cannot be computed by the machine directly. (The matching example in the previous section \nused the non-machine operation **, so that universal facts could be expressed conveniently.) We de.ne \na term (that is, a node of the E-graph) to be a machine term if it is an application of a machine operation, \nand a non-machine term otherwise. The arguments to a machine term need not themselves be machine terms. \n(We are not interested yet in whether the term is computable from the inputs, only whether it could be \ncomputed if its arguments were available in registers.) We introduce a number of boolean unknowns. Speci.cally, \nfor each cycle i of the K cycles available, for each machine term T , and for each equivalence class \nQ, we introduce the following boolean unknowns: L(i, T ): denotes that in the desired machine program, \na computation of term T is Launched at the beginning of cycle i A(i, T ): denotes that in the desired \nmachine program, a computation of T is completed At the end of cycle i B(i, Q): denotes that the desired \nmachine program has computed the value of equivalence class Q By the end of cycle i Thus we have (2m \n+ Q)K boolean unknowns, where m is the number of machine terms and Q is the number of equiva\u00adlence classes \nin the E-graph. In terms of these unknowns we can formulate the conditions under which a K-cycle machine \nprogram exists that computes all the goal terms. There are .ve basic conditions. In writing these condi\u00adtions, \nwe use the dummy i to range over all cycle indices, T and U to range over machine terms in the E-graph, \nand Q to range over equivalence classes in the E-graph. First, writing .(T ) for the latency of the machine \nterm T , that is, the number of cycles required to apply the root operator of T to its arguments, we \nobserve that the interval of time occupied by the computation of T consists of .(T ) consecutive cycles, \nleading to the following obvious relation between the cycle in which T s computation is launched and \nthe cycle in which it is completed: (L(i, T ) =A(i + .(T ) -1,T )) i,T Second, writing args(T ) for \nthe set of equivalence classes of the top level arguments of a term T , we observe that an operation \ncannot be launched until its arguments are available, and therefore: (L(i, T ) .B(i -1,Q)) i,TQ.args(T \n) Third, the only way to compute the value of an equiva\u00adlence class Q by the end of cycle i is by computing \nthe value of one of its machine terms T at the end of some cycle j =i and therefore: ) ) B(i, Q) =A(j, \nT ) i,Qj=i T .Q Fourth, since we are assuming for simplicity a single-issue architecture, only one operation \ncan be launched per cycle, so no two distinct machine terms T and U can be launched in any cycle i: \n(\u00acL(i, T ) .\u00acL(i, U)) i,T,U T =U Fifth, letting Gdenote the set of equivalence classes of goal terms, \neach of these equivalence classes must be computed within K cycles. Numbering cycles from zero, that \nwould be by the end of cycle K -1: B(K -1,Q) Q.G We need to continue adding constraints until the boolean \nunknowns are so constrained that any solution to them cor\u00adresponds to a K-cycle machine program that \ncomputes the goal terms. More constraints are needed than we have shown so far, but we have shown enough \nto convey the essence of the approach. For a .xed E-graph and a .xed cycle budget, the con\u00adstraints are \nexplicit propositional constraints on a .nite set of boolean unknowns. The assertion that no K-cycle \nma\u00adchine program exists is equivalent to the assertion that their conjunction is unsatis.able, a conjecture \nthat can be tested with a SAT solver, which is of all automatic theorem provers the one that most clearly \nsatis.es the search principle, since it refutes the conjecture by .nding explicit values for the L s, \nA s and B s that satisfy the constraints. The L s that are assigned true by the solver determine which \nmachine op\u00aderations are launched at each cycle, from which the required machine program can be read o.. \n We conclude this section with a few remarks about la\u00adtencies. The Denali method requires that the latency \n.(T ) of each term T be known to the code generator. For ALU operations, this requirement is not problematical, \nbut for memory accesses it may at .rst seem to be a show-stopper. Certainly an ordinary code generator \ncannot statically pre\u00addict the latencies of memory accesses. But the scenario in which Denali is designed \nto be used is not the scenario in which an ordinary compiler is used. The scenario is an inner loop or \ncrucial subroutine whose performance is important enough to warrant hand-coding in machine language. \nIn this scenario, the .rst step is to use pro.ling tools to de\u00adtermine which memory accesses miss in \nthe cache. Having found this information, the programmer can communicate it to Denali using annotations \nin the Denali source program. Our engineering goal is to do at least as well as the machine language \nguru, who also designs her code on the basis of pro.le-generated assumptions about memory latencies. \nSince the information gleaned from pro.ling is statistical, not absolute, we would still be in trouble \nif the correctness of the generated code depended on the accuracy of the memory latency annotations, \nbut (precisely because caching makes memory latencies unpredictable statically) any reasonable modern \nprocessor (including both the Alpha and the Ita\u00adnium) includes hardware to stall or replay when necessary, \nso that latency annotations are important for performance but not for correctness: the code generated \nwill be correct even if the annotations are inaccurate. Thus we can expect some stalls or replay traps \non the .rst few iterations of a Denali-optimized inner loop, but to the extent that statis\u00adtical information \nabout inner loops is reliable, the loop will soon settle into the optimal computation that was modelled \nby the boolean constraints. The statistical nature of pro.l\u00ading information is the second reason that \nwe call Denali s output near-optimal instead of optimal .  7. ADDITIONAL CONSTRAINTS The satis.ability \nconstraints in the previous section were simpli.ed by the assumption of a single-issue machine, since \nthe cycle index i could also be thought of as an index in the instruction stream. But the same approach \neasily accommo\u00addates a multiple instruction architecture where cycle indices and instruction indices \nboth appear and must be carefully distinguished. Some expressions (in particular, memory accesses) on \nthe right side of a guarded multi-assignment may be unsafe to compute if the guard expression is false. \nTherefore Denali generates satis.ability constraints that force the guard to be tested before any such \nexpressions are evaluated. It is straightforward to add additional propositional constraints on the boolean \nunknowns to enforce this order. The expressions on the right side of a guarded multiassign\u00adment may use \nthe same targets that it updates; for example, (reg6, reg7):= (reg6 + reg7, reg6) . In this case, the \n.nal instruction that computes the reg6 + reg7 may not be able to place the computed value in its .nal \n\\proc byteswap4 : [ a : int ] -> int = \\var r : int \\in r:=0 ; r<0> := a<3> ; r<1> := a<2> ; r<2> := \na<1> ; r<3> := a<0> ; \\res :=r ; \\end Figure 3: Envisioned program for 4-byte swap. w<i> denotes byte \ni of word w, that is, selectb(w, i). Our current prototype requires a parenthesized input syntax in the \nstyle of .gure 6. destination. In the worst case, we may be forced to choose between adding an early \nmove to save an input that will be overwritten by the rest of the code sequence or computing a value \ninto a temporary register and adding a late move to put it .nally into the correct location. On multiple-issue \narchitectures the choice between these two alternatives may make a di.erence to performance. Denali encodes \nthe choice into the boolean constraints where it becomes just one more bit for the solver to determine. \nThe ordering of procedure calls is more constrained than the ordering of other operations, because in \ngeneral, a proce\u00addure call is assumed to both modify and read the memory. This circumstance leads to \nadditional constraints that we also encode in the propositional constraints, but we must leave the details \nfor future papers. Some instructions of some architectures compute multi\u00adple results into multiple registers. \nIn this situation we model the instruction s operation as a machine operation that com\u00adputes a tuple \nof the various results. We also introduce into the axiom .les non-machine projection operations that \nex\u00adtract the individual components of the tuple. The .nal E\u00adgraph will contain expressions that apply \nthe composition of the machine operation followed by the projection func\u00adtion. Such a composition represents \na way of computing the value by applying the instruction and using the result that corresponds to the \nprojection function.  8. PRELIMINARY RESULTS We have implemented a prototype of Denali in Java and C \nfor the Alpha EV6, a quad-issue processor with multiple register banks and extra delays for moving values \nbetween banks, almost all of whose complexity is modeled by our code generator. All the experiments described \nin this section were carried out on a 667MhzAlpha machine with 500MB of main memory. We created many \ntests for our prototype; we also invited our colleagues to supply us with challenge problems. One source \nof test problems are byte swap problems: the problem of reversing the order of the n lower bytes of a \nreg\u00adister. For n = 4, this was was a challenge problem given long ago by a product engineering group \nwho supported a SPARC emulator running on the Alpha. Figure 3 shows a representation of the input program \nfor the 4 byte swap given to Denali. Our prototype takes just over a minute to generate code for this \nproblem. Less than 0.3 seconds is spent in the SAT solver. The sizes of the four SAT prob\u00ad // Register \nMap: {a=$16, r=$1, \\res=$0, 0=$31} byteswap4: # assume a = wxyz extbl $16,1,$2 #0, U1 ;$2=000y insbl \n$16,3,$3 #0, U0 ;$3=z000 nop #0 nop #0 insbl $2,2,$2 #1, U1 ;$2=0y00 extbl $16,3,$4 #1, U0 ;$4=000w \nnop #1 nop #1 or $4,$3,$3 #2, L0 ;$3=z00w extbl $16, 1, $4 # 2, U1 (unused) extbl $16,2,$4 #2, U0 ;$4=000x \nnop #2 insbl $4,1,$4 #3, U0 ;$4=00x0 or $2,$3,$2 #3, L0 ;$2=zy0w nop #3 nop #3 or $4,$2,$0 #4, U0 ;$0=zyxw \nret ($26) # 4, L0 nop #4 nop #4 .end byteswap4 Figure 4: Generated EV6 assembly program for four byte \nswap. The unused instruction is necessary: if it were a nop, the following extbl instruction would be \nscheduled on the wrong cluster. lems solved for this example range from 1639 variables and 4613 clauses \nfor the 4-cycle refutation to 9203 variables and 26415 clauses for the 8-cycle solution. The 5-cycle \nEV6code generated is shown in Figure 4. Each instruction in this program is annotated with the cycle \nnumber and functional unit (one of L0, L1, U0, U1) at which it is issued. To the best of our knowledge, \nthis .ve cycle program is optimal. With some e.ort, we were able to coax the production C compiler to \ntie this result, giving it aggressive switches (-fast -arch ev6), and helpful input: long r = ((a &#38; \n0xff) << 24) | ((a >> 8) &#38; 0xff) << 16) | ((a >> 16) &#38; 0xff) << 8) | ((a >> 24) &#38; 0xff) ; \n For the 5-byte swap problem, Denali does one cycle better than the C compiler. (In these experiments, \nthe running time of the code produced by the C compiler was computed by hand.) We attempted to compare \nDenali with the Alpha version of the GNU superoptimizer which is available on the web [5]. But our attempts \nwere not very successful. Version 2.5 of the GNU superoptimizer seems to model the Alpha incom\u00adpletely: \nIt models the Alpha instruction set architecture, not the EV6. It is missing several opcodes. Finally, \nwhile we were able to generate .ve-instruction sequences, we were unable to generate longer sequences \nin an amount of time that we were willing to wait (several days). The largest challenge program we have \ntackled so far with \\op add : [ long, long ] -> long ; \\op carry : [ long, long ] -> long ; \\axiom (\\forall \n[a b] add(a,b) = add(b,a)) ; \\axiom (\\forall [a b c] add(a,add(b,c)) = add(add(a,b),c)) ; \\axiom (\\forall \n[ a b] add(a,b) = \\add64(\\add64(a,b), carry(a,b))) ; \\axiom (\\forall [ a b ] carry(a,b) = \\cmpult(\\add64(a,b), \na)) ; \\axiom (\\forall [ a b ] carry(a,b) = \\cmpult(\\add64(a,b), b)) ; \\proc checksum : [ ptr,ptrend : \nlong* ] -> short = \\var sum : long := 0 \\in \\do ptr < ptrend -> sum := add(sum, *ptr) ; ptr := ptr + \n8 \\od ; sum := \\extwl(sum, 0) + \\extwl(sum,1) + \\extwl(sum,2) + \\extwl(sum,3) ; sum := \\extwl(sum,0) \n+ \\extwl(sum,1) ; \\res := \\cast(sum, short) \\end Figure 5: Envisioned program for checksum. Denali is \na packet checksum routine, which computes the 16\u00adbit sum of a set of 16-bit integers, with wraparound \ncarry. Three techniques are required to generate e.cient code for this problem: loop unrolling, software \npipelining (the com\u00adputation in one loop iteration of a result that is used on the next iteration), and \nword parallelism (in the case of this example, using 64-bit addition instead of four 16-bit addi\u00adtions). \nThe current Denali prototype implements loop un\u00adrolling. We have a design for software pipelining, but \nhaven t implemented it yet. In the meantime, to make progress on this example, we hand-speci.ed the required \npipelining by introducing temporaries to carry intermediate values across loop iterations. As for word \nparallelism, we don t aspire to do this automatically: we contend this is better done by the programmer \nusing Denali. Figure 5 shows the Denali input that we envision. Figure 6 shows the actual input to our \ncurrent prototype for this problem. As remarked above, the input uses temporaries (v1, v2, v3, v4) to \nhand-specify software pipelining. This forces us to hand-specify the loop unrolling as well, so we cannot \nuse Denali s unroll feature. Denali took about 4 hours to generate code for this program; the code for \nthe loop body consisted of 10 cycles and 31 instructions. In addition to the challenge problems above, \nwe have used Denali on a matrix routine rowop, and on the problem of the least common power of 2 of two \nregisters (in addition to a number of problems we invented for ourselves). Although few in number, these \ntests give us con.dence that the De\u00adnali approach can provide peak performance on ALU-bound register-to-register \ncomputations. The experiments we have done to date on memory-bound computations (the checksum example, \nand some matrix loops) suggest that Denali will also be e.ective on these increasingly important problems. \nHowever, we won t venture a .nal conclusion on memory\u00adbound computations until we have implemented software \npipelining and done more examples, which we plan to do in the next few months. ; carry returns the carry \nbit resulting from the 9. REFERENCES ; unsigned 64-bit sum of its arguments. [1] SAT Research at Princeton \nHome Page. (\\opdecl carry (long long) long) CHAFF satis.ability solver. (\\axiom (forall (a b) (pats \n(carry a b)) http://www.ee.princeton.edu/~chaff. (eq (carry a b) (\\cmpult (\\add64 a b) a)))) [2] David \nDetlefs, Greg Nelson, and James B. Saxe. A (\\axiom (forall (a b) (pats (carry a b)) theorem-prover for \nprogram checking. Technical (eq (carry a b) (\\cmpult (\\add64 a b) b)))) Report Research Report 178, Compaq \nSystems Research Center, 2002. In preparation. ; unsigned 64-bit carry-wraparound add (\\opdecl add (long \nlong) long) [3] David L. Detlefs, K. Rustan M. Leino, Greg Nelson, and James B. Saxe. Extended static \nchecking. ; associativity of add Technical Report 159, Compaq Systems Research (\\axiom (forall (a b c) \n(pats (add a (add b c))) Center, 1998. (eq (add a (add b c)) (add (add a b) c)))) [4] P. J. Downey, R. \nSethi, and R. E. Tarjan. Variations (\\axiom (forall (a b c) (pats (add (add a b) c)) (eq (add a (add \nb c)) (add (add a b) c)))) on the common subexpression problem. JACM, 27(4), 1980. ; commutativity of \nadd [5] Torbjorn Granlund et al. GNU Superoptimizer FTP (\\axiom (forall (a b) (pats (add a b)) site. \nftp://prep.ai.mit.edu/pub/gnu/superopt. (eq (add a b) (add b a)))) [6] Cormac Flanagan, K. Rustan M. \nLeino, Mark ; implementation of add Lillibridge, Greg Nelson, James B. Saxe, and Raymie (\\axiom (forall \n(a b) (pats (add a b)) Stata. Extended static checking for java. Proceedings (eq (add a b) (\\add64 (\\add64 \na b) (carry a b))))) of the ACMProgramming Language Design and Implementation (PLDI) Conference, 2002. \n; main procedure [7] Torbjorn Granlund and Richard Kenner. Eliminating (\\procdecl checksum ((ptr (\\ref \nlong)) (ptrend (\\ref long))) short branches using a Superoptimizer and the GNU C (\\var (sum1 long 0) \n(\\var (sum2 long 0) compiler. Proceedings of the ACMSIGPLAN (\\var (sum3 long 0) (\\var (sum4 long 0) \nconference on Programming Language Design and (\\var (v1 long (\\deref ptr)) Implementation, pages 341 \n52, 1992. (\\var (v2 long (\\deref (+ ptr 8))) [8] Henry Kautz, David McAllester, and Bart Selman. (\\var \n(v3 long (\\deref (+ ptr 16))) (\\var (v4 long (\\deref (+ ptr 24))) Encoding plans in propositional logic. \nProceedings of (\\semi the Fifth International Conference on Principles of (\\do (-> (< ptr ptrend) Knowledge \nRepresentation and Reasoning, 1996. (\\semi [9] Henry Kautzand Bart Selman. Planning as (:= (sum1 (add \nsum1 v1)) (sum2 (add sum2 v2)) satis.ability. Proceedings of the 10th European (sum3 (add sum3 v3)) (sum4 \n(add sum4 v4))) (:= (ptr (+ ptr 32))) Conference on Arti.cial Intelligence (ECAI 92), 1992. (:= (v1 \n(\\deref ptr))) [10] Henry Kautzand Bart Selman. Pushing the envelope: (:= (v2 (\\deref (+ ptr 8)))) Planning, \npropositional logic, and stochastic search. (:= (v3 (\\deref (+ ptr 16)))) Proceedings of the Thirteenth \nNational Conference on (:= (v4 (\\deref (+ ptr 24))))))) Arti.cial Intelligence (AAAI-96), 1996. (\\var \n(c1 long) (\\var (c2 long) (\\var (c3 long) [11] Tracy Larrabee. E.cient generation of test patterns (\\var \n(s1 long) (\\var (s2 long) (\\var (s long) using boolean satis.ability. PhD thesis, Stanford (\\semi University, \n1990. See also [12]. (:= (s1 (+ sum1 sum2))) [12] Tracy Larrabee. E.cient generation of test patterns \n(:= (c1 (carry sum1 sum2))) using boolean satis.ability. Technical Report 90/2, (:= (s2 (+ sum3 sum4))) \n (:= (c2 (carry sum3 sum4))) DEC Western Research Lab, February 1990. (:= (s (+ s1 s2))) [13] K. Rustan \nM. Leino, Greg Nelson, and James B. Saxe. (:= (c3 (carry s1 s2))) ESC/Java user s manual. Technical Report \n2000-002, (:= (s (+ (\\extwl s 0) (+ (\\extwl s 1) Compaq Systems Research Center, 2000. (+ (\\extwl s 2) \n(\\extwl s 3)))))) [14] Henry Massalin. Superoptimizer: a look at the (:= (s (+ (\\extwl s 0) (+ (\\extwl \ns 1) (+ c1 (+ c2 c3)))))) shortest program. Proceedings of the second (:= (\\res (\\cast short s)))))))))))))))))))) \ninternational conference on architectural support for programming languages and operating systems, pages \n122 26, 1987. Figure 6: Actual input to current prototype for [15] Charles Gregory Nelson. Techniques \nfor Program checksum program of .gure 5. Veri.cation. PhD thesis, Stanford University, 1979. A revised \nversion was published as a Xerox PARC Computer Science Laboratory Research Report [16]. [16] Greg Nelson. \nTechniques for Program Veri.cation. Technical Report CSL-81-10, Xerox PARC, 1981. This report is out \nof print, but the author still has a couple of photocopies. [17] Greg Nelson and Derek C. Oppen. Fast \ndecision algorithms based on congruence closure. JACM, 27(2), October 1979. [18] Norman Ramsey and Jack \nW. Davidson. Machine descriptions to build tools for embedded systems. ACMSIGPLAN Workshop on Languages, \nCompilers, and Tools for Embedded Systems (LCTES 98), pages 172 188, 1998. [19] Michael D. Schroeder \nand Michael Burrows. Performance of the Fire.y RPC. ACMTransactions on Computer Systems, 8(1):1 17, 1990. \n  \n\t\t\t", "proc_id": "512529", "abstract": "This paper provides a preliminary report on a new research project that aims to construct a code generator that uses an automatic theorem prover to produce very high-quality (in fact, nearly mathematically optimal) machine code for modern architectures. The code generator is not intended for use in an ordinary compiler, but is intended to be used for inner loops and critical subroutines in those cases where peak performance is required, no available compiler generates adequately efficient code, and where current engineering practice is to use hand-coded machine language. The paper describes the design of the superoptimizer, and presents some encouraging preliminary results.", "authors": [{"name": "Rajeev Joshi", "author_profile_id": "81100641984", "affiliation": "Compaq Systems Research Center, Palo Alto, CA", "person_id": "PP42053184", "email_address": "", "orcid_id": ""}, {"name": "Greg Nelson", "author_profile_id": "81100407919", "affiliation": "Compaq Systems Research Center, Palo Alto, CA", "person_id": "PP31040964", "email_address": "", "orcid_id": ""}, {"name": "Keith Randall", "author_profile_id": "81100063247", "affiliation": "Compaq Systems Research Center, Palo Alto, CA", "person_id": "P348266", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/512529.512566", "year": "2002", "article_id": "512566", "conference": "PLDI", "title": "Denali: a goal-directed superoptimizer", "url": "http://dl.acm.org/citation.cfm?id=512566"}