{"article_publication_date": "05-17-2002", "fulltext": "\n MaJIC: Compiling MATLAB for Speed and Responsiveness* George Alm\u00b4 asi and David Padua galmasi,padua@cs.uiuc.edu \nDepartment of Computer Science University of Illinois at Urbana-Champaign ABSTRACT This paper presents \nand evaluates techniques to improve the execution performance of MATLAB. Previous e.orts con\u00adcentrated \non source to source translation and batch compila\u00adtion; MaJIC provides an interactive frontend that looks \nlike MATLAB and compiles/optimizes code behind the scenes in real time, employing a combination of just-in-time \nand speculative ahead-of-time compilation. Performance results show that the proper mixture of these \ntwo techniques can yield near-zero response time as well as performance gains previously achieved only \nby batch compilers. Categories and Subject Descriptors D.3.4 [Programming Languages]: Interpreters, \nCompil\u00aders, Code Generation, Run-time environments General Terms Design, Languages, Algorithms, Performance \n1. INTRODUCTION MATLAB [15], a product of Mathworks Inc., is a pop\u00adular programming language and development \nenvironment for numeric applications. The MATLAB programming lan\u00adguage resembles FORTRAN 90 in that it \ndeals with vectors and matrices, but unlike FORTRAN it is weakly typed and polymorphic. The main strengths \nof MATLAB lie both in its interactive nature, which makes it a handy exploration tool, and the richness \nof its precompiled libraries and toolboxes. The main weakness of MATLAB is its slow execution, es\u00adpecially \nwhen compared to similarly written code in FOR-TRAN. Because MATLAB has weak typing, the interpreter \nin the development environment has to check types at run\u00adtime, resulting in substantial performance loss. \n* This work was supported in part by NSF contract ACI98\u00ad70687. Permission to make digital or hard copies \nof all or part of this work for personal or classroom use is granted without fee provided that copies \nare not made or distributed for profit or commercial advantage and that copies bear this notice and the \nfull citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior specific permission and/or a fee. PLDI 02, June 17-19, 2002, Berlin, Germany. \nCopyright 2002 ACM 1-58113-463-0/02/0006 ...$5.00. Previous work with MATLAB to FORTRAN translators, \nnotably the FALCON compiler [9, 8], has shown a perfor\u00admance increase of up to three orders of magnitude \nby em\u00adploying compile-time type analysis to reduce the number of runtime checks. MaJIC (Matlab Just-In-time \nCompiler) aims to achieve the same performance goals without sacri.cing the interac\u00adtive nature of MATLAB. \nLike FALCON, it attempts to re\u00admove the overhead of runtime type checks by compiling code instead of \ninterpreting it. Unlike FALCON, which is a batch compiler, MaJIC preserves interactive behavior by mini\u00admizing \n or hiding compilation time. MaJIC attempts to compile code ahead of time by speculation; whenever spec\u00adulation \nfails, MaJIC falls back to just-in-time compilation. MaJIC s dynamic (JIT) compiler reduces compile time \nas much as possible. It consists of an extremely fast type inference engine and a relatively naive, but \nfast, code gen\u00aderation engine. Compilation is performed as late as possible in order to gather more runtime \ninformation, in the idea that better runtime information allows the compiler to save time-consuming optimization \nsteps. In addition to JIT compilation, MaJIC also performs speculative ahead-of-time compilation. Looking \nat source code only, the compiler guesses the run-time context most likely to occur in practice. If the \nguess is correct, the end result is highly optimized code that will have been compiled by the time it \nis needed, e.ectively hiding compilation la\u00adtency. A wrong guess by the compiler results, at worst, in \ndegraded performance, but never a.ects program correct\u00adness: MaJIC contains a mechanism to insure that \ncode is only executed if its semantics are guaranteed. The rest of this paper is structured as follows. \nSection 2 describes the software architecture of MaJIC and optimiza\u00adtion techniques related to JIT type \ninference and speculative type inference. Section 3 presents and analyzes the perfor\u00admance results we \nobtained. Section 4 o.ers a brief survey of related work. In Section 5 we present our conclusions. 2. \nSOFTWARE ARCHITECTURE MaJIC s users interact with a MATLAB-like front end: a compatible interpreter that \ncan execute MATLAB code at approximately MATLAB s original speed. However, MaJIC s front end doesn t \nattempt to execute all code: it defers com\u00adputationally complex tasks (in the current implementation, \nfunction calls) to the code repository. To pass work to the repository, the MaJIC front end builds an \ninvocation con\u00adtaining the name of a MATLAB function and the values of the parameters (if any). native/object \ncode Figure 1: MaJIC compiler passes The code repository is a database of compiled code. It compiles \ncode on its own, ahead of time, by snooping the source code directories, maintaining dependency informa\u00adtion \nbetween source code and object code and triggering recompilations when the source code changes. The reposi\u00adtory \ncan also compile code as a result of user actions (such as invoking MATLAB functions). The code repository \ncollects the type information neces\u00adsary for compiling MATLAB code. This type information comes from \ndi.erent sources: directly from the user (i.e. when the user calls a function directly), from earlier \nruns of the same code, or from the type speculator. The code repository responds to requests for compiled \ncode by the interpreter. It has a type matching system (described in Section 2.2.1) that allows the retrieval \nof se\u00admantically correct compiled code for a given invocation by the interpreter. A failure to .nd appropriate \ncode usually triggers a compilation; since this typically happens during program execution, where time \nis at a premium, the JIT compiler is used in this situation. The generated code can later be recompiled \n(and replaced in the repository) using a better compiler. The compiler itself has the task of turning \nsource code into executable code. The compiler s passes are shown in Figure 1. The .rst pass is a scanner/parser \nwhich transforms MATLAB source into an abstract syntax tree (AST). MaJIC s parser is basedonFALCON sparserwitha \nfew minor improvements.  Next, preliminary data .ow analysis (disambiguation) is performed to build \na static symbol table. At this point the compiler can optionally perform function in\u00adlining (which then \nnecessitates the re-building of the symbol table).  When the symbol table is complete, the compiler \nper\u00adforms type inference. This pass conservatively assigns types to all expressions in the program text. \nIn JIT compilation mode, the type inference engine uses run\u00adtime information fed to it by the repository; \nin spec\u00adulative mode, the inference engine uses only the AST and the symbol table and produces speculative \nresults.  The last step of the compilation is code generation.  There exists a code generator each \nfor JIT and specu\u00adlative mode. The JIT code generator builds code fast and in memory; in speculative \nmode, the code gener\u00adator builds C or Fortran source code, which is then compiled and linked with platform \nnative tools. In the next few sections we present some of the compiler passes in more detail. 2.1 Disambiguating \nMATLAB symbols Other than keywords, symbols in MATLAB can repre\u00adsent variables, calls to built-in primitives, \nor calls to user functions. The interpreter recognizes a symbol as a variable when it appears on the \nleft side of an assignment, or else if it has an entry in the dynamic symbol table of the interpreter. \nA symbol not recognized as a variable is potentially a built\u00adin primitive; if it cannot be resolved as \neither a variable or a built-in, the MATLAB interpreter also consults the dy\u00adnamic table of existing \nuser functions. If the symbol cannot be found there either, its occurrence is treated as an error. Unlike \nthe MATLAB interpreter, MaJIC needs to iden\u00adtify symbol meanings at compile time; but some symbols meanings \nare hard to determine without running the code. Figure 2 shows code with ambiguous symbols. The left \nbox shows a loop where the .rst occurrence of the symbol i is v ambiguous, interpreted by MATLAB as \n-1inthe .rst iteration, and as a variable in all following iterations. The right code box contains a \nloop where compiler analy\u00adsis would recognize the right-hand-side occurrence of y is a possible unde.ned \nvariable, or even a user function, if con\u00adtrol .ow is not taken into account. Looking at control .ow, \nhowever, makes it obvious that y can only be accessed only after having been de.ned. clear clear while(...), \n x=0; z=i; for p=1:N, i = z+1; if (p = 2) thenx=y; end y=p; end Figure 2: Ambiguous symbols in MATLAB \n Ambiguous symbols are rare in practice and almost always a sign of buggy code. MaJIC does deal with \nthem: it de\u00adfers their processing until runtime. Non-ambiguous variables can, however, be identi.ed at \ncompile time by a variation of reaching de.nitions analysis: a symbol that has a reach\u00adingde.nition asavariableon \nall paths leading to it must be a variable. This analysis is the .rst pass of the MaJIC compiler. 2.2 \nThe type system MaJIC s type system is used by the type inference engine and by the code repository. \nThe type system is inspired from that of FALCON, which in turn was in.uenced by the APL [6] and SETL \ncompilers. MaJIC s notion of a type is represented by the Cartesian product of several lattices as follows: \nThe intrinsic type of the expression is an element in the .\u00adnite lattice Li formed by the elements real, \ninteger, boolean, complex and string, and the requisite comparison operator: Li ={J , .i, Ti, .i, Ui}, \nwhere J = {.i, bool, int, real, cplx, strg, Ti} .i .i bool .i int .i real .i cplx .i Ti and .i .i strg \n.i Ti A MaJIC expression s shape Ls consists of a pair of val\u00adues, one each for the number of rows and \ncolumns of the expression. In the current version of MaJIC we only con\u00adsider Fortran-like two-dimensional \nshapes: Ls ={N \u00d7 N, .s, Ts, .s, Us}, where .s =< 0, 0 >, Ts =< 8, 8 >; <a,b >.s<c,d > i. a = c and b \n= d An expression s range Ll is the interval of values the ex\u00adpression can take [4]. We de.ne ranges \nonly for real num\u00adbers; strings and complex expressions do not have associated ranges. The two numbers \nin the range de.ne the (inclusive) lower and upper limits of an interval. The lower limit is al\u00adways \nless than or equal to the upper limit, or else the range is malformed: Ll ={R \u00d7 R, .l, Tl, .l, Ul}, where \n.l =< nan, nan >; Tl =< -8, 8 >; <a,b >.l<c,d > i. <a,b >= .l or (c = a and b = d) The type system is \nthe Cartesian product T = Li \u00d7 Ls \u00d7 Ls \u00d7 Ll. Ls appears twice, because MaJIC tracks lower as well as \nupper bounds of shape descriptors. We will use the collective denominator shape to mean both descriptors \ntogether. Thus the type system consists of intrinsic type, shape and range information. 2.2.1 Type signatures \nSuppose that a function we are compiling has n formal parameters {f1,f2, ...fn}. We assign the following \ntypes to the parameters: T = {T1,T2, ...Tn},where Ti .T , 1 = i = n. We call T the type signature of \nthe compiled code. We use type signatures to determine whether compiled code is safe to execute, given \na particular invocation. MaJIC generates code in such a way that an invocation of the com\u00adpiled code \nwith the actual parameters {a1,a2, ...an} having types {Q1,Q2, ...Qn} is safe if Qi . Ti, 1 = i = n.An \nac\u00adtual invocation is safe as long the types of the inputs are subtypes of the type signature of the \ncompiled code. The code repository may contain, at any time, several compiled versions of the same code, \ndi.ering only in the assumptions about the types of input parameters (Figure 3 shows a simple function \nwith a single parameter as an exam\u00adple). The function locator has to match a given invocation to a version \nof compiled code in the repository that is safe to execute (i.e. preserves the semantics of the program), \nand at the same time is optimal performance-wise. In order to do so, the function locator checks the \ntype signature of the invocation against the signatures of the existing com\u00adpiled objects in the repository, \nuntil a matching object is found or all repository objects are exhausted. When sev\u00aderal matching objects \nexist, the code repository uses simple heuristics to .nd the best matching candidate for a particu\u00adlar \ncall, based on a Manhattan-like distance between the type signature of the invocation and the matching \ncompiled code.  2.3 Type inference The type inference engine is an iterative join-of-all-paths monotonic \ndata analysis framework [17]. It starts out with the control .ow graph (CFG) of a MATLAB program and, \nin the case of JIT type inference, a type signature T (where |T |is equal to the number of formal parameters \nof the function that is being compiled). The result of type inference is a set of type annotations S, \none type for each expression node in the abstract syntax tree. S is a conservative estimate of the types \nthat expression nodes can assume during execution. The annotations are later used by the code generator. \nBecause MaJIC has a relatively simple type system, and because the type inference engine avoids symbolic \ncompu\u00adtation and caps the number of iterations, the type inference engine is fast enough for use by the \nJIT compiler. 2.3.1 Transfer functions The transfer functions of the type inference engine are im\u00adplemented \nas a set of rules in a type calculator. The calcu\u00adlator has two modes of operation: in forward mode it \ninfers expression types from argument types; in backward mode it infers argument types from the expressions \ntypes (this mode is used by the type speculator). Multiple type calculation rules may exist for each \nAST node type. Each rule is guarded by a boolean precondition. When the type calculator is invoked with \na particular AST node as argument, the corresponding rules preconditions are tested in order until one \nevaluates to true; the rule is then applied to calculate the result(s). A rational way of ordering type \ninference rules is to progress from the most restrictive ones to the least restrictive ones. Evaluating \nmore restrictive rules .rst makes sense because these generally lead to better performance, whereas more \ngeneral rules tend to yield generic, low performance code. If no rules preconditions evaluate to true, \nthe type calculator applies the implicit default rule: all output types are set to T. This allows the \ntype inference engine to behave conser\u00advatively for language constructs that have no corresponding rules \nin the database. Thus for example the * operator in MaJIC can be eval\u00aduated successively as an instance \nof: integer scalar multiply; real scalar multiply; complex scalar multiply; real scalar \u00d7 vector or vector \n\u00d7 scalar; part of a dgemv operation; or a MATLAB code type signature generated code: C + MATLAB C library \nfunctions function p=poly(x) p = x. 5+3*x+2; return itype(x)=int shape(x)=scalar limits(x)=<3,3> int \npoly1 sig0() {return 254; } itype(x)=int shape(x)=scalar limits(x)=Tl int poly1 sig1(int x) {return x*x*x*x*x+3*x+2; \n} itype(x)=real shape(x)=scalar limits(x)=Tl int poly1 sig2(double x) {return x*x*x*x*x+3.0*x+2.0; } \nitype(x)=real minshape(x)=<1,3> maxshape(x)=<1,3> limits(x)=Tl double *poly sig3(double x[3]) {static \ntmp2[3]; tmp2[0]=x[0]*x[0]*x[0]*x[0]*x[0]+3.0*x[0]+2.0; tmp2[1]=x[1]*x[1]*x[1]*x[1]*x[1]+3.0*x[1]+2.0; \ntmp2[2]=x[2]*x[2]*x[2]*x[2]*x[2]+3.0*x[2]+2.0; } itype(x)=complex shape(x)=Ts limits(x)=Tl mxArray *poly4 \nsig1(mxArray *x) {mxArray *tmp1 = mlfScalar(5.0); mxArray *tmp2 = mlfPower(x,tmp1); mxFree(tmp1); mxArray \n*tmp3 = mlfScalar(3.0); mxArray *tmp4 = mlfTimes(tmp3,x); mxFree(tmp3); mxArray *tmp5 = mlfPlus(tmp2, \ntmp4); mxFree(tmp2); mxFree(tmp4); mxArray *tmp6 = mlfScalar(2.0); mxArray *tmp7 = mlfPlus(tmp5, tmp6); \nmxFree(tmp5); mxFree(tmp6); return tmp7; } Figure 3: Type signatures and generated code.The operators \nitype(x), shape(x) and limits(x) in the second column refer to type components from the type lattice \nde.ned earlier. generic complex matrix multiply. This does not exhaust all possibilities, but these are \nthe categories that MaJIC can generate successively less optimized code for. Currently, MaJIC s type \ncalculator contains about 250 rules. Each MATLAB expression/operator type has at least one entry in the \ndatabase; many of MATLAB s built-in func\u00adtions have several entries each. Our current implementation \ncovers just enough of MATLAB to execute the benchmarks e.ciently. The type inference engine can handle \nall other language features by resorting to the default.  2.4 JIT type inference In JIT mode, the type \ncalculator performs only forward analysis. Type inference propagates the type signature of the function \nto calculate type annotations for the function body. Since the type inference system is biased towards \nmore speed in detriment of precision, one would expect the quality of type annotations to su.er when \nperforming just\u00adin-time type inference. However, JIT type inference operates with very precise initial \ndata: the type signature of the code, derived directly from the input values of the runtime invo\u00adcation. \nUnder these circumstances type inference is not only precise but lends itself to a number of extensions, \nwhich ex\u00adtract additional information from the type inference process at little or no additional cost: \nConstant propagation: Range propagation (the part of type inference which deals with the Ll lattice) \ncan be thought of as a generalization of constant propa\u00adgation for real scalars. A real value is a constant \nif its lower and upper limits are equal. Given a type signature that contains many constants, most of \nthe transfer functions are able to calculate exact lower and upper limits for scalar objects, e.ectively \nperforming constant propagation as part of type inference. Range propagation does not work for complex \nnumbers and non-numeric values, so constant propagation does not work for these either. Exact shape inference: \nMaJIC propagates lower and upper bounds for array shape information. An ar\u00adray s shape is exactly determined \nif the lower and up\u00adper shape bounds are equal. Just as constants can be determined given good input \ndata, exact array shapes can be determined also. Sometimes value range propa\u00adgation and shape propagation \ncollaborate on determin\u00ading exact shapes. For example, in the statement A= zeros(m,n), the value ranges \nof m and n may uniquely determine the shape of A. In array assignments of the form A(i)=..., the range \nof the index can determine the shape of the array A (because MATLAB arrays reshape themselves to ac\u00adcommodate \nindices). There are a number of ways in which exact shapes can be used to achieve better performance. \nFor ex\u00adample, by completely unrolling simple operations on small arrays we can eliminate all control \n.ow from the operation. Subscript check removal: MATLAB mandates sub\u00adscript checks on all array accesses. \nThe removal of unnecessary subscript checks is a major source of per\u00adformance enhancement in MaJIC. Older \nversions of MATLAB s own compiler, mcc,have command line switches to disable subscript checks (in\u00adcluding \nresizing checks). This can cause code that oth\u00aderwise is correct to run incorrectly when compiled with \nmcc. Newer versions of mcc have consequently discon\u00adtinued the option. MaJIC removes subscript checks \nautomatically and conservatively, by using the range and shape informa\u00adtion readily available during \ntype inference. Because JIT type inference propagates these exactly, the extra e.ort needed for subscript \ncheck analysis is extremely low, comparing favorably with more conventional tech\u00adniques [13].  2.5 Type \nspeculation Just-in-time type inference assumes that the full calling context (i.e. the type signature) \nis available to the analyzer. By contrast, type speculation assumes nothing about the calling context: \nit guesses the likely types of the arguments. This allows the compiler to process the code ahead of time, \napplying advanced (and time consuming) loop optimizations in order to generate good code. The type speculator \ns trick is to back-propagate certain type hints from the body of the code to the input parame\u00adters. Type \nhints are collected from syntactic constructs that suggest, but do not command, particular semantic meanings. \nThese constructs originate in part from programmers ten\u00addency to avoid arcane MATLAB speci.c constructs, \nstick\u00ading instead to features already prevalent in Fortran-like lan\u00adguages. Other hints can be derived \nfrom some MATLAB built-in functons a.nity towards certain inputs. The fol\u00adlowing list summarizes the \ntype hints used by MaJIC s speculator: When processing the colon operator (:), used to spec\u00adify index \nranges, MATLAB silently ignores the imag\u00adinary part of the index arguments. Even if the index is a complex \narray, only the real part of its .rst el\u00adement is used, and all indices are of course rounded before \nuse. This suggests that operands of the interval operator are almost always integer scalars.  Relational \noperators disregard the imaginary compo\u00adnents of their operands. Also, relational operations between \nvectors are possible but are rare in practice, since their semantics are non-intuitive. This holds even \nstronger for expressions that form the condition of an if-statement of a while-statement.  The MATLAB \nbracket operator (vector constructor) collates several matrices into a new larger matrix. The components \nall have to have either the same number of rows or the same number of columns. In practice the bracket \noperator is often used to build vectors out of scalars. When we can prove that one of the arguments xi \nof the bracket operator [x1x2...xn] is a scalar, all other arguments are probably scalars too.  In matrix \nindex expressions of the form A(idx) and A(idx1,idx2), if the subscript is an expression or a variable \nthen it is likely scalar. This is a reasonable as\u00adsumption because a many MATLAB applications use either \nFortran77 or Fortran90 compatible array index\u00ading operations. Fortran90 syntax is indicated by the presence \nof the colon (:) operator; the lack of colons indicates Fortran77 syntax.  Arguments to a number of \nbuiltin functions, such as zeros, ones, rand, the second argument of size and many others, are likely \ninteger scalars. MATLAB is\u00adsues warnings when the arguments in question are non\u00adscalars or non-integers, \nbut does not stop processing. However most well-written MATLAB programs don t intentionally produce these \nwarnings. These hints are implemented as type calculator rules. Note that the hints involve backwards \npropagation of types, since they make statements about input arguments rather than the result types of \nMATLAB expressions. Thus, in order to propagate hints, the type inference engine must be used in backwards \nmode. Speculative type inference consists of a number of alter\u00adnating backward and forward type inference \npasses. A spec\u00adulative (backward) pass infers a credible type signature from the code body; it is immediately \nfollowed by a normal type inference pass to re-calculate the types in the body. The alternating backwards-forwards \nprocess can be iterated sev\u00aderal times until convergence. 2.6 Code generation MaJIC has two code generation \nsystems: a fast lightweight code generator used for JIT compilation, and a C (or For\u00adtran) based code \ngenerator that uses the host system to com\u00adpile, optimize and link the code. Both code generators use \nthe parsed AST and type annotations to drive code selec\u00adtion. The code generators follow the same general \nselection rules, but build radically di.erent code. The JIT code generator is able to build executable \ncode directly in memory by using the vcode [11] dynamic assem\u00adbler. The code generator makes a single \ncode selection pass through the parsed AST. No loop optimizations or instruc\u00adtion scheduling are performed. \nRegister allocation is done using the linear-scan register allocator [19]. This, and the small total \nnumber of code generation passes, results in a fast code generator. The source code generator is somewhat \nmore complicated. It uses the same code selection pass as the JIT code gener\u00adator, but builds C or Fortran \nsource code in a temporary .le. This .le is then compiled with the native compiler us\u00ading the most aggressive \noptimization mode that is available. The compiler generates a relocatable object which is then dynamically \nlinked into the MaJIC executable. Unlike the JIT code generator, the source code generator is quite slow, \nhampered by the large overheads of loading and executing the compiler and the linker. Compilation, optimization \nand linking can take several seconds. 2.6.1 Code selection rules As mentioned before, the two code generators \nuse the same selection rules even though they use them to produce di.erent code. A few of the selection \nrules are listed below. The implicit default rule for any operator is that the numeric operands are \ncomplex matrices. This is the unoptimized fall-back option for operations that have not been type-inferred. \nThe MATLAB C library pro\u00advides functions that implement these generic opera\u00adtors.  MaJIC inlines scalar \narithmetic and logical opera\u00adtions, elementary math functions and assignments of  scalar integers, reals \nand complex numbers. This is probably the most important performance optimiza\u00adtion in MaJIC: it relies \non type annotations to re\u00adplace MATLAB s polymorphic operations with single machine instructions. MaJIC \ninlines scalar and F90-like array index oper\u00adations. The MATLAB interpreter discriminates be\u00adtween array \nexpressions types at runtime, spending hundreds of cycles. By contrast, an inlined scalar in\u00addex operation \ntakes only a few cycles.  Small temporary arrays of known sizes are pre-allocated. MATLAB s expression \nevaluation semantics sometimes forces the existence of temporary bu.ers to hold in\u00adtermediary array results. \nReplacing dynamically allo\u00adcated bu.ers with statically allocated ones saves a lot of overhead at the \nexpense of a small amount of heap memory.  Elementary vector operations, such as arithmetic op\u00aderations \nand vector concatenation, are completely un\u00adrolled when exact array shapes are known. This tech\u00adnique \nis very e.ective on small (up to 3 \u00d7 3) matrices and vectors because it completely eliminates loop over\u00adhead. \n MaJIC performs code selection to combine several AST nodes into a single library call. For example, \nex\u00adpressions like a*X+b*C*Y are transformed into a single call to the BLAS routine dgemv [7].  Unlike \nFortran, MATLAB resizes arrays on demand. In general, this occurs when an array index over.ow occurs \non the left hand side. Repetitive array resiz\u00ading (e.g. in a loop) can be tremendously expensive. MaJIC \napplies the simple but e.ective technique of oversizing arrays, i.e. allocating about 10% more space \nfor a resized array than strictly necessary, so that subsequent growth of the array does not necessi\u00adtate \nanother resize operation. MaJIC performs oversizing carefully in order to pre\u00adserve the original semantics \nof the code. The oversized array, when queried, returns accurate size information. Oversizing is also \nlimited by the amount of available memory and the size of the array. Large arrays are never oversized. \n MaJIC inlines calls to small (less than 200 lines of code) functions. Inlining preserves the call-by-value \nsemantics of MATLAB by making copies of the actual parameters. However, read-only formal parameters are \nnot copied. This can result in huge performance gain when large matrices are being passed as read-only \nar\u00adguments in the call.    3. PERFORMANCE EVALUATION In this section, we evaluate the overall performance \nof the MaJIC compiler. Although the repository is part of the interactive MaJIC system, an evaluation \nof its performance is not a goal of this paper. We were interested in analyzing the quality and speed \nof the JIT and speculative compilers. To test JIT compilation, we started our experiments with an empty \nrepository. This resulted in the JIT compiler being invoked for any function call. To test speculative \ncompilation, we also started up MaJIC with an initially empty repository, but we invoked the bench\u00admarks \nonly after MaJIC s repository had ample time to .nd them and compile them speculatively. 3.1 Benchmarks \nMaJIC was tested with 15 MATLAB benchmarks, be\u00adtween 50 and 250 lines long each. Table 1 lists the names, \norigin and functional description of the benchmarks, as well as the associated problem size for which \nmeasurements were run (matrix sizes in some of the benchmarks). In addition, we list the number of lines \nin each benchmark and the run\u00adtime on a reference system (the SPARC platform described in Section 3.3) \nusing a stock MATLAB interpreter. Many of the benchmarks were originally used to evalu\u00adate FALCON; we \nreused them in order to facilitate a direct comparison of MaJIC and FALCON. In order to make the subsequent \ndiscussion easier, we group the benchmarks into four partially overlapping cat\u00adegories. Benchmarks in \nthe same categories tend to be op\u00adtimized in similar ways by the compiler, and show similar performance \ngains: Scalar, or Fortran-like, benchmarks: dirich, finedif, icn, mandel and, to some extent, crnich, \nare written in a style that closely resembles Fortran 77. All array indices in these benchmarks are scalars. \n Benchmarks with built-in functions: cgopt, qmr, sor and mei spend a large portion of their runtime \nin built\u00adin MATLAB library functions. Typically, these codes are hard to optimize, since the the library \nfunctions themselves are already optimized.  Array benchmarks: orbec, orbrk, fractal and adapt have \nmany operations on small .xed size MATLAB vectors. adapt features a large (and dynamically grow\u00ading) \narray as well as small vectors.  Recursive benchmarks: fibo and ack contain recur\u00adsion, which makes \ninlining and type inference harder.  3.2 Measurement methodology Our performance .gures are derived \nfrom the running times of the benchmarks. The most important gauge of performance we use is the speedup \nof compiled code relative to interpreted code, i.e. the expression s = ti/tc,where ti is the runtime \nof the code in MATLAB s interpreter, and tc is the runtime in the compiler. We measured MaJIC s speedups \nin both JIT and spec\u00adulative compilation mode. In JIT mode runtime includes the time spent by the JIT \ncompiler producing object code. In speculative mode the repository is assumed to have a generated the \ncode ahead of time; hence compile time is not included in the runtime in this case, unless the speculatively \ngenerated code turns out not to match the benchmarks in\u00advocation in this latter case the JIT compiler \nkicks in and helps out with the code generation. This mode of measur\u00ading runtimes is consistent with \nthe expected real-world usage pattern of MaJIC. For purposes of comparison, we also measured the speedups \nof mcc, the compiler supplied by the Mathworks Inc. We set a number of compile time options for this \ncompiler in order to guarantee the best performance: we manually eliminated benchmark source short description \nproblem size lines of code runtime (s) adapt [14] adaptive quadrature approx. 2500 81 5.24 cgopt [3] \nconjugate gradient w. diagonal preconditioner 420 x 420 38 0.43 crnich [14] Crank-Nicholson heat equation \nsolver 321 x 321 40 16.33 dirich [14] Dirichlet solution to Laplace s equation 134 x 134 34 277.89 finedif \n[14] Finite di.erence solution to the wave equation 1000 x 1000 21 57.81 galrkn [12] Galerkin s method \n(.nite element method) 40 x 40 43 8.02 icn R. Bramley Cholesky factorization 400 x 400 29 7.72 mei unknown \nfractal landscape generator 31 x 14 24 10.77 orbec [12] Euler-Cromer method for 1-body problem 62400 \npoints 24 19.10 orbrk [12] Runge-Kutta method for 1-body problem 5000 points 52 9.30 qmr [12] linear \nequation system solver, QMR method 420 x 420 119 5.29 sor [3] lin. eq. sys. solver, successive overrelaxation \n420 x 420 29 4.77 ackermann authors Ackermann s function ackermann(3,5) 15 3.84 fractal authors Barnsley \nfern generator 25000 points 35 26.55 mandel authors Mandelbrot set generator 200 x 200 16 8.64 fibonacci \nauthors recursive Fibonacci function .bonacci(20) 10 1.29 Table 1: MaJIC benchmarks subscript checks, \nand replaced operations on complex num\u00adber with real number operations where it was safe to do so. We \nmeasured the speedups of FALCON by repeating the experiments described in [9] on our test machines. We \nin\u00adstructed FALCON to eliminate subscript checks wherever this did not break the code. Execution times \nwere measured on a best of 10 runs basis on a quiet system. Our performance graphs show four bars for \neach bench\u00admark. The four bars are the speedups achieved by mcc, FALCON, MaJIC in JIT mode and MaJIC \nin speculative mode respectively. Because the speedups are distributed over four orders of magnitude, \nranging from 0.1 to about 1000, the graphs are represented on a logarithmic scale. 3.3 Testing platforms \nand speedups We measured the interpreted execution time ti of all bench\u00admarks using the MATLAB 6 (release \n12) integrated environ\u00adment on two architectures: The development platform for MaJIC is a 400MHz UltraSparc \n10 workstation with 256MB of RAM, run\u00adning Solaris 7 and equipped with the Sparcworks 5.0 C compiler. \nThe performance results for this machine aresummarizedinFigure4. As described above, the .gure has bars \nfor each bench\u00admark, called mmc , falcon , jit and spec respec\u00adtively. A few of the speedup bars are \nmissing: there are no FALCON speedup bars for the benchmarks ack, fractal, fibo and mandel, because these \nwere not part of the original FALCON benchmark series and are unsuitable for compilation with FALCON. \nThe speedup bars of cgopt appear to be missing be\u00adcause they are very close to 1.0. We also ran some \nof the experiments on an SGI Origin 200 machine equipped with 4 180MHz R10000 proces\u00adsors, IRIX 6.5 and \nthe MIPSPro C compiler. The JIT compiler on this platform is not yet completely imple\u00admented. Some benchmarks \n(like adapt)wereleftout of the graphs for this reason. Others are included, but 1000 100 10 1 0.1 Figure \n4: Performance on the SPARC platform run at reduced performance due to the poor quality of the generated \ncode. Figure 5 shows the results.  3.4 Comparative performance analysis The two groups of benchmarks \nthat most clearly bene.t from compilation are the Fortran-like benchmarks and the small vector benchmarks. \nThese types of codes incur the most overhead during interpreted execution; they pro.t the most from the \nremoval of overhead. By contrast, the benchmarks that are heavy in built-in function calls bene.t very \nlittle, and sometimes not at all, from compilation. Obviously, the execution speed of built-in functions \nis not in.uenced by compiling the calling code. The orbrk benchmark demonstrates that inlining at com\u00adpile \ntime is bene.cial. Recursive functions like fibo and ack also generally bene.t from inlining. MaJIC does \nnot attempt to inline more than 3 levels of recursive calls in order to avoid code explosion. While mcc \nis not particularly successful at removing the interpretive overhead, both FALCON and MaJIC do suc\u00adceed \nin eliminating it, although using di.erent strategies. FALCON relies heavily on the native Fortran compiler \nto generates relatively poor code, causing MaJIC to outper\u00ad 80% generator was optimized for this platform. \nThe performance 20% .gures are remarkable when considering that the code in question is generated in \na fraction of a second and with\u00ad 0% out the bene.t of backend optimizations. On the other hand there \nis room for future optimizations; however, be\u00adfore adding these on, it will be necessary to test whether \nthe increased compile time will destroy the performance gained  Figure 5: Performance on the MIPS platform \nFigure 6: The composition of JIT execution generate good code. MaJIC has a few speci.c optimiza\u00ad tions \n(described in Section 2.6.1) that make it less reliant on the native compiler and allow it to generate \nreasonable code even with the JIT code generator. 100% On the SPARC platform the native Fortran-90 compiler \nFor the analysis of JIT compilation we rely mostly on re\u00adsults gathered on the SPARC platform, since \nthe JIT code 3.5 Analysis of JIT compilation JIT compiler to fall behind FALCON. form FALCON in a few \nof the benchmarks. On the MIPS platform the native compiler is excellent, causing MaJIC s performance \nrelative to fully optimized JIT crnich dirich 40% 60% crnichdirichfinediffinedificn icnmandelcgopt mandel \ncgopt meimeiqmrqmrsorsoradaptadaptorbecorbecorbrkorbrk fractalfractal galrkn galrkn ackackfibo fibo \n8 by optimization. Figure 6 shows the time composition of the runtime of each JIT-compiled benchmark. \nWith the exception of orbrk, most benchmarks spend a relatively modest amount of time compiling the code. \nThe compile time/runtime ratio is ar\u00adti.cially high anyway, in part because the benchmarks run on modestly \nsized problems. There is de.nitely room for at least basic back-end optimizations in the JIT compiler, \nsuch as common subexpression elimination, loop unrolling, loop invariant removal and some form of instruction \nschedul\u00ading. Preliminary experiments with the finedif and dirich benchmark suggest that loop unrolling \nalone can reduce ex\u00adcution time by about 50% at a reasonable cost in overhead. 3.5.1 The effect of existing \nJIT optimizations The e.ect of optimizations in any compiler is cumulative and hard to study in isolation. \nIn this section we evalu\u00adate the e.ectiveness of JIT-speci.c optimizations by indi\u00advidually disabling \nthem and studying the resulting drop in performance. Figure 7 shows the measurement results. The .rst \nset of bars ( no range ) was obtained by dis\u00adabling range propagation during JIT type inference. The \nprimary e.ect of this measure is to disable subscript check Figure 7: Disabling JIT optimizations removal. \nThe relative increase in execution time is highest in the benchmarks that have many array accesses: dirich, \nfinedif and mandel are good examples. The second set of bars ( no min. shape ) was obtained by disabling \nthe propagation of minimum shape informa\u00adtion. This disables subscript check removal in some cases, and \ndoes not allow the compiler to unroll small vector op\u00aderations. orbec, orbrk and fractal are the most \na.ected, because these consist mostly of operations on small vectors and matrices. The last set of bars \n( no regalloc ) was obtained by forc\u00ading the linear-scan register allocator to spill every variable. \nThis is roughly equivalent to compiling with the -g .ag set on a regular compiler like gcc. The results \nclearly show that range propagation, mini\u00admum shape propagation and register allocation are essential \nto JIT performance.  3.6 Analysis of the speculator The speedup results produced by speculation generally \nmatch those of FALCON. We conclude that speculation is generally successful. However, we cannot expect \na spec\u00adulative technique to be universally successful; we need to analyze the consequences of failure. \nMaJIC s type speculator fails in two ways: by being too aggressive, and generating useless code, or by \nnot being ag\u00adgressive enough and generating suboptimal code. The .rst type of failure, unreasonable specialization \nof input types, is easily countered: the type signature check, done by the repository at runtime, will \neliminate such code from consid\u00aderation. A more insidious failure is when the speculator generates code \nthat is perfectly safe to execute, but suboptimal. Such cases are not caught at runtime. The performance \nof the invoked code will be lower, but it is not immediately clear by how much. benchmark crnich dirich \n.nedif icn mandel spec. 181 817 412 48 36 JIT 181 817 413 51 54.0 benchmark cgopt mei qmr sor adapt spec. \n1 4.24 4.52 1.68 4.09 JIT 1.16 5.67 5.68 1.79 4.16 benchmark orbec orbrk fractal galrkn ack spec. 146 \n465 663 61.7 4.04 JIT 174 465 664 72.9 6.00 benchmark .bo spec. 3.49 JIT 5.16 Table 2: JIT vs.speculative \ntype inference Table 2 attempts to quantify the speculator s performance. It compares the speedups produced \nby the same code gen\u00aderator using type annotations generated with either specu\u00adlation or JIT type inference \n(the speedups were calculated without considering compile time). Looking at this table, it is obvious \nthat speculative type inference closely matches the performance of JIT type inference in many cases. \nWe conclude that Speculation works best on scalar (Fortran 77-like) and vector codes. Speculative rules \nlook for exactly the kinds of features that are prevalent in these codes.  Benchmarks with built-in \nfunctions typically fare badly because the speculative rules currently present in MaJIC do not account \nfor the language features used by these codes. MaJIC mispredicts a * operator in qmr to represent scalar \nmultiplication, whereas in fact it is a matrix-vector multiplication. In mei the speculator is unable \nto predict that the arguments to an eig func\u00adtion call are reals; instead it considers them complex values \nwhich leads to performance loss. A similar sit\u00aduation occurs in mandel due to the use of the built-in \nfunction i.  Recursive benchmarks are not handled correctly by speculative compilation. They always \nneed to be re\u00adcompiled at runtime.   4. RELATED WORK MaJIC is patterned after FALCON [9, 8], a MATLAB \nto Fortran-90 translator developed by L. DeRose in 1996. FALCON performs type inference to generate declarations \nfor variables. It then generates Fortran code using these declarations. However, FALCON s type inference \nengine is facing a limiting factor. Because FALCON is a batch com\u00adpiler, it has no information about \nthe calling context of the functions it tries to compile. This makes type inference po\u00adtentially ine.ective. \nFALCON circumvents this problem by peeking into the input .les of the code it compiles and extracting \ntype information from there. MENHIR [5], developed by Francois Bodin at INRIA, is another batch compiler \nsimilar to FALCON: it generates code for MATLAB and exploits parallelism by using op\u00adtimized runtime \nlibraries. MENHIR s code generation is retargetable (it generates C or FORTRAN code). It also contains \na type inference engine similar to FALCON s. MATCH [2] is a MATLAB compiler targeted to hetero\u00adgeneous \narchitectures, such as DSP chips and FPGAs. It also uses type analysis and generates code for multiple \nlarge functional units. Vijay Menon s vectorizer [16] is an alternative to compi\u00adlation. Menon observed \nthat scalar operations in MATLAB were slower than vector operations because they involved more overhead \npear .oating-point operation. He proposed to eliminate this overhead not by compilation, but by trans\u00adlating \nFortran 77-like scalar operations into Fortran 90-like vector expressions in the MATLAB source code. \nMenon s vectorizer is built on top of the MaJIC infrastructure. Just-in-time compilation has been around \nsince 1984, when Deutsch described a dynamic compilation system for the Smalltalk language [10]. The \ntechnique became truly pop\u00adular with the Java language, and countless Java JIT com\u00adpilers have been proposed \nand implemented in recent times. MaJIC s JIT compiler reuses code and ideas from the vcode [11] and tcc \n[18] packages. vcode was originally built as a general\u00adpurpose, platform-independent RISC-like dynamic \nassembly language to facilitate dynamic code generation, and is used in almost unchanged form by MaJIC. \ntcc was built on top of vcode and provides an implementation of C, a C-like programming language with \na LISP-like backquote operator that facilitates the building of dynamic code by composition. We did not \nreuse the C parser, but we did use the tcc inter\u00admediate language speci.cation, ICODE, and re-implemented \nthe register allocator used by tcc. 5. CONCLUSIONS In an e.ort to bring high performance to the MATLAB \nin\u00adtegrated environment, we have designed, built and evaluated two paradigms for compiling MATLAB code: \nJIT compila\u00adtion and speculative compilation. JIT compilation is remarkably successful in bringing down \ncompile time to almost nil, while obtaining reasonable per\u00adformance gains (up to two orders of magnitude \nfaster than the MATLAB interpreter). It falls behind in terms of per\u00adformance when compared to the best \nthat a static compiler (like FALCON) can do. Of our benchmarks, the most af\u00adfected were the Fortran-like \nand small vector codes, where the lack of backend optimization is felt the most. In order to estimate \nthe e.ect of adding more optimiza\u00adtions to the JIT compiler, we hand-optimized the finedif benchmark \nby hand-unrolling its innermost loop and per\u00adforming common subexpression elimination. We obtained a \nversion of finedif that was almost 100% faster than the normal JIT-compiled finedif, and within 20% of \nthe perfor\u00admance of the best (native compiler-generated) version of the code. Preliminary data suggests \nthat similar, although less impressive, performance improvements can be obtained with some of the other \nFortran-like benchmarks, which leaves the door open for future enhancements of the JIT compiler. Speculative \ncompilation is successful in bringing up per\u00adformance to and beyond FALCON levels. However, gen\u00aderation \nof optimized code takes time; speculation is designed to allow the hiding of compilation latency. Speculation \nis not universally successful; it can result in loss of performance when it fails. It is interesting \nto note that the speculative type hints that are used most successfully by MaJIC s speculator are tied \nto the very same language features of MATLAB that slow down the interpreter. Hence, speculation tends \nto succeed when it is most needed. 6. REFERENCES [1] George Almasi. MaJIC: a Matlab Just-In-time Compiler. \nPhD thesis, University of Illinois at Urbana-Champaign, June 2001. [2] P. Banerjee, N. Shenoy, A. Choudhary, \nS. Hauck, C. Bachmann, M. Chang, M. Haldar, P. Joisha, A. Jones, A. Kanhare, A. Nayak, S. Periyacheri, \nand M. Walkden. Match: A matlab compiler for con.gurable computing systems. Technical Report CPDC-TR-9908-013, \nCenter for Parallel and Distributed Computing, Northwestern University, Aug. 1999. [3] R. Barrett, M. \nBerry, T. F. Chan, J. Demmel, J. Donato,J.Dongarra,V.Eijkhout, R. Pozo, C. Romine, and H. Van der Vorst. \nTemplates for the Solution of Linear Systems: Building Blocks for Iterative Methods, 2nd Edition. SIAM, \nPhiladelphia, PA, 1994. [4] William Blume and Rudolf Eigenmann. Symbolic range propagation. In Proceedings \nof the 9th International Parallel Processing Symposium, April 1995. [5] Francois Bodin. MENHIR: High \nperformance code generation for MATLAB. http://www.irisa.fr/caps/PEOPLE/Francois/. [6] Timothy Budd. \nAn APL Compiler. Springer Verlag, 1988. [7] J. Choi, J. Dongarra, and D.W. Walker. BLAS reference manual \n(version 1.0beta). Technical Report ORNL/TM-12469, Oak Ridge National Laboratory, March 1994. [8] Luiz \nDeRose and David Padua. Techniques for the translation of MATLAB programs into Fortran 90. ACM Transactions \non Programming Languages and Systems (TOPLAS), 21(2):285 322, March 1999. [9] Luiz Antonio DeRose. Compiler \nTechniques for MATLAB Programs. Technical Report UIUCDCS-R-96-1996, Department of Computer Science, University \nof Illinois, 1996.  [10] L Peter Deutsch and Alan Schi.man. E.cient Implementation of the Smalltalk-80 \nSystem. In Proceedings of the 11th Symposium on the Principles of Programming Languages, Salt Lake City, \nUT, 1984. [11] Dawson R. Engler. VCODE: a portable, very fast dynamic code generation system. In Proceedings \nof the ACM SIGPLAN Conference on Programming Languages Design and Implementation (PLDI 96), Philadelphia \nPA, May 1996. [12] Alejandro L. Garcia. Numerical Methods for Physics. Prentice Hall, 1994. [13] Rajiv \nGupta. Optimizing array bounds checks using .ow analysis. ACM Letters on Programming Languages and Systems, \n2(1-4):135 150, 1993. [14] John H. Mathews. Numerical Methods for Mathematics, Science and Engineering. \nPrentice Hall, 1992. [15] Mathworks Inc. homepage. www.mathworks.com. [16] Vijay Menon and Keshav Pingali. \nHigh-level semantic optimization of numerical codes. In 1999 ACM Conference on Supercomputing. ACM SIGARCH, \nJune 1999. [17] Steven S. Muchnick and Neil D. Jones. Program Flow Analysis: Theory and Application. \nPrentice Hall, 1981. [18] Massimiliano Poletto, Dawson R. Engler, and M. Frans Kaashoek. tcc: A system \nfor fast, .exible, and high-level dynamic code generation. In Proceedings of the ACM SIGPLAN Conference \non Programming Languages Design and Implementation (PLDI 97), pages 109 121, Las Vegas, Nevada, May 1997. \n [19] Massimiliano Poletto and Vivek Sarkar. Linear scan register allocation. ACM Transactions on Programming \nLanguages and Systems, 21(5):895 913, 1999.  \n\t\t\t", "proc_id": "512529", "abstract": "This paper presents and evaluates techniques to improve the execution performance of MATLAB. Previous efforts concentrated on source to source translation and batch compilation; <b>MaJIC</b> provides an interactive frontend that looks like MATLAB and compiles/optimizes code behind the scenes in real time, employing a combination of just-in-time and speculative ahead-of-time compilation. Performance results show that the proper mixture of these two techniques can yield near-zero response time as well as performance gains previously achieved only by batch compilers.", "authors": [{"name": "George Alm&#225;si", "author_profile_id": "81408595640", "affiliation": "University of Illinois at Urbana-Champaign", "person_id": "P348260", "email_address": "", "orcid_id": ""}, {"name": "David Padua", "author_profile_id": "81452612804", "affiliation": "University of Illinois at Urbana-Champaign", "person_id": "P63208", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/512529.512564", "year": "2002", "article_id": "512564", "conference": "PLDI", "title": "MaJIC: compiling MATLAB for speed and responsiveness", "url": "http://dl.acm.org/citation.cfm?id=512564"}