{"article_publication_date": "05-17-2002", "fulltext": "\n Dynamic Hot Data Stream Prefetching for General-Purpose Programs Trishul M. Chilimbi Martin Hirzel \nMicrosoft Research Computer Science Dept. One Microsoft Way University of Colorado Redmond, WA 98052 \nBoulder, CO 80309 trishulc@microsoft.com martin.hirzel@colorado.edu ABSTRACT Prefetching data ahead \nof use has the potential to tolerate the grow\u00ading processor-memory performance gap by overlapping long \nlatency memory accesses with useful computation. While sophisti\u00adcated prefetching techniques have been \nautomated for limited domains, such as scientific codes that access dense arrays in loop nests, a similar \nlevel of success has eluded general-purpose pro\u00adgrams, especially pointer-chasing codes written in languages \nsuch as C and C++. We address this problem by describing, implementing and evaluating a dynamic prefetching \nscheme. Our technique runs on stock hardware, is completely automatic, and works for general\u00adpurpose \nprograms, including pointer-chasing codes written in weakly-typed languages, such as C and C++. It operates \nin three phases. First, the profiling phase gathers a temporal data reference profile from a running \nprogram with low-overhead. Next, the profiling is turned off and a fast analysis algorithm extracts hot \ndata streams, which are data reference sequences that frequently repeat in the same order, from the temporal \nprofile. Then, the system dynamically injects code at appropriate program points to detect and prefetch \nthese hot data streams. Finally, the process enters the hibernation phase where no profiling or analysis \nis performed, and the program continues to execute with the added prefetch instructions. At the end of \nthe hibernation phase, the program is de\u00adoptimized to remove the inserted checks and prefetch instructions, \nand control returns to the profiling phase. For long-running programs, this profile, analyze and optimize, \nhibernate, cycle will repeat multiple times. Our initial results from applying dynamic prefetching are \npromising, indicating overall execution time improvements of 5 19% for several memory-performance-limited \nSPECint2000 benchmarks running their largest (ref) inputs. Categories and Subject Descriptors D.3.4 [Programming \nLanguages]: Processors code generation, optimization, run-time environments.  General Terms Measurement, \nPerformance.  Keywords dynamic profiling, temporal profiling, data reference profiling, dynamic optimization, \nmemory performance optimization, prefetching. Permission to make digital or hard copies of all or part \nof this work for personal or classroom use is granted without fee provided that copies are not made or \ndistributed for profit or commercial advantage and that copies bear this notice and the full citation \non the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires \nprior specific permission and/or a fee. 1. INTRODUCTION The demise of Moore s law has been greatly exaggerated \nand pro\u00adcessor speed continues to double every 18 months. By comparison, memory speed has been increasing \nat the relatively glacial rate of 10% per year. The unfortunate, though inevitable consequence of these \ntrends is a rapidly growing processor-memory performance gap. Computer architects have tried to mitigate \nthe performance impact of this imbalance with small high-speed cache memories that store recently accessed \ndata. This solution is effective only if most of the data referenced by a program is available in the \ncache. Unfortunately, many general-purpose programs, which use dynamic, pointer-based data structures, \noften suffer from high cache miss rates, and are limited by their memory system perfor\u00admance. Prefetching \ndata ahead of use has the potential to tolerate this processor-memory performance gap by overlapping \nlong latency memory accesses with useful computation. Successful prefetching is accurate correctly anticipating \nthe data objects that will be accessed in the future and timely fetching the data early enough so that \nit is available in the cache when required. Sophisticated automatic prefetching techniques have been \ndeveloped for scientific codes that access dense arrays in tightly nested loops (for e.g., [24]). They \nrely on static compiler analyses to predict the program s data accesses and insert prefetch instructions \nat appropriate program points. However, the reference pattern of general-purpose programs, which use \ndynamic, pointer-based data structures, is much more complex, and the same techniques do not apply. If \nstatic analyses cannot predict the access patterns of general\u00adpurpose programs, perhaps program data \nreference profiles may suffice. Recent research has shown that programs possess a small number of hot \ndata streams, which are data reference sequences that frequently repeat in the same order, and these \naccount for around 90% of program references and more than 80% of cache misses [8, 28]. These hot data \nstreams can be prefetched accurately since they repeat frequently in the same order and thus are predictable. \nThey are long enough (15 20 object references on average) so that they can be prefetched ahead of use \nin a timely manner. In prior work, Chilimbi instrumented a program to collect the trace of its data memory \nreferences; then used a compression algorithm called Sequitur to process the trace off-line and extract \nhot data streams [8]. These hot data streams have been shown to be fairly stable across program inputs \nand could serve as the basis for an off\u00adline static prefetching scheme [10]. On the other hand, for programs \nwith distinct phase behavior, a dynamic prefetching scheme that adapts to program phase transitions may \nperform better. In this paper, we explore a dynamic software prefetching scheme and leave a comparison \nwith static prefetching for future work. PLDI 02, June 17-19, 2002, Berlin, Germany. Copyright 2002 \nACM 1-58113-463-0/02/0006...$5.00. A dynamic prefetching scheme must be able to detect hot data execution \nwith Sequitur profiling  data reference grammar sequence profiling analysis  finite state hot data \nanalysis and machine stream s code state space optimization injection exploration execution with prefetching \n hibernation deoptim ization Figure 1. Dynamic prefetching overview streams online with little overhead. \nThis paper describes a dynamic framework for online detection of hot data streams and demonstrates that \nthis can be accomplished with extremely low\u00adoverhead. Rather than collect the trace of all data references, \nour dynamic framework uses sampling to collect a temporal data reference profile. Unlike conventional \nsampling, we sample data reference bursts, which are short sequences of consecutive data references. \nThe framework uses Sequitur to process the trace online, and a novel algorithm for fast detection of \nhot data streams from the temporal profile data. The hot data streams consist of a sequence of <pc, addr>pairs. \nOur hot data stream analysis is configured to only detect streams that are sufficiently long to justify \nprefetching (i.e., containing more than ten unique references). Once these streams have been detected, \nour prefetching engine dynamically injects checks in the program to match stream prefixes, followed by \nprefetch instructions for the remaining stream addresses. For example, given a hot data stream abacdce, \nonce the addresses a.addr, b.addr, a.addr are detected by checks inserted at a.pc, b.pc, a.pc respectively, \nprefetches are issued for the addresses, c.addr, d.addr, e.addr. The hot data stream prefix length that \nmust match before prefetching is initiated needs to be set carefully. A prefix that is too short may \nhurt prefetching accuracy, and too large a prefix reduces the prefetching opportunity and incurs additional \nstream matching overhead. Conceptually, one can think of the prefix-matching mechanism for a hot data \nstream as corresponding to a deterministic finite state machine (DFSM), where the states correspond to \npossible stream prefixes, and transitions are implemented by inserted prefix-match checks. To avoid redundant \nchecks, and efficiently orchestrate matches for all hot data streams, our prefetching engine constructs \na single DFSM that keeps track of matching prefixes for all hot data streams simultaneously (see Section \n3.1). The prefetching engine uses a dynamic implementation of Vulcan [32] (a binary editing tool for \nthe x86, similar to ATOM [31]), to insert checks into the running program that implement the stream prefix \nmatching DFSM. In addition, it adds prefetch instructions that target the remaining data stream addresses, \non successful stream prefix matches. Figure 1 provides an overview of our dynamic prefetching process \nthat operates in three phases profiling, analysis and optimization, and hibernation. First, the profiling \nphase collects a temporal data reference profile from a running program with low-overhead. This is accomplished \nusing bursty tracing [15], which is an extension of Arnold and Ryder s low-overhead profiling technique \n[3]. The Sequitur compression algorithm incrementally builds an online grammar representation of the \ntraced data references. Once sufficient data references have been traced, profiling is turned off and \nthe analysis and optimization phase begins. A fast analysis algorithm extracts hot data streams from \nthe Sequitur grammar representation. The prefetching engine builds a stream prefix matching DFSM for \nthese hot data streams, and dynamically injects checks at appropriate program points to detect and prefetch \nthese hot data streams. Finally, the process enters the hibernation phase where no profiling or analysis \nis performed, and the program continues to execute with the added prefetch instructions. At the end of \nthe hibernation phase, the program is de-optimized to remove the inserted checks and prefetch instructions, \nand control returns to the profiling phase. For long-running programs this profile, analyze and optimize, \nhibernate cycle will repeat multiple times. The paper makes the following contributions: It presents \na dynamic, low-overhead framework for detecting hot data streams (see Section 2).  It describes an automatic, \ndynamic prefetching scheme that works for general-purpose programs. The prefetching is driven by the \nhot data streams supplied by the online profiling and analysis framework (see Section 3).  It presents \nempirical evidence that dynamic prefetching is effective, producing overall execution time improvements \nof 5 19% for several memory performance limited SPECint2000 benchmarks (see Section 4).   2. DYNAMIC \nDATA REFERENCE PROFIL-ING AND ANALYSIS This section discusses our online, low-overhead framework for \ndetecting hot data streams. The framework first collects a temporal data reference profile with low-overhead, \nand then uses a fast analysis algorithm to extract hot data streams from this temporal profile. 2.1 Bursty \nTracing Framework for Low-Over\u00adhead Temporal Profiling A data reference r is a load or store of a particular \naddress, represented as a pair (r.pc,r.addr). Thesequenceofall data references during execution is the \ndata reference trace. A temporal data reference profile captures not only the frequencies of individual \ndata references in the trace, but also temporal relationships between them. For example, it would distinguish \nthe traces cdeabcdeabfg and abcdefabcdeg, even though all data  Figure 2. Instrumentation for low-overhead \ntemporal profiling references have the same frequencies in both of them. In the second trace, the subsequence \nabcde is a hot data stream and presents a prefetching opportunity. Our framework must collect a temporal \ndata reference profile with low overhead, because the slow-down from profiling has to be recovered by \nthe speed-up from optimization. A common way to reduce the overhead of profiling is sampling: instead \nof recording all data references, sample a small, but representative fraction of them. Our profiler obtains \na temporal profile with low overhead by sampling bursts of data references, which are subsequences of \nthe reference trace. We use the bursty tracing profiling framework [15], which is an extension of the \nArnold-Ryder framework [3]. The code of each procedure is duplicated (see Figure 2). Both versions of \nthe code contain the original instructions, but only one version is instrumented to also profile data \nreferences. Both versions of the code periodically transfer control to checks at procedure entries or \nloop back-edges. The checks use a pair of counters, nCheck and nInstr, to decide in which version of \nthe code execution should continue. At startup, nCheck is nCheck0 and nInstr is zero. Most of the time, \nthe checking code is executed, and nCheck is decremented at every check. When it reaches zero, nInstr \nis initialized with nInstr0 (where nInstr0<<nCheck0) and the check transfers control to the instrumented \ncode. While in the instrumented code, nInstr is decremented at every check. When it reaches zero, nCheck \nis initialized with nCheck0 and control returns back to the checking code. The bursty tracing profiling \nframework does not require operating system or hardware support and is deterministic. We implemented \nit using Vulcan [32], (an executable-editing tool for x86, similar to ATOM [31]), and hence it does not \nrequire access to program source code or recompilation. The profiling overhead is easy to control: there \nis a basic overhead for the checks, and beyond that the overhead is proportional to the sampling rate \nr = nInstr0/ (nCheck0+nInstr0). Via nCheck0 and nInstr0, we can freely chose the burst length and the \nsampling rate. 2.2 Extensions for Online Optimization The counters nCheck0 and nInstr0 of the bursty \ntracing profiling framework control its overhead and the amount of profiling information it generates. \nFor example, setting nCheck0 to 9900 and nInstr0 to 100 results in a sampling rate of 100/10000=1% and \na Figure 3. Profiling timeline. burst length of 100 dynamic checks. We term nCheck0+nInstr0 dynamic checks \na burst-period (see Figure 3). For online optimization, we extended the bursty tracing framework to alternate \nbetween two phases, awake and hibernating. The profiler starts out awake and stays that way for nAwake0 \nburst\u00adperiods, yielding nAwake0 *nInstr0 checks's worth of traced data references. Then, the online optimizer \nperforms the optimizations; after that, the profiler hibernates. This is done by setting nCheck0 to nCheck0+nInstr0 \n-1 and nInstr0 to1for thenext nHibernate0 burst-periods, where nHibernate0 >> nAwake0.Whenthe hibernating \nphase is over, the profiler is woken up by resetting nCheck0 and nInstr0 to their old values (see Figure \n3). While the profiler is hibernating, it traces next to no data references and hence incurs only the \nbasic overhead of executing checks. We designed the hibernation extension so that burst\u00adperiods correspond \nto the same time (measured in executed checks) in either phase (see Figure 3). This makes it easy to \ncontrol the relative length of the awake and hibernating phases using the counters, nAwake0 and nHibernate0. \nNote that with our extension, bursty tracing is still deterministic. Since our optimization is also deterministic, \nexecutions of deterministic benchmarks are repeatable, which helps testing. When nHibernate0 >> nAwake0 \n>> 1and nChecking0 >> nInstr0 >>1 the sampling rate approximates to (nAwake0 *nInstr0)/((nAwake0+nHibernate0)*(nInstr0+nCheck0)). \n 2.3 Fast Hot Data Stream Detection Bursty tracing collects a temporal data reference profile. This must \nS abaabcabcabcabc S-> AaBB Input string A-> ab B B-> CC C C-> Ac A SEQUITUR grammar a bc DAG representation \nFigure 4. Sequitur grammar for w=abaabcabcabcabc. //find reverse post-order numbering for non-terminals \n int next = nRules; function doNumbering = lambda(NonTerminal A){ if(have not yet visited A){ for(each \nchild B of A) doNumbering(B); next--; A.index = next; } } doNumbering(S); //find uses for non-terminals, \ninitialize coldUses to uses for(each non-terminal A) A.uses = A.coldUses = 0; S.uses = S.coldUses \n= 1; for(each non-terminal A, ascending order of A.index) for(each child B or A) B.uses = B.coldUses \n= (B.uses + A.uses); //find hot non-terminals for(each non-terminal A, ascending order of A.index){ \n A.heat = wA.length * A.coldUses; fHot = minLen<=A.length<=maxLen &#38;&#38; H<=A.heat; if(fHot) reportHotDataStream(wA, \nA.heat); subtract = fHot ? A.uses : (A.uses-A.coldUses); for(each child B of A) B.coldUses = B.coldUses \n-subtract; } Figure 5. Algorithm for fast approximation of hot data streams. be analyzed to find hot \ndata streams. Our online profiling and analysis framework first uses the Sequitur algorithm [23] to compress \nthe profile and infer its hierarchical structure. Each observed data reference can be viewed as a symbol, \nand the concatenation of the profiled bursts as a string w of symbols. Sequitur constructs a context-free \ngrammar for the language {w} consisting of exactly one word, the string w. Sequitur runs in time O(w.length). \nIt is incremental (we can append one symbol at a time) and deterministic. The grammar is a compressed \nrepresentation of the trace, it is unambiguous and acyclic in the sense that no non-terminal directly \nor indirectly defines itself. Figure 4 shows a Sequitur grammar for w=abaabcabcabcabc,and its representation \nas a multi-dag. Before describing our online analysis for finding hot data streams from this Sequitur \ngrammar, we review some definitions from [8]. A hot data stream is a data reference subsequence whose \nregularity magnitude exceeds a predetermined heat threshold, H. Given a data reference subsequence v, \nwe define its regularity magnitude as v.heat = v.length*v.frequency,where v.frequency is the number of \nnon-overlapping occurrences of v in the trace. Larus describes an algorithm for finding a set of hot \ndata streams from a Sequitur grammar [21]; we use a faster, less precise algorithm that relies more heavily \non the ability of Sequitur to infer hierarchical structure. Our analysis algorithm uses the observation \nthat each non-terminal A of a Sequitur grammar generates a language L(A) = {wA} with just one word wA. \nWe define the regularity magnitude of a non\u00adterminal A as A.heat = wA.length*A.coldUses,where A.coldUses \nis the number of times A occurs in the (unique) parse tree of the complete grammar, not counting occurrences \nin sub-trees belonging to hot non-terminals other than A. A non-terminal A is hot iff minLen <= A.length \n<= maxLen and H <= A.heat,where H is the predetermined heat threshold. The result of the analysis is \nthe set {wA | A is a hot non-terminal} of hot data streams. Figure 5 shows pseudo-code for the analysis. \nWe call a non\u00adterminal B, a child of another non-terminal A, if it occurs on the right-hand side of the \ngrammar rule for A. We assume that we already have wA.length for each non-terminal A; thisiseasyto maintain \nin Sequitur. The analysis first numbers the non-terminals such that whenever B is a child of A,we have \nA.index < B.index. This important property guarantees that in the rest of the algorithm, we never visit \na non-terminal before having visited all its predecessors. Then, the algorithm finds how often each non\u00adterminal \nis used in the parse-tree of the grammar. Finally, it finds hot non-terminals such that a non-terminal \nis only considered hot if it accounts for enough of the trace on its own, where it is not part of the \nexpansion of other non-terminals. The running time of the algorithm is linear in the size of the grammar. \nConsider, for example, the grammar shown in Figure 4. Assume the heat threshold, H = 8, and the length \nrestrictions are minLen = 2, maxLen = 7. The values computed by the analysis are shown in Figure 6 and \nTable 1. Note that the non-terminal C is completely subsumed by the hot non-terminal B and therefore \nnot considered hot. Note that even though the non-terminal A also appears outside of the parse trees \nof hot non-terminals, its regularity magnitude A.heat = 2 does not exceed the heat threshold H.In this \nexample, we would find just one hot data stream wB = abcabc with heat 12 that accounts for 12/15=80% \nof all data references. 0 1:15 1:15   S SSS S 1 2:6 2:6 B B B B BB 2 4:3 0:3  CCC C CCCC 3 5:2 \n1:2  A AA A AA A   abaabcabcabcabc grammar reverse uses: colduses: (omitting postorder word word parsetree \nterminals) numbering length length Figure 6. Hot data stream analysis example. X Child Length Index Use \ncold-Use Heat Report? S A,B ,B 15 0 1 1 15 no, start A - 2 3 5 1 2 no, cold B C,C 6 1 2 2 12 yes C A \n3 2 4 0 0 no, cold Table 1: Computed values for hot data stream analysis. a.pc: if(accessing a.addr){ \n if(v.seen == 2){ v.seen = 3; prefetch c.addr,a.addr,d.addr,e.addr; }else{ v.seen = 1; } }else{ \n v.seen = 0; } b.pc: if(accessing b.addr) if(v.seen == 1) v.seen = 2; else v.seen = 0; else v.seen \n= 0; Figure 7. Inserted prefetching code for stream abacadae.  2.4 Discussion Our online profiling \nand analysis framework implementation batches and sends traced data references to Sequitur, as soon as \nthey are collected, rather than at the end of the awake phase. This is possible since Sequitur constructs \nthe grammar representation incrementally. During the hibernation phase, our online profiler enters the \ninstrumented code once per burst period (see Figure 3). These data references traced during hibernation \nare ignored by Sequitur to avoid trace contamination and unnecessary additional trace analysis overhead. \n 3. DYNAMIC PREFETCHING Prior work has shown that the data references of programs have a high degree \nof regularity [8]. A data reference r is a load or store of a particular address, represented as a pair \n(r.pc,r.addr). Most data references of a program take place in only a few hot data streams, which are \nsequences of data references that repeat frequently, and these account for most of the program s cache \nmisses [8]. For example, if abacadae is a hot data stream, then the program often performs a data access \nat a.pc from address a.addr, followed by a data access at b.pc from address b.addr,and so on. {[v ,1]} \na Our prefetching optimizer matches hot data stream prefixes, and then issues prefetches for the remaining \ndata stream addresses. For example, given the hot data stream abacadae, when the optimizer detects the \ndata references aba, it prefetches from the addresses c.addr,a.addr,d.addr,e.addr. Ideally, the data \nfrom these addresses will be cache resident by the time the data references cadae take place, avoiding \ncache misses and speeding up the program. Figure 1 shows an overview of our optimizer. It profiles the \nprogram to find hot data streams. When it has collected enough profiling information, it stops profiling \nand injects code for detecting prefixes and prefetching suffixes of hot data streams. Then it continues \nrunning the optimized program. For long\u00adrunning applications, it may repeat these steps later. We use \ndynamic Vulcan [32], which is an executable editing tool similar to ATOM [31], to edit the binary of \nthe currently executing program. 3.1 Generating Detection and Prefetching Code After the profiling and \nanalysis phase finds the hot data streams, the optimizer must match their prefixes and prefetch their \nsuffixes. The optimizer uses a fixed constant headLen to divide each hot data stream v = v1v2...v{v.length} \ninto a head v.head = v1v2...vheadLen and a tail v.tail = When it v{headLen+1}v{headLen+2}...v{v.length}. \ndetects the data references of v.head, it prefetches from the addresses of v.tail. Consider how we might \nmatch and prefetch when headLen =3 and there is only one hot data stream, v = abacadae. The detection/ \nmatching code makes use of a counter v.seen, that keeps track of how much of v.head has been matched. \nWhen v.seen = 0 nothing has been matched, when v.seen =1, wehaveapartial match a, when v.seen = 2, we \nhave a partial match ab,and when v.seen =3 we have a complete match for v.head = abc, and prefetch from \nthe addresses in v.tail,i.e.from addresses c.addr, a.addr, d.addr, e.addr.To drive v.seen, we need to \ninsert detection and prefetching code at the pc's of v.head that make comparisons to the addresses of \nv.head and the variable v.seen. Figure 7 shows pseudo-code for this. Note in Figure 7 that we have exploited \nthe fact that the same symbol a occurs multiple times in v.head = aba. Also note that we treat the cases \nof initial, failed, and complete matches specially. The initial match of data reference a works regardless \nof how much of v.head we have seen. A failed match resets v.seen to 0. A {[v,2],[w ,1]} {[v,3],[v,1]} \n complete match, besides driving v.seen, prefetches the addresses in v.tail. Finally, note that it is \npossible that a.pc == b.pc,inwhich case the if(accessing b.addr) clause would appear in a.pc's instrumentation. \nNow that we know how to detect the head and prefetch the tail of a single hot data stream, there is a \nstraight-forward way to do it for multiple hot data streams. We could introduce one variable v.seen, \nfor each hot data stream v, and inject the code independently. While this simple approach works, it may \nlead to a lot of redundant work. Consider, for example, the hot data streams v = abacadae and w = bbghij.When \nv.seen == 2, we know that w.seen == 1, so we could save some work by combining the matching of v and \nw. This even holds inside one hot data stream: when w.seen == 2 and we observe another b, we should keep \nw.seen =2. Conceptually, each hot data stream v corresponds to a deterministic finite state machine (DFSM) \nv.dfsm, where the states are represented by v.seen and the detection code implements the transitions. \nInstead of driving one DFSM per hot data stream, we would like to drive just one DFSM that keeps track \nof matching for all hot data streams simultaneously. By incurring the one-time cost of constructing the \nDFSM, we make the frequent detection and prefetching of hot data streams faster. Figure 8 illustrates \na prefix-matching DFSM that simultaneously tracks hot data streams abacadae and bbghij. Before we describe \nhow to come up with a DFSM that matches all hot data streams simultaneously, let us consider how we would \ngenerate code to drive it. Without loss of generality, let S = {0,...,m} be the set of states and let \nA be the set of data references (symbols) that appear in prefixes of hot data streams. The transition \nfunction d:S*A-->S indicates that when you are in a state s and observe the data reference a, you drive \nthe state to s' = d(s,a).In other words, a.pc has instrumentation of the form a.pc: if((accessing a.addr) \n&#38;&#38; (state==s)) state = s'; Additionally, some states s in S would be annotated with prefetches \ns.prefetches, for the suffixes of the streams that have been completely matched when state s is reached. \nThus, the instrumentation would become a.pc: if((accessing a.addr) &#38;&#38; (state==s)){ state = s'; \nprefetch s'.prefetches; } We again treat the cases of initial, failed, and complete matches specially \nas indicated in Figure 7. Note that besides combining matches for the same address, but different states \nunder the same outer if branch, we can sort the if-branches in such a way that more likely cases come \nfirst. This further reduces the work for detecting prefixes of hot data streams. Now let us examine how \nto construct a DFSM that matches all hot data streams simultaneously. A state is a set of state elements, \nwhere state element e is a pair of a hot data stream e.hotDataStream and an integer e.seen. If the current \nstate is s={[v,2],[w,1]} this means the prefix matcher has seen the first two data accesses of the hot \ndata stream v, and the first data access of hot data stream w, and no data accesses of any other hot \ndata streams. State s0 = {} is the start state where nothing has been matched. add {} to the workList; \n while(!workList.isEmpty){ take state s out of workList; function addTransition = lambda(Symbol a){ \n if(s doesn't yet have a transition for a){ s' = {[v,n+1] | n<headLen &#38;&#38; [v,n] in s &#38;&#38; \n a==v{n+1}} union {[w,1] | a==w1} if(s' doesn't yet exist){ add s' to the states of the DFSM; add \ns' to the workList; } if(s' != {}) introduce the transition (a,s') for s; } } for(each state element \ne in s) if(e.seen < headLen) addTransition(e.hotDataStreame.seen+1); for(each symbol a for which there \n exists a hot data stream v with v1==a) addTransition(a); } Figure 9. Algorithm for prefetching FSM \nconstruction. Let s be a state and a be a data reference. The transition function d:S*A-->A yields a \ntarget state (set of state elements) as follows: d(s,a) = {[v,n+1] | n<headLen &#38;&#38; [v,n] in s \n&#38;&#38; a==v{n+1}} union {[w,1] | a==w1} We construct the DFSM with a lazy work-list algorithm starting \nfrom s0. We represent the DFSM as a directed graph, where the nodes are reachable states and a transition \nd(a,s) is stored as an edge from s to d(a,s) labelled with a. We do not explicitly represent any edges \nto the start state. Figure 9 shows the pseudo-code. Let n be the number of hot data streams, and n <= \n100 if H is set such that each hot data stream covers at least 1% of the profile. Then there are headLen*n \ndifferent state elements and thus up to 2(headLen*n)=O(2n) different states. We have never observed this \nexponential blow-up; we usually find close to headLen*n+1 states.  3.2 Injecting Detection and Prefetching \nCode Our online optimizer uses dynamic Vulcan to inject the detection and prefetching code into the running \nbenchmark image [32]. Dynamic Vulcan stops all running program threads while binary modifications are \nin progress and restarts them on completion. For every procedure that contains one or more pc s for which \nthe optimizer wants to inject code, it does the following. First, it makes a copy of the procedure. Second, \nit injects the code into the copy. Third, it overwrites the first instruction of the original with an \nunconditional jump to the copy. When the optimizer wants to deoptimize later, it need only remove those \njumps. Note that we do not patch any pointers to the original code of optimized procedures in the data \nof the program. In particular, the return addresses on the stack still refer to the original procedures. \nHence, we will return to original procedures for at most as many times as there were activation records \non the stack at optimization time. This is safe, but may lead to a few missed self-optimizing benchmark \nFigure 10. Dynamic Injection of Prefetching Code. prefetching opportunities. Figure 10 shows how our \nsystem uses Vulcan. Before execution, static Vulcan modifies the x86 binary of the benchmark to implement \nthe bursty tracing framework from Section 2.1. The resulting modified binary is linked with the runtime \nsystem of our dynamic optimizer, which includes code for the algorithms describedinSection2.3 andSection3.1. \n 4. EXPERIMENTAL EVALUATION This section evaluates our online profiling and analysis framework and investigates \nthe performance impact of dynamic prefetching. 4.1 Experimental Methodology The programs used in this \nstudy include several of the memory\u00adperformance-limited SPECint2000 benchmarks, and boxsim,a graphics \napplication that simulates spheres bouncing in a box. We applied our dynamic prefetching framework to \nthese benchmarks and used the prefetcht0 instruction supplied on the Pentium III to prefetch data into \nboth levels of the cache hierarchy. The following framework settings were used for all experiments, unless \nmen\u00adtioned otherwise. The bursty tracing sampling rate was set at 0.5% during the active profiling period, \nwith profiling bursts extending through 60 dynamic checks (i.e., nCheck0=11,940 and nInstr0 = 60). The \nonline optimization controls were set to actively profile and analyze 1 second of every 50 seconds of \nprogram execution, where active periods are 50 burst periods long (i.e., nAwake0 =50, instrument for \nprofiling inject detection and prefetching code  nHibernate0 = 2,450). The hot data stream analysis \ndetected streams that contain more than 10 references, and account for at least 1% of the collected trace. \nThese settings are not the result of careful tuning; rather our experience indicates that a fairly broad \nrange of reasonable settings performs equivalently. Measurements were performed on a uniprocessor 550 \nMhz Pentium III PC with 512 MB of memory, 256 KB, 8-way L2, and 16KB, 4-way L1 data cache, both with \n32 byte cache blocks, running Windows 2000 Server. The SPEC benchmarks were run with their largest input \ndata set (ref). boxsim was used to simulate 1000 bouncing spheres. All measurements report the average \nof five runs.  4.2 Evaluating the Online Profiling and Analy\u00adsis Framework Figure 11 reports the overhead \nof our online profiling and analysis infrastructure. The Basic bar indicates the overhead of just the \ndynamic checks without (virtually) any data reference profiling. This is measured by setting nCheck0 \nto an extremely large value and nInstr0 to 1. We applied the techniques described in [15] to reduce this \ndynamic check overhead. It is important that this over\u00adhead be small since any dynamic optimization must \novercome this to produce performance improvements. In addition, unlike other sampling-related overhead, \nthis cannot be reduced by changing the framework s counter settings. As Figure 11 shows, this overhead \nis reasonably low, ranging from around 2.5% for boxsim to 6% for parser. The Prof bar indicates the overhead \nof collecting the tempo\u00adral data reference trace at the counter settings discussed in Section 4.1. Data \nreference profiling at this sampling rate adds very little additional overhead, which ranges from almost \nnothing for mcf to 1.6% overhead for vortex. Thus, we can collect sampled temporal data reference profiles \nfor all our benchmarks with a maximal overhead of only 6.5%, in the case of twolf and parser. Finally, \nthe Hds bar indicates the overhead of collecting the temporal data ref\u00aderence profiles and analyzing \nthem to detect hot data streams according to the parameters in Section 4.1. Again, this adds very little \noverhead; vortex at 1.4% incurs the largest additional over\u00adhead. Considering all three contributors \nto overhead, we see that at the current sampling rate most of the overhead arises from the dynamic checks. \nThe overall overhead of our online profiling and analysis is reasonably low, and ranges from around 3% \nfor mcf to 7% for parser and vortex. Any dynamic optimization based on hot data streams, that operates \nin our framework must produce greater improvements than this to positively impact overall program per\u00adformance. \n  4.3 Dynamic Prefetching Evaluation Figure 12 shows the overall impact of our dynamic prefetching \nscheme on program performance, normalized to the execution time of the original unoptimized program. \nThe Y axis measures percentage overhead; positive values indicate performance degradation, and negative \nvalues indicate speedups. The No-pref bars report the cost of performing all the profiling, analysis \nand hot data stream prefix matching, yet not inserting prefetches. This measures the overhead of our \ndynamic prefetching analysis, which must be overcome by effective prefetching to yield net performance \ngains. The prefix-match checks add an additional 0.5% (mcf, parser) to 4% (boxsim) overhead compared \nwith the hot data stream analysis (compare No-pref with Hds bar in Figure 11), for a configuration that \nmatched the first two references of a hot data stream prior to initiating prefetching. Changing this \nto match a single data stream element before initiating prefetching lowered this overhead, but at the \ncost of less effective prefetching, yielding a net performance loss. Matching the first three data stream \nelements before initiating prefetching increased this overhead without providing any corresponding benefit \nin prefetching accuracy, resulting in a net performance loss as well.. In addition, our current implementation \nmakes no attempt to schedule prefetches (they are triggered as soon as the prefix matches). More intelligent \nprefetch scheduling could produce larger benefits. The Seq-pref bars measures the benefit of a prefetching \nscheme that uses the hot data stream analysis to insert dynamic prefetches at appropriate program points, \nbut ignores the data stream addresses. Instead, it prefetches cache blocks that sequentially follow the \nlast prefix-matched hot data stream reference (i.e., the stream reference, which when matched, causes \nthe prefetch sequence to be initiated). This scheme is equivalent to our dynamic prefetching scheme if \nhot data streams are sequentially allocated. The data indicates that with the sole exception of parser, \nwhich has several sequentially allocated hot data streams and runs around 5% faster overall, none of \nthe benchmarks benefit from this approach. The other benchmarks suffer performance degradations that \nrange from 7% (mcf) to 12% (twolf), which indicates that these prefetches pollute the cache. Finally, \nthe Dyn-pref bars reports the performance of our dynamic prefetching implementation (achieved by setting \nthe hot data stream prefix matching length to 2). Prefetching produces a net performance improvement \nof 5% (vortex) to 19% (vpr). This is despite the 4 8% overhead that the prefetching has to overcome to \nshow net performance improvements. Comparing these results to the Seq-pref numbers highlights the importance \nof using the hot data streams addresses as prefetch targets. In addition, manual examination of the hot \ndata addresses indicates that many will not be successfully prefetched using a simple stride-based prefetching \nscheme. However, a stride-based prefetcher could complement our Table 2: Detailed dynamic prefetching \ncharacterization Benchmark #of opt. cycles # of traced refs (per cycle avg.) #of hds (per cycle avg.) \n# of DFSM states, transitions (per cycle avg.) #of procs.modified (per cycle avg.) vpr 17 83,231 41 <79 \nstates, 68 checks> 7 mcf 36 72,537 37 <75 states, 74checks> 6 twolf 55 87,981 25 <42 states, 41checks> \n11 parser 4 73,244 21 <43 states, 42 checks> 9 vortex 3 67,852 14 <29 states, 28 checks> 12 boxsim 19 \n87,818 23 <40 states, 36 checks> 7 scheme by prefetching data address sequences that do not qualify \nas hot data streams. Table 2 provides a more detailed characterization of our dynamic prefetching implementation. \nThe second column indicates the number of prefetch optimization cycles performed during program execution. \nLonger running programs produce a greater number of these optimization cycles. The next three columns \nshow the number of traced references, hot data streams detected, and the size of the DFSMs used for prefix \nmatching, all averaged on a per optimization cycle basis. The last column contain the number of procedures \nmodified to insert prefix-match checks or prefetches, again averaged on a per cycle basis. The results \nindicate that the prefetching benefits arise from targeting a small set of program hot data streams. \n  5. RELATED WORK This section discusses related work on prefetching and software dynamic optimization. \n5.1 Prefetching Prefetching is a well known optimization that attempts to hide latency resulting from \npoor reference locality. We are concerned with data prefetching (as opposed to instruction prefetching) \ninto the processor cache. Prefetching mechanisms can be classified as software prefetching (using non-blocking \nload instructions provided by most modern processors) and hardware prefetching (extending the memory \nmanagement subsystem architecture). Prefetching mechanisms can also be characterized by the kind of regularity \nthey require of the target program and by their degree of automation. We review only the most closely \nrelated techniques here; a survey of prefetching techniques is [35]. Early prefetching techniques mainly \nfocused on improving the performance of scientific codes with nested loops that access dense arrays. \nBoth software and hardware techniques exist for such regular codes. The software techniques use program \nanalysis to determine the data addresses needed by future loop iterations, and employ program transformations, \nsuch as loop unrolling and software pipelining to exploit that information [20, 24]. Hardware prefetching \ntechniques include stride prefetchers and stream buffers. Stride prefetchers learn if load address sequences \nare related by a fixed delta and then exploit this information to predict and prefetch future load addresses \n[7]. Stream buffers can fetch linear sequences of data and avoid polluting the processor cache by buffering \nthe data [17]. These techniques are mostly limited to programs that make heavy use of loops and arrays, \nproducing regular access patterns. Jump pointers are a software technique for prefetching linked data \nstructures, overcoming the array-and-loop limitation. Artificial jump pointers are extra pointers stored \ninto an object that point to an object some distance ahead in the traversal order. On future traversals \nof the data structure, the targets of these extra pointers are prefetched. Natural jump pointers are \nexisting pointers in the data structure used for prefetching. For example, greedy prefetching makes the \nassumption that when a program uses an object o, it will use the objects that o points to, in the near \nfuture, and hence prefetches the targets of all pointer fields. These techniques were introduced by Luk \nand Mowry in [22] and refined in [5, 18]. Stoutchinin et al. describe a profitability analysis for prefetching \nwith natural jump pointers [33]. A limitation of these techniques is that their static analyses are restricted \nto regular linked data structures accessed by local regular control structures. Various hardware techniques, \nrelated to greedy prefetching, have been proposed for prefetching linked data structures. In dependence-based \nprefetching, producer-consumer pairs of loads are identified, and a prefetch engine speculatively traverses \nand prefetches them [26]. Dependence-based prefetching has also been combined with artificial jump-pointer \nprefetching in software or hardware [27]. In dependence-graph precomputation, a backward slice of instructions \nin the instruction fetch queue is used to chose a few instructions to execute speculatively to compute \na prefetch address [1]. And in content-aware prefetching, data that is brought in to satisfy a cache \nmiss is scanned for values that may resemble addresses, and those addresses are used for prefetching \n[12]. The hardware technique that best corresponds to history-pointers is correlation-based prefetching. \nAs originally proposed, it learns digrams of a key and prefetch addresses: when the key is observed, \nthe prefetch is issued [6]. Joseph and Grunwald generalized this technique by using a Markov predictor \n[16]. Nodes in the Markov\u00admodel are addresses, and the transition probabilities are derived from observed \ndigram frequencies. Upon a data cache miss to an address that has a node in the Markov model, prefetches \nfor a fixed number of transitions from that address are issued, prioritized by their probabilities. Our \ntechniques differs from prior software prefetching techniques in at least three ways. First, it is profile-based \nand does not rely on static analysis. Second, being profile-based it works for arbitrary data structure \ntraversals. Finally, it is a dynamic technique that is capable of adaptation as the program executes. \nOur dynamic prefetching is most similar to correlation-based hardware prefetching in that it observes \npast data accesses to predict future accesses. Unlike the correlation-based prefetchers mentioned above, \nit is a software technique that can be easily configured and tuned for a particular program, performs \nmore global access pattern analysis, and is capable of using more context for its predictions than digrams \nof data accesses. 5.2 Software Dynamic Optimization Common examples of software dynamic optimizers are \nsome of the more sophisticated Java virtual machines such as Intel's Microprocessor Research Lab VM [11], \nSun's HotSpot VM [25], and IBM's Jikes RVM [2]. All of these contain just-in-time compilers and use runtime \ninformation to concentrate optimization efforts on frequently executing methods. Unlike our system, they \ndo not focus on memory hierarchy optimizations, and possess only limited cross-procedure optimization \ncapabilities. Recently, some dynamic optimizers that operate on compiled object code have been proposed. \nThe Wiggins/Redstone system uses hardware performance counters to profile a program executing on the \nAlpha processor, and optimizes single-entry multiple-exit regions of hot basic blocks [13]. The University \nof Queensland Dynamic Binary Translator translates an program that is compiled for one architecture just \nin time for execution on another architecture, and collects a full edge-weight profile to identify groups \nof connected hot blocks for optimization [34]. The Dynamo system interprets a program to collect a basic \nblock profile. Once a basic block reaches a heat threshold, Dynamo considers the linear sequence of blocks \nexecuted directly afterwards as a hot path, which it then optimizes [4]. All of these systems optimize \ncode in hot control paths that may cross procedure boundaries. Unlike our system, they do not focus on \nmemory hierarchy optimizations. A few dynamic memory hierarchy optimizers implemented in software do \nexist. Saavedra and Park dynamically adapt the prefetch distance of array-and-loop software prefetching \nto the changing latencies of a NUMA architecture [29]. They also discuss adaptive profiling: when profiling \ninformation changes, the profiler starts polling more frequently. This idea may be a useful extension \nto our simpler hibernation approach. Chilimbi and Larus use a copying generational garbage collector \nto improve reference locality by clustering heap objects according to their observed data access patterns \n[9]. Harris performs dynamic adaptive pretenuring for Java programs by identifying allocation sites that \noften allocate long-lived objects [14]. His system modifies these allocations to directly place objects \ninto the old generation of a generational garbage collector, saving the work of repeatedly scanning them \nin the young generation. Kistler and Franz reorder fields in objects so fields accessed together reside \nin the same cache block, and discuss how this can be done during copying garbage collection [19]. 5.3 \nState Machine Predictor Generation Sherwood and Calder propose an algorithm that generates FSM predictors \nfrom temporal profiling data [30]. In their case study, the profile is a trace of branch executions. \nEach FSM is driven by the global branch direction bitstring, and predicts whether a particular branch \nis taken or not taken. While we also generate an FSM predictor from temporal profiling data, there are \nsome fundamental differences to the Sherwood-Calder approach. First of all, Sherwood and Calder generate \nFSM predictors in hardware for special-purpose processors, while we use a dynamic software approach. \nThey restrict FSMs to be driven by bitstrings and predict a single bit (one step of their FSM generation \nalgorithm represents the predictor by a boolean formula), while we predict sets of prefetch addresses. \nThey use fixed-sized histories, while our hot data streams are variable-length. They drive several FSMs \nin parallel, while we combine all FSMs into one.  6. CONCLUSIONS This paper describes a dynamic software \nprefetching framework for general-purpose programs. The prefetching scheme runs on stock hardware, is \ncompletely automatic, and can handle codes that traverse pointer-based data structures. It targets a \nprogram s hot data streams, which are consecutive data reference sequences that frequently repeat in \nthe same order. We show how to detect hot data streams online with low-overhead, using a combination \nof bursty tracing and a fast hot data stream analysis algorithm. Our experimental results demonstrate \nthat our prefetching technique is effective, providing overall execution time improvements of 5 19% for \nseveral memory-performance-limited SPECint2000 benchmarks running their largest (ref) inputs. 7. REFERENCES \n[1] M. Annavaram, J. Patel, and E. Davidson. Data prefetching by dependence graph precomputation. In \nInternational Sympo\u00adsium on Computer Architecture (ISCA), 2001. [2] M. Arnold et al. Adaptive optimization \nin the Jalapeno JVM , In Object-Oriented Programming, Systems, Languages, and Applications (OOPSLA), \n2000. [3] M. Arnold, and B. Ryder. A Framework for Reducing the Cost of Instrumented Code. In ACM SIGPLAN \n01 Conference on Programming Languages Design and Implementation (PLDI), 2001. [4] V. Bala, E. Duesterwald, \nand S. Banerjia. Dynamo: A transpar\u00adent dynamic optimization system. In ACM SIGPLAN 00 Con\u00adference on \nProgramming Languages Design and Implementation (PLDI), 2000. [5] B. Cahoon, and K. McKinley. Data flow \nanalysis for software prefetching linked data structures in Java. In International Conference on Parallel \nArchitectures and Compilation Tech\u00adniques (PACT), 2001. [6] M. Charney, and A. Reeves. Generalized correlation \nbased hardware prefetching. Tech report EE-CEG-95-1,Cornell University, 1995. [7] T. Chen, and J. Baer. \nReducing memory latency via non-block\u00ading and prefetching caches. In Architectural Support for Pro\u00adgramming \nLanguages and Operating Systems (ASPLOS),1992. [8] T.M. Chilimbi. Efficient Representations and Abstractions \nfor Quantifying and Exploiting Data Reference Locality. In Pro\u00adceedings of the ACM SIGPLAN 01 Conference \non Program\u00adming Language Design and Implementation, June 2001 [9] T. M. Chilimbi, and J. R. Larus. Using \ngenerational garbage collection to implement cache-conscious data placement. In Proceedings of the 1998 \nInternational Symposium on Memory Management,Oct.1998. [10] T. M. Chilimbi. On the stability of temporal \ndata reference profiles. In International Conference on Parallel Architec\u00adtures and Compilation Techniques \n(PACT), 2001. [11] M. Cierniak, G. Lueh, and J. Stichnoth. Practicing JUDO: Java under dynamic optimizations. \nIn ACM SIGPLAN 00 Conference on Programming Languages Design and Imple\u00admentation (PLDI), 2000. [12] R. \nCooksey, D. Colarelli, and D. Grunwald, Content-based prefetching: Initial results , In Workshop on Intelligent \nMemo\u00adry Systems, 2000. [13] D. Deaver, R. Gorton, and N. Rubin, Wiggins/Redstone: An online program specializer. \n, In Hot Chips, 1999. [14] T. Harris. Dynamic adaptive pre-tenuring. In International Symposium on Memory \nManagement (ISMM), 2000. [15] M. Hirzel and T. Chilimbi. Bursty Tracing: A Framework for Low-Overhead \nTemporal Profiling , In Workshop on Feed\u00adback-Directed and Dynamic Optimizations (FDDO), 2001. [16] D. \nJoseph and D. Grunwald. Prefetching using Markov pre\u00addictors , In International Symposium on Computer \nArchitec\u00adture (ISCA), 1997. [17] N. Jouppi. Improving direct-mapped cache performance by the addition \nof a small fully associative cache and prefetch buff\u00aders , In International Symposium on Computer Architecture \n(ISCA), 1990. [18] M. Karlsson, F. Dahlgren, and P. Stenstrom. A Prefetching Technique for Irregular \nAccesses to Linked Data Structures, In High Performance Computer Architectures (HPCA), 1999. [19] T. \nKistler and M. Franz. Automated data-member layout of heap objects to improve memory-hierarchy performance. \nIn Transactions on Programming Languages and Systems (TO-PLAS), 2000. [20] A. Klaiber and H. Levy. An \narchitecture for software-con\u00adtrolled data prefetching. In International Symposium on Com\u00adputer Architecture \n(ISCA), 1991. [21] J. R. Larus. Whole program paths. In Proceedings of the ACM SIGPLAN 99 Conference \non Programming Language Design and Implementation, pages 259-269, May 1999. [22] C. K. Luk, and T. Mowry. \nCompiler-based prefetching for re\u00adcursive data structures. In Architectural Support for Program\u00adming \nLanguages and Operating Systems (ASPLOS),1996 [23] C. G. Nevill-Manning and I. H. Witten. Linear-time, \nincre\u00admental hierarchy inference for compression. In Proceedings of the Data Compression Conference (DCC \n97), 1997. [24] T. Mowry, M. Lam, and A. Gupta. Design and Analysis of a Compiler Algorithm for Prefetching. \n, In Architectural Support for Programming Languages and Operating Systems (ASP-LOS), 1992. [25] M. Paleczny, \nC. Vick, and C. Click. The Java HotSpot server compiler. , In USENIX Java Virtual Machine Research and \nTechnology Symposium (JVM), 2001. [26] A. Roth, A. Moshovos, and G. Sohi. Dependence based prefetching \nfor linked data structures. In Architectural Support for Programming Languages and Operating Systems \n(ASP-LOS), 1998. [27] A. Roth and G. Sohi. Effective jump pointer prefetching for linked data structures. \nIn International Symposium on Com\u00adputer Architecture (ISCA), 1999. [28]S. Rubin, R. Bodik, and T. Chilimbi. \nAn Efficient Profile-Analysis Framework for Data-Layout Optimizations. In Prin\u00adciples of Programming \nLanguages, POPL 02, Jan 2002. [29] R. Saavedra and D. Park. Improving the effectiveness of soft\u00adware \nprefetching with adaptive execution. In International Conference on Parallel Architectures and Compilation \nTech\u00adniques (PACT), 1996. [30] T. Sherwood and B. Calder. Automated design of finite state machine predictors \nfor customized processors. In Internation\u00adal Symposium on Computer Architecture (ISCA), 2001. [31] A. \nSrivastava and A. Eustace. ATOM: A system for building customized program analysis tools. In Proceedings \nof the ACM SIGPLAN 94 Conference on Programming Language Design and Implementation, pages 196-205, May \n1994. [32] A. Srivastava, A. Edwards, and H. Vo. Vulcan: Binary trans\u00adformation in a distributed environment. \n, In Microsoft Re\u00adsearch Tech Report, MSR-TR-2001-50, 2001. [33] A. Stoutchinin et al. Speculative prefetching \nof induction pointers. In International Conference on Compiler Construc\u00adtion (CC), 2001. [34] D. Ung, \nand C. Cifuentes. Opimising hot paths in a dynamic binary translator. In Workshop on Binary Translation, \n2000. [35] S. VanderWiel, and D. Lilja. Data prefetch mechanisms , In-ACM Computing Surveys, 2000.  \n \n\t\t\t", "proc_id": "512529", "abstract": "Prefetching data ahead of use has the potential to tolerate the grow ing processor-memory performance gap by overlapping long latency memory accesses with useful computation. While sophisti cated prefetching techniques have been automated for limited domains, such as scientific codes that access dense arrays in loop nests, a similar level of success has eluded general-purpose pro grams, especially pointer-chasing codes written in languages such as C and C++. We address this problem by describing, implementing and evaluating a dynamic prefetching scheme. Our technique runs on stock hardware, is completely automatic, and works for general-purpose programs, including pointer-chasing codes written in weakly-typed languages, such as C and C++. It operates in three phases. First, the profiling phase gathers a temporal data reference profile from a running program with low-overhead. Next, the profiling is turned off and a fast analysis algorithm extracts hot data streams, which are data reference sequences that frequently repeat in the same order, from the temporal profile. Then, the system dynamically injects code at appropriate program points to detect and prefetch these hot data streams. Finally, the process enters the hibernation phase where no profiling or analysis is performed, and the program continues to execute with the added prefetch instructions. At the end of the hibernation phase, the program is de-optimized to remove the inserted checks and prefetch instructions, and control returns to the profiling phase. For long-running programs, this profile, analyze and optimize, hibernate, cycle will repeat multiple times. Our initial results from applying dynamic prefetching are promising, indicating overall execution time improvements of 5.19% for several memory-performance-limited SPECint2000 benchmarks running their largest (ref) inputs.", "authors": [{"name": "Trishul M. Chilimbi", "author_profile_id": "81100578606", "affiliation": "Microsoft Research, Redmond, WA", "person_id": "P285175", "email_address": "", "orcid_id": ""}, {"name": "Martin Hirzel", "author_profile_id": "81100572340", "affiliation": "University of Colorado, Boulder, CO", "person_id": "PP18002227", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/512529.512554", "year": "2002", "article_id": "512554", "conference": "PLDI", "title": "Dynamic hot data stream prefetching for general-purpose programs", "url": "http://dl.acm.org/citation.cfm?id=512554"}