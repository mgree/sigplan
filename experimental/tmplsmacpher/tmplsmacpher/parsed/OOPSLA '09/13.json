{"article_publication_date": "10-25-2009", "fulltext": "\n Strictly Declarative Speci.cation of Sophisticated Points-to Analyses Martin Bravenboer Yannis Smaragdakis \nDepartment of Computer Science University of Massachusetts, Amherst Amherst, MA 01003, USA martin.bravenboer@acm.org \nyannis@cs.umass.edu Abstract We present the Doop framework for points-to analysis of Java programs. Doop \nbuilds on the idea of specifying pointer analysis algorithms declaratively, using Datalog: a logic\u00adbased \nlanguage for de.ning (recursive) relations. We carry the declarative approach further than past work \nby describ\u00ading the full end-to-end analysis in Datalog and optimizing aggressively using a novel technique \nspeci.cally targeting highly recursive Datalog programs. As a result, Doop achieves several bene.ts, \nincluding full order-of-magnitude improvements in runtime. We compare Doop with Lhot\u00b4 ak and Hendren \ns Paddle, which de.nes the state of the art for context-sensitive analyses. For the exact same logical \npoints-to de.nitions (and, consequently, identi\u00adcal precision) Doop is more than 15x faster than Paddle \nfor a 1-call-site sensitive analysis of the DaCapo benchmarks, with lower but still substantial speedups \nfor other important analyses. Additionally, Doop scales to very precise analyses that are impossible \nwith Paddle and Whaley et al. s bddbddb, directly addressing open problems in past literature. Finally, \nour implementation is modular and can be easily con.gured to analyses with a wide range of characteristics, \nlargely due to its declarativeness. Categories and Subject Descriptors F.3.2 [Logics and Meanings of \nPrograms]: Semantics of Programming Languages Program Analysis; D.1.6 [Programming Techniques]: Logic \nProgramming General Terms Algorithms, Languages, Performance 1. Introduction Points-to (or pointer) \nanalysis intends to answer the question what objects can a program variable point to? This ques\u00adtion \nforms the basis for practically all higher-level program Permission to make digital or hard copies of \nall or part of this work for personal or classroom use is granted without fee provided that copies are \nnot made or distributed for pro.t or commercial advantage and that copies bear this notice and the full \ncitation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to \nlists, requires prior speci.c permission and/or a fee. OOPSLA 2009, October 25 29, 2009, Orlando, Florida, \nUSA. Copyright &#38;#169;c2009 ACM 978-1-60558-734-9/09/10. . . $10.00 analyses. It is, thus, not surprising \nthat a wealth of research has been devoted to e.cient and precise pointer analysis techniques. Context-sensitive \nanalyses are the most common class of precise points-to analyses. Context sensitive analysis approaches \nqualify the analysis facts with a context abstrac\u00adtion, which captures a static notion of the dynamic \ncontext of a method. Typical contexts include abstractions of method call-sites (for a call-site sensitive \nanalysis the traditional meaning of context-sensitive ) or receiver objects (for an object-sensitive \nanalysis). In this work we present Doop: a general and versatile points-to analysis framework that makes \nfeasible the most precise context-sensitive analyses reported in the literature. Doop implements a range \nof algorithms, including context insensitive, call-site sensitive, and object-sensitive analyses, all \nspeci.ed modularly as variations on a common code base. Compared to the prior state of the art, Doop \noften achieves speedups of an order-of-magnitude for several important analyses. The main elements of \nour approach are the use of the Dat\u00adalog language for specifying the program analyses, and the aggressive \noptimization of the Datalog program. The use of Datalog for program analysis (both low-level [13,23,29] \nand high-level [6,9]) is far from new. Our novel optimization ap\u00adproach, however, accounts for several \norders of magnitude of performance improvement: unoptimized analyses typically run over 1000 times more \nslowly. Generally our optimiza\u00adtions .t well the approach of handling program facts as a database, by \nspeci.cally targeting the indexing scheme and the incremental evaluation of Datalog implementations. \nFur\u00adthermore, our approach is entirely Datalog based, encoding declaratively the logic required both \nfor call graph construc\u00adtion as well as for handling the full semantic complexity of the Java language \n(e.g., static initialization, .nalization, reference objects, threads, exceptions, re.ection, etc.). \nThis makes our pointer analysis speci.cations elegant, modular, but also e.cient and easy to tune. Generally, \nour work is a strong data point in support of declarative languages: we ar\u00adgue that prohibitively much \nhuman e.ort is required for im\u00adplementing and optimizing complex mutually-recursive def\u00adinitions at an \noperational level of abstraction. On the other hand, declarative speci.cations both admit automatic opti\u00admizations \nas well as a.ord the user the ability to identify and apply straightforward manual optimizations.  We \nevaluate Doop in comparison to Lhot\u00b4ak and Hen\u00addren s Paddle framework [18]. Paddle is based on Binary \nDecision Diagrams (BDDs) and represents the state of the art in context sensitive pointer analyses, in \nterms of both semantic completeness (i.e., support for Java language fea\u00adtures) and scalability. Furthermore, \nPaddle is a highly .exi\u00adble framework that was used to illustrate the di.erent charac\u00adteristics and parameters \nof context-sensitivity. Doop has the same attractive features and yields identical analysis results (based \non a logically equivalent algorithm). Our 1-call-site\u00adsensitive analysis of the DaCapo benchmarks applications \n(and JDK 1.4) yields an average speedup of 16.3x, lower\u00ading analysis times from several minutes to below \na minute in many cases. For a 1-object-sensitive analysis, Doop is 15x faster than Paddle. Such speedups \nare rare in the pro\u00adgram analysis literature, especially for completely equiva\u00adlent analyses. Lhot\u00b4ak \nand Hendren recently speculated [18] it should be feasible to implement an e.cient non-BDD\u00adbased 1-object-sensitive \nanalysis . We show that such an analysis not only is feasible but also outperforms BDDs by an order of \nmagnitude. Generally, our approach reveals interesting insights re\u00adgarding the use of BDDs, compared \nto an explicit represen\u00adtation of relations, for points-to analysis. Our work raises the question of \nwhether the points-to analysis domain has enough regularity for BDDs to be bene.cial. Although we have \nfound analyses that are possible with BDDs yet we could not perform with an explicit representation, \nevery such analysis seemed to su.er from vast (but very regular) im\u00adprecision. Easy algorithmic enhancements \ncan be applied to reduce the unnecessary redundancy in the relations that a points-to analysis keeps, \nand produce an analysis that is both much faster without BDDs and more precise. For instance, Doop would \nnot scale to a 1H-object-sensitive analysis (i.e., 1-object-sensitive with a context-sensitive heap) \nin the form speci.ed in the Paddle analysis set. Yet this is only because a naive analysis speci.cation \nresults in high redundancy, which necessitates BDDs. Two simple algorithmic enhance\u00adments su.ce for making \nthe analysis feasible for Doop: 1) we perform exception analysis on-the-.y [3], computing contexts that \nare reachable because of exceptional control .ow while performing the points-to analysis itself. The \non\u00adthe-.y exception analysis signi.cantly improves both pre\u00adcision and performance; 2) we treat static \nclass initializers context-insensitively (since points-to results are equivalent for all contexts of \nstatic class initializers), thus improving performance while keeping identical precision. The result \nof combining Doop s optimization approach and our algorithmic enhancements is that Doop addresses several \nopen problems in the points-to analysis literature. Lhot\u00b4 ak and Hendren estimated that e.ciently implement\u00ading \na 1H-object-sensitive analysis without BDDs will require new improvements in the data structures and \nalgorithms used to implement points-to analyses [18]. Doop achieves this goal, with fairly routine data \nstructures (plain B-trees). Furthermore, Doop reproduces the most complex points-to analyses of the Paddle \nset a result previously considered impossible without BDDs. Even more importantly, Doop scales to analyses \nthat are impossible with current BDD\u00adbased approaches, such as a 2H-call-site-sensitive analysis. In \nsummary, our work makes the following contributions: We provide the .rst fully declarative speci.cation \nof com\u00adplex, highly precise points-to analyses. Our speci.cation distills points-to analysis algorithms \ndown to their essence, instead of confusing the logical statement of an analy\u00adsis with implementation \ndetails. Past work on specifying points-to analyses in Datalog has always been a hybrid be\u00adtween imperative \ncode and a logical speci.cation, omitting essential elements from the logic. For instance, the bddb\u00adddb \nsystem [28, 29] (which pioneered practical Datalog\u00adbased points-to analysis) expresses the core of a \npoints-to analysis in Datalog, while important parts (such as nor\u00admalization and call-graph computation \nexcept for sim\u00adple, context-insensitive, analyses) are done in Java code. In general, Doop o.ers the \n.rst declarative speci.cation of a context-sensitive points-to analysis with on-the-.y (i.e., fully interleaved) \ncall-graph computation. Addition\u00adally, our speci.cation of algorithms is quite sophisticated, addressing \nelements of the Java language (such as native code, .nalization, and privileged actions) that were absent \nfrom previous declarative approaches (e.g., bddbddb) and that crucially a.ect precision and performance. \nAs a result, Doop provides an analysis that emulates and often exceeds the rich feature set of the Paddle \nframework, while staying entirely declarative.  We introduce a novel optimization methodology, applied \nentirely at the Datalog level, for producing e.cient algo\u00adrithms directly from the logical speci.cation \nof an anal\u00adysis. The optimization approach employs standard pro\u00adgram transformations (such as variable \nreordering and folding a common logic programming optimization) yet determines when to do so by taking \ninto account the semi-naive algorithm for incremental evaluation of Dat\u00adalog rules, as well as the indexes \nthat are used for each relation. As a result, Doop achieves order-of-magnitude performance improvements \nover the closest comparable points-to framework in the literature for common context\u00adsensitive analyses. \n We show that Doop scales to perform the most pre\u00adcise context-sensitive analyses ever evaluated in \nthe re\u00adsearch literature. Doop not only implements the rich set of analyses of the Paddle system but \nalso scales to anal\u00adyses that are beyond reach for Paddle, such as a 2-call\u00adsite-sensitive analysis with \na context-sensitive heap, and   a 2-object-sensitive analysis with a (1-context) context\u00adsensitive \nheap. We contrast and study the performance of BDD-based representations for points-to analysis, relative \nto explicit representations. We show how performance is correlated with key BDD metrics and extrapolate \non the suitability of BDDs for fast and precise points-to analyses.  2. Background: Datalog Points-To \nAnalysis The use of deductive databases and logic programming lan\u00adguages for program analysis has a long \nhistory (e.g., [4,23]) and has raised excitement again recently [6, 9, 13, 28, 29]. Like our work, much \nof the past emphasis has been on using the Datalog language. Datalog is a logic program\u00adming language \noriginally introduced in the database do\u00admain. At a .rst approximation, one can view Datalog as ei\u00adther \nSQL with full recursion or Prolog without construc\u00adtors/functions . The essence of the language is its \nability to de.ne recursive relations. Relations (or equivalently predi\u00adcates) are the main Datalog data \ntype. Computation consists of inferring the contents of all relations from a set of in\u00adput relations. \nFor instance, in our pointer analysis domain, it is easy to represent the relevant actions of a Java \nprogram as relations, typically stored as database tables. Consider two such relations, AssignHeapAllocation(?var,?heap) \nand Assign(?from,?to). (We follow the convention of cap\u00aditalizing the .rst letter of relation names, \nwhile writing vari\u00adable names in lower case and pre.xing them with a question\u00admark.) The former relation \nrepresents all occurrences in the program of an instruction a = new A(); where a heap ob\u00adject is allocated \nand assigned to a variable. That is, a pre\u00adprocessing step takes a Java program (in our implementation \nthis is in intermediate, bytecode, form) as input and produces the relation contents. A static abstraction \nof the heap object is captured in variable ?heap it can be concretely repre\u00adsented as, e.g., a fully \nquali.ed class name and the alloca\u00adtion s bytecode instruction index. Similarly, relation Assign contains \nan entry for each assignment between two Java pro\u00adgram (reference) variables. The mapping between the \ninput Java program and the input relations is straightforward and purely syntactic. After this step, \na simple pointer analysis can be expressed entirely in Datalog as a transitive closure computation: \n1 VarPointsTo(?var, ?heap) <\u00ad2 AssignHeapAllocation(?var, ?heap). 3 VarPointsTo(?to, ?heap) <\u00ad4 Assign(?from, \n?to), VarPointsTo(?from, ?heap). The Datalog program consists of a series of rules that are used to \nestablish facts about derived relations (such as VarPointsTo, which is the points-to relation, i.e., \nit links every program variable, ?var, with every heap object abstraction, ?heap, it can point to) from \na conjunction of previously established facts. We use the left arrow symbol (<-) to separate the inferred \nfact (the head) from the previously established facts (the body). For instance, lines 3-4 above say that \nif, for some val\u00adues of ?from, ?to, and ?heap, Assign(?from,?to) and VarPointsTo(?from,?heap) are both \ntrue, then it can be in\u00adferred that VarPointsTo(?to,?heap) is true. Note the base case of the computation \nabove (lines 1-2), as well as the re\u00adcursion in the de.nition of VarPointsTo (line 3-4). The declarativeness \nof Datalog makes it attractive for specifying complex program analysis algorithms. Particu\u00adlarly important \nis the ability to specify recursive de.nitions, as program analysis is fundamentally an amalgam of mu\u00adtually \nrecursive tasks. For instance, in order to do accurate reachability analysis (i.e., answer the question \nis method m1 reachable from method m2? ) we need to have points-to information, so that the target objects \nof a virtual method call are known. But in order to do points-to analysis, we need to have reachability \ninformation, to know which vari\u00adable assignment actions are truly possible. A mutually re\u00adcursive de.nition \nof a reachability and points-to analysis is easy to specify in Datalog, and is part of the Doop frame\u00adwork. \nThe elegance of the approach is evident when con\u00adtrasted with common implementations of points-to analyses. \nEven conceptually clean program analysis algorithms that rely on mutually recursive de.nitions often \nget transformed into complex imperative code for implementation purposes (e.g., compare the straightforward \nlogic with the complex algorithmic speci.cation in Reference [26]). Datalog evaluation is typically bottom-up, \nmeaning that known facts are propagated using the rules until a maximal set of derived facts is reached. \nThis is also the link to the data processing intended domain of Datalog: evaluation of a rule can be \nthought of as a sequence of relational algebra joins and projections. For instance, the evaluation of \nlines 3-4 in our above example can be thought of as: Take the join of relation Assign with relation VarPointsTo \nover the .rst column of both (because of common .eld ?from) and project the join result on .elds ?to \nand ?heap. The result of the projection is added to relation VarPointsTo (skipping duplicates) and forms \nthe value of VarPointsTo for the next iteration step. Application of all rules iterates to .xpoint. Note \nthat this means that the evaluation of a Datalog pro\u00adgram comprises two distinct kinds of looping/iteration \nac\u00adtivities: the relational algebra joins and projections, and the explicit recursion of the program. \nThe former kind of loop\u00ading is highly e.cient through traditional database optimiza\u00adtions (e.g., for \njoin order, group-fetching of data from disk, locality of reference, etc.). We use a commercial Datalog \nengine, developed by our industrial partner, LogicBlox Inc. (The engine is freely avail\u00adable for research \nuse through us and we have already granted access to a handful of early adopters.) This version of Data\u00adlog \nallows strati.ed negation , i.e., negated clauses, as long as the negation is not part of a recursive \ncycle. It also allows specifying that some relations are functions, i.e., the vari\u00adable space is partitioned \ninto domain and range variables, and there is only one range value for each unique combina\u00adtion of values \nin domain variables. We will see these features in action in our algorithm speci.cation, next.  3. \nDoop Pointer Analysis Speci.cations Our Doop framework is a versatile Datalog implementation of a range \nof pointer analyses. Doop is available online at http://doop.program-analysis.org.Doop strives for full \nJava language support and follows closely the approach of Paddle the most complete analysis in prior \nliterature in dealing with various Java language features. We next discuss in more detail the features \nand precision of the framework. 3.1 Overview and Preliminaries Doop distills points-to analysis algorithms \nto a purely declar\u00adative speci.cation. An advantage of a declarative speci.ca\u00adtion is that it dissociates \nthe logic of the analysis (i.e., the precision of the end result as well as intermediate results) from \nthe implementation decisions used to perform the anal\u00adysis e.ciently. The resulting speci.cation is a \nDatalog pro\u00adgram, and is, therefore, executable. Nevertheless, the pro\u00adgram may not be e.cient as originally \nspeci.ed. The goal of our optimization methodology (described in Section 4) is to produce equivalent \nDatalog programs that are more e.cient. For the rest of this section, however, we are only concerned \nwith the logical speci.cation of the analyses. This separation of speci.cation from implementation is \nalready done informally, as a classi.cation, in the points\u00adto analysis literature. Several di.erent published \nalgorithms occupy the same point in the design space (e.g., they are all 1-object-sensitive analyses) \nbut di.er in properties such as their average runtime or asymptotic complexity, often be\u00adcause of di.erent \nchoices of indexing and storage data struc\u00adtures. Hence, the e.ort to specify an analysis in Doop con\u00adsists \nof, .rst, producing a logical speci.cation and, then, de\u00adriving an e.cient algorithm for that speci.cation. \nThe two steps are not entirely independent, because it is sometimes hard to tell which decisions are \npart of the speci.cation of an analysis and which are part of the implementation . For example, treating \nthe static initializers of Java classes context-insensitively (even for a context-sensitive analysis) \ndoes not a.ect the end result of an analysis, but has a ma\u00adjor impact on its runtime. However, expressing \nthis deci\u00adsion a.ects the speci.cation: the two Datalog programs are not equivalent. (What makes these \nspeci.cations equivalent is extra knowledge about the input relations, i.e., a restric\u00adtion of the input \ndomain that only the algorithm designer knows.) In this paper, we call optimizations the transforma\u00adtions \nthat produce an equivalent Datalog program (i.e., all relations have the same contents for all inputs), \nand call log\u00adical enhancements or algorithmic enhancements the trans\u00adformations that logically change \nthe original speci.cation.  3.2 Doop Contents Doop supports a general pointer analysis trunk and several \ndi.erent analysis variations. The main variants we have ex\u00adplored are a context-insensitive analysis, \nas well as context sensitive analyses with 1-and 2-object, as well as 1-and 2-call-site contexts, with \nor without a context-sensitive heap (a.k.a. heap cloning) with di.erent heap context depths. This variability \nis not directly supported in Datalog: for instance, for a context-sensitive analysis, the relation VarPointsTo \nneeds extra arguments representing the context (be it a call\u00adsite context, or an object context) of the \nvariable. Similarly, for analyses utilizing a context-sensitive heap, the abstrac\u00adtion of the heap object \nneeds to be quali.ed by extra vari\u00adables for its context. (In such analyses, an abstract object consists \nof the allocation site and the context of the method that contains that allocation site.) These di.erences \nare su\u00adper.cial, however. We have abstracted away from them by creating a small extension of Datalog \nthat allows tuples of variables in place of a single variable. The extension is im\u00adplemented as a macro \nand hides the con.guration of the particular analysis, to the extent possible. The plain-Datalog code \nfor each analysis is then generated by instantiating the macros. The total size of the analysis logic \nin Doop is less than 2500 lines of code (approximately 180 Datalog pro\u00adgram rules) and another some 1000 \nlines of relation declara\u00adtions (i.e., speci.cations of the database schema), comments, and minor support \ncode. These metrics include all pointer analysis variants, but commonalities are factored out using our \nvariable-tuple mechanism. The plain-Datalog size of a single analysis variant after macro-expansion is \nin the or\u00adder of 500-1000 lines, or 120-150 Datalog rules. In the code examples of this paper, unless \nstated otherwise, we will ig\u00adnore variations and concentrate on the standard 1-call-site\u00adsensitive analysis \nfor concreteness. The analysis logic in Doop can be viewed as an elabora\u00adtion of the simple Datalog example \nshown earlier. Consider the full-.edged analogues of the two basic rules from Sec\u00adtion 2. 1 VarPointsTo(?ctx, \n?var, ?heap) <\u00ad2 AssignHeapAllocation(?var, ?heap, ?inmethod), 3 CallGraphEdge(_, _, ?ctx, ?inmethod). \n4 5 VarPointsTo(?toCtx, ?to, ?heap) <\u00ad6 Assign(?fromCtx, ?from, ?toCtx, ?to, ?type), 7 VarPointsTo(?fromCtx, \n?from, ?heap), 8 HeapAllocation:Type[?heap] = ?heaptype, 9 AssignCompatible(?type, ?heaptype). (We use \nsome extensions and notational conventions in the code. First, some of our relations are functions, and \nthe func\u00adtional notation Relation[?domainvar] = ?val is used in\u00adstead of the relational notation, Relation(?domainvar, \n?val) . Semantically the two are equivalent, but the execu\u00adtion engine enforces the functional constraint \nand produces an error if a computation causes a function to have multiple range values for the same domain \nvalue. Second, the colon (:) in relation names is just a regular character with no se\u00admantic signi.cance \nwe use common pre.xes ending with a colon as a lexical convention for grouping related predicates. Finally, \n stands for any value , in the standard logic pro\u00adgramming convention.)  The full rules di.er from \ntheir simpli.ed versions in sev\u00aderal ways. First, all relations have extra arguments for the context \nof Java variables: wherever the original relations had a Datalog variable that corresponded to a Java \nprogram vari\u00adable (e.g., ?from, ?to) the full relations have .rst a Data\u00adlog variable corresponding to \na context, and then one corre\u00adsponding to the Java variable. Second, for an allocation to .ow to a variable \nin a given context, the allocation site has to be reachable in the given context, from any other method \nand context (line 3). Finally, variable assignments take into account the type system (through AssignCompatible, \non line 9) so that a variable is never considered to point to an object abstraction if its type prohibits \nit. Some more rules complete the de.nition of VarPointsTo. The full analysis takes into account method \ncalling, assign\u00adment to .elds, arrays, and more. Importantly, the entire analysis is speci.ed in Datalog, \nincluding call graph construction. That is, the interdepen\u00addency between call graph construction (i.e., \nwhich methods are reachable in a given context) and points-to analysis is ex\u00adpressed as plain Datalog \nmutual recursion. This allows call graph discovery on-the-.y, which Lhot\u00b4ak and Hendren [18] .nd to be \nan important asset for precision. Previous pointer analysis algorithms in Datalog (mainly Whaley et al. \ns bddb\u00adddb and its client analyses [21, 28, 29]) did not support on\u00adthe-.y call-graph discovery, except \nfor very simple, context\u00adinsensitive, analyses.1 For a concrete instance of the mutual recursion, we \ncan look at one of the rules de.ning the CallGraphEdge rela\u00adtion (which is used to compute VarPointsTo \nand itself uses VarPointsTo). The rule computes call graph edges due to virtual method invocations and \nis shown in Figure 1. The de.nition of CallGraphEdge also uses an auxiliary de.ni\u00adtion, shown in Figure \n2, of a virtual method lookup relation. Combined, this is the declarative speci.cation of fully on\u00adthe-.y \ncall graph discovery, which is more precise than a pre-computed call graph, as in bddbddb.  3.3 Support \nfor Java Language Features Doop o.ers full support for Java language semantics, en\u00adtirely in Datalog, \nwithout other peripheral analyses. We 1 Speci.cally, the bddbddb work [13, 29] computes the call-graph \non-the\u00ad.y with a context-insensitive analysis, and then uses it as input to context\u00adsensitive analyses. \nThus, the added precision of the context-sensitive points\u00adto analysis is not available to the call-graph \ncomputation, which, in turn, reduces the precision of the points-to analysis. This limitation is not \ninci\u00addental. Since context-sensitivity in bddbddb is handled through a cloning approach [29], a pre-computed \ncall-graph is necessary: cloning techniques are based on copying methods for each of their calling contexts. \nCallGraphEdge(?callerCtx, ?call, ?calleeCtx, ?callee) <\u00adVirtualMethodCall:Base[?call] = ?base, VirtualMethodCall:SimpleName[?call] \n= ?name, VirtualMethodCall:Descriptor[?call] = ?descriptor, VarPointsTo(?callerCtx, ?base, ?heap), HeapAllocation:Type[?heap] \n= ?heaptype, MethodLookup[?name, ?descriptor, ?heaptype] = ?callee, ?calleeCtx = ?call. Figure 1. Computing \n(context-sensitive) call graph edges from a call-site to a method, both under speci.c contexts. A call \ngraph edge exists if there exists a virtual method call, ?call, whose receiver object is referenced through \nvariable ?base, which points to a heap object, ?heap, whose type contains a method, ?callee, compatible \nwith the virtual call. The context of ?callee for this call is just the call-site, ?call, since the code \nis for a 1-call-site-sensitive analysis. MethodLookup[?name, ?descriptor, ?type] = ?method <-MethodImpl[?name, \n?descriptor, ?type] = ?method. MethodLookup[?name, ?descriptor, ?type] = ?method <-DirectSuperclass[?type] \n= ?supertype, MethodLookup[?name, ?descriptor, ?supertype] = ?method, not exists MethodImpl[?name, ?descriptor, \n?type]. MethodImpl[?name, ?descriptor, ?type] = ?method <-MethodDecl[?name, ?descriptor, ?type] = ?method, \nnot MethodModifier(\"abstract\", ?method). Figure 2. The de.nition of relation MethodLookup, used in Figure \n1. Looking up a method with a speci.c name (?name), return type, and parameter types (?descriptor) in \na given type (?type) is done by either .nding a non-abstract method declaration within ?type, or repeating \nthe lookup for the direct superclass of ?type if no such declaration exists. (The syntax not exists F[x] \nmeans that there is no value v for which F[x] =v.) closely modeled the handling of Java features after \nthe logic in the Paddle system. Paddle covers several complex Java features and semantic complexities \n(e.g., .nalization, privi\u00adleged actions, threads, etc.). Implementing an analysis that is logically equivalent \nto Paddle helps demonstrate that our Datalog-based approach is a full-featured implementation and not \na toy or a proof-of-concept. Indeed, in several cases we found ways to add more precision or model Java \nsemantics better than Paddle, thus improving over past state-of-the-art techniques and mak\u00ading Doop probably \nthe most sophisticated pointer analysis framework in existence for Java. This sophistication is im\u00adportant \nfor client analyses that need sound results. For in\u00adstance, compared to Paddle,Doop adds such features \nas: Better initialization of the Java Virtual Machine. For ex\u00adample, we model the system and main thread \ngroup, main thread.  Full support for Java s reference objects (such as WeakReference) and reference \nqueues. For example, ref\u00ad   erence queues are used by Java Virtual Machine to invoke finalize methods. \n More sophisticated re.ection analysis. For example, Doop uses distinct representations of instances \nof java.lang.Class for every class in the analyzed program. This reduces the number of human con.guration \npoints, solves more re.ection scenarios automatically, and im\u00adproves precision.  More precise class \ninitialization, modeling better the Java Language Speci.cation.  More precise handling of cast and assignment \ncompatibil\u00adity checking.  More precise exception analysis, using logic that is mu\u00adtually recursive with \nthe main points-to logic. Exceptions are propagated over the context-sensitive call graph, caught exceptions \nare .ltered, and the order of exception handlers is considered. In a separate publication [3] we describe \non-the-.y exception analysis in detail and demonstrate its impact on precision, especially for object-sensitive \nanaly\u00adses. On-the-.y exception analysis is expressible highly el\u00adegantly in Doop another bene.t of the \ndeclarative speci\u00ad.cation approach.  Native methods are simulated in a more principled way. In Paddle, \nindirect method calls via native code are some\u00adtimes not represented explicitly, but shortcut directly \nfrom the Java call to the Java method. We model the call graph edges more precisely, which is important \nif applications need a correct call graph.  The declarative approach was of great help in adding language \nfeature support. A major bene.t is that semantic extensions are well localized and do not a.ect the basic \nde.nitions (e.g., those in Section 3.2) at all. In contrast, several features in the Paddle framework \n(e.g., privileged actions, .nalization, threads) have their implementation span multiple components. \nA second advantage of the declarative approach is that the logic is high-level and often very close to \nthe Java Language Speci.cation. A striking example is the implementation of the logic for the Java cast \nchecking i.e., the answer to the question can type A be cast to type B? Figure 3 shows the full logic, \ndirectly from the Doop implementation, with the text of the Java Language Speci.cation in the comments \npreceding each rule. As can be seen, the Datalog code is almost an exact transcription of the Java speci.cation. \n(The main di.erence is that the speci.cation is written in a must style, whereas the Datalog code speci.es \nwhich casts may happen. The must property is ensured by the least-.xpoint evaluation of Datalog.)  3.4 \nDiscussion Doop currently supports a rich range of analyses with standard precision enhancements from \nthe research litera\u00ad /** If S is an ordinary (nonarray) class, then: * o If T is a class type, then S \nmust be the * same class as T, or a subclass of T. */ CheckCast(?s, ?s) <-ClassType(?s). CheckCast(?s, \n?t) <-Subclass(?t, ?s). /** o If T is an interface type, then S must * implement interface T. */ CheckCast(?s, \n?t) <-ClassType(?s), Superinterface(?t, ?s). /** If S is an interface type, then: * o If T is a class \ntype, then T must be Object */ CheckCast(?s, \"java.lang.Object\") <-InterfaceType(?s). /** o If T is an \ninterface type, then T must be the * same interface as S or a superinterface of S */ CheckCast(?s, ?s) \n<-InterfaceType(?s). CheckCast(?s, ?t) <-InterfaceType(?s), Superinterface(?t, ?s). /** If S is a class \nrepresenting the array type SC[], * that is, an array of components of type SC, then: * o If T is a class \ntype, then T must be Object. */ CheckCast(?s, \"java.lang.Object\") <-ArrayType(?s). /** o If T is an array \ntype TC[], that is, an * array of components of type TC, then one * of the following must be true: * \n+ TC and SC are the same primitive type */ CheckCast(?s, ?t) <-ArrayType(?s), ArrayType(?t), ComponentType(?s, \n?sc), ComponentType(?t, ?sc), PrimitiveType(?sc). /** + TC and SC are reference types (2.4.6), * and \ntype SC can be cast to TC by * recursive application of these rules. */  CheckCast(?s, ?t) <-ComponentType(?s, \n?sc), ComponentType(?t, ?tc), ReferenceType(?sc), ReferenceType(?tc), CheckCast(?sc, ?tc). /** o If T \nis an interface type, T must be one of * the interfaces implemented by arrays (2.15). */ CheckCast(?s, \n\"java.lang.Cloneable\") <-ArrayType(?s). CheckCast(?s, \"java.io.Serializable\") <-ArrayType(?s). Figure \n3. Checkcast implementation in Doop. ture. This range includes or exceeds practically all precise context-sensitive \nanalyses demonstrated to be feasible in prior literature. We refer throughout the paper to the pre\u00adcision \ncharacteristics of the analyses in Doop, especially by reference to other systems. In order, however, \nto classify the Doop-supported analyses in the larger spectrum of pointer analysis mechanisms, it is \nconvenient to explicitly list the major features for completeness:  Doop implements subset-based (or \ninclusion-based) anal\u00adyses, which preserve the directionality of assignments (un\u00adlike equivalence-based \nanalyses).  There is fully on-the-.y callgraph discovery. Additionally, the propagation of analysis \nfacts is limited to reachable methods (i.e., takes the callgraph into account).  The analyses are .eld-sensitive, \nwhich distinguishes be\u00adtween the di.erent .elds of an object (as opposed to .eld\u00adinsensitive ), and between \n.elds of di.erent objects (as op\u00adposed to .eld-based ).  The analyses can have di.erent kinds of context\u00adsensitivity \n(call-site, thread-or object-sensitivity) as well as a context-sensitive heap abstraction ( heap cloning \n). The context of a called method can be chosen from the current context as well as the context of the \nreceiver ob\u00adject.  The analyses are array-element insensitive, i.e. elements of an array are not distinguished. \n The analyses take type information into account: points\u00adto facts are not propagated if they would violate \nthe JVM type system.  Doop integrates several specialized precision enhance\u00adments. For instance, a straightforward \nbut imprecise way to model the .ow of the receiver object in virtual method dis\u00adpatch is by an assignment \nof the base variable of the virtual call (?base in Figure 1) to this. This is imprecise, since the same \nvirtual method call can invoke di.erent methods, depending on the type of the receiver object. These \nmeth\u00adods all receive the same points-to set for this if the base variable is assigned to this. Instead, \nwe combine the as\u00adsignment of receiver objects with virtual method dispatch and assign a speci.c receiver \nobject (?heap in Figure 1) to this. This precision improvement is borrowed from Pad\u00addle.  Doop only \nconsiders special methods (constructors, pri\u00advate, and superclass methods) reachable if the base vari\u00adable \nof the invocation points to any objects. Unlike virtual method invocations, the target of a special method \ninvo\u00adcation does not depend on the run-time class of the object. Therefore, it is tempting to ignore \nthe objects the base vari\u00adable points to. However, if the variable does not point to any objects, then \nthe method cannot be invoked. This pre\u00adcision improvement is borrowed from Paddle as well.  Just as \nin the Paddle framework, Doop can achieve some of the bene.ts of .ow-sensitivity for local variables, \nby applying the analysis on the static single assignment (SSA) form of the program, e.g. the SSA variant \nof Soot s Jimple intermediate representation of Java bytecode.  The above list immediately serves to \nclassify the Doop\u00adsupported analyses as much more precise and full-featured than previous declarative \npointer analyses in the literature. Speci.cally, the bddbddb system [28, 29] lacks in support for many \nJava features, such as native code, re.ection, .\u00adnalization, etc., whose handling constitutes a large \npart of the Doop analyses. Although sophisticated client analyses have been implemented on top of bddbddb \n(e.g., jchord [21]) these analyses are such that they can tolerate unsound han\u00addling of Java features, \nand they act as pure clients: their sophistication does not bene.t in any way the precision of the base \npoints-to analysis. Similarly, the quite sophis\u00adticated re.ection analysis of Livshits et al. [19] is \nexpressed on top of bddbddb s points-to analysis, but is not strictly declarative since it depends on \nfacts computed by a Java pre-analysis, and only applies to context-insensitive analy\u00adses. (As mentioned \nearlier, context-sensitivity in bddbddb is cloning-based and, thus, relies on having a pre-computed call-graph. \nIntegrating this with re.ection would be non\u00adtrivial.) Furthermore, the re.ection analysis of Livshits \net al. produces an incorrect call-graph, because it does not take into account the possibility of dynamic \ndispatch for methods invoked re.ectively. This observation is perhaps indicative of the di.erence between \ntreating language features as an in\u00adtegral part of a declarative points-to analysis intended as the basis \nfor sound inferences, vs. separating the base points-to analysis from language feature support. Doop \ns handling of re.ection can be viewed as analogous to adding a sophisti\u00adcated analysis similar to Livshits \net al. s, but in conjunction with a context-sensitive points-to analysis, to obtain the full bene.t from \nthe mutual increase in precision of both com\u00adponent analyses. To illustrate the gap in analysis sophistication \nbetween bddbddb and Doop (as well as Paddle), we performed the same context-insensitive analysis in both \nframeworks for the DaCapo benchmark programs. (DaCapo v.2006-10-MR2, JDK 1.4 j2re1.4.2 18, bddbddb svn \nrevision 654, joeq com\u00adpiler framework revision 2483.) Compared to Doop, bddb\u00adddb reports roughly half \nthe reachable methods (max: 74%, min: 17%, median: 53%, over the 10 DaCapo applications) and less than \none-quarter of the points-to facts (max: 64%, min: 3%, median: 21%). The discrepancy is due entirely \nto the incompleteness of the points-to logic in bddbddb, since the analyses have the same inherent precision. \n(Increased precision would be unlikely to account for such a dramatic reduction in reachable methods \nanyway: even the most pre\u00adcise, highly context-sensitive analysis in the Doop and Pad\u00addle set barely \nreduces the number of reachable methods by 3-4%.) In the past, researchers have questioned whether it \nis even possible to express purely declaratively a full-featured points-to analysis (comparable to Paddle, \nwhich uses im\u00adperative code with support for relations [17]). Lhot\u00b4ak [15] writes:  [E]ncoding all the \ndetails of a complicated program analysis problem (such as the interrelated analyses [on\u00adthe-.y call \ngraph construction, handling of Java features]) purely in terms of subset constraints [i.e., Datalog] \nmay be di.cult or impossible. Doop demonstrates that an elegant declarative speci.cation is possible \nand even easy. Although Doop is a .exible framework, it is not suited to all kinds of analyses. A clear \nlimitation, for instance, is that the context-depth used in the analysis has to be bounded. Doop cannot \nsupport analyses that keep an unbounded num\u00adber of calling contexts, even if the number is guaranteed \nto be .nite (e.g., recursive cycles are .attened). This is due to the lack of constructors/functions \nin Datalog. This ob\u00adservation is unlikely to have any bearing in practice, how\u00adever, since other precision \nenhancements, such as a context\u00adsensitive heap, have been shown to be a better trade-o. than an unbounded \nnumber of contexts [18]. Combining a context-sensitive heap with even small bounds in context sensitivity \n(e.g., 4-context-sensitive) is su.cient to make an analysis explode in complexity.  4. Illustration \nof Doop Optimizations A declarative speci.cation has advantages in terms of modu\u00adlarity, ease of understanding, \nand conciseness of expression. One more advantage, however, is that it decouples the analy\u00adsis logic \nfrom its implementation, and allows high-level rea\u00adsoning about implementation choices. In Doop we have \nused a novel optimization methodology to convert initial speci\u00ad.cations into highly e.cient algorithms. \nBecause Doop is expressed in a version of Datalog that exposes indexing de\u00adcisions to the language level, \nwe can illustrate the optimiza\u00adtions as just Datalog program transformations. We begin with some background \ninformation on Datalog runtimes and the particular engine we use. 4.1 Background: E.cient Datalog Evaluation \nA standard optimization for Datalog (indeed, a virtual pre\u00adrequisite for high performance implementations) \nis the semi\u00adnaive evaluation strategy. Semi-naive evaluation keeps track of relation deltas on every \nrecursive step, which corre\u00adspond to the new facts produced by the step. In this way, the next step s \nresults are derived incrementally by using only the previous step s deltas, in all their possible join \ncombina\u00adtions with full relations. Consider the evaluation of the ex\u00adample from Section 2, reproduced \nbelow: 1 VarPointsTo(?var, ?heap) <\u00ad2 AssignHeapAllocation(?var, ?heap). 3 VarPointsTo(?to, ?heap) <\u00ad4 \nAssign(?from, ?to), VarPointsTo(?from, ?heap). Initially, relation VarPointsTo is empty. The .rst step \npopulates relation VarPointsTo with the facts from AssignHeapAllocation, as dictated by lines 1-2. The \nrule in lines 3-4 has nothing to contribute, since VarPointsTo was empty at the beginning of the step. \nIn the second step, however, this rule joins the new members of VarPointsTo from step 1, .VarPointsTo1, \nwith those of input relation Assign. This produces .VarPointsTo2, i.e., the new mem\u00adbers of VarPointsTo \nfrom step 2. The next step only needs to join .VarPointsTo2 with Assign, in order to produce .VarPointsTo3, \nand so on. This optimization is straightforward, yet crucial. It is a major bene.t that we get for free \nfrom using a declarative language for specifying our analysis. There are more bene.ts that Doop receives \nfor free through standard Datalog imple\u00admentation techniques. Speci.cally, local join optimization is \nperformed: a good order of joins in a single Datalog rule is automatically determined based on statistics \non the size of relations and selectivity of joins. This baseline is valuable but still leaves us orders \nof magnitude away from the perfor\u00admance of a state-of-the-art context-sensitive program analy\u00adsis. For \nthis we need optimizations across rules, introduction of new database indexes, etc. These optimizations \nare typi\u00adcally not well-automatable: they correspond to producing an e.cient algorithm from a speci.cation, \nand require human intervention. In order to execute Datalog programs e.ciently, the low\u00adlevel representation \nof relations should be compact and an indexing scheme should be in place so that all rules are ex\u00adecuted \ne.ciently. The LogicBlox Datalog engine used for Doop allows the user to specify maximum cardinalities \nfor the domains of variables (e.g., the maximum number of val\u00adues for ?var in relation VarPointsTo(?var, \n?heap)). These are used to store domain values as integers and all values of variables (keys) in the \nsame relation (?var and ?heap in our example) are packed together in the smallest number of ma\u00adchine \nwords possible using bit shifts and mask operations. A relation is then represented as a sequence of \nthese packed in\u00adtegers for which the relation is true. (Alternatively, the user can specify that the \ndefault value for the relation is false , in which case the system stores all packed keys for which the \nrelation is false. So far we have not used this capability in Doop because all points-to results are \nvery sparse relations.) As in all database languages, e.ciency of execution typ\u00adically depends on what \nindexes are de.ned on the data so that relational operations can be highly e.cient. A unique feature \nof the Datalog engine that we use is that the index\u00ading is exposed to the Datalog language level. In \nthis way, introducing and eliminating indexes can be viewed as just a program transformation, instead \nof needing to edit the data schema or other con.guration .les. Speci.cally, a relation, e.g., VarPointsTo(?var, \n?heap), is stored with its contents (pairs of packed variable values) ordered by innermost vari\u00adable, \ni.e., ?heap, and then by the next innermost variable, i.e., ?var, etc. The relation is indexed using \na B-tree with a key consisting of all variables together. Since, however, a B\u00adtree is an ordered map, \nknowing the value of the innermost variable alone is su.cient for e.cient indexing. (I.e., the in\u00adnermost \nvariable is the major index, the second innermost is the next major index, etc.) Thus, variable ordering \nis very important. The user can change the indexing e.ciency to op\u00adtimize joins, by just reordering variables. \nFor instance, a join between two relations is very fast if both relations have the join variables in \ntheir innermost positions and in the same order. In that case, both relations just need to be traversed \nlinearly and their contents merged. Another scheme for an e.cient join is when joining over the innermost \nvariable of one relation and the second relation is small (so it can be iterated exhaustively and bind \nthe index variable of the .rst relation). As a rule of thumb, when a relation is known to be small, the \nlocal query optimizer will automatically choose to perform the join by iterating exhaustively over its \ncon\u00adtents. The iteration will bind variables of other relations be\u00ading joined. These variables should \nbe in the innermost posi\u00adtions, so that their values can be used for e.cient indexing. Our optimization \nmethodology, described next, exploits this technique, in particular considering semi-naive evaluation. \n In summary, the use of Datalog in Doop separates the speci.cation of an analysis from its implementation, \nthere\u00adfore allowing multiple techniques for e.cient execution, all expressed at the level of Datalog \nevaluation. Our current Datalog engine is in many ways mature, but only uses very simple data structures \n(B-trees and an explicit representation of relations). It is tempting in the future to consider alterna\u00adtive \nDatalog execution techniques (e.g., the option to trans\u00adparently use BDDs to represent relations) especially \nif these are provided in a well-engineered implementation.  4.2 Optimization Methodology Based on this \nunderstanding of Datalog evaluation and op\u00adtimization opportunities, we next present the optimization \ntechniques we use in Doop through examples. Consider a re.nement of our above rudimentary two\u00adrule pointer \nanalysis logic. We will add to our analy\u00adsis .eld sensitivity: heap objects can be stored to and loaded \nfrom instance .elds and the analysis keeps track of such actions. (This example ignores other language \nfeatures such as method calls i.e., we assume the ana\u00adlyzed program is just a single main function.) \nTwo new input relations are derived from the code of a Java pro\u00adgram: LoadInstanceField(?base, ?signature, \n?to) and StoreInstanceField(?from, ?base, ?signature). The former tracks a load from the object referenced \nby vari\u00adable ?base in the .eld identi.ed by ?signature. If, for in\u00adstance, the Java program contains \nan action x = v.fld; , then LoadInstanceField contains an entry with ?base be\u00ading the representation \nof Java variable v , ?signature identifying .eld fld , and ?to corresponding to x . StoreInstanceField \ntracks store actions in a similar manner: Every Java program action v.fld = u; corre\u00adsponds to an entry \nin StoreInstanceField(?from, ?base, ?signature), with v represented by variable ?base, u repre\u00adsented \nby ?from, and an identi.er for .eld fld captured by ?signature. Our simple analysis can then be elaborated: \n(The .rst two rules are the same but two more rules are added.) A new relation, InstanceFieldPointsTo, \nis used to compute which heap object (?baseheap) can point to which other (?heap) through a given .eld \n(?signature). 1 VarPointsTo(?var, ?heap) <\u00ad2 AssignHeapAllocation(?var, ?heap). 3 VarPointsTo(?to, ?heap) \n<\u00ad4 Assign(?from, ?to), VarPointsTo(?from, ?heap). 5 VarPointsTo(?to, ?heap) <\u00ad6 LoadInstanceField(?base, \n?signature, ?to), 7 VarPointsTo(?base, ?baseheap), 8 InstanceFieldPointsTo(?baseheap, ?signature, ?heap). \n9 10 InstanceFieldPointsTo(?baseheap, ?signature, ?heap) <\u00ad 11 StoreInstanceField(?from, ?base, ?signature), \n12 VarPointsTo(?base, ?baseheap), 13 VarPointsTo(?from, ?heap). Reordering Transformation. The above \nis a straightfor\u00adward way to express the analysis, but the resulting program is highly ine.cient. (Recall \nthat the order of variables in the above relations re.ects how the relations are indexed.) In particular, \nthe joins of line 4, 6-8, and 11-13 are all costly. In line 4, neither relation has the join variable \nin its inner\u00admost position. In particular, relation VarPointsTo is recur\u00adsive. After the .rst step, Datalog \ns semi-naive evaluation will only need to join the delta of the VarPointsTo relation (i.e., a small relation) \nto produce the new results for the next step. Therefore, it makes sense to reorder the variables of rela\u00adtion \nAssign so that it is indexed e.ciently based on vari\u00adable bindings produced by VarPointsTo. That is, \nthe pro\u00adgram will be more e.cient if relation Assign is stored as Assign(?to, ?from) rather than Assign(?from, \n?to), be\u00adcause variable ?from is bound by iterating over the contents of small relation .VarPointsTo. \n(Of course, this decision on how to store Assign may adversely a.ect joins in other parts of the program \nwe will soon see how to resolve this.) Sim\u00adilar observations apply to the joins in lines 6-8 and 11-13: \nno relation has a join variable in its innermost position. Just by applying simple reorderings we can \nproduce a much more e.cient implementation: 1 VarPointsTo(?heap, ?var) <\u00ad2 AssignHeapAllocation(?heap, \n?var). 3 VarPointsTo(?heap, ?to) <\u00ad4 Assign(?to, ?from), VarPointsTo(?heap, ?from). 5 VarPointsTo(?heap, \n?to) <\u00ad6 LoadInstanceField(?to, ?signature, ?base), 7 VarPointsTo(?baseheap, ?base), 8 InstanceFieldPointsTo(?heap, \n?signature, ?baseheap). 9 10 InstanceFieldPointsTo(?heap, ?signature, ?baseheap) <\u00ad 11 StoreInstanceField(?from, \n?signature, ?base), 12 VarPointsTo(?baseheap, ?base), 13 VarPointsTo(?heap, ?from). Folding Transformation. \nThe idea we used in the above transformation is general. The key novel principle of our op\u00adtimization \nmethodology is that, for highly recursive Data\u00adlog programs (such as our points-to analyses), the primary \ndeterminant of performance is whether the relation deltas produced by semi-naive evaluation bind all \nthe variables needed to index into other relations. In this way, exhaus\u00adtive traversal of non-deltas \nis avoided. To achieve this e.ect, we often need to introduce new indexes. Since in our Dat\u00adalog engine \nan index is always tied to the order of relation variables, to obtain a new index we need to introduce \nnew relations. This is done through applications of the folding program transformation [5]. Folding introduces \na temporary relation that holds the result of intermediate joins. This can improve performance in many \nways. First, it can re-order variables in the intermediate relation and, thus, introduce a new index, \nso that further joins are more e.cient. Second, it can cache intermediate results, implementing the view \nma\u00adterialization database optimization. Third, it can be used to guide the query optimizer to perform \njoins between smaller relations .rst, so as to minimize intermediate results. Fi\u00adnally, it can be used \nto project out unnecessary variables, thus keeping intermediate results small.  Many of these bene.ts \ncan be obtained in our simple pointer analysis program. Consider the 3-way join in lines 11-13 of the \nabove optimized program. Since relation VarPointsTo is recursive and used twice, either of its in\u00adstances \ncan be thought of as a small relation from the perspective of join e.ciency. Speci.cally, under semi-naive \nevaluation, one can think of the above rule (in lines 10-13) as equivalent to the following delta-rewritten \nprogram: .InstanceFieldPointsTo(?heap, ?signature, ?baseheap) <\u00ad StoreInstanceField(?from, ?signature, \n?base), .VarPointsTo(?baseheap, ?base), VarPointsTo(?heap, ?from). .InstanceFieldPointsTo(?heap, ?signature, \n?baseheap) <\u00ad StoreInstanceField(?from, ?signature, ?base), VarPointsTo(?baseheap, ?base), .VarPointsTo(?heap, \n?from). (We elide version numbers, since we are just making an e.ciency point. Note that the deltas are \nalso part of the full relation i.e., they are the deltas from the previous step. Hence, we do not need \na third rule that joins two deltas together.) The .rst rule is fairly e.cient as-is: the delta relation \nbinds variable ?base, which is used to index into rela\u00adtion StoreInstanceField and bind variable ?from, \nwhich is used to index into relation VarPointsTo(?heap, ?from). The second rule, however, would be disastrous \nif executed as-is: none of the large relations has its innermost variable bound by the delta relation. \nWe could improve the perfor\u00admance of the second rule by reordering the variables of StoreInstanceField \nbut there is no way to do so without destroying the performance of the .rst rule. This con.ict can be \nresolved by a fold. We introduce a temporary relation that captures the result of a two-relation join, \nprojects away unnecessary variables, and reorders the remaining variables so that the join with the third \nrelation is highly e.cient. This results in the following optimized pro\u00adgram, with intermediate relation \nStoreHeapInstanceField introduced. 1 VarPointsTo(?heap, ?var) <\u00ad2 AssignHeapAllocation(?heap, ?var). \n3 VarPointsTo(?heap, ?to) <\u00ad4 Assign(?to, ?from), VarPointsTo(?heap, ?from). 5 VarPointsTo(?heap, ?to) \n<\u00ad6 LoadInstanceField(?to, ?signature, ?base), 7 VarPointsTo(?baseheap, ?base), 8 InstanceFieldPointsTo(?heap, \n?signature, ?baseheap). 9 10 InstanceFieldPointsTo(?heap, ?signature, ?baseheap) <\u00ad 11 StoreHeapInstanceField(?baseheap, \n?signature, ?from), 12 VarPointsTo(?heap, ?from). 13 14 StoreHeapInstanceField(?baseheap, ?signature, \n?from) <\u00ad 15 StoreInstanceField(?from, ?signature, ?base), 16 VarPointsTo(?baseheap, ?base). Note that \nthe last two rules only contain relations with the same innermost variables, therefore any delta-execution \nof those rules is e.cient. Implicitly, this is achieved because the folding also adds a new index, for \nthe new intermediate relation. The above program still admits more optimization, as one more ine.cient \njoin remains. Consider the joins in lines 6-8 of the above program. Both relation VarPointsTo and relation \nInstanceFieldsPointsTo are recursively de.ned. (There is direct recursion in VarPointsTo, as well as \nmu\u00adtual recursion between them.) Thus, after the .rst step, their deltas will be joined with the full \nother relations. Speci.\u00adcally, in semi-naive evaluation the above rule (lines 5-8) is roughly equivalent \nto: .VarPointsTo(?heap, ?to) <\u00adLoadInstanceField(?to,?signature,?base), .VarPointsTo(?baseheap,?base), \nInstanceFieldPointsTo(?heap, ?signature, ?baseheap). .VarPointsTo(?heap, ?to) <-LoadInstanceField(?to, \n?signature, ?base), VarPointsTo(?baseheap, ?base), .InstanceFieldPointsTo(?heap, ?signature, ?baseheap). \nAs before, the performance problem is with the second delta rule: the innermost variable of the large \nrelations is not bound by the delta relation. It is tempting to try to eliminate the ine.ciency with \na di.erent variable order, without performing more folds. Indeed, we could optimize the joins in lines \n3-8 without an extra fold, by reordering the variables of VarPointsTo as well as LoadInstanceField the \nlatter so that ?signature is last. This would con.ict with the joins in lines 10-16, however, and would \nrequire further rewrites. Therefore, the ine.ciency can be resolved with a fold, which will also reorder \nvariables so that all joins are highly e.cient: the joined relations always have a common in\u00adnermost \nvariable. We introduce the intermediate relation LoadHeapInstanceField, and get our .nal highly-optimized \nprogram:  1 VarPointsTo(?heap, ?var) <\u00ad2 AssignHeapAllocation(?heap, ?var). 3 VarPointsTo(?heap, ?to) \n<\u00ad4 Assign(?to, ?from), VarPointsTo(?heap, ?from). 5 VarPointsTo(?heap, ?to) <\u00ad6 LoadHeapInstanceField(?to, \n?signature, ?baseheap), 7 InstanceFieldPointsTo(?heap, ?signature, ?baseheap). 8 9 LoadHeapInstanceField(?to, \n?signature, ?baseheap) <\u00ad 10 LoadInstanceField(?to, ?signature, ?base), 11 VarPointsTo(?baseheap, ?base). \n12 13 InstanceFieldPointsTo(?heap, ?signature, ?baseheap) <\u00ad 14 StoreHeapInstanceField(?baseheap, ?signature, \n?from), 15 VarPointsTo(?heap, ?from). 16 17 StoreHeapInstanceField(?baseheap, ?signature, ?from) <\u00ad 18 \nStoreInstanceField(?from, ?signature, ?base), 19 VarPointsTo(?baseheap, ?base). Programmer Insights. \nNote that the above optimization decisions are intuitively appealing, although no intuition was used \nin deriving them. For instance, a programmer with an understanding of the domain will likely prefer this \norder\u00ading of variables in VarPointsTo. (Recall that the innermost variable yields the most important \nindexing with our B-tree ordering.) The relation seems intuitively much more useful when treated as a \nmap of program variables to heap objects, rather than as a map of heap objects to variables that can \npoint to them. Values .ow through variables in a points-to analysis, not through heap objects directly. \nAdditionally, the introduction of temporary relation LoadHeapInstanceField orders the three-way join \nof LoadInstanceField, VarPointsTo, and InstanceFieldPointsTo so that the .rst two relations are joined \n.rst. This is good, since LoadInstanceField is likely smaller than InstanceFieldPointsTo: the former \nis an input relation, with its contents in one-to-one correspondence with a sub\u00ad set of program instructions, \nwhile the latter is inferred from a subset of program instructions joined with the (multiply recursive) \npoints-to relation, resulting in a transitive closure computation. Such insights can sometimes guide \nthe optimization ef\u00adfort, but they are just heuristics. In the end, we have not found dramatic performance \ndi.erences between optimiza\u00adtion paths that both end up with joins that are syntactically e.cient, i.e., \nhave the join variables in innermost positions and always bound by a recursive relation so that its delta \nis used. This syntactic criterion is, more than anything else, the primary determinant of performance. \nImpact. Perhaps surprisingly, the above compact set of op\u00adtimizations and insights are the main source \nof the e.ciency of Doop, compared to a naive Datalog implementation. Ap\u00adplying these optimizations on \na full pointer analysis for re\u00adalistic programs results in improvements of over 3 orders of magnitude: \nrun-time is often dropped from many hours to mere seconds. Furthermore, the optimizations are robust \nwith respect to the di.erent analysis variants supported in Doop. The same optimized trunk of code is \nused for analy\u00adses with several di.erent kinds of context sensitivity.  5. Doop Performance We next \npresent performance experiments for Doop, and especially contrast it with Paddle a BDD-based framework \nthat is state-of-the-art in terms of features and scalability. Because of the variety of experimental \nresults, a roadmap is useful: We .rst evaluate Doop in Paddle-compatibility mode. In this mode, Doop \nresults are precisely equivalent to Paddle. This, however, means that the analysis does not support ex\u00adceptions, \nwhich Doop treats very di.erently. In this mode, Doop is much faster (6.6x to 16.3x in median speedup) \nthan Paddle for standard context-sensitive analyses (1-call-site, 1-call-site+heap, 1-object, 1-object+heap). \n We then compare the full analyses of Doop with the full Paddle. The Doop analyses are not exactly equivalent, \nbut are strictly more precise than their Paddle counterparts. In this full-mode , Doop outperforms Paddle \nby 10x for call-site-sensitive analyses (including heavier ones, such as 1-call-site+heap) scales similarly \nor better than Paddle for even the heaviest object-sensitive analyses in Paddle s experiment set, and \neven handles analyses that Paddle does not, such as a 2-call-site-sensitive and a 2-object-sensitive \nanalysis, both with a context-sensitive heap.  Finally, we discuss the lessons learned from comparing \nan explicit representation approach with a BDD-based one. We see that the performance discrepancy between \nDoop and Paddle is well-explained when one considers the total size of BDDs for the call-graph, var-points-to, \nand .eld\u00adpoints-to relations. The numbers cast doubt on whether BDDs can be the best representation of \nrelations in anal\u00adyses similar to the ones we have considered.  Preliminaries and Experimental Setup. \nWe use a 64-bit machine with two quad-core Xeon 2.33GHz CPUs (only one thread was active at a time, except \nfor Paddle runs, where the Java garbage collector ran on a di.erent thread). The machine has 16GB of \nRAM and 4MB of L2 cache (actually 8MB of L2 cache per CPU, but every 2 cores share 4MB). (For comparison, \nthe Paddle study [18] was conducted on a fairly comparable 4-way 2.6GHz Opteron machine, also with 16GB \nof RAM. Although we do not compare absolute numbers with that study, it is useful for context to know \nthat qualitative scalability estimates are not due to hardware discrepancies.) We analyzed the DaCapo \nbenchmark programs, v.2006-10-MR2, with JDK 1.4 (j2re1.4.2 18), which is much larger than JDK 1.3 used \n(with the same programs) by Lhot\u00b4ak and Hendren [18]. Since recent points-to analysis algorithms (e.g., \n[10, 11]) claim scaling to a million lines of code , we should point out that our benchmarks are the \nlargest in the literature on context-sensitive points-to analysis.  We contacted Paddle s author Ond.rej \nLhot\u00b4 ak to con.rm input parameters for optimal performance (including opti\u00admal BDD variable orderings). \nThe initial settings of the anal\u00adysis are identical to those in the most recent Paddle study [18]. Paddle \ntakes an option for an initial number of BDD nodes to allocate, which can be used to reduce garbage col\u00adlection. \nWe do not use this option for several reasons. 1) This initial number is also the maximum number of nodes, \nwhich requires knowing up-front how complex an analysis will be for a speci.c benchmark. 2) Setting this \nnumber to the maxi\u00admum required value would immediately consume all virtual memory, independent of the \nspeci.c benchmark. For com\u00adparison, we want memory consumption to be proportional to the complexity of \nan analysis. 3) The performance bene\u00ad.t of setting an initial number of BDD nodes was limited in our \nexperiments (less than 10%), and does not change any conclusions. When referring to di.erent analyses \nwe use the pre.xes N-call-site-sensitive , N-call-site , or just N-call for an N-call-site-sensitive \nanalysis, and N-object-sensitive or just N-object for an N-object-sensitive analysis, as well as the \nsu.xes +N-heap or just +NH for an analysis with a context-sensitive heap with N (object or call-site) \ncontexts kept. (We omit the N if it can only be 1.) E.g., 2-call+1H designates a 2-call-site-sensitive \nanalysis with a context\u00adsensitive heap using 1 call-site as context for heap object ab\u00adstractions; 1-call+H \ndesignates a 1-call-site-sensitive anal\u00adysis with a context-sensitive heap (which can only have 1 call-site \nas context). We consider any analysis that takes more than 7200 sec\u00adonds (2 hours) to have failed. The \nsoftware, benchmark scripts, and more statistics are available at http://doop.program-analysis.org/oopsla09. \n 5.1 Paddle-Compatibility We .rst evaluate Doop and Paddle in a mode in which the results are equivalent. \nWe worked hard to ensure semantic equivalence to a high degree. All operations on relations are designed \nto be logically equivalent. That is, all propagation of facts and all intermediate relations are virtually \nidentical. Comparing the results of pointer analyses is challenging because of many minor di.erences \nbetween the analyses. Also, minor di.erences frequently propagate everywhere, making it di.cult to locate \nthe source of an issue. Never\u00adtheless, we achieved exact equivalence of reachable meth\u00adods, reachable \nmethod contexts, context-sensitive call graph edges, instance .eld points-to, static .eld points-to, \nand vari\u00adable points-to information. We compared the results auto\u00admatically and report any di.erences. \nThe various improve\u00adments of Doop over Paddle in support for Java language fea\u00adtures (Section 3.3) have \nbeen patched in Paddle and submit\u00adted as bug reports (e.g., reference object support), or disabled in \nDoop (e.g., re.ection analysis) for this comparison. There is one algorithmic enhancement applied to \nDoop as well as Paddle in Paddle-compatibility mode: Doop treats static initializer methods (clinit) \ncontext-insensitively. Static initializers are not a.ected by any context, so they can be treated context-insensitively \nfor all of the context abstrac\u00adtions we study. (For very di.erent kinds of analyses, e.g., a thread-sensitive \nanalysis, this will not be true.) This en\u00adhancement (as well as other logical enhancements discussed \nlater) is not signi.cant for Paddle (it strictly improves per\u00adformance but only marginally), because \nPaddle can avoid redundancy through its use of BDDs. The enhancement is, however, important for Doop \ns explicit representation of re\u00adlations. Notably, for the experiments in Paddle-compatibility mode, both \nDoop and Paddle ignore control-and data-.ow induced by exceptions. This is necessary, since the Doop \nhandling of exceptions is signi.cantly di.erent from Pad\u00addle s. Figures 4 to 8 display the execution \ntimes of Doop vs. Paddle for .ve analyses, ranging from context-insensitive to 1-object-sensitive+heap. \nAs can be seen, Doop is an order of magnitude faster than Paddle for the context\u00adinsensitive analysis \n(min: 7.4x, max: 10.9x, median: 10x), the 1-call-site-sensitive analysis (min: 7.9x, max: 19.6x, median: \n16.3x), and the 1-object-sensitive analysis (min: 3.2x, max: 18.1x, median: 15.2x). For the heavier analyses, \nDoop is almost always signi.cantly faster than Paddle,ex\u00adcept for the bloat benchmark and a 1-object-sensitive+heap \nanalysis. Speci.cally, Doop exhibits a median speedup of 7.3x for the 1-call-site-sensitive+heap analysis \n(min: 1.8x, max: 9.2x) and a median speedup of 6.6x for the 1-object\u00adsensitive+heap analysis (min: 0.9x, \nmax: 7.3x). (Recall that the latter is the analysis that Lhot\u00b4ak and Hendren considered to require a \nresearch breakthrough to implement e.ciently without BDDs.) The analysis times in seconds illustrate \nthe signi.cance of the speedup: for most programs, analysis time is dropped from several hundreds of \nseconds to just a few tens of sec\u00adonds. Generally, Doop in Paddle-compatibility mode scales very well \neven to much more complex analyses (e.g., 2-object+heap). Nevertheless, recall that the Paddle\u00adcompatibility \nmode does not support Java exception han\u00addling. Adding exception handling in a way that is compati\u00adble \nwith Paddle would arti.cially distort Doop performance. Paddle exception handling is highly imprecise, \ntreating ev\u00adery exception throw as an assignment to a single global vari\u00adable. The variable is then read \nat the site of an exception catch. This approach ignores the information about what exceptions can propagate \nto a catch site: all catch sites be\u00adcome related with all type-compatible throw sites and with each other. \nThis very approximate treatment a.ects the pre\u00adcision of the analysis results but barely a.ects performance \nfor Paddle: the BDD representation of relations tolerates the  redundancy in the computed relations \n(e.g., a higher num\u00ad ber of facts in the call-graph or the var-points-to relations) 250 200 150 100 50 \n0 analysis time (seconds) analysis time (seconds) analysis time (seconds) analysis time (seconds) analysis \ntime (seconds) since the extra facts are highly regular. The BDD represen\u00adtations of relations in Paddle \nare hardly larger, even with signi.cant exception-handling-induced imprecision. In con\u00adtrast, Doop s \nexplicit representation of relations cannot tol\u00aderate the addition of such regular imprecision without \nsuf\u00adfering performance penalties. This phenomenon is perhaps counter-intuitive: Doop performs much better \nwhen impre\u00adcision is avoided, which is also a desirable feature for the Figure 4. (Paddle-compatibility \nmode) context-insensitivequality of the analysis. 1200 1000 800 600 400 200 0 Figure 5. (Paddle-compatibility \nmode) 1-call  5.2 Full Doop Performance and Precision Our main experimental results compare the full \nversion of Doop with the full Paddle, and present detailed statistics on the precision of Doop analyses. \nThe full mode of Doop is not exactly equivalent to the full Paddle, yet the Doop analysis logic is always \nstrictly more precise and more complete, resulting in higher-quality anal\u00adyses. The di.erences are in \nthe more precise and complete handling of re.ection, more precise handling of exceptions, etc. 3000 \n2500 2000 1500 1000 500 0 Figure 6. (Paddle-compatibility mode) 1-call+H Figures 9 to 16 compare the \nperformance of Doop and Paddle. (The analyses presented are a representative selec\u00ad tion for space and \nlayout reasons.) This range of analy\u00adses reproduces the most demanding analyses in Lhot\u00b4 ak and Hendren \ns experiment set [18] and includes analyses that even exceed the capabilities of Paddle: 2-call+1-heap, \n2\u00adobject+1-heap, and 2-call+2-heap. As can be seen, Doop is often signi.cantly faster, especially for \ncall-site-sensitive analyses (e.g., a large speedup for 1-call-site min: 5.0x, max: 12.9x, median: 9.7x \nand for 1-call-site+heap min: 2.3x, max: 16.7x, median: 12.3x). 600 500 400 300 200 100 0 Figure 7. \n(Paddle-compatibility mode) 1-object Doop is not as fast for object-sensitive analyses, but recall that \nit performs a much more precise analysis than Paddle because of its precise exception handling. On-the-.y \nexcep\u00adtion handling results in a dramatic, 2x increase in var points\u00adto precision (i.e., on average each \nvariable is inferred to point to half as many objects) for object-sensitive analyses [3]. Still, Doop \noutperforms Paddle for the vast majority of data points, even for the heaviest analyses in the Paddle \nset. For the 1-object+heap analysis Doop is faster for 8 out of 10 benchmarks (min: 0.4x, max: 4.0x, \nmedian: 3.0x). The only benchmark for which Doop is signi.cantly slower is xalan, but this outlier is \ndue to Paddle s less complete re.ection analysis. Paddle misses a large part of the call graph (only \nreports 3722 reachable methods, instead of 6468 reported by Doop) and analyzes much less code. The signi.cance \nof these results cannot be overstated: The conventional wisdom has been that such analyses can\u00adnot be \nperformed without BDDs. For instance, Lhot\u00b4ak and Hendren write regarding the Paddle study: It is the \nuse of BDDs and the Paddle framework that .nally makes this study possible. Moreover, some of the characteristics \nof the analysis results that we are interested in would be very costly 1600 1400 1200 1000 800 600 400 \n200 0 Figure 8. (Paddle-compatibility mode) 1-object+H  1600 1400 1200 1000  analysis time (seconds) \nanalysis time (seconds) analysis time (seconds) analysis time (seconds) analysis time (seconds) analysis \ntime (seconds) analysis time (seconds) analysis time (seconds) 800 600 400 50 200 0 0 Figure 9. (Full \nmode) context-insensitiveFigure 13. (Full mode) 1-call 1400 7000  6000 5000 4000 3000 2000 200 1000 \n0 0 Figure 10. (Full mode) 1-objectFigure 14. (Full mode) 1-call+H 4500 2500  2000 3000 2500 2000 1500 \n1000 1500 1000 500 500 0 0 Figure 11. (Full mode) 1-object+HFigure 15. (Full mode) 2-call+1H 70001400 \n 6000 5000 4000 3000 2000 1000 Figure 12. (Full mode) 2-object+1H to measure on an explicit representation. \n[18] (Recall also that the Paddle study analyzed the DaCapo benchmarks with the smaller JDK 1.3.1 01.) \nThe last three analyses of our set (2-call+1-heap, 2\u00adobject+1-heap, and 2-call+2-heap) are more precise \nthan any context-sensitive analyses ever reported in the research literature. With a time limit of 2 \nhours, Doop analyzed most of the DaCapo applications under these analyses. All three analyses are impossible \nwith Paddle. The .rst two are not 0 Figure 16. (Full mode) 2-call+2H supported by the Paddle framework, \nwhile the third is too heavy to run. (In our tests, the analysis times-out even for the smallest of the \nDaCapo benchmarks. Lhot\u00b4ak also reports that [He] never managed to get Paddle to run in available memory \nwith these settings .2) The range of Doop-supported analyses allows us to obtain insights regarding analysis \nprecision. Figure 17 shows some 2 http://www.sable.mcgill.ca/pipermail/soot-list/2006-March/000601.html \n of the most important statistics on our analyses results for representative programs. Perhaps the most \ninformative met\u00adric is the average points-to set size for plain program vari\u00adables.3 The precision observations \nare very similar to those in the Paddle study: object-sensitivity is very good for en\u00adsuring points-to \nprecision, and a context-sensitive heap can only serve to signi.cantly enhance the quality of results. \nWe can immediately see the value of our highly precise analyses, and especially the combination of a \n2-object-sensitive anal\u00adysis with a context-sensitive heap. This most precise analy\u00adsis typically drops \nthe average points-to set size to one-tenth of the size of the least precise (context insensitive) analy\u00adsis. \nRemarkably, this even impacts the number of call-graph edges a metric that notoriously improves very \nlittle with increasing the precision of the points-to analysis. In future work we expect to conduct a \nthorough evaluation of the pre\u00adcision of a wide range of analyses for several end-user met\u00adrics.  5.3 \nBDDs vs. Explicit Representation Generally, the performance di.erences between Doop and Paddle are largely \nattributable to the use of BDDs vs. an explicit representation of relation contents. The comparison of \nthe two systems reveals interesting lessons regarding the representation of relations in points-to analysis. \nBDDs are a maximally reduced data structure (for a given variable ordering) so they naturally trade o. \nsome time for space, at least for large relations. Furthermore, BDDs have heavy overheads, in the case \nof irregular relations that cannot be reduced. Consider the worst-case scenario for BDDs: a relation \nwith a single tuple. The BDD representation in Pad\u00addle uses a node per bit e.g., the single tuple in \na relation over a 48-bit variable space will be represented by 48 BDD nodes. Each node in the BuDDy library \n(used by Paddle) is 20 bytes, or 160 bits. This represents a space overhead of 160x, but it also represents \na time overhead, since what would be a very quick operation in an explicit tuple repre\u00adsentation now \nrequires traversing 48 heap objects (allocated in a single large region, but with no structure-locality). \nThe di.culty in analyzing the trade-o. is that results on smaller data sets and operations do not translate \nto larger ones. For instance, we tried a simple experiment to com\u00adpare the join performance of Doop and \nPaddle, without any other recursion or iteration. We read into memory two pre\u00adviously computed points-to \nanalysis relations (including the VarPointsTo relation, for which Paddle s BDD variable or\u00adder is highly \noptimized) and computed their join. The fully expanded relation size in Doop was a little over 1GB, or \n7 million tuples. Doop performed the join in 24.4 seconds. 3 Note the apparent paradox of having the \naverage number of var-points-to facts often be higher when computed over context-sensitive variables \nthan over plain variables. Although each context-sensitive variable has fewer points-to facts than its \ncontext-insensitive version, the average over all context-sensitive variables can be higher: program \nvariables that have many points-to facts are also used in many more contexts, skewing the results. analysis \nnodes edges var points-to insens 4510 24K 2.8M 67 - - 1-call 4498 24K 897K 22 4.9M 31 antlr 1-call+H \n2-call+1H 4495 4484 24K 23K 887K 719K 22 18 14M 48M 90 84 2-call+2H 4451 23K 570K 14 79M 171 1-obj 4486 \n24K 748K 18 4.7M 16 1-obj+H 4435 23K 435K 11 25M 86 2-obj+1H 4382 22K 264K 7 7.8M 8 chart insens 1-call \n1-call+H 2-call+1H 2-call+2H 1-obj 1-obj+H 2-obj+1H 7873 7820 7816 7800 \u00d7 7803 7676 7570 41K 40K 40K \n40K \u00d7 40K 37K 35K 5.9M 2.6M 2.5M 2.2M \u00d7 2.4M 1.2M 414K 84 36 36 31 \u00d7 34 17 6 -18M 43M 202M \u00d7 18M 81M \n24M -66 162 173 \u00d7 27 123 7 insens 5536 27K 3.5M 73 - - 1-call 5519 26K 1.1M 22 5.8M 31 pmd 1-call+H 2-call+1H \n5516 5506 26K 26K 1.0M 925K 22 20 16M 65M 89 94 2-call+2H 5473 25K 803K 17 136M 219 1-obj 5504 26K 964K \n21 5.2M 15 1-obj+H 5440 25K 682K 15 25M 77 2-obj+1H 5372 24K 302K 7 7.4M 7 xalan insens 1-call 1-call+H \n2-call+1H 2-call+2H 1-obj 1-obj+H 2-obj+1H 6580 6568 6565 6551 6505 6549 6468 \u00d7 33K 33K 33K 32K 32K 33K \n31K \u00d7 3.4M 1.4M 1.4M 1.2M 939K 1.2M 696K \u00d7 62 25 25 22 17 22 13 \u00d7 -7.5M 22M 78M 125M 19M 106M \u00d7 -35 104 \n88 170 30 173 \u00d7 Figure 17. Precision statistics of Doop analyses for a subset of the DaCapo benchmarks. \nThe columns show call-graph nodes and edges, as well as total and average (per variable) points-to facts, \n.rst for plain program variables and then for context-sensitive variables (i.e., context-variable tuples). \n Paddle spent 40x more time, 957 seconds, creating the BDD, but then performed the join in just 0.527 \nseconds. In terms of space, the BDD representation of the 7 million tuples con\u00adsisted of just 148.7 thousand \nnodes less than 3MB of mem\u00adory! This demonstrates how di.erent the cost model is for the two systems. \nIf Paddle can exploit regularity and build a new BDD through e.cient operations on older ones, then its \nperformance is unparalleled. Creating the BDD, however, can often be extremely time consuming. Furthermore, \na sin\u00adgle non-reducible relation can become a bottleneck for the whole system. Thus, it is hard to translate \nthe results of mi\u00adcrobenchmarks to more complex settings, as the complexity of BDDs depends on their \nredundancy. To gain a better understanding of performance, we ana\u00adlyzed the sizes of BDDs in Paddle for \nsome major relations in its analyses, relative to the size of the explicit representa\u00adtions of the same \nrelations. Figure 18 shows the sizes of rela\u00adtions nodes (representing the context-sensitive call-graph \nnodes, i.e., context-quali.ed reachable methods), edges (i.e., context-sensitive call-graph edges), var \npoints-to (the main points-to relation, for context-quali.ed vars), and .eld points-to (the points-to \nrelation for object .elds). For each relation, the table shows the size of its explicit represen\u00adtation \n(measured in number of rows i.e., number of total facts in the relation), the size of the BDD representation \n(in number of BDD nodes) and the ratio of these two numbers although they are in di.erent units the variation \nof the ratios is highly informative.  The above numbers are for Paddle as con.gured for our Paddle-compatibility \nexperiments, so that the BDD statis\u00adtics can be directly correlated to the performance of Doop (explicit \nrepresentation) vs. Paddle (BDDs). Examination of the table in comparison with Figures 4-8 reveals that \nthe per\u00adformance of Paddle relative to Doop is highly correlated with the overall e.ectiveness of BDDs \nfor relation representation. For benchmarks and analyses for which Paddle performs better compared to \nDoop, we .nd that all four relations (or at least the largest ones, if their size dominates the sizes \nof oth\u00aders) exhibit a much lower ratio of BDD-nodes-to-facts than in other benchmarks or analyses. Consider, \nfor instance, the 1-object+heap analysis. The BDD size statistics reveal that bloat and jython are signi.cant \noutliers compared to the rest of the DaCapo applications: their BDD-nodes-to-facts ratios are much lower \nfor all large relations. A quick com\u00adparison with Figure 8 reveals that Paddle performs unusually well \nfor these two benchmarks. This understanding of the performance model for the BDD-based representation \nleads to further insights. The ul\u00adtimate question we want to answer is whether (and under what conditions) \nthere is enough regularity in relations in\u00advolved in points-to analyses for BDDs to be the best rep\u00adresentation \nchoice. Figure 18 suggests that this is not the case, at least for the analyses studied here. The main \nway to improve the performance of the BDD representation is by changing the BDD variable ordering. The \nBDD variable or\u00addering used in our Paddle experiments is one that minimizes the size of the var points-to \nrelation (which, indeed, consis\u00adtently has a small BDD-nodes-to-facts ratio in Figure 18). This order \nwas observed by Lhot\u00b4ak to yield the best results in terms of performance. (It is worth noting that the \nPad\u00addle authors were among the .rst to use BDDs in program analysis, have a long history of experimentation \nin multiple successive systems, and have experimented extensively with BDD variable orderings until deriving \nones that yield im\u00adpressive results [2].) Nevertheless, what we see in Figure 18 is that it is very hard \nto provide a variable ordering that min\u00adimizes all crucial BDDs. Although the var points-to relation \nis consistently small, the (context-sensitive) call-graph edge relation is ine.cient and it is usually \nlarge enough to matter. All current techniques utilizing BDDs for points-to analy\u00adsis (e.g., in bddbddb \nor Paddle) require BDD variable order\u00adings that are simultaneously good for the many BDDs in a system \nof interrelated analyses [15]. It does not, therefore, seem likely that BDDs will be the best representation \noption for precise context-sensitive points-to analyses without sig\u00adni.cant progress in our understanding \nof how BDDs can be employed.  6. Related and Future Work Fast and Precise Pointer Analysis. There is \nan immense body of work on pointer analysis, so we need to restrict our discussion to some representative \nand recent work. Fast and precise pointer analysis is, unfortunately, still a trade\u00ado.. This is unlikely \nto change. Most recent work in pointer analysis explores methods to improve performance by re\u00adducing \nprecision strategically. The challenge is to limit the loss of precision, yet gain considerably in performance. \nFor instance, Lattner et al. show [14] that an analysis with a context-sensitive heap abstraction can \nbe very e.cient by sacri.cing precision using uni.cation constraints. This is a common sacri.ce. Furthermore, \nthere are still considerable improvements possible in solving the constraints of the clas\u00adsic inclusion-based \npointer analysis of Andersen, as illus\u00adtrated by Hardekopf and Lin [10]. In full context-sensitive pointer \nanalysis, there is an on\u00adgoing search for context abstractions that provide precise pointer information, \nand do not cause massive redundant computation. Milanova suggested that an object-sensitive analysis \n[20] is an e.ective context abstraction for object\u00adoriented programs, which was con.rmed by Lhot\u00b4ak s \nexten\u00adsive evaluation [18]. Several researchers have argued for the bene.ts of using a context-sensitive \nheap abstraction to im\u00adprove precision [18,22]. The use of BDDs attempts to solve the problem of the \nlarge amount of data in context-sensitive pointer analysis by representing its redundancy e.ciently [2, \n29]. The redun\u00addancy should ideally be eliminated by choosing the right context abstraction. Xu and Rountev \ns recent work [30] ad\u00addresses this problem. Their method aims to determine con\u00adtext abstractions that \nwill yield the same points-to informa\u00adtion. This is an exciting research direction, orthogonal to our \nwork on declarative speci.cations and optimization. How\u00adever, in their speci.c implementation, memory \nconsumption is growing quickly for bigger benchmarks, even on Java 1.3. IBM Research s Wala [7] static \nanalysis library is de\u00adsigned to support di.erent pointer analysis con.gurations, but no results of Wala \ns accuracy or speed have been re\u00adported in the literature. It will be interesting to compare our analyses \nto Wala in future work. Re.ection and Program Analysis. Re.ection, dynamic class loading, and native \nmethods are a major issue for static program analysis. Paddle inherits support for many native methods \nfrom its predecessor, Spark [16]. Paddle s support for re.ection is relatively unsophisticated compared \nto the re.ection analysis of Livshits speci.ed in Datalog on top  facts call-g nbdd raph odes ratio \ncall-graph edges facts bdd ratio var points-to facts bdd ratio .eld points-to facts bdd ratio antlr 4K \n1K 0.35 23K 95K 4.23 2.0M 58K 0.03 766K 28K 0.04 context-insensitive bloat chart eclipse hsqldb jython \nluindex lusearch 6K 8K 5K 4K 6K 4K 4K 2K 3K 2K 1K 2K 1K 2K 0.26 0.35 0.34 0.41 0.31 0.38 0.34 46K 132K \n39K 163K 24K 104K 17K 80K 32K 123K 18K 86K 21K 98K 2.86 4.19 4.39 4.71 3.90 4.70 4.65 7.9M 81K 0.01 5.3M \n101K 0.02 2.4M 63K 0.03 1.5M 50K 0.03 3.3M 72K 0.02 1.5M 53K 0.03 1.8M 59K 0.03 1.0M 1.8M 746K 493K 750K \n567K 606K 38K 0.04 51K 0.03 31K 0.04 23K 0.05 34K 0.04 25K 0.04 28K 0.05 pmd 5K 2K 0.32 25K 113K 4.51 \n2.5M 62K 0.02 652K 28K 0.04 xalan 4K 1K 0.40 17K 80K 4.78 1.4M 50K 0.04 501K 23K 0.05 1-call-site-sensitive \nantlr bloat chart eclipse hsqldb jython luindex lusearch pmd xalan 22K 45K 39K 23K 17K 31K 18K 21K 25K \n17K 37K 55K 64K 38K 29K 47K 31K 36K 42K 29K 1.64 1.21 1.67 1.64 1.73 1.51 1.73 1.69 1.69 1.74 83K 682K \n266K 1.1M 164K 1.2M 113K 705K 61K 523K 139K 907K 65K 559K 76K 638K 94K 769K 60K 519K 8.26 4.32 7.09 6.22 \n8.62 6.53 8.63 8.41 8.14 8.64 2.9M 735K 0.26 30M 1.5M 0.05 18M 1.6M 0.09 4.0M 852K 0.21 2.1M 590K 0.28 \n5.7M 1.0M 0.18 2.4M 645K 0.27 2.9M 751K 0.26 4.7M 843K 0.18 2.1M 595K 0.29 636K 792K 1.4M 572K 395K 539K \n459K 488K 512K 396K 28K 0.04 39K 0.05 52K 0.04 32K 0.06 24K 0.06 35K 0.06 26K 0.06 29K 0.06 29K 0.06 \n24K 0.06 1-call-site-sensitive+heap antlr bloat chart eclipse hsqldb jython luindex lusearch pmd xalan \n22K 45K 39K 23K 17K 31K 18K 21K 25K 17K 37K 55K 64K 38K 29K 47K 31K 36K 42K 29K 1.63 1.22 1.66 1.64 1.73 \n1.50 1.73 1.70 1.69 1.74 83K 682K 251K 1.1M 164K 1.2M 113K 706K 61K 523K 139K 908K 65K 560K 76K 637K \n94K 768K 60K 518K 8.26 4.55 7.11 6.23 8.61 6.54 8.63 8.40 8.13 8.64 8.9M 2.4M 0.27 159M 7.3M 0.05 42M \n6.3M 0.15 14M 3.1M 0.23 6.2M 1.8M 0.30 22M 4.2M 0.19 7.0M 2.1M 0.30 8.5M 2.5M 0.30 14M 3.1M 0.22 6.1M \n1.8M 0.30 12M 27M 26M 9.4M 5.7M 15M 6.4M 7.8M 8.2M 5.7M 7.3M 0.59 10M 0.38 16M 0.63 7.1M 0.75 4.3M 0.76 \n8.6M 0.58 5.0M 0.78 5.7M 0.74 6.7M 0.82 4.3M 0.77 antlr 36K 19K 0.54 218K 489K 2.25 1.5M 324K 0.22 25K \n33K 1.33 bloat 71K 27K 0.38 1.8M 1.2M 0.65 14M 646K 0.05 307K 44K 0.14 1-object-sensitive chart eclipse \nhsqldb jython luindex lusearch 81K 40K 31K 64K 32K 35K 38K 22K 17K 26K 18K 20K 0.47 0.55 0.55 0.40 0.57 \n0.57 1.0M 1.1M 312K 596K 170K 412K 746K 742K 178K 436K 202K 492K 1.14 1.91 2.43 0.99 2.44 2.43 16M 763K \n0.05 1.9M 381K 0.20 1.1M 271K 0.25 4.9M 455K 0.09 1.2M 294K 0.24 1.5M 335K 0.23 60K 27K 17K 38K 18K 20K \n58K 0.97 36K 1.33 28K 1.69 39K 1.02 30K 1.73 34K 1.71 pmd 42K 21K 0.50 309K 557K 1.80 2.6M 373K 0.14 \n40K 34K 0.85 xalan 30K 17K 0.56 168K 411K 2.45 1.1M 274K 0.25 16K 28K 1.73 1-object-sensitive+heap antlr \nbloat chart eclipse hsqldb jython luindex lusearch pmd xalan 35K 69K 76K 39K 30K 62K 31K 34K 41K 30K \n19K 27K 37K 22K 17K 25K 18K 20K 21K 17K 0.55 0.39 0.49 0.56 0.56 0.41 0.58 0.58 0.52 0.57 161K 448K 1.4M \n1.0M 647K 973K 212K 544K 131K 380K 638K 684K 134K 402K 147K 447K 216K 499K 129K 379K 2.79 0.73 1.50 2.56 \n2.90 1.07 2.99 3.04 2.31 2.93 8.6M 797K 0.09 56M 1.9M 0.03 41M 1.9M 0.05 11M 1.0M 0.10 6.3M 656K 0.10 \n76M 1.4M 0.02 6.4M 695K 0.11 7.3M 785K 0.11 10M 892K 0.09 6.0M 665K 0.11 2.3M 13M 9.1M 2.8M 1.7M 15M \n1.7M 1.8M 2.8M 1.5M 505K 0.22 1.2M 0.09 1.3M 0.14 631K 0.23 409K 0.24 1.1M 0.07 427K 0.26 488K 0.26 539K \n0.19 411K 0.27 Figure 18. BDD statistics for the most important context-sensitive relations of Paddle: \ntotal number of facts in the context\u00adsensitive relation, number of BDD nodes used to represent those \nfacts, and the ratio of BDD nodes / total number of facts.  of Whaley s bddbddb [19]. In particular, \nPaddle does not maintain information about Class objects created through Class.forName, which requires \nvery conservative assump\u00adtions about later Class.newInstance invocations. However, the re.ection analysis \nof Livshits was only integrated in a context-insensitive pointer analysis. The fully declarative nature \nof Doop allows us to use very similar Datalog rules also in context-sensitive analyses. Declarative Programming \nAnalysis. Program analysis us\u00ading logic programming has a long history (e.g., [4, 23]), but this early \nwork only considers very small programs. In re\u00adcent years, there have been e.orts to apply declarative \npro\u00adgram analysis to much larger codebases and more complex analysis problems. We discussed the relation \nto Whaley s work on context-sensitive pointer analysis using Datalog and BDDs [29] throughout this paper. \nThe Dimple [1] analysis framework has shown to be competitive in performance for context-insensitive \npointer analysis using tabled Prolog. The demonstrated pointer analysis of Dimple uses a conservative, \npre-computed call graph, so the analysis is reduced to prop\u00adagation of points-to information of assignments, \nwhich can be very e.cient. Doop expresses all the logic of a context\u00adsensitive pointer analysis in Datalog. \nDemand-Driven and Incremental Analysis. A demand\u00addriven evaluation strategy reduces the cost of an analysis \nby computing only those results that are necessary for a client program analysis [12, 26, 27, 31]. This \nis a useful approach for client analyses that focus on speci.c locations in a program, but if the client \nneeds results from the entire program, then demand-driven analysis is typically slower than an exhaustive \npointer analysis. Reps [24] showed how to use the standard magic-sets optimization to automatically derive \na demand-driven analysis from an exhaustive analysis (like ours). This optimization combines the bene.ts \nof top\u00addown and bottom-up evaluation of logic programs by adding side-conditions to rules that limit \nthe computation to just the required data. More recently, Saha and Ramakrishnan [25] explored the application \nof incremental logic program evaluation strate\u00adgies to context-insensitive pointer analysis. As pointed \nout in this work, the algorithms for materialized view mainte\u00adnance and incremental program analysis \nare highly related. As we discussed, incremental evaluation is also crucial for Doop s performance. The \nlarge number of reachable meth\u00adods in an empty Java program4 suggests that incremental analysis could \nbring down the from-scratch evaluation time substantially. We have not explored these incremental eval\u00aduation \nscenarios yet. The engine we use also supports in\u00adcremental evaluation after deletion and updates of \nfacts us\u00ad 4 Even an empty Java program causes the execution of a number of methods from the standard \nlibrary. This causes a static analysis to compute an even larger number of reachable methods, especially \nwhen no assumptions are made about the loading environment (e.g., security settings and where the empty \nclass will be loaded from). ing the DRed [8] algorithm. E.cient incremental evaluation might make context-sensitive \npointer analysis suitable for use in IDEs.  7. Conclusions We presented Doop: a purely declarative points-to \nanalysis framework that raises the bar for precise context-sensitive analyses. Doop is elegant, full-featured, \nmodular, and high\u00adlevel, yet achieves remarkable performance due to a novel optimization methodology \nfocused on highly recursive Dat\u00adalog programs. Doop uses an explicit representation of re\u00adlations and \ncha(lle)nges the community s understanding on how to implement e.cient points-to analyses. Acknowledgments \nThis work was funded by the NSF (CCF-0917774, CCF-0934631) and by LogicBlox Inc. We thank Ond.rej Lhot\u00b4 \nak for his advice on benchmarking Pad\u00addle, Oege de Moor and Molham Aref for useful discus\u00adsions, the \nanonymous reviewers for helpful comments, and the LogicBlox developers for their practical help and sup\u00adport. \n References [1] W. C. Benton and C. N. Fischer. Interactive, scalable, declar\u00adative program analysis: \nfrom prototype to implementation. In PPDP 07: Proc. of the 9th ACM SIGPLAN int. conf. on Principles and \npractice of declarative programming, pages 13 24, New York, NY, USA, 2007. ACM. [2] M. Berndl, O. Lhot\u00b4ak, \nF. Qian, L. J. Hendren, and N. Umanee. Points-to analysis using bdds. In PLDI, pages 103 114. ACM, 2003. \n[3] M. Bravenboer and Y. Smaragdakis. Exception analysis and points-to analysis: Better together. In \nL. Dillon, editor, ISSTA 09: Proceedings of the 2009 International Symposium on Software Testing and \nAnalysis, New York, NY, USA, July 2009. To appear. [4] S. Dawson, C. R. Ramakrishnan, and D. S. Warren. \nPractical program analysis using general purpose logic programming systems a case study. In PLDI 96: \nProc. of the ACM SIGPLAN 1996 conf. on Programming language design and implementation, pages 117 126, \nNew York, NY, USA, 1996. ACM. [5] S. K. Debray. Unfold/fold transformations and loop optimization of \nlogic programs. In PLDI 88: Proc. of the ACM SIGPLAN 1988 conf. on Programming Language design and Implementation, \npages 297 307, New York, NY, USA, 1988. ACM. [6] M. Eichberg, S. Kloppenburg, K. Klose, and M. Mezini. \nDe.ning and continuous checking of structural program dependencies. In ICSE 08: Proc. of the 30th int. \nconf. on Software engineering, pages 391 400, New York, NY, USA, 2008. ACM. [7] S. J. Fink. T.J. Watson \nlibraries for analysis (WALA). http://wala.sourceforge.net.  [8] A. Gupta, I. S. Mumick, and V. S. Subrahmanian. \nMain\u00adtaining views incrementally. In SIGMOD 93: Proc. of the 1993 ACM SIGMOD int. conf. on Management \nof data, pages 157 166, New York, NY, USA, 1993. ACM. [9] E. Hajiyev, M. Verbaere, and O. de Moor. Codequest: \nScalable source code queries with datalog. In Proc. European Conf. on Object-Oriented Programming (ECOOP), \npages 2 27. Spinger, 2006. [10] B. Hardekopf and C. Lin. The ant and the grasshopper: fast and accurate \npointer analysis for millions of lines of code. In PLDI 07: Proc. ACM SIGPLAN conf. on Programming Language \nDesign and Implementation, pages 290 299, New York, NY, USA, 2007. ACM. [11] B. Hardekopf and C. Lin. \nSemi-sparse .ow-sensitive pointer analysis. In POPL 09: Proceedings of the 36th annual ACM SIGPLAN-SIGACT \nsymposium on Principles of programming languages, pages 226 238, New York, NY, USA, 2009. ACM. [12] N. \nHeintze and O. Tardieu. Demand-driven pointer analysis. In PLDI 01: Proc. of the ACM SIGPLAN 2001 conf. \non Programming language design and implementation, pages 24 34, New York, NY, USA, 2001. ACM. [13] M. \nS. Lam, J. Whaley, V. B. Livshits, M. C. Martin, D. Avots, M. Carbin, and C. Unkel. Context-sensitive \nprogram analysis as database queries. In PODS 05: Proc. of the twenty-fourth ACM SIGMOD-SIGACT-SIGART \nsymposium on Principles of database systems, pages 1 12, New York, NY, USA, 2005. ACM. [14] C. Lattner, \nA. Lenharth, and V. Adve. Making context\u00adsensitive points-to analysis with heap cloning practical for \nthe real world. SIGPLAN Not., 42(6):278 289, 2007. [15] O. Lhot\u00b4Program Analysis using Binary Decision \nak. Diagrams. PhD thesis, McGill University, Jan. 2006. [16] O. Lhot\u00b4ak and L. Hendren. Scaling Java \npoints-to analysis using Spark. In G. Hedin, editor, Compiler Construction, 12th Int. Conf., volume 2622 \nof LNCS, pages 153 169, Warsaw, Poland, April 2003. Springer. [17] O. Lhot\u00b4ak and L. Hendren. Jedd: a \nbdd-based relational ex\u00adtension of java. In PLDI 04: Proc. of the ACM SIGPLAN 2004 conf. on Programming \nlanguage design and implemen\u00adtation, pages 158 169, New York, NY, USA, 2004. ACM. [18] O. Lhot\u00b4ak and \nL. Hendren. Evaluating the bene.ts of context-sensitive points-to analysis using a BDD-based implementation. \nACM Trans. Softw. Eng. Methodol., 18(1):1 53, 2008. [19] B. Livshits, J. Whaley, and M. S. Lam. Re.ection \nanalysis for Java. In K. Yi, editor, Proceedings of the 3rd Asian Symposium on Programming Languages \nand Systems, volume 3780. Springer-Verlag, Nov. 2005. [20] A. Milanova, A. Rountev, and B. G. Ryder. \nParameterized object sensitivity for points-to analysis for java. ACM Trans. Softw. Eng. Methodol., 14(1):1 \n41, 2005. [21] M. Naik, A. Aiken, and J. Whaley. E.ective static race detection for java. In Proceedings \nof the 2006 ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI 06), pages \n308 319, 2006. [22] E. M. Nystrom, H.-S. Kim, and W. mei W. Hwu. Importance of heap specialization in \npointer analysis. In PASTE 04: Proc. of the 5th ACM SIGPLAN-SIGSOFT workshop on Program analysis for \nsoftware tools and engineering, pages 43 48, New York, NY, USA, 2004. ACM. [23] T. Reps. Demand interprocedural \nprogram analysis using logic databases. In R. Ramakrishnan, editor, Applications of Logic Databases, \npages 163 196. Kluwer Academic Publishers, 1994. [24] T. W. Reps. Solving demand versions of interprocedural \nanalysis problems. In CC 94: Proc. of the 5th Int. Conf. on Compiler Construction, pages 389 403, London, \nUK, 1994. Springer-Verlag. [25] D. Saha and C. R. Ramakrishnan. Incremental and demand\u00addriven points-to \nanalysis using logic programming. In PPDP 05: Proc. of the 7th ACM SIGPLAN int. conf. on Principles and \npractice of declarative programming, pages 117 128, New York, NY, USA, 2005. ACM. [26] M. Sridharan and \nR. Bod\u00b4ik. Re.nement-based context\u00adsensitive points-to analysis for java. In PLDI 06: Proc. of the 2006 \nACM SIGPLAN conf. on Programming language design and implementation, pages 387 400, New York, NY, USA, \n2006. ACM. [27] M. Sridharan, D. Gopan, L. Shan, and R. Bod\u00b4ik. Demand\u00addriven points-to analysis for \njava. In OOPSLA 05: Proc. of the 20th annual ACM SIGPLAN conf. on Object oriented programming, systems, \nlanguages, and applications, pages 59 76, New York, NY, USA, 2005. ACM. [28] J. Whaley, D. Avots, M. \nCarbin, and M. S. Lam. Using datalog with binary decision diagrams for program analysis. In K. Yi, editor, \nAPLAS, volume 3780 of Lecture Notes in Computer Science, pages 97 118. Springer, 2005. [29] J. Whaley \nand M. S. Lam. Cloning-based context-sensitive pointer alias analysis using binary decision diagrams. \nIn PLDI 04: Proc. of the ACM SIGPLAN 2004 conf. on Programming language design and implementation, pages \n131 144, New York, NY, USA, 2004. ACM. [30] G. Xu and A. Rountev. Merging equivalent contexts for scalable \nheap-cloning-based context-sensitive points-to analysis. In ISSTA 08: Proc. of the 2008 int. symposium \non Software testing and analysis, pages 225 236, New York, NY, USA, 2008. ACM. [31] X. Zheng and R. Rugina. \nDemand-driven alias analysis for c. In POPL 08: Proc. of the 35th annual ACM SIGPLAN-SIGACT symposium \non Principles of programming languages, pages 197 208, New York, NY, USA, 2008. ACM.  \n\t\t\t", "proc_id": "1640089", "abstract": "<p>We present the DOOP framework for points-to analysis of Java programs. DOOP builds on the idea of specifying pointer analysis algorithms declaratively, using Datalog: a logic-based language for defining (recursive) relations. We carry the declarative approach further than past work by describing the full end-to-end analysis in Datalog and optimizing aggressively using a novel technique specifically targeting highly recursive Datalog programs.</p> <p>As a result, DOOP achieves several benefits, including full order-of-magnitude improvements in runtime. We compare DOOP with Lhotak and Hendren's PADDLE, which defines the state of the art for context-sensitive analyses. For the exact same logical points-to definitions (and, consequently, identical precision) DOOP is more than 15x faster than PADDLE for a 1-call-site sensitive analysis of the DaCapo benchmarks, with lower but still substantial speedups for other important analyses. Additionally, DOOP scales to very precise analyses that are impossible with PADDLE and Whaley et al.'s bddbddb, directly addressing open problems in past literature. Finally, our implementation is modular and can be easily configured to analyses with a wide range of characteristics, largely due to its declarativeness.</p>", "authors": [{"name": "Martin Bravenboer", "author_profile_id": "81100378172", "affiliation": "University of Massachusetts, Amherst, Amherst, MA, USA", "person_id": "P1728769", "email_address": "", "orcid_id": ""}, {"name": "Yannis Smaragdakis", "author_profile_id": "81100614708", "affiliation": "University of Massachusetts, Amherst, Amherst, MA, USA", "person_id": "P1728770", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1640089.1640108", "year": "2009", "article_id": "1640108", "conference": "OOPSLA", "title": "Strictly declarative specification of sophisticated points-to analyses", "url": "http://dl.acm.org/citation.cfm?id=1640108"}