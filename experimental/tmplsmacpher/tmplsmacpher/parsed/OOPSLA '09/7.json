{"article_publication_date": "10-25-2009", "fulltext": "\n How a Java VM Can Get More from a Hardware Performance Monitor Hiroshi Inoue and Toshio Nakatani IBM \nTokyo Research Laboratory 1623-14, Shimo-tsuruma, Yamato-shi, Kanagawa-ken, 242-8502, Japan {inouehrs, \nnakatani}@jp.ibm.com Abstract This paper describes our sampling-based profiler that exploits a processor \ns HPM (Hardware Performance Monitor) to collect information on running Java applications for use by the \nJava VM. Our profiler provides two novel features: Java-level event profiling and lightweight context-sensitive \nevent profiling. For Java events, we propose new techniques to leverage the sampling facility of the \nHPM to generate object creation profiles and lock activity profiles. The HPM sampling is the key to achieve \na smaller overhead compared to profilers that do not rely on hardware helps. To sample the object creations \nwith the HPM, which can only sample hardware events such as executed instructions or cache misses, we \ncorrelate the object creations with the store instructions for Java object headers. For the lock activity \nprofile, we introduce an instrumentation-based technique, called ProbeNOP, which uses a special NOP instruction \nwhose executions are counted by the HPM. For the context\u00adsensitive event profiling, we propose a new \ntechnique called CallerChaining, which detects the calling context of HPM events based on the call stack \ndepth (the value of the stack frame pointer). We show that it can detect the calling contexts in many \nprograms including a large commercial application. Our proposed techniques enable both programmers and \nruntime systems to get more valuable information from the HPM to understand and optimize the programs \nwithout adding significant runtime overhead. Categories and Subject Descriptors. D.3.4 [Programming Languages]: \nProcessors Run-time environments Permission to make digital or hard copies of all or part of this work \nfor personal or classroom use is granted without fee provided that copies are not made or distributed \nfor profit or commercial advantage and that copies bear this notice and the full citation on the first \npage. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior \nspecific permission and/or a fee. OOPSLA 2009, October 25-29, 2009, Orlando, Florida, USA. Copyright \n&#38;#169; 2009 ACM 978-1-60558-734-9/09/10 $10.00. General Terms. Measurement, Performance Keywords. \nHardware Performance Monitor; Profiling; Calling Context 1. Introduction Many modern high-performance \nprocessors have an HPM (Hardware Performance Monitor) to count performance\u00adrelated hardware events and \nto sample events at specified sampling intervals. Such hardware events include executed instructions, \ncache misses (at each level of a memory hierarchy), and branch mispredictions. Many profilers are capable \nof using an HPM to provide programmers with profiles of hardware events. In this paper, we introduce \nnew techniques in our profiler that extend the scope of HPM-based profilers in two ways. First, our profiler \ncan capture Java-level events, such as object creation or lock activities, by correlating them with the \nhardware events directly supported by the HPM. Second, we make it possible to detect calling context \nin addition to the program location (method name and instruction address) for each HPM event with minimal \nadditional runtime overhead in many applications. As examples of Java-level event profiling, here we \nstudy object creation profiling and lock activity profiling. We show how our profiler can derive object \ncreation profiles, including information on both allocated objects and allocation sites, from the store \ninstruction profiles collected by the HPM. For lock activity profiling, which cannot be easily derived \nfrom hardware events, we propose a new instrumentation-based technique, called ProbeNOP. It uses a special \nNOP instruction, the ProbeNOP instruction, which does not affect the program execution, but whose executions \nare counted by the HPM. To correlate a piece of code with a hardware event, the JIT compiler generates \na ProbeNOP instruction in the code of interest, such as a lock acquisition code sequence. It also encodes \ninformation on which register or memory location to profile at the location of the ProbeNOP instruction \nwithin the unused bits of the ProbeNOP instruction. The handler for the HPM interrupt decodes the information \nencoded in the ProbeNOP instruction and collects the values of the specified targets. Our ProbeNOP technique \nmakes it possible to leverage the sampling facility of the HPM for value profiling in the JVM with low \nprofiling overhead.  Another new feature in our profiler is an efficient context detection technique, \nCallerChaining. It detects the calling context for the HPM event based on the call stack depth, calculated \nfrom the value in the stack frame pointer. It does not incur significant additional runtime overhead \ncompared to profiling that is not aware of the context of the events. In our CallerChaining, we first \ncollect quadruples of {caller method, caller call stack depth, callee method, callee call stack depth} \nusing the HPM. We construct a CCT (Calling Context Tree) [1] with a call stack depth in each CCT node \nby chaining pairs of a caller and callee that have the same call stack depth. We use the call stack depth \nas a hint to distinguish among the calling contexts that include the same method. While profiling the \nHPM events, we capture the call stack depth and the instruction address for each event and map the event \nonto a CCT node using these values. We found that this simple technique works surprisingly well with \nmany tested benchmarks including a large application server workload, though it was not able to uniquely \ndistinguish the full calling context information in those programs which have complicated CCTs.. We implemented \nour new profiler in the IBM J9/TR Java VM and JIT compiler [2] for Linux running on an IBM POWER6 processor \n[3]. The runtime overhead of profiling was 2.2% or less when we controlled the sampling interval to generate \n16,000 samples/sec. Our techniques do not depend heavily on the specific environment and thus most of \nthe proposed techniques can be applied to other processors and other dynamic language runtimes. This \npaper makes the following contributions. (1) We demonstrate that the HPM can be used to profile high-level \nevents in language runtime systems by correlating them with hardware events. We present how our profiler \ngenerates object creation profiles and lock activity profiles with low overhead. (2) We introduce a lightweight \ncontext detection technique called CallerChaining, which detects the calling contexts for events without \nadding runtime overhead. (3) We present efficient techniques to identify the Java object that caused \nan HPM event based on a data address captured by the HPM. For example, our technique for identifying \nobjects in the Java heap can avoid the overhead of costly memory scanning [4]. Our proposed techniques \nenable both programmers and runtime systems to get more valuable information from the HPM to understand \nand optimize the running programs without adding excess runtime overhead. For example, the lock activity \nprofiles can help optimizing a lock protocol based on its behavior and traits, such as the owner locality \n[5, 6]. Also, it is known that the calling context of the allocation site is important to predict object \nlifetime [7], and hence our object creation profiles with the calling context can support adaptive optimizations \nbased on the objects lifetimes [8]. The rest of the paper is organized as follows. Section 2 gives an \noverview of the hardware performance monitor of the POWER6 processor. Section 3 discusses related profiling \ntechniques. Section 4 describes our HPM-based sampling profiler. Section 5 illustrates our new techniques \nto profile high-level events using the HPM. Section 6 uses our technique to detect the calling context \nusing the call stack depth. Section 7 describes the experimental environment and our results. Finally, \nSection 8 summarizes our new techniques.  2. HPM of the POWER6 Processor In this paper we use the IBM \nPOWER6 processor [3] to present our new profiling techniques. Like many other processors, POWER6 has \na built-in HPM (Hardware Performance Monitor) that can monitor and count various performance-related \nevents, such as the cache misses or instructions executed on the processor. The POWER6 provides four \nperformance counters (programmable from the operating system) so that up to four performance\u00adrelated \nevents can be counted simultaneously. An HPM counter can be configured to generate an interrupt when \nthe counter value overflows. To provide detailed information on the HPM event that caused the interrupt, \nPOWER6 has two special purpose registers called SIAR (sampled instruction address register) and SDAR \n(sampled data address register) [9]. The SIAR contains the instruction address of the sampled instruction \nand the SDAR contains the data address if the sampled event is a memory-related event . By using the \nHPM interrupts, we can sample events for specified sampling intervals and obtain instruction and data \naddresses for each HPM event. The implementation of an HPM depends on the processor, but most of today \ns processors have similar features. For example, recent Intel s x86 processors support PEBS (precise \nevent based sampling), which arrows profilers to obtain an architectural state information of the processor \nwhen a selected event occurs [11] like the To have accurate values in the SIAR and SDAR registers, we \nneed to use HPM events named with the prefix PM_MRK. The list of the HPM events on POWER6 is available \nin the Oprofile distribution [10].  SIAR and SDAR of the POWER6. The PEBS is useful to implement our \nProbeNOP technique and CallerChaining. However the PEBS does not support the store instruction executed \nevent and thus our object creation profiling is not directly applicable for the PEBS. To implement the \nProbeNOP technique with the PEBS, x87 floating-point instructions or SIMD instructions can be used as \nthe ProbeNOP instruction if these instructions are not used in the JVM.  3. Related Work There are existing \nprofilers, such as Oprofile [10] in Linux and the tprof command in IBM AIX, which can generate hardware \nevents profiles using the HPM. The scope of those existing profilers is, however, limited to the profiles \nfor hardware events directly supported as an event in the HPM, such as cache miss profiles. In contrast, \nour profiler can provide low-overhead Java-level events profiles, such as object creation profiles and \nthe lock activity profiles, in addition to the hardware event profiles. There are profilers to capture \nsuch Java-level events, which typically use JVM Tool Interface (JVMTI) [12] to communicate with the JVM. \nHowever they often incur unacceptable overhead during program execution. For example, HPROF as included \nin JRE distributions can provide a rich set of useful information including CPU time profiles, object \ncreation profiles, and monitor contention profiles. Because HPROF incurs significant overhead, the performance \nof profiled applications often degrades by more than a factor of 10. Such overhead may make the CPU time \nor monitor contention profiles unreliable and different from the unmonitored profiles. Our profiler can \nprovide similar Java-level event profiles with much lower overhead by exploiting the sampling facility \nof the HPM. Due to the importance of the object creation profiles, many profilers have been developed \nto collect various information on object creation. Some profilers insert a hook in the object creation \ncode to identify the allocation site, while others count the number of objects for each class in the \nJava heap. However a hook incurs significant overhead, while object counting cannot capture the allocation \nsite information accurately. Compared to these techniques, our HPM-based approach can capture information \non both the created objects and on their allocation sites with low overhead by exploiting the sampling \nfacility of the HPM. Some software-based sampling techniques have been proposed for the object creation \nsampling [13, 14]. The overhead of these techniques are typically small, such as less than 3.0% [13]. \nThe advantage of our HPM-based object creation sampling is that we do not need to generate any additional \ninstructions in the JIT compiler and thus our technique does not impose additional overhead while the \nHPM is not configured to generate interrupts. We created ProbeNOP, an instrumentation technique for value \nprofiling, which uses HPM sampling features. In this technique, we insert only a special NOP instruction \nin the program code and thus the overhead is negligible while the sampling is disabled. In contrast to \nan existing low\u00adoverhead profiling technique [15], which generates duplicated code with instrumentations, \nour technique does not impose significant space overhead because it only adds one instruction per sampling \npoint. This advantage makes it possible to insert instrumentation code for all locations of interest \nto generate complete profiles. Our profiler supports context-sensitive profiling by our CallerChaining \ntechnique. Profiles with calling context information, consisting of current program locations and sequences \nof call sites on the stack, are more informative in characterizing the program behaviors compared to \nprofiles with only current locations. Often stack walking is used when a profiler needs to know the current \ncalling context. Alternatively, it is also possible to track the current calling context at each method \ninvocation and exit by inserting the instrumentation code [1, 16]. However both of these techniques are \nvery costly. For example, Oprofile supports context-sensitive profiling by walking the stack at each \ninvocation of the handler for an HPM interrupt, but this is much more work than profiling that ignores \nthe contexts. Our technique avoids this work in the interrupt handler by not using stack walking. There \nare advanced techniques for context-aware profiling [17, 18, 19, 20]. Among them, Bond and McKinley s \n[17] Probabilistic Calling Context (PCC) can be directly applicable for the HPM profiling. The PCC allows \nfor lightweight context-sensitive profiling by maintaining only one value that represents the current \ncalling context. Both the PCC and our CallerChaining are probabilistic approaches to detect the calling \ncontext. In contrast to ours, the PCC generates special code and data structure to do context-sensitive \nprofiling and thus imposes overhead in both computation time and memory space in exchange for higher \naccuracy. Our CallerChaining does not impose visible runtime overhead as long as the HPM is not configured \nto generate interrupts. This advantage makes our technique more attractive to use as part of the runtime \nsupport for adaptive optimizations. Because we only use call stack depth to identify the calling context \ninstead of a special value in PCC, our technique cannot distinguish contexts accurately in programs that \nhave very complicated CCTs. For such programs, we can combine our technique with the PCC to improve the \naccuracy in profiling with additional overhead. Recently, Mytkowicz et al. [21] proposed a technique \ncalled inferred call path profiling for C and C++ programs. Although developed independently, their technique \nalso uses pairs of the current instruction pointer and the call stack depth to identify the calling context. \nThey focused on disambiguating the calling contexts that have the same call stack depth.  There are \nsome existing techniques that use cache miss profiles obtained by the HPM when applying adaptive optimizations \nin compilers and runtime systems [4, 22, 23, 24]. For example, Adl-Tabatabai et al. [4] exploit cache \nmiss statistics in their Java JIT compiler to insert effective prefetch instructions on the Intel Itenium2 \nprocessor. Later, Schneider et al. [22] also used cache miss statistics in the garbage collector to optimize \nthe placement of objects in the Jikes RVM on the Intel Pentium4 processor. These techniques identify \nthe instructions and objects that cause many cache misses and exploit the information for optimizations. \nWe seek to provide more information on running programs, including Java-level event profiles and context-sensitive \nevent profiles, without adding significant runtime overhead.  4. Profiling Framework In this section, \nwe describe the implementation of our profiler to capture the hardware events and generate the profiles \nfor those events. In particular, we focus on our effective translation techniques to identify the Java \nobject that caused an event based on the data address of the event. Our new technique to collect Java-level \nevents using the HPM and our context-sensitive profiling technique will be described in later Sections \n5 and 6, respectively. 4.1 Oprofile device driver in the Linux kernel Our profiler consists of two parts: \nan operating system device driver (to access the HPM from the user space) and a built-in profiler implemented \nin the JVM, which accesses the HPM via the interface exported by the device driver. We used Linux as \nour target operating system and used the existing Oprofile [10] device driver in the Linux kernel as \nthe basis of our device driver. The Oprofile device driver for the PowerPC architecture provides an interface \nto set the HPM-related special purpose registers from user space. By using this interface, we can select \nup to four HPM events to count in the HPM counters and also specify a sampling interval for each counter \nto generate an interrupt when that counter reaches its sampling interval. We did not modify this interface. \nOprofile also provides an OS-space buffer to store the results of the HPM samples. User-space processes \ncan read the OS-space buffer using a read system call to a special device file. A user process will block \non the system call and return when the OS-space buffer becomes full or is explicitly flushed.  4.2 Overview \nof our profiler What we extended in the device driver is the information to capture in the HPM interrupt \nhandler as implemented in the device driver. The original Oprofile only captures the HPM counter ID, \nto label the source of the interrupt, and the instruction address that caused the event for each HPM \nsample. Our extended version also captures a data address from the SDAR register (for memory-related \nevents) and the value of the stack frame pointer. We use this frame pointer value in three ways: (1) \nto identify the software thread that caused the event, (2) to identify the Java object that caused an \nevent if that object is allocated on stack, and (3) to track the calling context for the events as described \nin Section 6. The JVM has two kinds of stack frame pointers, GPR1 for the system stack and GPR14 for \nthe Java stack. We use GPR14 in our profiler. We create a dedicated thread for the HPM profiler in the \nJVM to read the HPM samples from the OS-space buffer and generate the statistics. Though the JVM has \nanother profiling mechanism based on timer interrupts to control the JIT compilation, our HPM profiler \nthread only handles the HPM-based profiling and does not affect the timer\u00adbased profiling mechanism. \nThe HPM profiler thread first sets the HPM events to profile, and then synchronously read the OS-space \nbuffer for the HPM samples, so the thread blocks on this read system call until the OS-space buffer becomes \nfull. When the buffer becomes full, then the HPM profiler thread returns from the read system call and \ncopies the content of the buffer into a user-space buffer. Once the data has been copied to user space, \nthe HPM profiler thread translates the instruction address of each sample into a Java method, when the \naddress is included in a JIT-compiled method, or into a JVM module, when the address is not in a JIT-compiled \nmethod. The JVM module will be, for example, the interpreter, the memory manager, or the JIT compiler. \nThe original JVM already has the data to convert an instruction address of the JIT-compiled method into \na Java method, so we do not need a new data structure for that purpose. Most JVM implementations should \nhave similar data structures to identify program locations when exceptions occur in JIT-compiled methods. \nFor memory-related events, the HPM profiler thread also translates the data address to a Java class, \nan offset in the object, and the location of the objects (nursery, survivor, tenure, or stack). We give \ndetails on the data address to Java object translation process in the next sections. Focusing on running \nJava applications, we do not use the HPM sampling during stop-the-world GC. When the Java heap becomes \nfull and stop-the-world GC begins, we stop the HPM sampling and explicitly flush the OS-space buffer. \nThe GC threads must wait for the HPM profiler thread to complete the data address translation. This is \n (a) without data address to Java object translations stop-the-world GC  block in read system call \nblock in read block in read (1) (2) (3) (4)  HPM samplingHPM sampling HPM sampling enabled enabled \nenabled time (1) When the buffer for HPM samples in the OS space becomes full, the HPM profiler thread \nreturns from the read system call. HPM sampling is disabled. (2) When the HPM profiler thread finishes \nprocessing the samples, HPM sampling is re-enabled. (3) When a stop-the-world GC starts, the GC threads \nforce flushing the OS-space buffer and the HPM profiler thread returns from the read system call. (4) \nWhen the stop-the-world GC ends, GC threads re-enable the HPM sampling. (If the HPM profiler thread has \nnot finished its processing, the GC thread does not re-enable the sampling.)  (b) with data address \nto Java object translations (for memory-related events) stop-the-world GC (1) (2) (3A) (4) HPM samplingHPM \nsampling HPM sampling enabled enabled enabled time  (3A) When a stop-the-world GC starts, the GC threads \nforce flushing the OS-space buffer and the HPM profiler thread returns from read system call. The GC \nthreads wait for the HPM profiler thread to finish translating the data addresses of all samples. (3B) \nAfter the HPM profiler thread completes the translations, it wakes the GC threads and continues running \nto generate profiles based on the already translated samples. Figure 1. Schematics of collaboration among \nan HPM profiler thread, GC threads, and application threads: (a) when the data address to Java object \ntranslations are not involved (b) when the translations are involved. because the HPM profiler thread \ncannot identify the Java object from the data address if the GC threads move the object in the Java heap. \nFigure 1 depicts how the HPM profiler thread collaborates with the other threads with and without the \ndata address translation. In the current implementation, we do not parallelize the HPM profiler thread \nand use only one dedicated thread.  4.3 Identifying objects in the Java heap This section describes \nour new techniques to effectively find the object that caused a memory-related event, such as a cache \nmiss, based on a data address obtained from the HPM, when the data address points at an address inside \nthe Java heap. As already mentioned in Section 4.2, we ensure that objects in the Java heap have not \nbeen moved by the garbage collector before the data address to Java object translation completes. Adl-Tabatabai \net al. [4] do this translation by scanning memory backward from the sampled data address to find the \nnearest valid object header, starting with a pointer to a virtual function table of the class, in their \ntechnique called Mississippi delta. To avoid the large cost of this backward scan, we introduce a new \ntechnique, which tries a heuristic before the scan. It first checks the instruction that caused the event, \nand if the instruction is a load or store instruction that points to an address to access with a value \nin a register and a constant offset, it is likely that the value in the register is  Existing Technique \nOur Technique doBackwordMemoryScan (dataAddress) { findObjectHeader (dataAddress, instructionAddress) \n{ p = dataAddress; // shortcut path by checking instruction count = 0; instruction = *instructionAddress; \nwhile (count < LIMIT) { if (instruction is a load/store addressed by register + constant) { if (*p contains \na valid object header) { if (*(dataAddress constant) contains valid object header) { return p; // successfully \nidentified object } return dataAddress constant; count++; } p = p 8; // assuming object is 8-byte aligned \n} } // fall back to memory scanning return HEADER_NOT_FOUND; return doBackwardMemoryScan (dataAddress); \n} } Figure 2. Pseudocode of the two methods to identify the Java object that caused an event. a pointer \nto an object and the constant is an offset within the object. We then test that location to see if it \ncontains a valid object header. Because the IBM JIT compiler generates this form of load and store instructions \nfor field accesses, this works in many cases. If the instruction is not in the form or if the assumed \nlocation does not contain a valid object header, we fall back to the memory scan to find a valid object \nheader. Figure 2 shows pseudocode for our technique and the existing memory scanning technique. In SPECjbb2005, \nwe successfully identified the objects using this heuristic in 84.2% and 83.6% of the L1 and L2 cache \nmiss events. This technique cannot be used if the JIT compiler uses optimization techniques that change \nthe object format, such as object inlining [25].  4.4 Identifying stack-allocated objects This section \ndescribes how our profiler identifies a stack\u00adallocated object that generates an event from a data address. \nWe can assure that the location of the object in the Java heap is not changed before the HPM profiler \nexecutes the data address to Java object translations by controlling the GC activities. However, it is \nnot possible to retain the state of a stack-allocated object because the stack frame that included the \nobject was discarded when the application thread exited from the method. Thus we need another technique \nto track stack-allocated objects. As already described, our profiler captures the value of the stack \nframe pointer for each HPM sample. We use the frame pointer value to identify the stack-allocated objects. \nIf the sampled data address is in a stack for any Java thread, the profiler calculates the offset of \nthe data address in the stack frame by subtracting the frame pointer value from the data address. Then \nit looks up a table that contains offsets and sizes of all stack-allocated objects for each Java method. \nWe modified the JIT compiler to maintain this table because the original JVM did not track the information. \nThe space overhead for this additional data structure is small because the stack allocation of local \n . objects based on escape analysis is a costly optimization and is applied only to a few very hot methods. \n 5. How to Get Java-level Event Profiles Using the HPM In this section, we give details about our techniques \nto obtain high-level information in a Java VM using the HPM. First we describe a simple example for an \nobject creation profile. Then we describe our techniques for a more complicated example using lock activity \nprofiling. 5.1 Identifying object creation events by screening store instructions Here, we show how our \nprofiler derives an object creation profile from the HPM samples. Though what we describe in this section \nlooks simple and naive, it yields accurate statistics, including information for both the created objects \nand allocation sites. Our observation is that, in our JVM, the first word in the object header, which \nis the virtual function table pointer, is not modified once the object is initialized. Based on this \nobservation, we create the object creation profiles by first collecting the store instruction profiles \nusing the HPM, which includes a data address and an instruction address for each sampled store instruction, \nand then translate the data address of each sample into a Java object and offset in the object. Here \nwe used the PM_MRK_ST_NEST event on POWER6. Next, we filter out any sample whose offset value is not \nzero. After this filtering, the store instruction profile has become an object creation profile, because \na store to the virtual function table pointer appears only when the object is created. Of course, the \ngarbage collector also writes this word in the headers, but we disable the HPM sampling during garbage \ncollection to prevent those store instructions during GC from being included in the profile. Compared \nto existing profiling techniques for object creation profiling, our HPM-based approach can capture information \non both the created objects and their allocation  User-space pseudocode sum = 0; HPtuprreM int for (i \n= 0; i < 100; i++) { val = array[i]; ProbeNOP(val); sum = sum + val; } Corresponding pseudo-assembly \ncode load_imm reg1, 0 // i load_imm reg2, 0 // sum loop: load reg3 = array [reg1] // val ProbeNOP reg3 \nadd reg2 = reg2 + reg3 add reg1 = reg1 + element_size compare reg1, 100 * element_size jump_if_less loop \n HPM interrupt handler in OS handleInterruptForProbeNOP () { instructionAddress = read SIAR register; \ninstruction = *instructionAddress; if (instruction is ProbeNOP) { target = decode information encoded \nin the ProbeNOP; profiledValue = current value of the target; store {instructionAddress, profiledValue} \nin buffer; return; } // should not reach here return; } Figure 3. A simple example of ProbeNOP usage. \nsites with low overhead by exploiting the sampling facility of the HPM. Our technique does not require \nany special code for object allocations nor does it impose limitations in compiler optimizations. By \napplying a similar technique to a profile of store instructions, we can also create a profile of those \nobjects and methods which invoke a write barrier for the generational garbage collector. 5.2 Identifying \nLock activities by inserting ProbeNOP instructions In this section, we introduce a new instrumentation-based \nprofiling technique using a special NOP instruction to capture Java-level events that cannot be easily \nassociated with hardware events. This technique involves the JIT compiler generating special NOP instructions \nwhose execution is counted by the HPM in the code of interest, such as lock acquisition code sequences. \n 5.2.1 ProbeNOP The basic idea of our technique is that we insert a special NOP instruction where we \nwant to probe. This does not affect the program meaning or performance but only increments an HPM counter \nwhen encountered. We call this NOP instruction ProbeNOP. We use the ProbeNOP instruction to invoke an \nHPM interrupt and also to send context-dependent information to the HPM interrupt handler by encoding \nthe information in unused fields in the NOP instruction. We show a simple example of using ProbeNOP in \nFigure 3. The left side of the figure shows simple pseudocode to calculate the sum of the elements in \nan array with a ProbeNOP instruction inserted in the loop. This ProbeNOP does not affect the program \nexecution or performance if HPM sampling is not active. If the HPM is configured to count the ProbeNOP \nexecutions and to generate an interrupt with a sampling interval of 10, it still does not affect the \nmeaning of the code, but it generates an HPM interrupt once per ten iterations. The pseudocode for the \nHPM interrupt handler in the OS is shown on the right side of the figure. The handler first identifies \nthe instruction that generates the interrupt and checks if that instruction is a ProbeNOP. Then it decodes \nthe target information encoded in the ProbeNOP instruction s bit pattern. Here the target is the val \nstored in reg3. The handler stores the pair of the instruction address and the value of the target, reg3, \nin the OS-space buffer for HPM samples instead of the pair of the instruction address and the data address \nas when the HPM handler is tracking a memory-related event. The profiler can later determine the value \nof the target by reading the HPM samples from the OS-space buffer. As Vector Pe r m u t e V A-form \nv p e r m V R T , V RA, V RB, V RC  those 20 bits are used to encode the targets first 5 bits: the \nfirst target (GPR only)  next 1 bit: kind of the second target (register or memory)  next 14 bits: \nthe second target  -for memory 5 bits for base register (GPR only) 9 bits for offset from the base (-1024 \nto +1020 by assuming 4-byte alignment) -for register the target register (GPR, Link Register etc.) Figure \n4. ProbeNOP instruction format on POWER6 using the vector permute instruction [26].  Monitor enter code \nsequence in JIT-generated code ... ProbeNOP(obj); // at monitor enter // monitor enter for obj if (obj \nis in inflated mode || tryLock(obj) == FAILED) { // tryLock failed if obj is locked by another thread \n// or recursion count overflows monitorEnterHelper(obj); } // critical section begins ... Monitor enter \nhelper in JVM monitorEnterHelper(obj) { caller = read link register; ProbeNOP(obj, caller); // at helper \nenter sync(); if (obj is in inflated mode) { inflatedMonitorEnterHelper(obj); return; } for (i=0; i<loopCount1; \ni++) { for (j=0; j<loopCount2; j++) { if (tryLock(obj) == SUCCESS) return; ProbeNOP(obj, caller); // \nat spin loop sync(); doIdleLoop(); // just consume cycles } yield_cpu(); } inflateTheObject(); // spin \nlocking failed return; } Figure 5. ProbeNOPs for lock activity profile. a result, the profiler gets \nthe values of the 10th element, 20th element, .., and the 100th element in the array as a result of the \nsampling. This technique using the ProbeNOP enables value profiling of the specified target while minimizing \nthe performance degradation, especially when the HPM sampling is not enabled. Note that the interrupt \nhandler in Figure 3 is simplified for ease of explanation. We will show the pseudocode for the full interrupt \nhandler in Section 6.   5.2.2 Implementation on POWER6 To implement this technique on POWER6, we use \nthe vector permute (vperm) instruction of the VMX (Altivec) instruction set as a ProbeNOP [26]. We selected \nthis instruction because our JVM does not use VMX instructions and the HPM of the POWER6 can count the \nnumber of vperm instructions executed by the PM_MRK_VMX_PERMUTE_ISSUED event. Also, the VMX instructions \nare executed in a dedicated pipeline and thus do not affect the execution of other instructions significantly. \nThe vperm instruction format has 20 bits that we can use to encode the target information, because it \nuses four registers as operands and each register ID is 5 bits. In the current implementation, we support \nup to two targets when collecting values in the interrupt handler in one ProbeNOP. The first target is \ndedicated for general purpose registers and the second target is for one of the general-purpose registers, \nanother register such as the link register, or a memory location referenced by the value in the specified \ngeneral-purpose register with an offset. If two targets are specified in a ProbeNOP instruction, the \ninterrupt handler stores the instruction address and the values of both targets for each HPM sample. \nFigure 4 depicts the instruction format of the ProbeNOP. We have a limitation in implementing our technique \non POWER6 since the HPM interrupt is not precise; the processor state when an event occurs are not preserved \nuntil the HPM interrupt handler invoked. It means the target values, such as those in general purpose \nregisters, may change before the profiler read the values in the interrupt handler. We can still determine \nthe correct instruction address and data address because the POWER6 keeps the information in the SIAR \nand SDAR registers until the interrupt handler is invoked. To ensure that the interrupt handler reads \na valid target value on POWER6, we can add a sync instruction after each ProbeNOP as a workaround . In \nthe next section, we insert a ProbeNOP in each lock acquisition operation to generate lock acquisition \nprofiles. Fortunately, in this case the monitor acquisition operation inherently includes a sync operation \nand thus we did not need to explicitly add an expensive sync instruction.  5.2.3 Lock activity profile \nusing ProbeNOP In this section, we use the profiling technique with ProbeNOP to generate a detailed lock \nactivity profile. We empirically found this workaround worked on the POWER6 processor, although the Power \nInstruction Set Architecture [26] does not explicitly require this behavior for the sync instruction. \n ca l l ee me thod c a lle e call s t ack depth c a lle r met hod c a lle r c a ll s tack de pth number \nof sa m p l es B 50 A 10 10 C 40 A 10 10 D D 90 80 B C 50 40 20 40 E E 50 10 0 C D 40 90 10 80  \n constructed calling context tree (number at each node means call stack depth)  Figure 6. Constructing \ncalling context tree from HPM samples based on call stack depths. The IBM JVM implements a bimodal locking \nalgorithm [27, 28, 29] in which a lock word in the object header has one of two modes: a flat mode for \nspin locking and an inflated mode for suspend locking. Each object starts from the flat mode and enters \nthe inflated mode when a Java thread fails to acquire the lock of the object by spin locking. We focus \non profiling the flat mode locks with our techniques. The IBM JVM supports a powerful lock profiling \ntool named Java Lock Monitor (JLM), included in the Performance Inspector [30], which can provide detailed \ninformation on lock contentions. However the scope of JLM is limited to the inflated locks, and it cannot \nprovide information on the program locations that caused the lock contention. Our technique can overcome \nthese limitations, and combining our profiler with the JLM can give a complete picture of the lock activities. \nWe insert ProbeNOP instructions at the following three kinds of locations: 1) all lock acquisition operations \nin the JIT-compiled methods (labeled monitor enter in the figure) 2) at the entry point of the monitor \nenter helper function (labeled helper enter) 3) in the spin loop for flat locks in the helper function \n(labeled spin loop) Figure 5 shows the locations of the inserted ProbeNOPs using simplified pseudocode \nfor the JIT-compiled code and the helper function. Each ProbeNOP has a different purpose. The ProbeNOP \nin the JIT-compiled code is to capture all of the lock acquisitions, including the successes that do \nnot call the monitor enter helper function. The ProbeNOP at the helper entry point is to provide the \nprogram location information for the inflated monitor activities captured by the JLM. The ProbeNOP in \nthe spin loop is to identify the locks which consume CPU time in this spin loop. Note that the two ProbeNOPs \nin the helper code are followed by sync instructions to assure that an HPM interrupt for the ProbeNOP \nis generated at those places. These sync instructions are in rarely executed paths and they do not incur \nsignificant overhead in most cases. The HPM profiler thread can distinguish among the samples from these \nthree types of ProbeNOPs by the instruction address in each sample. Then it generates high\u00adlevel lock \nactivity profiles, such as the hot locks that often spent long periods in the spin loop. Programmers \ncan combine such information with the output of the JLM to understand the lock activities.  6. How \nto Get Context-Sensitive Event Profiles Using HPM In this section we present our technique, called CallerChaining, \nto provide context-sensitive profiles without walking a stack for each HPM sample. 6.1 Frame-pointer-based \nCalling Context Tree (CCT) generation We first explain how we generate a calling context tree with a \ncall stack depth in each CCT node from the HPM samples based on the values in the stack frame pointer. \nIn this technique, we assume that the size of stack frame for each method is a constant during the measurements. \nThis implies we need to stop the JIT compiler during the measurements and this particular technique is \nonly suitable for working with an application in a steady state, but is not suitable for transient states \nsuch as those at boot time. For this technique, we need to collect samples as quadruples of {caller method, \ncaller call stack depth, callee method, callee call stack depth}. The sampling of those values does not \nrequire complicated operations and is much less expensive than the stack walking. Figure 6 shows examples \nof HPM sample profiles and a constructed CCT. In the table, the call stack depths for the callee and \ncaller are shown as offsets from the base of the stack area of the thread. Though the stack grows downward \nin our JVM, we show the offset value as positive in the figure. We construct the CCT by chaining the \ncallers and callees based  on call stack depths. From the HPM samples shown in the table, it is obvious \nthat Methods B and C are called from Method A. Also, Method D is called from Methods B and C via the \npaths A-B-D and A-C-D, respectively. The method E is called from Method C and D, but for Method D we \ncannot tell whether the method E is called via path A-B-D-E or path A-C-D-E from the callee and caller \nrelationship data alone. For such a case, we can use the call stack depth to distinguish the paths. In \nthe example, Method D has a call stack depth of 90 only when it is called from Method B and thus we can \ndetermine that method E is called via the path A-B-D-E rather than A-C-D-E. The right side of figure \n6 shows the constructed CCT with the call stack depth in each node. Note that because this technique \ndistinguishes among the call sites based on the call stack depth, it cannot distinguish between call \nsites in the same method if a method has multiple call sites for the same callee. A good way to collect \nthese quadruples is to insert a ProbeNOP at each method entry point to profile the link register value \n(caller address) and the call stack depth. On the POWER6 processor, however, we need to add a sync instruction \nfor each ProbeNOP to assure a precise interrupt. Adding a sync instruction for each method entry point \nincurs unacceptable overhead and so we took a different approach as a workaround. We first configure \nthe HPM to generate an interrupt after a certain number of instructions are executed. In this configuration, \ninterrupts are generated almost randomly throughout the program. If an interrupt happens in a method \nprologue, while the link register still has the caller address, we capture the link register value (caller \naddress) and the frame pointer value (call stack depth). This approach does not require ProbeNOP and \nsync instructions for each method, but it requires more samples because we throw away most of the samples \nthat miss the method prologues. That means that we need a long time to generate the CCT. If the stack \nframe structure allows the interrupt handler to access the caller address stored in the stack frame, \nwe can use the samples that do not hit the method prologue. In the current stack frame structure in our \nJVM, however, the location of the stack slot which contains the caller address is different from method \nto method, so it is difficult to obtain the caller address stored on the stack in the interrupt handler. \nAnother disadvantage of not using ProbeNOP is that the number of samples cannot always be used as an \nindicator for the hotness of the edge in the CCT. In Figure 6, for example, the edges A-B and A-C have \nthe same number of samples (10) as shown in the table, but that does not mean that Method A calls Methods \nB and C with the same frequency, because the size of prologue differs for each methods. However it is \nstill possible to use the number of samples to determine the relative hotness of the edges, as long as \nthose edges point at the same callee. For example, we can estimate that Method D calls Method E eight \ntimes more often than Method C based on numbers of samples in the table (10 and 80). We can use these \nestimates of edge hotness to distribute the HPM events. Figure 7 illustrates two cases where our technique \nfails to identify the context based on call stack depths. The first case is one in which some methods, \nB and F in the figure, have the same stack frame size. This can be generalized as to cases where two \ncall sequences, each consisting of multiple methods, have the same total stack frame size. The other \ncase is when some methods, B and C here, appear in the calling context in different orders. In both cases, \nwe add a node as needed in this phase. 6.2 Mapping HPM events on Calling Context Tree In this section, \nwe map the HPM events, such as cache misses, onto the generated CCT. Because our profiler collects a \nvalue in the stack frame pointer for each sample, it is mostly straightforward to map the events based \non the instruction addresses and the call stack depth. For example, if Method E caused a cache miss when \nthe call stack depth was 100, the event is mapped on the node of Method E on the path A-B-D-E. When there \nare multiple nodes having the same pair of a method and call stack depth, such as the cases shown in \nFigure 7, we distribute the HPM events based on hotness of the edges as many existing profilers, such \nas gprof [31], do when distributing the events or execution times for the callers. In the example of \nFigure 7(A), cache miss events generated by Method D are distributed into two paths A-B-D and A-F-D in \nproportion to the estimated hotnesses of the edges B-D and F-D, which we have already calculated when \nwe built the CCT. If this uncertainty affects the overall profile too greatly, we can change the stack \nframe size of a method by simply adding padding in the stack frame and retry the profiling [21]. 120 \nA) different methods B) same methods same stack frame size different order Figure 7. Two examples of \ncalling context trees for which our techniques cannot uniquely identify the calling context of the Method \nE.  handleInterrupt () { for each HPM counter which overflows { instructionAddress = read SIAR register; \n// instruction address which caused the interrupt if (running in Oprofile compatible mode) { // to allow \nthe original Oprofile running with this driver store {instructionAddress, counterID} in buffer; } else \nif (the counter is counting ProbeNOPs) { // handler for the ProbeNOP profiling (see section 4) instruction \n= *instructionAddress; if (the instruction is a ProbeNOP) {   // to wipe out irregular samples targets \n= decode information encoded in the ProbeNOP; // one ProbeNOP includes up to two targets profiledValue1 \n= current value of the target1; profiledValue2 = current value of the target2; store {instructionAddress, \nprofiledValue1, profiledValue2, framePointerValue, counterID} in buffer; } }  else if (generating a \ncalling context tree) { // handler for the CCT reconstruction (see section 5) currentInstAddr = current \ninstruction address // to get the address consistent with the frame pointer store {currentInstAddr, linkRegisterValue, \nframePointerValue, counterID} in buffer; } else if (SDAR register contains a valid value) { // by checking \na flag in a special purpose register dataAddress = read SDAR register;    // handler for memory-related \nevents store {instructionAddress, dataAddress, framePointerValue, counterID} in buffer; } else { // handler \nfor non-memory events store {instructionAddress, framePointerValue, counterID} in buffer; } reset the \ncounter according to the sampling interval; } } Figure 8. Pseudocode for the HPM interrupt handler. \nThe lack of precision of the HPM interrupts in POWER6 is a more severe problem. As already mentioned, \nwe can get the exact instruction and data addresses that caused an event, but the frame pointer value \n(call stack depth) at the time of the event may have already changed when the interrupt handler is invoked. \nDue to this problem, some HPM events do not have a corresponding node in the CCT. We try to find the \ncorrect nodes for such HPM events by changing the call stack depth to that of a possible caller or callee \nin the CCT. In the CCT of Figure 6, for example, a cache miss generated by Method E with a call stack \ndepth of 40 is mapped to the node of Method E in the path A-C-E, because the node s caller also has the \ncall stack depth of 40. This can happen when Method E causes a cache miss but the HPM interrupt for the \ncache miss occurs after returning to the caller, method C. With this fitting process, we successfully \nfound the corresponding nodes for most of the samples, even in the very large application. Complete pseudocode \nfor the entire HPM interrupt handler appears in Figure 8.   7. Experimental Results This section evaluates \nour profiling techniques by using standard benchmarks and a very large Web application server, the IBM \nWebSphere Application Server. 7.1 Profiling overhead We evaluate our profiler using SPECjbb2005 [32], \nSPECjvm2008 (compiler.compiler, derby, sunflow, xml.validation, serial, and mpegaudio) [33] and DayTrader \n2.0 [34] running on IBM WebSphere Application Server version 7.0 [35]. We implemented our profiler in \nthe 32-bit JVM included in the IBM SDK for Java 6 SR2. We ran the benchmarks on an IBM BladeCenter JS22 \nusing 2 cores of the 4.0-GHz POWER6 processors with 2 SMT threads per core. This means SPECjbb2005 and \nSPECjvm2008 were configured to run with 4 threads. Each core has 64 KB of L1 data cache and 64 KB of \nL1 instruction cache and 4 MB of L2 cache. The size of the Java heap was 2 GB using 16-MB large pages \nand the generational garbage collector was selected. The full JVM command line options were -Xgcpolicy:gencon \n-Xms2000m -Xmx2000m -Xmo400m -Xgcthreads4 -Xlp. The system has 16 GB of system memory and runs RedHat \nEnterprise Linux 5.2. For DayTrader, the DB2 database server and the client emulator ran on separate \nmachines. Figure 9 compares the performance of programs with and without the profiler attached to determine \nthe overhead of the profiler. The overhead of the profiling depends on the sampling interval. Shorter \nintervals (higher interrupt frequencies) give higher accuracies and larger overheads. We controlled the \nsampling interval to generate 8,000  A) sampling rate = 8,000 samples/sec B) sampling rate = 2,000 samples/sec \n 1.04 1.02 1 0.98 0.96 0.94 without profiler with profiler (CPI)0.92 with profiler (L1miss) with profiler \n(lock)0.9 higher is faster (non-zero (non-zero origin) origin) Figure 9. Average throughputs without \nprofiler and with three profiler configurations. The error bars show 95% confidence intervals. samples/sec \nfor each event (one sample per 500 \u00b5sec per HW thread, or per 2-million CPU cycles) or 2,000 samples/sec \nfor each event. The figure shows the overhead of three different configurations for the profiler. In \nthe first configuration (labeled CPI), the profiler counts the active CPU cycles (PM_RUN_CYC event) and \nthe instructions executed (PM_INST_CMPL event) simultaneously and calculates the CPI (cycles per instruction) \nfor each method. The profiler generated a CPI value for each method every 30 sec. In the second configuration \n(labeled L1miss), the profiler counts the L1 cache misses (PM_MRK_ LD_MISS_L1 event) and the instructions \nexecuted (PM_MRK_INST_FIN event). This configuration involved the expensive data address to Java object \ntranslation for the cache miss events, which requires stopping the GC threads during the translation \nand consumed more CPU time in the HPM profiler. Then the profiler generated a summary of the cache miss \nratios and a sorted list of the objects and fields that caused the cache misses for each method. The \nadditional statistics led to more overhead compared to the first configuration. The overhead to generate \nan object creation profile is almost the same as the overhead of the second configuration because the \nHPM profiler thread executes similar operations. In the third configuration (labeled lock), we inserted \nProbeNOPs to monitor the lock activities as described in Section 5 and the profiler counts the ProbeNOP \ninstructions executed and all of the instructions executed in the same interval. In this configuration, \nthe overhead due to the HPM profiler thread was similar to the second configuration, confirming that \nthe inserted ProbeNOPs (vperm instructions) do not significantly affect the performance. For SPECjbb2005 \nand SPECjvm2008, we ran the performance measurements 24 times with four iterations each and averaged \nthe best score of each run. For DayTrader, we ran and averaged four measurements. We also show a 95% \nconfidence interval for each data in the figure. An example of each type of the profiles appear in appendix. \nFrom Figure 9, the overhead of the profiler is generally small, within 2.2%, even for the second and \nthe third configurations that involve the costly data address to Java object translations. The difference \nbetween the first configuration and the other configurations mostly comes from the overhead of the translation. \nFor the third configuration, the inserted ProbeNOPs did not affect the performance of the tested programs. \nThe increase in the code size of JIT-compiled code due to the ProbeNOP, one instruction per lock acquisition \noperation, was fairly small and smaller than the fluctuations due to the dynamic nature of the JIT compiler. \nIn some programs, the overhead of the third configuration was slightly smaller than the second configuration \nbecause the data address of each HPM sample, as captured by the ProbeNOP technique, always pointed at \nthe Java object header, and the overhead of the data address to Java object translation was smaller than \nthe translation in the second configuration. The sampling frequencies in the configurations were high \nenough to generate accurate profiles in most cases. Actually much lower sampling frequencies, which would \nimpose much smaller overheads, would provide sufficiently accurate profiles for most purposes. For example, \nSchneider et al. [22] proposed 200 samples/sec as a reasonable choice on a single-core Pentium4 processor \nfor their optimizations, while we used 8,000 samples/sec on the two POWER6 cores. We used a higher sampling \nfrequency here because some of our advanced techniques, such as creating the object creation profiles, \nrequire a large number of samples compared to simple cache miss profiles.  1 1  other classes integer \narray 0.9 0.9 0.8 0.8 char array 0.7 0.7 0.6 0.6 0.5 0.4 0.3   other classes java/lang/Integer spec/jbb/Orderline \njava/math/BigDecimal java/lang/String char array      com/sun/tools/javac/code/Types$Subst com/sun/tools/javac/tree/JCTree$JCIdent \n0.5 com/sun/tools/javac/code/Type$MethodType java/util/HashMap$Entry 0.4 0.3 byte array java/lang/Integer \n0.2 0.2 com/sun/tools/javac/util/ListBuffer 0.1 0.1 com/sun/tools/javac/util/List 0 0 Our actual Our \nactual profiler profiler Figure 10. Breakdown of created objects by numbers of objects, as obtained \nfrom the HPM. 1 overlap metric 0.9 0.8 0.7 0.6 0.5 0.4 SPECjbb2005 compiler.compiler derby sunflow 0.3 \n0.2 xml.validation serial 0.1 mpegaudio daytrader 0 30 seconds 30 seconds 240 seconds 2,000 samples/sec \n8,000 samples/sec 8,000 samples/sec number of samples higher is more accurate Figure 11. Accuracy of \nthe object creation profiles as measured by the overlap metric. 7.2 Accuracy of the object creation \nprofiling To confirm the accuracy of our object creation profiling technique, Figure 10 compares the \nbreakdowns of the numbers of objects created in the Java heap as measured by our profiler for two benchmarks, \nSPECjbb2005 and compiler.compiler, to the numbers counted in the garbage collector. The figure shows \nthat the object creation profiles from our profiler were consistent with those generated by the more \naccurate method. To quantitatively evaluate the accuracy of our technique, we use the overlap metric \n[15] as calculated by the following formula: overlap(profile1, profile2) = . min(ratio(class) in profile1, \nratio(class) in profile2) class.profiles where ratio(class) = number_of_samples(class) / total_number_of_samples \nThe overlap metric shows how large portion of the samples are included in both profiles. This metric \nbecomes 100% for a pair of identical profiles. Figure 11 show the overlap metric of the profiles generated \nby our profiler and the profiles generated by counting objects in the garbage collector. We show the \naverage overlap metric calculated from samples gathered in 30 seconds of measurements with the sampling \nrates of 8,000 samples/sec and 2,000 samples/sec. We also show the overlap metric calculated from 4 minutes \nmeasurements with the 8,000 samples/sec sampling rate. The accuracy was limited for mpegaudio because \nthe number of newly created objects was so small that it was not possible to sample a lot of events required \nto generate an accurate profile. These results show our simple technique to derive the object creation \nprofile from the store instruction profile is an effective and accurate way to capture the object creation \nbehavior with very low overhead. Note that, though Figure 10 shows only the breakdown of object classes, \nour profiler got information on the allocation sites for every sampled object at the same time as shown \nin an example in appendix.  Table 1. Statistics for the calling context trees for each benchmark. Our \ntechnique cannot uniquely identify the locations of the children of those nodes in a CCT which have multiple \ncallers with the same frame pointer values, such as Method D in Figure 7. benchmark number of nodes in \nCCT number of methods in CCT average number of nodes per method ratio of L1 cache miss events whose callers \nwere uniquely identified for at least one level whose callers were uniquely identified for at least three \nlevels total having multiple callers with the same call stack depth SPECjbb2005 346 10 (2.9%) 122 2.8 \n99.5% 99.2% compiler.compiler 78,264 20,184 (25.8%) 1,477 53.0 71.0% 52.5% derby 1,092 8 (0.7%) 432 2.5 \n98.9% 98.9% sunflow 1,039 57 (5.5%) 104 10.0 97.6% 94.6% xml.validation 1,745 59 (3.4%) 442 3.9 97.9% \n95.6% serial 1,813 36 (2.0%) 218 8.3 90.8% 80.2% mpegaudio 54 0 (0.0%) 50 1.1 99.2% 99.2% DayTrader 37,616 \n2,962 (7.9%) 2,983 12.6 91.9% 86.4% The ratios in parenthesis show the ratio to the total number of \nnodes. 7.3 Accuracy of the context-sensitive profiling To evaluate how accurately our CallerChaining \ntechnique can generate the calling context tree (CCT) based on the call stack depths, we compare the \nnumber of nodes in the generated CCT and the number of nodes that have multiple callers with the same \ncall stack depth for each benchmark in the first two columns of Table 1. As described in Section 5, our \ntechnique cannot uniquely identify the locations of the children of such nodes in a CCT and we distribute \nthe HPM events based on the edge hotness in the CCT for the calling contexts that include such nodes. \nFrom the table, at most 7.9% of the nodes were problematic for the bench\u00admarks other than compiler.compiler. \nFor compiler.compiler, more than 25% of the nodes have multiple callers with the same call stack depth, \nand thus our technique has limited accuracy for the calling context in each sample. The compiler.compiler \nhas a very complicated CCT consisting of the largest number of nodes among the benchmarks even though \nthe number of methods involved in the CCT is smaller compared to DayTrader, as shown in the next two \ncolumns of Table 1. This means that each method was called from a variety of calling contexts, which \nled to frequent conflicts of the call stack depths in the different calling contexts. Note that we enabled \nthe HPM sampling for five minutes to generate the CCT. This measurement time can be much shorter if the \nprocessor supports precise HPM interrupts so we could use the ProbeNOP technique for this purpose. To \nevaluate how accurately our technique can identify the calling context of each HPM event, we show the \nnumber of L1 cache miss events for which we can track their calling contexts accurately against the total \ncache miss events occurring in the JIT-compiled code in the last two columns of Table 1. For the programs \nother than compiler.compiler, we can identify the call sites for at least one level in more than 90% \nof the cache miss events, and for at least three levels in more than 80% of the cache miss events without \nambiguity. Jones and Ryder [7] reported that only one level of the calling context improved the prediction \nof object lifetime though predictions based on allocation site alone were not accurate enough. Because \nof its low overhead and good accuracy, we believe that our CallerChaining is an attractive way to provide \nvaluable information to use with adaptive optimizations. For mpegaudio, we failed to identify the calling \ncontexts for 0.8% of the cache miss events, even though the benchmark did not suffer from any call stack \ndepth conflicts. These failures were caused by the imprecise HPM interrupts that returned mismatched \npairs of an instruction address and a call stack depth. For benchmarks other than compiler.compiler, \nthe ratios of such failures were 0.1% to 1.3%, while it was 4.4% for compiler.compiler. Figure 12 shows \nthe L1 cache miss profiles mapped for the calling contexts for SPECjbb2005. Each line of the profile \nshows a method signature, the call stack depth shown in the offset from the stack base of the thread, \nthe number of samples in the method (labeled s:), the ratio of the total samples, the number and ratio \nfor all descendants and itself (labeled d:). We only show those nodes whose descendants generated at \nleast 0.3% of the total cache misses. Also, we do not show inlined methods as separate nodes in the CCT. \nIn the CCT, the root of all of the calling contexts is the TransactionManager.go method. However this \nmethod is not the true root in the program and it has a caller. However the Java threads did not exit \nand reenter the TransactionManager.go method during the measurement period and so the profiler was not \nable to identify the caller of this method. By combing the results of a few stack-walk-based stack traces \nwith our  +-spec/jbb/TransactionManager.go()V:0x160 s:0(0.0%) d:221721(91.6%)+-spec/jbb/TransactionManager.goManual(ILspec/jbb/TimerData;)J:0x210 \ns:2953(1.2%) d:221376(91.5%)+-spec/jbb/CustomerReportTransaction.process()Z:0x2d0 s:74758(30.9%) d:75946(31.4%)+-spec/jbb/Company.getCustomerByLastName(SBLjava/lang/String;)Lspec/jbb/Customer;:0x340 \ns:454(0.2%) d:1188(0.5%)+-spec/jbb/CustomerReportTransaction.processTransactionLog()V:0x2d8 s:1581(0.7%) \nd:6002(2.5%)+-spec/jbb/infra/Util/XMLTransactionLog.clear()V:0x370 s:669(0.3%) d:985(0.4%)+-spec/jbb/infra/Util/XMLTransactionLog.populateXML(Lspec/jbb/infra/Util/TransactionLogBuffer;)V:0x360 \ns:1432(0.6%) d:1533(0.6%)+-spec/jbb/DeliveryTransaction.process()Z:0x278 s:122(0.1%) d:77080(31.9%)+-spec/jbb/DeliveryTransaction.preprocess()Z:0x3b0 \ns:74438(30.8%) d:76904(31.8%)+-spec/jbb/District.removeOldNewOrders(I)V:0x418 s:780(0.3%) d:883(0.4%)+-spec/jbb/District.removeOldOrders(I)V:0x420 \ns:691(0.3%) d:990(0.4%)+-spec/jbb/NewOrderTransaction.init()V:0x248 s:777(0.3%) d:860(0.4%)+-spec/jbb/NewOrderTransaction.process()Z:0x330 \ns:1486(0.6%) d:20621(8.5%)+-java/util/TreeMap.rbInsert(Ljava/lang/Object;)Ljava/util/TreeMap$Entry;:0x360 \ns:2092(0.9%) d:2821(1.2%)+-spec/jbb/Company.getCustomer(JZ)Lspec/jbb/Customer;:0x388 s:1004(0.4%) d:1004(0.4%)+-spec/jbb/Order.processLines(Lspec/jbb/Warehouse;SZ)Z:0x4b8 \ns:7935(3.3%) d:15310(6.3%)+-spec/jbb/Orderline.<init>(Lspec/jbb/Company;IBSSSZ)V:0x500 s:1393(0.6%) d:1405(0.6%)+-spec/jbb/Orderline.process(Lspec/jbb/Item;Lspec/jbb/Stock;)V:0x5d8 \ns:3162(1.3%) d:5232(2.2%)+-java/math/BigDecimal.multiply(Ljava/math/BigDecimal;)Ljava/math/BigDecimal;:0x640 \ns:1513(0.6%) d:1513(0.6%)+-spec/jbb/NewOrderTransaction.processTransactionLog()V:0x2c8 s:3754(1.6%) d:13269(5.5%)+-spec/jbb/infra/Util/TransactionLogBuffer.putDollars(Ljava/math/BigDecimal;III)V:0x318 \ns:504(0.2%) d:1040(0.4%)+-spec/jbb/infra/Util/TransactionLogBuffer.putText(Ljava/lang/String;III)V:0x308 \ns:1793(0.7%) d:1793(0.7%)+-spec/jbb/infra/Util/XMLTransactionLog.clear()V:0x360 s:1236(0.5%) d:1630(0.7%)+-spec/jbb/infra/Util/XMLTransactionLog.populateXML(Lspec/jbb/infra/Util/TransactionLogBuffer;)V:0x350 \ns:3115(1.3%) d:3482(1.4%)+-spec/jbb/OrderStatusTransaction.processTransactionLog()V:0x268 s:89(0.0%) \nd:935(0.4%)+-spec/jbb/PaymentTransaction.init()V:0x250 s:696(0.3%) d:1078(0.4%)+-spec/jbb/PaymentTransaction.process()Z:0x2c0 \ns:1058(0.4%) d:7627(3.2%)+-spec/jbb/Company.getCustomerByLastName(SBLjava/lang/String;)Lspec/jbb/Customer;:0x330 \ns:840(0.3%) d:2227(0.9%)+-spec/jbb/TreeMapDataStorage.getMedianValue(Ljava/lang/Object;Ljava/lang/Object;)Ljava/lang/Object;:0x390 \ns:28(0.0%) d:1238(0.5%)+-java/util/TreeMap$AbstractSubMapIterator.<init>(Ljava/util/TreeMap$NavigableSubMap;)V:0x3b0 \ns:3(0.0%) d:1207(0.5%)+-java/util/TreeMap$AscendingSubMapIterator.getBoundaryNode()Ljava/util/TreeMap$Entry;:0x3c8 \ns:0(0.0%) d:1120(0.5%)+-java/util/TreeMap$NavigableSubMap.smallerEntry(Ljava/lang/Object;)Ljava/util/TreeMap$Entry;:0x3f0 \ns:1(0.0%) d:1120(0.5%)+-java/util/TreeMap$NavigableSubMap.findLowerEntry(Ljava/lang/Object;)Ljava/util/TreeMap$Entry;:0x428 \ns:2(0.0%) d:1119(0.5%)+-java/util/TreeMap$NavigableSubMap.findEndNode()Ljava/util/TreeMap$Entry;:0x498 \ns:1068(0.4%) d:1068(0.4%)+-spec/jbb/Warehouse.removeOldestHistory()V:0x300 s:49(0.0%) d:1121(0.5%)+-java/util/TreeMap.find(Ljava/lang/Object;)Ljava/util/TreeMap$Entry;:0x330 \ns:654(0.3%) d:814(0.3%)+-spec/jbb/Warehouse.updateHistory(Lspec/jbb/History;)V:0x300 s:321(0.1%) d:2433(1.0%)+-java/util/TreeMap.put(Ljava/lang/Object;Ljava/lang/Object;)Ljava/lang/Object;:0x320 \ns:0(0.0%) d:2112(0.9%)+-java/util/TreeMap.rbInsert(Ljava/lang/Object;)Ljava/util/TreeMap$Entry;:0x350 \ns:1568(0.6%) d:2112(0.9%)+-spec/jbb/PaymentTransaction.processTransactionLog()V:0x2e8 s:2242(0.9%) d:7964(3.3%)+-spec/jbb/infra/Util/XMLTransactionLog.clear()V:0x380 \ns:1045(0.4%) d:1303(0.5%)+-spec/jbb/infra/Util/XMLTransactionLog.populateXML(Lspec/jbb/infra/Util/TransactionLogBuffer;)V:0x370 \ns:1959(0.8%) d:2126(0.9%)+-spec/jbb/StockLevelTransaction.process()Z:0x2b8 s:4127(1.7%) d:4127(1.7%) \nFigure 12. An example of HPM events mapped to the calling context trees (L1 data cache miss profile for \nSPECjbb2005). We show only those nodes whose descendants generated at least 0.3% of the total events. \nprofiler, the results give a much better picture of the calling context tree. Note that these tree-structured \nprofiles were constructed from the outputs of the HPM profiler in the post-processing phase, and the \nprofiler did not generate the trees at runtime. This post-processing took only a few seconds using a \nnon\u00adoptimized Perl script. An entire tree-structured profile is not required for the adaptive optimizations, \nsuch as when identifying the callers of selected nodes that cause many events. While investigating various \nworkloads with our profiler, we often observed that collection classes such as hashmap generated many \ncache misses. In such cases, more valuable than the program locations was information about the calling \ncontext of the hashmap functions. The information about the callers of the hashmap functions is not satisfactory \nbecause programmers often wrap a hashmap with their own class and all callers of the hashmap functions \nbecome the same. Our technique can provide enough information to identify the callers of such wrapped \nhashmaps, even though it cannot uniquely distinguish the full calling context information. When more \nexact calling context information is required, we could use the PCC [17], which maintains a value to \nidentify the current calling context at each method entry and exit. To read the PCC value from the interrupt \nhandler, we would need to put the value in a location that the handler can easily find, such as a slot \nin the stack with a constant offset from the frame pointer.  8. Summary In this paper, we described \nour sampling-based profiler that exploits a hardware performance monitor (HPM) available in the processor \nto collect information on running Java applications for use by the Java VM. Our profiler provides two \nnovel features: Java-level events profiling and lightweight context-sensitive event profiling. For the \nformer feature, we showed that the HPM can be used to profile high-level events in language runtime systems \nby correlating them with hardware events. We showed how our profiler generates object creation profiles \nand lock activity profiles with low overhead. For lock activity, we presented a lightweight context detection \ntechnique called CallerChaining, which detects the calling context for events. Our proposed techniques \nenable both programmers and runtime systems to get valuable information from the HPM to understand and \noptimize running programs without adding major overhead. Based on the insights in this paper, we hope \nthat future processors support the precise HPM interrupts that allow the profiling tools to obtain the \ndetailed processor states at the time of HPM events. Also NOP instructions whose execution can be counted \nby the HPM can provide more Acknowledgments freedom for programmers to exploit the HPM sampling We are \ngrateful to the anonymous reviewers for their facility and thus offers another interesting extended use \nof valuable comments and suggestions. We thank Mauricio the HPM.  Serrano and Peter F. Sweeney for their \nuseful feedback and detailed comments on earlier drafts. .  Appendix This appendix shows examples \nof various profiles to show what kind of information is included in the profiles. (a) An example of CPI \nprofile (for SPECjbb2005). RUN CYCLE INSTRUCTION RATIO METHOD ============== ============== ======= \n================================================== % samples % samples 32.2% (77637) 9.8% (23753) 1131.31% \nspec/jbb/DeliveryTransaction.preprocess()Z 13.9% (33441) 9.4% (22828) 507.04% spec/jbb/CustomerReportTransaction.process()Z \n 4.3% (10255) 9.6% (23263) 152.58% spec/jbb/infra/Util/XMLTransactionLog.populateXML( 2.5% (6049) 1.1% \n(2572) 814.04% spec/jbb/StockLevelTransaction.process()Z 2.4% (5677) 3.5% (8456) 232.37% spec/jbb/infra/Util/TransactionLogBuffer.putText(L \n ... (b) An example of cache miss profile (L1 data cache miss profile for SPECjbb2005). L1D$ MISS MRKD \nINSTRUCTION RATIO METHOD ============== ============== ======= ================================================== \n % samples % samples 30.6% (74107) 11.2% (15992) 2.93% spec/jbb/CustomerReportTransaction.process()Z \n30.1% (72905) 11.4% (15178) 2.83% spec/jbb/DeliveryTransaction.preprocess()Z  4.7% (11480) 2.8% (4371) \n1.84% spec/jbb/Order.processLines(Lspec/jbb/Warehouse;SZ 3.0% (7251) 8.7% (15535) 0.37% spec/jbb/infra/Util/XMLTransactionLog.populateXML( \n... spec/jbb/DeliveryTransaction.preprocess()Z 30.1% (72905) L1D$ MISS MRKD LOCATION OFFSET CLASS ============== \n======== ====== =============================== % samples 4.0% (9720) tenure 0 spec/jbb/Stock 2.5% (6108) \ntenure 32 spec/jbb/Stock 2.0% (4847) nursery 8 spec/jbb/Orderline 1.9% (4552) nursery 0 java/math/BigDecimal \n ... (c) An example of lock activity profile (for DayTrader). MONITOR ENTER INSTRUCTION RATIO METHOD \n============== ============== ======= ================================================== % samples % \nsamples 9.0% (22629) 0.5% (1258) 1.29% org/apache/openjpa/jdbc/sql/SQLBuffer.append(Ljava 4.5% (11287) \n0.5% (1143) 0.71% org/apache/openjpa/jdbc/meta/strats/HandlerFieldSt 4.3% (10778) 0.6% (1399) 0.55% org/apache/openjpa/jdbc/sql/SelectImpl.getTableInd \n2.9% (7397) 0.3% (606) 0.88% org/apache/openjpa/jdbc/sql/SelectImpl$SelectResul ... org/apache/openjpa/jdbc/sql/SQLBuffer.append(Ljava \n9.0% (22629) MONITOR ENTER LOCATION CLASS ============== ======== =============================== % \nsamples 9.0% (22625) nursery java/lang/StringBuffer ...  SPIN LOOP INSTRUCTION RATIO METHOD ============== \n============== ======= ================================================== % samples % samples 36.9% \n(75) 0.0% (90) 0.06% com/ibm/ejs/ras/Tr.register(Ljava/lang/Class;Ljava 14.3% (29) 0.1% (187) 0.01% org/apache/openjpa/meta/MetaDataRepository.getMeta \n6.4% (13) 0.2% (455) 0.00% com/ibm/io/async/ResultHandler.runEventProcessingL 3.0% (6) 0.0% (58) 0.01% \ncom/ibm/ws/persistence/EntityManagerImpl.createNam ... com/ibm/ejs/ras/Tr.register(Ljava/lang/Class;Ljava \n36.9% (75) SPIN LOOP LOCATION CLASS ============== ======== =============================== % samples \n36.9% (75) tenure com/ibm/ws/bootstrap/WsLogManager ... HELPER ENTER INSTRUCTION RATIO METHOD ============== \n============== ======= ================================================== % samples % samples 44.9% \n (48) 0.1% (185) 0.01% org/apache/openjpa/meta/MetaDataRepository.getMeta 13.1%  (14) 0.3% (650) 0.00% \njava/util/Hashtable.put(Ljava/lang/Object;Ljava/la 6.5% (7) 0.0% (5) 0.08% com/ibm/ws/util/BoundedBuffer.waitGet_(J)V \n5.6% (6) 0.2% (377) 0.00% java/util/Hashtable.get(Ljava/lang/Object;)Ljava/l ... org/apache/openjpa/meta/MetaDataRepository.getMeta \n44.9% (48) HELPER ENTER LOCATION CLASS ============== ======== =============================== % samples \n44.9%  (48) tenure org/apache/openjpa/jdbc/meta/MappingRepository ... (d) An example of object creation \nprofile (for SPECjbb2005). OBJ. CREATION LOCATION CLASS ============== ======== =============================== \n % samples 36.4% (40780) nursery [C 27.5% (30820) nursery java/lang/String 16.7% (18726) nursery java/math/BigDecimal \n   4.1% (4595) stack java/lang/Integer 1.8% (2013) nursery java/lang/Integer ...  spec/jbb/infra/Util/XMLTransactionLog.populateXML( \n31.9% (35628) OBJ. CREATION LOCATION CLASS ============== ======== =============================== % \n samples 15.8% (17695) nursery [C 14.9% (16734) nursery java/lang/String  0.9% (1014) stack java/lang/String \n...  [3] H. Q. Le, W. J. Starke, J. S. Fields, F. P. O Connell, D. Q. References Nguyen, B. J. Ronchetti, \nW. M. Sauer, E. M. Schwarz, and M. T. Vaden. IBM POWER6 microarchitecture . IBM [1] G. Ammons, T. Ball, \nand J. R. Larus. Exploiting hardware Journal of Research and Development, Vol. 51 (6), pp. 639 performance \ncounters with flow and context sensitive 662, 2007. profiling . In Proceedings of the ACM Conference \non Programming Language Design and Implementation, pp. 85 [4] A. Adl-Tabatabai, R. L. Hudson, M. J. Serrano, \nand S. 96, 1997. Subramoney. Prefetch injection based on hardware monitoring and object metadata . In \nProceedings of the ACM [2] N. Grcevski, A. Kielstra, K. Stoodley, M. Stoodley, and V. Conference on Programming \nLanguage Design and Sundaresan. Java just-in-time compiler and virtual machine Implementation, pp. 267 \n276, 2004. improvements for server and middleware applications . In Proceedings of the USENIX Virtual \nMachine Research and [5] T. Ogasawara, H. Komatsu, and T. Nakatani. To-lock: Technology Symposium, pp. \n151 162, 2004. Removing lock overhead using the owners temporal locality . In Proceedings of the Conference \non Parallel Architectures and Compilation Techniques, pp. 255-266, 2004. [6] K. Kawachiya, A. Koseki, \nand T. Onodera. Lock reservation: Java locks can mostly do without atomic operations . In Proceedings \nof the Conference on Object-Oriented Programming, Systems, Languages, and Applications, pp. 292 310, \n2002. [7] R. Jones and C. Ryder. A Study of Java Object Demographics . In Proceedings of the ACM International \nSymposium on Memory Management, pp. 121-130, 2008. [8] M. L. Seidl and B. G. Zorn. Segregating heap objects \nby reference behavior and lifetime . In Proceedings of the eighth Architectural Support for Programming \nLanguages and Operating Systems, pp 12-23, 1998. [9] F. E. Levine. A programmer's view of performance \nmonitoring in the PowerPC microprocessor . IBM Journal of Research and Development, Vol 41 (3), pp. 345-356, \n1997. [10] OProfile - A System Profiler for Linux. http://oprofile.sourceforge.net/news/ [11] Intel Corp. \nIA-32 Intel Architecture Software Developer's Manual. [12] JVM Tool Interface version 1.0. http://java.sun.com/j2se/1.5.0/docs/guide/jvmti/jvmti.html \n[13] M. Jump, S. M. Blackburn, and K.S. McKinley. Dynamic object sampling for pretenuring , In Proceedings \nof the International Symposium on Memory Management, pp. 152 162, 2004. [14] M. Hauswirth and T. M. Chilimbi. \nLow-overhead memory leak detection using adaptive statistical profiling , in Proceedings of the international \nconference on Architectural support for programming languages and operating systems table of contents, \npp. 156 164, 2004. [15] M. Arnold, and B. G. Ryder. A framework for reducing the cost of instrumented \ncode . In Proceedings of the ACM Conference on Programming Language Design and Implementation, pp. 168 \n179, 2001. [16] J. M. Spivey. Fast, Accurate Call Graph Profiling . Software: Practice and Experience, \nVol. 34 (3), pp. 249 264, 2004. [17] M. D. Bond, and K. S. McKinley. Probabilistic Calling Context . \nIn Proceedings of the ACM Conference on Object Oriented Programming Systems Languages and Applications, \npp. 97 112, 2007. [18] X. Zhuang, M. J. Serrano, H. W. Cain, and J Choi. Accurate, efficient, and adaptive \ncalling context profiling . In Proceedings of the ACM Conference on Programming Language Design and Implementation, \npp. 263 271, 2006. [19] M. Arnold and P. F. Sweeney. Approximating the calling context tree via sampling \n. IBM Research Report, 2000. [20] J. Whaley. A portable sampling-based profiler for java virtualmachines \n. In Proceedings of ACM Java Grande, pp. 78 87, 2000. [21] T. Mytkowicz, D. Coughlin, and A. Diwan. Inferred \nCall Path Profiling , In Proceedings of the Conference on Object-Oriented Programming, Systems, Languages, \nand Applications, to appear, 2009. [22] F. T. Schneider, M. Payer, and T. R. Gross. Online optimizations \ndriven by hardware performance monitoring . In Proceedings of the ACM Conference on Programming Language \nDesign and Implementation, pp. 373 382, 2007. [23]J. Cuthbertson, S. Viswanathan, K. Bobrovsky, A. Astapchuk, \nE. Kaczmarek, and U. Srinivasan. A Practical Approach to Hardware Performance Monitoring Based Dynamic \nOptimizations in a Production JVM . In Proceedings of the International Symposium on Code Generation \nand Optimization, pp. 190 199, 2009. [24] M. Serrano and X. Zhuang, Placement Optimization Using Data \nContext Collected During Garbage Collection , In Proceedings of the International Symposium on Memory \nManagement, pp. 69 78, 2009. [25] J. Dolby. Automatic Inline Allocation of Objects , In Proceedings of \nthe ACM SIGPLAN Conference on Programming Language Design and Implementation, pp 7 17, 1997. [26] Power.org, \nPower Instruction Set Architecture Version 2.05. http://www.power.org/resources/reading/PowerISA_V2.05.p \ndf [27] N. Grcevski, Effective method for Java Lock Reservation for Java Virtual Machines that Have Cooperative \nMultithreading 6th Workshop on Compiler-Driven Performance, 2007. [28] D. F. Bacon, R. Konuru, C. Murthy, \nand M. Serrano. Thin Locks: Featherweight Synchronization for Java . In Proceedings of the ACM Conference \non Programming Language Design and Implementation, pp. 258 268, 1998. [29] T. Onodera and K. Kawachiya. \nA study of locking objects with bimodal fields . In Proceedings of the ACM Conference on Object Oriented \nProgramming Systems Languages and Applications, pp. 223 237, 1999. [30] Performance Inspector, http://perfinsp.sourceforge.net/ \n[31] S. L. Graham, P. B. Kessler, and M K. McKusick. An execution profiler for modular programs . Software: \nPractice and Experience, Vol. 13 (8), pp. 671 685, 1983. [32] Standard Performance Evaluation Corporation. \nSPECjbb2005. http://www.spec.org/jbb2005/ [33] Standard Performance Evaluation Corporation. SPECjvm2008. \nhttp://www.spec.org/jvm2008/ [34] The Apache Software Foundation. DayTrader. http://cwiki.apache.org/GMOxDOC20/daytrader.html \n[35] IBM Corporation. WebSphere Application Server. http://www-01.ibm.com/software/webservers/appserv/was/ \nJava is a trademark of Sun Microsystems, Inc. Other company, product, and service names may be trademarks \nor service marks of others.   \n\t\t\t", "proc_id": "1640089", "abstract": "<p>This paper describes our sampling-based profiler that exploits a processor's HPM (Hardware Performance Monitor) to collect information on running Java applications for use by the Java VM. Our profiler provides two novel features: Java-level event profiling and lightweight context-sensitive event profiling. For Java events, we propose new techniques to leverage the sampling facility of the HPM to generate object creation profiles and lock activity profiles. The HPM sampling is the key to achieve a smaller overhead compared to profilers that do not rely on hardware helps. To sample the object creations with the HPM, which can only sample hardware events such as executed instructions or cache misses, we correlate the object creations with the store instructions for Java object headers. For the lock activity profile, we introduce an instrumentation-based technique, called ProbeNOP, which uses a special NOP instruction whose executions are counted by the HPM. For the context-sensitive event profiling, we propose a new technique called <i>CallerChaining</i>, which detects the calling context of HPM events based on the call stack depth (the value of the stack frame pointer). We show that it can detect the calling contexts in many programs including a large commercial application. Our proposed techniques enable both programmers and runtime systems to get more valuable information from the HPM to understand and optimize the programs without adding significant runtime overhead.</p>", "authors": [{"name": "Hiroshi Inoue", "author_profile_id": "81100619710", "affiliation": "IBM, Kanagawa, Japan", "person_id": "P1728746", "email_address": "", "orcid_id": ""}, {"name": "Toshio Nakatani", "author_profile_id": "81100311827", "affiliation": "IBM, Kanagawa, Japan", "person_id": "P1728747", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1640089.1640100", "year": "2009", "article_id": "1640100", "conference": "OOPSLA", "title": "How a Java VM can get more from a hardware performance monitor", "url": "http://dl.acm.org/citation.cfm?id=1640100"}