{"article_publication_date": "10-25-2009", "fulltext": "\n Empirical Assessment of Object-Oriented Implementations with Multiple Inheritance and Static Typing \n Roland Ducournau Flor\u00e9al Morandat Jean Privat LIRMM Universit\u00e9 Montpellier 2, France Universit\u00e9 du \nQu\u00e9bec \u00e0 Montr\u00e9al, Canada {ducournau,morandat}@lirmm.fr privat.jean@uqam.ca Abstract Object-oriented \nlanguages involve a threefold tradeoff be\u00adtween runtime ef.ciency, expressiveness (multiple inheri\u00adtance), \nand modularity, i.e. open-world assumption (OWA). Runtime ef.ciency is conditioned by both the implementa\u00adtion \ntechnique and compilation scheme. The former speci.es the data structures that support method invocation, \nattribute access and subtype testing. The latter consists of the produc\u00adtion line of an executable from \nthe source code. Many imple\u00admentation techniques have been proposed and several com\u00adpilation schemes \ncan be considered from fully global com\u00adpilation under the closed-world assumption (CWA) to sepa\u00adrate \ncompilation with dynamic loading under the OWA, with midway solutions. This article reviews a signi.cant \nsubset of possible combinations and presents a systematic, empiri\u00adcal comparison of their respective \nef.ciencies with all other things being equal. The testbed consists of the PRM com\u00adpiler that has been \ndesigned for this purpose. The considered techniques include C++ subobjects, coloring, perfect hash\u00ading, \nbinary tree dispatch and caching. A variety of proces\u00adsors were considered. Qualitatively, these .rst \nresults con\u00ad.rm the intuitive or theoretical abstract assessments of the tested approaches. As expected, \nef.ciency increases as CWA strengthens. From a quantitative standpoint, the results are the .rst to precisely \ncompare the ef.ciency of techniques that are closely associated with speci.c languages like C++ and EIFFEL. \nThey also con.rm that perfect hashing should be considered for implementing JAVA and .NET interfaces. \nCategories and Subject Descriptors D.3.2 [Programming languages]: Language classi.cations object-oriented \nlan\u00adguages, C++, C#, JAVA,EIFFEL; D.3.3 [Programming languages]: Language Constructs and Features classes \nand objects, inheritance; D.3.4 [Programming languages]: Permission to make digital or hard copies of \nall or part of this work for personal or classroom use is granted without fee provided that copies are \nnot made or distributed for pro.t or commercial advantage and that copies bear this notice and the full \ncitation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to \nlists, requires prior speci.c permission and/or a fee. OOPSLA 2009, October 25 29, 2009, Orlando, Florida, \nUSA. Copyright c &#38;#169; 2009 ACM 978-1-60558-734-9/09/10. . . $10.00 Processors compilers, linkers, \nrun-time environments; E.2 [Data]: Data Storage Representations object represen\u00adtations General Terms \nExperimentation, Languages, Measure\u00adment, Performance Keywords binary tree dispatch, closed-world assumption, \ncoloring, downcast, dynamic loading, interfaces, late bind\u00ading, method tables, multiple inheritance, \nmultiple subtyping, open-world assumption, perfect hashing, single inheritance, subtype test, type analysis, \nvirtual function tables 1. Introduction In spite of its 30-year maturity, object-oriented programming \nstill has a substantial ef.ciency drawback in the multiple in\u00adheritance context and it is worsened by \ndynamic loading. In a recent article [Ducournau 2008], we identi.ed three re\u00adquirements that all implementations \nof object-oriented lan\u00adguages, especially in this context, should ful.l namely (i) constant time; (ii) \nlinear space; and (iii) inlining. Indeed, object-oriented implementation concerns a few basic mech\u00adanisms \nthat are invoked billions of times during a 1-minute program execution. Constant time is the only way \nto bound the worst-case behaviour. Space linearity ensures that the implementation will scale up gracefully \nwith the program size; we shall see, however, that linearity must be under\u00adstood in a slightly speci.c \nmeaning. Finally, the basic mech\u00adanisms must be implemented by a short sequence of instruc\u00adtions that \nmust be inlined. This general implementation is\u00adsue is exempli.ed by the way the two most used languages, \nnamely C++ and JAVA, that support both multiple inheri\u00adtance and dynamic loading, do not meet these criteria. \nWhen the virtual keyword is used for inheritance, C++ provides a fully reusable implementation, based \non subobjects, which however involves many compiler-generated .elds in the ob\u00adject layout and pointer \nadjustments at run-time1. Moreover, it does not meet the linear-space requirement and there was, until \nvery recently, no ef.cient subtype testing available for this implementation. JAVA provides multiple \ninheritance of interfaces only but, even in this restricted setting, current 1 The effect of omitting \nthe virtual keyword is discussed below.  interface implementations are generally not time-constant (see \nfor instance [Alpern et al. 2001a]). The present research was motivated by the following observation. \nThough object\u00adoriented technology is mature, the ever-increasing size of object-oriented class libraries \nand programs makes the need for scalable implementations urgent and there is still con\u00adsiderable doubt \nover the scalability of existing implementa\u00adtions. The implementation of object-oriented languages relies \nupon three speci.c mechanisms: method invocation, subtype testing and attribute access. Method invocation \nimplies late binding; that is the address of the actually called procedure is not statically determined \nat compile-time, but depends on the dynamic type of a distinguished parameter known as the receiver. \nSubtyping and inheritance introduce another origi\u00adnal feature, i.e. run-time subtype checks, which amounts \nto testing whether the value of x is an instance of some class C or, equivalently, whether the dynamic \ntype of x is a subtype of C. This is the basis for so-called downcast operators. An issue similar to \nlate binding arises with attributes (aka .elds, instance variables, slots, data members according to \nthe lan\u00adguages), since their position in the object layout may depend on the dynamic type of the object. \nSingle vs. Multiple Inheritance. Message sending, at\u00adtribute access and subtype testing need speci.c \nimplemen\u00adtations, data structures and algorithms. In statically typed languages, late binding is usually \nimplemented with tables called virtual function tables in C++ jargon. These tables reduce method calls \nto pointers to functions, through a small .xed number, usually 2, of extra indirections. It follows that \nobject-oriented programming yields some overhead, as compared to usual procedural languages. When static \ntyping is combined with single inheritance this is single subtyp\u00ading two major invariants hold; (i) a \nreference to an object does not depend on the static type of the reference; (ii) the position of attributes \nand methods in the tables does not depend on the dynamic type of the object. These invari\u00adants allow \ndirect access to the desired data and optimize the implementation. Hence, all three mechanisms are time\u00adconstant \nand their constant is small and optimal. The code sequence is short and easily inlinable. Finally, the \noverall memory occupation is linear in the size of the specialization relationship; this can be understood \nas a consequence of the fact that constant-time mechanisms require some compila\u00adtion of inheritance. \nOtherwise, dynamic typing or multiple inheritance make it harder to retain these two invariants. Implementation \nis thus not a problem with single sub\u00adtyping. However, there are almost no such languages. The few examples, \nsuch as OBERON [M\u00f6ssenb\u00f6ck 1993], MOD-ULA-3 [Harbinson 1992], or ADA 95, result from the evolu\u00adtion of \nnon-object-oriented languages and object orientation is not their main feature. In static typing, some \ncommonly usedpureobject-orientedlanguages,suchas C++or EIFFEL [Meyer 1992, 1997], offer the programmer \nplain multiple inheritance. More recent languages like JAVA andC#offer a limited form of multiple inheritance, \nwhereby classes are in single inheritance and types, i.e. interfaces,are in multi\u00adple subtyping. Furthermore, \nthe absence of multiple subtyp\u00ading was viewed as a de.ciency in the ADA 95 revision, and this feature \nwas incorporated in the next version [Taft et al. 2006]. This is a strong argument in favour of the importance \nof multiple inheritance. Hence, there is a real need for ef.\u00adcient object implementation in the multiple \ninheritance and static typing context. The multiple inheritance requirement is less urgent in the dynamic \ntyping context. An explana\u00adtion is that the canonical static type system corresponding to a language \nlike SMALLTALK [Goldberg and Robson 1983] would be that of JAVA, i.e. multiple subtyping. Anyway, dy\u00adnamic \ntyping gives rise to implementation issues which are similar to that of multiple inheritance, even though \nthe so\u00adlutions are not identical, and the combination of both, as in CLOS [Steele 1990], hardly worsens \nthe situation. This arti\u00adcle focuses on static typing and multiple inheritance. Compilation Schemes. \nBesides the implementation tech\u00adniques, which concern low-level data structures and code sequences, the \noverall run-time ef.ciency strongly depends on what we call here compilation schemes that involve the \nproduction of an executable from the source code .les and include various processors like compilers, \nlinkers and load\u00aders. We consider that the object-oriented philosophy is best expressed under the open-world \nassumption (OWA). Each class must be designed and implemented while ignoring how it will be reused, especially \nwhether it will be specialized in single or multiple inheritance. OWA is ensured by sepa\u00adrate compilation \nand dynamic loading/linking.However, as JAVA and C++ exemplify it, we do not know any implemen\u00adtation \nof multiple inheritance under the OWA that would be perfectly ef.cient and scalable, i.e. time-constant \nand space\u00adlinear. In contrast, the closed-world assumption (CWA), that is ensured by global compilation, \nallows both ef.cient im\u00adplementations and various optimizations that partly offset the late binding overhead. \nThis approach is exempli.ed by the GNU EIFFEL compiler [Zendra et al. 1997, Collin et al. 1997]. A variety \nof combinations fall between these two extremes. For instance, the program elements can be sepa\u00adrately \ncompiled under the OWA while the executable is pro\u00adduced by an optimized global linker [Boucher 2000, \nPrivat and Ducournau 2005]. Alternatively, some parts of the pro\u00adgram, namely libraries, can be separately \ncompiled under the OWA, whereas the rest is globally compiled under the CWA. A last example is given \nby adaptive compilers [Arnold et al. 2005] that can be thought of as separate compilation under a temporary \nCWA, which can be questioned when further loading invalidates the assumptions partial recompilation is \nthus required. In this paper, we do not consider adaptive compilers and we mostly consider compilation \nschemes that do not involve any recompilation.  Implementation techniques and compilation schemes are \nclosely related; when excluding recompilations, not all pairs are compatible. Compilation schemes can \nbe ordered from full OWA to full CWA and the compatibility of techniques w.r.t. schemes is monotonic; \nwhen a technique is compatible with a scheme, it is also compatible with all schemes that are more closed \nthan the considered one. Languages. In principle, language speci.cations should be independent of implementation. \nHowever, in practice, many languages are closely dependent on a precise imple\u00admentation technique or \ncompilation scheme. For instance, the virtual keyword makes C++ inseparable from its subobject-based \nimplementation [Ellis and Stroustrup 1990, Lippman 1996], whereas EIFFEL cannot be considered other than \nwith global compilation, because of its unrestricted co\u00advariance which would make it unsafe and inef.cient \nwith separate compilation. Therefore, an objective comparison of the respective ef.ciencies of these \nlanguages is almost impossible because full speci.cations are not comparable. This article focuses on \nthe core of object-oriented program\u00adming that is common to all languages. Therefore, the target languages \ncan be thought of as mainstream languages like C++, JAVA,C# or EIFFEL. However, these languages only \nrepresent convenient well-known examples. The considered techniques should actually appear as universal \ntechniques, that could apply to all languages, apart from general func\u00adtional requirements like typing \n(static vs dynamic), inheri\u00adtance (single vs multiple), compilation (separate vs global) and linking \n(static vs dynamic) that markedly hamper the implementation. Contributions. Since the beginning of object-oriented \npro\u00adgramming, many implementation techniques have been pro\u00adposed. Some of them are commonly used in production \nrun\u00adtime systems, in JAVA and C# virtual machines or C++ and EIFFEL compilers. Many others have been \nstudied in theory, their time-ef.ciency may have been assessed in an abstract framework like [Driesen \n2001] and their space-ef.ciency may have been tested on some benchmarks made of large class hierarchies. \nMost often, however, no empirical assess\u00adment has been made or, alternatively, the empirical assess\u00adment \nof the considered technique did not yield a fair compar\u00adison with alternative techniques, with all other \nthings being equal. There are many reasons for such a situation. Imple\u00admenting an object-oriented language \nis hard work and im\u00adplementing alternative techniques is markedly harder the compiler needs an open architecture \nand fair measurements require perfect reproducibility. This article is thus a step in a project intended \nto produce fair assessments of various alternative implementation tech\u00adniques, with all other things \nbeing equal. The previous steps included abstract analysis in Driesen s framework, and sim\u00adulation of \nthe memory occupation based on large-scale class hierarchies [Ducournau 2006, 2008, 2009]. In the past \nfew years, we developed a new language, called PRM, and a com\u00adpiler with an open modular architecture \nwhich makes it rel\u00adatively easy to test alternative techniques. Early results pre\u00adsented empirical measures \nof program ef.ciency based on arti.cial micro-benchmarks [Privat and Ducournau 2005]. In this article, \nwe present an empirical assessment of the time\u00adef.ciency of a real program on a variety of processors \nand according to the underlying implementation techniques and compilation schemes that are used to produce \nthe executable. Our testbed involves meta-compiling. It consists of the PRM compiler, which compiles \nPRM source code to C code and is applied to itself. The tests consider the following tech\u00adniques: (i) \ncoloring [Ducournau 2006] which represents an extension of the single-subtyping implementation to multi\u00adple \ninheritance under partial CWA; (ii) binary tree dispatch (BTD) [Zendra et al. 1997, Collin et al. 1997] \nwhich requires stronger CWA; (iii) perfect hashing [Ducournau 2008] that has been recently proposed for \nJAVA interfaces under pure OWA, (iv) incremental coloring [Palacz and Vitek 2003] also proposed for JAVA \ninterfaces, that is an incremental ver\u00adsion of coloring which requires load-time recomputations, (v) \ncaching, which amounts to memoizing the last access and might improve less ef.cient techniques. C++ subobjects \nare discussed but not tested because they have not yet been integrated in the testbed. The contribution \nof this article is thus reliable time mea\u00adsurements of different executables produced from the same program \nbenchmark, according to different implementations and compilations. From a qualitative standpoint, the \nconclu\u00adsions are not new, and our tests mostly con.rm the intuitive or theoretical abstract assessments \nof the tested approaches. As expected, ef.ciency increases as CWA strengthens. How\u00adever, from a quantitative \nstandpoint, the conclusions are quite new, as these tests represent, to our knowledge, the .rst systematic \ncomparisons between very different approaches with all other things being equal. Another contribution \nof this work is a careful analysis of the testbed to ensure that all other things are actually equal. \nAmong other results, these tests give the .rst empirical assessment of (i) a recently pro\u00adposed technique, \nperfect hashing; (ii) the overhead of OWA vs. CWA; (iii) the overhead of multiple vs. single inheri\u00adtance; \nand (iv) a .rst step towards an empirical comparison between C++ and EIFFEL implementations. Plan. This \narticle is structured as follows. Section 2 sur\u00adveys the implementation techniques that are tested here \nand discusses their expected ef.ciency. Section 3 presents com\u00adpilation schemes and their compatibility \nwith the different implementation techniques. Section 4 describes the testbed and some statistics on \nthe tested program, then discusses the precise experimental protocol, its reliability and repro\u00adducibility. \nSection 5 presents the time measures and dis\u00adcusses the relative overhead of the different combinations. \nFinally, the last section presents related work, .rst conclu\u00adsions and prospects. Preliminary results \nhave been presented in French in [Morandat et al. 2009].  A A a Ba BC b C D  c b  cd Two class \nhierarchies with associated instances, in single (left) and multiple (right) inheritance. Solid arrows \nrepresent class specialization and dashed arrows represent instantiation. Figure 1. Single and multiple \ninheritance hierarchies  2. Implementation Techniques Implementation techniques are concerned with object \nrep\u00adresentation, that is the object layout and the associated data structures that support method invocation, \nattribute access and subtype testing. 2.1 Single Subtyping In separate compilation of statically typed \nlanguages, late binding is generally implemented with method tables, aka virtual function tables (VFT) \nin C++ jargon. Method calls are then reduced to calls to pointers to functions through a small .xed number \n(usually 2) of extra indirections. An object is laid out as an attribute table, with a pointer at the \nmethod table. With single inheritance, the class hierarchy is a tree and the tables implementing a class \nare straightfor\u00adward extensions of those of its single direct superclass (Fig\u00adure 2). The resulting implementation \nrespects two essential invariants:(i) a reference to an object does not depend on the static type of \nthe reference; (ii) the position of attributes and methods in the table does not depend on the dynamic \ntype of the object. Therefore, all accesses to objects are straight\u00adforward. This accounts for method \ninvocation and attribute access under the OWA. The ef.cacy of this implementation is due to both static \ntyping and single inheritance. Dynamic typing adds the same kind of complication as multiple in\u00adheritance, \nsince the same property name may be at different places in unrelated classes. Regarding subtype testing, \nthe technique proposed by [Cohen 1991] also works under the OWA. It involves as\u00adsigning a unique ID to \neach class, together with an invariant position in the method table, in such a way that an object x is \nan instance of the class C if and only if the method table of x contains the class ID of C, at a position \nuniquely determined by C. Readers are referred to [Ducournau 2008] for imple\u00admentation details, especially \nfor avoiding bound checks. In this implementation, the total table size is roughly lin\u00adear in the cardinality \nof the specialization relationship, i.e. linear in the number of pairs (x,y) such that x is a subtype \n(subclass) of y (x j y). Cohen s display uses exactly one en\u00adtry per such pair and the total table size \nis linear if one as-  CB A The single subtyping implementation of the example from Fig. 1-left. Object \nlayouts (right) are drawn from left to right and method tables (left) from right to left. In the object \nlayouts (resp. method tables) the name of a class represents the set of attributes (resp. methods) introduced \nby the class. Figure 2. Single subtyping implementation // attribute access load [object + #attrOffset], \nvalue // method invocation load [object + #tableOffset], table load [table + #methOffset], method call \nmethod // subtype test load [object + #tableOffset], table load [table + #classOffset], classId comp \nclassId, #targetId bne #fail // succeed class id method class Offset   table  meth Offset attr \nOffset  value object The code sequences are expressed in the intuitive pseudo-code proposed by [Driesen \n2001]. The diagram depicts the corresponding object representation. Pointers and pointed values are in \nroman type with solid lines, and offsets are italicized with dotted lines. Each mechanism relies on a \nsingle invariant offset. The grey parts represent the groups of attributes and methods introduced by \na given class. Cohen s display amounts to reserving an entry in the method group for the class ID. Figure \n3. Code sequences and object representation in sin\u00adgle subtyping sumes that methods and attributes are \nuniformly introduced in classes. Moreover, the size occupied by a class is also lin\u00adear in the number \nof its superclasses. More generally, linear\u00adity in the number of classes is actually not possible since \nef.cient implementation requires some compilation of in\u00adheritance, i.e. some superclass data must be \ncopied in the tables for subclasses. Therefore, usual implementations are, in the worst case (i.e. deep \nrather than broad class hierar\u00adchies), quadratic in the number of classes, but linear in the size of \nthe inheritance relationship. The inability to do bet\u00adter than linear-space is likely a consequence of \nthe constant\u00adtime requirement. As a counter-example, [Muthukrishnan and Muller 1996] propose an implementation \nof method in\u00advocation with O(N + M) table size, but O(loglogN) invo\u00adcation time, where N is the number \nof classes and M is the number of method de.nitions.  subobjects Object layout and method table of \na single instance of the class D from the diamond example of Fig. 1-right. Figure 4. Subobject-based \nimplementation Notwithstanding ef.ciency considerations that are indeed central to this article, the \nthree mechanisms that we consider (method invocation, attribute access and subtype testing) are equivalent \nin the extent that they are reducible to each other. Obviously, method tables are object layout at the \nmeta-level. Hence, apart from memory-allocation considerations, they are equivalent. Moreover, an attribute \ncan be read and writ\u00adten through dedicated accessor methods; hence, attribute ac\u00adcess can always reduce \nto method invocation (Section 2.7). An interesting analogy between subtype tests and method calls can \nalso be drawn from Cohen s display. Suppose that each class C introduces a method amIaC? that returns \nyes. In dynamic typing, calling amIaC? on an unknown receiver x is exactly equivalent to testing if x \nis an instance of C;in the opposite case, an exception will be signaled. In static typing, the analogy \nis less direct, since calling amIaC? is only legal on a receiver statically typed by C, or a subtype \nof C;this is type safe but quite tautological. However, subtype testing is inherently type unsafe and \none must understand amIaC? as a pseudo-method, which is actually not invoked but whose presence is checked. \nThe test fails when this pseudo-method is not found, i.e. when something else is found at its expected \nposition. This informal analogy is important, as it implies that one can derive a subtype testing implementation \nfrom almost any method call implementation. We actually know a single counter-example, when the implementation \ndepends on the static type of the receiver, as in subobject-based im\u00adplementations (Section 2.2). In \nthe following, we will use this general equivalence in several ways.  2.2 Subobjects (SO) With multiple \ninheritance, both invariants of reference and position cannot hold together, at least if compilation \n(i.e. computation of positions) is to be kept separate. For in\u00adstance, in the diamond hierarchy of Figure \n1-right, if the implementations of B and C simply extend that of A,as in single inheritance, the same \noffsets will be occupied by dif\u00adferent properties introduced in B and C, thus prohibiting a sound implementation \nof D. Therefore, the standard im\u00adplementation (i.e. that of C++) of multiple inheritance in a static \ntyping and separate compilation setting is based on subobjects (SO). The object layout is composed of \nseveral // pointer adjustment load [object + #tableOffset], table load [table + #castOffset], delta1 \nadd object, delta1, object1 // inherited attribute access // lines 1-3 load [object1 + #attrOffset], \nvalue // method invocation load [object + #tableOffset], table load [table + #methOffset+fieldLen], \ndelta2 load [table + #methOffset], method add object, delta2, object2 call method  The diagram depicts \nthe precise object representation restricted to method invocation, attribute access and pointer adjustment. \nobject is the current reference to the con\u00adsidered object. delta1 is the pointer adjustment that is required \nto go from object to object1 subobjects, e.g. for accessing an attribute de.ned in the class correspond\u00ading \nto the latter. delta2 is the pointer adjustment that is required to go from object subobject to that \nof the class which de.nes the invoked method. Figure 5. Code and object representation with subobjects \nsubobjects, one for each superclass of the object class. Each subobject contains attributes introduced \nby the correspond\u00ading superclass, together with a pointer to a method table which contains the methods \nknown by the superclass (Fig. 4 and 5). Both invariants are dropped, as both reference and position depend \nnow on the current static type. This is the C++ implementation, when the virtual keyword annotates each \nsuperclass [Ellis and Stroustrup 1990, Lippman 1996, Ducournau 2009]. It is time-constant and compatible \nwith dynamic loading, but method tables are no longer space\u00adlinear. Indeed, the number of method tables \nis exactly the size of the specialization relationship. When a class is in single inheritance, its total \ntable size is itself quadratic in the number of superclasses; hence, in the worst case, the total size \nfor all classes is cubic in the number of classes. Furthermore, all polymorphic object manipulations \n(i.e. as\u00adsignments and parameter passing, when the source type is a strict subtype of the target type) \nrequire pointer adjustments between source and target types, as they correspond to dif\u00adferent subobjects. \nThese pointer adjustments are purely me\u00adchanical and do not bring any semantics, but they are quite numerous. \nThey are also safe i.e. the target type is always a supertype of the source type and can be implemented \nmore ef.ciently than subtyping tests. Pointer adjustments can also be done with explicit point\u00aders, called \nVBPTRs, in the object layout, instead of off\u00adsets in the method tables as in Fig. 5. Although VBPTRs \nare more time-ef.cient since they save an access to method table at each pointer adjustment, they are \nalso over space\u00adconsuming. Therefore, we only consider here implementa\u00adtions that rely on offsets in \nthe method table. They are also closest to most actual implementations.  Usually, pointer adjustments \nconsist of upcasts, i.e. they involve an assignment from a subtype to a supertype. How\u00adever, covariant \nreturn types, though type-safe, need down\u00adcasts. Indeed the type returned by the callee is a subtype \nof the static type in the caller, but the returned type is statically unknown in the caller, hence the \nadjustment must be done as if it were a downcast. Furthermore, contrary to single inheri\u00adtance, there \nis no known way of deriving a subtype test from the technique used for method invocation. It is no longer \npos\u00adsible to consider that testing if an object is an instance of C is a kind of method introduced by \nC, because this pseudo\u00admethod would not have any known position other than in static subtypes of C. In \nour tests, we will thus supplement subobject-based implementations with perfect hashing (Sec\u00adtion 2.5) \nfor subtype testing. Empty-subobject optimization (ESO) [Ducournau 2009] represents a further improvement \nthat applies when a class does not introduce any attribute hence the corresponding subobject is empty \nespecially when it does not introduce any method and have a single direct superclass. In this case, both \nsubobjects can be merged and the statistics presented in the aforementioned article show that the improvement \nis signi.cant. Although the designers of C++ compilers do not seem to be aware of the possibility of \nESO, it is required in the PRM testbed for ef.cient boxing and unboxing of primitive types. Indeed, unlike \nC++ and like JAVA 1.5 and EIFFEL,the PRM type system considers that primitive types are subtypes of some \ngeneral types like Object or Any. Unoptimized subobjects would yield heavy boxes. Subobjects can also \napply to JAVA interfaces, with an improved empty-subobject optimization that relies on class single inheritance. \nThe technique, detailed in [Ducournau 2009] based on the bidirectional layout of [Myers 1995], is less \nexpensive than general subobject-based implementa\u00adtions, though likely not space-linear. It would be \ninteresting to test it but it is incompatible with our PRM testbed, be\u00adcause of the distinction between \nclasses and interfaces, and with current JVMs, because it is not reference-invariant. When the virtual \nkeyword is not used we call it non\u00advirtual inheritance C++ provides a markedly more ef.\u00adcient implementation \nwith different semantics. In the dia\u00admond situation of Fig. 1, this semantics yields repeated in\u00adheritance \nof the diamond root. However, repeated inheri\u00adtance is not a sensible feature because it might force \nthe pro\u00adgrammer to syntactically distinguish between an exponential number of interpretations consider \na chain of n diamonds. Hence, one must consider that non-virtual inheritance is not compatible with both \nmultiple inheritance and OWA. As col\u00adoring is certainly more ef.cient than non-virtual implemen\u00adtation \nunder the CWA, we only consider C++, in the follow\u00ading, under the virtual implementation and the OWA. \n With the same class diamond as in Fig. 1 and 4, implementation of A and B is presumed to be the same \nas in Fig. 2. Hence, the implementation of C leaves a gap (in grey) for B in anticipation of D. The code \nsequences and object representation are the same as in Fig. 3. Figure 6. Coloring implementation  2.3 \nColoring (MC/AC) The coloring approach is quite versatile and naturally ex\u00adtends the single inheritance \nimplementation to multiple in\u00adheritance, while meeting all requirements except compati\u00adbility with dynamic \nloading. The technique takes its name from graph coloring, as its computation amounts to color\u00ading some \nparticular graph2. Method coloring was .rst pro\u00adposed by [Dixon et al. 1989] for method invocation, under \nthenameof selector coloring. [Pugh and Weddell 1990] and [Ducournau 1991] applied coloring to attribute \naccess and [Vitek et al. 1997] to subtype testing (under the name of pack encoding). Hereafter, MC denotes \ncoloring when used for method invocation and subtype testing, and AC denotes attribute coloring. The \ngeneral idea of coloring is to keep the two invariants of single inheritance, i.e. reference and position. \nAn injective numbering of attributes, methods and classes veri.es the po\u00adsition invariant, so this is \nclearly a matter of optimization for minimizing the size of all tables or, equivalently, the num\u00adber \nof holes (i.e. empty entries). However, this optimization cannot be done separately for each class; it \nrequires a global computation for the whole hierarchy. The problem of min\u00adimizing the total table size \nis akin to the minimum graph coloring problem [Garey and Johnson 1979]. Like minimal graph coloring, \nthe coloring problem considered here has been proven to be NP-hard in the general case. Therefore heuristics \nare needed and various experiments have shown the overall tractability. Finally, an important improvement \nis bidirectionality, introduced by [Pugh and Weddell 1990], which involves using positive and negative \noffsets and re\u00adduces the hole number. Figure 6 depicts the implementation yielded by unidirectional coloring \nin the diamond example from Figure 4. The implementation of classes A and B is pre\u00adsumed to be identical \nto that of Figure 2. Hence, computing the tables for C must reserve some space for B in the tables of \nD, their common subclass. Thus, some holes appear in the C tables and these holes are .lled, in D, by \nall data speci.c to B. In bidirectional coloring, all holes would have been saved by placing C at negative \noffsets. 2 This graph is a con.ict graph with a vertex for each class and an edge between any two vertices \nthat have a common subclass and thus must have their attributes (resp. methods or class IDs) stored at \ndistinct offsets, since attributes (resp. methods or class IDs) of both classes coexist in objects (resp. \nmethod tables) of the common subclass.  load [object + #idOffset], id comp id, id0 bgt #branch1 comp \nid id1 bgt #branch2 call #method1 jump #end branch2: call #method2 jump #end branch1: comp id id2 bgt \n#branch3 call #method4 jump #end branch3: call #method3 end: Figure 7. Binary tree dispatch (BTD2) A \ndetailed presentation of coloring is beyond the scope of this paper and readers are referred to [Ducournau \n2006] for a review of the approach. The point to get is 2-fold; (i) in practice, object layout, method \ntables and code sequences are exactly those of single subtyping, except for the pres\u00adence of holes; (ii) \nthis is obtained by rather sophisticated algorithms which require complete knowledge of the class hierarchy. \nActually, we have exchanged multiple inheritance for dynamic loading.  2.4 Binary Tree Dispatch (BTD) \nNot all object-oriented implementations are based on VFT. In SMART EIFFEL, the GNU EIFFEL compiler, method \nta\u00adbles are not used. Instead, objects are tagged by their class identi.er and all three mechanisms, \nparticularly method in\u00advocation, are implemented using balanced binary dispatch trees [Zendra et al. \n1997, Collin et al. 1997]. However, the approach is practical only because the compilation is global, \nhence all classes are statically known. Furthermore, type analysis restricts the set of concrete types \n[Bacon and Sweeney 1996, Grove and Chambers 2001] and makes dis\u00adpatch ef.cient. BTD is also an interesting \nexample of the possible disconnection between code length, thus inlining, and time ef.ciency. Indeed, \nhere, both values are in an expo\u00adnential relationship, hence proving that not all ef.cient code sequences \nare inlinable. Anyway, BTD is not time-constant. The ef.ciency of BTD relies on the conditional branching \nprediction of modern processors. Thanks to their pipe-line architecture, well-predicted branchings are \nfree. On the con\u00adtrary, mispredictions break the pipe and cost about 10 cycles or more, and most undirect \nbranches are mispredicted this misprediction cost thus holds for all VFT-based techniques. Readers are \nreferred to [Driesen 2001] for a more in-depth analysis. An overall consequence is that BTD is statistically \nmore ef.cient than VFT when the number of tests is small. It depends, however, on the statistical distribution \nof dynamic types on each call site, and it is easy to construct worst-case arti.cial programs whereby \nall predictions fail, making VFT far better than BTD. In the following, BTDi will denote BTD of depth \nbounded by i.BTD0 corresponds to static calls and BTD8 denotes unbounded BTD. Figure 7 depicts a dispatch \ntree of depth 2. Overall, BTD is ef.cient when the number of expected types and competing methods is \nlow the corresponding call sites are then called oligomorphic but coloring should be preferred when this \nnumber is higher, for megamorphic call sites. An interesting tradeoff involves combining BTDk and coloring, \nwith k not greater than 3 or 4. This makes the resulting technique time-constant and inlinable. Further\u00admore, \nmethod tables are restricted to the subset of methods that have a megamorphic call site. BTD also applies \nto sub\u00adtype testing and attribute access but, in the context of global compilation, coloring is likely \nbetter.  2.5 Perfect Hashing (PH) In a recent article [Ducournau 2008] we proposed a new technique based \non perfect hashing for subtype testing in a dynamic loading setting. The problem can be formalized as \nfollows. Let (X,j) be a partial order that represents a class hierarchy, namely X is a set of classes \nand j the special\u00adization relationship that supports inheritance. The subtype test amounts to checking \nat run-time that a class c is a su\u00adperclass of a class d,i.e. d j c. Usually d is the dynamic type of \nsome object and the programmer or compiler wants to check that this object is actually an instance of \nc.The point is to ef.ciently implement this test by precomputing some data structure that allows for \nconstant time. Dynamic loading adds a constraint, namely that the technique should be inherently incremental. \nClasses are loaded at run-time in some total order that must be a linear extension (aka topo\u00adlogical \nsorting)of (X,j) that is, when d . c, c must be loaded before d. The perfect hashing principle is as \nfollows. When a class c is loaded, a unique identi.er idc is associated with it and the set Ic = {idd \n| c j d} of the identi.ers of all its super\u00adclasses is known. If needed, yet unloaded superclasses are \nrecursively loaded. Hence, c j d iff idd . Ic.This set Ic is immutable, hence it can be hashed with some \nperfect hash\u00ading function hc, i.e. a hashing function that is injective on Ic [Sprugnoli 1977, Czech \net al. 1997]. The previous condi\u00adtion becomes c j d iff htc[hc(idd )] = idd , whereby htc de\u00adnotes the \nhashtable of c. Moreover, the cardinality of Ic is denoted nc. The technique is incremental since all \nhashta\u00adbles are immutable and the computation of htc depends only on Ic. The perfect hashing functions \nhc are such that hc(x)= hash(x,Hc), whereby Hc is the hashtable size de.ned as the least integer such \nthat hc is injective on Ic.Two hash func\u00adtions were considered, namely modulus (noted mod) and bit\u00adwise \nand3. The corresponding techniques are denoted here\u00adafter PH-mod and PH-and. However, these two functions \ninvolve a time/space ef.ciency tradeoff. The former yields more compact tables but the integer division \nlatency may 3 With and, the exact function maps x to and(x,Hc - 1).  // preamble // preamble load [object \n+ #tableOffset], table load [object + #tableOffset], table load [table + #hashingOffset], h load [table \n+ #ctableOffset], ctable and #interfaceId, h, hv load [targetTable + #classColor], color mul hv, #2*fieldLen, \nhv add ctable, color, entry sub table, hv, htable // method invocation // method invocation load [entry \n+ #2], iOffset load [htable +#htOffset], iOffset add table, iOffset, methgr add htable, iOffset, itable \nload [methgr + #methOffset], method load [itable +#methOffset], method call method call method // subtype \ntest // subtype testing load [table + #clenOffset], clen load [htable +#htOffset-fieldLen], id comp \nclen, color comp #interfaceId, id ble #fail bne #fail load [entry], id // succeed comp id, #targetId \nbne #fail interface // succeed Id ioffset iOffset  iOffset id offset method h color table   hv \n hashing htOffset offset  method table The preamble is common to both mechanisms. The grey rectangle \ndenotes the group of methods introduced by the considered interface. Figure 8. Perfect hashing for JAVA \ninterfaces be more than 20 cycles, whereas the latter takes 1 cycle but yields larger tables. In a static \ntyping setting, the technique can also be ap\u00adplied to method invocation and we did propose, in the afore\u00admentioned \narticle, an application to JAVA interfaces. For this, the hashtable associates, with each implemented \ninterface, the offset of the group of methods that are introduced by the interface. Figure 8 recalls \nthe precise implementation in this context. The method table is bidirectional. Positive off\u00adsets involve \nthe method table itself, organized as with sin\u00adgle inheritance. Negative offsets consist of the hashtable, \nwhich contains, for each implemented interface, the offset of the group of methods introduced by the \ninterface. The ob\u00adject header points at its method table by the table pointer. #hashingOffset is the \nposition of the hash parameter (h) and #htOffset is the beginning of the hashtable. At a posi\u00adtion hv \nin the hashtable, a two-fold entry is depicted that con\u00adtains both the implemented interface ID, that \nmust be com\u00adpared to the target interface ID, and the offset iOffset of the group of methods introduced \nby the interface that intro\u00adduces the considered method. The table contains, at the posi\u00adtion #methodOffset \ndetermined by the considered method in the method group, the address of the function that must be invoked. \nIn a forthcoming paper [Ducournau and Moran\u00addat 2009], we improve the technique in several directions, \nespecially with a new hashing function that combines bit\u00adwise and with a shift for truncating trailing \nzeros (PH\u00adand+shift). It reduces the total hashtable size at the ex\u00ad ctable  table  The preamble is \ncommon to both mechanisms. The implementation resembles PH, apart from the fact that the interface position \nresults from a load instead of being the result of speci.c hashing. The location of the color can be \nthe method table of the target class that is not represented here. The loads concern different memory \nareas and they can yield 3 cache misses. Moreover, the recomputable color table requires an extra indirection, \ntogether with its size (clen) and bound checking for subtype testing, and the color itself requires memory \naccess (not represented in the diagram).  Figure 9. Incremental coloring for JAVA interfaces pense of \na few extra instructions that are expected to be run in parallel. To our knowledge, PH is the only constant-time \ntechnique that allows for both multiple inheritance and dynamic load\u00ading at reasonable spatial cost and \napplies to both method in\u00advocation and subtype testing. Subobject-based implementa\u00adtion has the same \nproperties but applies only to method invo\u00adcation. Moreover, the space requirement of perfect hashing \nis far lower than that of subobjects.  2.6 Incremental Coloring (IC) An incremental version of coloring \n(denoted IC) has been proposed by [Palacz and Vitek 2003] for implementing inter\u00adface subtype testing \nin JAVA. For the sake of comparison, an application to method invocation in the same style as for PH \nhas been proposed in [Ducournau 2008]. As coloring needs the CWA, IC can require some load-time recomputations. \nHence, its data structures involve extra indirections and sev\u00aderal unrelated memory locations that should \nincrease cache misses (Figure 9). Readers are referred to [Ducournau 2008] for a discussion on implementation \ndetails.  2.7 Accessor Simulation (AS) An accessor is a method that either reads or writes an at\u00adtribute. \nSuppose that all accesses to an attribute are through  // attribute access load [object + #tableOffset], \ntable load [table + #classOffset+fieldLen], attrGroupOffset add object, attrGroupOffset, attgr, load \n[attgr + #attrOffset], value class attrGroup class id Offset Offset  table attrGroup Offset attr \n Offset  object layout value object The diagram depicts the precise object representation with accessor \nsimulation coupled with class and method coloring, to be compared with Fig. 3. The offset of the group \nof attributes introduced by a class (attrGroupOffset) is associated with its class ID in the method table \nand the position of an attribute is now determined by an invariant offset (attrOffset) w.r.t. this attribute \ngroup. Figure 10. Accessor simulation with method coloring an accessor. Then the attribute layout of \na class does not have to be the same as the attribute layout of its superclass. A class will rede.ne \nthe accessors for an attribute if the attribute has a different offset in the class than it does in the \nsuperclass. True accessors require a method call for each access, which can be inef.cient. However, a \nclass can simulate accessors by replacing the method address in the method table with the attribute offset. \nThis approach is called .eld dispatching by [Zibin and Gil 2003]. Another improvement is to group attributes \ntogether in the method table when they are intro\u00adduced by the same class. Then one can substitute, for \ntheir different offsets, the single relative position of the attribute group, stored in the method table \nat an invariant position, i.e. at the class color with coloring (Fig. 10) [Myers 1995, Ducournau 2009]. \nWith PH and IC, the attribute-group off\u00adset is associated with the class ID and method-group offset in \nthe hash-or color-table, yielding 3-fold table entries. Accessor simulation is a generic approach to \nattribute access which works with any method invocation technique; only grouping can be conditioned by \nstatic typing, since attributes must be partitioned by the classes which introduce them. It is, however, \nmeaningless to use it with subobject\u00adbased implementation (SO) which provides two different accesses \nto attributes according to whether the receiver static type (rst) is the attribute introduction class \n(aic) or not. The former is identical to attribute coloring (AC), whereas the latter is identical to \naccessor simulation (AS) with method coloring (MC). For instance, in Fig. 5, rst. =aic. Among the various \ntechniques that we have described, some apply only to method invocation and subtype testing, e.g. perfect \nhashing and incremental coloring. Hence, these techniques can be used for JAVA interface implementation. \nAccessor simulation is a way of applying them to full mul\u00adtiple inheritance. It can also replace attribute \ncoloring, if // method invocation load [object + #tableOffset], table load [table + #cacheMethId_i], \nid comp id, #targetId beq #hit store #targetId, [table + #cacheMethId_i] // usual sequence of PH or IC \n(4-5 instructions) store iOffset, [table + #cacheMethOffset_i] jump #end hit: load [table + #cacheMethOffset_i], \niOffset end: add table, iOffset, itable load [itable + #methodOffset], method call method // subtype \ntesting load [object + #tableOffset], table load [table + #cacheTypeId_i], id comp id, #targetId beq \n#succeed // usual sequence of PH or IC (6-8 instructions) store #targetId, [table + #cacheTypeId_i] succeed: \n Figure 11. Separate caches in method tables holes in the object layout are considered to be over space\u00adconsuming. \n 2.8 Caching and Searching (CA) An implementation policy that is often applied to dynamic typing or \nJAVA interfaces involves coupling some implemen\u00adtation technique (that is expected, here, to be rather \nnaive and inef.cient) with caching for memoizing the result of the last search. For instance, with JAVA \ninterfaces, the underly\u00ading technique could be PH or IC and the method table would cache the interface \nID and the group offset of the last suc\u00adceeding access [Alpern et al. 2001a,b, Click and Rose 2002]. \nOf course this cache might be used for any table-based sub\u00adtyping technique and for all three mechanisms, \nat the ex\u00adpense of caching three data, namely class ID and method and attribute group offsets. Finally, \nthe cache may be common to all three mechanisms, or speci.c to each of them. Obviously, the improvement \nis a matter of statistics and those presented in [Palacz and Vitek 2003] for subtype testing show that, \nac\u00adcording to the different benchmarks, cache miss rates can be as low as 0.1% or more than 50%. Fig. \n11 presents the code sequences of caching for method invocation and sub\u00adtype testing they are markedly \nlonger than for the original implementations. Moreover, the worst case remains rather inef.cient and \nthe best case is hardly better than PH-and for method invocation and identical to Cohen s display for \nsub\u00adtype testing. Like IC and unlike all other techniques, caching also requires method tables to be \nwritable, hence allocated in data memory segments. Cache-hit rates can be further improved with multiple \ncaches (CAn). For instance, with n caches, classes (or in\u00adterfaces) are statically partitioned into n \nsets, for instance by hashing their name. In the method tables, the cache data structure (i.e. the offsets \n#cacheTypeId_i, #cacheMethId_i and #cacheMethOffset_i) is then replicated n times. Fi\u00adnally the code \nof each call site is that of Fig. 11, with the cache structure, i.e. the index i, corresponding to the \n#targetId of the given site. Hence the cache miss rate should asymptotically tend towards 0 as n increases \nhowever, the best and worst cases are not improved. In this approach, the tables of the underlying implementation \nare only required to contain class or interface IDs for which there is a collision on their proper cache. \n In our tests, we will also consider PH when it is coupled with caching. One might expect, for instance, \nthat caching degrades PH-and but improves PH-mod.  3. Compilation Schemes Compilation schemes represent \nthe production line of exe\u00adcutable programs from the source code .les. They can in\u00advolve various processors \nsuch as compilers, linkers, virtual machines, loaders, just-in-time compilers, etc. 3.1 Under Pure OWA \nDynamic Loading (D) As already mentioned, object-oriented philosophy, espe\u00adcially reusability, is best \nexpressed by the OWA. Pure OWA corresponds to separate compilation and dynamic loading this scheme will \nbe denoted D hereafter. Under the OWA, aclass C (more generally, a code unit including several classes) \nis compiled irrespective of the way it will be used in different programs, hence ignoring its possible \nsubclasses and clients4. On the contrary, a subclass or a client of C must know the model (aka schema \n) of C, which contains the interface of C possibly augmented by some extra data e.g. it is not restricted \nto the public interface. This class model is included in speci.c header .les (in C++) or automatically \nextracted from source or compiled .les (in JAVA). Without loss of generality, it can be considered as \nan instance of some metamodel [Ducournau and Privat 2008]. The code itself is not needed. Separate compilation \nis a good answer to the modular\u00adity requirements of software engineering; it provides speed of compilation \nand recompilation together with locality of errors, and protects source code from both infringement and \nhazardous modi.cations. With separate compilation, the code generated for a program unit, here a class, \nis correct for all correct future uses. 3.2 Under Pure CWA Global Compilation (G) Complete knowledge \nof the whole class hierarchy offers many ways of ef.ciently implementing multiple inheritance. CWA presents \nseveral gradual advantages: (i) the class hier\u00adarchy is closed and the models of all classes can be known \nas a whole; (ii) the code of each class is also known; (iii) the program entry point can also be known. \nWhen only (i) holds, a simple class hierarchy analysis (CHA) [Dean et al. 1995b] provides a rough approxima\u00ad \n4 A client of C is a class that uses C or a subclass of C, as a type annotation (e.g. x : C) or for creating \ninstances (new C). tion of the call graph that is suf.cient for identifying many monomorphic call sites. \nWith global compilation (scheme denoted G), when the program entry point is known (point (iii)) and the \nlanguage does not provide any metaprogram\u00adming facility, a more sophisticated type analysis precisely \napproximates the receiver concrete type at each call site, thus making it easy to identify mono-, oligo-and \nmega-morphic sites, so that each category can be implemented with the best technique, respectively, static \ncalls, BTDi and color\u00ading. Well-known algorithms are RTA [Bacon et al. 1996] and CFA [Shivers 1991]. \nMoreover, dead code can be ruled out and other optimiza\u00adtions like code specialization [Dean et al. 1995a, \nTip and Sweeney 2000] can further reduce polymorphism the for\u00admer decreases the overall code size but \nthe latter increases it. We do not consider them here. 3.3 Separate Compilation, Global Linking (S) \nThe main defect of coloring is that it requires complete knowledge of all classes in the hierarchy. This \ncomplete knowledge could be achieved by global compilation. How\u00adever, giving up the modularity provided \nby separate compi\u00adlation may be considered too high a price for program op\u00adtimization. An alternative \nwas already noted by [Pugh and Weddell 1990]. Coloring does not require knowledge of the code itself \n(point (ii) above), but only of the model of the classes (point (i)), all of which is already needed \nby separate compilation. Therefore, the compiler can separately generate the compiled code without knowing \nthe value of the colors of the considered entities, representing them with speci.c symbols. At link time, \nthe linker collects the models of all classes and colors all of the entities, before substituting val\u00adues \nto the different symbols, as a linker commonly does. The linker also generates method tables.  3.4 Separate \nCompilation, Global Optimization (O) [Privat and Ducournau 2005] propose a mixed scheme which relies \non some link-time type analysis. As the class hierar\u00adchy is closed, CHA can be applied, which will determine \nwhether a call site is monomorphic or polymorphic. Link\u00adtime optimization is possible if, at compile-time, \nthe code generated for a call site is replaced by a call to a special sym\u00adbol, which is, for instance, \nformed with the name of the con\u00adsidered method and the static type of the receiver. Then, at link-time, \na stub function called a thunk as in C++ imple\u00admentations [Lippman 1996] is generated when the call site \nis polymorphic. For monomorphic sites, the symbol is just replaced by the name of the called procedure, \nthus yielding a static call. More sophisticated type analyses are possible if a model of internal type \n.ow, called an internal model in con\u00adtrast, the model discussed in Section 3.1 is called external model \nis generated at compile time [Privat and Ducournau 2005]. [Boucher 2000] proposed a similar architecture \nin a functional programming setting.  Compilation Scheme Space Time S separate O optimized H hybrid \nG global Code Static Dyn. Run Compile Load/Link SO   ++ ++ IC + +++ ++ PH-and +++ ++ + Implementation \nTechnique D dynamic subobjects SO 0 0 * * * perfect hashing PH .  * * * incremental coloring IC .  \n* * * caching CA .  * * * method coloring MC *  0 binary tree dispatch BTD * *  0 attribute coloring \naccessor simulation AC AS *.   0 0 :Tested, .: Extrapolated, 0: Not yet tested, *: Non-interesting, \n*: Incompatible PH-mod PH-and +CA PH-mod +CA MC ++ ++ +++ ++ + BTDi<2 +++ +++ +++ +++ ++ BTDi>4  +++ \n+++ AC ++ ++ + ++ + AS + + +++  + + + +++  +++  +++ ++ + ++ + ++ + +++: optimal, ++: very good, \n+: good, : bad, : very bad,  : unreasonable Table 1. Compatibility between compilation schemes and \nimplementation techniques An hybrid scheme (H) would involve separate compila\u00adtion of common libraries, \ncoupled with global compilation of the speci.c program and global optimization of the whole.  3.5 Compilation \nvs. Implementation Table 1 presents the compatibility between implementation Table 2. Expected ef.ciency \n compiler source techniques and compilation schemes. Table 2 recalls the ex\u00ad pected ef.ciency that can \nbe deduced from previous ab\u00ad stract studies. Ef.ciency must be assessed from the space executable vn \nand time standpoints. Space-ef.ciency assessment must con\u00adsider code length, static data (i.e. method \ntables) and dy\u00adnamic data (i.e. object layout). Time-ef.ciency assessment must consider run-and compile-time \ntogether with load-or link-time remember that our tests consider only run-time ef.ciency. Not all compatible \ncombinations are interesting to test. For instance, all techniques that are compatible with the OWA are \nless ef.cient than coloring and BTD. Testing them in O, H and G schemes would thus be wasting time. More\u00adover, \nfor these techniques, there is no practical difference be\u00adtween D and S in our testbed because it cannot \nreproduce the memory-allocation effects of dynamic loading. Hence, S is the right scheme for comparing \nthe ef.ciency of implemen\u00adtation techniques like SO, PH, IC and MC. O, H and G are the right ones for \ncomparing MC and BTD. Moreover, with O and G, the comparison can also consider various type anal\u00adysis \nalgorithms (CHA, RTA or CFA) and polymorphism de\u00adgrees for BTD. AC and AS can be compared in all schemes \nbut D and the comparison closely depends on the underly\u00ading method invocation technique. Coupling AC \nwith PH or IC is, however, possible as it provides an assessment of the use of the considered method \ninvocation technique in the re\u00adstricted case of JAVA interfaces. On the contrary, coupling these techniques \nwith AS amounts to considering them in a full multiple inheritance setting. In contrast, the H scheme \nhas not been tested, partly for want of time, and partly be\u00adcause of the dif.culty of distinguishing \nbetween libraries and program. Overall, several tens of combinations can be tested and we present only \nthe most signi.cant. Some compiler source is compiled by some compiler executable, according to different \noptions, thus producing different variants v1, .., vn, of the same executable compiler (solid lines). \nAnother compiler source (possibly the same) is then compiled by each variant, with all producing exactly \nthe same executable (dashed lines), and the duration of this compilation (i.e. the dashed lines) is measured. \nFigure 12. The PRM testbed  4. Compilation Testbed These experiments are original as they compare different \nimplementation techniques in a common framework that allows for a fair comparison with all other things \nbeing equal. Tested Program. We have implemented all these tech\u00adniques in the PRM compiler, which is \ndedicated to exhaustive assessment of various implementation techniques and com\u00adpilation schemes [Privat \nand Ducournau 2005]. The bench\u00admark program is the compiler itself, which is written in PRM and compiles \nthe PRM source code into C code. There are many compilers in the picture, so Figure 12 depicts the pre\u00adcise \ntestbed. In these tests, the C code generated by the PRM compiler and linker is the code of the considered \ntechniques in the considered compilation scheme. This code can be gen\u00aderated at compile-or link-time \naccording to the scheme. The PRM compiler is actually not compatible with dy\u00adnamic loading (D) but the \ncode for PH or IC has been gen\u00aderated in separate compilation (S) exactly as if it were gen\u00aderated at \nload-time, with hash parameters and tables being computed at link-time. In this case, all PRM link-time \nopti\u00admizations are deactivated. Hence, although these tests rep\u00ad  number of static dynamic class introductions \ninstantiations subtype tests 532 6651 194 35 M 87 M method introductions de.nitions calls 2599 4446 \n15873  1707 M BTD 0 1 2 3 4..7 8 124875 848 600 704 1044 32 1134 M 61 M 180 M 26 M 306 M 228 K attribute \nintroductions accesses rst=aic accessor 614 4438 2996 361 2560 M 1903 M 229 M pointer adjustments upcasts \ndowncasts 9133 3254 783 M 1393 M The static column depicts the number of program elements (classes, \nmethods and attributes) and the number of sites for each mechanism. The dynamic column presents the invocation \ncount at run-time (in millions or thousands). Method call sites are separately counted according to their \npolymorphism degree with CHA, i.e. the BTD depth that can implement them. Attribute accesses are separately \ncounted when the access is made through an accessor or on a receiver whose static type (rst)is the attribute \nintroduction class (aic) (Section 2.7). The former is a special case of the latter. Like rst=aic, pointer \nadjustments only concern subobjects (SO). Upcasts consist of all polymorphic assignments and parameter \npassings, when the source type is a strict subtype of the target type, together with equality tests, \nwhen the two types are different. Downcasts consist of covariant return types. Table 3. Characteristics \nof the tested program resent a kind of simulation, they must provide reliable ex\u00adtrapolation. Only the \neffect of cache misses is likely under\u00adestimated, especially for incremental coloring. Here all color \ntables are allocated in the same memory area, whereas load\u00adtime recomputations should scatter them in \nthe heap. Table 3 presents the static characteristics of the tested pro\u00adgram, i.e. the PRM compiler, \nnamely the number of different entities that are counted at compile-time, together with the run-time \ninvocation count for each mechanism. The Table details statistics of method call sites according to their \npoly\u00admorphism degree, that is the BTDi that can implement them according to the CHA type analysis. A \ncall site is counted in BTDi if its branch number is between (2i-1 + 1) and 2i . Finally, the cache-hit \nrate has been measured (Table 4) when coupling perfect hashing (PH) with caching (CA). Of course, it \ndoes not depend on the speci.c technique that is associated with CA. Cache hits and monomorphic calls \nrep\u00adresent dual data. Monomorphism is a permanent link-time feature of a call site that always calls \nthe same method, whereas a cache hit is a momentary run-time characteris\u00adtics of a method table that \nis used for an access to some data that is introduced by the same superclass as in the previous access. \nWith attribute coloring (AC), the measured cache\u00adhit rate is between 60 and 80% according to whether \nthe cache is common or separate, single or multiple. As ex\u00adpected, multiple separate caches have better \nef.ciency. The cache with AC with AS number separate common separate common 1 68 63 62 39 2 71 64 4 79 \n67 The cache-hit rate is presented (in percentage), with attribute coloring or accessor simulation, \nwith separate caches or a common cache for all mechanisms, and according to the number of caches. Table \n4. Method table cache-hit rate effect of separate caches is more marked with accessor simu\u00adlation (AS), \nwith the cache-hit rate increasing from less than 40% to more than 60%. With separate caches, the observed \ncache-hit rates are about the average of those reported by [Palacz and Vitek 2003]. Multiple caches are \nneeded because the cache is used here for all classes, whereas its use was re\u00adstricted, in the aforementioned \npaper, to interfaces. The dynamic statistics from Tables 3 and 4 have been used for selecting variants \nthat would be interesting to mea\u00adsure. Indeed, it would be vain to compare the execution times of variants \nthat present very close statistics. Test Signi.cance. These statistics show that the program size is \nsigni.cant and that it makes intensive usage of object\u00adoriented features. Moreover, the high number of \nmonomor\u00adphic calls (about 79 or 64% of calls sites, according to whether static or dynamic statistics \nare considered) is con\u00adsistent with statistics that are commonly reported in the liter\u00adature. Of course, \nthe validity of such experiments that rely on a single program might be questioned. This large and fully \nobject-oriented program intensively uses the basic mecha\u00adnisms that are tested, namely a total of more \nthan 4 billions invocations of all three mechanisms. Moreover, as the exper\u00adiments compare two implementations \nwith all other things being equal, the sign of the differences should hold for most programs and only \nthe order of magnitude should vary, at least when the difference is not close to 0 and when the com\u00adparison \nfocuses on a single parameter, i.e. in the same row or column in Tables 5 and 6. Our argument for supporting \nthis claim is that the differences only concern the tested code se\u00adquences and the related cache misses. \nAll other things are equal. Moreover the behaviours of all implementations are very similar from the \ncache standpoint, apart from BTD, since it does not access method tables, and IC, because it accesses \nextra memory areas. For the former, both consid\u00aderations act in the same direction, i.e. bounded BTD \nare in\u00adtrinsically faster with less cache misses. For the latter, the testbed is unable to take all load-time \neffects into account, hence it underestimates the cost of cache misses. IC is thus likely the only exception \nto our claim and we will take it into account in our conclusions. This single-benchmark limitation is \nalso inherent to our experimentation. The PRM compiler is the only one that allows such versatility in \nthe basic implementation of object\u00adoriented programs. The compensation is that the language has been \ndeveloped with this single goal, so its compiler is the only large-scale program written in PRM.  A \nlast objection can be raised, namely that the PRM com\u00adpiler might be too inef.cient to draw any .rm conclusions. \nIn the following, we consider relative differences, of the form (test - ref )/ref . Of course, if the \nnumerator is small and the denominator is overestimated, for instance if the PRM compiler was one order \nof magnitude slower than it could be, the results might be meaningless. Therefore, we must also convince \nreaders that the PRM compiler is not too in\u00adef.cient. This is, however, much more dif.cult to prove, \nsince it requires an external comparison that cannot be done with all other things being equal. The GNU \nEIFFEL com\u00adpiler, SMART EIFFEL,represents,however,aconvenientref\u00aderence because it involves a similar \nmeta-compilation frame\u00adwork. It uses global compilation (G) and is considered very ef.cient see Section \n2.4. Both languages are fully-.edged object-oriented languages that provide similar features, and both \ncompilers have close characteristics such as type anal\u00adysis and the same target language. Thus we compared \nthe compilation time of both compilers, from the source lan\u00adguage (EIFFEL or PRM) to C. The compilation \ntimes were quite similar, about 60 seconds on the considered processor. Although this does not mean that \nthe PRM compilerisasef\u00ad.cient as SMART EIFFEL, it is however a strong indication that it is not too inef.cient. \nOf course, further improvements will strengthen our results. Runtime Reproducibility. Tested variants \ndiffer only by the underlying implementation technique, with all other things being equal. This is even \ntrue when considering exe\u00adcutable .les, not only the program logic. Indeed, the compi\u00adlation testbed \nis deterministic, that is two compilations of the same program by the same compiler executable produce \nex\u00adactly the same executable. This means that (i) the compiler always proceeds along the program text \nand the underlying object model in the same order; (ii) the memory locations of program fragments, method \ntables and objects in the heap are roughly the same. Thus two compiler variants differ only by the code \nsequences of the considered techniques, with all program components occurring in the executables in the \nsame order. Moreover, when applied to some program, two compiler variants (vi and vj) produce exactly \nthe same code. Hence, the fact that all dashed arrows point at the same ex\u00adecutable (Fig. 12) is not \nonly a metaphor. Incidentally, this determinism ensures that the compiler bootstrap represents an actual \n.xpoint. All claimed program equalities have been checked with the diff command on both C and binary \n.les. Overall, the effect of memory locality should be roughly constant, apart from the speci.c effects \ndue to the consid\u00adered techniques5. 5 In early tests, compilation was not deterministic and there were \nmarked variations in execution times between several generations of the same vari\u00adant. Hence, the variation \nbetween different variants was both marked and meaningless. Therefore, in principle, the statistics shown \nin Table 3 should not depend on compilation variants, though some of them interest only some speci.c \nvariants. However, in spite of the compilation determinism, a compiled program is not exactly deterministic \nfor these .ne-grained statistics. In\u00addeed, hashing object addresses is inherently not determinis\u00adtic. \nHence, two runs of the same program can produce differ\u00adent collisions. As hash structures are PRM objects, \nthe pre\u00adcise run-time statistics (column dynamic in Table 3) are not exactly reproducible. The variations \nare actually very low (less than one to a thousand) and do not affect the measures. Above all, it does \nnot modify the program logic because all hash structures used by the PRM compiler are order-invariant \nthat is all iterations follow the input order. Processors. The tests were performed on a variety of pro\u00adcessors \n(Tables 5 and 6): I-2, I-4, I-5, I-8 and I-9, from the Intel R &#38;#169; PentiumTM family;  A-6 and \nA-7 are AMD R &#38;#169; processors; all x86 are under Linux Ubuntu 8.4 with gcc 4.2.4;  S-1 is a &#38;#169; \nSparcTM, under SunOS 5.10, with SUN R gcc 4.2.2;  P-3 is a PowerPC G5, designed by IBM R&#38;#169;, \n &#38;#169; and Apple Runder Mac OS X 10.5.3, with gcc 4.0.1. Non-Linux linkers presented technical \ndrawbacks that cur\u00adrently hinder global optimizations (O) on processors S-1 and P-3. All tests use Boehm \ns garbage collection [Boehm 1993] and the test on processor I-8 has also been run without GC, for the \nsake of comparison. The measure itself is done with Unix function times(2) which considers only the time \nof a single process, irrespective of the system scheduler this is required by multicore technology. Two \nruns of the same compiler on the same computer should take the same time were it not for the noise produced \nby the operating system. A solution involves running the tests under single\u00aduser boot, e.g. Linux recovery-mode. \nThis has been done for some processors (e.g. I-2, I-4, I-8) but was actually not pos\u00adsible for remote \ncomputers. Finally, a last impediment con\u00adcerned laptops. Modern laptop processors (e.g. I-5 and I-8) \nare frequency-variable. The frequency is low when the pro\u00adcessor is idle or hot. When running a test, \nthe processor must .rst warm up before reaching its peak speed, then it .nishes by slowing down and cooling. \nThus the peak speed can be very high but only for a short duration. Inserting a pause be\u00adtween each two \nruns seemed to .x the point and I-8 now provides one of the most steady testbeds. Overall, we assume \nthat the difference between two runs of the same executable is pure noise and this noise does not depend \non the speci.c variant, but only on the test time and duration. As the noise is not symmetrical it is \nalways positive we took, for each measure, the minimum value among several tens of runs.  processor \nfrequency S-1 UltraSPARC III 1.2 GHz I-2 Xeon Prestonia 1.8 GHz P-3 PowerPC G5 1.8 GHz I-4 Xeon Irwindale \n2.8 GHz I-5 Core T2400 2.8 GHz L2 cache year 123.2s 8192 K 2001 87.4s 512 K 2001 62.3s 512 K 2003 43.3s \n2048 K 2006 34.8s 2048 K 2006 technique scheme AC AS AS/AC AC AS AS/AC AC AS AS/AC AC AS AS/AC AC AS \nAS/AC MC-BTD8 RTA G -22.6 -11.5 14.4 -10.9 -2.2 9.7 *** *** *** -13.3 -1.0 14.1 -8.9 2.9 13.0 MC-BTD2 \nRTA G -22.2 -10.9 14.6 -11.7 -3.8 10.6 -28.4 -13.0 21.5 -13.7 -1.3 14.4 -10.2 -0.8 10.5 MC-BTD8 CHA O \n*** *** *** -2.9 1.4 4.5 *** *** *** -3.0 4.9 8.2 2.7 19.4 16.2 MC-BTD2 CHA O *** *** *** -5.4 -2.8 2.8 \n*** *** *** -5.9 2.3 8.7 -2.7 14.2 17.3 MC S 0 9.8 9.8 0 5.7 5.7 0 18.2 18.2 0 7.9 7.9 0 11.1 11.1 IC \nD 13.7 34.1 17.9 5.3 14.5 8.7 13.4 27.2 12.1 4.3 16.6 11.8 7.7 28.5 19.2 PH-and D 13.4 35.4 19.5 2.5 \n14.5 11.6 8.1 24.9 15.6 4.2 19.0 14.2 6.2 31.4 23.8 PH-and+shift D 14.7 38.0 20.3 10.6 25.5 13.6 13.4 \n35.5 18.9 6.9 28.7 20.4 10.3 45.3 31.8 PH-mod D 81.1 226.0 80.0 28.6 104.3 58.8 49.1 146.1 65.1 55.2 \n172.0 75.2 24.4 106.3 65.8 PH-mod+CA4 D 33.1 121.0 66.1 21.1 81.2 49.6 24.7 87.5 50.3 28.1 98.2 54.7 \n21.4 82.7 50.5 Each subtable presents the results for a precise processor, with the processor characteristics \nand the reference execution time. All other numbers are percentage. Each row describes a method invocation \nand subtype testing technique. For all techniques, the .rst two columns represent the overhead vs pure \ncoloring (MC-AC-S), respectively with attribute coloring (AC) and accessor simulation (AS). The third \ncolumn is the overhead of accessor simulation vs attribute coloring. *** Results are currently unavailable. \nOn P-3, BTD2 is replaced by BTD0. Table 5. Execution time according to implementation techniques and \nprocessors 5. Results and Discussion Tables 5 and 6 present, for each tested variant and processor, \nthe time measurement and overhead with respect to the full coloring implementation (MC/AC). Overall, \nnotwithstand\u00ading some exceptions that will be discussed hereafter, these tests exhibit many regularities. \nDifference Magnitude. Before discussing measures, it is important to correctly interpret the overhead \nmagnitude. The differences are all the more signi.cant since all measures in\u00adclude the time consumed \nby garbage collection (GC), which is roughly the same for all variants as it does not rely on any object-oriented \nmechanism. In order to provide an es\u00adtimation of the collector cost, the last column of Table 6 presents \nthe same test with a deactivated GC, on processor I-8. It turns out that garbage collection takes between \n14.5 and 15 seconds with all variants but MC-BTD-O, i.e. almost 50% of the reference time. Hence, all \noverheads are roughly doubled. Strangely enough, with MC-BTD-O, garbage col\u00adlection takes only 13.4 seconds. \nWe cannot explain this dif\u00adference. Of course, the GC overhead depends on executable and memory sizes. \nThis test was performed on a computer equipped with 2 G-bytes of memory, which is suf.cient for running \nall variants without swapping. These measures must not be used to deny the usefulness of garbage collection. \nOn a real-life computer, the tested program should share mem\u00adory with many other programs and 2 G-bytes \ncould not be dedicated to it. Moreover, Boehm s collector is conserva\u00adtive and not optimized for the \nsimple object representations that are tested. A type accurate collector would be markedly more ef.cient \n[Jones and Lins 1996]. Overall, below 1%, a difference is likely meaningless, hence decimal digits must \nbe handled with care. In con\u00adtrast, 10% represents a marked difference, since considering the object-oriented \npart doubles the overhead. Finally, 50% overhead is dramatic. Compilation Schemes. As expected, global \ncompilation (G) was found to be markedly better than separate compilation (S). The high ratio of monomorphic \ncalls explains this result, since the difference between MC-S and MC-BTD0-G results only from monomorphic \ncalls and BTD2 hardly improves it.  In contrast, link-time optimization (O) provides only a small improvement. \nThis means that the gain resulting from 64% of static calls is almost offset by the thunk overhead in \nthe 36% of polymorphic calls. This is un\u00adexpected because one might have thought that pipelines would \nhave made the thunk almost free, apart from cache misses.  Dynamic loading (D) yields clear overhead \ncompared to S; it represents the overhead of multiple versus single in\u00adheritance in a dynamic loading \nsetting. Apart from AMD processors, this overhead is, however, slighter than be\u00adtween S and G.  Summing \nboth overheads makes the difference between G and D impressive, about 15% with multiple subtyping (AC) \nand 40% with full multiple inheritance (AS).  Global Optimization Levels (O and G). In contrast with \nthe signi.cant differences between compilation schemes, the differences between global optimization levels, \ne.g. type analysis algorithms or BTD depths, are too weak to allow us to draw .rm conclusions. This is \na consequence of the statistics in Table 3, which show that the main improvement should come from monomorphic \ncalls (BTD0) which repre\u00adsent 64% of method calls. In contrast, BTD1 and BTD2 only amount to 20% of BTD0 \nand the expected improvement would be less than proportional, because of mispredictions, hence hardly \nmeasurable on most processors. Finally, when i > 2, the number of BTDi is too low to conclude whether \n processor frequency A-6 Athlon 64 2.2 GHz A-7 Opteron Venus 2.4 GHz I-8 Core2 T7200 2.0 GHz I-9 Core2 \nE8500 3.16 GHz I-8 without GC L2 cache year 34.0s 1024 K 2003 32.8s 1024 K 2005 30.4s 4096 K 2006 18.5s \n6144 K 2008 15.7s 2 G-bytes technique scheme AC AS AS/AC AC AS AS/AC AC AS AS/AC AC AS AS/AC AC AS AS/AC \nMC-BTD8 RTA G -12.5 2.9 17.6 -13.7 9.9 27.3 -9.4 7.5 18.6 -10.3 0.6 12.1 -19.2 12.7 39.5 MC-BTD2 RTA \nG -12.5 1.1 15.5 -13.1 10.7 27.3 -9.7 7.0 18.5 -9.7 0.6 11.5 -18.5 12.9 38.6 MC-BTD8 CHA O 5.8 27.8 20.8 \n4.0 23.3 18.6 1.1 12.1 10.8 1.8 15.0 12.9 10.2 31.0 18.9 MC-BTD2 CHA O 3.4 27.3 23.1 2.0 22.8 20.4 -1.8 \n10.1 12.2 -2.5 13.0 16.0 5.4 27.9 21.3 MC S 0 15.8 15.8 0 15.2 15.2 0 11.3 11.3 0 10.3 10.3 0 21.6 21.6 \nIC D 14.9 40.1 21.9 12.6 38.9 23.3 7.7 29.7 20.5 8.0 29.7 20.1 14.8 56.5 36.4 PH-and D 15.2 52.4 32.2 \n16.8 57.3 34.7 5.1 30.0 23.7 6.2 30.1 22.6 9.6 55.9 42.2 PH-and+shift D 19.5 64.4 37.6 18.8 61.5 36.0 \n7.8 43.6 33.2 8.3 43.3 32.3 14.1 81.9 59.4 PH-mod D 80.1 235.4 86.2 73.4 221.5 85.4 19.5 108.7 74.7 17.6 \n85.7 57.9 35.2 209.1 128.6 PH-mod+CA4 D 40.4 141.2 71.8 38.3 129.3 65.8 19.6 81.3 51.6 20.8 74.6 44.5 \n37.4 155.7 86.1 Last column presents measures on processor I-8 without garbage collection. Table 6. \nExecution time according to implementation techniques and processors (cont.) BTDi is an improvement on \ncoloring or not. Thus we present only the statistics for BTD8 and BTD2. With both G and O, the observations \ncon.rm this expec\u00adtation. Therefore, this testbed is certainly unable to .nely tune the optimal level \nof BTD for a given processor and it is doubtful that any testbed could do it, since the optimal closely \ndepends on the speci.c type-.ow of programs. Thus the decision must be drawn from abstract considerations. \nBTD1 should always be better than MC, since a mispre\u00addicted conditional branching has the same cost as \nan undi\u00adrect branching. BTD2 should likely be better than MC, since a single well-predicted branching \nmakes it better. BTD3 and BTD4 probably represent the critical point. It would seem that BTD8 often improves \non BTD2 in G but not in O. This is consistent with the fact that dispatch trees are inlined in G, hence \npredictions are proper to a given call site, whereas they are shared in the thunks of O. Indeed, sharing \nincreases branching mispredictions. In contrast, the code is far smaller with sharing (Fig. 7). Of course, \nthis discussion should be reconsidered with processors without branching prediction, e.g. on small embedded \nsystems. Similar conclusions hold with type analysis algorithms. As CHA, in spite of its simplicity, \ngives very good results, more accurate approximations cannot change the conclu\u00adsions. Hence, we do not \npresent the statistics of polymor\u00adphism and run-time measures with RTA and CFA. With G, the solution \nwould be to use the best tradeoff between accuracy and compilation time, for instance with an explicit \nor implicit option that would allow the programmer to choose between various optimization levels. With \nO, the solution might be to use the simple CHA algorithm, which does not require any other data than \nexternal models and simpli.es the overall architecture. Dynamic Loading (D). The comparison between the \ndif\u00adferent techniques compatible with dynamic loading mostly con.rms previous theoretical analyses. When \nused for method invocation and subtype testing, PH-and yields very low over\u00adhead of about 3-8% on most \nprocessors. This could be ex\u00adplained by the few extra loads from a memory area that is already used by \nthe reference technique, hence without extra cache misses, plus a few 1-cycle instructions; these extra \ncycles represent real overhead that is, however, slight in comparison with the overall method call cost. \nThe extra instructions of PH-and+shift entails extra overhead, that is higher than expected since the \nextra instructions could have been done in parallel. Incremental coloring (IC) is close to PH-and, but \nnot better. With accessor simulation, the dif\u00adference is below the measurement precision. In contrast, \nthe overhead of PH-mod is much higher and highly variable, be\u00adtween 17 and 80%, when only used for method \ninvocation and subtype tests. Overall, PH-and outclasses all considered alternatives for method invocation \nand subtype testing from the time stand\u00adpoint. It is also better than PH-and+shift and not worse than \nPH-mod from the space standpoint (Fig. 7). In view of the respective load-time costs and of the underestimation \nof IC cache misses, PH-and should also be preferred over IC. This complete win is rather unexpected and \nPH-and should provide very high ef.ciency in JAVA virtual machines for implementing interfaces. When \nused for attribute access, the overhead becomes less reasonable. In contrast, the integer division overhead \nis higher than expected on many processors and it con.rms that PH-mod should be reserved for processors \nthat have very ef.cient integer division, contrary to many processors tested here which use the .oating-point \nunit for integer division. Cache (CA). As mentioned above, the observed cache-hit rate is highly variable \naccording to the cache con.guration, and the time measurements corroborate the statistics from Table \n4. We tested caches with PH-and and PH-mod.As ex\u00adpected, caching degrades PH-and on all processors and \nwith all cache con.gurations. Moreover, only multiple separate caches improve PH-mod on some processors. \nTherefore, we only present 4-fold separate caches (CA4) with PH-mod. On all processors, the cache slightly \nimproves PH-mod with accessor simulation. In contrast, on I-8 and I-9 proces\u00adsors which have rather ef.cient \ninteger division, the cache yields slight extra overhead with attribute coloring. On all other processors, \nthe cache slightly improves PH-mod but PH-and remains far better. As the cache markedly increases the \noverall table and code size, the winner is clearly PH-and.  Overall, this con.rms that caching can only \nbe a solu\u00adtion if (i) the underlying technique is inef.cient and (ii) the number of cachable entities \nis not too high, e.g. with JAVA interfaces or multiple caches. Accessor Simulation (AS). In all cases, \naccessor simula\u00adtion entails signi.cant overhead compared to attribute color\u00ading. This was of course \nexpected, especially in view of the high number of attribute accesses (Table 3), since accessor simulation \nreplaces the single load of attribute coloring by a sequence similar to method invocation, apart from \nan actual function call. Hence, it adds extra access to memory areas that are possibly not in cache with \nattribute coloring, and it increases cache-miss risks. Moreover, the single load of at\u00adtribute coloring \ncan be more easily done in parallel than the several-instruction sequence of accessor simulation. For \nall techniques used with dynamic loading (D), acces\u00adsor simulation provokes apparent non-additive overhead, \nas a kind of inverted triangular inequality. For a given tech\u00adnique, say IC, the difference between IC-AS \nand MC-AC is far greater than the sum of differences on the same row and column. This is explained by \nthe fact that the .rst differ\u00adence only concerns method invocation. Hence, it must be ex\u00adtrapolated to \nattributes by multiplying it by 2560+1707 2.5, 1707 according to statistics in Table 3. Therefore, IC-AS/MC-AC \nmust be compared to IC-AC/MC-AC (multiplied by 2.5) plus MC-AS/MC-AC. This explains the observed overhead \nfor IC, PH-and or even PH-mod, in spite of the dramatic magnitude of the latter. From a methodological \nstandpoint, it shows that cautious extrapolation is possible from tested techniques to a non-tested one. \nWe shall apply this idea to subobjects just below. Nevertheless, these results are not de.nitive, because \nthe accessor simulation overhead has been overestimated in our tests. Indeed, true accessors are also \nintensively used in the tested programs, in such a way that they add both over\u00adheads of accessor methods \nand simulation. Hence, accessor methods should be implemented by direct access to the at\u00adtribute, as \nwith AC, at least when the method is generated by the compiler in S and D schemes. However, in view of \nthe statistics of accessors in Table 3, the improvement should be slight. In G, accessor simulation should \nbe used only on the attributes whose position vary according to classes. The resulting improvement should \nbe as important as with monomorphic calls. Subobjects (SO). We could not achieve on time subobject\u00adbased \nimplementation. This is indeed the most demanding as pointer adjustments do not pardon any error. However, \nac\u00adcessor simulation (AS) involves pointer adjustment and rep\u00adresents a convenient lower bound of the \noverhead yielded by subobjects. Indeed, an upcast adjustment is equivalent to AS with method coloring, \nwhereas a downcast adjust\u00adment is roughly equivalent to AS with PH-and, since PH is used for subtype \ntesting with subobjects. Statistics in Table 3 show that the upcast count (783), augmented by the number \nof attribute accesses with rst. =aic (657), represents more than 50% of the attribute access count (2560). \nThe down\u00adcast count (1393) too is more than 50% of the attribute ac\u00adcess count. Therefore, the overhead \nof subobjects, when re\u00adstricted to attribute access and pointer adjustment, should be at least 50% of \nthe difference between AS and AC with method coloring (MC-S) plus 50% of the same difference with PH-and. \nOverall, without taking receiver adjustment into account, the overhead of subobjects would be about 20% \non processor I-8. Hence, the total overhead of subob\u00adjects is expected to be markedly higher than IC \nand PH-and with attribute coloring, though likely always lower than the overhead of the same techniques \nwith accessor simulation. This con.rms that all implementations that are compatible with dynamic loading \nare costly. There remains, however, to precisely compare SO to PH-and. With attribute coloring, this \nwould provide an estimation of the overhead of full mul\u00adtiple inheritance w.r.t. multiple subtyping under \nOWA. With accessor simulation, subobjects should be more ef.cient, but this requires an experimental \ncon.rmation. Processor In.uence. The processor in.uence is also sig\u00adni.cant, even though it does not \nreverse the conclusions. Most processors present similar behaviour, although several provide some speci.c \nexceptions that make them unique. For instance, A-7 is the only processor for which IC is markedly better \nthan PH-and. On all non-Intel processors like S-1, P-3, A-6 and A-7, the magnitude of most differences \nis al\u00admost doubled in comparison to Intel processors. A particular case is PH-mod, which is markedly \nmore ef.cient on recent Pentiums (I-8 and I-9) and dramatically inef.cient on some other processors (S-1, \nI-4, A-6, A-7). These variations might be explained, either by some artefact in the experiment, or by \nsome speci.c feature of the processor. The sample is how\u00adever too small to draw any conclusion about \nprocessor fam\u00adilies. Processors are presented and numbered in the decreas\u00ading order of the reference \nduration, which is strongly cor\u00adrelated with the manufacturing time. It is, however, hard to .nd close \ncorrelations between the observed overheads and time or overall performance. Size of Executable. Table \n7 presents similar statistics of executable size instead of duration. Although the PRM testbed is not \noptimized from the memory occupation stand\u00adpoint, some conclusions can be drawn from these statis\u00adtics. \nGlobal compilation (G) and link-time optimizations (O) markedly reduce the executable size. G is also \nimproved by dead code elimination, but the PRM modular architecture makes this improvement slight. In \ncontrast, BTD8 markedly increase the program size when dispatch trees are speci.c to each site, as in \nG. However, BTD is expected to be time\u00ad  Stripped executable on processor I-5 ref. size: 914 KB technique \nscheme AC AS AS/AC MC-BTD8 RTA G 22.0 30.9 7.3 MC-BTD2 RTA G -26.9 -17.0 13.5 MC-BTD8 CHA O -5.1 -0.1 \n5.2 MC-BTD2 CHA O -14.9 -10.4 5.3 MC S 0 11.7 11.7 IC D 19.0 33.3 11.9 PH-and D 24.5 45.5 16.9 PH-and+shift \nD 37.0 61.6 18.0 PH-mod D 25.9 41.6 12.5 PH-mod+CA4 D 75.6 104.7 16.7 In K-bytes and percentage. Conventions \nare the same as for time measures. Table 7. Size of stripped executable on processor I-8 ef.cient only \nwith proper trees. So space is another argument in favor of combining BTD with coloring. With dynamic \nloading, the space statistics may be less re\u00adliable. First, IC involves dynamic reallocations that are \nnot taken into account here. Moreover, PH has not been imple\u00admented in the most ef.cient way from the \nspace standpoint [Ducournau and Morandat 2009], and the space overhead is certainly overestimated. However, \na .rm conclusion is possible for PH-and+shift. Indeed, this technique was de\u00adsigned to reduce the table \nsize, but the statistics show that it markedly increases the code size. Hence, a de.nitive conclu\u00adsion \nwould be to rule out PH-and+shift, since it degrades both time and space. The difference between PH-mod \nand PH-and is not marked. This is apparently contradictory with the aforementioned time/space tradeoff \n[Ducournau 2008]. Actually, this tradeoff would be obtained with class hierar\u00adchies larger by an order \nof magnitude, but the space increase is likely low in comparison with the overall size. Finally, caching \nproves to be over space-consuming when it is inlined, like all techniques considered here. So caching \nmight be reserved to non-inlined techniques, for instance the bytecode interpreter of virtual machines. \nAn alternative would be to use it with shared thunks, as in O.  6. Related Work, Conclusions and Prospects \nRelated Work. There has been a lot of work on imple\u00admentation and compilation of object-oriented languages \nand programs. The most important part was made in a dynamic typing setting and applied to SMALLTALK,SELF \nor CECIL, and also to multiple dispatch in languages like CLOS,CE-CIL or DYLAN. Although the techniques \nconsidered here of\u00adten originate from these dynamic typing studies (e.g. color\u00ading or BTD, with the latter \nbeing an improvement of poly\u00admorphic inline caches [H\u00f6lzle et al. 1991]) static typing makes them much \nmore ef.cient. Besides implementation techniques, substantial work has also been done to optimize object-oriented \nprograms. For instance, the Vortex compiler is dedicated to the assessment of various optimization tech\u00adniques \nfor JAVA and CECIL programs [Grove and Chambers 2001]. In the C++ context, the various implementations \nof pointer adjustments (VBTRs, thunks, etc.) have been com\u00adpared [Sweeney and Burke 2003] and different \napproaches have been proposed for optimizing the generated code by de\u00advirtualization [Gil and Sweeney \n1999, Eckel and Gil 2000]. However, under the CWA, these optimizations are outclassed by coloring. JAVA \nand .NET gave also rise to many studies about interface implementation [Alpern et al. 2001a,b, Click \nand Rose 2002, Palacz and Vitek 2003] and adaptive com\u00adpilers [Arnold et al. 2005]. Our testbed could \nnot include the latter because of its incompatibility with dynamic load\u00ading. Regarding interface implementations, \nbesides PH and IC, they are generally not time-constant and mostly based on caching and searching. Their \nscalability is doubtful but it was not possible to include them in our testbed for a fair comparison, \nsince PRM does not distinguish between classes and interfaces. Finally, object-oriented implementation \nis not limited to method invocation, attribute access and subtype testing. A lot of little mechanisms \nare also implied interested readers are referred to [Ducournau 2009] for a survey. Genericity is a major \nef.ciency concern. The implementations of gener\u00adics lie between two extremes [Odersky and Wadler 1997]. \nIn heterogeneous implementations, e.g. C++ templates, each instance of the parametrized class is separately \ncompiled. In homogeneous implementations, e.g. JAVA 1.5, a single instance is compiled, after replacing \neach formal type by its bound (this is called type erasure). These two extremes present an interesting \ntime-space ef.ciency tradeoff. A het\u00aderogeneous approach is markedly more time-ef.cient than the homogeneous \none when the formal type is instantiated by a primitive type. In contrast, in such situations, type era\u00adsure \nforces automatic boxing and unboxing. On the other hand, the code and method tables are duplicated for \neach instantiation whereas homogeneous implementations share the same code and method table for different \ninstantiations. .NET offers an intermediate implementation [Kennedy and Syme 2001] that should be markedly \nmore ef.cient but still represents a research issue on JAVA platforms and for PRM. Conclusions. In this \narticle, we have presented the empir\u00adical results of systematic experiments of various implemen\u00adtation \ntechniques and compilation schemes, on a variety of processors. To our knowledge, this is the .rst systematic \nex\u00adperiment that compares such a variety of implementation techniques and compilation schemes, with all \nother things being equal. The latter point was a major challenge of this work. Although these tests were \nperformed on an original language and compiler, they provide reliable assessment of the use of the considered \ntechniques for the implementation of any object-oriented language, as only common elemen\u00adtary mechanisms \nare tested. The conclusions apply in partic\u00adular to production languages like C++, JAVA or EIFFEL.  \nWhen .xing a parameter, the tests provide an estimation of the difference in ef.ciency that must be expected \nwhen the other parameter varies. For instance, in a multiple inher\u00aditance setting, comparing G and D \ngives an estimation of the cost of dynamic loading. It also amounts to comparing EIF-FEL and C++ that \nare closely related to their speci.c compi\u00adlation scheme. Of course, C++ is only considered under the \nvirtual implementation. In a dynamic loading setting, com\u00adparing S and D with attribute coloring is an \nestimation of the cost of JAVA-like multiple subtyping. In contrast, compar\u00ading attribute coloring and \naccessor simulation in D provides an estimation of the extra cost of full multiple inheritance in a dynamic \nloading setting. In practice, the results con.rm that global compilation markedly improves the runtime \nef\u00ad.ciency, even when many optimizations are not considered like inlining. In this setting, the combination \nof coloring and BTD certainly provides the highest ef.ciency. In contrast, dynamic loading always implies \nmarked overhead, even in the restricted case of JAVA interfaces, i.e. when coupled with attribute coloring \n(AC). Another contribution is a .rst empirical assessment of a new technique, perfect hashing, which \nis the .rst known technique that is both time-constant and space-linear in a general multiple inheritance \nand dynamic loading setting. The conclusions are two-fold. PH-and overhead is quite rea\u00adsonable and makes \nit a recommended technique for imple\u00admenting JAVA interfaces, all the more so since recent re\u00adsearch \nshows that its space occupation can also be very good [Ducournau and Morandat 2009]. On the contrary, \nPH-mod is unreasonably inef.cient on many processors. Finally, PH\u00adand+shift is likely not justi.ed, as \nits slight gain in method tables does not offset the slight time overhead and code length increase. Moreover, \nour tests show that an ef.cient underlying technique like PH must be preferred to caching. In contrast, \nthe conclusion concerning the mixed com\u00adpilation scheme with link-time global optimization (O) is somewhat \ndisappointing. A slight improvement was ex\u00adpected and the tests instead show a slight overhead on several \nprocessors. From the time standpoint, the link-time compli\u00adcation of these global optimizations might \nnot be justi.ed since simple global linking (S) is functionally equivalent. However, this is only a .rst \ntest. More complete optimiza\u00adtions, coupled with the hybrid scheme (H), should increase the time and \nspace ef.ciency. The tests were performed on a variety of processors, mainly with the same x86 architecture. \nThough most pro\u00adcessors behave in a similar way, several exceptions lead us to conclude that language \nimplementors should offer alter\u00adnative implementations that might be customized on each speci.c computer \nand operating system. This would be es\u00adpecially useful and easy to carry out for virtual machines that \nrely on portable bytecode. Attempt at Prescription. Of course, the .ndings of these experiments did not \nallow us to de.nitely decide on all pro\u00adcessors and programs. The choice of an implementation will always \ndepend on functional requirements such as dynamic loading. This article is not aimed at providing a prescription \non how object-oriented languages should be implemented. However, starting from the initial three-fold \ntradeoff be\u00adtween modularity, ef.ciency and expressiveness, these .rst results can be formulated in a \nmore prescriptive form as fol\u00adlows. If the point is ef.ciency, e.g. for a small embedded sys\u00adtem, the \nsolution is de.nitely global compilation (G), with a mixing of bounded BTD (restricted to BTD1 if the \nprocessor is not equipped with branching prediction) and coloring. In this framework, the overhead of \nmultiple inheritance is not signi.cant.  If the point is expressiveness, link-time coloring (S) cer\u00adtainly \nrepresents an interesting tradeoff between modu\u00adlarity and ef.ciency. It can be improved with link-time \noptimizations (O). If dynamic loading is not required, the hybrid compilation scheme that combines separate \ncom\u00adpilation of libraries and global compilation of programs  (H) likely provides the best tradeoff \nbetween .exibility for rapid recompilations and ef.ciency of production run\u00adtimes. In this framework, \nthe combination of coloring and BTD provides the most compact and ef.cient code. If the point is modularity, \nC++ is a proven solution in the framework of both multiple inheritance and dynamic loading, that could \nbe improved with empty-subobject optimization. Moreover, compared to JAVA, the overhead of subobjects \nis in practice counterbalanced by template heterogeneous implementation and the fact that primi\u00adtives \ntypes are not integrated in the object type system. This implementation presents, however, some scalability \nrisks, as the worst-case table size is cubic in the num\u00adber of classes and compiler-generated .elds in \nthe ob\u00adject layout can be over space-consuming. Thus the con\u00adclusion may only hold for middle-size programs \nlike the PRM compiler. In contrast, multiple subtyping represents an interesting tradeoff between expressiveness \nand ef.\u00adciency. The ef.ciency of actual runtime systems mostly comes from JIT compilers but PH-and should \nbe consid\u00adered for the underlying interface implementation. Prospects. Our experiments and the PRM testbed \nmust be completed in several directions. For want of time and space, the presented statistics are not \ncomplete; compilation and link time, processor cache misses, runtime memory occupation should also be \ncon\u00adsidered.  The tested implementation techniques, especially color\u00ading and perfect hashing, are not \n.nely optimized from the space standpoint.  The optimization of schemes O and G is not achieved, especially \nwith regard to inlining for G, and dead code   elimination for O. Dead code elimination would be more \ndif.cult with O than with G, as usual linkers are not equipped for deleting code. The hybrid compilation \nscheme (H, Section 3.4) should also be tested. The techniques used in the PRM compiler are fully portable \nwith respect to processors; however global op\u00adtimizations involved in the O scheme closely depend on \nlinkers and operating systems; so a general solution is re\u00adquired before using this scheme in a production \ncompiler. Global link-time coloring (S) represents an ef.cient and simple fallback position.  The subobject-based \nimplementation must be achieved, completed with empty-subobject optimization (ESO). The alternative implementation \nwith thunks [Ducournau 2009] should be considered too. Both are to be precisely compared to PH-and with \nboth attribute coloring and accessor simulation. Anyway, the .nal implementation will never fully mimic \nC++, because of the PRM need for boxing and unboxing primitives types and its current homogeneous implementation \nof generics.  Some techniques can still be optimized, for instance, accessor simulation. Generally, \nit should not be used with accessor methods. With global compilation (G), it should be optimized to take \npossible invariance into account, in a way similar to monomorphic calls.  Polymorphic handling of primitive \ntypes is done in PRM through a mixin of tagging for small integers, characters and boolean, and automatic \nboxing and unboxing,as in JAVA; the testbed should also consider a precise assess\u00adment of these techniques. \n An ef.cient implementation of generics goes midway between homogeneous and heterogeneous implementa\u00adtions, \nas in .NET this is a matter of further research for PRM.  Apart from subobjects which might justify \na fully con\u00adservative collector, a dedicated type accurate or at least semi-conservative garbage collection \nshould reduce the overall time, while increasing the relative differences. This might reverse the conclusions \nof the comparison be\u00adtween subobjects and perfect hashing.  Testing processors from other architectures \nis a condi\u00adtion for these techniques being widespread. The testbed should also consider other C compilers \nthan gcc.  Several experiments with production virtual machines could also take advantage of the techniques \npresented in this article. First, the ef.ciency of perfect hashing for interface implementation should \nbe con.rmed by large-scale tests. Moreover, the thunk-based technique of link-time global optimization \n(O) could also apply to adaptive compilers. Instead of recompiling methods when load-time assump\u00adtions \nare invalidated by some subsequent class loading, only thunks would need recompilation. In view of the \nhigh rate of monomorphic calls and the overhead of all techniques compatible with dynamic loading, this \nwould certainly be an improvement for method invocation when the receiver is typed by an interface maybe \nalso when its is typed by a class. It could be tested in the PRM testbed by coupling PH with global optimizations \n(O) but, as for IC, this would not fully account for the recompilations required by adaptive compilers. \nFinally, in the state space of object-oriented programming language design, there remains a blind spot, \nnamely a lan\u00adguage with full multiple inheritance, like C++ and EIFFEL; fully compatible with dynamic \nloading, like C++, C# and JAVA; with a clean integration of primitive types, like C#, EIFFEL and JAVA.JAVA-likeboxingandunboxingwouldde\u00adgrade \nusual C++ subobject-based implementation and adap\u00adtive compiler techniques are likely less adapted to \nsubobjects than to invariant-reference implementations. However, the best alternative that we can currently \npropose, PH-and with accessor simulation, is about 50% slower than the most ef.\u00adcient implementation \nwith global compilation. Hence, there is room for further research.  References B. Alpern, A. Cocchi, \nS. Fink, and D. Grove. Ef.cient implementa\u00adtion of Java interfaces: Invokeinterface considered harmless. \nIn Proc. OOPSLA 01, SIGPLAN Notices, 36(10), pages 108 124. ACM Press, 2001a. B. Alpern, A. Cocchi, and \nD. Grove. Dynamic type checking in Jalape\u00f1o. In Proc. USENIX JVM 01, 2001b. M. Arnold, S.J. Fink, D. \nGrove, M. Hind, and P.F. Sweeney. A sur\u00advey of adaptive optimization in virtual machines. Proceedings \nof the IEEE, 93(2):449 466, Feb. 2005. D. F. Bacon, M. Wegman, and K. Zadeck. Rapid type analysis for \nC++. Technical report, IBM Thomas J. Watson Research Center, 1996. D.F. Bacon and P. Sweeney. Fast static \nanalysis of C++ virtual function calls. In Proc. OOPSLA 96, SIGPLAN Notices, 31(10), pages 324 341. ACM \nPress, 1996. H.-J. Boehm. Space-ef.cient conservative garbage collection. In Proc. ACM PLDI 93, ACM SIGPLAN \nNotices, 28(6), pages 197 206, 1993. D. Boucher. GOld: a link-time optimizer for Scheme. In M. Felleisen, \neditor, Proc. Workshop on Scheme and Functional Programming. Rice Technical Report 00-368, pages 1 12, \n2000. C. Click and J. Rose. Fast subtype checking in the Hotspot JVM. In Proc. ACM-ISCOPE Conf. on Java \nGrande (JGI 02), pages 96 107, 2002. N.H. Cohen. Type-extension type tests can be performed in con\u00adstant \ntime. ACM Trans. Program. Lang. Syst., 13(4):626 629, 1991. S. Collin, D. Colnet, and O. Zendra. Type \ninference for late bind\u00ading. the SmallEiffel compiler. In Proc. Joint Modular Languages Conference, LNCS \n1204, pages 67 81. Springer, 1997. Z. J. Czech, G. Havas, and B. S. Majewski. Perfect hashing. Theor. \nComput. Sci., 182(1-2):1 143, 1997.   J. Dean, C. Chambers, and D. Grove. Selective specialization \nfor object-oriented languages. In Proc. ACM PLDI 95, pages 93 102, 1995a. J. Dean, D. Grove, and C. Chambers. \nOptimization of object\u00adoriented programs using static class hierarchy analysis. In W. Olthoff, editor, \nProc. ECOOP 95, LNCS 952, pages 77 101. Springer, 1995b. R. Dixon, T. McKee, P. Schweitzer, and M. Vaughan. \nA fast method dispatcher for compiled languages with multiple inheritance. In Proc. OOPSLA 89, pages \n211 214. ACM Press, 1989. K. Driesen. Ef.cient Polymorphic Calls. Kluwer Academic Pub\u00adlisher, 2001. R. \nDucournau. Coloring, a versatile technique for implement\u00ading object-oriented languages. Rapport de Recherche \n06-001, LIRMM, Universit\u00e9 Montpellier 2, 2006. R. Ducournau. Perfect hashing as an almost perfect subtype \ntest. ACM Trans. Program. Lang. Syst., 30(6):1 56, 2008. R. Ducournau. Implementing statically typed \nobject-oriented pro\u00adgramming languages. ACM Computing Surveys, 2009. (to ap\u00adpear). R. Ducournau. Yet \nAnother Frame-based Object-Oriented Lan\u00adguage: YAFOOL Reference Manual. Sema Group, Montrouge, France, \n1991. R. Ducournau and F. Morandat. More results on perfect hashing for implementing object-oriented \nlanguages. Rapport de Recherche 09-001, LIRMM, Universit\u00e9 Montpellier 2, 2009. R. Ducournau and J. Privat. \nMetamodeling semantics of multiple inheritance. Rapport de Recherche 08-017, LIRMM, Universit\u00e9 Montpellier \n2, 2008. N. Eckel and J. Gil. Empirical study of object-layout and opti\u00admization techniques. In E. Bertino, \neditor, Proc. ECOOP 2000, LNCS 1850, pages 394 421. Springer, 2000. M.A. Ellis and B. Stroustrup. The \nannotated C++ reference man\u00adual. Addison-Wesley, Reading, MA, US, 1990. M.R. Garey and D.S. Johnson. \nComputers and Intractability. A Guide to the Theory of NP-Completeness. W.H. Freeman and Company, San \nFrancisco (CA), USA, 1979. J. Gil and P. Sweeney. Space and time-ef.cient memory layout for multiple \ninheritance. In Proc. OOPSLA 99, SIGPLAN Notices, 34(10), pages 256 275. ACM Press, 1999. A. Goldberg \nand D. Robson. Smalltalk-80, the Language and its Implementation. Addison-Wesley, Reading (MA), USA, \n1983. D. Grove and C. Chambers. A framework for call graph construc\u00adtion algorithms. ACM Trans. Program. \nLang. Syst., 23(6):685 746, 2001. S. P. Harbinson. Modula-3. Prentice Hall, 1992. U. H\u00f6lzle, C. Chambers, \nand D. Ungar. Optimizing dynamically\u00adtyped object-oriented languages with polymorphic inline caches. \nIn P. America, editor, Proc. ECOOP 91, LNCS 512, pages 21  38. Springer, 1991. R. Jones and R. Lins. \nGarbage Collection. Wiley, 1996. A. Kennedy and D. Syme. Design and implementation of gener\u00adics for the \n.NET Common Language Runtime. In Proc. ACM PLDI 01, pages 1 12. ACM Press, 2001. S. B. Lippman. Inside \nthe C++ Object Model. Addison-Wesley, New York, 1996. B. Meyer. Eiffel: The Language. Prentice-Hall, \n1992. B. Meyer. Object-Oriented Software Construction. Prentice-Hall, second edition, 1997. F. Morandat, \nR. Ducournau, and J. Privat. Evaluation de l ef.cacit\u00e9 des impl\u00e9mentations de l h\u00e9ritage multiple en \ntypage statique. In B. Carr\u00e9 and O. Zendra, editors, Actes LMO 2009, pages 17 32. C\u00e9padu\u00e8s, 2009. H. \nM\u00f6ssenb\u00f6ck. Object-Oriented Programming in Oberon-2. Springer, 1993. S. Muthukrishnan and M. Muller. \nTime and space ef.cient method lookup for object-oriented languages. In Proc. ACM-SIAM Symp. on Discrete \nAlgorithms, pages 42 51. ACM/SIAM, 1996. A. Myers. Bidirectional object layout for separate compilation. \nIn Proc. OOPSLA 95, SIGPLAN Notices, 30(10), pages 124 139. ACM Press, 1995. M. Odersky and P. Wadler. \nPizza into Java: Translating theory into practice. In Proc. POPL 97, pages 146 159. ACM Press, 1997. \nK. Palacz and J. Vitek. Java subtype tests in real-time. In L. Cardelli, editor, Proc. ECOOP 2003, LNCS \n2743, pages 378 404. Springer, 2003. J. Privat and R. Ducournau. Link-time static analysis for ef.cient \nseparate compilation of object-oriented languages. In ACM Workshop on Prog. Anal. Soft. Tools Engin. \n(PASTE 05), pages 20 27, 2005. W. Pugh and G. Weddell. Two-directional record layout for mul\u00adtiple inheritance. \nIn Proc. PLDI 90, ACM SIGPLAN Notices, 25(6), pages 85 91, 1990. O. Shivers. Control-Flow Analysis of \nHigher-Order Languages. PhD thesis, Carnegie Mellon University, 1991. R. Sprugnoli. Perfect hashing functions: \na single probe retrieving method for static sets. Comm. ACM, 20(11):841 850, 1977. G.L. Steele. Common \nLisp, the Language. Digital Press, second edition, 1990. P. F. Sweeney and M. G. Burke. Quantifying \nand evaluating the space overhead for alternative C++ memory layouts. Softw., Pract. Exper., 33(7):595 \n636, 2003. S. T. Taft, R. A. Duff, R. L. Brukardt, E. Ploedereder, and P. Leroy, editors. Ada 2005 Reference \nManual: Language and Standard Libraries. LNCS 4348. Springer, 2006. F. Tip and P. F. Sweeney. Class \nhierarchy specialization. Acta Informatica, 36(12):927 982, 2000. J. Vitek, R.N. Horspool, and A. Krall. \nEf.cient type inclusion tests. In Proc. OOPSLA 97, SIGPLAN Notices, 32(10), pages 142  157. ACM Press, \n1997. O. Zendra, D. Colnet, and S. Collin. Ef.cient dynamic dispatch without virtual function tables: \nThe SmallEiffel compiler. In Proc. OOPSLA 97, SIGPLAN Notices, 32(10), pages 125 141. ACM Press, 1997. \nY. Zibin and J. Gil. Two-dimensional bi-directional object layout. In L. Cardelli, editor, Proc. ECOOP \n2003, LNCS 2743, pages 329 350. Springer, 2003.  \n\t\t\t", "proc_id": "1640089", "abstract": "<p>Object-oriented languages involve a threefold tradeoff between runtime efficiency, expressiveness (multiple inheritance), and modularity, i.e. open-world assumption (OWA). Runtime efficiency is conditioned by both the <i>implementation technique</i> and <i>compilation scheme</i>. The former specifies the data structures that support method invocation, attribute access and subtype testing. The latter consists of the production line of an executable from the source code. Many implementation techniques have been proposed and several compilation schemes can be considered from fully global compilation under the closed-world assumption (CWA) to separate compilation with dynamic loading under the OWA, with midway solutions. This article reviews a significant subset of possible combinations and presents a systematic, empirical comparison of their respective efficiencies with <i>all other things being equal</i>. The testbed consists of the Prm compiler that has been designed for this purpose. The considered techniques include C++ subobjects, coloring, perfect hashing, binary tree dispatch and caching. A variety of processors were considered. Qualitatively, these first results confirm the intuitive or theoretical abstract assessments of the tested approaches. As expected, efficiency increases as CWA strengthens. From a quantitative standpoint, the results are the first to precisely compare the efficiency of techniques that are closely associated with specific languages like C++ and Eiffel. They also confirm that perfect hashing should be considered for implementing Java and .Net interfaces.</p>", "authors": [{"name": "Roland Ducournau", "author_profile_id": "81100004519", "affiliation": "Universit&#233; Montpellier 2, CNRS, Montpellier, France", "person_id": "P1728716", "email_address": "", "orcid_id": ""}, {"name": "Flor&#233;al Morandat", "author_profile_id": "81444607842", "affiliation": "Universit&#233; Montpellier 2, CNRS, Montpellier, France", "person_id": "P1728717", "email_address": "", "orcid_id": ""}, {"name": "Jean Privat", "author_profile_id": "81309481459", "affiliation": "Universit&#233; du Qu&#233;bec &#224; Montr&#233;al, Montr&#233;al, Canada", "person_id": "P1728718", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1640089.1640093", "year": "2009", "article_id": "1640093", "conference": "OOPSLA", "title": "Empirical assessment of object-oriented implementations with multiple inheritance and static typing", "url": "http://dl.acm.org/citation.cfm?id=1640093"}