{"article_publication_date": "10-25-2009", "fulltext": "\n Allocation Wall: a Limiting Factor of Java Applications on Emerging Multi-core Platforms YiZhao Jin \nShi Kai Zheng IBM China Research Lab Tsinghua University IBM China Research Lab zhaoyizy@cn.ibm.com shijin1984@gmail.com \nzhengkai@cn.ibm.com Haichuan Wang Haibo Lin LingShao IBM China Research Lab IBM China Research Lab IBM \nChina Research Lab wanghaic@cn.ibm.com linhb@cn.ibm.com shaol@cn.ibm.com Abstract Multi-coreprocessors \nare widely usedin computer systems. As theperformance of microprocessorsgreatly exceeds that of memory, \nthe memory wall becomes a limiting factor. It isimportantto understandhowthelargedisparity of speed betweenprocessor \nand memoryin.uences theperformance and scalability of Java applications on emerging multi-core platforms. \nIn this paper, we studied two popular Java benchmarks, SPECjbb2005 and SPECjvm2008, on multi-core platforms \nincludingIntelClovertown andAMDPhenom.Wefocus on the partially scalable benchmarkprograms.With smaller \nnumber of CPU cores these programs scale perfectly, but when more cores and software threads are used, \nthe slope of the scalability curvedegradesdramatically. Weidenti.eda strongcorrelationbetween scalability, \nob\u00adject allocation rate andmemory bus write traf.c in our ex\u00adperimentswithourpartially scalableprograms.We \n.nd that these applications allocate large amounts of memory and consumealmostallthe memorywritebandwidthin \nourhard\u00adwareplatforms. Becausethewritebandwidthissolimited, weproposethefollowinghypothesis:the scalabilityandper\u00adformance \nis limited by the object allocation on emerging multi-core platforms for those objects-allocation intensive \nJava applications, as if these applications are running into an allocation wall . In orderto verifythishypothesis, \nseveralexperiments are performed,including measuringkey architecturelevel met- Permission to make digital \nor hard copies of all or part of this work for personal or classroomuseisgranted withoutfeeprovided that \ncopiesarenot madeordistributed forpro.tor commercial advantage andthat copiesbearthis notice andthefull \ncitation onthe .rstpage.Tocopy otherwise,torepublish,topostonserversortoredistribute tolists, requiresprior \nspeci.cpermission and/or afee. OOPSLA2009, October25 29,2009,Orlando,Florida,USA. Copyright c &#38;#169; \n2009ACM978-1-60558-734-9/09/10. . .$10.00 rics, composing a micro-benchmarkprogram,and studying theeffect \nof modifying someof the partially scalable pro\u00adgrams.Allthe experiments strongly suggestthe existence \nof the allocation wall. Categories and Subject Descriptors D.3.4Processors[Pro\u00adgramming Languages]: Memorymanagement(garbage \ncol\u00adlection) General Terms Experimentation,Performance Keywords Allocation,Scalability,Java 1. Introduction \nWhile theperformance of singleprocessor reaches thelimit of so called frequency wall , power wall and \nmemory wall [13], and single core or hardware thread limits the parallelism availablefor many applications,CMP[12] \nand SMT[41]becomethe technologytrendtheseyears. Although the issues of frequency and power are ad\u00addressed \nwith multi-core processors and better semiconduc\u00adtor technologies, the performance gap between processor \nand memory is even wider. While more and more cores are integrated into a processor package, the interconnec\u00adtionbandwidthbetweenprocessorandmemorygrows \nrather slowly. It is important to understand how the large dispar\u00adity of speed between processor and \nmemory in.uences the performance and scalability of Java applications on emerg\u00ading multi-coreplatforms.With \nthispurpose, we studied two wellknownJavabenchmark suites,SPECjbb2005[34] and SPECjvm2008[36],on multi-coreplatformsbased \nonIn\u00adtel Clovertown (Xeon with core2 architecture) and AMD Phenom(QuadcoreK10). The study of these benchmarks \nreveals that the memory bandwidth for write operations is limiting the performance and scalability of \na speci.c partially scalable group of Javaprograms on emergingmulti-coreplatforms.Programs in thisgroupall \nallocatelarge amounts of objectsduringthe entire execution time. The memory write traf.c caused by cache \neviction ondirty cache-lines consumes almost all the writebandwidth of the memory sub-system. Byperforming \nseveral carefully designed experiments, we identify the ob\u00adject allocation rate as the main reasonfor \nthe massive write operations.This means thelargegapbetweenprocessor and memory leads to a limitation \non the object allocation rate andfurtherin.uences the overallperformance and scalabil\u00adity of allocation-intensive \nJava applications, as if these ap\u00adplicationsarerunningintoan allocationwall .  We categorize the benchmarks \nbased on our scalability experimentsinthreegroups:the hardly scalable programs whichhaveissues such aslock \ncontention orlimited thread number, the fully scalable programs which are already good enough, and the \npartially scalable programs which scale well with smaller number of cores, but the scalability degrades \ndramatically when larger number of cores are en\u00adabled. In this paper, we focus on the partially scalable \npro\u00adgrams in the benchmarks. Several veri.cation experiments were performed with different hardware platforms \n(Intel Clovertown andAMDPhenom),differentJVMs(SunJVM andIBMJ9VM),differentGCpolicies(generationalvs. \nnon\u00adgenerational, parallel vs. sequential), different heap sizes (from 2MB to 2GB). We observe that the \nphenomenon of partially scalable programs remains the same with dif\u00adferent combinations for hardware \nand software. This result leads us to believe that the partially scalable programs are not causedby some \naccidentalfactorsfor some speci.c hardware or softwareissues. Bystudying theGClogs,we.nd thatthe partially \nscal\u00adable programs all allocate signi.cantlarger amount(2to 20 times more) of memory compared to those \nfully scal\u00adable ones.Thebenchmark applications are then studied on micro-architecturelevel.TheL1/L2 cache,TLB, \nand mem\u00adorybehavior are measuredby usinghardware counters.The experimentsrevealthatthere are onlytwofactorsthat \nsignif\u00adicantly differentiate the partially scalable programs from the fully scalable ones: the memory \nwrite bandwidth and theL2 miss rate. We found that the amount of memory allocated and the memorybus writetraf.c \nare so closeforallbenchmarksin the partially scalable group, that we believe that there is a strong correlationbetween \nthese twofactors.Wedesigned two experimentsto understandthis correlation: Recording the address of each \nallocated object to mea\u00adsure the actualfootprint ofJava applications;  Composinga micro-benchmarkprogramtotestobject \nal\u00adlocation alone, whichdiscards all objects allocated with\u00adout modifying anything onthe objects.  Besides \nthe results from these experiments, a detailed step\u00adby-step explanationof why objectallocationleadsto \nmem\u00adory buswritetraf.cisgiven.We .nd thatobjectallocation contributes at least 60% -90% of the memory \nwrite traf.c forthe partially scalable programs. Onthe otherhand,the memorybandwidthforwrite op\u00aderationsis \nfar not enough. By measuring the read and write bandwidthonourhardwareplatforms,we .ndthatthewrite bandwidthis \nnotonly muchlowerthanthe nominal value of theDDR orFB-DIMM speci.cationbut also much smaller thanthe \nreadbandwidth. As a result of combination of large write requirements for object allocation and limited \nwrite bandwidth from the memorysub-system,the overallperformanceof such alloca\u00adtionintensiveJava applications \nareboundbythe object allo\u00adcation rate.The allocation wallissue only affectstheperfor\u00admanceandscalabilityofsuchapplicationswithlargenumber \nof cores.With smaller number of cores, the applications can not runfast enoughto allocatethat many objectsto \nconsume all the memory writebandwidth. To further verify our theory of allocation wall, we mod\u00adi.ed two \nof our partially scalable programs to reduce the numberofallocatedobjects.SPECjbb2005andSPECjvm.de\u00adrbyweremodi.edto \nreusethe mostheavilyallocatedobjects with the help of object pools. The memory write traf.c is reduced \nand signi.cantimprovement on scalability andper\u00adformance is observed. This positive result proves that \nour hypothesis ofthe allocation wallis correct. Major contributions ofthispaperinclude: The scalability \nof SPECjbb2005 and SPECjvm2008 is studied withIntelClovertown[16] andAMDPhenom [2] multi-core processors. \nEspecially we focus on the programswhichareidenti.edtobe partially scalable .  A new hypothesis of allocation \nwall is proposed and veri.ed. It describes the phenomenon that the memory write bandwidth becomes the \nlimiting factor of alloca\u00adtion intensive Java applications on emerging multi-core platforms.  The many \ncarefully designed experimentsintroducedin thispapercanalsobeused asa methodologyfordetect\u00ading whether \naprogramhits the allocation wall or notfor otherJava applications.  This paper is organized as follows: \nSection 2 summa\u00adrizes relatedwork on memorysub-system of multi-coreplat\u00adforms,Java application characteristics \nandJava memorybe\u00adhavior, The experiment method, platforms and parameters are described in Section 3, \nthe scalability of SPECjbb2005 andSPECjvm2008isstudiedinSection4 withthe partially scalable programshighlighted, \nandthe correlationbetween object allocation and memory bus write traf.c is explained and veri.edindetail,inSection5 \nweexperimented the ob\u00adjectreusetofurtherprove ourhypothesis,inSection6, more veri.cation experiment results \nare given to make our anal\u00adysis more solid, and in Section 7 the conclusion and future work arediscussed. \n  2. RelatedWork 2.1 JavaMemoryBehavior InJava,pointers or memory addresses are not exposed, and garbage \ncollector is in charge of reclaiming unreachable memory chunks. Thus, application performance and mem\u00adorybehaviorwillheavilydepend \nongarbage collectionpoli\u00adcies. Blackburn et al [3] compared many GC algorithms based on MMTk of JikesRVM \non AMD Athlon and Intel Pentium4platforms, andpointedout that thebene.t of con\u00adtiguous allocation dominates \nthe overheads of generational collectors disputing the myth that no garbage collection is good garbage \ncollection . However, in our experiments withproductJVMs(SunJVM andIBMJ9VM) on emerging mulit-core platforms, \nthe object allocation becomes a seri\u00adousperformancebottleneckin mutatortimefor allGC algo\u00adrithms that \nwe tried. Thegarbage collectionpoliciesin commercialJVMs[38, 28] employ basic garbage collection algorithms \nsuch as mark-and-sweep[27], semi-space copying[9] and genera\u00adtional collection[23, 42]. Such garbage \ncollectors will only reclaim memory for a whole region of memory, and there\u00adforememorychunkscan notbe \nreusedintime andhavetobe written back to memory from cache. Thus, identifying and reusing unreachable \nmemory chunks early enoughis critical for memorylocality. Cherem et al[6] andDillig et al[8] proposed \noptimiza\u00adtionstodeterminelifetime ofobjects.Suchobjectscanbere\u00adclaimed as soon as theybecomeinvalid statically.Xian \net al [45]proposedtoinvokegarbagecollectorearlierinthe al\u00adlocationpause (aperiodwithfew object allocations),based \nontheirobservationthat most objectsjustdieinthe alloca\u00adtionpause . Shankar et al [31] proposed a new \napproach to reduce object churn in JIT of J9, and found that the new approach eliminates over 4 times \nas many allocations as a state-of\u00adthe-artescape analysis.They also evaluatedtheperformance of SPECjbb2005 \nwith there churn reduction approach, and found that by eliminating 26.5% more of object allocation compared \nto the baseline, 13.2% performance gain is ob\u00adtained, which is consistent with the results of our experi\u00adments. \nHuang et al [14] studied the object allocation behavior of Java multithreaded applications, and suggested \nthat sub\u00adheap couldimprovetheperformanceofJava memory man\u00adagement,andidenti.ed thatSPECjbb2000is an allocation\u00adintensive \napplication.Iyer etal[18] studiedtheperformance ofSPECjbb2005 onLargeCMP andSmallCMPplatforms with theapproach \nof simulation.Theyfound thatthe mem\u00adorybandwidthis a criticallimitingfactorforlargemulti-core CMP architectures, \nand there is a signi.cant performance gainbydoublingthe memorybandwidth. Reference counting [19] has \nseveral advantages: early identi.cation ofgarbage,incremental collection, andbetter spacelocality.However,it \nsuffersfrom cyclic structures and overheads of maintaining the reference count, so it is not popular \nin modern programming languages. To equip Java with real-time abilities,RTSJ(Real-TimeSpeci.cationfor \nJava)[1] added new memory models. The scoped memory has a speci.clifetime associatedwith aparticularsection \nor a scope, and all objectsinit willbedestroyed onleavingthe scope. 2.2 GapBetweenProcessorandMemory \nAs the improvement speed of microprocessors greatly ex\u00adceedsthat of memory,the memory wall [44]hasbeenwell \nknown for a long time. Many techniques to reduce mem\u00adory latency require more memory bandwidth. Burger \net al [5]points out thatwasted cyclesdue toinsuf.cient memory bandwidthcan exceed cyclesdue to raw memorylatency.It \nwas also claimedin[5]that off-chipmemorybandwidth can bebarrierforCMP(ChipMultiprocessor). Inthe multi-core \nera, more cores andthreadsinprocessor will enlarge theprocessor memorygap.Spracklen et al[37] addressed \nthe off-chip bandwidth as a potential bottleneck limitingboththe number andaggressiveness of cores. Asincreasingbusfrequency \nand widthforDDRx mem\u00adory cannot be achieved easily in near future, the Fully-Buffered DIMM[43] memory \narchitecture has emerged as an alternative. FB-DIMM is by design more favorable to read operationssince \nburstread (readforablock,forex\u00adample a cacheline)is morefrequent on most workloads, so the northbound \nchannel(14bits) is widerthan southbound channel(10bits).Ganesh et al[10] gave adetailedperfor\u00admance analysis \noftheFB-DIMM memory.Theypointed out southboundchannelas a resource under contention and even apotentialbottleneck.They \nalso studied memorybandwidth andlatency respectivelytodifferent read to write ratios. Stream[26] is a \nwidely used memorybandwidthbench\u00admark. Based onthecollectedbenchmark resultsontheweb site ofStream,thegap \nbetweenprocessorcomputing capa\u00adbility with the memorybandwidthhasbeengrowingfor2.5 times from year 2000 \nto 2007, and is keep growing more rapidlywhenmorecores arebuiltinto singleprocessorpack\u00adages.  2.3 JavaWorkloads \nSPECjbb2005 and SPECjvm2008 have been widely used in representing workload characteristicsonmulti-coreplat\u00adforms. \nLuo et al [24] found that CPI correlates well with throughputinSPECjbb2000;wefoundit also trueinSPEC\u00adjbb2005.Tsengetal[40]studiedscalabilityofSPECjbb2005 \nin details on a Sun Niagara[20] system, and found that SPECjbb2005scaled almostlinearly with core-scaling(over \n90%ef.ciency)and also well withthread-scaling(over60% ef.ciency).Shiv et al[33] studiedSPECjbb2005 on \nanIn\u00adtel s Core 2 system, and identi.ed the critical role of cache and memory sub-system.They evenpointed \noutthat object allocationcontributed nearlyhalf ofdata cachemisses, and proposedseveral micro-architecturelevel \nsolutions.  Marden et al [25] compared two implementations of SPECweb99 on Java and Perl, and found \nthat the memory behavior of the two versionsis verydifferent.TheJava ver\u00adsion incurs 213% more cache \nmisses than the Perl version for a1MBL2cache and179% more missesfor a16MBL2 cache. Much of this increase \nis due to true sharing, which is 222% to 415% higher in the Java workload than in Perl. Seshadri et al[29] \nstudiedSPECjbb2000 andVolanoMark, and compared them to SPECint2000. They found that Java workloads exhibited \nhigh L2 miss rates due mostly to data loads.Tikir etal[39]proposedthe memorylayout ofNuma-AwaredJavaHeapand \nachieved40% reduction of execution time with 41% reduction of non-local memory accesses on SPECjbb2000,implying \nthatthe memory accessis adomi\u00adnantfactor of theperformance ofSPECjbb2000. Blackburn et al [4] proposed \nthe DaCapo benchmark suite, with more complex code, richer object behaviors and more demanding memory \nsystem requirements. However, wedid not useDaCapoin our experimentsbecauseitis not suitable for evaluating \nscalability issues. Only three of the programs(hsqldb,lusearchand xalan)inDaCapo are multi\u00adthreaded, \nand none of them makes fully use of all the CPU resource. Shiv et al[32] studiedSPECjvm2008 on multi-coreplat\u00adforms, \nmany factors of SPECjvm2008 are measured, in\u00adcluding the object allocation rate, the memory bandwidth \nrequirement and micro-architecture metrics of TLB, CPI, FPopetc.Duetodifferences onthehardwareplatform,their \nabsolute values are notprecisely equal to ours,but the trend representedby their values matches with \nour experiment re\u00adsults.  3. ExperimentPlatformsandMethod 3.1 HardwarePlatforms AnIBM eServer BladeCenterHS21[15]withtwoIntelXeon \nE5345[16](Clovertown)quad-coreprocessorsis usedin all the experimentsin thispaper.There aretwoprocessorpack\u00adages \nofXeonE5345runningat2.33GHzinstalledinthe sys\u00adtem.Therearefourcoresin each of theprocessorpackage. Each \ncore has its independent L1 data cache, L1 instruc\u00adtion cache andTLBs.TheL1 cache sizeis64KB(Instruc\u00adtion) \n+64KB(Data).Apair of cores are sharing a4MBL2 cache(16MBin total).Our systemdoes nothaveL3 cache. Each \nprocessor package, is connected to Intel 5000P[17] MCH(MemoryControllerHub) with a1333MHzPoint-to\u00adPointindependentFSBnamedDIB(DualIndependentBus). \nThere are8GBDDR2-667FB-DIMMmemoryindualchan\u00adnel. TheHS21 serveris used asthe majorplatformformost of \nthe experiments,since webelievethatthelargerproces\u00adsor/memorygap(cores:mem channels = 4:1) onHS21 better \nrepresents the trend of next generation platforms such as Intel Nehalem [7] with 16 hardware threads \nand  (b) AMDPhenom Figure 1. Core, cache and memory structure of the hard\u00adwareplatformsthat we usein \nthe experiments 4memory channelsper eachprocessorpackage(hw threads :mem channels =4:1). We also used \nan AMD Phenom[2] 9950 quad-core pro\u00adcessor as a supplemental platform in some of our experi\u00adments,to \nverifyour .ndingontheClovertownplatform.The AMDPhenomisbasedonAMD s newK10 architectureand have the similar \ncore, cache and memory layout with the Opteron(the server version of the same architecture).There is \na single Phenom 9950 processor running at 3.0GHz in\u00adstalled.There arefour coresin theprocessor.Each corehas \nits independent L1 and L2 cache as well as TLBs. The L1 cache size is 64KB (Instruction) + 64KB (Data) \nfor each core. The L2 cache size is 512KB for each core. The pro\u00adcessorisinstalled on aAMD780G motherboard.There \nare 4GBDDR2-800memoryindual channel connecteddirectly to the processor s built-in memory controller. \nThe AMD Phenom system is used as the veri.cation platform for the IntelClovertown system.  3.2 SoftwareCon.guration \nFor both the Clovertown and the Phenom platforms, we use the same software stack to achieve consistent \nresults. Detailedcon.gurationislistedinTable1.  Table1. Software con.guration Software Name Version \nOS RHEL 5.2 Kernel Linux 64bit 2.6.18-92 JVM SunJVM 32bit 1.6.0 JVM IBMJ9VM 32bit 1.6.0 Benchmark SPECjbb2005 \n1.07 Benchmark SPECjvm2008 1.00  3.3 WorkloadIntroduction We focus on multi-threaded Java server-side \napplications. SPECjbb2005andSPECjvm2008havebeenchosenfortheir goodthread-levelparallelism. SPECjbb2005 \nis abenchmarksimulating anonlinemar\u00adketing system. It has thepopular3-tier architecture, and emphasizes \nonthebusinesslogic.Itis speciallydesigned for evaluatingperformance ofserver-sideJavaplatforms.  SPECjvm2008 \nis a benchmark suite, containing several realworldapplications.Itis ageneralpurposebenchmark suiteforJRE(Java \nRuntimeEnvironment) with associ\u00adatedOS andhardware.Most ofbenchmarkprogramsin SPECjvm2008are multi-threaded \nand suitablefor evalu\u00adating scalabilityissues on multi-coreplatforms.  Most of the benchmarks have good \nthread level paral-lelism.Lockor resource contentionis not adominantfactor. As a result, mostofthebenchmarkprogramstendtotakefull \nCPU utilization andimpose morepressure ontheJRE, op\u00aderating system and hardware, making potential bottlenecks \nmore evident. 3.4 ExperimentMethod andParameters For eachofthe experiments,we runthetest scriptfor atleast \n5times, andthebestresultis selected.Althoughthis method is notthe most optimal as suggestedbyGeorges \net al[11] forJava experiments,itis enoughforthe analysis ofthedata in thispaper. We measured scalability \nwith the same number of cores enabledinOS asthenumberof threadsof the applications. Sothe core-number \ninall the.guresalsomeansthenum\u00adber ofthreads of the measured applications. The default parameters of \nthe JVM and benchmarks are listed asbelow: SunJVM: java-Xmx3g -Xms3g -Xmn2g -server -Xss128k -XX:+UseParallelGC \n-XX:+UseParallelOldGC -XX:+UseBiasedLocking -XX:+AggressiveOpts IBMJ9VM: java-Xmx3g -Xms3g -Xmn2g -Xjvm:perf \n-XlockReservation -Xnoloa -XtlhPrefetch -Xgcpolicy:gencon  SPECjbb2005: java$JVM PARAMSspec.jbb.JBBmain \n-prop.leSPECjbb.props  SPECjvm2008: java$JVM PARAMS-jarSPECjvm2008.jar -ikv -ict -wt30-it30 -bt $THREAD \nNUMBER  The default parameter is used in most of the experiments exceptforthose with explicit claims \non theparameters. Aslisted above,thedefaultJVMheap sizeis3GB, with 2GBdedicated totheyounggeneration. \nBoth theSunJVM andIBMJ9VM areusinggenerationalandconcurrentGCby default. These parameters are chosen \naccording to most of submittedbenchmark results on www.spec.org.The2G/3G heap size looks extremely large \nat .rst glance, but many vendors used similar con.gurations(AMD:2.9G/3.7G,HP: 3.2G/3.7G,IBM 1.4G/1.8G).Although \nwe believe this size was well tunedby the vendors, we stillperformed extensive veri.cation experiment \non selecting the heap size in Figure 16, and our experiments also shows that most of ourbench\u00admarks achievethebestperformance \nwith ayoung space size of2GB.ThedefaultGCpolicy was alsobased oncommon choices of many vendors.  3.5 \nMeasuringMemoryWriteBandwidth We obtained two memory bandwidth measurements in this paper: Memorybus \ntraf.cbandwidth:the actualmemoryband\u00adwidth consumedby an application;  Hardwarebandwidthlimit: the upperboundof \nthe mem\u00adorybandwidthprovidedby thehardwareplatform.  Memory bus traf.c is measured by monitoring hardware \ncounters(BUS TRAN BRDandBUS TRAN WB)torecord thehardware event of memorybus operations while the ap\u00adplicationis \nrunning. Measuring the bandwidth limit is more dif.cult. In our experiments,wehave noticedthatthe upperboundforband\u00adwidth \nlimit varies with the application. This is contrary to our intuition that the hardware bandwidth should \nbe equal and constant for all the applications. After studying the memory bus structure like DDR2 and \nFB-DIMM, we have discoveredthat thebandwidthlimitisheavilyin.uencedby the application s memory access \npattern, especially the ra\u00adtio ofreadto write operations.The read and write operations arein.uencingeachotheronthe \nmemorybus.Moredetailed information couldbefoundin[43].The maximum readand writebandwidtharevariableondifferent \nread:write ratios. In ordertogetthe accurate readand writebandwidthdata for aparticular workload,thebandwidthhastobe \nmeasured with the same read:write ratio as the workload does. Since traditional memorybenchmarksdoes \nnot supportthe adjust\u00admentof read:writeratio, wetriedtwo approachesto simulate thedifferent read:write \nratio.     1. Threadlevelmixing of read and write operations We usethelmbench3[21] toolfor a singlethread \nmea\u00adsurements. Since the lmbench3 is only able to measure onekind of operation, either read or writebut \nnotboth, a groupoflmbench3 threads arelaunched simultaneously. In order to measure a particular ratio \nof read:write, the same ratio of reading threadsand writing threadsoflm\u00adbench3is used. 2. Instructionlevelmixing \nof read and write operations Real world applications usually access memory with a .ne read and write \nmixture from a single instruction stream. In order to simulate such access behavior accu\u00ad  rately, we \ndeveloped a new simple memory benchmark tool.The coreidea of this toolis toiterate: write buf[i] = read \nbuf1[i] + read buf2[i] + read buf3[i] + ....+ read bufk[i]; through(k+1) large arraysto simulatethe(read:write) \nratio of(k:1). Theperformanceresultsobtainedfromthesetwo approaches are similarin ourexperiments,thus \nwe onlypresent results basedon thelmbench3tool.  4. CorrelationbetweenScalability and ObjectAllocationRate \nTheSPECjbb2005 andSPECjvm2008benchmark suite are studied in this section. By running these benchmark \npro\u00adgrams with different number of CPU cores, the scalability of these programs are measured and categorized \ninto three groups, the fully scalable , the partially scalable and the hardly scalable .The partially \nscalable programsattract most of our attention,sincetheseprogramsscales nearlin\u00adearly with smaller number \nof cores, while the scalabilityde\u00adgrades dramatically with larger number of cores. The phe\u00adnomenon of \nthe fast degradation of scalability is studied in this section, and the correlationbetween scalability \nand ob\u00adject allocation rate is revealed by micro-architectural level analysis. 4.1 ThreeCategoriesofScalability \nScalabilityofSPECjbb2005andSPECjvm2008benchmarks areshowninFigure2,Figure3 andFigure4,with normal\u00adizedvalue. \nThese benchmark programs are categorized into three groupsbasedontheirscalabilitybehaviorwithdifferentcore \nnumbers. Hardly scalableprograms The group of hardly scalable benchmark programs are depicted in Figure \n2(a) including scimark.*.large, se\u00adrial and sun.ow.In accordance withSPECjvm2008 user guide[35]: The \nscimark.*.large benchmarks are de\u00adsigned to stress the memory sub-system with very large data arrays, \nresulting in cache and TLB inef.ciencies. 9 8 7 6 5 4 3 2 1 0Number of cores (a) Normalized throughput \n CPU utilization Normalized throughput 100% 90% 80% 70% 60% 50% 40% 30% 20% 10% 12345678 Number of cores \n(b) CPUUtilization Figure2. hardly scalable programs ofSPECjvm2008 Although the problem of the scimark.*.large \nis also causedbyprocessor-memorygap,itis notthefocus of this paper, because the cause of the performance \nissue is well understood. Sun.ow and serial have other con\u00adtentionissues sotheycannotfullyusetheCPU resources, \nasdepictedinFigure2(b). This group of programs does not take bene.t from the multi-coreprocessor.Thebad \nscalability oftheprograms are caused by the program design or code quality is\u00adsues, and can notbe easily \naddressedby system-level ap\u00adproaches, sodetailsfor theseprograms are notdiscussed in thispaper. Fully \nscalableprograms The group of fully scalable benchmark programs is de\u00adpictedinFigure3(a),includingcompress, \ncrypto.*, mpe\u00adgaudio, scimark.*.small and scimark.monte carlo. This group of programs is already good \nenough on the cur\u00adrenthardwareplatforms, sotheyare used asthe reference sampleto study why otherprograms \nare not scaling well. Partially scalableprograms Thegroup ofpartially scalableprograms aredepictedin \nFigure 4(a) including SPECjbb2005, compiler.compiler,  9  9 8 8 7 7 CPU utilization Normalized throughput \nCPU utilization Normalized throughput 6 6 5 5 4 4 3 3 2 2 1 1 0 0Number of cores Number of cores (a) \nNormalized throughput (a) Normalized throughput  100% 90% 80% 70% 60% 50% 100% 90% 80% 70% 60% 50% 40% \n30% 20% 10% 12345678 Number of cores (b) CPUUtilization Figure3. fully scalable programs ofSPECjvm2008 \n compiler.sun.ow,derby,xml.transformandxml.validation. These benchmarks scale perfectly with a small \nnumber of cores, scalability degrades substantially with a large number of cores. Comparing the CPU utilization \ndata in Figure 4(b) with the performance in Figure 4(a), this groupofprogramsconsumeall theCPU cycles,demon\u00adstrating \ngood software parallelism, but the performance is not scaling along with theCPU utilization.Thisgroup \nofprogramsisthefocus of thispaper.Wetry to answer why suchinterestingbehavior exists ontheseprograms. \nBased on such results, since there are many programs with the same scalability issue, we tend to believe \nthat the partial scalability is not accidental.Itis commonin certain type ofJava applications, and maybecomepotentialbottle\u00adneck \non emerging multi-coreplatforms.Soitisimportantto understandthe root cause of suchproblem.  4.2 ObjectAllocationRateandMicroarchitecture \nLevelAnalysis We measuredmanyfactorsthatare commonlybelievedtobe in.uencing the performance of applications, \nincluding the L1/L2 Cache miss rate, the D-TLB miss rate by hardware counters. We also measured the memory \nallocation rate by 40% 30% 20% 10% 12345678 Number of cores (b) CPUUtilization Figure 4. partially scalable \nprograms of SPECjvm2008 andSPECjbb2005 readingtheGClog.The scalability characteristic wasfound tohave \nsigni.cant correlation with object allocation rate. Figure 5, compares the object allocation rate for \nfully scalable and partially scalable programs. The object allocation rate is calculated from garbage \ncollection fre\u00adquency and the size of memory reclaimed by the GC. Al\u00adloc Rate=(UsedMem BeforeGC -UsedMem \nAfterGC) * GC Frequency.The objectallocation rateis clearlydifferent forthe fully scalable programs,demonstratingsigni.cant \ncorrelationbetweenthe scalabilityandobjectallocationrate. The fully scalable programs allocate reasonable \namount of objects during the execution, indicating a good balance of processingpower andobject allocation. \nThe object allocation rate of the partially scalable pro\u00adgramsis extremelyhigh.SPECjvm2008.derbyhasthehigh\u00adest \nrate, at about1400MB/s, andSPECjbb2005hasthelow\u00adest allocation rate at 853 MB/s, which still greatly \nexceeds the allocation rate of thefully scalablebenchmarks. The abnormally signi.cantcorrelationbetweenthe \nscal\u00adability and object allocation rateleadsto an assumption that the extensive object allocationis the \ncause of the rapid scal\u00adabilitydegradation ofpartially scalableprograms.Also note    1800 1800 Partially \nScalable 1600Programs 1600 1400 1400 1200 D-TLB Miss Rate % L1 Cache Miss Rate % L2 Cache Miss Rate %Memory \nBUS write bandwidth(MB) Object allocation rate(MB/s) 1200 1000 800 1000 800 600 400 Fully Scalable 600 \n200 Programs 0 400 200 0 (a) Memorybuswritebandwidth 0.4 0.35 0.3 0.25 0.2 0.15 Figure 5. Comparing object \nallocation rate for fully scal\u00ad able and partially scalable .Allocationrateismarked 0 if not garbage \ncollection is triggered, so it does not mean zero allocation. 0.1 0.05 that though allocation intensive, \nnone of the partially scal-0ableprogramssufferfromexcessivegarbagecollectiontime. Among all such benchmarks, \ncompiler.compiler spends up to7%CPUtimeingarbagecollection, while otherslessthan (b) L2CacheMiss Rate \n4%, so theGCis notresponsiblefortheperformancedegra\u00ad 30 dation. 25  4.3 FootprintofJavaApplications \n20 15 10 AsdepictedinFigure6(b),the signi.cantlyhigherL2Cache miss rate of the partially scalable applications \nsuggests that the applicationshave afairlylargefootprint. 5encouragesprogrammersto allocate new object \nondemand, 0and relegate reclamation of memoryspace ofdead objectsto thegarbage collector. The automatic \nmemorymanagementpolicyhas manyad\u00advantages such as ease of use andless memoryleak etc.How- Java as a language \nwith automatic memory management (c) L1D-CacheMiss Rate everwhenJavais usedindevelopingthe server-side \napplica\u00adtions, this memory managementpolicy sometimesbecomes 5 theobstacletotheperformance,especiallyonmulticoreplat\u00adforms. \n4 Since theGCis notinvokeduntil thefree memory space of the entireheap of memory regionis used up,the \nmemory footprintofJava applicationstends tobe verylarge,in spite 3 2 1 of a possibly small working set. \nLarge heap is used on modern platforms to reduce the frequency of GC, but the 0 largeheapsize makestheproblemofmemoryfootprinteven \nworse. Java memory footprint can be viewed as a .uctuating curve overthe wholeheap space.Assume aJava \napplication (d) D-TLBMiss Rate works with mostly short-living objects. It allocates objects fromthebeginningofitsheapmemory(orthe \neden spacein Figure 6. Measurement of architecture level factors that generational garbage collectors). \nIn mutator time, new ob\u00admayin.uence theperformance and scalability jects arelocatedconsequentlytowards \nthehigher end of the Address of allocated objects 4G 3G 2G 1G 40000 60000 80000 100000 120000 140000 \nObject allocated Figure7. Allocatedobjects footprintlogforSPECjbb2005 onIBMJ9VM with gencon GCpolicy \nmemory space.When the memoryheap runs out, agarbage collectionis triggered.As all objects are short-living, \nnearly the whole memoryheap canbe reclaimed.Then another cy\u00adcle of such memory behavior begins. As a \nresult, memory footprintof objects willbe repeatingin suchpattern. Figure 7 depicts the memory footprint \nof SPECjbb2005 on a J9VM with minor modi.cation to record the address of each allocated object.It canbe \nseen that memory address of objects scatters all overthe memoryheap, no matterhow small the actual working \nsetis.  4.4 WhyObjectAllocationLeadstoLargeAmount of MemoryBus WriteTraf.c The value of the allocated \nmemory size and the write traf.c onthememorybusaresocloseforevery partiallyscalable program.This strongcorrelationis \nnot accidental. SinceJava relies on theGarbageCollector to reclaim the memory,the objectswill notbefreed \nuntil aGC operation. Java has the semantics that every newly allocated object should be cleared with \nzeros . Combined with the rather large footprint depicted in Figure 7 it becomes the source of the large \namount of write traf.c on the memory bus, as explainedbelow 1 Allocated objects are located on different \nmemory ad\u00ad dresses, asdepictedinFigure7 2 Newlyallocated objects arepre-zeroedbyJVM, enforced byJavalanguage \nspeci.cation 3 Thepre-zeroingoperationhas two effects: 3.a Put the newlyallocated objectinto cache 3.b \nMakecache-linesholdingtheobject dirty 4 The size of L1 and L2 cache is rather small compared to the main \nmemory, while the footprint of Java programs are ratherlarge 5 When new memory blocks are brought into \nthe cache, some old cache-lines willbe evicted 6 Oneviction,allthe dirty cache-lineswillbewrittenback \nto memory 7 Sinceall allocated objectsare dirty incache(3.b),all allocated objects willlead to the memorybustraf.c \nwith the same size Forstatement6, dirty L1 cache-lineswillbeevictedtoL2 cacheforIntelClovertown, and \nwillbe evicted to L3 cache forAMDPhenom.WhiletheL2/L3cache-linesholdingthat contents are still dirty \n, and will be evicted and written to memory sooner orlater, so the net effectis the same. Also for statement \n6, no matter the L2/L3 cache policy is write-back or write-through, dirty cache-lines will be writtenbackto \nmemory anyway. The pre-zeroing is not the only source that makes al\u00adlocated cache-lines dirty .Themutator(application) \nmay also write to the objects. The time between when the ob\u00adject allocated and when it is written for \nthe .rst time is called Lag [30]. Even if the pre-zeroing operations are eliminated, according tothe \nresultsin[30], most of the al\u00adlocated objects are still writtenlaterby the mutator, making thecache-lines \ndirty soonerorlater. In orderto verifythe statementsabove,a micro-benchmark programnamed alloc mark iscomposed.Thealloc \nmark programisdesignedtobe as simple aspossible to eliminate all otherfactosin.uencingthe results.The \ncore source code islisted asinFigure8  Figure 8. Core source code for AllocMark micro\u00adbenchmarkprogram \nAs depictedinFigure8, theAllocMarkdoespure object allocation with no other operations, especially, the \nallocated objects are discarded directly without writing or modifying anything onthe objects.TheVectoroperationsinline1,4,5 \nare merely used to bypass the Escape Analysis [31] opti\u00admization built in modern JVMs. With such a simple \noper\u00adation of putting the allocated object into a Vector and re\u00admoving it out immediately, the Escape \nAnalysis module is skipped.All the objects are allocatedfrom theheapjustlike mostreal worldprograms.Figure9depictsthesize \nof allo\u00adcated objects per second and the memory bus write traf.c measuredbyhardware counters.When the \nobject sizeis not too small,thetwo value matchexactly.Thisprovesourstate\u00adments explaining why object \nallocationleadsto write traf.c on the memorybus.    5. EffectsofReducingObjectsAllocation Thecorrelationbetweenthescalabilitydegradation,the \nob\u00ad jectallocation rate and the memory write traf.cis analyzed intheoryand examinedwith carefullydesigned \nexperiments in previous sections. But we think the conclusion will be moresolidif we canactuallydecreasethe \nobjectallocation rate for some of these partially scalable programs to avoid hitting the allocation wall \nand see theimprovement on scal\u00ad ability. Two of the partially scalable programs, SPECjbb2005 and SPECjvm2008.derby \nare selected to be modi.ed using the object reuse technology. The most frequently allocated Size of each \nallocated object object are found by pro.ling the programs on source code Figure 9. Comparing thesizeof \nallocated objectspersec-level.Theprogramsaremodi.ed sothatobjectreusetech\u00adond withthememorybuswritetraf.cof \nthe alloc mark nology canbeapplied. micro-benchmarkprogramThe object allocation rate is decreased after \napplying the approach of object reuse for both the two programs. And our experiments demonstrates sigini.cant \ndecrease on memorywritetraf.c with muchimprovedscalabilityonboth of them. This result strongly supports \nour hypothesis on  the allocation wall issue.However the object reuse requires extensive pro.ling and \nanalysis work on the programs one byone.Findingthe mostheavilyallocatedobjectsandtrying to reuse such \nobjectisquitetime consuming,and may notbe a productive approach for application developers. And that \nFigure 10. Comparison of memory write traf.c, allocated is why we onlyimplemented objectreuseontwo of \nthesix partiallyscalableprograms. 5.1 TheObjectReuseTechnique Theobject reuseis akind of techniquethat \nmanagesmem\u00adory manually.The coreideaisto recycle used objects at user applicationlevel, andlater reuse \nthese objectsinstead of al\u00ad object sizeper second andhardwarelimit on memoryband\u00ad width at the speci.c \nread:write ratiofor all allocationinten\u00ad sivebenchmarksprograms. In order to further verify that this \ncorrelation between object allocation and memory bus write traf.c, Figure 10 compares the object allocation \nsize, the actual write traf.c bandwidth on the memory bus and the hardware limit of the memory writebandwidth \nat thegiven read:write ratio of the speci.c application.It revealsthat object allocation con\u00adtributes \nthe most signi.cant part of the write traf.c on the memory bus, and consumes the largest portion of memory \nwrite bandwidth. For all of the 6 benchmarks, object allo\u00adcation contributes atleast60% of memory write \ntraf.c, and forhalf of them nolessthan70%.It evenexceeds90%for derby. AsdepictedinFigure10,all the partially \nscalable pro\u00adgramshitthe allocationwall at somepoint,andthatisthe verypoint wherethey stop scaling.It \ncanbe claimedthatthe allocationwallisthe rootcauseforthe scalabilitydegrada\u00adtion of thepartially scalablebenchmarkprograms \nwhich are all allocationintensive. locating newones.Objectswill notbecomeunreachableon disposal,butwillbeput \nasideforfuture useinstead.Forfre\u00ad quentlyconstructedandshort-livingobjects,the overheadof objectconstructionandgarbagecollectionisgreatlyreduced \nby reusing theseobjects. But moreimportantly,thereuseof object constrains the memoryfootprint, and resultsin \nmuch betterL2 cache ef.ciency and muchless memory write op\u00aderations which solve the allocation wallproblemdirectly. \nObjectpoolingis a widelyusedtechnologytoimplement object reuse.Objects that are nolonger used areputinto \nan objectpool,instead ofleavingthemtothegarbage collector. And later if an object of the same class is \nrequired, one of the objectsin the objectpool willbe extracted and used,in\u00adstead of allocating a new \nobject.Objectpools are alwaysim\u00adplementedas astackFILOqueue,sothatrecentlydisposed objectsis reusedimmediatelyfornewallocationstoreduce \nthe memoryfootprint andincrease the cache ef.ciency.  5.2 Bene.tsofObjectReuse In orderto verifythe \neffectiveness of object reuse, we mod\u00adi.edSPECjbb2005andderby.The String objectsusedby transactionlog \nmoduleinSPECjbb2005 and the BigDeci\u00ad   160 180 200 220 240 260 280 300 1 2 3 4 5 6 7 8 Program execution \ntime(s) Number of cores 1800 9  Memory write bandwidth(MB/s) 1600 8 1400 1200 1000 800 600 400 200 Normalized \nthroughput 7 6 5 4 3 2 1 0 1 2 3 4 5 6 7 8 Number of cores mal objectsinSPECjvm2008.derby areidenti.ed \ntobe the mostintensivelyallocatedtypeofobjectsandtheyarereused byusing objectpool.We only modi.edthese \ntwoprograms, becauseit requirestime-consumingpro.ling work toiden\u00adtify the object types that are mostfrequently \nallocated. The object allocation rate is depicted in Figure 11. We use only4 threads and coresin this \nexperimentto reveal the actual allocation rate without the interference of allocation wall or cache contention.Allocation \nratesforthetwobench\u00admarksdecrease roughly45% and30% respectively. Memory write traf.c of SPECjbb2005 \nand derby with object reuseareshowninFigure13. SPECjbb2005 actual means the actual memory write traf.c \nof SPECjbb2005. SPECjbb2005 hardware limit means the up limit of the available memory bandwidth as the \nread:write ratio of SPECjbb2005.Andsamemeaning appliesto derby actual and derbyhardwarelimit . Since \nobject allocation in Java can bring heavy memory write traf.c, reductionin object allocation rate also \nreduces the write traf.c to main memory. Thus, object reuse has positiveimpacton avoidingthe allocation \nwallproblem. 0 1 2 3 4 5 6 7 8 Number of cores Figure14. PerformanceImprovementofSPECjbb2005and SPECjvm2008.derbyonIntelClovertownplatform(normal\u00adizedvalue). \nCompared with Figure 12, it is obvious that the actual memory traf.c of both the benchmark programs are \nmuch lower than the hardware limits of maximum memory write bandwidth, even though the maximum memory \nwriteband\u00adwidthdrops alittlebecause of theincrease of read:write ra\u00adtioduetothe modi.cation.Notethatderby \ns actual memory write traf.c is up to 2% higher than maximum at a point, which maybe causedbyinaccuracyofthe \nopro.letool.The hardwarelimitchangesslightlyfordifferentnumberofcores andapplicationthreads,becausethe \nread:write ratio changes for an application runningwithdifferent number of threads. Thehardwarelimitis \nmeasuredwithdifferent read:write ra\u00adtio correspondingto the application s read:write ratio. Withoutthelimitationof \nallocationwall,betterscalabil\u00aditycanbe achieved.The signi.cant scalabilityimprovement by applying object \nreuse techniqueisdepictedinFigure14. Itis rather obviousforderby, asit achieves nearlinear scal\u00adability \nwith object reuse. SPECjbb2005 is more cache sen\u00adsitive, soit achieves slightlylower scalability,but \nthe scala\u00adbilityis alsogreatlyimprovedcomparedto the originalpro\u00adgram withoutobject reuse.Withfewer cores \nspeedup of ob-5 jectreuseis roughly10%;withmore andmore coresenabled  it also increases steadily. With \n8 cores, the speedup climbs 4 to33%. The problem of allocation wall is identi.ed to be a lim\u00aditing factor \nfor allocation intensive Java applications on emerging multi-core platforms. Such problem will become \nNormalized throughput 3 2 1 more signi.cantinfuture, as cores andhardwarethreads are goingwith afasterpacethan \nmemorybandwidth.Of course, adding more memory channels is a straight forward solu\u00adtiontoimprove memorybandwidthanddiminishingproces\u00adsor/memory \ngap. But there is a physical limitation on the number of memory channels on a server mainboard by the \narea and wiredensity whichis notlikely tobeimprovedin thefuture.  6. Supplemental Experiments In this \nsection, several supplemental experiments are intro\u00adduced. Many important factors are evaluated including \ndif\u00adferenthardwareplatforms,differentheapsizes anddifferent GC policies etc. These experiments are vital \nto the correct\u00adness and completeness ofthispaper.Manyparameters ofthe experimentsinprevious sections \narebased onthe evaluation resultsinthis section, andmanyconclusionsinprevioussec\u00adtions are made with \nthe support on experimentshere. 6.1 Veri.cationonAMDPhenomPlatform AlthoughIntelCore2 architecturebasedprocessors(includ\u00ading \ntheCore seriesfordesktop and notebook as well asthe Xeon seriesforthe server,Clovertown,Hypertownetc) \nare dominatingthelargestpart of market, other architecture are also important to verify, especially the \nAMD K10 architec\u00adture (Phenom, Shanghai etc). The Intel s new Nehalem/i7 architecturehas many similarities \nwiththeAMDK10 archi\u00adtecture. For example, they both have on chip memory con\u00adtroller,NUMA architecture, \non chipL3 cache andindepen\u00ad 0 1234 Number of cores generationorthenurseryspaceshouldbeset ata compara\u00adblesizeoftheL2 \ncachesizeof theCPU. Blackburnet al[3] pointedoutthatthe nurserysize shouldbesetto around4MB toachievethebestperformanceonJikesRVM. \nButmostven\u00addors chose ratherlargeheapsize, accordingto submitted re\u00adsults on the SPEC web site, at least \n1.5GB for young and 2GBfortotalheap. In orderto .nd therightheap size andyounggeneration size, a series \nof experiments are performed on SunJVM and IBM J9VM with many GC policies. Our experiments shows that \nthe size of the old generation does not have signi.cantimpactonperformance,sincethe oldgeneration GCis \nnotusedfrequentlyfor most of thebenchmarks.Thus we onlylist the results of experiments on varyingtheyoung \ngenerationsize.The resultforthegenerationalGC onboth SunJVM andIBMJ9VM are similar, soFigure16 onlylists \nthe resultsfomSunJVM. 25 dent L2 cache per core. We think it is important to run the partiallyscalableprogramsontheAMDPhenomplatformto \nverify thatthepartial scalability is not causedby somebug ordesign .awof theClovertownplatform. AsdepictedinFigure15, \nalthoughthere are only4cores in the Phenom processor, the partial scalability of these programs are observable.Thedegradation \nof the scalability Normalized throughput 20 15 10 5 is not that obvious asinFigure4(b),because the memoryis \nrunning at200MHz(DDR2-800) onPhenom system while 0at166Mhz(DDR2-667) forClovertown, andPhenomhas only \n4 cores while Clovertown has 8 cores. The processor\u00ad memorygapis smallerforthePhenom system.  6.2 ChoosingtheBestHeapSize \nTheheapsizeis a veryimportantfactorin.uencingtheper\u00adformance of Java applications. Modern JVMs uses genera\u00adtional \nGC to handle objects that die young. However, tra\u00additionally there was widely accepted belief that the \nyoung Young Generation Size Figure 16. Performance with different young space sizes for partially scalable \nprogramsonIntelClovertownusing 8cores,8 software threads, andSunJVM TheperformancedatainFigure16 clearly \ncon.rmsthat the vendors are choosing the correctheap size.Such alarge younggenerationsizewilllead toa \ncomparablelargenurs-9 ery orso-called eden size(often1/2 to1/3 of theyoung  8 generation) for object \nallocation, which leads to the rather large allocation footprint depicted in Figure 7. Although 7 Normalized \nthroughput 6 5 4 3 2 1 much smalleryounggeneration size(comparable with the L2 cache size) will reduce \nthe footprint effectively, but the veryfrequentGC activities will consumetoo much resource that slows \ndown the over-all performance of Java applica\u00ad tions.  6.3 Veri.cationonDifferentJVMsandGarbage Collectors \nThe choice onJVM andgarbage collectionpolicies are very importfactorsin.uencingJava applicationsperformance.In \nthis section, many combination of JVM and GC are eval\u00aduated, including the SunJVM vs. IBM J9VM, generational \nGC vs. non-generationalGC, concurrentGC vs. sequential GC. The Intel Clovertown system is used in this \ngroup of experiments,AMDPhenomsystemhas similarresults.Stan\u00ad 0 12345678 Number of cores 9  dardheapsize,JVMparameters, \nandbenchmarkparameters 8 are used as theprevious experiments. Normalized throughput 7 6 5 4 3 2 9 8 \nNormalized throughput 7 6 1 5 0 4 12345678 Number of cores 3 2 able programs usingIBMJ9VM with mark-and-sweepGC \n(-Xgcpolicy:optthruput) 1 0Number of cores 6.4 Eliminating theIn.uenceofL2-CacheLayout on Figure 17. \nNormalized scalability of the partially scal\u00adable programs usingSunJVM with sequentialGC(without -XX:+UseParallelGC \n-XX:+UseParallelOldGC ) There are othertypes ofGCpoliciesinbothSunJVMand IBMJ9VM,forexamplethe optavgpause \nand metronome GC optimizedforpause timeinIBMJ9VM and the concur\u00adrent GC speci.ed by -XX:+UseConcMarkSweepGC \nand the incremental GC speci.ed by -XX:+UseTrainGC in SunJVM.Wedo notgivedetaileddatafor otherGCoptions, \nsince theyhave similar results. Althoughthere are some minordifferencesbetweenFig\u00adure 4(a), Figure 17, \nFigure 18 and Figure 19, the trends onscalability arealmostthesame and thedegradationwith morethan5 cores \nare equally obvious,provingthatthe scal\u00adabilityphenomenonidenti.edin thispaperis not speci.c to somedesign \norimplementationissue of aJVM orGC algo\u00adrithm.  Clovertown OnIntelClovertownprocessors, everytwo cores \nare sharing oneL2 cache.This cachelayout mayincurthe contentionon L2cache.In ordertoprovethatthe scalabilitydegradationof \nthepartiallyprogramsare notcausedbyL2cache contention or thrashing, an experiment is designed to eliminate \nthe in.uence of L2 cache layout on scalability by binding the software threadsto speci.c cores. The experimentisdesigned \nasfollows:Since each4MB L2 cache is shared by two cores on the same die on the Clovertown processor, \nL2 cache has two states: monopo\u00adlized and shared. Monopolized means each 4M L2 cache is only used by \none software thread, and shared means each 4M L2 cache is shared by multiple software threads. By using \nthe controlled software thread number and setting CPU af.nity, each L2 cache can be arranged to be shared \nbyonly2 software threads running on the2 cores on thedie and never monopolized. Of course, the number \nof enabled cores must be even. As each software thread has the same behavior, an approximatelyconstantL2 \ncache miss rate can bekept.The constantL2 cache miss rateis observedby the pro.lingdata(notlistedhere)fromperformance \ncounter on L2 MISS using opro.le[22].  Normalized throughput 8 7 6 5 4 3 2 1 0 2468 Number of cores \nFigure 20 shows the scalability of SPECjbb2005 with constant L2 cache miss rate by using thread af.nity. \nWith a constant L2 cache miss rate, the scalability still degrades signi.cantly with core numberlargerthan6.As \na result, we canclaimthatthescalabilitydegradationof these partially scalable benchmarksis not causedbyL2 \ncache contention of thrashing. Based on the above experiments, we eliminates most of the assumptions \nof scalability degradation of the partially scalableprograms,andleave only theintensive object allo\u00adcation \nas the explanation.  7. ConclusionandFutureWork In this paper, we focus on scalability of Java applications \non multi-coreplatforms.Two wellknownJavabenchmarks, SPECjbb2005 and SPECjvm2008, are studied in depth \non micro-architecture and source code level. In these bench\u00admarks, a group of programs demonstrating \ndegraded scal\u00adability withlarge number of cores areidenti.ed and named partially scalable .Thisgroup \nofprograms scaleperfectly with small numberof cores,butthe scalabilitydegradessub\u00adstantially and some \nof them even stop scaling when more cores and software threads are used. By pro.ling and ana\u00adlyzing thebehavior \nof thesebenchmarkprograms,the cor\u00adrelation between scalability degradation and object alloca\u00adtion rateis \nrevealed.Andbyfurthermeasuringthe consump\u00adtion ofmemorywritebandwidthofthese allocationintensive programsandthe \nmaximumavailable writebandwidthofthe memory sub-system,thekeyfactorthatin.uencesthe scala\u00adbility on this \nmulti-coreplatformisidenti.ed and named as allocation wall . By analyzing the memory usage behavior of \nJava pro\u00adgrams, webelievethatthe allocation wallis a commonissue forJava applicationsonthe emerging multi-coreplatforms. \nJava applicationsusuallyhavealargememoryfootprintscat\u00adtered overthe entireheap space.Thisis a serious \nside effect of the automatic memory management policy. CPU cache does not help much on reducing the memory \ntraf.c with such memoryaccesspattern.Object allocationis consuming large fraction of the memory bandwidth \nwhich is already a scarce resource.Webelieve that theJava spolicy ofleaving allgarbagecollection atGCtimeis \nnotef.cienton emerging mulit-coreplatformsdue to the allocation wallissue. Since theperformancegapbetweenprocessor \nand mem\u00adoryisgettinglargerandlarger,theallocationwall willbea much more severe problem in the future. \nReducing the ob\u00adjectallocation rateisthe only effective solutionthat wehave foundbynow.EscapeAnalysisis \nan another usefultechnol\u00adogy,butdue to thelimited ability ofJVM to analyze thebe\u00adhavior of user application, \nstate-of-the-art Escape Analysis algorithms are not enough to avoid the allocation-intensive applications \nfrom hitting the allocation wall. Even for the most aggressiveEscapeAnalysis under research[31]by the \nyear 2008, only 12% of the allocated object are eliminated on average. On thehand, although object reuseis \neffective on reduc\u00ading the object allocation, it is mostly done manually. Pro\u00ad.ling and modifying the \nuser code for object reuse is hard to perform for large and complex real world applications. Based onthis \nobservation,webelievethatthe allocationwall is still aquite challengingissueforJava. The future work \nwill be focused on the automatic solu\u00adtionsforthe allocationwallproblem.Approachesofdelay\u00adingthepre-zeroingwillbe \nexplored.Automatic object reuse or other JVM level optimization technologies will also be studied, so \nthat thefrequently allocated and short-living ob\u00adjects will be identi.ed byJava runtime and reclaimed \nsoon enoughbeforetheygetevictedfromtheCPU cacheandwrit\u00adtenbackto main memory.  Acknowledgments The authors \ndeepestgratitudegoes toMauricioJ. Serrano, Yan Li, YiXin Zhao, Tao Liu for their efforts in reviewing \nandimprovingthispaper.  References [1] RTSJ(RealTimeSpeci.cationforJava)MainPage. http://www.rtsj.org/. \n [2] AMD. Amd phenom x4 quad-core and amd phenom x3 triple-coreprocessors. http://www.amd.com/us-en/Processors/ProductInformation/ \n0,,30 118 15331 15%332,00.html. [3] BLACKBURN,S. M.,CHENG,P., AND MCKINLEY,K.S. Myths and realities: \nThe performance impact of garbage collection. In Proceedings of the ACM Conference on Measurement &#38; \nModeling Computer Systems (2004), ACM Press,pp.25 36.   [4] BLACKBURN, S. M., GARNER, R., HOFFMAN, \nC., KHAN, A. M., MCKINLEY,K. S., BENTZUR, R., DIWAN,A., FEINBERG, D., FRAMPTON, D., GUYER, S. Z., HIRZEL, \nM., HOSKING, A., JUMP, M., LEE, H., MOSS,J.E.B., PHANSALKAR, A., STEFANOVIC\u00b4,D., VANDRUNEN,T., VON DINCKLAGE, \nD., AND WIEDERMANN, B. The DaCapo benchmarks: Java benchmarking development and analysis. In OOPSLA 06: \nProceedings of the 21st annual ACMSIGPLAN conference onObject-OrientedPrograming, Systems,Languages, \nandApplications (NewYork,NY,USA, Oct.2006),ACMPress,pp.169 190. \u00a8 bandwidth limitations of future microprocessors. \nIn ISCA (1996). [5] BURGER, D., GOODMAN, J. R., AND KAGI, A. Memory [6] CHEREM,S., AND RUGINA,R. Uniquenessinferencefor \ncompile-time objectdeallocation. In ISMM (2007). [7] CORP.,I.Intel microarchitecture(nehalem). http://www.intel.com/technology/architecture-silicon/next\u00adgen/index.htm. \n[8] DILLIG, I., DILLIG, T., YAHAV, E., AND CHANDRA,S. Thecloser:Automating resourcemanagementinjava. \nIn ISMM (2008). [9] FENICHEL, R. R., AND YOCHELSON, J. C. A lisp garbage-collector for virtual memory \ncomputer systems. Communications of theACM (1969). [10] GANESH, B., JALEEL, A., WANG, D., AND JACOB,B. \nFully-buffered DIMM memory architectures: Understanding mechanisms, overheads and scaling. In HPCA (2007). \n[11] GEORGES, A., BUYTAERT, D., AND EECKHOUT, L. Statisticallyrigorousjavaperformance evaluation. SIGPLAN \nNot.42,10(2007),57 76. [12] HAMMOND,L.,NAYFEH,B.A., AND OLUKOTUN,K. A single-chip multiprocessor. Computer30,9(1997),79 \n85. [13] HOFSTEE,H. Poweref.cientprocessorarchitectureand the cell processor. High-Performance Computer \nArchitecture, 2005. HPCA-11. 11th International Symposium on (12-16 Feb.2005),258 262. [14] HUANG, W., \nQIAN, Y., SRISA-AN, W., AND CHANG, J. Object allocation and memory contention study of java multithreaded \napplications. Performance, Computing, and Communications, 2004 IEEE International Conference on (2004),375 \n382. [15] IBM CORP. http://www.ibm.com/systems/bladecenter/ hardware/servers/hs21/index.html. [16] INTEL \nCORP. http://processor.nder.intel.com/details.aspx?sspec=slac5. [17] INTEL CORP. http://www.intel.com/Products/Server/Chipsets/5000P/5000P\u00adoverview.htm. \n[18] IYER,R.,BHAT,M.,ZHAO,L.,ILLIKKAL,R.,MAKINENI, S., JONES, M., SHIV, K., AND NEWELL, D. Exploring \nsmall-scale andlarge-scale cmp architecturesfor commercial java servers. IEEEWorkload Characterization \nSymposium 0 (2006),191 200. [19] JOISHA, P. G. A principled approach to nondeferred reference-countinggarbage \ncollection. In VEE (2008). [20] KONGETIRA, P., AINGARAN, K., AND OLUKOTUN.,K. Niagara: A 32-way multithreaded \nsparc processor. In IEEE Micro (2005). [21] LARRY,M., AND CARL,S. lmbench:Portabletoolsfor performance \nanalysis. Proceedings of the USENIX 1996 Annual TechnicalConference (1996). [22] LEVON,J., AND ELIE.,P. \nOpro.le:A systempro.lerfor linux. [23] LIEBERMAN, H., AND HEWITT, C. A realtime garbage collectorbased \non thelifetimes of objects. Communications of theACM (1983). [24] LUO,Y., AND JOHN,L.K. Simulatingjavacommercial \nthroughput workload:A case study. In ICCD (2005). [25] MARDEN, M., LIEN LU, S., LAI, K., AND LIPASTI, \nM. Comparison of memory systembehaviorinjava and non\u00adjava commercialworkloads. InProceedings of theWorkshop \non Computer Architecture Evaluation using Commercial Workloads (2002). [26] MCCALPIN,J.D. Memorybandwidth \nand machinebalance in current high performance computers. IEEE Computer Society Technical Committee on \nComputer Architecture (TCCA)Newsletter(Dec.1995),19 25. [27] MCCARTHY,J.Recursivefunctionsof symbolicexpressions \nand their computation by machine. Communications of the ACM (1960). [28] PERSSON, M. Java technology, \nIBM style: Garbage collectionpolicies. IBMdeveloperWorks (2006). [29] SESHADRI,P., AND JOHN,L.K.Workload \ncharacterization ofjavaserverapplicationsontwopowerpcprocessors. In In Proceedings of theThirdAnnual \nAustinCenter forAdvanced Studies Conference (2002),pp.328 333. [30] SHAHAM, R., KOLODNER, E. K., AND \nSAGIV, M. Heap pro.ling forspace-ef.cientjava. In PLDI 01: Proceedings of the ACM SIGPLAN 2001 conference \non Programming language design and implementation (New York, NY,USA, 2001),ACM,pp.104 113. [31] SHANKAR, \nA., ARNOLD, M., AND BODIK, R. Jolt: lightweight dynamic analysis and removal of object churn. In OOPSLA \n08: Proceedings of the 23rd ACM SIGPLAN conference on Object-oriented programming systems lan\u00adguages \nand applications (NewYork,NY,USA,2008),ACM, pp.127 142. [32] SHIV, K., CHOW, K., WANG, Y., AND PETROCHENKO, \nD. Specjvm2008 performance characterization. In SPEC Benchmark Workshop (2009),pp.17 35. [33] SHIV, K., \nIYER, R., BHAT, M., ILLIKKAL, R., JONES, M., MAKINENI,S.,DOMER,J., AND NEWELL,D. Addressing cache/memory \noverheadsin enterprisejava cmp servers. In ISSWC (2007).  [34] SPEC. SPECjbb2005(JavaServerBenchmark). \nhttp://www.spec.org/jbb2005/. [35] SPEC. SPECjvm2008 Benchmarks. http://www.spec.org/jvm2008/docs/benchmarks/index.html. \n[36] SPEC.SPECjvm2008(JavaVirtual MachineBenchmark). http://www.spec.org/jvm2008/. [37] SPRACKLEN,L., \nAND ABRAHAM,S.G. Chip multithread\u00ading:Opportunities and challenges. In HPCA (2005). [38] SUN MICROSYSTEMS. \nTuning Garbage Collection with the 5.0Java VirtualMachine. [39] TIKIR,M.M., AND HOLLINGSWORTH,J.K. Numa-aware \njavaheapsforserverapplications.InIPDPS 05:Proceedings of the 19th IEEE International Parallel and Distributed \nProcessingSymposium(IPDPS 05) -Papers (Washington, DC,USA,2005),IEEEComputerSociety,p.108.2. [40] TSENG, \nJ. H., YU, H., NAGAR, S., DUBEY, N., FRANKE, H., PATTNAIK, P., INOUE, H., AND NAKATANI, T. Per\u00adformance \nstudies of commercial workloads on a multi-core system. In IISWC (2007). [41] TUCK,N., AND TULLSEN,D.M.Initial \nobservationsof the simultaneous multithreading pentium 4 processor. Parallel Architectures and Compilation \nTechniques, International Conference on0 (2003), 26. [42] UNGAR, D. Generation scavenging: a non-disruptive \nhigh performance storage reclamation algorithm. In ACM SIGSOFT SoftwareEngineering Notes (1984). [43] \nVOGT, P. D. Fully buffered DIMM (FB-DIMM) server memory architecture:Capacity,performance, reliability,and \nlongevity. IntelDeveloper Forum (2004). [44] WULF,W.A., AND MCKEE,S.A. Hitting thememory wall:implications \nof the obvious. ACMSIGARCHComputer Architecture News (1995). [45] XIAN,F.,SRISA-AN,W., AND JIANG,H. Microphase: \nAn approach to proactively invoking garbage collection for improvedperformance. In OOPSLA (2007).  \n\t\t\t", "proc_id": "1640089", "abstract": "<p>Multi-core processors are widely used in computer systems. As the performance of microprocessors greatly exceeds that of memory, the memory wall becomes a limiting factor. It is important to understand how the large disparity of speed between processor and memory influences the performance and scalability of Java applications on emerging multi-core platforms.</p> <p>In this paper, we studied two popular Java benchmarks, SPECjbb2005 and SPECjvm2008, on multi-core platforms including Intel Clovertown and AMD Phenom. We focus on the \"partially scalable\" benchmark programs. With smaller number of CPU cores these programs scale perfectly, but when more cores and software threads are used, the slope of the scalability curve degrades dramatically.</p> <p>We identified a strong correlation between scalability, object allocation rate and memory bus write traffic in our experiments with our partially scalable programs. We find that these applications allocate large amounts of memory and consume almost all the memory write bandwidth in our hardware platforms. Because the write bandwidth is so limited, we propose the following hypothesis: the scalability and performance is limited by the object allocation on emerging multi-core platforms for those objects-allocation intensive Java applications, as if these applications are running into an \"allocation wall\".</p> <p>In order to verify this hypothesis, several experiments are performed, including measuring key architecture level metrics, composing a micro-benchmark program, and studying the effect of modifying some of the \"partially scalable\" programs. All the experiments strongly suggest the existence of the allocation wall.</p>", "authors": [{"name": "Yi Zhao", "author_profile_id": "81444605021", "affiliation": "IBM, Beijing, China", "person_id": "P1728786", "email_address": "", "orcid_id": ""}, {"name": "Jin Shi", "author_profile_id": "81310482236", "affiliation": "Tsinghua University, Beijing, China", "person_id": "P1728787", "email_address": "", "orcid_id": ""}, {"name": "Kai Zheng", "author_profile_id": "81100580587", "affiliation": "IBM, Beijing, China", "person_id": "P1728788", "email_address": "", "orcid_id": ""}, {"name": "Haichuan Wang", "author_profile_id": "81444609043", "affiliation": "IBM, Beijing, China", "person_id": "P1728789", "email_address": "", "orcid_id": ""}, {"name": "Haibo Lin", "author_profile_id": "81539261356", "affiliation": "IBM, Beijing, China", "person_id": "P1728790", "email_address": "", "orcid_id": ""}, {"name": "Ling Shao", "author_profile_id": "81540948556", "affiliation": "IBM, Beijing, China", "person_id": "P1728791", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1640089.1640116", "year": "2009", "article_id": "1640116", "conference": "OOPSLA", "title": "Allocation wall: a limiting factor of Java applications on emerging multi-core platforms", "url": "http://dl.acm.org/citation.cfm?id=1640116"}