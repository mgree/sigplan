{"article_publication_date": "10-25-2009", "fulltext": "\n Coherent Reaction Jonathan Edwards MIT Computer Science and Arti.cial Intelligence Lab edwards@csail.mit.edu \nAbstract 1. Introduction Side effects are both the essence and bane of imperative pro\u00adgramming. The \nprogrammer must carefully coordinate ac\u00adtions to manage their side effects upon each other. Such co\u00adordination \nis complex, error-prone, and fragile. Coherent re\u00adaction is a new model of change-driven computation \nthat co\u00adordinates effects automatically. State changes trigger events called reactions that in turn change \nother states. A coherent execution order is one in which each reaction executes be\u00adfore any others that \nare affected by its changes. A coherent order is discovered iteratively by detecting incoherencies as \nthey occur and backtracking their effects. Unlike alternative solutions, much of the power of imperative \nprogramming is retained, as is the common sense notion of mutable state. Automatically coordinating actions \nlets the programmer ex\u00adpress what to do, not when to do it. Coherent reactions are embodied in the Coherence \nlan\u00adguage, which is specialized for interactive applications like those common on the desktop and web. \nThe fundamental building block of Coherence is the dynamically typed muta\u00adble tree. The fundamental abstraction \nmechanism is the vir\u00adtual tree, whose value is lazily computed, and whose behav\u00adior is generated by coherent \nreactions. Categories and Subject Descriptors D.1.1 [Programming Techniques]: Applicative (Functional) \nProgramming; D.1.3 [Programming Techniques]: Concurrent Programming; F.1.2 [Computation by Abstract Devices]: \nModes of Computation Interactive and reactive computation General Terms Languages Keywords interactive \nsystems, reactive systems, synchronous reactive programming, functional reactive programming, bidirectional \nfunctions, trees Permission to make digital or hard copies of all or part of this work for personal or \nclassroom use is granted without fee provided that copies are not made or distributed for pro.t or commercial \nadvantage and that copies bear this notice and the full citation on the .rst page. To copy otherwise, \nto republish, to post on servers or to redistribute to lists, requires prior speci.c permission and/or \na fee. OOPSLA 2009, October 25 29, 2009, Orlando, Florida, USA. Copyright c . 2009 ACM 978-1-60558-768-4/09/10. \n. . $10.00 I see no reasonable solution that would allow a paper presenting a radically new way of working \nto be ac\u00adcepted, unless that way of working were proven better, at least in a small domain. Mark Wegman, \nWhat it s like to be a POPL referee [28] This paper presents a new kind of programming language and illustrates \nits bene.ts in the domain of interactive ap\u00adplications (such as word processors or commercial web sites). \nThe fundamental problem being addressed is that of side effects, speci.cally the dif.culties of coordinating \nside effects. Coordinating side effects is the crux of imperative pro\u00adgramming, the style of all mainstream \nlanguages. Imperative programming gives the power to change anything anytime, but also imposes the responsibility \nto deal with the conse\u00adquences. It is the programmer s responsibility to order all actions so that their \nside effects upon each other are correct. Yet it is not always clear exactly how actions affect each \nother, nor how those interdependencies might change in the future. Coordinating side effects is a major \nproblem for inter\u00adactive applications, for two reasons. Firstly, interaction is a side effect. The whole \npurpose of user input is to change the persistent state of the application. The issue can not be side-stepped. \nSecondly, the size and complexity of modern applications demands a modular architecture, such as Model \nView Controller (MVC) [26] and its descendants. But coor\u00addination of side effects inherently cross-cuts \nmodules, lead\u00ading to much complexity and fragility. A common example is that of a model constraint that \nensures multiple .elds have compatible values. If a view event, say submitting a form, changes two of \nthose .elds, it is necessary that they both change before the constraint is checked. Otherwise the constraint \nmight falsely report an error. There are many common workarounds for this problem, none of them entirely \nsatisfactory. In the MVC architecture it falls to the controller to coor\u00addinate change. One approach \nto the example problem is to defer checking the constraint until after all relevant changes have been \nmade. This erodes modularity, for now the model must publish all its constraints and specify what .elds \nthey depend upon, limiting the freedom to changes such internals. Even still it may not be obvious to \nthe Controller what im\u00adplicitly called methods may changes those .elds, so it can not be sure when to \ncall the check. It could defer all checks till the very end. But that presumes that the code is itself \nnever called by other code, again eroding modularity. The dif.culty of modularizing MVC controllers has \nbeen dis\u00adcussed by others. [3, 17, 24]  Another approach, no more satisfactory, is to have the model \npublish special methods that bundle the changes to all constraint-related .elds into a single call. Once \nagain this defeats modularity, for the model is exposing its internal semantics, limiting the freedom \nto change them. Worse, the controller is given the impossible task of accumulating all changes to the \nrelevant .elds, made anywhere in the code it calls, so that it can change them atomically. Modern application \nframeworks employ an event-driven publish/subscribe model to respond to input more modlarly. Event handlers \ncan subscribe to be called back whenever an event is published. The subscribers and publishers need not \nknow of each other s existence. This approach eliminates many hard-wired interdependencies that obstruct \nmodular\u00adity, but does not solve the example problem. The constraint can not subscribe to changes on the \ninvolved .elds, for it will be triggered as soon as the .rst one changes. One re\u00adsponse is to queue up \nthe constraint checks to be executed in a separate phase following all the model change events. The popular \nweb framework JavaServer Faces [4] de.nes ten dif\u00adferent phases. Phasing is an ad hoc solution that works \nonly for pre\u00adconceived classes of coordination problems. Unfortunately event-driven programming can create \nmore coordination problems than it solves. The precise order of interrelated event .rings is often undocumented, \nand so context-dependent that it can defy documentation.1 You don t know when you will be called back \nby your subscriptions, what callbacks have already been called, what callbacks will be subse\u00adquently \ncalled, and what callbacks will be triggered implic\u00aditly within your callback. Coordinating changes to \ncommu\u00adnal state amidst this chaos can be baf.ing, and is far from modular. The colloquial description \nis Callback Hell. An analysis [21] of Adobe s desktop applications indi\u00adcated that event handling logic \ncomprised a third of the code and contained half of the reported bugs. The dif.culties of event coordination \nare just one of the more painful symptoms of the disease of unconstrained global side effects. It has \nlong been observed that global side effects destroy both referential transparency [19] and behavioral \ncomposition [16]. Unfortunately, attempts to ban\u00adish side effects from programming languages have required \n1 For example, when the mouse moves from one control to another, does the mouseLeave event .re on the \n.rst before the mouseEnter event .res on the second? Does your GUI framework document that this order \nis guaranteed? The order is seemingly random in one popular framework. signi.cant compromises, as discussed \nin the Related Work section. The primary contribution of this paper is coherent re\u00adaction, a new model \nof change-driven computation that con\u00adstrains and coordinates side effects automatically. The key idea \nis to .nd an ordering of all events (called reactions) that is coherent, meaning that each reaction is \nexecuted be\u00adfore all others that it has any side effects upon. Coherent ordering is undecidable in general. \nIt can be found with a dynamic search that detects incoherencies (side effects on previously executed \nreactions) as they occur. All the effects of a prematurely executed reaction are rolled back, as in a \ndatabase transaction, and it is reexecuted later. From the pro\u00adgrammer s point of view, coordination \nbecomes automatic. The programmer can concentrate on saying what to do, not when to do it. Coherent reaction \nis discussed in more detail in the next section. The Coherence programming language uses coherent re\u00adactions \nto build interactive applications. The fundamental building block of the language is the dynamically \ntyped mu\u00adtable tree. The key idea is that abstraction is provided by virtual trees, whose values are \nlazily computed, and whose behaviors are generated by coherent reactions. Further de\u00adtails can be found \nin the full version [11] of this paper. 2. Coherent Reaction This section explains coherent reaction \nin the simple setting of a Read Eval Print Loop (REPL). Programmer input is pre.xed with a >, the printed \nvalue of inputs with a =, and program output with a <. Printed values will be omitted when they do not \nfurther the discussion. 1 > task1: { 2 name: task1 , 3 start: 1, 4 length: 2, 5 end = Sum(start, length)} \n6 = {name: task1 , start: 1, length: 2, end: 3} 7 > task1.start := 2 8 > task1 9 = {name: task1 , start: \n2, length: 2, end: 4} Lines 1 5 de.ne the variable task1 to be a structure contain\u00ading the .elds within \nthe curly braces. This structure is meant to represent a task in some planning application, and has a \nstarting time and length de.ned in the .elds start and length. For simplicity these .elds are given plain \nnumeric values rather a special time datatype. Variables and .elds are dy\u00adnamically typed. The .eld end \nis de.ned on line 5 as the total of the start and length .elds using the Sum function. (Functions are \ncapital\u00adized by convention. Traditional in.x mathematical notation can be supported, but will not be \nused in this paper.) The end .eld is said to be derived, indicated by de.ning it with an equals sign \ninstead of a colon, followed by an expression to calculate the value. The value of task1 is printed on \n6, with end correctly computed.  Figure 1. Arrows denote derivation, faded elements are in\u00adherited. \nThe derivation expression of end is recalculated every time the .eld is accessed (although the implementation \nmay cache the last calculated value and reuse it if still valid). The persistence of derivation is demonstrated \non line 7, where an assignment statement changes the value of the start .eld. (Assignment statements \nuse the := symbol instead of a colon or equals sign.) The effect of the assignment is shown by referencing \ntask1 on line 8, whose value is output on line 9, where the derived value of end has been recomputed. \nDerivation is a fundamental concept in Coherence. A derivation is computed lazily upon need, and as will \nbe seen is guaranteed to have no side-effects, so it is like a well-behaved getter method in OO languages. \nA derivation expression is also like a formula in a spreadsheet cell: it is attached to the .eld and \ncontinually links the .eld s value to that of other .elds. The following example shows more ways that \nderivation is used. 10 > task2: task1(name: task2 , start = task1.end) 11 = {name: task2 , start: 4, \nlength: 2, end: 6} Line 10 derives the variable task2 as an instance of task1, meaning that it is a copy \nwith some differences. The dif\u00adferences are speci.ed inside the parentheses: a new name is assigned, \nand the start .eld is derived from the end .eld of task1. Figure 1 diagrams this example. The length \n.eld was not overridden, and is inherited from the prototype, as shown by its value output on line 11. \nAny subsequent changes to task1.length will be re.ected in task2.length. However since task2.name has \nbeen overridden, changes to task1.name will not affect it. Derivation functions are inherited and overrid\u00adden \nin the same manner. Instantiation behaves as in proto\u00adtypical languages [25]. Functions are also treated \nas proto\u00adtypes and their calls as instances (justifying the use of the same syntax for calling and instantiation). \nMaterializing ex\u00adecution in this way has large rami.cations on the design of the language, including \nthe interpretation of names and the status of source text [9, 10], but those issues are beyond the scope \nof this paper. Figure 2. Reaction .ow. Bold arrows show the .ow. Values are post-states. 2.1 Reaction \nDerivation is bidirectional: changes to derived variables can propagate back into changes to the variables \nthey were de\u00adrived from. This process is called reaction, and is used to handle external input. A Coherence \nsystem makes certain structures visible to certain external interfaces (the program\u00admer s REPL can see \neverything). All input takes the form of changes to such visible .elds, which react by changing internal \n.elds, which in turn can react and so on. Multiple input changes can be submitted in a batch, and the \nentire cascade of reactions is processed in a transaction that com\u00admits them atomically or not at all. \nOutput consists of reading visible .elds, which are rederived if necessary from the lat\u00adest changed state. \nThe following example illustrates. 12 > task2.end := 5 13 > task2 14 = {name: task2 , start: 3, length: \n2, end: 5}15 > task1 16 = {name: task1 , start: 1, length: 2, end: 3} On line 12 the task2.end .eld is \nassigned the value 5, and the results are shown on the following four lines and diagrammed in Figure \n2. Because task2.end is a derived .eld its derivation function reacts to the change. Every function reacts \nin some way, if only to declare an error. The Sum function s reaction is to adjust its .rst argument \nby the same amount as the result, so that the result is still the sum of the arguments.2 Thus a change \nto the end of a task will adjust its start to maintain the same length: task2.start is changed to 3. \nSince task2.start is derived from task1.end, the reaction propagates to the latter .eld, and in turn \ncauses task1.start to be set to 1. The task1.start .eld is not derived, so the chain reaction grounds \nout at that point, leaving the .eld changed. If you don t like the built-in reaction of a function you \noverride it with your own custom reaction, as follows. 2 The second argument could be adjusted instead. \nA function is expected to document such choices.  17 > task1: { 18 name: task1 , 19 start: 1, 20 length: \n2, 21 end = Sum(start, length) 22 => {start := Di.erence(end , length )}} 23 = {name: start1 , start: \n1, length: 2, end: 3} Here task1 has been rede.ned to include a custom reaction for the end .eld on \nline 22. The symbol => speci.es the reaction for a .eld, in counterpoint to the use of = for the derivation, \nas reaction is opposite to derivation. The reaction is speci.ed as a set of statements that execute when \nthe .eld is changed. Note that while these statements may look like those in a conventional imperative \nlanguage, they behave quite differently. For one thing, they execute in parallel. Section 2.3 will explain \nfurther. The reaction speci.ed above duplicates the built-in reac\u00adtion of the Sum function. The changed \nvalue of the end .eld is referenced as end , adopting the convention of speci.cation languages that primed \nvariables refer to the post-state. Non\u00adprimed references in reactions always refer to the pre-state prior \nto all changes in the input transaction. The post-state of end has the post-state of length subtracted \nfrom it to com\u00adpute the post-state of start. The post-state of length is used rather than its pre-state \nbecause it could have changed too, discussed further below.  2.2 Actions Reactions can make arbitrary \nchanges that need not be the inverse of the derivation they are paired with. An example of this is numeric \nformating, which leniently accepts strings it doesn t produce, effectively normalizing them. An extreme \ncase of asymmetry is an action, which is a derivation that does nothing at all and is used only for the \neffects of its reaction. Here is a Hello world action. 24 > Hello: Action{do=>{ 25 consoleQ << Hello \nworld }} 26 > Hello() 27 < Hello world The Hello action is de.ned on line 24 with the syntax Action{do=>...}. \nThis indicates that a variant of the prototype Action is derived, incorporating a reaction for the do \n.eld. A variant is like an instance, except that it is allowed to make arbitrary internal changes, whereas \ninstances are limited to changing only certain public aspects like the input arguments of a function. \nCurly braces without a leading prototype, like those used to create the task1 structure in line 17, are \nactually creating a variant of null, an empty structure. Actions are triggered by making an arbitrary \nchange to their do .eld (conventionally assigning it null), which has the sole effect of triggering the \nreaction de.ned on it. A statement consisting of only a function call will trigger its action by changing \nits do .eld. The Hello action is triggered in this way on line 26. By encoding actions as the reactions \nof do .elds we establish the principle that all behavior is in reaction to a change of state, which is \nessential to the semantics described below. The body of the action on line 25 outputs to the console \nwith the syntax consoleQ<< Hello world . The << symbol de\u00adnotes an insertion statement. It creates a \nnew element within consoleQ and assigns its value to be the string Hello world . If the input transaction \ncommits, any elements inserted into the consoleQ will be printed and then removed from the queue. Driving \nconsole output from a queue preserves the principle that all behavior is in reaction to a change of state. \n 2.3 Coherent execution Enough preliminaries are now in place to explain the seman\u00adtics of coherent \nreactions. Say that inside some action we need to change a task s end and length, as in the following \ncode snippet. 28 TaskAction: Action{task, do=>{ 29 ... 30 task.end := e, 31 task.length := d}} The question \nis, what is the value of the task s start .eld afterwards? One might expect it to be e - d. That would \nbe wrong if this code were executed in an OO language, where the reaction of end would be encoded into \nits set method. The set method would use the value of length at the time it was called to calculate start. \nBut length is set after the call, so the value of start will actually be e-oldLength and the value of \nend recalculated by its get method will not be e as expected but e - oldLength + d. Obviously it is necessary \nto set length before end to get the correct result. But in practice such issues are often far from obvious. \nThe side-effects of methods (especially those caused by deeply nested method calls) are often undocu\u00admented \nand subject to change. For example if task were refactored so that length was instead derived from the \ndiffer\u00adence of start and end, then any code like ours depending on the ordering of the side-effects would \nbreak. This example is indicative of the fundamental quandary of imperative pro\u00adgramming: it is up to \nthe programmer to orchestrate the exact order in which all events takes place, yet the programmer of\u00adten \nlacks the omniscience and clairvoyance required to do so perfectly. The result is much complexity and \nfragility. Coherence avoids these problems by automatically deter\u00admining the correct execution order \nof all events. In the above example, the reaction on end will be automatically executed after the assignments \nto end and length. A correct execution order is called coherent, de.ned as an order in which every reaction \nexecutes before any others that it affects. A reaction affects another in only one way: if it writes \n(assigns to) a location whose post-state is read by the other. Finding a coherent order may seem at .rst \nto be a straight\u00adforward problem of constraint satisfaction. We form a graph of reactions whose edges \nare such effects. A coherent order is a topological sort of this graph. The problem is that form\u00ading \nthis graph is undecidable. Reactions can use pointers: they are free to do arbitrary computations to \ncompute the lo\u00adcations which they read and write. For example, TaskAction might take some user input \nas a key with which to search all tasks with a spelling-similarity algorithm, and then modify the found \ntask. Allowing arbitrary computation of locations makes the effect graph undecidable in general. Coherence \nis not a problem of constraint satisfaction it is a prob\u00adlem of constraint discovery. Previously there \nhave been two alternative solutions: reduce the expressive power of the lan\u00adguage so that constraint \ndiscovery becomes decidable (as in state machines and data.ow languages), or leave it to the programmer \nto deal with.  This paper introduces a new technique that dynamically discovers effects between reactions \nand .nds a coherent ex\u00adecution order. Every reaction is run in a micro-transaction that tracks both its \nwrites and post-state reads. Reactions are initially executed in an arbitrary order. Incoherencies are \nde\u00adtected as they occur: whenever a reaction writes a location whose post-state was read by a previously \nexecuted reac\u00adtion. In that case the previous reaction s micro-transaction is aborted and it is run again \nlater. The abort cascades through all other reactions that were transitively affected. This al\u00adgorithm \nis essentially an iterative search with backtracking, using micro-aborts to do the backtracking. If there \nare no errors a coherent execution order will found and the whole input transaction is committed. Cyclic \neffects are an error: a reaction can not transi\u00adtively affect itself. Errors are handled tentatively \nbecause they might be later rolled back errors that remain at the end cause an abort of the whole input \ntransaction. The search for a coherent ordering converges because reactions are de\u00adterministic (randomness \nis simulated as a .xed input). It will terminate so long as the reactions themselves terminate, as only \na .nite number of reactions can be triggered.  2.4 The price of coherence Clearly a naive implementation \nof coherence will be slower than hand-written coordination logic in an imperative lan\u00adguage. But at this \npoint worrying about performance op\u00adtimization would be both premature and misguided. The history of \nVM s shows that clever implementation tech\u00adniques can yield large speedups. There is a large body of \nprior research that could be exploited, from static analysis to feedback-directed optimization. Coherent \ncode reveals in\u00adherent parallelism that might be exploited by multicore pro\u00adcessors. Annotations could \npartially instruct how to order reactions (but still be checked for coherence, which is easier than solving \nfor it). In any case the major problem of inter\u00adactive applications is not CPU performance but programmer \nperformance the dif.culty of designing, building, and maintaining them. Coherence imposes certain constraints \non reactions: 1. A .eld can change at most once per input transaction. Multiple reactions can change \nthe same .eld, but only to the same value. This situation might occur in the above example if the code \nsnippet also assigned the start .eld. That would be OK so long as the value agreed with what the reaction \ncomputed it should be, which would effectively become an assertion: if the values disagreed an error \nwould abort the input transaction. 2. All reactions can see the entire global pre-state. Each can see \nthe pending post-state of the .eld it is attached to, and decides how to propagate those changes to other \n.elds. Each can also see the pending post-state of other .elds. But in no case can a reaction see the \nconsequences of any changes it makes, because that would create a causal loop whereby it depends upon \nitself. Causality violation is punished by aborting the transaction. 3. A consequence of the above property \nis that all of the assignment statements inside a reaction execute as if in parallel. Causal ordering \nonly occurs between different reactions.  This paper suggests that much of the manual sequencing of \nactions that is the crux of imperative programming is an accidental complexity [2], and that coherent \nexecution can handle it automatically, at least in the domain of interactive applications. But there \nare cases when sequential execution is truly essential. For such cases, Coherence offers an encap\u00adsulated \nform of imperative programming called progression.  2.5 Progression Say that we want to execute the \nprevious TaskAction on a task, but also want to ensure that whatever it does, the task s length ends \nup no more than 10. We could do that by creating an alternate version of TaskAction that maximized the \nlength before assigning it. But it is simpler to just execute TaskAction and then cap the length if it \nis too large. However reactions only get a single shot to change each .eld, and can not see the consequences \nof their own actions. Instead we can use a progression: 32 33 BoundedAction: Action{task, do=>{prog (task) \n[ 34 TaskAction(task); 35 if (Gt(task.length, 10)) then 36 {task.length := 10}]}} The prog statement \non line 33 takes a parenthesized list of one or more versioned variables, which here is just task. That \nis followed by square brackets containing a sequence of statements separated by semicolons. The statements \ncan be read somewhat imperatively: the statement on line 34 executes TaskAction on task, and then the \nif statement on the following line checks the resulting length value and sets it to 10 if it is greater. \nWhat actually happens is that a separate version of task is made for each statement. Each statement changes \nits version, which then becomes the pre-state of the next version. An example reaction .ow for BoundedAction(task2) \nis dia\u00adgrammed in Figure 3. The .rst version of task2 is modi.ed by  Figure 3. Progression reaction \n.ow. All values are post-states. TaskAction, creating the second version which is modi.ed by the if statement. \nThe changes made in the .rst version are en\u00adcapsulated. The change to length gets overridden in the sec\u00adond \nversion, and the change to end is discarded because it is consumed by the Sum reaction. The Sum reaction \ns change to start gets inherited into the second version. The accumulated changes to start and length \nin the second version are exported out of BoundedAction. The exported change to task2.start then propagates \ninto task1.end. Note that while internal reactions like Sum execute in each version, any external reactions \nlike the link to task1 only execute at the very end. Progressions are encapsulated: the internal unfolding \nof events is isolated from the outside. External changes are visible only at the beginning, and internal \nchanges become visible externally only by persisting till the end. Progression depends on the fact that \nCoherence can make incremental versions of entire structures like a task. As dis\u00adcussed in the full version \n[11] of this paper, the state of a Co\u00adherence program is a tree. Progressions can version any sub\u00adtree, \nsuch as collections of structures, or even the entire state of the system. In the latter case, progression \nbecomes a sim\u00adulation of imperative programming, capable of making arbi\u00adtrary global changes in each \nversion, and constrained only from doing external I/O. This simulation also reproduces the usual pitfalls \nof imperative programming. Progression is an improvement over imperative programming only to the ex\u00adtent \nthat it is quarantined within localized regions of state, and used as a special case within a larger \ncoherent realm. Progressions also support looping with a for statement. The whatif statement is a hypothetical \nprogression with no ef\u00adfect, executed only to extract values produced along the way. Hypotheticals function \nlike normal progressions, except that all emitted changes are silently discarded. Values produced within \na hypothetical can be accessed from its calling con\u00adtext. Hypotheticals turn imperative code into pure \nfunctions, and can thus be used inside derivations. Hypothetical pro\u00adgressions on the global state can \nbe used for scripting behav\u00adioral tests, running the entire system in an alternate timeline.  2.6 Coherence \nas a model of computation Derivation and reaction are quite different, yet work well together. To summarize: \n1. Interaction is cyclic: input and output alternate. 2. Output is derivation: external interfaces query \nvisible state, which may be derived from internal state. 3. Input is reaction: external interfaces stimulate \nthe system by submitting batches of changes to visible .elds, which react by propagating changes to internal \n.elds. Input is transactional: all the changes happen atomically or not at all. 4. Derivation (output) \nis pure lazy higher-order functional programming. It executes only upon demand, and can not have any \nside-effects. Derivation is discussed further in the full version [11] of this paper. 5. Reaction (input) \nis coherent. A set of input changes cas\u00adcades through reactions until they all ground out into state \nchanges or errors. Reactions are automatically ordered so that each executes before any others that it \naffects. Reac\u00adtions that transitively affect themselves are an error. Er\u00adrors abort the entire input \ntransaction. 6. Coherence is dynamic. State can grow and change. Re\u00adactions can have arbitrary dynamically \ncomputed effects, though they may need to use progressions to do so. 7. Derivation, as pure functional \nprogramming, does not naturally handle the state mutation inherent in input. Re\u00adaction does not naturally \nhandle output, for that would lead to cyclic effects on inputs. Derivation and reaction need each other. \n 8. Coherence is the dual of laziness. They both remove tim\u00ading considerations from programming. A lazy \nfunction   Figure 4. OO contrasted with Coherence. executes before its result is needed. A coherent \nreaction executes before its effect matters. 9. It is often said that functional programs let one express \nwhat should be computed, not how. Coherent reactions let one express what should be done, not when. These \nsymmetries are pleasing. Derivation and reaction are complementary opposites that .t together like yin \nand yang to become whole. 3. Related Work The fundamental issue of this paper, managing side effects, \nhas been researched so extensively that the related work can only be brie.y summarized in the space available \nhere. A more detailed consideration is in the full version [11] of this paper. Many languages that improve \nstate mutation and event handling do so by mandating a static dependency structure, as in data.ow languages \n[7, 22] and state machines [15]. Such languages do not allow the program to dynamically choose the target \nof changes: you can not assign through a pointer, or update the result of a query. Likewise for Syn\u00adchronous \nReactive Programming (SRP) [1, 5], which was an inspiration for coherent reaction, and is more general \nin some ways. But SRP is intended for embedded systems and so limits itself to static state spaces that \ncan be compiled into state machines or gate arrays. Bidirectional computation is supported in constraint \nand logic languages, and in Lenses [14]. Such bidirectional com\u00adputations are symmetric, whereas the \nasymmetry of deriva\u00adtion and reaction allow arbitrary changes to be expressed. Trellis [8] is a Python \nlibrary that appears to have been the .rst invention of the essential idea of coherent reaction: using \ntransactional rollback to automatically order event de\u00adpendencies. While Coherence was developed independently \nof Trellis, the prior work on Reactors [13] was a direct in\u00ad.uence. Reactors offer a data-driven model \nof computation where data is relational and code is logical rules. It could be said that Reactors are \nto logic programming as Coherence is to functional programming. Monads [27] simulate imperative programming \nthrough higher-order constructions, allowing some parts of the pro\u00adgram to remain pure. But all the usual \nproblems of co\u00adordinating side effects still exist inside the monadic code. Functional Reactive Programming \n(FRP) [6, 12, 18] entirely abandons the notion of mutable state. The normal order of cause-and-effect \nis inverted: effects are de.ned in terms of all causes that could lead to them. FRP requires that pro\u00adgrammers \nlearn a new way of thinking about change. Coher\u00adence retains the common sense notions of mutable state \nand causality, abandoning only the Program Counter. 4. Conclusion Smalltalk s design and existence is \ndue to the in\u00adsight that everything we can describe can be repre\u00adsented by the recursive composition \nof a single kind of behavioral building block that hides its combina\u00adtion of state and process inside \nitself and can be dealt with only through the exchange of messages. Alan Kay [23] The conceptual model \nof Coherence is in a sense opposite to that of Object Oriented languages. As Alan Kay s quote above indicates, \nthe central metaphor of OO is that of mes\u00adsaging: written communication. The central metaphor of Co\u00adherence \nis that of observing a structure and directly manip\u00adulating it. These two metaphors map directly onto \nthe two primary mechanisms of the mind: language and vision. Fig\u00adure 4 contrasts several other language \naspects. The pattern that emerges strikingly matches the division of mental skills into L-brain and R-brain \n[20]. From this per\u00adspective, OO is verbal, temporal, symbolic, analytical, and logical. In contrast \nCoherence is visual, spatial, concrete, synthetic, and intuitive. This observation raises a tantaliz\u00ading \npossibility: could there be such a thing as an R-brain programming language one that caters not just \nto the an\u00adalytical and logical, but also to the synthetic and intuitive?  Acknowledgments This paper \nbene.ted from discussions with William Cook, Derek Rayside, Daniel Jackson, Sean McDirmid, Jean Yang, \nEunsuk Kang, Rishabh Singh, Kuat Yessenov, Aleksandar Milicevic, Frank Krueger, Thomas Lord, and John \nZabroski. References [1] G. Berry and G. Gonthier. The synchronous program\u00adming language ESTEREL: Design, \nsemantics, imple\u00admentation. Science of Computer Programming, 19(2), 1992. [2] F. Brooks. No silver bullet: \nEssence and accidents of software engineering. IEEE computer, 20(4), 1987. [3] S. Burbeck. How to use \nModel-View-Controller (MVC). Technical report, ParcPlace Systems Inc, 1992. [4] E. Burns and R. Kitain. \nJavaServer Faces Speci.cation v1.2. Technical report, Sun Microsystems, 2006. [5] P. Caspi, D. Pilaud, \nN. Halbwachs, and J. Plaice. LUS-TRE: A declarative language for programming syn\u00adchronous systems. In \n14th ACM Symposium on Princi\u00adples of Programming Languages, 1987. [6] G. Cooper and S. Krishnamurthi. \nEmbedding Dynamic Data.ow in a Call-by-Value Language. In 15th Euro\u00adpean Symposium on Programming, ESOP \n2006, 2006. [7] J. Dennis. First version of a data .ow procedure lan\u00adguage. Lecture Notes In Computer \nScience; Vol. 19, 1974. [8] P. J. Eby. Trellis. June 2009. URL http://peak. telecommunity.com/DevCenter/Trellis. \n[9] J. Edwards. Subtext: Uncovering the simplicity of pro\u00adgramming. In OOPSLA 05: Proceedings of the \n20th annual ACM SIGPLAN conference on Object oriented programming, systems, languages, and applications, \npages 505 518. ACM Press, 2005. [10] J. Edwards. Modular Generation and Customization. Technical report, \nMassachusetts Institute of Technol\u00adogy Computer Science and Arti.cial Intelligence Lab\u00adoratory TR-2008-061, \nOctober 2008. URL http://hdl. handle.net/1721.1/42895. [11] J. Edwards. Coherent Reaction. Technical \nreport, Massachusetts Institute of Technology Computer Sci\u00adence and Arti.cial Intelligence Laboratory \nTR-2009\u00ad024, June 2009. URL http://hdl.handle.net/1721.1/ 45563. [12] C. Elliott and P. Hudak. Functional \nreactive animation. In International Conference on Functional Program\u00adming, 1997. [13] J. Field, M. Marinescu, \nand C. Stefansen. Reac\u00adtors: A Data-Oriented Synchronous/Asynchronous Pro\u00adgramming Model for Distributed \nApplications. In Co\u00ad ordination 2007, Paphos, Cyprus, June 6-8, 2007, Pro\u00adceedings. Springer, 2007. [14] \nJ. N. Foster, M. B. Greenwald, J. T. Moore, B. C. Pierce, and A. Schmitt. Combinators for bidirectional \ntree transformations: A linguistic approach to the view\u00adupdate problem. ACM Transactions on Programming \nLanguages and Systems, 29(3), May 2005. [15] D. Harel. Statecharts: A visual formalism for complex systems. \nSci. Comput. Program., 8(3), 1987. [16] D. Harel and A. Pnueli. On the development of reactive systems. \nIn Logics and models of concurrent systems. Springer-Verlag New York, Inc., 1985. [17] G. T. Heineman. \nAn Instance-Oriented Approach to Constructing Product Lines from Layers. Technical report, WPI CS Tech \nReport 05-06, 2005. [18] P. Hudak, A. Courtney, H. Nilsson, and J. Peterson. Arrows, robots, and functional \nreactive programming. Lecture Notes in Computer Science, 2638, 2003. [19] J. Hughes. Why Functional Programming \nMatters. Computer Journal, 32(2), 1989. [20] A. Hunt. Pragmatic Thinking and Learning: Refactor Your \nWetware (Pragmatic Programmers). 2008. [21] J. J\u00a8arvi, M. Marcus, S. Parent, J. Freeman, and J. N. Smith. \nProperty models: From incidental algorithms to reusable components. In Proceedings of the 7th international \nconference on Generative Programming and Component Engineering, 2008. [22] W. M. Johnston, J. R. P. Hanna, \nand R. J. Millar. Advances in data.ow programming languages. ACM Comput. Surv., 36(1), 2004. [23] A. \nC. Kay. The early history of smalltalk. In HOPL-II: The second ACM SIGPLAN conference on History of programming \nlanguages. ACM, 1993. [24] S. Mcdirmid and W. Hsieh. Superglue: Component programming with object-oriented \nsignals. In Proc. of ECOOP. Springer, 2006. [25] J. Noble, A. Taivalsaari, and I. Moore. Prototype-Based \nProgramming: Concepts, Languages and Applications. Springer, 2001. [26] K. Pope and S. Krasner. A Cookbook \nfor using the Model-View-Controller User Interface Paradigm in Smalltalk-80. Journal of Object-Oriented \nProgram\u00adming, 1, 1988. [27] P. Wadler. Monads for functional programming. Lec\u00adture Notes In Computer \nScience; Vol. 925, 1995. [28] M. N. Wegman. What it s like to be a POPL referee; or how to write an extended \nabstract so that it is more likely to be accepted. ACM SIGPLAN Notices, 21(5), 1986.  \n\t\t\t", "proc_id": "1639950", "abstract": "<p>Side effects are both the essence and bane of imperative programming. The programmer must carefully coordinate actions to manage their side effects upon each other. Such coordination is complex, error-prone, and fragile. Coherent reaction is a new model of change-driven computation that coordinates effects automatically. State changes trigger events called reactions that in turn change other states. A coherent execution order is one in which each reaction executes before any others that are affected by its changes. A coherent order is discovered iteratively by detecting incoherencies as they occur and backtracking their effects. Unlike alternative solutions, much of the power of imperative programming is retained, as is the common sense notion of mutable state. Automatically coordinating actions lets the programmer express what to do, not when to do it.</p> <p>Coherent reactions are embodied in the Coherence language, which is specialized for interactive applications like those common on the desktop and web. The fundamental building block of Coherence is the dynamically typed mutable tree. The fundamental abstraction mechanism is the virtual tree, whose value is lazily computed, and whose behavior is generated by coherent reactions.</p>", "authors": [{"name": "Jonathan Edwards", "author_profile_id": "81100596781", "affiliation": "MIT, Cambridge, MA, USA", "person_id": "P1728339", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1639950.1640058", "year": "2009", "article_id": "1640058", "conference": "OOPSLA", "title": "Coherent reaction", "url": "http://dl.acm.org/citation.cfm?id=1640058"}