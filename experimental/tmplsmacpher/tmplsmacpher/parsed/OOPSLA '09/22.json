{"article_publication_date": "10-25-2009", "fulltext": "\n Optimizing Programs with Intended Semantics Daniel von Dincklage Google, Inc. danielvd@google.com \nAbstract Modern object-oriented languages have complex features that cause programmers to overspecify \ntheir programs. This overspeci.cation hinders automatic optimizers, since they must preserve the overspeci.ed \nsemantics. If an optimizer knew which semantics the programmer intended, it could do a better job. Making \na programmer clarify his intentions by placing assumptions into the program is rarely practical. This \nis be\u00adcause the programmer does not know which parts of the pro\u00adgrams overspeci.ed semantics hinder the \noptimizer. There\u00adfore, the programmer has to guess which assumption to add. Since the programmer can \nadd many different assumptions to a large program, he will need to place many such assump\u00adtions before \nhe guesses right and helps the optimizer. We present IOpt, a practical optimizer that uses a spec\u00adi.cation \nof the programmers intended semantics to enable additional optimizations. That way, our optimizer can \nsignif\u00adicantly improve the performance of a program. We present case studies in which we use IOpt to \nspeed up two programs by a factor of 2. To make specifying the intended semantics practical, IOpt communicates \nwith the programmer. IOpt identi.es which assumptions the programmer should place, and where he should \nplace them. IOpt ranks each assumption by (i) the likelyhood that the assumption conforms to the program\u00admers \nintended semantics and (ii) how much the assumption will help IOpt improve the programs performance. \nIOpt proposes ranked assumptions to the programmer, who just picks those that conform to his intended \nsemantics. With this approach, IOpt keeps the programmers speci.cation burden low. In our case studies, \nprogrammers had to add just a few assumptions to realize signi.cant performance speedups. Permission \nto make digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior speci.c permission and/or a fee. OOPSLA 09, October \n25 29, 2009, Orlando, Florida, USA. Copyright c &#38;#169; 2009 ACM 978-1-60558-734-9/09/10. . . $10.00 \nAmer Diwan Department of Computer Science University of Colorado diwan@cs.colorado.edu Categories and \nSubject Descriptors D.3.4 [Programming Languages]: Processors; D.2.6 [Software Engineering]: Programming \nEnvironments General Terms Performance Keywords Optimization, Intended Semantics 1. Introduction Modern \nobject-oriented languages have features that cause programmers to overspecify their programs. This causes \nproblems for tools, such as optimizers because an optimizer cannot know which parts of the semantics \nare due to unin\u00adtended overspeci.cation and which parts were intended by the programmer. To be sound, \nthe optimizer has to respect the full, overspeci.ed, semantics. This degrades the optimiz\u00aders effectiveness \nwhen compared to an optimizer that is not constrained by the overspeci.ed semantics. For example, consider \nthe following Java statement: new EmptyConstructor(); This statement is useless: it creates an object \nthat imme\u00addiately becomes garbage. Thus, the statements intended semantics are that of a no-op. However, \nas written, the state\u00adments actual semantics are overspeci.ed and not a no-op: in Java, object creation \nmay throw an OutOfMemoryException and run class initializers. These initializers may have unpre\u00addictable \nside effects. Since the optimizer must respect the actual semantics, it must preserve these side effects \nand thus cannot remove this statement. In contrast, an optimizer that preserves just the intended semantics \nof the code may remove the useless statement. Such an optimizer is still correct because it preserves \nthe behavior that the programmer actually cares about. That the optimizer may have changed the unintended, \noverspeci.ed, semantics of the program is irrelevant: the programmer never cared about them in the .rst \nplace. Thus, if an optimizer knows the intended semantics of the program, it can do a better job. However, \nit is impossible for an optimizer to determine which semantics are intended and which are not because \nthe optimizer only sees the program source with its actual semantics. Programmers can communicate their \nintended semantics by placing assumptions into the program ([5, 6]). For ex\u00adample, when an assumption-aware \noptimizer encounters a statement that has an assumption  @neverThrows(NullPointerException) the optimizer \ncan ignore the case when the statement throws the NullPointerException. Thus, the optimizer will gen\u00ad \nerate fewer exception edges, which may enable many op\u00ad timizations. In practice, this approach is of \nlimited use because the programmer does not know which assumptions would be useful to an optimizer. Consequently, \nthe programmer may waste time placing many assumptions throughout his pro\u00ad gram without seeing any bene.t. \nFor example, let s suppose we add the following assump\u00ad tion to our statement: @neverThrows(OutOfMemoryError) \nnew EmptyConstructor(); Even with this assumption, the optimizer cannot remove the useless statement \nbecause the exception is not the only side effect of this statement. Speci.cally, the statement may invoke \nclass initializers with arbitrary side effects. To remove the statement, the programmer would have to \nknow that (i) the object construction runs code inside the class initializer (ii) the unknown side effects \nof this code prevent the optimization from applying. It is unlikely that a non-expert programmer will \never consider clarifying his intended semantics in such a way. Instead, the programmer will waste his \ntime by adding assumptions that do not help the optimizer. Thus, assumption-aware optimizers are im\u00ad \npractical unless they guide the programmer towards bene\u00ad .cial assumptions. We present IOpt, a practical \nassumption-aware opti\u00ad mizer. To avoid consuming signi.cant programmer time, IOpt guides the programmer \ntowards assumptions that will quickly improve performance. To guide the programmer, IOpt suggests assumptions. \nThe programmer picks from the suggested assumptions or rejects all suggested assumptions. IOpt prioritizes \noptimizations and assumptions to maximize performance improvement and minimize user effort. IOpt works \nas follows (Figure 1): (i) IOpt .nds optimization opportunities. Each opportunity is a program location \nwhere IOpt can not apply one of its optimizations safely. For every optimization opportu\u00adnity, IOpt computes \na reason. The reason describes why IOpt was unable to safely apply the optimization. The programmer can \nenable the optimization opportunity if he places assumptions that resolve the reason.  (ii) IOpt ranks \nthe optimization opportunities by their cost and bene.t. To measure the cost, IOpt estimates the like\u00adlyhood \nthat the programmer can enable the optimization opportunity. To measure the bene.t, IOpt estimates the \nspeedup due to the optimization.  (iii) IOpt asks the programmer to select an instance of a failed optimization. \n Figure 1: The overall system (iv) IOpt computes the list of assumptions that will enable the programmers \nchosen opportunity. Further, IOpt gen\u00aderalizes the assumptions to give the programmer a wide selection \nof assumptions. (v) IOpt ranks its list of assumptions by (i) how much each assumption will bring the \nIOpt closer to improving the programs performance and (ii) how likely it is that the programmer will \naccept the assumption. (vi) IOpt asks the programmer to examine the highly ranked assumptions and select \nthose that conform to his intended semantics. After the programmer has selected the as\u00adsumptions he wants, \nIOpt repeats the process until it has enough information to apply the programmers chosen optimization \nopportunity.  The contributions of this paper are as follows: It presents a novel approach to assist \na programmer with placing assumptions. Our approach guides the program\u00admer towards assumptions that have \na high bene.t for lit\u00adtle user effort. By supplying guidance, our system makes it feasible to optimize \nprograms by specifying intended semantics with assumptions.  It presents an implementation of our approach, \nIOpt, that supports the full Java language.  It evaluates IOpt and demonstrates that it successfully \nal\u00adlows programmers to place assumptions to aid optimiza\u00adtions. We present case studies where we used \nIOpt to re\u00adduce the run-time of 2 programs to less than half of their original run time.  The rest of \nthis paper is organized as follows. Section 2 il\u00adlustrates our approach with an example. Section 3 describes \nour prior work on computing reasons which we extend in this paper. Section 4 describes our system in \ndetail. Section 5 de\u00adscribes how to add new optimizations to IOpt. Section 6 ex\u00adperimentally evaluates \nthe implementation of our approach. Section 7 reviews related work and Section 8 concludes.  boolean \nfunc(a, b) {Object x, temp1, temp2; x = new Object (); int counter = 0; while ( counter < b.length ) \n{ temp1 = b[counter ]; temp2 = a. .eld ; if (temp1 == temp2) { return true ; } counter++; } ... } Listing \n1: The example program 2. Example We now illustrate IOpt with an example. Section 4 describes our system \nin detail. 2.1 Finding opportunities and reasons Lets suppose IOpt identi.es the following optimization \nop\u00adportunities in the program of Listing 1: Apply loop-invariant code motion (LICM) to temp2 = a.field; \n Apply dead-store elimination to x = new Object();  For these opportunities, IOpt computes the reason \nthat causes them to fail using an extended version of an existing algorithm [9]. Section 3 gives an overview \nof the algorithm; Section 4.2 describes the extensions we add in IOpt. In our example, IOpt determines \nthe following reason for the LICM opportunity: temp2 = a. .eld may not be executed Explanation : b[counter] \nmay be executed and b[counter] may throw an exception This reason means that IOpt cannot apply LICM \nbecause temp2 = a.field may not always be executed. IOpt uses a nested reason to explain why it thinks \nthat the statement may not always be executed. The nested reason ( Explanation ) shows that IOpt thinks \nthat temp2 = a.field may not al\u00adways be executed because the array access b[counter] may throw an exception \nand affect the control .ow of the pro\u00adgram. For the dead-store elimination IOpt produces the follow\u00ading \nreason: new Object() may exit the method Explanation : new Object() may throw an exception and new Object() \nmay change some state Explanation : new Object() is non-analyzed code and non-analyzed code may modify \nany .eld This reason means that IOpt cannot apply dead-store elim\u00adiniation because of two main causes: \nFirst, the new Object() may exit the method because the object construction might throw an exception. \nSecond, the constructor of Object might change the programs state. IOpt explains that this is because \nthe programmer only asked IOpt to analyze the program of Listing 1 and not the contents of the class \nlibrary. IOpt as\u00adsumes worst-case behavior for unanalyzed code. 2.2 Ranking the optimization opportunities \nIn a large program, IOpt will .nd many optimization oppor\u00adtunities. This makes it hard for the programmer \nto decide which optimization opportunity to enable. Therefore, IOpt guides the programmer towards the \nbest optimization oppor\u00adtunities. To determine the best optimization opportunities, IOpt ranks them by \ntheir ratio of estimated cost to estimated bene.t. To determine the cost of an optimization opportunity, \nIOpt estimates how much time a programmer needs to invest in order to enable the opportunity. To determine \nthe bene\u00ad.t of an optimization opportunity, IOpt estimates how much enabling the opportunity will improve \nthe programs perfor\u00admance. This means that an opportunity with few reasons and a high potential speedup \nwill be ranked .rst. To keep this example simple, we estimate programmer time and speedup as follows. \nWe assume that the cost of an optimization opportunity is the number of reasons the programmer must resolve \nto enable the opportunity. Further, we assume that speedup of every optimization is the same. Section \n4.3 describes the actual heuristic IOpt uses to rank optimization opportunities. In our example IOpt \nranks the LICM opportunity higher than the dead-store elimination opportunity since the LICM opportunity \nhas only one reason (dead-store elimination has two). Based on this ranking, the programmer decides to \npursue the LICM opportunity.  2.3 Determining assumptions From each reason, IOpt produces possible assumptions \nthat may resolve the reason. Speci.cally, for every reason, IOpt generates an assumption that directly \ncontradicts the reason and its explanations. We call such assumptions basic as\u00adsumptions. For example, \nfor the reason temp2 = a. .eld may not be executed IOpt generates the basic assumption Assume temp2 = \na. .eld is always executed Similarly, IOpt generates the following basic assumptions for the explanations \nof the reason: Assume b[counter] is never executed Assume b[counter] never throws an exception If the \nprogrammer accepts one of these assumptions, he will make forward progress towards enabling the optimization \nopportunity. While these assumptions can help resolve the reason, they are overly speci.c: they only \nconvey informa\u00adtion about a speci.c array reference. A programmer may wish to add assumptions that resolve \nmany distinct reasons (and not just one). To support this, IOpt automatically gen\u00aderalizes the assumptions. \n To generalize an assumption, IOpt replaces each code fragment with a generalized version of it. For \nexample, IOpt may use any expression or any statement to generalize b[counter]. Thus, IOpt may generalize \nAssume b[counter] never throws an exception into Assume any statement never throws an exception  2.4 \nRanking the assumptions Since there are many possible assumptions that may resolve a reason (e.g., some \nof the assumptions may be generaliza\u00adtions of other assumptions) the programmer needs guidance on which \nassumptions to use. To help with this, IOpt ranks the assumptions using their estimated cost/bene.t ratio. \nTo keep our example short, we again use a simple method of determining the cost/bene.t ratio. First, \nevery assumption has the same speci.cation cost. Second, the bene.t of an assumption is the number of \noptimization opportunities that will bene.t from it. Section 4.4.3 gives the full heuristic that IOpt \nuses. In our example, we determine that the assumption Assume any statement never throws an exception \nis better. This is because this assumption .xes the following two reasons: new Object() may throw an \nexception b[counter] maythrowan exception All other assumptions in our example only resolve a single \nreason. Our system presents the ranked list of candidate assump\u00adtions to the user. The user examines \nthe .rst assumption, and accepts it. The assumption resolves the single reason of the programmers chosen \nopportunity and one of the two rea\u00adsons of the other opportunity. Since the programmer has resolved all \nreasons of his chosen opportunity and thus enabled it, IOpt can now apply the optimization, provided \nthat it passes validation.  2.5 Validating the Assumptions Before applying all optimizations the programmer \nhas en\u00adabled, IOpt validates the programmer s assumptions. To do this, IOpt instruments the program and \nruns the pro\u00adgrams tests. The instrumentation checks whether or not the assumptions hold at run time. \nIf any of the checks fail during the testing runs, IOpt identi.es the problematic assumptions to the \nprogrammer. If the checks for all assumptions succeed, IOpt applies the enabled optimizations. In our \nexample, the validation is successful. Thus, IOpt applies the LICM optimization to line 7 the program \nof Figure 1. 3. Background Since we extend our prior work to compute reasons [9] for optimization opportunities, \nwe summarize that work here. Each optimization opportunity has an analysis that checks whether or not \napplying the opportunities optimization is safe; our prior work shows how to produce reasons when that \nanalysis fails. For simplicity we assume here that the analysis fails when it evaluates to false. Our \nprior work shows how we handle non-boolean analyses. 3.1 Reasons There are two kinds of reasons: (i) \nroot reasons, written as failed(analysis, x1, ... , xn); and (ii) constructed rea\u00adsons that combine reasons \nusing and/or. A root reason de\u00adscribes the failure of an analysis with a speci.c set of argu\u00adments. Constructed \nreasons combine multiple reasons. For example, an analysis may fail with the following root rea\u00adson: \nfailed (DoesNotModify, T::x, x = f ()) This reason says that the analysis failed because x = f() may \nmodify the variable T::x. Another analysis may com\u00adpute this failure reason: failed (DoesNotModify, T::x, \nx = f ()) and failed (DoesNotModify, T::y, x = f ()) This means that the analysis failed because the \nstatement x = f() might modify both T::x and T::y. A programmer can resolve a failure reason. When the \npro\u00adgrammer resolves a reason, he tells IOpt that he knows that this reason is wrong according to his \nintended semantics. The way to resolve a reason depends on the form of the reason: (i) For an and, the \nprogrammer must resolve all children of the and; (ii) For an or the programmer may resolve any of the \nor s children; and (iii) For a root reason, the programmer must place an appropriate assumption. Our \nprior work computes reasons that are necessary and suf.cient. Necessary means that a reason for an analysis \nfail\u00adure may only describe problems that contribute to the failure. Suf.cient means that the reason for \nan analysis failure must describe all problems that contribute to the failure.  3.2 The Failure Reason \nAnalysis Our prior work shows how to produce failure reasons pro\u00advided the analysis is written in a special \nguard language. The guard language can express at least any data.ow analysis. The guard language provides \nthe following constructs: (i) A property construct that allows the analysis to access the results of \nanalyses implemented outside of the system. An invocation property(a, x1,...,xn) returns true if the \nanalysis a succeeds for arguments x1 to xn. Otherwise, the invocation returns false. (ii) The constructs \nand and or. A use and(x,y) (or(x,y)) returns true, if x and (or) y are true; (iii) The construct forall \nx:y in z. It returns true if z evaluates to true for every element in y.  1 isDeadStore(s = e) := 2 \nforall x in property ( usesForDe.nition , s = e) 3 property (isDeadCode, x) 4 and 5 property ( isStraightLineCode \n, s = e) 6 and 7 property (doesNotChangeState, e) Listing 2: The analysis for dead-code elimination \nThe guard language uses positive logic; it does not con\u00ad tain negation or existential quanti.cation. \nAs a consequence, the only way an analysis in the guard language can fail is if one or more boolean properties \nproduce false. The guard language forces an analysis author to differ\u00ad entiate between two parts of his \nanalysis: (i) parts that can cause an analysis analysis failure; and (ii) parts that propa\u00ad gate a failure. \nThis distinction makes it possible to compute reasons. If an analysis fails, it must be because one or \nmore of the properties that the analysis uses must have failed all other constructs in the guard language \ncan just propagate failures. We construct the reasons as follows: whenever a property fails, we create \na root reason. When we combine properties with and/or, we create appropriate and/or reasons.  3.3 Example \nWe now give a brief example to demonstrate how we com\u00ad pute the failure reasons for the dead-store elimination \nexam\u00ad ple in Section 2. Listing 2 shows the dead-store analysis in our guard lan\u00ad guage. The analysis \ndetermines a store to be dead iff (i) all uses of the store are dead; (ii) it contains no branch; and \n(iii) does not change any state (other than in the store itself). We now attempt to apply the dead-store \nelimination to the following statement: List list = new FooList(); To keep the example simple, we assume \nthat the local vari\u00ad able list is never used. To evaluate the analysis, we bind s to list, and e to new \nFooList(), and start evaluating the analysis. Because list is never used, the forall from lines 1 and \n2 evaluates to true. Because the term is true, we do not consider it for the reason computation. Recall \nthat in the guard language, a construct can cause an analysis to fail only if it evaluates to false. \nThis leaves us with lines 5 7. The analysis implement\u00ading the isStraightLineCode property fails because \nthe ob\u00adject creation might throw an exception. The analysis for the doesNotChangeState property fails \nbecause the construc\u00adtor of FooList might modify program state. We generate root reasons for both property \ninvocations. Because both property invocations are combined with an and, we com\u00adbine both their root \nreasons with an and. 1 x = this .example( test ); 2 EXPR1 = this.ID1(EXPR2); 3 EXPR1 = this.ID1(EXPR*); \n2 Listing 3: The example patterns for dead store elimination This gives us: failed ( IsStraightLineCode \n, list = new FooList ()) and failed (DoesNotChangeState, new FooList()) This reason tells us that the \nstatement list = new FooList() is not a dead store because (i) the statement is not straight line code; \nand (ii) the statement might change the programs state. 4. Details of our system So far, we have only \nused examples to illustrate IOpt. We now present IOpt in detail. 4.1 Optimization Opportunities IOpt \nstarts by identifying optimization opportunities (Fig\u00ad ure 1). An optimization opportunity is a program \nlocation where the programs actual semantics prevent IOpt from ap\u00ad plying one of its optimizations. Section \n4.3 shows how IOpt ranks these opportunities to guide the user towards the most pro.table ones. IOpt \ntries to .nd as many realistic opportunities as pos\u00ad sible. An optimization opportunity is realistic \nif there is a chance that the programmer can enable it by placing assump\u00ad tions. For example, an expression \ninside of a loop is a realis\u00ad tic opportunity for loop-invariant code motion; conversely, an expression \noutside of a loop is an unrealistic opportu\u00ad nity for loop-invariant code motion. Telling the programmer \nabout unrealistic opportunities is a waste of the program\u00ad mer s time. To .nd realistic opportunities \nfor an optimization, IOpt searches the program with a syntactic pattern speci.c to that optimization. \nWhenever IOpt .nds a match to the pattern, it analyzes the match (using program analyses) to determine \nif the optimization is safe. If it is safe, IOpt applies the opti\u00ad mization; if not, IOpt records the \nmatch as an optimization opportunity. The notation for specifying the syntactic patterns has three kinds \nof constructs, illustrated in Listing 3. IOpt matches these constructs to a three-address form of the \npro\u00ad gram: (i) Java constructs Every Java construct in a pattern matches the same con\u00adstruct in the program; \nLine 1 of Listing 3 shows a pat\u00adtern that just uses Java constructs. This pattern will only match statements \nthat are identical to the pattern. (ii) Syntactic variables of the form TYPE, EXPR, STMT, ID, ...  These \nconstructs match exactly one Java construct of the kind indicated by the name of the syntactic variable \n(a type, an expression, a statement, an identi.er, ... ). Line 2 of Listing 3 shows a pattern that uses \nsuch syntactic variables. That pattern matches an assignment with an expression on the left-hand side \nand a call to a method with an arbitrary name and a single expression argument on the right-hand side. \n (iii) Syntactic variables of the form EXPR * , STMT * , ... These match zero or more Java constructs \nof the kind indicated by the variables name. Line 3 of Listing 3 shows a pattern that uses such syntactic \nvariables. This pattern generalizes the pattern in Line 2 to allow zero or more expression arguments. \n 4.2 Extending our prior reasons work Given optimization opportunities, we now interact with the user \nto obtain the necessary assumptions (Figure 1). To do this, IOpt must compute reasons for failed optimization \nop\u00adportunities. For these reasons to be useful, they must be un\u00adderstandable by a programmer. After extensively \nsurveying the reasons produced by our prior work (Section 3) we found that many reasons required sophisticated \nknowledge of the failing program analysis or the Java programming language to be comprehensible. Thus, \nwe extended our prior work to make the reasons easier to understand; this section describes those extensions. \n4.2.1 Explaining Reasons Our prior work on computing reasons assumes that the prop\u00aderties used in reasons \nare simple enough that the programmer will fully understand them (Recall from Section 3 that prop\u00aderty \nis a mechanism for one analysis to query the results of another analysis). This assumption allows the \nreason analy\u00adsis to create the same root reason whenever a property fails because it can rely on the \nprogrammer to correctly interpret the root reason. Unfortunately, correctly interpreting root reasons \ncan be dif.cult without deep knowledge of the underlying program\u00adming language and the failing analysis. \nSpeci.cally, a root reason only tells the programmer that the property failed but not why it failed. \nIf the programmer does not understand the (external) analysis that implements the property he may mis\u00adunderstand \nthe root reason. Thus the programmer may pro\u00advide an incorrect assumption that does not correctly capture \nhis intended semantics. To see an example of this, consider the reason we com\u00adputed in Section 3: failed \n( IsStraightLineCode , list = new FooList ()) and failed (DoesNotChangeState, new FooList()) Now assume \nthat IOpt proposes the assumption assume(DoesNotChangeState, new FooList()) To approve this assumption \nthe programmer may examine the constructor of FooList. Since the constructor is empty, the programmer \nmay reasonably assume that the assumption is valid and approve it. Unfortunately this assumption may \nnot capture the pro\u00adgrammer s intended semantics. Speci.cally, there are sev\u00aderal reasons that caused \nIOpt to report that new FooList() can change state. For example, the object construction might cause \na class initializer to run, which might call other state\u00adchanging methods. Since the programmer approved \nthe as\u00adsumption without considering this possibility his assumption may not be correct. To address this \nproblem, IOpt extends the guard language and the kind of reasons the reason analysis computes. In our \nprior work, an analysis could not use the results of other analyses written in the guard language. This \nsimpli.ed the semantics of the property construct. In IOpt, we allow analyses to use the results of other \nanalyses. For this, extend the notion of what a reason is and modify the semantics of the property construct \nto compute these extended reasons. We extend reasons by giving each root reason an optional explanation. \nThe explanation of a root reason says why IOpt computed the root reason. Explanations are reasons also; \nthus any root reason within an explanation may have its own explanation. To compute explanations we also \nexpress properties in our language. Thus, when a property fails, our system automatically produces a \nreason for it. If that property is used in an analysis, the reason for the failure of the property becomes \nthe explanation for the overall analysis failure. This gives us hierarchical reasons that explain why \nIOpt computed its results: failed ( IsStraightLineCode , list = new FooList ()) Explanation : failed \n(DoesNotThrowException, new FooList()) and failed (DoesNotChangeState, new FooList()) Explanation : failed \n(DoesNotChangeState,FooList:: (clinit)()) and failed (DoesNotCall, new FooList(), FooList :: (clinit)()) \nProviding explanations has another bene.t: it gives pro\u00adgrammers greater .exibility in resolving reasons. \nSpeci.\u00adcally, a programmer can resolve a root reason by either re\u00adsolving the reason itself or by resolving \nthe explanation for the root reason. Because an explanation may have its own explanations, there are \noften many ways in which the pro\u00adgrammer can resolve a given reason.  4.2.2 Self-contained reasons Even \nwith the explanations, we found that in some cases, our reasons were inadequate. Speci.cally, while the \nreasons were technically correct, they did not contain enough detail for the programmer to make an informed \ndecision. For example, consider the following analysis to determine if a call to method T::f modi.es \na variable v.  CallDoesNotModify(v, T::f ()) := forall s in property(statementsOf,T::f) property (StatementDoesNotModify,v,s) \nand property( IsFinal ,T::f) This analysis fails if method f is not .nal. This is because if f is non-.nal, \na (possibly dynamically loaded) subclass may override f so that it now modi.es v. Unfortunately, while \ngiving a reason failed(IsFinal, f) is technically correct, it does not explain to the programmer why \na non\u00ad.nal f is problematic. The logic behind the property(IsFinal, f) is in the mind of the analysis \nwriter. Our system, as described so far, does not provide a way for the analysis writer to write down \nthis logic because our language does not have a way of referring to entities (e.g., methods) that are \nnot part of the current program. To address this limitation, we provide ways of express\u00ading generalizations \nof speci.c entities (Table 1). Using these generalizations, we can write CallDoesNotModify as fol\u00adlows: \nCallDoesNotModify(v, T::f ()) := forall S in subtypes T property (MethodDoesNotModify, v, S::f ()) The \nhelper (MethodDoesNotModify) is overloaded, with one implementation for available methods and one for \ndy\u00adnamically loaded methods. We write the implementation for available methods in IOpt s guard language: \nMethodDoesNotModify(v, T::f) := forall s in property(statementsOf, T::f()) property(StatementDoesNotModify, \nv, s) We write the implementation for dynamically loaded meth\u00adods (MethodDoesNotModify(v, (dynamicType \nT)::f)) out\u00adside of the guard language. This implementation of the helper is conservative and always \nreturns false. Let us assume we now want to use CallDoesNotModify to determine if a call to the (non-.nal) \nmethod m, declared in (non-.nal) class C, will modify variable x. We do this by evaluating the call CallDoesNotModify(x, \nC::m) which in\u00advokes MethodDoesNotModify on all subtypes of C. IOpt uses the de.nition from Table 1 to \nexpand subtypes C into the set userTypes C . libraryTypes C . dynamicTypes C. Next, IOpt attempts to \nexpand the elements of that set. First, IOpt expands userTypes C. In our example, there is exactly one \ntype that is a subtype or equal of C C itself. Second, IOpt expands libraryTypes C. In our example, \nthere is no subtype of C in the class library so the expansion is empty. Last, IOpt expands dynamicTypes \nC. For this expansion, IOpt checks if C is .nal. Since C is not .nal, dynamicTypes C expands into dynamicType \nC (where dynamicType stands for a possible subtype of C that is dynamically loaded). (If C had been .nal, \ndynamicTypes C would have expanded into the empty set). This gives us the expanded generalization {C, \ndynamicType C }. IOpt now knows that it must evaluate MethodDoesNotModify for the method C::m() and the \nmethod (dynamicType C)::m(). For each method, IOpt evaluates the most speci.c over\u00adloaded implementation \nof the helper. We therefore produce the reason: failed (CallDoesNotModify, x, m()) Explanation : failed \n(MethodDoesNotModify, x, (dynamicType C)::m) This explanation is far superior to failed(IsFinal, m) \nsince it explains that it is just a dynamically-loaded subclass of C that may modify x. In summary, our \nsystem provides generalizations that en\u00adable analyses to produce good reasons even when the reason must \nrefer to code that may or may not be part of the actual running program.  4.3 Ranking and Choosing \nOpportunities IOpt typically .nds many optimization opportunities. For example, every computation in \na loop is a potential candidate for loop-invariant code motion. To guide the programmer towards the most \npro.table opportunities, IOpt ranks the opportunities based on their estimated cost and bene.t. The programmer \ncan then pick which opportunity he wants to enable. Focusing on one opportunity at a time ensures that \nthe programmer is always making progress towards enabling at least the current opportunity. If the programmer \ndid not focus on one opportunity at a time, he may end up providing many assumptions that ultimately \nenable no opportunity. IOpt uses three heuristics to rank optimization opportuni\u00adties: rank by risk (Section \n4.3.1), rank by commonality (Sec\u00adtion 4.3.2), and rank by speedup (Section 4.3.3). We com\u00adbine the three \nranking heuristics into a comesBefore func\u00adtion which determines the ranking that we present to the pro\u00adgrammer. \nOur ranking gives .rst priority to risk, second to commonality, and third to speedup. In our experiments \nwe found that this combination gives the best results. We now explain in how IOpt computes the values \nof each of the three heuristics. 4.3.1 Rank by risk Rank-by-risk determines the cost of enabling an \noptimiza\u00adtion opportunity. Let s suppose the an optimization fails due to 19 reasons. After resolving \nthe .rst 18 reasons, the programmer .nds that he cannot resolve the 19th reason. In other words, the \neffort in resolving the .rst 18 reasons is wasted: the opti\u00admization is inapplicable because of the 19th \nreason. We say this optimization is high risk because it contains a reason that the programmer cannot \nresolve: i.e. a programmer may spend much time on it without getting any bene.t. The rank\u00adby-risk heuristic \ntries to identify risky optimization opportu\u00adnities by looking for reasons that the programmer may not \nbe able to resolve. To determine risk scores for reasons, IOpt .rst determines the initial risk score \nfor every root reason. Then, IOpt per\u00adforms a .xed point computation to revise scores for root rea\u00adsons \nand to compute a risk score for every non-root reason.  Types T type t libraryType t, t . T dynamicType \nt, t . T userTypes t, t . T libraryTypes t, t . T dynamicTypes t, t . T subtypes t, t . T the analyzed \ntype t the non-analyzed type t from the class library a (hypothetical) dynamically loaded subtype of \nt {t'|t' <: t and t is part of the user code}{t'|t' <: t and t is part of a library}{t'|t' <: t and t' \nis dynamically loaded}userTypes t . libraryTypes t . dynamicTypes t Program Locations L statement s method(m) \nallMethods(t::m), t . T entireType t, t . T the statement s {s|s is a statement in m}{t' :: m()|t' . \nt}{t' :: n()|t' . t, n is a method/initializer in t'} Table 1: Example generalizations in IOpt To compute \nthe scores for root reason, IOpt uses a func\u00adtion called root-risk (Listing 4). The root-risk function \nexploits the observation that to produce a precise output, an analysis must have precise input. Speci.cally, \nroot-risk examines the precision of the arguments to the property mentioned in the reason. If a property \nwith general argu\u00adments fails, it may be the fault of the general arguments; thus the user may be able \nto resolve the failure (i.e., low risk ). Conversely, if a property with speci.c arguments fails, it \nis less likely that a user can resolve the failure (i.e., high risk ). For example, consider the following \nreason: failed (MethodDoesNotModify, x, T::f ()) The arguments to the property (MethodDoesNotModify) \nare quite precise: a speci.c (local) variable and a call to a partic\u00adular function. Thus, the property \ns failure is probably not due to an imprecision. Consequently, it is unlikely that the pro\u00adgrammer can \nresolve this reason. In contrast, consider the following reason: failed(MethodDoesNotModify,.elds(subtypes \nObject), T::f()) Since the .rst argument is general (all .elds in the program), it is likely that MethodDoesNotModify \nfailed because of this imprecise argument. Thus a programmer can probably re\u00adsolve this reason. risk-for-arg \nproduces a risk score for a single argument to a property. As discussed above, this score directly re.ects \nthe generality of the argument with higher numbers indi\u00adcating lesser generality (and thus higher risk). \nThe function root-risk combines the scores for the argument by raising each arguments score to the power \nof the next argument. Thus, if a reason has three arguments with a risk score of 10, it is considered \nriskier than if it had a single higher risk argument (say at a score of 100). Function rank-by-risk in \nFigure 5 shows how we com\u00adbine the risk scores of individual root-reasons into a risk score for all reasons \nincluding constructed reasons. The risk score for the reason of an optimization failure is the risk score \nfor that optimization. risk -for-arg(arg) := if arg = type t || arg = statement s : # These are the most \nspeci.c values return 1000 if arg = libraryTypes t || arg = userTypes t : return 100 if arg = subtypes \nt || arg = allMethods( t :: m) : return 10 if arg = entireType t || arg = dynamicTypes t : return 1 \nroot-risk( failed (a, x1, ...,xn)) := result := risk -for-arg(x1) for xin x2, ...,xn: arg score := risk \n-for-arg(x) score result := result arg return result Listing 4: The algorithm to compute risk for a \nreason. High-Risk, MediumRisk, LowRisk, and MinimalRisk are numeric constants, with larger numbers indicating \nhigher risk. Function rank-by-risk initializes all risk scores to MAX-INT except for root reasons which \nget scores according to root-risk. Then, rank-by-risk revises the initial scores for every reason. This \nis necessary since the graph of reasons may have cycles. rank-by-risk continues revising until the scores \nreach .xed point. Function rank-by-risk revises scores as follows: Root Reasons The programmer can resolve \na root reason by placing an assumption that will resolve the reason itself or by resolving the root reasons \nexplanation. The pro\u00adgrammer will probably resolve whichever one is easier. Thus, the score of the root \nreason is the minimum of the root reason s risk score and the explanation s risk score.  sum-with-ceiling(a, \nb) := if (a+b > MAXINT): return MAXINT return a +b rank-by-risk() := .nished := false risk -scores := \n[] for reason in all -reasons: if (is -root-reason(reason)): risk -scores[reason] := root-risk(reason \n), else : risk -scores[reason] := MAXINT while ! .nished : .nished := true for reason in all -reasons: \n if (is -root-reason(reason)): val := root-risk(reason ), if (has-explanation(reason ): val := min(val \n, risk -scores[reason. explanation ]) if (is -and-reason(reason)): val := sum-with-ceiling( risk -scores[reason. \nleft ], risk -scores[reason. right ]) if (is -or-reason(reason)): val := min(risk-scores[reason. left \n], risk -scores[reason. right ]) if val != risk -scores[reason ]: .nished := false risk -scores[reason] \n:= val Listing 5: The algorithm to compute risk scores Or Reasons The programmer can resolve an or reason \nby resolving either of the or-reasons children. Similar to root reasons, we set the score of an or reason \nto the minimum of the scores of the children. And Reasons The programmer must resolve an and reason by \nresolving both of the and reasons children. Since the programmer must resolve both children, we set the \nresolvability score of an and-reason to the sum of the resolvability scores of the children. To ensure \nthat the .xed point computation completes, we put a ceiling on the resolvability scores for and reasons. \nThis .xed point computation will terminate because every cycle must traverse an explanation edge. Since \nthe score for a root reason is the minimum of the score of the root reason itself and the score of its \nexplanation, propagation through the cycle will stop as soon as the value of all scores on the cycle \nis equal to the lowest score of any root reason on the cycle.  4.3.2 Rank by commonality The previous \nheuristic (rank-by-risk) quanti.es the cost of enabling an optimization opportunity. The rank-by-commonality \nheuristic quanti.es one of the bene.ts of enabling an opti\u00admization opportunity: enabling an optimization \nmay enable other optimizations also. Speci.cally, rank-by-commonality ranks opportunities by the commonality \nof their reasons. IOpt uses this heuristic to make sure that the user looks at optimization opportuni\u00adties \nthat have failure reasons that appear often. This heuristic is useful because in our experience, the \nsame reason often appears for many different optimization opportunities. If the programmer resolves such \na reason for one opportunity, he resolves the reason for many other opportunities, too. Besides giving \nthe programmer a greater payoff for his effort, a high commonality score also lowers the impact of risk. \nFor example, a programmer may resolve many reasons for an optimization opportunity only to later discover \nthat one of the reasons is unresolvable. If that optimization op\u00adportunity has a high commonality score, \nthe work on the optimization is not wasted: the assumptions added for this opportunity will likely bene.t \nother optimizations also. We compute the commonality score in two steps. First, for every reason, IOpt \ncounts the number of optimization opportunities that contain that reason. This gives IOpt an in\u00addication \nhow common each reason is. Second, for every opti\u00admization opportunity IOpt sums up the commonality scores \nfor all reasons of the optimization opportunity. This gives IOpt a commonality score for each optimization \nopportunity.  4.3.3 Rank by speedup The rank-by-speedup heuristic quanti.es the other bene.t of enabling \nan optimization opportunity: the speedup incurred by the program. We quantify the speedup score for an \nopti\u00admization opportunity by dividing its expected speedup by the number of assumptions that the opportunity \nneeds. Rank-by\u00adspeedup estimates the speedup due to an optimization oppor\u00adtunity using basic-block counts \ndetermined using pro.ling.  4.4 Identifying and Ranking Assumptions Given an optimization opportunity \nthat the user wishes to work on, IOpt produces candidate assumptions that can en\u00adable the optimization. \nThis section describes what we mean by assumptions, how we produce assumptions from reasons and how we \nrank the assumptions to determine which as\u00adsumptions the programmer should examine .rst. 4.4.1 What \nare Assumptions An assumption is a trusted assertion that a property will suc\u00adceed for a given set of \narguments. We write assumptions in the form assume(a, v1, ... , vn). This assumption means that property \na always succeeds for arguments v1,...,vn and thus resolves the failure reason failed(a, v1 ...vn). Since \nonly failing properties contribute to analysis failure, this is the only form of assumption that we need. \nTo resolve a constructed reason (e.g., using and/or) the programmer adds assumptions for one or more \nroot reasons within the con\u00adstructed reason. IOpt does this to encourage the programmer to add general \nassumptions which may help multiple oppor\u00adtunities. Speci.cally the root reasons within an and/or rea\u00adson \nwill always at least as general as the and/or reason itself.  For example, take the assumption assume(MethodDoesNotModify, \nT::x, C::f ()) With this assumption, IOpt assumes that the analysis Method-DoesNotModify will succeed \nif the analysis is run for ar\u00adguments T::x and C::f(). Thus, in the guard language, the property invocation \nproperty (MethodDoesNotModify, T::x, C::f ()) will never fail. As a consequence, the assumption resolves \nall failure reasons of the form failed (MethodDoesNotModify, T::x, C::f ()) To increase our assumptions \nexpressiveness, we allow gen\u00aderalizations from our program domains (Section 4.2.2) as ar\u00adguments of assumptions. \nIf an argument of an assumption is a generalized element (e.g., subtype t) then the assumption affects \nall elements in the generalized element. For example, take the assumption assume(MethodDoesNotModify, \nAny Field, C::f()) This assumption forces IOpt to assume that the analysis success for any .eld and C::f(). \nThus, all of the property invocations property (MethodDoesNotModify, T::x, C::f ()) property (MethodDoesNotModify, \nG::y, C::f ()) ... property(MethodDoesNotModify,subtypesObject::x, C::f()) will succeed.  4.4.2 Finding \nAssumptions As indicated above, assumptions come in two .avors: (i) base assumptions which directly address \na failing reason; and (ii) generalized assumptions which use generalized ar\u00adguments. IOpt automatically \ngenerates both kinds of as\u00adsumptions for the bene.t of the programmer. To compute the set of base assumptions, \nIOpt starts with the programmers chosen optimization opportunity. It takes the opportunities failure \nreason and extracts all root reasons reachable via and/or reasons. It then takes those root rea\u00adsons \nexplanations, and .nds all root reasons in them. IOpt does this until it reaches a .xed point. Then, \nfor every root reason failed (a, x1, ...,xn) that IOpt has found, it constructs the base assumption assume(a, \nx1, ...,xn) To construct a generalized assumption, IOpt generalizes the arguments for the base assumptions. \nSpeci.cally, to gener\u00adalize an assumption assume(a, x1,...,xn), IOpt computes a set of generalizations \nfor each of the assumptions argu\u00adments. Then, IOpt creates an assumption for every element of the cross-product \nof the arguments sets of generaliza\u00adtions. To compute the generalizations for a single argument, IOpt \nreplaces the argument with its generalizations from Ta\u00adble 1. 4.4.3 Ranking Assumptions For a typical \noptimization opportunity, the set of candidate assumptions may be large. Therefore, IOpt must rank the \ncandidate assumptions to make sure it suggests the best possible assumption to the programmer. As an \nexample, consider an optimization opportunity that fails because of a single reason without an explanation: \nfailed (MethodDoesNotModify, T::x, C::f ()) IOpt will generate many assumptions for this reason: assume(MethodDoesNotModify, \nT::x, C::f ()) ... assume(MethodDoesNotModify, subtypes T::x, C::f ()) assume(MethodDoesNotModify, subtypes \nObject::x, C::f ()) ... assume(MethodDoesNotModify, subtypes Object::any Field , subtypes Object :: \nany Method) IOpt produces many assumptions even for a single reason and asking the programmer to wade \nthrough these assump\u00adtions is unreasonable. Thus, IOpt ranks the assumptions be\u00adfore presenting them \nto the programmer. This ranking must accommodate two con.icting goals. First, we want users to pick assumptions \nthat resolve as many reasons as possible and thus enable many optimizations at once. Second, we do not \nwant the user to reject assumptions and users are likely to reject assumptions that are overly general. \nIOpt s assumption ranking consists of two heuristics, one for each of the above two goals: (ii) A heuristic \nthat mea\u00adsures an assumptions commonality; (ii) A heuristic that measures an assumptions risk. Both heuristics \nare similar to the heuristics for ranking opportunities in Section 4.3. Rank according to commonality \nIOpt counts the number of reasons, across all optimization opportunities, that the assumption will resolve. \nA higher count results in a higher rank. Rank according to risk When ranking optimization opportunities \n(Section 4.3) IOpt considered a reason with more general arguments to be riskier than a reason with less \ngeneral arguments. However, a user is more likely to approve an assumption with speci.c arguments than \none with general arguments. Thus, this rank\u00ading is the exact inverse of the root-risk method (Section \n4.3). We combine the two heuristics by .rst ranking according to risk and breaking ties using commonality. \n 4.4.4 Asking the Programmer When IOpt asks the programmer to select assumptions, it presents the programmer \nwith two things: (i) The ranked list of assumptions it computed; and (ii) The expanded tree of the entire \nfailure reason of the programmers chosen opti\u00admization opportunity, including (transitive) explanations. \nTo place assumptions, the programmer examines the rea\u00adson that prevents his chosen optimization opportunity. \nHe then picks a root reason and examines the assumptions IOpt proposes, starting with the highest-ranked \nassumption.  The programmer can inspect any assumption IOpt presents by selecting it. If he selects \nan assumption, IOpt highlights all root reasons in the tree that will be affected by the as\u00adsumption. \nThat way, the programmer can double-check that he does not miss any reason that would prevent him from \nplacing the assumption. The programmer can continue adding IOpt s proposed as\u00adsumptions until either \n(i) the programmer identi.es a rea\u00adson that he cannot resolve; or (ii) the programmer has added enough \nassumptions to enable his chosen opportunity. If the programmer has added enough assumptions to en\u00adable \nhis chosen opportunity, he can either choose a new op\u00adtimization opportunity, or he can apply his optimization. \nIf he chooses to apply his optimization, IOpt will .rst validate the assumptions he has placed (Section \n4.5) If the programmer identi.es a root reason that he cannot resolve, there are two possibilities. First, \nif the programmer is lucky, he may not need to resolve the reason. This happens if the root reason is \na child of an or or inside another root reasons explanation. In this case, the programmer can ignore \nthe root reason he cannot resolve and continue resolving other reasons. Second, if the programmer is \nunlucky, he must resolve the reason to enable the optimization opportunity. In this case, the programmer \ncannot enable his chosen optimization opportunity. To make his program faster, he will have to choose \na new optimization opportunity. To take into account any assumptions the programmer may have already \nplaced, IOpt re-ranks all remaining optimization opportunities.  4.5 Discouraging Incorrect Assumptions \nIf a programmer adds an incorrect assumption, it may result in an incorrect optimization. Thus, IOpt \ntakes two measures to reduce the likelihood of incorrect assumptions. First, IOpt presents the full explanation \nof its reasons to the programmer. Second, IOpt experimentally checks the programmers assumptions before \nit applies any optimiza\u00adtion based upon them. 4.5.1 Fully explaining the reasons As we have described \nin Section 4.2.1 IOpt computes most of its analyses within its guard language. That way, IOpt can compute \nreasons whenever these analyses fail. This allows IOpt to compute extremely verbose explanations that \nlist all causes for an optimization failure. IOpt makes the full tree of explanations available to the \nprogrammer at the point where he has to accept or reject assumptions. That way, the programmer can check \nthat he has considered all implications of his assumption.  4.5.2 Checking the programmers assumptions \nAlthough IOpt guides the programmer away from dangerous assumptions the programmer may still make a mistake \n i.e. inadvertently accept an invalid assumption. IOpt checks the programmers assumptions to .nd such \nmistakes. Checking the programmers assumptions statically is im\u00adpossible because IOpt asks the programmer \nto place assump\u00adtions that contradict the programming languages speci.ca\u00adtion. No correct static checker \ncould accept such assump\u00adtions. Thus, IOpt checks assumptions dynamically. For each as\u00adsumption, IOpt \ninstruments the program with checks that validate the assumption. IOpt then runs the programs test suite. \nIf the instrumentation detects a violation of the as\u00adsumption, it is clear that the programmer made a \nmistake. To enable IOpt to translate assumptions into instrumen\u00adtation, each property implemented outside \nof our system comes with an instrumentation template. We use standard instrumentation tools (AspectJ) \nto implement our instrumen\u00adtations. For example, let s suppose the programmer places the assumption assume(MethodDoesNotModify, \nT::x, allMethods(MyList)) Before IOpt will optimize a program with this assumption, IOpt will use AspectJ \nto instrument the programs code with an assertion before every write access to x. The assertion fails \nif any method from MyList is currently active. If the assertion fails while running the programmers test \nsuite, IOpt will reject the assumption. 5. Implementing new optimizations in IOpt Because IOpt interacts \nwith the programmer, it can apply more aggressive optimizations than a traditional optimizer. This is \nbecause given the right assumptions IOpt can apply optimizations that are illegal in traditional optimizers. \nThese aggressive optimizations allow IOpt to realize larger speedups than traditional optimizers (Section \n6). IOpt incorporates intended semantics into its optimiza\u00adtions into two ways. First (as we have already \ndescribed), IOpt engages in a dialog with the user. Second, an IOpt op\u00adtimization may incorporate some \nintended semantics about a library. For example, later we present results for an IOpt optimization that \nhoists invocations of List::size(). Obvi\u00adously, such an optimization will be useful only to programs \nthat use List. Since these aggressive optimizations are specialized to the programs libraries, we cannot \nanticipate all such opti\u00admizations that a programmer might want to apply. Thus, we allow programmers \nto extend IOpt with new optimizations. For most aggressive optimizations, the programmer can extend IOpt \nby simply specializing an existing optimiza\u00adtion. This works because most aggressive optimizations are \njust a variant of a standard compiler optimization that is ap\u00adplied on a large scale. For example, hoisting \ninvocations of List::size() is just a specialization of standard LICM; A large-scale variant of strength \nreduction might replace spe\u00adcial cases of complex method invocations with a simpler call. To extend IOpt \nwith a new optimization, a programmer has to supply three things: (i) An optimization pattern. The pattern \ndescribes where IOpt should attempt to apply the optimization; (ii) The optimizations analysis. The analysis \ndetermines if IOpt can safely apply the optimization; and  (iii) The optimizations rewriting. The rewriting \nchanges the programs source code to apply the optimization. To write the optimization pattern, the programmer \nwrites a syntactic pattern in the form we have described in Section   4.1. IOpt will search the program \nfor matches of the pattern. To write the analysis, the programmer writes a guard ex\u00adpression in the extended \nreason analysis language of Sec\u00adtion 4.2. While writing the analysis, the programmer must be careful \nto avoid encoding implicit knowledge into his anal\u00adysis IOpt will only produce meaningful guidance if \nthe programmer avoids the problem described in Section 4.2.2 For the rewriting, the programmer again \nwrites a syntactic pattern in the form we have described in Section 4.1. When\u00adever IOpt .nds an match \nof an optimization pattern where the optimizations analysis succeeds, it applies the rewriting. To apply \nthe rewriting, IOpt takes all the variables matched by the optimization pattern and inserts them into \nthe rewrit\u00ading pattern. IOpt then replaces the part of the program that matched the optimization pattern \nby the expanded rewriting pattern. 6. Evaluation We evaluate IOpt in two ways. First, we evaluate the \nheuris\u00adtics of our system by showing that they are effective in guid\u00ading programmers. Second, we present \ncase studies that use use IOpt to optimize standard benchmarks. To enable us to evaluate IOpt, we implemented \nmany interprocedural optimizations and analyses in IOpt. These optimizations range from standard compiler \noptimizations (such as constant propagation) to complex optimizations (such as loop invariant code motion \nfor method invocations). In total, we implemented over 25 different optimizations and analyses. All of \nour optimizations use interprocedural analyses and are therefore more powerful than their counterparts \nin typical compilers. Indeed, many of these optimizations are inapplicable if we consider only the actual \nsemantics of the program. For example, our loop-invariant code motion can hoist invocations of the method \nsize() on arbitrary subtypes of List. This optimization is rarely safe according to the actual semantics \nwhich must consider the possibility of dynamic classloading and precise exceptions. We evaluate IOpt \nusing (i) selected benchmarks from the SPECjvm98 suite [8], (ii) the SpecJBB benchmark [7], and (iii) \na network simulator [2]. Since these studies re\u00adquired signi.cant manual effort to check and classify \nthe results from IOpt, we picked only two benchmarks from SPECjvm98: jess and db. Many of the SPECjvm98 \nbench\u00admarks ship only in bytecode form; since IOpt needs the source code to interact with the programmer, \nIOpt cannot optimize them. Figure 2: Number of opportunities with a given risk score 6.1 Heuristics \nMany optimization opportunities identi.ed by IOpt require signi.cant amount of speci.cation effort. Some \noptimization opportunities have huge reasons and explanations containing thousands of root reasons. Clearly, \nasking the user to place a thousand assumptions is inacceptable. In this section we show that IOpt s \nheuristics are effective in identifying op\u00adportunities with a high cost-bene.t ratio. For this evaluation \nwe present data for the loop-invariant code motion (LICM) optimization implemented in IOpt. Re\u00adsults \nfor other optimizations are similar and thus we omit them. 6.1.1 Is it useful to rank optimization opportunities \nby risk? Figure 2 shows the distribution of risk scores across opti\u00admization opportunities. A point (x, \ny) says that y opportuni\u00adties have risk x (higher risk numbers indicate higher risk). To determine whether \nor not IOpt s risk ranking was accurate, we manually inspected all the opportunities. We found: All \nopportunities that IOpt ranked as low risk (score = 10000) were actually resolvable.  All opportunities \nthat IOpt ranked as high risk (score > 1000000) were either obviously unresolvable or were so hard to \nvalidate that we could not conclude whether they were resolvable or not.  Most opportunities that IOpt \nranked as medium risk (10000 < score = 1000000) were resolvable but some were not.  In summary, IOpt \ns rank-by-risk heuristic is effective at least for our benchmark programs: when an opportunity has a \nlow risk score it consistently turned out to be valid: i.e., a programmer could provide the assumptions \nto enable the optimization. Moreover, from Figure 2 we see that there are many more high-risk opportunities \nthan low-risk opportuni\u00adFigure 3: Number of reasons for a given number of recur\u00adrences  ties. Thus, \nrank-by-risk also helps to whittle down the set of opportunities that the programmer should even consider. \n 6.1.2 Is it useful to rank optimization opportunities by commonality? For ranking by commonality to \nbe useful, reasons must sat\u00adisfy two requirements (i) there are many reasons that reoccur in different \noptimizations; and (ii) these reoccurring reasons are not obvious (i.e., a programmer would not immediately \nidentify them as reoccurring reasons) and thus there is a ben\u00ade.t to having IOpt do this automatically. \nFigure 3 addresses the .rst requirement. It has one set of points per benchmark. In this Figure, a mark \nat (x, y) means that there are y reasons that reoccur in x optimiza\u00adtion opportunities. For example, \na mark at (100, 500) means that there are 500 different reasons that reoccur in 100 op\u00adtimization opportunities \neach. From the .gure we see that while there are many reasons that do not reoccur, there are some reasons \nthat reoccur in many optimization opportuni\u00adties. Thus, there is bene.t to ranking reasons (and thus \nopti\u00admizations) by commonality since it can signi.cantly reduce the programmer s burden. To address the \nsecond requirement we resorted to man\u00adual inspection of reasons. We found that there were no obvi\u00adous \npatterns that set apart the frequently reoccurring reasons from the reasons that did not reoccur. Thus, \nautomatically classifying reasons by their commonality provides informa\u00adtion that is not otherwise easily \navailable to the programmer.  6.2 Case Studies We now evaluate IOpt by optimizing two programs. We use \nour system to propose assumptions. We manually verify that the assumptions are correct and approve them. \nAfter we have added enough assumptions to enable a few high-payoff optimization opportunities, we apply \nthe optimizations. We apply this case study to two applications: A network simulator for research on \ncognitive radio networks [2], and jess, a program from the SpecJVM benchmark suite [8]. Our optimizations \nspeed up our programs by more than a factor of two.  6.3 Network Simulator We applied our system to \na simulator for cognitive radio networks. Originally the simulator was too slow to simulate large networks. \nWe used IOpt to optimize parts of the sim\u00adulator. We applied three optimizations variants of strength \nreduction, precomputation and loop invariant code motion. All of these optimizations exploit IOpt s interactiveness \nby being very aggressive: no semantics-preserving optimizer for Java could ever apply any of them. In \ntotal, we spent about 30 minutes working with IOpt to apply the optimiza\u00adtions we presented. Since we \nwere unfamiliar with the net\u00adwork simulator, we spent by far most of the time validating the assumptions \nthat IOpt proposed. Table 2 gives detailed results for each optimization. In Table 2, the Baseline row \ngives the runtime of the unopti\u00admized network simulator for three input sizes. For each in\u00adput size, \nwe show the simulators run time both in seconds and as percent of the baseline runtime. The Strength \nRe\u00adduction , Precomputation and LICM rows give the sim\u00adulators run time for each input size after we \napplied each optimization in separation. The row Cumulative gives the simulators run time after applying \nall optimizations. Strength Reduction. First, we applied an aggressive ver\u00adsion of strength reduction. \nIt replaces method invocations by less-complicated computations. The most important opportunity identi.ed \nby our system was to replace an expensive call to exponentiation (to com\u00adpute a square) with multiplication. \nIOpt determined three reasons that prevented this optimization. First, the exponen\u00adtiation function might \ndepend on or modify the state of the program. Second, the execution of the exponentiation func\u00adtion might \nthrow an exception. Third, the user needed to as\u00adsure our system that the computational result of the \nexpo\u00adnentiation function was the same as the result of the intended replacement. We added the recommended \nassumptions for each of the problems. First, we added an assumption that the exponen\u00adtiation function \nneither depends on nor modi.es program state. We did this because we expected that pure functions in \nthe library do not modify our programs state. Second, we added the assumption that the exponentiation \noperator will not throw exceptions. Again, this matches how a user per\u00adceives this operator. Third, we \nadded the assumption that the multiplication indeed performs the same operation as the corresponding \nexponentiation operator. After adding these assumptions, our system determined that the optimization \nwas applicable. Applying the optimization reduced the sim\u00adulators runtime by 27.5%. Precomputation. Second, \nwe applied an aggressive pre\u00adcomputation for parts of methods. This is helpful in case a method gets \ninvoked often.  Time (s)/Input Size (nodes) Optimization applied 75 150 300 Baseline Strength Reduction \nPrecomputation LICM 1.70 1.54 0.76 1.46 100.0% 90.5% 44.7% 85.9% 6.65 5.53 3.10 5.35 100.0% 83.2% 46.6% \n80.5% 25.86 18.74 10.27 20.56 100.0% 72.5% 39.7% 79.5% Cumulative 0.59 34.7% 2.07 31.1% 5.94 22.96% \nTable 2: Performance of the Network Simulator For this optimization, our system identi.ed many opti\u00admization \nopportunities. However, our heuristics determined that most of opportunities were unlikely to be resolvable. \nFurther, our heuristics determined that many of remaining opportunities were prevented by the same failure \ncauses. The most common failure cause was that potentially redun\u00addant calls to library class could throw \nexceptions. As the simulator was written under the assumption that no excep\u00adtions would be thrown, we \nresolved this issue by placing an assumption that speci.es that our program does not care whether library \nclasses throw exceptions. After reapplying the optimization, our system determined that many optimization \nopportunities failed because library routines might modify or use state within the simulator. We added \nassumptions that resolved these failures. After that, our system indicates that a large number of optimization \nop\u00adportunities were applicable. On applying these optimizations we obtained a runtime reduction by 55-60%. \nLoop Invariant Code Motion. Third, we applied an ag\u00adgressive version of loop invariant code motion. This \nopti\u00admization extracts .eld dereferences and invocations of the method List::size() from loops. For this \noptimization, our system identi.ed 224 optimiza\u00adtion opportunities. Of these, our system estimated that \n84 were likely to be resolvable. The .rst issue identi.ed by our system was that many optimization opportunities \nsuffered from the presence of dynamic classloading and exceptions potentially thrown by class libraries. \nAs both of these fea\u00adtures can lead to undetermined code being executed they af\u00adfect almost all instances \nof our optimization. Since the sim\u00adulator does not use dynamic classloading and does not rely on exceptions, \nwe added assumptions that forced our system to ignore both of these issues. Unfortunately, these assumptions \ndid not enable any op\u00adtimization opportunities. Some optimization opportunities failed due to of intervening \ninvocations of List::get(int) and Collection.size() because our system assumed that these methods had \nthe potential to modify .elds. Thus, we added assumptions that get and size do not modify state relevant \nto our program. As a consequence of these assump\u00adtions, our system was now able to move 42 statements \nout of loops. After this, the next set of optimization oppor\u00adtunities that were inapplicable failed because \nour system could not determine that methods from java.util.Math and java.io.PrintStream would not modify \nvariables within our program. Again, we added corresponding as\u00adsumptions, thus enabling 10 additional \noptimization oppor\u00adtunities. After this, the bene.t of adding assumptions dimin\u00adished signi.cantly. To \nmake the next optimization candidate applicable, we had to add 5 assumptions to enable 10 ad\u00additional \noptimization opportunities. Subsequent addition of assumptions only resulted in 1 or 2 enabled opportunities \nfor 3 assumptions. At this point, we stopped adding addi\u00adtional assumptions because of the diminishing \nreturns, and because the assumptions needed to resolve further issues all required signi.cant knowledge \nof the program. We applied the applicable optimization opportunities, resulting in a de\u00adcrease of the \nprograms runtime by 14% to 20% (depending on the input).  6.4 jess The second program we applied our \nsystem to was jess from SpecJVM98 [8]. We chose this program since it is one of the most realistic programs \nin SpecJVM98 to come with source code. Again, we used IOpt s abilities to apply aggressive optimizations \nto the program: none of the optimizations we applied to jess would have been legal with a traditional \ncompiler optimizer. In total, we spent about 15 minutes working with IOpt to apply the optimizations \nwe presented. This time is signi.cantly lower than the time we invested into optimizing the network simulator \nbecause we were far more familiar with jess code. Table 3 shows the overall results , the Baseline row \ngives the runtime of jess for three input sizes. For each input size, we show jess s run time both in \nseconds and as percent of the baseline runtime. The PRE , Remove Rewrite , and LICM rows give the run \ntime for each input size after we applied each optimization individually. The row Cumulative gives the \njess s run time after applying all optimizations. Partial Redundancy Elimination. First, we applied an \naggressive implementation of partial redundancy elimina\u00adtion (PRE) to jess. This implementation of PRE \neliminates partially-redundant object creations and thus reduces mem\u00adory pressure. For this optimization, \nour system identi.ed many opti\u00admization opportunities with a low risk. IOpt used the com\u00admonality heuristic \nto recommend a group of four optimiza\u00adtion opportunities as starting point for our optimization ef\u00adforts. \nThese four opportunities had in common that they all created a new Token object and invoked a test method \n(runTestsVaryRight) on this object. Depending on the re\u00adsult of the test, the object would either be \ndiscarded or used. To enable these optimizations, our system asked us to place a large number of assumptions. \nFirst, it required assump\u00adtions to prohibit dynamic classloading and precise excep\u00adtions. Then, we had \nto place 13 further assumptions. These assumptions all assured our system that the test-functions jess \ninvokes do not use or modify state that is relevant to the creation of the token object we are moving \n(e.g., we had to assure IOpt that methods such as get do not modify program state relevant to the tests). \nAfter adding these assumptions, we enabled the four optimizations and decided to stop.  When we applied \nthe optimization, jess s runtime reduced by 75%. Remove Rewriting. Second, we applied an optimization \nthat eliminates useless membership checks before a remove operation. Speci.cally, this optimization checks \nif a program invokes remove on a container only if the item to be re\u00admoved is already in the container \n(determined using a call to indexOf). This check turns out to be redundant because remove also performs \nthe same check. Our system identi\u00ad.es two instances of this optimization within jess. After we added \nassumptions that the calls to indexOf have no side effects, the optimizations become applicable, leading \nto a re\u00adduction of run-time between 1% and 3%. Loop Invariant Code Motion. Third, we applied the aggressive \nversion of the loop invariant code motion de\u00adscribed above. For jess, this optimization .nds 560 opti\u00admization \nopportunities of which IOpt classi.es 188 as re\u00adsolvable. After adding assumptions that disallowed any \ndy\u00adnamic classloading and exceptions thrown from the class li\u00adbrary, many optimization opportunities \nfailed because our system inferred that the createSuccessor method within the jess benchmark may modify \nstate within our program by route of several library routines invoked by it. We added an assumption that \neliminated this problem, yielding 19 ap\u00adplicable optimization opportunities. We then added assump\u00adtions \ndenoting that the elementAt, add and size opera\u00adtion of the Vector class did not modify state within \nour user code. This led to 15 further applicable optimization opportu\u00adnities. A further 12 optimization \nopportunities were enabled by assuring our system that the methods Value.equals and Value.numericValue \ndid not modify state themselves. Af\u00adter this, we stopped adding further assumptions as the ex\u00adpected \nbene.t diminished. We instead applied the optimiza\u00adtion for a reduction of run-time between 2.7% and \n4%.  6.5 Conclusion We have demonstrated that our system can interact with a programmer to apply specialized \nand aggressive optimiza\u00adtions to programs. Our system identi.es instances of opti\u00admizations that are \nnot semantics preserving and guides the user towards adding assumptions that describe his intended semantics \nthat makes these optimizations applicable. Our system can correctly identify promising optimization op\u00adportunities \nand guide the programmer towards resolving the failures of optimization opportunities that we could make \nap\u00adplicable. 7. Related Work We are not aware of any prior work that enables complex optimizations by \nguiding programmers towards placing ben\u00ade.cial assumptions. Some research has been done on using assumptions \nto aid optimizers. For example, Pechtchanski and Sarkar de.ne immutability assumptions [5]. For each \nassumption, a pro\u00adgrammer can specify the lifetime, reachability and context of the immutability. In \nexperiments, they annotate a number of benchmarks and report an improvement in the runtime of the benchmarks \n(5% 10%). Similarly, Pominville et. al. [6] annotate Java programs with assumptions for nullness and \nbounds information. The optimizations that use this informa\u00adtion achieve better results (2% 10%) on \nsome benchmarks. Hummel [3] examines the impact of different types of as\u00adsumption on Java programs. They \nuse assumptions that help avoid run-time checks. For a number of handcrafted bench\u00admarks, their assumptions \nimprove performance between 9 and 25%. All these approaches assume that a programmer is willing to spend \ntime to fully annotate his program with as\u00adsumptions. In contrast, our approach quickly guides the pro\u00adgrammer \ntowards those assumptions that improve his pro\u00adgrams performance most. Also, our approach takes care \nto minimize the potential for mistakes by the programmer. Also related are interactive program transformation \nsys\u00adtems. For example, the SUIF Explorer [4] and the ParaScope environment [1] are interactive systems \nthat help program\u00admers parallelize their programs. Both systems allow pro\u00adgrammers to see which dependencies \nprevent a loop paral\u00adlelization. They then allow the programmer to disregard sin\u00adgle dependencies, thus \nexpressing his intentions. These sys\u00adtems are similar to ours in that they guide the programmer towards \nthose assumptions that he must make to improve his programs performance. However, the interactive paralleliza\u00adtion \nsystems do not guide programmers towards dependen\u00adcies that have a larger impact than others. Furthermore, \ntheir approach is limited to a single property (dependency) that affects a single class of optimization \n(parallelizations). In contrast, our approach allows optimizations to use many dif\u00adferent kinds of properties \nthat affect arbitrary optimizations. 8. Conclusion Language features, such as dynamic class loading and \nex\u00adception handling, inhibit traditional compiler optimizations. However, since programmers rarely use \nthese features to their full extent, these features needlessly inhibit optimiza\u00ad  Time (s)/Input Size \nOptimization applied 50 75 100 Baseline 15.65 100.0% 33.02 100.0% 59.27 100.0% PRE Remove Rewrite LICM \n8.03 14.78 15.27 51.3% 94.4% 97.6% 13.39 30.77 32.10 40.5% 93.2% 97.2% 22.82 57.42 56.77 38.5% 96.9% \n95.8% Cumulative 7.39 47.2% 13.44 40.7% 21.21 35.8% Table 3: Performance of jess tions. If an optimizer \nknew what the programmer actually intended (rather than what the programming language dic\u00adtates) it could \napply aggressive optimizations. We show how an optimizer can interact with the user to obtain the intended \nsemantics. Our system assists a programmer in placing assumptions that aid optimizations. By ranking \noptimization opportuni\u00adties and assumptions with a cost/bene.t metric, our sys\u00adtem guides the programmer \ntowards assumptions that allow a signi.cant progress towards additional enabled optimiza\u00adtions while \nkeeping the speci.cation effort low. We evaluate our system by applying a number of opti\u00admizations to \nbenchmark programs. We show that our ap\u00adproach is able to speed up programs signi.cantly (by a factor \nof two) and that our ranking heuristics effectively guide pro\u00adgrammers towards the most pro.table optimizations. \nReferences [1] Keith D. Cooper, Mary W. Hall, Robert T. Hood, Ken Kennedy, Kathryn S. McKinley, John \nM. Mellor-Crummey, Linda Torc\u00adzon, and Scott K. Warren. The ParaScope parallel programming environment. \nProceedings of the IEEE, 81(2):244 263, 1993. [2] Christian Doerr, Douglas Sicker, and Dirk Grunwald. \nWhat a cognitive radio network can learn from a school of .sh. Third Annual Wireless Internet Conference \n(WICON), 2007. [3] Joseph Hummel, Ana Azevedo, David Kolson, and Alexandru Nicolau. Annotating the Java \nbytecodes in support of opti\u00admization. Concurrency: Practice and Experience, 9(11):1003 1016, 1997. [4] \nShih-Wei Liao, Amer Diwan, Robert P. Bosch Jr., Anwar M. Ghuloum, and Monica S. Lam. SUIF explorer: An \ninteractive and interprocedural parallelizer. In Principles Practice of Par\u00adallel Programming, pages \n37 48, 1999. [5] I. Pechtchanski and V. Sarkar. Immutability speci.cation and its applications, 2002. \n[6] Patrice Pominville, Feng Qian, Raja Vall\u00b4ee-Rai, Laurie Hen\u00addren, and Clark Verbrugge. A framework \nfor optimizing Java using attributes. In R. Wilhelm, editor, CC 2001, volume 2027 of Lecture Notes in \nComputer Science, pages 334+, 2001. [7] Standard performance evaluation corporation. SPECjbb2000 (Java \nBusiness Benchmark). http://www.spec.org/jbb2000. [8] Standard performance evaluation corporation. SPECjvm98 \nbenchmarks. http://www.spec.org/jvm98. [9] Daniel von Dincklage and Amer Diwan. Explaining failures of \nprogram analyses. In PLDI 08: Proceedings of the 2008 ACM SIGPLAN conference on Programming language \ndesign and implementation, pages 260 269, New York, NY, USA, 2008. ACM.   \n\t\t\t", "proc_id": "1640089", "abstract": "<p>Modern object-oriented languages have complex features that cause programmers to overspecify their programs. This overspecification hinders automatic optimizers, since they must preserve the overspecified semantics. If an optimizer knew which semantics the programmer intended, it could do a better job.</p> <p>Making a programmer clarify his intentions by placing assumptions into the program is rarely practical. This is because the programmer does not know which parts of the programs' overspecified semantics hinder the optimizer. Therefore, the programmer has to guess which assumption to add. Since the programmer can add many different assumptions to a large program, he will need to place many such assumptions before he guesses right and helps the optimizer.</p> <p>We present IOpt, a practical optimizer that uses a specification of the programmers' intended semantics to enable additional optimizations. That way, our optimizer can significantly improve the performance of a program. We present case studies in which we use IOpt to speed up two programs by a factor of 2.</p> <p>To make specifying the intended semantics practical, IOpt communicates with the programmer. IOpt identifies which assumptions the programmer <i>should</i> place, and where he should place them. IOpt ranks each assumption by (i) the likelyhood that the assumption conforms to the programmers' intended semantics and (ii) how much the assumption will help IOpt improve the programs' performance. IOpt proposes ranked assumptions to the programmer, who just picks those that conform to his intended semantics.With this approach, IOpt keeps the programmers' specification burden low. In our case studies, programmers had to add just a few assumptions to realize significant performance speedups.</p>", "authors": [{"name": "Daniel von Dincklage", "author_profile_id": "81100459374", "affiliation": "Google, Inc., Mountain View, CA, USA", "person_id": "P1728797", "email_address": "", "orcid_id": ""}, {"name": "Amer Diwan", "author_profile_id": "81100202872", "affiliation": "University of Colorado, Boulder, CO, USA", "person_id": "P1728798", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1640089.1640120", "year": "2009", "article_id": "1640120", "conference": "OOPSLA", "title": "Optimizing programs with intended semantics", "url": "http://dl.acm.org/citation.cfm?id=1640120"}