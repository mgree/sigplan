{"article_publication_date": "10-25-2009", "fulltext": "\n Demystifying Model Transformations: An Approach Based on Automated Rule Inference Mangala Gowri Nanda \nSenthil Mani Vibha Singhal Sinha Saurabh Sinha IBM Research, India {mgowri,sentmani,vibha.sinha,saurabhsinha}@in.ibm.com \nAbstract Model-driven development (MDD) is widely used to de\u00advelop modern business applications. MDD \ninvolves creating models at different levels of abstractions. Starting with mod\u00adels of domain concepts, \nthese abstractions are successively re.ned, using transforms, to design-level models and, even\u00adtually, \ncode-level artifacts. Although many tools exist that support transform creation and veri.cation, tools \nthat help users in understanding and using transforms are rare. In this paper, we present an approach \nfor assisting users in under\u00adstanding model transformations and debugging their input models. We use \nautomated program-analysis techniques to analyze the transform code and compute constraints under which \na transformation may fail or be incomplete. These code-level constraints are mapped to the input model \nele\u00adments to generate model-level rules. The rules can be used to validate whether an input model violates \ntransform con\u00adstraints, and to support general user queries about a transfor\u00admation. We have implemented \nthe analysis in a tool called XYLEM. We present empirical results, which indicate that (1) our approach \ncan be effective in inferring useful rules, and (2) the rules let users ef.ciently diagnose a failing \ntrans\u00adformation without examining the transform source code. Categories and Subject Descriptors D.2.5 \n[Software Engi\u00adneering]: Testing and Debugging Debugging aids; F.3.2 [Logics and Meanings of Programs]: \nSemantics of Program\u00adming Languages Program analysis General Terms Algorithms, Experimentation Keywords \nModel-driven development, model-to-model transform, model validation, transformation comprehension, precondition \nanalysis Permission to make digital or hard copies of all or part of this work for personal or classroom \nuse is granted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. OOPSLA \n2009, October 25 29, 2009, Orlando, Florida, USA. Copyright c &#38;#169; 2009 ACM 978-1-60558-734-9/09/10. \n. . $10.00 1. Introduction Model-driven development (MDD) is a paradigm of soft\u00adware development that \nis based on the use of software mod\u00adeling as the primary form of expression [10, 24]. It enables application \ndesign in terms of high-level domain concepts (from the problem space) instead of low-level program\u00adming \nconcepts. A model is speci.ed in a well-de.ned no\u00adtation referred to as the metamodel. A model instance \nde\u00adscribes an actual system and conforms to the grammar of the metamodel. A transform1 takes an instance \nof a model and converts it into another model (model to model transforma\u00adtion) or into code (model to \ncode transformation). Typically, projects that follow an MDD methodology create a series of models at \nvarious levels of abstractions that are suc\u00adcessively re.ned before actual code is created. As an ex\u00adample, \na business analyst outlines the application work-.ow as process models, which are captured in notations \nsuch as UML. The UML models are used to generate code skeletons to which developers add application logic. \nThe conversion of a high-level model to a lower-level model or code can be done either manually or using \nautomated transforms. There are many factors that determine the effectiveness and ef.ciency of MDD. First, \ngiven a model transform, it must be veri.ed that, for a valid input model, it generates the correct output \nmodel. To address this problem, existing research has developed many techniques for verifying and validating \nmodel transforms (e.g., [1, 3, 9, 12, 16, 20]). Sec\u00adond, even if a transform is correct, the user of \nthe transform might have to spend signi.cant effort in creating a valid in\u00adput model that does not violate \ntransform assumptions. Un\u00adlike the .rst problem, the second problem has mostly been overlooked; this \nis the problem that we address in this paper. 1.1 Illustration of the Problem Consider the ECORE2 metamodel \nshown in Figure 1(a). The metamodel declares an element type Attribute, which has 1 In this paper, we \nfollow the terminology introduced by Baxter [2]. A transform is a function, or a program, that maps one \nmodel to another model (or text). A transformation is the application, or the execution, of a transform \non a model instance. 2 http://www.eclipse.org/modeling/emf  two features name and type neither of which \nis manda\u00adtory, as indicated by lowerBound = 0. Figure 1(b) presents an example model based on this metamodel. \nA Java trans\u00adform is used to convert this model to a UML class; an excerpt of the code is shown in Figure \n1(c). A user of this trans\u00adform could potentially face the following problems while executing the transform \nwith the input model illustrated in Figure 1(b). EXAMPLE 1. Transformation failure. The transformation \nfails with a null-pointer exception at line 5, leaving the transform user with the task of going over \nthe transform code to debug the failure. Going backwards from line 5, we see that type src is assigned \nfrom attr.getType() at line 4 and attr is assigned from source at line 1. There\u00adfore, by manual inspection \nof the code the user can infer that source.getType() should not be null. Further, based on knowledge \nof the framework used to create the trans\u00adform, it is possible to infer that execute() is invoked only \nfor model object instances of type Attribute. Therefore, the actual violated rule is that Attribute.getType() \nshould not be null. The user .xes the problem by adding type = \"\" to the model, as shown in Figure 1(d). \nMoreover, to avoid this failure again with other input models, the user can add a rule to the metamodel \nto specify that type is a mandatory fea\u00adture. For this example, the constraint is speci.ed by adding \nlowerBound = 1 to the type feature in the metamodel, as shown in Figure 1(f). D EXAMPLE 2. Incomplete \noutput model. After the input model is .xed, the transformation runs to completion. How\u00adever, in the \ngenerated output model, the UML property in\u00adstance for the .rst Attribute instance does not have the \ntype attribute. Once again, by manually inspecting the source code, the user determines that prop.setType() \non line 7 generates the type attribute on the output Property instance but only if the condition on line \n5 evaluates true. Thus, the user infers the rule Attribute.getType().equals(\"String\") . UMLUtilities.findType(...) \nnull = Applying this rule, the user modi.es the Attribute instance to have type = \"String\" as shown \nin Figure 1(e). D As illustrated in these examples, the user has to inspect manually the transform source \ncode to identify the violated constraints that cause the transformation to fail or be in\u00adcomplete. However, \nthe source code of a transform is of\u00adten not available for inspection by the transform user; or, if available, \nthe users would prefer not to examine the code (which is usually written by someone else). Therefore, \na debugging technique that supports users in understanding why a transformation failed or generated an \nincomplete out\u00adput model, without requiring them to examine the transform source code, would be very \nuseful. Depending on the metamodel used to specify the input model, some of the simpler constraints could \nbe speci.ed (a) Example metamodel de.nition <eClassifiers xsi:type=\"ecore:EClass\" name=\"Attribute\"> \n<eStructuralFeatures name=\"name\" lowerBound=\"0\" .../> <eStructuralFeatures name=\"type\" lowerBound=\"0\" \n... /> </eClassifiers> (b) A model de.ned using the metamodel <Model> 1: <Attribute name=\"id\" /> 2: \n<Attribute name=\"name\" type=\"String\"/> </Model>  (c) A Java transform for the metamodel public void \nexecute( EObject source, EObject target ) { 1. Attribute attr = (Attribute)source; 2. Property prop \n= (Property)target; 3. PrimitiveType ptype = null; 4. String type_src = attr.getType(); 5. if (type_src.equals(\"String\")) \n 6. ptype = UMLUtilities.findType(...); 7. if (ptype != null) prop.setType(ptype); }  (d) First correction \nto the model <Model> 1: <Attribute name=\"id\" type=\"\" /> 2: <Attribute name=\"name\" type=\"String\"/> </Model> \n (e) Second correction to the model <Model> 1: <Attribute name=\"id\" type=\"String\" /> 2: <Attribute name=\"name\" \ntype=\"String\" /> </Model>  (f) Enhanced metamodel <eClassifiers xsi:type=\"ecore:EClass\" name=\"Attribute\"> \n<eStructuralFeatures name=\"name\" lowerBound=\"0\" .../> <eStructuralFeatures name=\"type\" lowerBound=\"1\" \n... /> </eClassifiers> Figure 1. An example metamodel, model, and Java transform. in the metamodel de.nition \nby the transform author. More complex constraints could be documented in plain text or using a constraint \nlanguage, such as OCL.3 However, in ei\u00adther case, it is up to the transform author to maintain the constraints \nmanually; a manual approach can cause the con\u00adstraints to be incomplete, incorrect, and become outdated \nas the transform code evolves. Therefore, automated inference of transform constraints and mapping of \nconstraints to rules is essential for developing a practical and effective debug\u00adging technique. 1.2 \nOverview of our Solution In this paper, we present a new approach for assisting users in diagnosing the \ncause of a failed or an incomplete transfor\u00admation. Overall, our goals are to support Model validation. \nWe infer rules that state the conditions on the input model under which a transform fails; this addresses \nthe problem illustrated in Example 1.  Transform comprehension. We infer rules that state the conditions \non input models under which a transform gen\u00aderates an incomplete output model; this addresses the problem \nillustrated in Example 2.  3 http://www.omg.org/technology/documents/formal/ocl.htm  Figure 2. Overview \nof our solution. Our approach consists of three steps; Figure 2 presents an overview of the steps. In \nthe .rst step, we analyze the transform code to extract automatically exception constraints, which can \ncause the transform to terminate with a runtime exception, and output constraints, which can cause the \ntransform to generate an output model element. Because we are interested in only those constraints that \ncan be mapped back into the input or output model, we .rst identify the variables in the transform code \nthat map to the input and output model. For example in Figure 1(c) in the method: public void execute(EObject \nsource, EObject target) source and target map to root elements in the input and output model, respectively. \nThese entry points are manually identi.ed and provided as inputs to our analysis. At a potential exception-generating \nstatement in the transform code, we identify the postconditions from which we derive the exception constraints. \nFor example, at line 5 in Figure 1(c), we generate the postcondition (type src = null), which is the \ncondition under which a null-pointer exception will occur. Starting from this postcondition, we compute \nweakest preconditions using a backward, interpro\u00adcedural analysis. The preconditions generated are (source \n= null).(source.getType() = null) Finally, we check that the preconditions are rooted in the input model \nelement (source in this example) before accepting them as valid exception constraints. At each code point \nwhere the output model data structure is modi.ed, we identify the postconditions from which we derive \nthe output constraints. In Figure 1(c), for example, at line 7, we generate the postcondition (ptype \nnull), = which is the condition under which prop.setType(ptype) executes. Applying pointer and escape \nanalysis [21], we further determine that prop.setType(ptype) writes to prop.type, which is an alias of \ntarget.type. Thus, a post\u00adcondition for an output constraint is the condition under which the transform \ncode writes to an access path that is rooted in the target. The preconditions generated are (source.getType().equals(\"String\")) \n. (UMLUtilities.findType(... ) =null) In the case of output constraints, we accept all precon\u00additions \nthat are rooted in input or output model elements (source, target respectively in this example). Preconditions \ncontaining other method calls with parameters that are in turn rooted in source, target, are also accepted. \nAll other constraints get .ltered out. In practice, we also take as input a set of user-provided .lters \nto remove unin\u00adteresting constraints. Constraints that do not fall into this pattern may be reported \nto the transform author as potential bugs in the transform code. In the second step, we map the code-level \nconstraints to metamodel-level rules. This is possible as all the constraints are rooted at the source \nor target elements, which are the elements that have been identi.ed in step 1 as entry points into the \ninput or output models. The mapping step basically raises the abstraction level of the code-level constraints \nso that they are stated in the language of the input metamodel and, therefore, are easier for the transform \nuser to compre\u00adhend. A user-provided mapping .le is used to translate the constraints to rules: exception \nconstraints are mapped to val\u00adidation rules, whereas output constraints are mapped to com\u00adprehension \nrules. Depending on the framework being used to write the transform, the generation of the mapping .le \ncan be automated to varying degrees. In the third step, the metamodel-level rules may be used to construct \na validity checker and a querying tool. The va\u00adlidity checker, given an input model for a transform, \nchecks whether a model violates any of the validation rules. The querying tool can help the user understand \nthe conditions on the input model elements under which an output model el\u00adement is created; thus, the \nuser can diagnose the cause of missing output-model elements. Assumptions and Requirements Transforms \ncan map a model to another model or to text (e.g., code). They can be implemented in an imperative manner \nor using declarative or rule-based languages. Our approach is applicable only to model-to-model transforms \nimplemented in an imperative style. Our current implementation analyzes transforms writ\u00adten in the Java \nlanguage. Our analysis works on the assump\u00adtion that the entire input (and output) is captured in a single \ndata structure that is mapped to the input (output) model. The input / output object may be passed in \nas a parameter or be constructed within the transform. Our approach requires the identi.cation of failure \npoints in the code (for inferring exception constraints), and points at which output model elements are \ngenerated (for inferring output constraints). For Java, these program points can be identi.ed automatically \nby identifying instructions that can throw runtime exceptions, such as NullPointerException, or instructions \nthat de.ne the output object.  1.3 Contributions The main bene.t of our approach is that it provides \nauto\u00admated support for diagnosing the cause of a failing or an in\u00adcomplete transformation, without requiring \nan examination of the transform code. Our analysis is similar to the computation of weakest preconditions \n(e.g., [6, 8]). However, we apply the analysis to the domain of MDD, in which the inferred constraints \nare mapped to metamodel-level rules. Alternatively, such rules could be provided manually by the transform \ndeveloper and used to annotate the model elements using a constraint\u00adspeci.cation language. These rules \ncould then be used to construct validity checkers. However, manual computation of rules may be time-consuming \nand error-prone [5]. Another important aspect of our work is the mapping of code-level constraints to \nmetamodel-level rules, which has been recognized as an important feature affecting the usability of automatically \ninferred constraints [5]. Our tool partially automates these tasks, thereby reducing the burden on transform \ndevelopers. We implemented the solution for Java transforms and per\u00adformed empirical evaluation using \nreal applications. Our re\u00adsults indicate that, for the subjects considered, our approach can infer a \nsigni.cant number of useful rules. To validate this, we conducted a user study, in which we compared \nthe ef.ciency of users in identifying and .xing problems with incorrect input models to a transform. \nIn the study, all the users performed the debugging tasks much faster when they were guided by the inferred \nrules than when they were not. The main contributions of the paper are The presentation of a static-analysis-based \napproach for inferring rules from model-to-model transforms and ap\u00adplying the rules to support model \nvalidation and transfor\u00admation comprehension  An implementation of the approach for transforms writ\u00adteninJava \nandmodels speci.edin EMF2  Results of empirical studies, conducted using different types of models and \ntransforms, that illustrate the bene.ts of the approach  The rest of the paper is organized as follows. \nIn the next section, we introduce an example transform that we use to describe our solution in the remaining \nsections. In the subsequent three sections, we present the steps of our approach: constraint inference, \nrule generation, and valida\u00adtion/comprehension. Section 6 presents the empirical evalu\u00adation of our work. \nSection 7 discusses related work; .nally, Section 8 summarizes the paper and lists directions for fu\u00adture \nresearch. E = {DataModel, Artifact, ContextArtifact, Attribute, Annotation, ThisPackage}R = {C: contains, \nI: inheritsFrom}P = {name, type, value, multiplicity, isSimple}dr : . DataModel .{((C,Artifact),artifacts,many), \n((C,ContextArtifact),contextArtifacts,many), Artifact . . {((C,Attribute),attributes,many)}ContextArtifact \n. . {((C,Attribute),attributes,many)}Attribute . . {((C,Annotation),annotations,many)}Annotation . \n .\u00d8 ThisPackage .  .{((I,Annotation),NA, NA)}dp : . DataModel .{name} Artifact . . {name} ContextArtifact \n.  . {name} Attribute .  . {name, type, isSimple, multiplicity}Annotation . . {name, value} ThisPackage \n.   .{name, value}et = DataModel r Figure 3. The input metamodel for INFOTRANS.  2. De.nitions and \nExample In this section, we present de.nitions and introduce an exam\u00adple model transform that we use \nto illustrate our approach. 2.1 Example To illustrate the concepts described in this paper, we use an \napplication called INFOTRANS4 that takes as input a domain-speci.c information model, converts the model \nto a database schema, and creates a set of services that let users interact with the data. A domain subject-matter \nexpert is ex\u00adpected to provide the input information model using a pre\u00adde.ned metamodel. INFOTRANS converts \nthe information model to a UML class model, using the model-to-model transformation framework provided \nby the Rational Soft\u00adware Architect (RSA).5 Next, the class model is converted, using model-to-text transforms, \nto create a database-schema de.nition .le and Java classes that implement the data ser\u00advices. For the \npurpose of this paper, we focus only on the model-to-model transformation part of INFOTRANS. 2.2 Metamodels \nand Models A metamodel describes the structure or the abstract syntax of a model in terms of the types \nof elements and relations that the model may be constructed from. DEFINITION 1. (Metamodel) A metamodel \nM is a tuple t (E, R, P,dr,dp,e). E a set of element types. R isaset of r relation types. P is a set \nof properties. dr : E. (R\u00d7 E, String, cardinality) maps an element type to its related element types, \nwhere String is used as the representative name to declare the relationship; cardinality ( one , many \n, or NA ) represents the allowed number of related element instances of that type. dp : E. P(P) maps \nan element t type to its associated properties. e.E is the unique root r element type. 4 INFOTRANSisamodi.edversion \nofarealapplication thatwasdeveloped in the context of a project at IBM Research. 5 http://www-01.ibm.com/software/awdtools/architect/swarchitect \n  Figure 5. Mapping of some of the input model elements to output model elements (created by the transform \nfor INFOTRANS). <DataModel> <artifacts name=\"Gap\"> <attributes name=\"name\" isSimple=\"true\" type=\"String\"/> \n<attributes name=\"resolution\" isSimple=\"false\" type=\"GapResolution\"> <ThisAnnotation name=\"current\"/> \n</attributes> </artifacts> <contextArtifacts name=\"GapResolution\"> <attributes name=\"description\" isSimple=\"true\" \ntype=\"String\"/> </contextArtifacts> </DataModel> Figure 4. An input model (an instance of the metamodel \nshown in Figure 3) for INFOTRANS. Examples of some commonly used schema languages for 2 de.ning metamodels \ninclude ECORE, XSD,6 and MOF.7 These languages serve the purpose of specifying the syntax of models, \nand thus, are analogous to language grammars that de.ne the syntax of programming languages. Figure 3 \nshows the INFOTRANS input metamodel.8 Ar\u00adtifact, Attribute etc. are examples of element types declared \nby the metamodel. The metamodel allows for two different types of relationships between object types; \ninheritance and containment. The function dr maps each element type to its relationship with other element \ntypes. The result of the map\u00adping is a 3-tuple consisting of a pair of relation type and an element type, \na name, and a cardinality. Artifact can contain multiple elements of instance Attribute and the relationship \nis identi.ed with the name attributes. Except for the rela\u00adtionship between Annotation and ThisPackage, \nall other rela\u00adtionships are containment relations. Similarly, dp maps each element type to the properties \nassociated with the element. For example, an Artifact can have a property called name.  DEFINITION 2. \n(Metamodel access path) Let (E, R, P,dr, t dp,e) be a metamodel. Let e(n) be the element with name r \nn.A metamodel access path pmodel is a sequence derived from the grammar 6 http://www.w3.org/XML/Schema \n7 http://www.omg.org/technology/documents/formal/mof.htm 8 This is a representation of the metamodel \nfor the purposes of the paper only. The actual on-.le representation varies, based on the framework used. \nFor example, the ECORE syntax would look as shown in Figure 1(a). pmodel ::= ert .p p ::= n.p | prec.p \n| n | p | c | m(params) prec ::= n.prec | (prec)+ | n params ::= param, params | param param ::= const \n| pmodel where e(n).E; p .P; ni.nj : e(nj). dr(e(ni)); n.p : p . dp(e(n)); c is an integer constant; \nand m is an external method that takes as input a list of parameters that may be constants or access \npaths. A metamodel access path is a sequence of containment relations that starts at the root element \ntype and ends at An element name: DataModel.contextArtifacts.attributes  A property type: DataModel.artifacts.attributes.isSimple \n An integer constant: DataModel.artifacts.0  An external method: DataModel.contextArtifacts.name.equals( \nCurrent ),or UMLUtilities..ndType(DataModel.contextArtifacts.name) where, equals is a standard external \nmethod from the Java API, UMLUtilities is a method from transform code that the user did not want to \nanalyze. Further, an access path can represent recursive model ele\u00adments. Suppose, for example, in the \nmetamodel of INFO-TRANS, Attributecontained an Artifact called fact. Then, the metamodel could have a \nrecursive access path that contains one or more occurrences of attributes.fact as follows: DataModel.artifacts.(attributes.fact)+ \nGiven a metamodel M,a model can be constructed by creating instances of the element types, relations, \nand prop\u00aderties speci.ed in M. Figure 4 shows an input model, an instance of the metamodel (Figure 3), \nfor INFOTRANS.The t model has DataModel as the root element (e), which, in r turn, contains instances \nof Artifact and ContextArtifact, iden\u00adti.ed by the relationship names artifacts and contextArtifacts, \nrespectively. The Artifact instance with the property name whose value is Gap further contains an element \nof type At\u00adtribute, identi.ed by the relationship name attributes.An Attribute instance has the following \nproperties: name (with value resolution), type (with value GapResolution), and isSim\u00adple (with value \nfalse). public void execute1( EObject source, EObject target ) { 1. Attribute attr = (Attribute)source; \n 2. Property prop = (Property)target; 3. PrimitiveType ptype = null; 4. org.eclipse.uml2.uml.Package \numlprimitives = 5. UMLUtilities.loadPackage(URI.createURI( 6. Constants.UML_LIBRARY)); 7. String type_src \n= attr.getType(); 8. if (attr.isIsSimple()) { 9. if (type_src.equals(\"String\")) 10. ptype = UMLUtilities.findType(umlprimitives,\"String\"); \n} 11. if (ptype != null) 12. prop.setType(ptype); } private void handleComplexType( Class cls, Iterator \nattrItr ) { 13. Attribute attr = null; 14. Property prop = null; 15. Association assoc = null; 16. \nwhile (attrItr.hasNext()) { 17. attr = (Attribute) attrItr.next(); 18. if (!attr.isIsSimple()) { 19. \nThisPackage annotation = 20. (ThisPackage)attr.getAnnotations().get(0); 21. if (annotation != null) \n{ 22. prop = UMLUtilities.findProperty(cls, attr.getName()) 23. prop.setType(cls.getType()); } public \nvoid execute2( EObject source, EObject target ) { 24. DataModel sourceModel = (DataModel)source; 25. \nModel targetModel = (Model)target; 26. Iterator artifacts = sourceModel.getArtifacts(); 27. while (artifacts.hasNext()) \n{ 28. Artifact artifact = (Artifact)artifacts.next(); 29. Class cls = UMLUtilities.findClass(artifact.getName()); \n 30. handleComplexType(cls, artifact.getAttributes()); ... } }  Examples of potential runtime exceptions: \n1. line 9 null-pointer exception if type src is null 2. line 20 class-cast exception if the .rst element \nin the list returned by getAnnotations() is not an instance of ThisPackage 3. line 20 array-index exception \nif getAnnotations() returns an empty list  Example of output statement: 4. line 12 prop.setType de.nes \nan output model element if ptype is not null, and the predicates in lines 8 and 9 evaluate true Figure \n6. Three methods from the Java transform code for INFO-TRANS; three potential runtime exceptions that \ncan occur, and the execution condition for an output statement.  2.3 Model-to-Model Transform DEFINITION \n3. (Model-to-model transform) A model-to\u00admodel transform t : MI .MO is a program that given an input \nmodel MI (an instance of metamodel MI ) generates an output model MO (an instance of metamodel MO). For \nthe INFOTRANS application, the input ECORE model is converted to a UML class model using a transform. \nFig\u00adure 5 shows the input-to-output mappings for some of the model elements. For example, for each Artifact \nin the input model, the transform creates a Class, with a set of operations and parameters, in the output \nmodel. Imperative transforms can be written in general-purpose programming languages (e.g., Java) or \nscripting languages (e.g., XSLT). Some tools, such as RSA, provide specialized transformation-authoringframeworks.The \nINFOTRANS trans\u00adform is implemented in Java using the RSA transformation\u00adauthoring framework. Figure \n6 shows the code fragments for three of the transform methods for INFOTRANS.  The bottom part of Figure \n6 lists three potential excep\u00adtions that can occur during the execution of the transform. For example, \nif in the input model, an Attributeinstance does not have a type property, attr.getType() returns null \nat line 7, which causes a null-pointer exception at the derefer\u00adence of type src at line 9. Our approach \ninfers such condi\u00adtions (validation rules) on input model elements under which the transform can fail \nwith an exception. The rules can en\u00adable a user to diagnose ef.ciently the cause of the failure (i.e., \ninvalid input model elements), without having to exam\u00adine the source code. The .gure also illustrates \nan example of conditions under which an output statement executes. Statement 12 de.nes an output model \nelement. It is reached if, in the input model, an Attribute instance has property isSimple set to true \nand prop\u00aderty type set to String. Our approach infers such conditions (comprehension rules) under which \nan output model element is generated. In the next three sections, we describe our solution in detail. \nIn Section 3, we describe how we generate code constraints by analyzing the transform code. In Section \n4, we describe how the code-level constraints are converted to metamodel-level rules. Finally, in Section \n5, we describe the usage of inferred rules for input model validation and transformation comprehension. \n 3. Step 1: Constraint Inference In Step 1 of our solution (Figure 2), we compute exception and output \nconstraints from the transform code. In this sec\u00adtion, we elaborate on the approach used to infer these \ncon\u00adstraints. The section is organized as follows. In Section 3.1, we formally de.ne exception and output \nconstraints. In Sec\u00adtion 3.2, we present the algorithm for computing exception and output constraints. \nThe analysis has been built on top of an existing tool XYLEM [22].InSection3.3, weexplain XYLEM and the \nenhancements required to generate these constraints. In the following two sections, we explain some other \nintricacies of the analysis: how our path-sensitive al\u00adgorithm overcomes the limitations of the pointer \nanalysis (Section 3.4); and how we handle recursive access paths (Section 3.5). Finally, we discuss the \nsoundness and com\u00adpleteness of our algorithm in Section 3.6. 3.1 De.nitions DEFINITION 4. (Code access \npath) A code access path pcode is a sequence derived from the grammar pcode ::= p|pcode .(p)+ p ::= vref \n|vref .p|m(params)|m(params).p params ::= param, params | param param ::= const | pcode  . ::= (pcode \nref null)|(pcode ref strConst)|(pcode,1 ref pcode,2)|(pcode ref true)|(pcode,1 int pcode,2)|(pcode int \nintConst)|(type(pcode ) type T) | true (1) (2) (3) (4) (5) (6) *(7) (8) \u00ac. (9) ref ::= = type ::= . int \n::= < |= | = Figure 7. Predicates tracked by the analysis. type(v) returns the type of a reference variable \nv. T is a set of types. Typically, in Java, a code access path is de.ned as a vari\u00adable or a variable \nfollowed by a sequence of .eld derefer\u00adences (v.f1.f2...). In our analysis, a .eld may be replaced by \na method call v.f1.mx(). This is similar to the access path de.ned by Buse [4]. However, unlike Buse \ns access paths that can have only simple methods with no parameters, in our analysis the method call \ncould also have parameters, where the parameters are either constants or access paths v1.f1.mx().my(\"const\", \nv2.mz()). The method calls can be nested to any level. An example of a code access path with method calls \nis source.getArtifacts().iterator().next().getName().equals( source.getArtifacts().iterator().next().getType()) \nIn our analysis, the code access path can also contain a set of repeated .eld dereferences or method \ncalls in the presence of recursive model elements (explained in Section 3.5). An example of a code access \npath with repeated method calls is source.(getArtifacts().iterator().next())+ .getName() A predicate \n., is a condition on one access path (unary predicate) or two (binary predicate) access paths. As shown \nin Figure 7, predicates 1 and 2 are unary reference and pred\u00adicate 3 is a binary reference predicate; \npredicate 4 is a unary boolean predicate, and 5 and 6 are binary and unary in\u00adteger predicates, respectively. \nPredicates 8 and 9 are stan\u00addard nullary and negating predicates. Predicate 7 is a special predicate \non types of variables that indicates the set of class types an access path may belong to. This is required \nto perform the analysis for class-cast exceptions (explained later in this section). A unary or a binary \npredicate may actually have multiple access paths if the predicate contains an access path with methods \nthat have multiple non-constant parameters. An example predicate is \u00ac(source.getArtifacts().iterator().next().getName().equals( \nsource.getArtifacts().iterator().next().getType()) = true) An abstract state G is a conjunction of predicates. \nDEFINITION 5. (Code constraint) Let paths(s) be the set of paths from the entry statement se of a transform \nto state\u00adment s.Let . be a predicate on a variable used at s.For a path, . . paths(s),let C(., .) be \nthe state G. at se such that if the predicates in G. at se are true, then . is true at s.Let I = {i1,...,ik},k \n= 1, be the set of input variables to the program. A code constraint CI (., s) is the disjunc\u00ad tion \nG.[I],where G.[I] contains the predicates ..paths(s) in G. with respect to the variables in I. A code \nconstraint is a formula in the Disjunctive Normal Form (DNF), where each disjunct represents one program \npath from the entry of the program to the given statement s. Each program path is in turn represented \nas a conjunct of predicates that need to evaluate to true to be able to reach statement s. Consider the \nfollowing code constraint: (source.getName().equals(source.getType())) . (source = null) . (source.getName() \n= null) . (source.getProp() = null) W \u00ac(source.getName().equals(source.getType())) . (source = null). \n(source.getName() = null). (source.getType() = null) Here, the .rst disjunct, represents the constraints \nobtained along one path, whereas the second disjunct (after the .), represents the constraints obtained \nalong another path. In general, there may be an exponential number of paths. How\u00adever, we compute only \na .xed number of paths (based on a user-speci.ed threshold) for each program point. DEFINITION 6. (Exception \nconstraint) An exception con\u00adstraint is a constraint CI(ex)(., s),where . represents the condition under \nwhich a runtime exception can occur at s in some execution of the program. An exception constraint can \nonly contain code access paths rooted in the source model element (source). More\u00adover, for method calls \nin the access path, the parameters should be rooted at the source element, be a method call, or be a \nconstant. An exception constraint whose access path does not satisfy these criteria is .ltered away. \nDEFINITION 7. (Output constraint) Let v be an output variable of a transform t.Let se be the exit statement \nof 9 t.Let rdefs(se,v) be the reaching de.nitions of v. An output constraint Cout (v) is the disjunction \n= d.rdefs(v) CI (.cd ,scd ), where d is control dependent on (scd ,L)10 and .cd is the predicate asserting \nthat the condition at scd evaluates to L , where L is either true or false. An output constraint can \ncontain code access paths rooted in the source model element (source) and target model el\u00ad 9A reaching \nde.nition de.ned for a statement variable pair (s, v) is a statement d such that d de.nes v and there \nexists a path from d to s in the program such that no statement along the path (other than d)de.nes v. \n10 A statement s is control dependent on a predicate (p, L),if there are two branches out of p such that \nby following the branch labeled L , s is de.nitely reached, whereas by following the other branch, s \nmay not be reached. algorithm TransformAnalysis input t transform output CI (ex) exception constraints \nfor t CI (out) output constraints for t global CI set of input constraints begin // Identify exception \nconstraints for null-pointer exceptions 1. foreach statement s that dereferences v do 2. . = (v = null); \nCI = \u00d8 3. ComputeConstraints(s, {.});add CI to CI (ex) // Identify exception constraints for class-cast \nexceptions  4. foreach typecast statement s : x=(T)y do 5. . = (type(x) : (subtypes(T ) .{null})); \nCI = \u00d8 6. ComputeConstraints(s, {.});add CI to CI (ex) // Identify exception constraints for array-index \nexceptions  7. foreach get statement s : c.get(intConst) do 8. . = (c = null); CI = \u00d8  9. ComputeConstraints(s, \n{.}.{(c.size = intConst)}) 10. if there is no statement on which s is directly/indirectly control dependent \nand that checks c.size then 11. add CI to CI (ex) // Identify output constraints  12. foreach output \nvariable v do 13. foreach reaching de.nition d of v at the exit of t do 14. Let d be control dependent \non (scd ,L) 15. Let . be the predicate asserting that the condition at scd evaluates to L (true or false) \n 16. ComputeConstraints(scd , {.});add CI to CI (out) // post-process  17. remove non-input-variable \npredicates from CI (ex) and CI (out)  end Figure 8. The analysis for computing exception and output \ncon\u00adstraints for a transform. ement (target). Additionally, it can contain access paths rooted in library \nmethod calls (standard JDK or user-de.ned library classes). The method parameters could, in turn, be \ncode access paths that satisfy the above criteria or be con\u00adstants.  3.2 The Algorithm Figure 8 presents \nthe algorithm TransformAnalysis to com\u00adpute input and output constraints for a given transform. The .rst \nstep in the algorithm is the computation of appropriate postconditions. Postconditions We check for three \ntypes of exceptions Null Pointer Exceptions (NPE), Class Cast Exceptions (CCE) and Array Index Exceptions \n(AIE). These might generate exception constraints. However, the only exception constraints we are interested \nin are those that occur because of problems in the input elements. Additionally, we also check for conditions \nunder which an output model element is written. These postconditions might generate an output constraint. \nThe code variables that map to the input and out\u00adput model elements are speci.ed by the user as an input \nto the analysis. For ease of exposition, all through this paper we assume that the input model maps to \na local parameter source and the output model to target. For null-pointer exceptions, at the dereference \nof a vari\u00adable x, we de.ne a postcondition (x = null) (lines 1 3) procedure ComputeConstraints input \ns statement to start backward analysis from . stating predicate at s output CI constraints on input variables \nglobal CS call stack of methods s(s, G) summary information at a call site s that maps an incoming state \nG to a set of outgoing states begin 1. initialize state G to {.}; initialize worklist with (s, G) 2. \nwhile worklist = \u00d8 do 3. remove (s, G) from worklist 4. foreach predecessor sp of s do 5. if sp is \nnot the entry and not a call then 6. compute G' for the transformation induced by sp 7. if G' is consistent \nthen 8. add (sp, G') to worklist if not visited 9. else if sp is a call that invokes M then  10. Gmx \n= map G to the exit of M 11. Gme = s(M, Gmx ) 12. if Gme = \u00d8 then // no summary exists 13. push M \nonto CS;analyze M with Gmx ; pop CS 14. Gme = states at the entry of M 15. add Gme to s(M, Gmx ) 16. \nG' = map states in Gme to sp 17. add (sp, G') to worklist if not visited 18. if CS = \u00d8 then // method \nnot being analyzed in a speci.c context 19. if this is the entry method of t then 20. CI = CI . G // \nadd path constraint to DNF 21. else foreach call site sc that calls this method do 22. G' =map G to \nsc; analyze caller starting at sc with state G'  end Figure 9. The new XYLEM analysis used to compute \nconstraints on input variables under which the given predicate . evaluates true at the given statement \ns. For class-cast exceptions, for a typecast statement x= (T)y, we de.ne a postcondition \u00ac(type(y) .{subtypes(T) \n. null}) which states that y is neither null nor of a type that can be cast to T (lines 4 6)  For array-index \nexceptions, for a statement c.get(const), we de.ne a postcondition (c = null) (lines 7 11)  For output \nconstraints, we .rst determine each statement s in the code where a .eld of target (or recursively, any \n.eld of a .eld of target) is written. If this write reaches the end of the transform (that is, the .eld \nis not overwritten), we .nd all conditionals that s is directly control dependent on. For each conditional \nC where s is control dependent on the true branch, we generate a postcondition C; similarly, for each \nconditional C where s is control dependent on the false branch, we generate a postcondition \u00acC (lines \n12 17)   After computing the postconditions, we call procedure ComputeConstraints, which starts with \nthe postcondition and executes a .x-point computation of the path-sensitive and context-sensitive analysis \nusing XYLEM. Fix-point computation and termination Given a predi\u00adcate . and an input statement s, ComputeConstraints \ncom\u00adputes the constraints on input variables under which . eval\u00aduates to true at s. The algorithm for \nComputeConstraints is presented in Figure 9. Statement State transformation (1) x= y G ' =G[x/y] (2) \n(3) (4) x= r.f ifxopy x= yopz G ' =G[x/r.f] .{(r = null)}G ' =G .{(xop y)} (true branch) G ' =G .{(\u00ac(xop \ny))} (false branch) G ' =G \\ G[x] *(5) *(6) *(7) *(8) x= new T x = (T)y x instanceof T x = r.m() (ext) \nG ' =G .{(x = null), (type(x) .{T })}G ' =G .{(type(y) . subtypes(T ))}G ' =G .{(x = null), (type(x) \n. subtypes(T ))}G ' =G[x/r.m()] (9) x = r.m() (app) G ' = s(r.m, G) .{(r = null)} Figure 10. State transformations \nat some of the statements. G represents the state following a statement; G ' represents the state preceding \na statement.  ComputeConstraints essentially starts at the given state\u00adment s and works backward along \nthe control .ow graph (CFG) and applies a state transformation on each statement. The statement is followed \nby a consistency check to en\u00adsure that no con.icting predicates (such as, (x = null) and (x = null)) \nhave been generated. In case of con.ict, that path is discarded. ComputeConstraints abstracts away arithmetic \nexpres\u00adsions, which bounds the number of predicates that can be generated from arithmetic operations. \nThe algorithm tra\u00adverses a loop until the state no longer changes from one itera\u00adtion to the next. Because, \ninteger arithmetic over the loop in\u00adduction variable is abstracted away, the analysis of a loop is bounded. \nThe presence of recursive data structures can also cause an unbounded number of predicates to be generated. \nOur approach uses the standard method of k-limiting [17] to restrict the number of access paths that \ncan be generated for recursive data structures. Section 3.5 illustrates in detail the processing of recursive \nmodel elements. State transformation The analysis uses back substitu\u00adtions to update state predicates. \nFigure 10 shows the state transformations that occur at some of the statements. The notation G[x/y] represents \nthe state with each syntactic oc\u00adcurrence of variable x replaced with y. Since we are particularly interested \nin deriving access paths that are rooted in the source or target objects, the state transformations are \ngeared towards generating ex\u00adtended access paths. Consider the state transformation at statement x = \nr.f. The updated state contains the predicates in the incoming state, with each occurrence of x in a \npredi\u00adcate replaced with r.f, and predicate (r = null).Since we are substituting the left-hand-side of \na computation by the right-hand-side, the generated predicate is precise for the given path except for \nrecursive paths. This works correctly as the analysis works on an SSA language representation where each \nuse of a variable has exactly one de.nition. In Figure 10 transformation 8 is related to the code access-path \nrepresentation that can contain nested method calls. At a statement x = m() that calls an external method, \nthe incoming state is updated by replacing occurrences of x with the expression for the method call. \nThis lets the analysis identify conditions involving external method calls, where the parameters of the \nmethod have dependences on input variables. Interprocedural path exploration To perform ef.cient interprocedural \nanalysis, the algorithm computes method summaries. The summary s for method M maps a state G at the exit \nof M to a set of states G. 1,..., G. (n = 1) at the n entry of M, where each G. represents the transformation \nof i G along a path in M . When the analysis (Figure 9), reaches a call site to M , it maps G to the \nexit of M (state Gmx ) and reuses a sum\u00admary if it exists (line 16). If not, the algorithm descends into \nthe called methods to analyze them (line 13). It uses a call stack to ensure a context-sensitive processing \nof called methods.11 After analyzing the called method, the algo\u00adrithm saves the summary information \nfor reuse in subsequent traversals (lines 14 15). On reaching the entry of the method that is not being \nan\u00adalyzed in a speci.c context (line 18), the algorithm ascends to all call sites that call the method \n(lines 21 22). If the en\u00adtry of the transform is reached, the algorithm adds the state predicates as \na disjunct to the input constraints (lines 19 20). Example 4 gives an example of how to generate access \npaths across method boundaries. Null-pointer exceptions To compute constraints for po\u00adtential null-pointer \nexceptions, TransformAnalysis pro\u00adcesses each statement in the transform that dereferences a variable \nto check whether a null-pointer exception could occur at that statement (lines 1 4). For a dereference \nof variable v at statement s, the algorithm initializes . to (x = null); then, it calls procedure ComputeConstraints, \nwhich computes the conditions on input variables under which . evaluates true at s. Class-cast exceptions \nTo identify class-cast exceptions, TransformAnalysis keeps track of predicates on types of reference \nvariables. For a reference variable v, predicate (type(v) .T) asserts that v points to an instance of \none of the types in the set T ; the negation of this predicate, \u00ac(type(x) .T) asserts that the type of \nv is not in the set T . State transformations 5 7 shown in Figure 10 are rele\u00advant for the analysis of \nclass-cast exceptions. For example, transformation 7 (that occurs at a statement x instanceof T) adds \ntwo predicates to the incoming state G: the .rst pred\u00adicate asserts that x cannot be null because, in \nthe Java se\u00admantics, a null is not an instance of any type; the second predicate constrains the type \nof x to be a subtype of T.Trans\u00adformation 6 is similar, but it does not add (x = null) to the state because, \nin Java, a null can be cast to any type. 11 A context-sensitive analysis propagates states along interprocedural \npaths that consist of valid call return sequences only the path contains no pair of call and return that \ndenotes control returning from a method to a call site other than the one that invoked it. Exception \nConstraint Validation Rule 1. (source.isIsSimple() = true).(source.getType() = null) (DataModel.artifacts.attributes.isSimple \n= true) . (DataModel.artifacts.attributes.type = null) =. NPE (DataModel.contextArtifacts.attributes.isSimple \n= true) . (DataModel.contextArtifacts.attributes.type = null) =. NPE 2. 3. (source.getArtifacts()*.getAttributes()*.isIsSimple() \n= true). (type(source.getArtifacts()*.getAttributes()*.getAnnotations(). get(0)): {ThisPackage}) (source.getArtifacts()*.getAttributes()*.isIsSimple() \n= true). (source.getArtifacts()*.getAttributes()*.getAnnotations(). size = 0) (DataModel.artifacts.attributes.isSimple \n= true) . (type(Datamodel.artifacts.attributes.annotations.0) :{ThisPackage}) =. CCE (DataModel.artifacts.attributes.isSimple \n= true) . (Datamodel.artifacts.attributes.annotations.size = 0) =. AIE Output Constraint Comprehension \nRule 4. (source.isIsSimple() = true). (DataModel.artifacts.attributes.isSimple = true) . (source.getType().equals(\"String\") \n= true). (DataModel.artifacts.attributes.type.equals(\"String\") (UMLUtilities.findType(...) = null) =.. \nModel.classes.properties.type (DataModel.contextArtifacts.attributes.isSimple = true) . (DataModel.contextArtifacts.attributes.type.equals(\"String\") \n=.. Model.classes.properties.type Table 1. The exception and output constraints and the corresponding \nvalidation and comprehension rules inferred for the three exceptions and output statement in the INFOTRANS \ncode fragment (Figure 6). The .rst constraint causes two transform rules to be generated, whereas the \nother constraints result in one rule each. The * in the exception constraints represents .next() . Given \ntwo type predicates on a variable v, they are re\u00adsolved as (type(s) .T1).(type(s) .T2) = if T1 nT2 = \n\u00d8, if (T1 nT2)= \u00d8 con.ict, otherwise EXAMPLE 3. Consider the program fragment public class A {} public \nclass A1 extends A {} public class A2 extends A {} public class A21 extends A2 {} public class A22 extends \nA2 {} [1] A a= new A1(); [2] if ( a instanceof A2 ) { [3] A21 a21 = (A21)a; To determine whether a class-cast \nexception can occur at line 3, the procedure ComputeConstraints is invoked with predicates (a = null) \nand \u00ac(type(a) .{A21}). State\u00adment 2 generates the predicates (a = null) and (type(a) . {A2,A21,A22}). \nThe resolved set of predicates now consists of .1 = (a = null) and .2 = (type(a) .{A2,A22}). Then, statement \n1 generates predicate (type(a) .{A1}),which is inconsistent with .2. Therefore, the path (1, 2, 3) is \ninfeasi\u00adble and, consequently, no class-cast exception can occur at line 3. D Array-index exceptions \nWe perform a limited analysis of statements, such as statement 20 in the INFOTRANS code (Figure 6), that \nretrieve a value from a collection using an integer constant as the index value. At such a statement \ns: c.get(intConst), if the size of collection c is less than or equal to intConst, an array-index exception \nis thrown. We compute the conditions on input variables under which (1) s is reached with (c=null), and \n(2) and there is no condition that checks the size of c on which s is directly or transitively control \ndependent. EXAMPLE 4. For the call to get() at statement 20 in Fig\u00adure 6, the analysis calls ComputeConstraints \nwith the initial state containing .1 = (attr.getAnnotations() = null).(attr = null). At statement 18, \nit picks up predicate .2 = (attr.isIsSimple() = true). Statement 17 updates both the predicates by replacing \nattr with attrItr.next(). Next, at statement 16, the analysis adds the predicate .3 = (attrItr.hasNext() \n= true). At line 13, (attr = null) is not added as it con.icts with (attr = null). At the entry of handleComplexTypes(), \nthe analysis as\u00adcends to the call site at line 30 of execute2(). Using, the actual-to-formal parameter \nmatching, it updates the predi\u00adcates by replacing attrItr with artifact.getAttributes(). Thus, at this \npoint .1 is (artifact.getAttributes().next().getAnnotations() = null).(artifact.getAttributes() = null) \nContinuing in the same manner through statements 28, 27, 26, and 24, the analysis computes .1 at entry \nas (source.getArtifacts().next().getAttributes(). next().getAnnotations()= null). ... Predicate .2 and \n.3 are updated similarly. Next the algorithm checks if statement 20 is control de\u00adpendent on any statement \nv.size op intConst where v is an alias of getAnnotations() and op is one of {<, =, =}.The algorithm does \nnot analyze how v.size is actually updated in the program for example, by statements that add elements \nto the collection. Thus, for array-index exceptions, the anal\u00adysis computes constraints on unchecked \naccess to an array location. To aid in rule generation, we now convert .1 into the more meaningful predicate \n(source.getArtifacts().next().getAttributes(). next().getAnnotations().size()= 0)We .lter out predicates \nlike (source.getArtifacts() = null) that are implicit. D Column 2 of Table 1 shows the constraints inferred \nfor the three INFOTRANS exceptions (Figure 6). For brevity, we have replaced occurrences of .next() in \nthe constraints with * in the table. Output constraints Intuitively, an output constraint cap\u00adtures \nthe conditions on input variables under which an out\u00adput element is generated. However, once the postconditions \nhave been identi.ed, the rest of the computation is similar to that of exception constraints. EXAMPLE \n5. Consider statement 12 in the INFOTRANS code fragment (Figure 6), which de.nes the property type in \nthe target object. For the output variable corresponding to property type, statement 12 is one of the \nreaching de.ni\u00adtions. The analysis starts at the control dependence of this statement statement 11 with \na predicate (ptype = null). At statement 10, ptype is replaced with the external method call to UMLUtilities.findType(...). \nNext, the analysis picks up predicate (type src.equals(\"String\") = true)at statement 9 and (attr.isIsSimple() \n= true) at state\u00adment 8. After processing the assignments at statements 7 and 1, the analysis identi.es \nthe following conditions at the entry of the method (source.isIsSimple() = true).(source.getType().equals(\"String\") \n= true).(UMLUtilities.findType(...) = null) D Constraint Filtering Our approach uses .lters to improve \nthe accuracy of the computed constraints. The .lters con\u00adsist of invalid constraints and bug constraints. \nThe .rst category .lters predicates that cannot be true because of constraints imposed by the transformation-authoring \nframe\u00adwork. For example, if EMF were used to serialize the input model .le into a Java object, any list \naccessed from such ob\u00adjects and corresponding iterators cannot be null. Thus, at a dereference list.f \nof such a list, predicate (list = null)need not be generated. Our studies (Section 6.1) indicate that \nthe use of only a few framework-speci.c .lters can improve the results of the analysis signi.cantly by \nremoving many false predicates. Moreover, the .lters need to be speci.ed only once, and, in our experience, \ncan be identi.ed with lit\u00adtle effort. The second category .lters out constraints that indicate potential \nbugs in the transform code. Such constraints, al\u00adthough relevant for the transform author, are uninteresting \nfrom the transform user s perspective. In fact, the transform author would either .x the potential bugs \nor .lter out the constraints before computing the transform rules. EXAMPLE 6. Consider the following \nexample [1] x = null; [2] if ( getTransformType().equals(\"f1\") ) [3] x = new T(\"1\"); [4] else if ( getTransformType().equals(\"f2\") \n) [5] x = new T(\"2\"); [6] x.foo() The dereference of x at line 6 is a potential null-pointer exception. \nHowever, the transform author may know that getTransformType() always returns \"f1\" or \"f2\".In this case, \nwe need to add a .lter so as not to generate an unnec\u00adessary rule. If getTransformType() may return other \nstrings, it is the author s responsibility to .x the bug. D 3.3 Implementation over XYLEM To compute \nthe constraints, we leverage the null-dereference analysis implemented in the XYLEM tool [22]. The goal \nof the XYLEM analysis is to identify a program path along which a dereferenced variable can be null. \nStarting at a statement sr that dereferences variable v, XYLEM performs a backward, path-sensitive and \ncontext-sensitive analysis to identify such a path. While the basic infrastructure of backward, path-sensitive \nand context-sensitive analysis remains the same, the driver of the analysis is completely new. The current \nanalysis drives the paths through library methods and tries to reach the top of the call graph. The older \nanalysis used heuristics to handle library methods and aimed to stop at any appropriate method boundary. \nThe statement transformations have also been modi.ed to support generation of access paths. So rather \nthan use symbolic heap locations, we build the entire access path by concatenating individual access \npaths. Since this may gener\u00adate an exponential number of paths, we limit the number of paths we explore \nusing a user-de.ned threshold. In addition, we made several extensions and enhance\u00adments to the original \nanalysis: (1) the analysis computes a form of access paths that can contain nested method calls and parameters \nto those method calls; (2) in addition to iden\u00adtifying null-pointer exceptions, the extended analysis \niden\u00adti.es potential class-cast exceptions and (limited forms of) array-index exceptions; and (3) instead \nof identifying one feasible path (to a null dereference), the analysis identi.es constraints on input \nvariables along all paths. 3.4 Pointer Analysis and Aliasing The accuracy of pointer analysis affects \nthe number of com\u00adputed constraints: a less accurate analysis would cause more spurious constraints to \nbe computed, which could affect the usefulness of the approach. Our implementation uses a .ow and context-sensitive \npointer analysis [21]. The lack of path\u00adsensitivity in the pointer analysis is made up for by the path-sensitive \nXYLEM propagation. To illustrate the effects of pointer analysis, consider program (a) below: [1] if \n( src.type.val ) [1] !<src.type.val> [2] x = src.attr.prop1; [2] [3] else [3] [4] x = src.attr.prop2; \n[4] <src.attr.prop2.g1 = null> [5] if ( src.type.val ) [5] !<src.type.val> [6] y = x.g2; [6] [7] else \n[7] [8] y = x.g1; [8] <x.g1 = null> [9] y.foo(); [9] <y = null> (a) (b)  A path-sensitive pointer analysis \nwould identify that at line 6, x can point to src.attr.prop1 only, whereas at line 8, x can point to \nsrc.attr.prop2 only. Using such precise points-to information, our analysis would compute two con\u00adstraints \nfor the dereference of y at line 9 ((src.type.val = true).(src.attr.prop1.g2 = null)) . ((src.type.val \n= false).(src.attr.prop2.g1 = null)) However a path-insensitive pointer analysis does not take into account \nbranch correlation and assumes that at lines 6 and 8, x may point to either src.type.prop1 or src.type.prop2. \nThis less accurate points-to information combined with the older XYLEM analysis would cause our analysis \nto compute two additional, spurious constraints ((src.type.val = true).(src.attr.prop2.g2 = null)) . \n((src.type.val = false).(src.attr.prop1.g1 = null)) There are two factors that help make our analysis \nprecise (1) we generates constraints on extended access paths rather than on precomputed points-to information, \nand (2) the pred\u00adicate propagation is path sensitive. By the .rst criterion, we build access paths by \nreplacing the left-hand-side of a com\u00adputation by the right-hand-side which makes it precise for the \ngiven path; and by the second criterion, we ensure that only valid paths are traversed. Thus, in the \nexample above, the analysis ensures that if the predicate (src.type.val = true), then the path traversed \nis through the lines 1, 2, 5, 6, 9 and similarly for (src.type.val = false), then the path traversed \nis through the lines 1, 4, 5, 8, 9 and thus computes only the correct constraints. Part (b) of the .gure \nabove shows the predicates generated along the path through the lines 9, 8, 5, 4, 1.  3.5 Recursive \nModel Elements To illustrate the processing of recursive model elements, consider the transform code \nshown in Figure 11. The input model of the transform has recursive elements, as shown by the following \ncontainment relations Model . . {(C,Artifact,rootArtifact,one)} Artifact .  . {(C,Attribute,attributes,many)} \nAttribute .  .{(C,Artifact,fact,one)} The program contains a recursive method transform() (line 5), \nwhich is called at line 4 from the entry method execute() and recursively at line 12. The right side \nof the .gure shows the state predicates that are propagated by the analysis. The numbers next to the \npredicates indicate the order in which the predicates are generated. Suppose that the dereference of \nname at line 7 could cause a null-pointer exception. The analysis starts at line 7 with predicate (name \n= null). Using the standard back substitu\u00adtion, the predicate gets transformed to (src.getName() = null)at \nline 6. At the entry of transform(), the predicate is prop\u00adagated, after mapping to actual parameters, \nto call site 4, and .nally to the entry of execute() (as predicate 4). At call site 12, the mapped predicate \nbecomes (srcAttr. getFact().getName() = null) (predicate 5). Statements 10 and 8 transform the predicate \nto (src.getAttributes(). iterator().next().getFact().getName() = null) (pred\u00adicate 7), which reaches \nthe entry of transform(). This is the end of iteration 1 of the algorithm. In the next iteration, predicate \n8 is propagated to call sites 4 and 12. At call site 12, the mapped predicate 10 is propagated to line \n10, where predicate 10 is transformed to (srcIter.next().getFact().getAttributes().iterator(). next().getFact().getName() \n= null) (predicate 11). Fi\u00adnally, at line 8, the analysis computes predicate 12, whose access path has \nthe repeating sub-sequence getAttributes(). iterator().next().getFact(), illustrated in the .gure by \nthe shaded portion of predicate 12.12 We mark off the recur\u00adsive parts of the access path and propagate \nonly the folded structure. Thus, the recursive sub-sequence is folded to gen\u00aderate predicate (src.getAttributes().iterator().next(). \ngetFact().getName() = null), which is the same as predi\u00adcate 7 that was computed at line 8 in the previous \niteration. Thus, the analysis .nds no new predicates and hence termi\u00adnates.  3.6 Soundness and Completeness \nWe now evaluate the soundness and completeness of our analysis for computing code constraints. Recall \nfrom Def\u00adinition 5 that a code constraint CI (., s) is a disjunction of constraints along the paths to \nstatement s. An incomplete analysis could either fail to compute CI (., s), or fail to compute a disjunct \n(i.e., a path con\u00adstraint) for CI (., s), for statement s. The .rst type of in\u00adcompleteness can occur \nbecause our current implementation analyzes only three types of runtime exceptions; a trans\u00adform could \nfail because of a runtime exception type, such as ArrayStoreException, not currently handled by the analy\u00adsis. \nThe analysis handles only limited forms of array-index exceptions. Moreover, a transform could fail because \nof ex\u00adceptions thrown by calls to external (e.g., JDK API) meth\u00adods; our implementation does not compute \nconstraints for such exceptions. The second type of incompleteness can occur because the analysis along \na path can abort. Our algo\u00adrithm uses three parameters to bound the analysis: the time required to analyze \na path, the state size, and the number of paths through a method [22]. If the upper bounds for these \nparameters are reached, the algorithm can miss computing some path constraints. However, in our empirical \nstudies (Section 6.1), this did not occur for any of the subjects. An unsound analysis could compute \nspurious input con\u00adstraints for a statement (i.e., constraints that cannot be satis\u00ad.ed in any execution). \nThe sources of unsoundness include limitations of static analysis in processing loops and arith\u00admetic \nexpressions; and, as illustrated earlier, imprecision in points-to analysis. Related to the discussion \nof soundness and completeness is whether a path constraint in CI (., s) represents a neces\u00ad 12 To identify \na recursive sub-sequence in an access path, we ignore the actual parameters of any method calls that \nappear in the path; we use only the signatures of these methods. [RULEPREDICATE] . ::= [pmodel ref null] \n|[pmodel ref strConst] | [pmodel ref true] [pmodel .size int intConst] |[pmodel type T ] | T.E ref ::= \n= | = int ::= < |=| = | = | > |= type ::= .|: [TRANSFORMRULE] . ::= .1 . ... . .k =. result k = 1 result \n::= excp |.(pmodel ) excp ::= NPE | CCE | AIE Figure 12. Rule predicates de.ned with respect to metamodel \naccess paths (top). Transform rules de.ned with respect to a pair of input and output metamodels (bottom). \nsary and/or suf.cient condition (or neither). The factors that introduce unsoundness can also cause a \npath constraint to not be a suf.cient condition. However, a path constraint is a necessary condition: \nif an input object does not satisfy a path constraint to a statement s, the relevant behavior (failure \nor output generation) cannot occur at s along the path.  4. Step 2: Rule Generation Step 2 of our approach \n(Figure 2) converts code-level con\u00adstraints to model-level rules. DEFINITION 8. (Rule predicate) A rule \npredicate .,de\u00ad.ned with respect to an access path in a metamodel M,is a predicate of the form shown \nin Figure 12. A rule predicate is de.ned in terms of a metamodel access path, and speci.es constraints \non the path. For example, for INFOTRANS, DataModel.artifacts.attributes.type = null is a rule predicate. \nA transform rule is de.ned over a conjunction of rule predicates. DEFINITION 9. (Transform rule) A transform \nrule . de\u00ad.ned with respect to a transform t : MI .MO is a rule, of the form shown in Figure 12. The \nleft-hand side (the an\u00adtecedent) is a conjunction of rule predicates. The right-hand side (the consequent) \n, which states the result of the rule, is either an exception or the creation of an output metamodel \nelement or property (de.ned as an access path). Table 1 shows the transform rules generated for the three \nexception constraints and one output constraint for INFO-TRANS. For example, the .rst rule states that \nif the isSim\u00adple property of DataModel.artifacts.attributes is true and the type property is null, a \nnull-pointer exception occurs in the transform. Exception constraints are mapped to validation rules, \nwhereas output constraints are mapped to comprehen\u00adsion rules. Note that whereas a code constraint is \na DNF formula (a disjunction of conjunctions) over predicates, a transform rule is a conjunction of rule \npredicates. The rule generator trans\u00adlates each conjunct (or, a path constraint) in a code-level con\u00adstraint \nto a transform rule. Thus, an input constraint with n path constraints leads to the generation of n transform \nrules. We de.ne rules as a conjunction, instead of a DNF formula, because it enables the identi.cation \nand elimination of dupli\u00adcate rules. For example, an exception constraint may cause null-pointer exceptions \nat several statements in the trans\u00adform. The code analysis will compute the same constraint for each \nof these statements. Consequently, the rule set will have multiple rules that have the same antecedent \nand the same consequent (NPE); such duplicate rules are removed during rule generation. The constraint-to-rule \ntranslation requires converting a code predicate . to a rule predicate ., which, in turn, es\u00adsentially \ninvolves converting a code access path pcode to a metamodel access path pmodel . Thus, the core of the \nrule\u00adgenerator component is the access-path translation step. Re\u00adcall from De.nitions 4 and 2 that pcode \nis a sequence of dereferences of variables or method return values, whereas pmodel is a sequence of containment \nrelations that is com\u00adposed of metamodel element names and method names and possibly ending with a property. \nFor each reference variable v or method m() in pcode , the rule generator has to identify the metamodel \nelement to replace v or m() with. For example, consider the .rst exception constraint and its corresponding \ntransform rule in Table 1. To perform the translation, the rule generator has to replace variable source \nwith metamodel access path DataModel.artifacts.attributes, method isIsSimple() with property isSimple, \nand method getType() with property type. To do the translation, our approach uses a mapping .le; Figure \n13 shows a partial XML representation of the map\u00adping information for INFOTRANS.13 A mapping .le, in \ngen\u00aderal, links metamodel element names and properties to enti\u00adties in the transform inputs. For INFOTRANS, \nthis requires linking the input ECORE metamodel element names and properties to Java methods and .elds. \nIn the representation shown in Figure 13, each meta-ModelElement entry maps a metamodel element name \nor property type to Java method names. (Although not illus\u00adtrated in this example, the mapping .le represents \nmeth\u00adods by their signatures, which can accommodate over\u00adloaded and overridden methods.) For example, \nproperty isSimple maps to method isIsSimple() in the input Java class. Similarly, the element type named \nartifacts maps to method getArtifacts(). Each methodName entry in the .le states the metamodel access \npaths for the inputs and outputs of a main method in the transform a method that is invoked to perform \na transformation. (In the RSA transformation-authoring framework, a transform can have multiple main \nmethods; this may not be true for other frameworks.) For example, method execute1() takes as 13 Figure \n13 presents only the information that is required for mapping the exception and output constraints shown \nin Table 1 to transform rules. <mapping> <metaModelElement name=\"isSimple\"> <method name=\"isIsSimple\"/> \n</meta-model-element> <metaModelElement name=\"type\"> <method name=\"getType\"/> <method name=\"setType\"/> \n</meta-model-element> <metaModelElement name=\"artifacts\"> <method name=\"getArtiacts\"/> <method name=\"setArtifacts\"/> \n</meta-model-element> <metaModelElement name=\"attributes\"> <method name=\"getAttributes\"/> <method name=\"setAttributes\"/> \n</meta-model-element> <metaModelElement name=\"annotations\"> <method name=\"getAnnotations\"/> <method name=\"setAnnotations\"/> \n</meta-model-element> <methodName name=\"execute1()\"/> <source name=\"DataModel.artifacts.attributes\"/> \n<target name=\"Model.Class.Property\"/> </methodName> <methodName name=\"execute1()\"/> <source name=\"DataModel.contextArtifacts.attributes\"/> \n<target name=\"Model.Class.Property\"/> </methodName> <methodName name=\"execute2()\"/> <source name=\"DataModel\"/> \n<target name=\"Model\"/> </methodName>  </mapping> Figure 13. Mapping .le used for translating the INFOTRANS \nexception and output constraints to transform rules. input a Java class that corresponds to the metamodel \nac\u00adcess path DataModel.artifacts.attributes or the path Data\u00adModel.contextArtifacts.attributes; its output \nJava class cor\u00adresponds to the access path Model.classes.properties in the output UML metamodel. An input \nor output class can corre\u00adspond to more than one access paths. Similarly, for method execute2(), the \ninput Java class maps to access path Data-Model, whereas the output class maps to access path Model. \nUsing this mapping information, the rule generator can perform the constraint-to-rule translation by \nmapping code access paths to metamodel access paths. EXAMPLE 7. Consider the constraints and transform \nrules shown in Table 1. For the code access path of the .rst exception constraint, the rule generator \ncreates two meta\u00admodel access paths, and, therefore, two transform rules. It replaces source (the input \nJava class name of execute1()) with DataModel.artifacts.attributes in the .rst rule, and Data\u00adModel.contextArtifacts.attributes \nin the second rule. Next, it replaces method name isIsSimple() with property isSimple in the .rst rule, \nand getType() with property type in the sec\u00adond rule. Thus, the exception constraint for the null-pointer \nexception is translated to two validation rules. D EXAMPLE 8. Consider the access path for the predicate \non types in the second constraint in Table 1. For this constraint, the relevant entry method is execute2(); \ntherefore, using the parameter-mapping information for execute2(),the rule generator translates source \nto DataModel. Next, using the third mapping rule in Figure 13, the rule generator replaces getArtifacts.* \nwith artifacts. In general, an access of an el\u00adement in a collection (e.g., c.next())in pcode corresponds \nto a metamodel element with cardinality many in pmodel . Similarly, the generator translates getAttributes().* \nand getAnnotations.*. Finally, the generator replaces get(0) with 0 to compute the translated metamodel \naccess path. In general, the rule generator replaces a method that retrieves a collection element using \na constant index with the constant index value in the metamodel access path. D For a library method \nin a rule, we make no assumptions about any side effects that it may cause. We simply output the library \nmethod in the generated rules. If the consumers using the rules (i.e., a validity checker or a transform \nuser or a transform author) know the side-effects of this method, they may choose to keep the rule; otherwise \ndiscard it. The extent to which the generation of the mapping .le can be automated depends on the transformation-authoring \nframework and the representation of the input and output models that are being used. For example, if \nthe models are represented using EMF, the mapping of metamodel elements to methods can be generated automatically, \nwith no man\u00adual intervention by the user. However, in other standard or custom model-transformation frameworks, \nless automation may be possible, which would require the transform author to provide the information \nmanually. Similarly, in the RSA framework, the information about method to source/target mapping can \nbe generated automatically. In other frame\u00adworks, such automation may not be possible.  5. Step 3: Model \nValidation and Transformation Comprehension The transform rules inferred in Step 2 of our approach can \nbe used to support model validation and transformation com\u00adprehension. Because the rules are stated in \nthe metamodel vocabulary, they can be used in a straight-forward manner to support these tasks. Our approach \ndistinguishes validation rules from query\u00adingrules.Ina validation rule, the consequent is an excep\u00adtion \nthat can be thrown if the antecedent is satis.ed. In a querying rule, the consequent is an existential \nquanti.er on an output metamodel access path; such a rule states that if the antecedent is satis.ed, \nan element or property is created in the output model. The validation rules are used for check\u00ading whether \na model is a valid input to a transform, whereas querying rules are used for supporting general transforma\u00adtion \ncomprehension. We illustrate both of these use cases. 5.1 Model Validation Given a set of validation \nrules . for a transform t : MI . MO and an input model MI , a validity checker returns a subset of the \nrules in . that are satis.ed by MI . If none of the rules are satis.ed, MI is a valid instance that the \ntransform can be executed on. However, if at least one of the validation rules is satis.ed, MI is not \na valid input to the transform; the transform can fail with an exception when executed on MI . To check \nwhether a rule . . . is satis.ed by MI , the validity checker .nds the matching instances for the metamodel \naccess path in the antecedent of .,and applies the condition stated in the antecedent rule predicate \nto the instances. If the condition is satis.ed, MI is an invalid input model to t. The validity checker \ncan be used in a batch mode, in which it .ags a list of matching rules and corresponding problematic \ninput model elements. Alternatively, the validity checker can be used in an interactive mode; in this \nmode, while the user is creating an input model, the validity checker .ags the problematic input model \nelements that could cause exceptions. The rules can be translated to a model constraint language such \nas OCL, for which a validity checker can be constructed by leveraging existing tools or frameworks such \nas Naomi14 and EMF Validation Framework in Eclipse.15 5.2 Transformation Comprehension A querying rule \ncan be used to support user queries in a comprehension tool. For example, if an element that was ex\u00adpected \nin the output model is missing, the querying rules can be searched to .nd the ones that determine the \ncreation of the missing element in the output model. These rules in\u00addicate the dependences of the missing \noutput element to the input model elements. The user can then identify the cause by examining the input \nmodel elements to see whether they satisfy the rules, and correct the input model appropriately.  6. \nEmpirical Evaluation To evaluate the feasibility and usefulness of our approach, we conducted two empirical \nstudies. In the .rst study, we evaluated the accuracy of the analysis in terms of inference of useful \nconstraints and transform rules. The second study was a user study, in which we investigated whether \nthe use of transform rules can help users in diagnosing the causes of failing and incomplete transformations \nmore ef.ciently. 6.1 Feasibility Study In the .rst study, we evaluated the feasibility of our approach \nin terms of whether the approach can infer enough useful rules to support validation and comprehension \ntasks. 6.1.1 Experimental setup We implemented the algorithm shown in Figure 8 using XYLEM. XYLEM uses \nthe WALA analysis infrastructure16 to construct the call graph and the CFGs. XYLEM performs the 14 http://mocl.sourceforge.net/ \n 15 http://help.eclipse.org/galileo/index.jsp?topic=/org.eclipse. emf.validation.doc/tutorials/oclValidationTutorial.html \n16 http://wala.sourceforge.net  (1) (2) (3) (4) (5) (6) (7) (8) (9) (10) (11) (12) (13) (14) (15) Exception \nType Subject Total Neg Total Positives NonSrc Src Filtered Constraints Rules Unique Rules Rule Predicates \nMin Max Avg Access-path Min Len Max Null-Pointer exception Subject-1 Subject-2 Subject-3 Subject-4 1125 \n321 1301 688 1032 280 1222 639 193 41 79 49 106 8 35 21 87 33 44 28 79 19 40 21 609 1008 4576 34 80 29 \n174 23 1 2 1 1 5 6 6 1 2 3.3 2.6 1 2 3 2 4 10 5 8 9 Class-cast exception Subject-1 Subject-2 Subject-3 \nSubject-4 185 30 198 71 130 27 180 49 55 3 18 23 20 1 2 16 35 2 16 7 11 2 8 6 17 36 96 12 12 2 37 9 1 \n2 1 1 2 2 4 1 1.2 2 1.7 1 3 4 3 4 8 4 6 8 Array-index Subject-1 12 4 8 6 2 2 2 1 1 1 1 5 5 exception \nSubject-2 2 0 2 0 2 2 36 2 1 1 1 4 4 Subject-3 10 10 0 0 0 0 0 0 0 0 0 0 0 Subject-4 3 1 2 2 0 0 0 0 \n0 0 0 0 0 Table 3. Inferred exception constraints and validation rules. Bytecode Subject Classes Methods \ninstructions Time Subject-1 41 280 4904 285.5s Subject-2 13 77 1212 15.7s Subject-3 48 399 5340 280.5s \nSubject-4 29 157 2449 33.08s Table 2. Subjects used in the empirical evaluation. analysis in two steps. \nIn the .rst step, it performs points-to analysis, escape analysis, and control-dependence analysis. In \nthe second step, it uses the results of the .rst step and com\u00adputes exception constraints; we are currently \nimplementing the computation of output constraints. We used four experimental subjects; Table 2 lists \nthese subjects along with information about the number of classes, methods, and bytecode instructions \nin each subject. The last column lists the time taken to execute XYLEM on these sub\u00adjects. These subjects \nare real model transforms that have been developed as part of ongoing research projects in IBM. All of \nthe subjects was developed using RSA model-to model transformation framework. Subject-1 and Subject-4 \ntransform a SOMA Service Model [26] to an application\u00adspeci.c ECORE model; these were intermediate models \nthat were eventually transformed to different types of code arti\u00adfacts. Subject-2 is the INFOTRANS transform \nintroduced in Section 2.1. Subject-3 transformed a SOMA Service Model to an RSA Software Services Model.17 \n 6.1.2 Goals and method The goals of the study were to investigate (1) the number of constraints and \nrules identi.ed by our approach, (2) the effectiveness of .lters in removing uninteresting constraints, \nand (3) the extent to which duplicate rules are computed. To compute the results, we ran XYLEM twice \non each subject to compute exception constraints. After the .rst run, we asked the transform authors \nto examine the computed constraints and identify .lters that would remove invalid and bug constraints. \nWe used the .lters in the second run http://www.ibm.com/developerworks/rational/library/05/510 svc/ \n of XYLEM.We wroteasimpleJavaprogram to translate .ltered exception constraints to validation rules and \nremove duplicate rules. For Subject-1 and Subject-2,the .nal rule set was examined by the transform authors \nto determine the validity of the rules. All reported validation rules were found to be valid rules. \n6.1.3 Results and analysis Table 3 presents the results of the study. We show the data for the three \ntypes of exceptions separately, so that usefulness of each analysis is illustrated. Column 3 shows the \ntotal number of traversals performed by XYLEM. The maximum number of traversals were performed for null-pointer \nexceptions. This is expected because dereference statements occur much more frequently in Java programs \nthan typecast statements or statements that access of collections. Column 4 shows the number of negatives \nthat is, the number of traversals that XYLEM determined could not re\u00adsult in a null-pointer exception, \na class-cast exception, or an array-index exception. Column 5 is the number of true pos\u00aditives and is \ndivided into two categories: Column 7 shows the number of positive constraints that were rooted in the \nsource and, therefore, are potential candidates for rules; Col\u00adumn 6 shows the number of positive constraints \nthat were not rooted in the source and, hence, could not be mapped back to the model. For example, SolutionUtils.specialchars \n= null is a local predicate that cannot be mapped into the input model and target.eContainer().getRole()=null \nis rooted in the target model and has no mapping into the source model. Some of the constraints in Column \n7 get .ltered out dur\u00ading post-processing. Column 8 gives the number of con\u00adstraints left after the .lters \nhave been applied. For example, source.eContainer().getPackage()=null gets .ltered out since we know \nthat getPackage() can never return null.For our subjects, we have a set of 30 .lters that have been man\u00adually \nspeci.ed. The data illustrate that .lters are effective in removing many uninteresting constraints. On \naverage over all subjects and exceptions, the number of constraints was reduced by over 23%, from 247 \ninitial constraints to 190 constraints, af\u00adter .ltering. The maximum reduction over 38% occurred for \nSubject-2. Column 9 shows the number of validation rules that were translated from the .nal constraints. \nAs mentioned in Sec\u00adtion 4, the constraints are stated as a DNF formula over ab\u00adstract predicates, whereas \nrules are stated as a conjunction of rule predicates. Thus, each disjunct, or path constraint, in an \nexception constraint gets translated as a validation rule. The data in column 9 indicate that the .nal \nconstraints contained a large number of path constraints. For example, for Subject-3, 40 exception constraints \nfor null-pointer ex\u00adceptions resulted in 4576 rules on average, 114 path con\u00adstraints per exception constraint. \nThere is a wide variation in the number of path constraints for the subjects: on average, Subject-3 had \n97 path constraints, whereas Subject-4 had only 2 path constraints, per exception constraint; Subject-1 \nand Subject-2 had 7 and 47 path constraints, respectively, per exception constraint. Column 10 illustrates \nthat a very small percentage of the rules were unique rules. For Subject-3, 4402 of the 4576 rules for \nnull-pointer exceptions were duplicates; thus, after the removal of duplicate rules, only 174 rules remained. \nOver all subjects, the number of rules decreased from 6426 to 369 after the removal of duplicates a reduction \nof over 99%. Columns 11 13, show the minimum, maximum, and av\u00aderage number of rule predicates per transform \nrule. The data show that, typically, the transform rules are fairly simple in that the antecedent of \nthe rules contains conjunctions of very few predicates. None of the rules, over all subjects, had more \nthan six rule predicates in the antecedent. Columns 14 15 show the minimum and maximum lengths of the \nmetamodel access paths for the rules. An access\u00adpath length illustrates the chain of relations that occurs \nin a model, and, thus, is an indicator of the complexity of a metamodel. For our subjects, the maximum \nmetamodel access-path length ranged from four to 10.  6.1.4 Discussion Our study reveals several trends \nthat illustrate the bene.ts of our approach. For our subjects, the approach inferred 369 useful rules, \nwhich is a signi.cant number. The use of .lters is essential because it can remove many uninteresting \ncon\u00adstraints; by doing so, it improves the effectiveness of model validation and transform comprehension. \nMoreover, many of the validation rules were duplicates; thus, removal of dupli\u00adcate rules is an important \nstep in our approach that is es\u00adsential for improving its usability. The number of path con\u00adstraints \nper exception constraint varied widely among our subjects from 114 to two. The number of path constraints \ndepends on the structure of the program and complexity of the input metamodel; therefore, the variation \nindicates that our subjects are structured quite differently, in terms of the number of program paths \nand the input metamodel. The data also demonstrate the effectiveness of XYLEM in that it is able to analyze \nmany paths. We manually analyzed the code base of INFOTRANS (Subject-2) for each traversal that XYLEM \nreported to be a negative (Column 4 of Table 3). For null-pointer exceptions, we sampled 210 negatives. \nOf these, eight were found to be false negatives. Therefore, at eight points in the transform code, exceptions \ncould be thrown because of null values being passed in some input model element, but that were ignored \nby our analysis. These eight constraints led to two rules, one of which was already computed (by the \nanalysis of a difference dereference point); the other rule was missed by the analysis. Similarly, for \nclass-cast exceptions, we sampled 16 negatives, out of which one was a false negative and would have \nled to a new rule being identi.ed. The main source of the false negatives was the presence of calls to \nexternal methods for which bytecode was not available for analysis. For such method calls, XYLEM cannot \ndetermine whether the return values may be null and hence misses some positives. Another source of false \nnegatives was failures caused by exceptions that are not analyzed by our implementation. As discussed \nin Section 3.6, exceptions thrown by calls to external methods cause the analysis to be incomplete. An \nexample of such a call that we found is new Integer(source.getMultiplicity()).intValue() If the string \nreferenced by source.getMultiplicity() were not a parsable integer, a NumberFormatException would be \nthrown. Because our current implementation han\u00addles only a limited set of exceptions, it cannot compute \ncon\u00adstraints for such statements.  6.2 User Study Our second study was a user study, in which we tested \nthe following hypothesis: A user can perform the task of identifying and .xing bugs in an invalid input \nmodel more ef.ciently when guided by the transform rules than without the rules. 6.2.1 Experimental \nSetup To select participants with different degrees of expertise, we identi.ed the factors on which the \nexpertise assessment could be based. Familiarity with MDD concepts is a key fac\u00adtor. We used INFOTRANS \nas the subject, which is created using the RSA transformation-authoring framework. Thus, familiarity \nwith the RSA capabilities for model creation, model browsing, and transformtion authoring is another \nim\u00adportant factor. Finally, knowledge of code-navigation fea\u00adtures provided by tools, such as Eclipse, \nis a factor that de\u00adtermines the ef.ciency with which a participant can navi\u00adgate the transform code \nto identify violated input model con\u00adstraints. Based on these factors, we grouped the participants into \nthree categories: expert (one participant, referred to as  E1 5 3 (60%) 2 1 (50%) I1 7 4 (57%) 14 2 \n(14%) I2 6 4 (67%) 8 3 (38%) I3 13 5 (38%) 7 6 (86%) N1 16 7 (41%) 14 7 (50%)   Table 4. Time taken \nby the participants to complete the tasks. E1), intermediate (three participants, referred to as I1, \nI2, and I3), and novice (one participant, referred to as N1). We created two debugging tasks: Task T1,inwhich \nINFO-TRANS fails with an exception, and Task T2,inwhich INFO-TRANS generates an incomplete output model. \nFor each of the tasks, we created two subtasks, one in which the partici\u00adpants had to debug the problem \nwithout using the transform rules (Twr ), and another in which the participants had to de\u00adbug the problem \nwhile guided by the rules (Tr ). To enable a fair comparison of the effort required to complete the tasks, \nwe ensured that each pair of subtasks (T(1,wr),T(1,r)) and (T(2,wr),T(2,r)) were of similar dif.culty. \nWe created four input models accordingly with errors, one each for T(1,wr), T(1,r), T(2,wr),and T(2,r). \nFor each task, the participants were asked to .x the input models. For T(1,wr) and T(2,wr), the participants \nwere given access to the transform code and were also allowed to use code-debugging features. For the \nT(1,r) and T(2,r), the par\u00adticipants were allowed to use the rules only (with no access to the transform \ncode). Thus, we simulated the scenario in which transform users have to debug their models without needing \nto examine the transform source code. The trans\u00adform rules were created by running XYLEM on the INFO-TRANS \ntransform; the computed rules were augmented with manually created querying rules for output constraints. \nWe measured the time each user took to complete the tasks. During the study, the participants were allowed \nto ask questions about usage of the tools, but not about the input or output model instances.  6.2.2 \nResults and analysis Table 4 lists the time taken by the participants to perform the tasks. As the table \nillustrates, all users irrespective of their expertise levels completed the tasks faster when they were \nguided by the rules than when they were not. For example, the expert participant took .ve minutes to \ncomplete the .rst task without rules and three minutes when using rules. The participants with intermediate \nexpertise took, on average, nine minutes to complete the .rst task without rules and only four minutes \nto complete it with rules. The novice participant took 16 and 14 minutes, respectively, to complete T(1,wr) \nand T(2,wr), and seven minutes each to complete T(1,r) and T(2,r). The maximum reduction (from 14 minutes \nto two minutes) occurred for user I1 for task T2. The minimum reduction occurred for user I3 for task \nT2.  Figure 14 presents a different view of the data: it shows the percentage of time taken by each \nparticipant to complete the four tasks. As shown in the .gure, the participants spent 62% to 78% of the \ntotal time in .xing the models without the rules, whereas they spent signi.cantly less time (22% to 38%) \nin .xing the models using the rules. In the feedback after the study, all participants mentioned that \nthe transform rules were very useful in identifying and .xing the problems with the input models. They \nalso felt that debugging transforms was different from debugging normal Java applications, as the inputs \nto transforms are typically more complex and have more elaborate syntax and semantics. Therefore, automated \ndebugging support that is customized for such applications can be useful; our approach provides such \nsupport. The participants unanimously wanted a visual representation of the rules for better usability. \nThe novice participant suggested that the visual representation could return the matching input model \nelements for the rules that explain a failing transformation. One of the participants wanted a more interactive \ncomponent that guides the user during model creation. Another user mentioned that a self healing or recommendation \nfeature that suggested .xes for the invalid model elements would be very useful. 6.2.3 Discussion Although \nour study is limited in nature, the results support our hypothesis that transform rules can enable a \nuser to iden\u00adtify the cause of a failing transformation or an incomplete output model more ef.ciently. \nAll the users found the rules useful, and each user performed the debugging tasks much faster when guided \nby the rules than when the rules were not used.  7. Related Work There exists a rich body of work in \nthe area of veri.cation and validation of model transformations. However, all of the existing research \nfocuses on checking the correctness of transforms. Giese et al. [12] present an approach, based on for\u00admal \nspeci.cations and theorem proving, for verifying the correctness of a model-to-code transformation algorithm. \nNarayanan and Karsai [23] present a veri.cation technique that uses bisimulation to check whether the \nsemantic prop\u00aderties of the input in a particular execution of a transform are preserved in the output \nfor that execution. Their approach fo\u00adcuses on transform implementations that are based on graph transformations. \nLano and Clark [18] present a constraint\u00adbased technique for specifying and verifying transforms. Unlike \nthese approaches, our work does not focus on trans\u00adform veri.cation. Instead, the goal of our work is \nto assist users in creating valid input models to a transform and in identifying problems with the input \nmodel for a failing or incomplete transformation. In addition to veri.cation techniques, many researchers \nhave addressed problems that MDD poses for testing activ\u00adities. Baudry et al. [1] present of overview \nof MDD charac\u00adteristics that can complicate different testing tasks. For ex\u00adample, the complexity of \ninput models can complicate test\u00adinput generation, and the heterogeneity of transform imple\u00admentations \ncan make de.nition of test adequacy dif.cult. Existing research has addressed many such testing prob\u00adlems, \nsuch as test-input generation (e.g., [3, 7]), test-oracle construction (e.g., [20]), de.nition of test-adequacy \ncriteria (e.g., [9, 11]), assessment of test quality (e.g., [19]), and de.nition of fault models (e.g., \n[16]). Our work addresses an important testing-related task debugging of failing and incomplete transformations \nthat has largely been ignored; therefore, it .lls a gap in existing research. Our analysis for computing \nconstraints is similar to the computation of weakest preconditions (e.g., [6, 8]). How\u00adever, we apply \nthe analysis to the domain of MDD, in which the inferred constraints are mapped to rules that are stated \nin the language of the input metamodel. Existing research has not explored this application of precondition \nanalysis. Analysis for identifying input constraints has most com\u00admonly been used for generating test \ninputs. Compared with such test-data generation techniques that use symbolic ex\u00adecution to generate test \ninputs (e.g., [13, 14, 25]), our ap\u00adproach does not generate test inputs. Therefore, its effective\u00adness \nis not dependent on the power of constraint solvers, which despite recent advances, continue to have \npractical limitations. For our application, the constraints are mapped to model-level rules. Buse and \nWeimer [4] present a static analysis for iden\u00adtifying exception conditions to assist with documentation. \nTheir approach locates exception-throwing statements and symbolically tracks paths to those statements. \nThe symbolic execution generates predicates describing feasible paths, and yields a boolean formula over \nprogram variables. This for\u00admula is used to generate human-readable documentation. In contrast, our approach \ncomputes exception and output con\u00adstraints that are mapped to model-level rules and used for model validation \nand transformation comprehension.  8. Summary and Future Work In this paper, we presented an approach \nfor assisting users of model transforms in debugging their input models without examining the transform \ncode. The approach uses static code analysis to compute constraints on the input model under which a \ntransform could fail with an exception (exception constraints) or generate an incomplete output model \n(output constraints). The computed constraints are abstracted from code-level conditions to validation \nand comprehension rules that are stated in the metamodel language. The rules are used to support model \nvalidation and transformation comprehen\u00adsion: the validation rules can be used for checking whether a \nmetamodel instance is a valid input to a transform, whereas the comprehension rules can help a user understand \nwhy an incomplete output model is generated. Our empirical results indicate that the approach can be \nef\u00adfective in computing a signi.cant number of useful rules. We also conducted a user study to investigate \nhow the inferred rules could enable transform users to perform debugging and comprehension tasks more \nef.ciently. All the participants in the study performed the debugging tasks faster with the rules than \nwithout them. These results suggest that our ap\u00adproach can be used to improve model-transformation tools \nby providing automated support for understanding transfor\u00admations. There are several interesting problems \nthat future research could address. Non-Java-based transforms In this paper, we focused on model-to-model \ntransforms that are written in Java. How\u00adever, models are often represented using XML and XSLT is frequently \nused in practice for writing transforms. Thus, fu\u00adture research could extend our approach to handle transforms \nimplemented in XSLT (or, other transform-implementation technologies). Model-to-text transformations \nModel-to-model transfor\u00admations are usually intermediate steps in MDD, with the end objective being to \ngenerate code (Java code, HTML pages, JavaScript code, etc.). Applying our approach to model-to\u00adcode \nor, more generally, to model-to-text transformations is an interesting direction for future research. \nFuture work could identify the salient features in model-to-text transfor\u00admations and explore how the \nstatic-analysis approach needs to be extended to generate useful rules. Interfaces for improving usability \nIn our current ap\u00adproach, the transform rules are presented as plain text to end-users, who need to use \nsimple or advanced search fea\u00adtures to browse through the rules. The usability and com\u00adprehension of \nthese rules can be signi.cantly improved by developing intuitive graphical interfaces that are integrated \nwith model-browsing capabilities. Reference [15] presents an interesting interactive debugging approach, \nin which a developer can select questions about program output from a set of Why did? and Why did not? \nqueries that are derived using static and dynamic analyses. A similar kind of interface could be developed \nfor understanding why ele\u00adments are generated in output models. Improvements to code analysis The static \nanalysis per\u00adformed by XYLEM could be improved to perform better analysis of array-index exceptions, \ncollection classes, and include additional runtime exceptions. The analysis could also be improved to \ncompute better constraints in the pres\u00adence of calls to external methods. Currently, the conditions on \nreturn values from external method calls are inlined in the constraints, if the parameters of those calls \nhave depen\u00addences on inputs. However, users might .nd such constraints dif.cult to understand especially \nif the transform code is not available for inspection. Future improvements could also combine static \nanalysis with dynamic information gathered from transform executions to provide more accurate results \nto users.  References [1] B. Baudry, S. Ghosh, F. Fleurey, R. France, Y. Le Traon, and J.-M. Mottu. \nBarriers to systematic model transformation testing. Communications of the ACM, 2009. To appear. [2] \nI. D. Baxter. Design maintenance systems. Communications of the ACM, 35(4):73 89, April 1992. [3] E. \nBrottier, F. Fleurey, B. Baudry, and Y. Le Traon. Metamodel-based test generation for model transformations: \nAn algorithm and a tool. In Proc. of the 17th Intl. Symp.on Softw. Reliability Eng., pages 85 94, November \n2006. [4] R. P. L. Buse and W. R. Weimer. Automatic documentation inference for exceptions. In Proc. \nof the Intl. Symp. on Softw. Testing and Analysis, pages 273 281, July 2008. [5] J.Cabot and R.Claris\u00b4o. \nUML/OCL veri.cation in practice. In Proc. of the 1st Intl. Workshop on Challenges in Model-Driven Softw. \nEng., pages 31 35, September 2008. [6] S. Chandra, S. J. Fink, and M. Sridharan. Snugglebug: A powerful \napproach to weakest preconditions. In Proc. of the ACM SIGPLAN Conf. on Prog. Lang. Design and Impl., \npages 363 374, June 2009. [7] T. T. Dinh-Trong, S. Ghosh, and R. B. France. A systematic approach to \ngenerate inputs to test UML design models. In Proc. of the 17th Intl. Symp.on Softw. Reliability Eng., \npages 95 104, November 2006. [8] C. Flanagan, K. R. M. Leino, M. Lillibridge, G. Nelson, J. B. Saxe, \nand R. Stata. Extended static checking for Java. In Proc. of the ACM SIGPLAN Conf. on Prog. Lang. Design \nand Impl., pages 234 245, June 2002. [9] F. Fleurey, J. Steel, and B. Baudry. Validation in model-driven \nengineering: Testing model transformations. In Proc. of the 1st Intl. Workshop on Model, Design and Validation, \npages 29 40, November 2004. [10] D. S. Frankel. Model Driven Architecture: Applying MDA to Enterprise \nComputing. John Wiley and Sons, 2003. [11] S. Ghosh, R. France, C. Braganza, N. Kawane, A. Andrews, and \nO. Pilskalns. Test adequacy assessment for UML design model testing. In Proc. of the 14th Intl. Symp.on \nSoftw. Reliability Eng., pages 332 346, November 2003. [12] H. Giese, S. Glesner, J. Leitner, W. Sch\u00a8afer, \nand R. Wagner. Towards veri.ed model transformations. In Proc. of the 3rd Workshop on Model Design and \nValidation, pages 78 93, October 2006. [13] P. Godefroid, N. Klarlund, and K. Sen. DART: Directed Automated \nRandom Testing. In Proc. of the ACM SIGPLAN Conf. on Prog. Lang. Design and Impl., pages 213 223, June \n2005. [14] J. C. King. Symbolic execution and program testing. Commu\u00adnications of the ACM, 19(7):385 \n394, July 1976. [15] A. J. Ko and B. A. Myers. Debugging reinvented: Asking and answering why and why \nnot questions about program behavior. In Proc. of the 30th Intl. Conf. on Softw. Eng., pages 301 310, \nMay 2008. [16] Jochen M. K\u00a8uster and Mohamed Abd el razik. Validation of model transformations -First \nexperiences using a white box approach. In Proc. of the 3rd Workshop on Model Design and Validation, \npages 62 77, October 2006. [17] W. Landi and B. G. Ryder. A safe approximate algorithm for interprocedural \npointer aliasing. In Proc. of the ACM SIGPLAN Conf. on Prog. Lang. Design and Impl., pages 235 248, June \n1992. [18] K. Lano and D. Clark. Model transformation speci.cation and veri.cation. In Proc. of the 8th \nIntl. Conf. on Quality Softw., pages 45 54, August 2008. [19] J.-M. Mottu, B. Baudry, and Y. Le Traon. \nMutation analy\u00adsis testing for model transformations. In Proc. of the Model Driven Architecture Foundations \nand Applications, 2nd Eu\u00adropean Conf., volume 4066 of Lecture Notes in Computer Sci\u00adence, pages 396 390, \n2006. [20] J.-M. Mottu, B. Baudry, and Y. Le Traon. Reusable MDA components: A testing-for-trust approach. \nIn Proc. of the 9th Intl. Conf. on Model Driven Eng. Lang. and Syst., volume 4199 of Lecture Notes in \nComputer Science, pages 589 603, 2006. [21] M. Nanda, C. Grothoff, and S. Chandra. Deriving object typestates \nin the presence of inter-object references. In Proc. of the 20th ACM SIGPLAN Conf. on Object-Oriented \nProg., Syst., Lang., and Applications, pages 77 96, October 2005. [22] M. G. Nanda and S. Sinha. Accurate \ninterprocedural null\u00addereference analysis for Java. In Proc. of the 31st Intl. Conf. on Softw. Eng., \npages 133 143, May 2009. [23] A. Narayanan and G. Karsai. Towards verifying model trans\u00adformations. Electron. \nNotes Theor. Comput. Sci., 211:191 200, 2008. [24] D. C. Schmidt. Model-driven engineering. IEEE Computer, \n39(2):25 31, February 2006. [25] W. Visser, C. S. P.as.areanu, and S. Khurshid. Test input generation \nwith Java PathFinder. In Proc. of the Intl. Symp. on Softw. Testing and Analysis, pages 97 107, July \n2004. [26] L. J. Zhang, N. Zhou, Y. M. Chee, A. Jalaldeen, K. Ponnalagu, R. R. Sindhgatta, A. Arsanjani, \nand F. Bernardini. SOMA-ME: A platform for the model-driven design of SOA solu\u00adtions. IBM Systems Journal, \n47(3):397 413, 2008.  \n\t\t\t", "proc_id": "1640089", "abstract": "<p>Model-driven development (MDD) is widely used to develop modern business applications. MDD involves creating models at different levels of abstractions. Starting with models of domain concepts, these abstractions are successively refined, using transforms, to design-level models and, eventually, code-level artifacts. Although many tools exist that support transform creation and verification, tools that help users in understanding and using transforms are rare. In this paper, we present an approach for assisting users in understanding model transformations and debugging their input models. We use automated program-analysis techniques to analyze the transform code and compute constraints under which a transformation may fail or be incomplete. These code-level constraints are mapped to the input model elements to generate model-level rules. The rules can be used to validate whether an input model violates transform constraints, and to support general user queries about a transformation. We have implemented the analysis in a tool called XYLEM. We present empirical results, which indicate that (1) our approach can be effective in inferring useful rules, and (2) the rules let users efficiently diagnose a failing transformation without examining the transform source code.</p>", "authors": [{"name": "Mangala Gowri Nanda", "author_profile_id": "81100183505", "affiliation": "IBM Research India, New Delhi, India", "person_id": "P1728781", "email_address": "", "orcid_id": ""}, {"name": "Senthil Mani", "author_profile_id": "81387601272", "affiliation": "IBM Research India, New Delhi, India", "person_id": "P1728782", "email_address": "", "orcid_id": ""}, {"name": "Vibha Singhal Sinha", "author_profile_id": "81314485489", "affiliation": "IBM Research India, New Delhi, India", "person_id": "P1728783", "email_address": "", "orcid_id": ""}, {"name": "Saurabh Sinha", "author_profile_id": "81100336015", "affiliation": "IBM Research India, New Delhi, India", "person_id": "P1728784", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1640089.1640114", "year": "2009", "article_id": "1640114", "conference": "OOPSLA", "title": "Demystifying model transformations: an approach based on automated rule inference", "url": "http://dl.acm.org/citation.cfm?id=1640114"}