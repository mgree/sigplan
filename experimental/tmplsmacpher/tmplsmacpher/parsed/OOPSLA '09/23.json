{"article_publication_date": "10-25-2009", "fulltext": "\n Minimizing Dependencies within Generic Classes for Faster and Smaller Programs Dan Tsafrir Robert W. \nWisniewski IBM T.J. Watson Research Center {dants,bobww,bacon}@us.ibm.com Abstract Generic classes can \nbe used to improve performance by al\u00adlowing compile-time polymorphism. But the applicability of compile-time \npolymorphism is narrower than that of run\u00adtime polymorphism, and it might bloat the object code. We advocate \na programming principle whereby a generic class should be implemented in a way that minimizes the depen\u00addencies \nbetween its members (nested types, methods) and its generic type parameters. Conforming to this principle \n(1) re\u00adduces the bloat and (2) gives rise to a previously unconceived manner of using the language that \nexpands the applicabil\u00adity of compile-time polymorphism to a wider range of prob\u00adlems. Our contribution \nis thus a programming technique that generates faster and smaller programs. We apply our ideas to GCC \ns STL containers and iterators, and we demonstrate notable speedups and reduction in object code size \n(real ap\u00adplication runs 1.2x to 2.1x faster and STL code is 1x to 25x smaller). We conclude that standard \ngeneric APIs (like STL) should be amended to re.ect the proposed principle in the interest of ef.ciency \nand compactness. Such modi.cations will not break old code, simply increase .exibility. Our .nd\u00adings \napply to languages like C++, C#, and D, which realize generic programming through multiple instantiations. \nCategories and Subject Descriptors D.3.3 [Programming Languages]: Polymorphism; D.3.3 [Programming Lan\u00adguages]: \nData types and structures General Terms Design, measurement, performance Keywords Generics, templates, \nSCARY assignments and initializations, generalized hoisting 1. Introduction Generic programming is supported \nby most contempo\u00adrary programming languages [24] to achieve such goals as Permission to make digital \nor hard copies of all or part of this work for personal or classroom use is granted without fee provided \nthat copies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. OOPSLA 2009, October 25 29, 2009, Orlando, \nFlorida, USA Copyright c.2009 ACM 978-1-60558-734-9/09/10. . . $10.00 David F. Bacon Bjarne Stroustrup. \n.Texas A&#38;M University bs@cs.tamu.edu compile-time type safety. In languages like C++, C#, and D, \ngeneric programming also allows for improved performance through compile-time polymorphism as follows \n[48, 36]. Rather than generating only one version of the code (by us\u00ading dynamic binding to hide the \ndifferences between type parameters), the compiler emits a different code instantia\u00adtion for each new \ncombination of the parameterized types. It is therefore able to perform static binding, which enables \na host of otherwise inapplicable optimizations, notably, those based on inlining. The price is a potential \nincrease in object code size, sometimes denoted as bloat [8, 29, 4].1 Generic classes often utilize nested \ntypes when de.ning their interface [9, 25]. A notable example is the iterators of STL, the ISO C++ Standard \nTemplate Library. STL is among the most widely used generic frameworks. We will use it throughout this \npaper to demonstrate our ideas (in Section 8 we will generalize to other libraries/languages). The iterator \nconcept is interwoven in almost every aspect of STL. Nested classes implicitly depend on all the generic \npa\u00adrameters of the outer class in which they nest. Consider for example the STL sorted container std::set<T,C,A> \n(which stores items of the type T, compares items with a compara\u00adtor of the type C, and (de)allocates \nmemory with an allocator of the type A). If two sets agree on T but disagree on C or A, then the corresponding \nnested iterators are of different types. This means that the code snippet in Figure 1 does not typically \ncompile due to type-mismatch errors. set<int,C1,A1>::iterator i1; set<int,C2,A1>::iteratori2 =i1;// di.erentcomparator \nset<int,C1,A2>::iteratori3 =i1;// di.erentallocator Figure 1. Can this code have a valid meaning? Can \nit be com\u00adpiled by existing compilers? Can it be useful? And indeed, our repeated experience is that, \nwhen pre\u00adsented with Figure 1, well-read and experienced program\u00ad mers initially react negatively and \nfeel that this code snip\u00adpet is in .agrant violation of the type system. When further presented with \na hypothetical possibility that the snippet 1 The resulting generated code can actually be smaller than \nwhat is obtained when using dynamic binding; but this is unrelated to our de.nition of bloat , which \nis the increase in size caused by additional instantiations. might nevertheless compile on existing \ncompilers, they do not understand the semantics of the code, and they fail to see why it could ever be \nuseful. This paper is dedicated to refuting the perception of pro\u00adgrammers regarding Figure 1. Speci.cally, \nwe show that it is possible (and rather easy) to implement the nested type (the iterator) and its encapsulating \nclass (the container) in a way that makes Figure 1 be ISO standard conforming and accepted by existing \nunmodi.ed C++ compilers. We further show that doing so is highly bene.cial, because it yields a superior \ndesign that has two important advantages: 1. it emits less code when instantiating generic algorithms, \nand so it yields smaller executables, and 2. it allows us to write faster programs and improve the performance, \nby utilizing statements as in Figure 1.  Consequently, the answer to the questions raised in the cap\u00adtion \nof Figure 1 is yes . 1.1 Minimizing Dependencies Let us denote assignments and initializations like \nthose shown in Figure 1 as SCARY assignments .2 We contend that a container design that explicitly allows \nSCARY assignments to compile is more correct than a de\u00adsign that does not allow them. The well-known \ndesign prin\u00adciple that underlies this claim is that independent concepts should be independently represented \nand should be com\u00adbined only when needed [49]. The inability to compile Fig\u00ad ure 1 serves as indication \nthat this principle was violated, because it proves that iterators depend on comparators and allocators, \nwhereas STL iterators need not depend on com\u00adparators or allocators, as there is nothing in the ISO C++ \nspeci.cation that indicates otherwise. We note that the only meaningful implication of such un\u00adwarranted \ndependencies is that SCARY assignments do not compile and so the aforementioned bene.ts (reduced bloat, \nbetter performance) are prevented. The fact that the speci.\u00adcation of ISO C++ is silent regarding this \nissue (namely, it does not specify whether or not iterators should depend on comparators and allocators) \nattests the lack of awareness to our proposed approach and its bene.ts. Technically, the unwarranted \ndependencies can be easily eliminated by moving the de.nition of the nested iterator to an external scope \nand replacing it with an alias to the now\u00adexternal iterator; by using only T as its generic parameter, \nwe eliminate the unwarranted dependencies. Doing so al\u00adlows Figure 1 to compile under unmodi.ed compilers \nand provides semantics to its SCARY assignments: the iterators i1, i2, and i3have the same type, which \nis a generic class that only depends on T. The iterators thus become interchange\u00ad 2 The acronym SCARY \ndescribes assignments and initializations that are Seemingly erroneous (appearing Constrained by con.icting \ngeneric param\u00adeters), but Actually work with the Right implementation (unconstrained bY the con.ict due \nto minimized dependencies). able, regardless of the comparators and allocators utilized by the associated \ncontainers. So while the SCARY assignments appear new and possibly counterintuitive, there is no need \nto invent new semantics and to modify the compiler and lan\u00adguage in order to make them work. 1.2 Improving \nPerformance When programmers need to handle objects with different types in a uniform manner, they typically \nintroduce an ab\u00adstraction layer that masks the differences between the types. For example, to uniformly \nhandle a Circle and a Trian\u00adgle , we use runtime polymorphism and make them part of a class hierarchy \nheaded by an abstract Shape base class. The same technique (introducing an abstract base class) is used \nto handle iterators with different types in a uniform manner. But when dependencies are minimized as \nadvocated above, the type differences may no longer exist, making iter\u00adators interchangeable and obviating \nthe need for abstraction and runtime polymorphism. (This is analogous to discover\u00ading that Circle and \nTriangle actually have the same type and are in fact interchangeable.) As noted, runtime polymor\u00adphism \nincurs a performance penalty (e.g., hindering inlin\u00ading), which is avoided if compile-time polymorphism \nand static binding are employed instead. This is the source of our performance improvement. Notice, however, \nthat the improvement is not merely the result of minimizing dependencies, which is necessary but insuf.cient \nfor this purpose. Rather, programmers must pro\u00adgram in a certain way: they must utilize SCARY assign\u00adments, \nas these constitute the only way by which the inter\u00adchangeability can be exploited to improve the performance. \nIn Sections 2 and 3 we show how to solve the classical multi-index database problem without and with \nSCARY as\u00adsignments, and we highlight the advantages of the latter ap\u00adproach. In Section 4 we evaluate \nthe competing designs us\u00ad ing microbenchmarks and a real application, and we demon\u00adstrate speedups between \n1.2x to 2.1x for the application. 1.3 The Need for Standardization Since the above bene.ts are nonnegligible \nand since obtain\u00ading them is nearly effortless, we contend that classes should be implemented to allow \nSCARY assignments. But this is not enough. We further contend that ability to utilize SCARY assignments \nshould be speci.ed as part of the API; other\u00adwise, their use would be nonportable and might break with \ndifferent or future versions of an implementation. The general conclusion is that designers should be \nmind\u00adful when utilizing nested types as part of the interface. Speci.cally, they should aspire to minimize \nthe dependen\u00adcies between the inner classes and the type parameters, and they should specify interfaces \nto re.ect that. This will not break existing code. Rather, it would provide programmers with the .exibility \nto leverage the interchangeability, and, as discuss next, it would eliminate code bloat caused by over\u00adconstrained \ninner classes. # vendor compiler operating system iterator 1 Intel C++ Compiler 11.0 Professional (ICC) \nWindows dependent 2 Microsoft Visual C++ 2008 (VC++) Windows dependent 3 IBM XL C/C++ V10.1 (xlC) AIX \ndependent 4 Sun Sun Studio 12 C++ 5.9 OpenSolaris, Linux dependent 5 Borland CodeGear C++ Builder 2009 \nWindows dependent 6 GNU GCC 4.3.3 *NIX not dependent 7 Intel C++ Compiler 11.0 Professional (ICC) Linux \n(using the STL of GCC) not dependent 8 IBM XL C/C++ V10.1 (xlC) Linux (using the STL of GCC) not dependent \n Table 1. Iterators may be declared as inner or outer, and therefore they may or may not depend on the \ncomparator and allocator; the compiler s vendor is free to make an arbitrary decision. Until now, this \nhas been a non-issue. (Listing includes most recent compiler versions as of Feb 2009. The iterator column \nis based on the default compilation mode. Borland has recently sold CodeGear to Embarcadero Tech.) 1.4 \nReducing Code Bloat Replacing inner classes with aliases that minimize depen\u00addencies reduces code bloat \nfor two reasons. First, it uni.es redundant multiple instantiations of the inner classes. With\u00adout this \nuni.cation, member methods of a nested iterator could be instantiated once for each comparator and alloca\u00adtor \ncombination, even though all instantiations yield identi\u00adcal object code. The second, more important, \nreason is that any generic algorithm for which the inner class serves as a type parameter would, likewise, \nbe uselessly duplicated. For example, iterators are used to parameterize most STL algo\u00adrithms (e.g., \nstd::copy, std::.nd, std::sort, etc.). When such an algorithm is used, any change in the iterator type \nwill prompt another algorithm instantiation, even if the change is meaningless. Reducing bloat by replacing \ninner classes with aliases can be further generalized to also apply to member methods of generic classes, \nwhich, like nested types, might uselessly de\u00adpend on certain type parameters simply because they reside \nwithin a generic class s scope. (Again, causing the compiler to uselessly generate many identical or \nnearly-identical in\u00adstantiations of the same method.) To solve this problem we propose a generalized \nhoisting design paradigm, which decomposes a generic class into a hierarchy that eliminates unneeded \ndependencies. We de.ne this technique in Sec\u00adtion 5, apply it to standard GCC/STL containers in Section \n6, and show that the resulting code can be up to 25x smaller. 1.5 Generalizing We note that generalized \nhoisting is not just useful to min\u00adimize dependencies between member methods and generic parameters; \nit can also be similarly applied as an alterna\u00adtive way to minimize dependencies between member classes \n(that is, inner classes) and generic parameters. Accord\u00ading to this doctrine, instead of moving the iterator \nde.ni\u00adtion to an external scope, we could (1) de.ne a base class for std::set<T,C,A> that is parametrized \nby only T, and (2) move the iterator de.nition, as is, to this base class. Con\u00adsequently, generalized \nhoisting can be viewed as a general\u00adization of our idea from Section 1.1.  1.6 Contributions and Paper \nRoadmap The novelty of our work is not in coming up with a technical way to reduce the dependencies between \ninner classes and type parameters (see Table 1). Rather, it is (1) in identifying that this issue matters, \n(2) in recognizing that minimizing dependencies between the members and the type parameters of a generic \nclass is a valuable design principle that can be utilized to improve performance and reduce bloat, (3) \nin conceiving SCARY assignments and generalized hoisting that make it possible to realize and exploit \nthis principle, and (4) in doing the experimental work that quanti.es the bene.ts and substantiates the \ncase. To summarize, our contribution is a technique that can reduce the amount of emitted generic code \nand make it run faster. This statement is supported by Sections 2 6 (as described above) in the context \nof C++. We then discuss how the compiler and language can provide support to our ideas (Section 7), we \ngeneralize our results to other programming languages (Section 8), we discuss related work (Section 9), \nand we conclude (Section 10).  2. Motivation In this section we describe the problem chosen to demon\u00adstrate \nthe bene.ts of the technique we propose (Section 2.1). We then describe the two standard ways to solve \nthe problem (Sections 2.2 and 2.3). In Section 3 we will develop a third, nonstandard, solution that \nutilizes SCARY assignments, and we will compare it to the latter two. The three solutions are short, \nwhich allows us to provide their full (compiling) code, promoting clarity, and, more importantly, allowing \nus to precisely identify the reasons for the performance bene.ts of our approach. 2.1 The Problem In \na nutshell, what we want is a database of items that (1) is sorted in different ways to allow for different \ntraver\u00adsal orders, and that (2) supports ef.cient item insertion, re\u00admoval, and lookup. Numerous applications \nmake use of such databases. For brevity, we assume that the items are integers (these may serve as handles \nto the associated objects). Let operation return complexity description add (int i) void O(K \u00b7 logN) \nadd ito the database del (int i) void O(K \u00b7 logN) delete ifrom the database begin (int k) Iter t O(1) \nreturn iterator to beginning of database when sorted by the k-th sorting criterion end (int k) Iter t \nO(1) return iterator to end of database when sorted by the k-th sorting criterion .nd (int k, int i) \nIter t O(logN) return iterator to iwithin sequence that starts with begin(k), return end(k) if not found \n Table 2. The operations we require our database to support. The variable i denotes an item and may hold \nany value. The variable k denotes a sorting criteria and is in the range k = 0, 1,..., K-1. The Iter \nt type supports the standard pointer-like iterator operations. K denote the number of different sorting \ncriteria, and let N denote the number of items that currently populate the database. Table 2 speci.es \nthe database operations and their required runtime complexity. The operation add and del respectively \nadd and delete one item to/from the database and do so in O(K \u00b7 logN ) time. The operations begin and \nend respectively return the beginning and end of the sequence of items that populate the database, sorted \nby the k-th sorting criterion. Both oper\u00adations return an object of the type Iter t, which is an iterator \nthat supports the usual iterator interface (of primitive point\u00aders) similarly to all the STL containers; \ne.g., Figure 2 shows how to print all the items ordered by the k-th sorting crite\u00adrion. All the operations \nin Figure 2 are O(1), including begin and end and the iterator operations (initialization = , in\u00adequality \n!= , increment ++ , and dereference * ). Con\u00adsequently, the entire traversal is done in O(N )time. Iter \nt b = db.begin(k); Iter t e = db.end(k); for(Iter t p=b; p != e; ++p) {printf( %d , *p); } Figure 2. \nIterating through the multi-index database using the k-th sorting criterion and printing all items. The \nlast supported operation is .nd, which searches for i within the sequence of database items sorted by \nthe k-th criterion. If i is found, the associated iterator is returned (dereferencing this iterator would \nyield i); otherwise, end(k) is returned. Thus, if users just want to check whether or not i is found \nin the database (and do not intend to use the re\u00adturned iterator), they can arbitrarily use, e.g., k=0, \nas below: if( db..nd(0,i) != db.end(0)) {/*found! ... */} (An arbitrary kcan be used, because .nding \niin some sorted sequence means iis found in all the other sequences.) Users may alternatively be interested \nin the returned iterator of some speci.c k, e.g., if they want to examine the neighbors of iaccording \nto a speci.c order. The runtime complexity of .nd is O(logN ).  2.2 Using an Abstract Iterator If the \nstated problem appears familiar, it is because it is sim\u00adilar to the problem that motivates the classic \niterator design pattern as de.ned in the seminal work by Gamma et al. [23, pp. 257 271] and as illustrated \nin Figure 3. The solution is  Figure 3. The classic iterator design pattern. While the notation is adapted \nto match that of STL iterators, the latter do not model the classic pattern, and they have a narrower \napplicability. also similar. We are going to implement each sorting cri\u00adterion as a container ( concrete \naggregate ) that is sorted differently. Naturally, we are going to use STL containers (these are readily \navailable and provide performance similar to that of hand-specialized code), such that each container \nemploys a different comparator. But different comparator types imply different iterator types, whereas \nTable 2 dictates just one iterator type for all sorting criteria. We therefore have no choice but to \nutilize an abstract iterator base class in order to hide the type differences as shown in Figure 3. We \nstress that, contrary to common belief [17, 19, 53], C++/STL iterators do not model the classic design \npattern. They do not involve runtime polymorphism and dynamic binding, there is no iterator base class, \nand different con\u00adtainers have different iterators that do not share a common ancestor. STL iterators \nare thus more performant (facilitate inlining), but they are applicable to a narrower range of prob\u00adlems. \nIn particular, they are not applicable to our problem, which requires dynamic binding as illustrated \nin Figure 3. Figures 4 8 include the complete database implementa\u00ad tion, and Figure 9 exempli.es how \nto de.ne one database instance. We shall now address these .gures one by one. Figure 4 shows Sorter t, \nwhich is the abstract aggre\u00adgate interface (for each sorting criterion there will be one Sorter t). Figure \n5 uses Sorter t to implement the database in a straightforward way. If Sorter t s insertion, deletion, \nand lookup are O(logN ), and if its beginand end are O(1), then the database meets our complexity requirements \n(Table 2).    struct Sorter_t { virtual ~Sorter_t() {} virtual void add (int i) = 0; virtual void \ndel (int i) = 0; virtual Iter_t find (int i) = 0; virtual Iter_t begin() = 0; virtual Iter_t end () = \n0; }; Figure 4. The aggregate (pure virtual interface). // IB_t = Iterator Base Type // IA_t = Iterator \nAdapter Type struct IB_t { virtual ~IB_t() {} virtual bool operator!=(const IB_t&#38; r)= 0; virtual \nIB_t&#38; operator= (const IB_t&#38; r)= 0; virtual IB_t&#38; operator++() = 0; virtual int operator* \n() = 0; virtual IB_t* clone () const = 0; }; struct Database_t { std::vector<Sorter_t*> v; const int \nK; Database_t(const std::vector<Sorter_t*>&#38; u) : v(u), K(u.size()) { } void add (int i) { for(int \nk=0; k<K; k++) v[k]->add(i); } void del (int i) { for(int k=0; k<K; k++) v[k]->del(i); } Iter_t find \n(int k, int i) { return v[k]->find(i); } Iter_t begin(int k) { return v[k]->begin(); } Iter_t end (int \nk) { return v[k]->end(); } }; Figure 5. The database encapsulates a vector of aggregates. template <typename \nIntSetIter_t> struct IA_t : public IB_t { IntSetIter_t i; const IA_t&#38; dc(const IB_t&#38; r) // dc \n= downcast (IB_t to IA_t) { return *dynamic_cast<const IA_t*>(&#38;r); } IA_t(IntSetIter_t iter) : i(iter) \n{} virtual bool operator!=(const IB_t&#38; r) { return i != dc(r).i; } virtual IB_t&#38; operator= (const \nIB_t&#38; r) { i=dc(r).i; return *this;} virtual IB_t&#38; operator++() { ++i; return *this; } virtual \nint operator* () { return *i; } virtual IB_t* clone () const { return new IA_t(i); } }; Figure 6. Left: \nthe abstract (pure virtual) iterator interface IB t. Right: a concrete implementation IA t of the iterator \ninterface. As the latter is generic, it in fact constitutes a family of concrete implementations. Speci.cally, \nit adapts any std::set<int,C>::iterator to the IB t  interface, regardless of the speci.c type of the \ncomparator C. struct Iter_t { IB_t *p; Iter_t(const Iter_t&#38; i) {p=i.p->clone(); } Iter_t(const IB_t&#38; \ni) {p=i.clone(); } ~Iter_t() {delete p; p=0; } bool operator!=(const Iter_t&#38; r) {return *p != *r.p; \n} Iter_t&#38; operator++() {++(*p); return *this;} int operator* () {return **p; } Iter_t&#38; operator= \n(const Iter_t&#38; r) {delete p; p=r.p->clone(); return *this;} }; template<typename IntSet_t> struct \nContainer_t : public Sorter_t { IntSet_t s; typedef typename IntSet_t::iterator INative_t; Iter_t wrap(const \nINative_t&#38; i) {return Iter_t( IA_t<INative_t>(i) );} Container_t() {} virtual void add (int i) {s.insert(i); \n} virtual void del (int i) {s.erase(i); } virtual Iter_t find (int i) {return wrap(s.find(i));} virtual \nIter_t begin() {return wrap(s.begin());} virtual Iter_t end () {return wrap(s.end() );} }; Figure 7. \nThe Iter t proxy rids users from the need to work with Figure 8. The generic (template) Container t adapts \nany pointers to iterators, and from having to explicitly deallocate them. std::set<int,C> type to the \nSorter t interface, regardless of the This is the class which is used in Figures 4 5. type of C. Figure \n6 (left) shows IB t, which stands for iterator base STL containers and iterators as is. We must adapt \nthem to type . This is the abstract iterator interface. It declares all the our interfaces. Figure 6 \n(right) shows IA t, which stands pointer-like iterator operations as pure virtual. For reasons for iterator \nadapter type . This generic class adapts any to be shortly addressed, IB t is not the iterator type used \nin set<int,C>::iterator type to the IB t interface, regardless Figures 4 5. For the same reasons, in \naddition to the pointer-of the actual type of the comparator C. Having to handle like operations, IB \nt also declares the clone operation, which different iterator types necessitates IA t s genericity. returns \na pointer to a copy of the iterator object; the copy IB t and IA t are seemingly all that we need to \ncomplete resides on the heap and is allocated with new. the implementation of our database. But technically \nruntime As noted, the concrete iterators and containers we use polymorphism only works through pointers \nor references, as examples are the highly optimized STL containers and typically to heap objects. While \nin principle we could de.ne iterators. STL std::sets are suitable for our purposes, as Iter t (from Table \n2) to be a pointer, this would place the they sort unique items by user-supplied criteria, and they burden \nof explicitly delete-ing iterators on the users, which meet our complexity requirements. However, we \ncannot use is unacceptable. The solution is to de.ne Iter t as a proxy to        struct lt { \nbool operator() (int x, int y) const {return x < y;} }; struct gt { bool operator() (int x, int y) const \n{return x > y;} }; Container_t< std::set<int,lt> > cont_lt; Container_t< std::set<int,gt> > cont_gt; \n std::vector<Sorter_t*> v; v.push_back( &#38;cont_lt ); v.push_back( &#38;cont_gt ); Database_t db(v); \nFigure 9. Creating a database that utilizes two sorting criteria, under the design that abstracts the \niterator, which is implemented in Figures 4 8. (Variables with lt or gt types are function objects.) \nan IB t pointer, as shown in Figure 7. We can see that Iter t manages the pointer without any user intervention. \nFigure 8 completes the picture by showing Container t, the generic class that adapts any std::set<int,C> \ntype to the Sorter t interface, regardless of the type of C. Once again, having to handle different std::set \ntypes means Con\u00adtainer t must be generic. Notice how Container t uses its wrap method to transform the \nSTL iterator into an Iter t. Figure 9 demonstrates how the database may be de.ned. This example uses \ntwo sorting criteria in the form of two comparator classes: lt (less than) and gt (greater than), re\u00adsulting \nin ascending and descending sequences. Two corre\u00adsponding std::sets are de.ned and adapted to the Sorter \ntin\u00adterface by using the generic Container t. Although the two containers have different types, they \nhave a common ances\u00adtor (Sorter t), which means that they can both reside in the vector v that is passed \nto the database constructor. 2.2.1 Drawbacks of Using an Abstract Iterator Our Database t has some attractive \nproperties. It ef.ciently supports a simple, yet powerful set of operations (as listed in Table 2), and \nit is .exible, allowing to easily con.gure arbi\u00ad trary collections of sorting criteria. The price is \nthe overhead of abstraction and of runtime polymorphism. Let us compare the overheads of using a database \nthat has only one sorting criterion (K=1) to using a native std::set directly. Obviously, the two are \nfunctionally equivalent, but there are several sources of added overhead. Firstly, the .ve set operations \nlisted in Table 2 require an additional virtual function call, as they are invoked through the Sorter \nt base class. Conversely, when using std::sets to invoke the same operations, no virtual calls are involved. \nSecondly, those operations that return an iterator require dynamic memory allocation through new; this \nmemory is later deleted when the iterators go out of scope. In contrast, std::sets do not invoke new \nor deletein these operations. Finally, every iterator operation (increment, dereference, equality test, \nand assignment) incurs an additional virtual       bool cmp_lt(int x, int y) {return x < y;} bool \ncmp_gt(int x, int y) {return x > y;} typedef bool (*CmpFunc_t) (int x, int y); typedef std::set<int,CmpFunc_t> \nSorter_t; typedef Sorter_t::iterator Iter_t; Sorter_t set_lt( cmp_lt ); Sorter_t set_gt( cmp_gt ); std::vector<Sorter_t*> \nv; v.push_back( &#38;set_lt ); v.push_back( &#38;set_gt ); Database_t db(v); Figure 10. Creating the \ndatabase with the design that abstracts the comparator is simpler, requiring only Figure 5 and three \naddi\u00ad tional type de.nitions. (Compare with Figure 9.) call overhead when (indirectly) used through the \nIB t inter\u00adface. This price might be heavy when compared to native std::set iterators, because the latter \nare not only non-virtual, but are also inlined. The overhead is magni.ed by the fact that iterator operations \nare typically sequentially invoked for N times when traversing the items of the container.  2.3 Using \nan Abstract Comparator There is a second standard way to implement our database, which is far simpler. \nIn Section 2.2, we used a collection of std::set<int,C> containers with different C comparators in order \nto implement the different sorting criteria. This mandated us to deal with the fact that the containers \n(and associated iterators) have different types. We have done so by abstracting the variance away through \nthe use of the aggregate and iterator interfaces (Sorter t and IB t), and by adapting the std::sets and \ntheir iterators to these interfaces. Multiple sorting criteria may be alternatively imple\u00admented by abstracting \nthe comparator C, such that all std::sets use the same comparator type, but each is associ\u00adated with \na different comparator instance. As the sets agree on C, they have identical types, and so their iterators \nhave identical types too. Our implementation would therefore be exempt from handling type variance. Indeed, \nto implement this design, we only need the code in Figure 5 along with the following three type de.nitions: \ntypedefbool(*CmpFunc t)(int x,inty); typedef std::set<int,CmpFunc t> Sorter t; typedef Sorter t::iterator \nIter t; (we no longer need the code in Figures 4, 6, 7, and 8). A CmpFunc t variable can hold pointer-to-functions \nthat take two integers as input and return true iff the .rst is smaller than the second. The variable \nis not bound to a spe\u00adci.c value and thus abstracts the comparator away.3 Accord\u00ad 3 If we need to add \nstate to the comparison functions and turn them into ob\u00adject functions, we could do so by using a comparator \nthat has a CmpFunc t       ingly, we de.ne the Sorter t type as set<int,CmpFunc t>, which eliminates \nthe need for Figures 4 and 8. We likewise de.ne the iterator Iter tto be set<int,CmpFunc t>::iterator, \nwhich eliminates the need for Figures 6 and 7. Figure 10 shows how the new implementation of the database \nmay be instantiated. Similarly to the example given in Figure 9, we use two sorting criteria. But this \ntime, instead of function objects, we use ordinary functions: cmp lt and cmp gt; both have a prototype \nthat agrees with the Cmp-Func t type, and so both can be passed to constructors of objects of the type \nSorter t. We next instantiate two objects of the Sorter t type, set ltand set gt, and, during their con\u00adstruction, \nwe provide them with the comparator functions that we have just de.ned. (Sorters and comparators are \nas\u00adsociated by their name). As planned, we end up with two objects that have the same type but employ \ndifferent com\u00adparator instances. We can therefore push the two objects to the vector v, which is passed \nto the database constructor. 2.3.1 Drawbacks of Using an Abstract Comparator At .rst glance, it appears \nthat abstracting the comparator yields a cleaner, shorter, and more elegant implementation than abstracting \nthe iterator (instead of Figures 4 8 we only need Figure 5). Moreover, abstracting the comparator does \nnot generate any of the overheads associated with abstract\u00ading the iterator (see Section 2.2.1), because \nwe do not use the abstraction layers depicted in Figure 3. It consequently seems as though abstracting \nthe comparator yields a solu\u00adtion that is superior in every respect. But this is not the case. There \nis a tradeoff involved. Sorted containers like std::set, which are required to deliver O(logN )performance, \nare inevitably implemented with balanced trees. When a new item is inserted to such a tree, it is compared \nagainst each item along the relevant tree path. If the comparator is abstracted, each comparison trans\u00adlates \nto a non-inlined function call (this is the price of run\u00adtime polymorphism). But if the comparator is \nnot abstracted, its code is typically inlined, as it is known at compile time. For example, in Figure \n9, comparisons resolve into a hand\u00ad ful of inlined machine operations. This observation applies to insertion, \ndeletion, and lookup. (Later, we quantify the penalty of abstract comparators and show it is signi.cant.) \nWe conclude that there are no clear winners. If users want to optimize for iteration, they should abstract \nthe compara\u00adtor. (Comparators do not affect the iteration mechanism in any way, as discussed in the next \nsection.) But if they want to optimize for insertion and deletion, they should abstract the iterator \ninstead. In the next section, we show that it is in fact possible to obtain the bene.ts of both approaches. \n  3. Independent Iterator: The New Approach Let us reconsider the two alternative database designs \nfrom the previous section. The speci.cation of the problem (Ta\u00ad data member, which is invoked in the \noperator() of the class [29].       ble 2) requires supporting iteration over the database accord\u00ad \ning to multiple sorting criteria using the same iterator type. We have utilized std::sets with different \ncomparators to al\u00adlow for the different sorting criteria, and we were therefore required to face the \nproblem of having multiple iterator types instead of one. The heart of the problem can be highlighted \nas follows. Given two comparator types C1 and C2, and given the fol\u00adlowing type de.nitions typedef std::set<int,C1> \nS1 t;// sorting criterion #1 typedef std::set<int,C2> S2 t;// sorting criterion #2 typedef S1 t::iterator \nI1 t; typedef S2 t::iterator I2 t; the iterator types I1 t and I2 t are different. In the previous section \nwe have dealt with this dif.culty by either 1. adapting I1 t and I2 t to an external iterator hierarchy \nrooted by a common ancestor which is an abstract iterator (Section 2.2), or by 2. morphing I1 tand I2 \ntinto being the same type, by favor\u00ading to use multiple instances of one abstract comparator type, over \nusing multiple types of comparators that are unrelated (Section 2.3).  Both solutions required trading \noff some form of compile\u00adtime polymorphism and excluded the corresponding inlin\u00ading opportunities. Importantly, \nthe need for abstraction has arisen due to a perception that has, so far, been undisputed: that if we \ninstantiate a generic class (std::set) with differ\u00adent type parameters (C1 and C2), then the type of \nthe cor\u00adresponding inner classes (I1 t and I2 t) will differ. We chal\u00adlenge this perception, both conceptually \nand technically. 3.1 The Conceptual Aspect As noted, the data structure underling std::sets is inevitably \na balanced search tree, because of the O(logN ) STL\u00admandated complexity requirement. A distinct feature \nof search trees is that the order of the items within them is exclusively dictated by the structure of \nthe tree [12]. Speci.\u00ad cally, by de.nition, the minimal item in a tree is the leftmost node; and (assuming \nthe search tree is binary) the successor of each node x is the leftmost item in the subtree rooted by \nx.right (if exists), or the parent of the closest ancestor of x that is a left child. These two algorithms \n( minimal and successor ) completely determine the traversal order. And both of them never consult the \nkeys that reside within the nodes. Namely, the algorithms are entirely structure-based. As keys are not \nconsulted, then, obviously, the compara\u00adtor function associated with the tree (which operates on keys) \nis unneeded for realizing an in-order traversal. Like\u00adwise, as nodes are not created or destroyed within \nthe two algorithms, the memory allocator of the tree is unneeded too.         template<typename \nT, typename C, typename A> class set { public: class iterator { // code does not utilize C or A ... }; \n // ... }; template<typename T> class iterator { // ... }; template<typename T, typename C, typename \nA> class set { public: typedef iterator<T> iterator; // ... }; Figure 11. Left: the iterator is dependent \non the comparator C and the allocator A. Right: the iterator is independent. It follows that, by de.nition, \nin-order traversal is an ac\u00adtivity which is independent of comparators and allocators. And since iterators \nare the technical means to conduct such a traversal, then, conceptually, iterators should be indepen\u00addent \nof comparators and allocators too. In particular, there is no conceptual reason that requires std::sets \nthat disagree on comparators or allocators to have different iterator types.4  3.2 The Technical Aspect \nThe question is therefore whether we are able, technically, to eliminate the unwarranted dependencies \nand utilize a single iterator type for different integer std::sets that have different comparators or \nallocators. The answer is that we can as shown in Figure 11. All that is required is removing the code \nof the iterator class from within the internal scope of the set, placing it in an external scope, and \npreceding it with a template declaration that accurately re.ects the dependencies, including only the \nitem type T(integer in our example) and excluding the comparator and allocator types C and A. The removed \niterator code is then replaced with an alias (typedef) that points to the iterator de.nition that is \nnow external. The functionality of the alias is identical to that of the original class for all practical \npurposes.5 We conclude that our goal is achievable. Namely, it is possible to de.ne a nested class of \na generic class such that the nested class only depends on some, but not all, of the generic parameters. \nThus, there is no need to modify the language or the compiler. Rather, the issue is reduced to a mere \ntechnicality: how the generic class is implemented, or, in our case, how the STL is implemented. Table \n1 lists several mainstream compilers and speci.es if the std::set iterator class that they make available \n(in their default mode) is dependent on the comparator or allocator. It should now be clear that this \nspeci.cation is a product of the STL that is shipped with the compiler. Table 3 lists the four most widely \nused STL implementa\u00ad tions. All the compilers in Table 1 that are associated with a dependent iterator \nmake use of Dinkum STL; the excep\u00adtion is the compiler by Sun, which uses an implementation that is based \non an early commercial version of RogueWave 4 Technically, to share an iterator type, std::sets must \nagree on the fol\u00adlowing nested types and nothing else: value type (T), pointer (to T), const pointer \n(to T), and di.erence type (of subtracting two pointers). 5 Alternatively, we could have (1) de.ned a \nbase class for std::set that only depends on Tand (2) cut-and-pasted the iterator to the base class s \nscope.  STL iterator Dinkum dependent libstdc++ independent STLPort independent RogueWave both (depends \non version and mode) Table 3. Standard template library implementations. STL. Conversely, the compilers \nwith an independent iterator all make use of the GNU open source libstdc++ STL. Some compilers ship with \nmore than one STL implemen\u00adtation and allow users, through compilation .ags, to spec\u00adify whether they \nwant to use an alternative STL. For exam\u00adple, when supplied with the .ag -library=stlport4 , the Sun \ncompiler will switch from its commercial RogueWave-based implementation to STLport; the iterator will \nthen become in\u00addependent of the comparator and allocator. Interestingly, the iterator of the most recent \nRogueWave (open source) is dependent on or independent of the com\u00adparator and allocator, based on whether \nthe compilation is in debug or production mode, respectively. The reason is that, in debug mode, one \nof the generic parameters of the itera\u00adtor is the speci.c std::set type with which it is associated (which, \nin turn, depends on the comparator and allocator). The debug-iterator holds a pointer to the associated \nstd::set instance and performs various sanity checks using the be\u00adginand end methods (e.g., when the \niterator is dereferenced, it checks that it does not point to the end of the sequence). Such sanity checks \nare legitimate and can help during the de\u00advelopment process. But there is no need to make the iterator \ndependent on its std::set in order to perform these checks; this is just another example of an unwarranted \ndependency that delivers no real bene.t. Indeed, instead of the std::set, the iterator can point to the \nroot node of the balanced tree (which, as explained in Section 3.1, should not depend on the comparator \nand allocator); the begin and end of the tree are immediately accessible through this root. 3.3 The \nDatabase with an Independent Iterator To implement our database with the new approach we need Figures \n4, 5, and 8, as well as the following type de.nition typedef std::set<int, SomeC, SomeA>::iterator Iter \nt; It does not matter which C or A we use, because we as\u00adsume that the iterator do not depend on them. \nFigure 9 ex\u00ad empli.es how to instantiate this type of database. This is the same example we used in Section \n2.2 ( abstract itera\u00ad tor ). But, in contrast to Section 2.2, we now do not need Figures 6 7 (the external \niterator hierarchy), because all set<int,C,A>::iterator types are one and the same regard\u00adless of C or \nA, and so there is no reason to introduce an abstraction layer to hide the differences.  Importantly, \nnotice that, with the current type de.nition of Iter t, we now use SCARY assignments in all the .gures \ninvolved (4, 5, and 8). Speci.cally, every return statement in every method within these .gures that \nhas an Iter t return\u00adtype is such a statement, because the returned value is asso\u00adciated with containers \nthat utilize different comparator types. Only if these containers share the same iterator type will this \ncode compile. Thus, this implementation is only valid with STLs like libstdc++, which de.ne an independent \niterator; it will not compile with STLs like Dinkum.  3.4 Advantages of Using an Independent Iterator \nThe overheads induced by the new approach are similar to that of the abstract iterator design (Section \n2.2.1) in that we cannot avoid using the Sorter t interface. This is true because we are utilizing different \ntypes of std::sets (have different comparator types), and so the std::sets must be adapted to conform \nto one interface in order to facilitate uniform access (which is required by the database imple\u00admentation \nin Figure 5). Every operation that is done through the Sorter t interface involves an added virtual function \ncall, which is entirely avoided when utilizing the abstract com\u00adparator design. And since there are K \nsorting criteria, there are actually K such extra function invocations. This, however, does not mean \nthat the new approach is inferior. In fact, the opposite is true. To understand why, assume that the \ndatabase is currently empty, and that we have now added the .rst item. In this case, contrary to our \nclaim, the abstract comparator design is superior, because, as noted, the new approach induces K extra \nvirtual function calls that are absent from the abstract comparator design. We now add the second item. \nWhile the abstract com\u00adparator design avoids the virtual calls, it must compare the second item to the \n.rst. This is done with the help of K pointers to comparison functions and therefore induces the overhead \nof K function invocations. Conversely, the com\u00adparisons performed by the std::sets of the new approach \nare inlined, because the implementation of the comparator types is known at compile time. Thus, for the \nsecond item, the two designs are tied: K vs. K invocations. We now add the third element. With the new \napproach, there are still only K function calls; nothing has changed in this respect. But with the abstract \ncomparator design, there might be up to 2K function invocations (and no less than K), depending on the \nvalues of the items involved. In the general case, whenever a new item is added, the abstract comparator \ndesign performs O(K \u00b7 logN )function invocations (logN comparisons along each of the K inser\u00adtion paths), \nwhereas the new approach performs exactly K. The same observation holds for deletion and lookup. Focusing \non iteration, we note that the new approach does not (de)allocate iterators through new and delete. The \nab\u00adstract comparator design still has the advantage that its be\u00adgin and end are not virtual. But in accordance \nto the iter\u00adation procedure shown in Figure 2, this advantage occurs only once per iteration, during \nwhich N elements are tra\u00adversed. In both designs, the pointer-like iterator operations that are exercised \nN times are identical, as both directly uti\u00adlize the native set<int>::iterator without abstraction layers. \nThus, the advantage due to the one extra virtual call quickly becomes negligible as N increases. We later \nshow that the difference is noticeable only while N = 4. We conclude that, excluding a few small N values, \nthe new approach is superior to the two standard designs: It is better than the abstract iterator design \nwhen iterating and .nding, and it is better than the abstract comparator design when .nding, adding, \nand deleting. 3.5 Consequences In relation to our running example, we contend that the independence \nof the iterator should be made part of the STL speci.cation, or else programmers would be unable to use \nthe new approach if their environment does not support the right kind of STL, or if they wish to write \nportable programs that compile on more than one platform. But this is just one example. The more general \nprinci\u00adple we advocate is that, when designing a generic class, designers should (1) attempt to minimize \nthe dependencies between the class s type parameters and nested types, and (2) should make the remaining \ndependencies part of the user contract, declaring that no other dependencies exist. Reducing dependencies \ndirectly translates to increased compile-time interchangeability; and explicitly declaring that no other \ndependencies exist makes it possible for pro\u00adgrammers to leverage this increased interchangeability for \nwriting faster programs. 3.6 Disadvantages of Using an Independent Iterator Independent iterators make \none problem slightly worse. As\u00adsume, e.g., that vectors v1 and v2 hold elements of type T but utilize \ndifferent allocator types. The following error p = v1.begin(); q = v2.end(); std::sort(p,q);// error! \ncan be caught at compile time if v1 and v2 have different iterator types (which is the case if the iterator \ndepends on the allocator); otherwise, the error can only be caught at runtime. Such problems do occur \nin real life, however, the only complete solution is to have related iterators extracted from their container \nby code rather than by hand, as is made possible by C++0x, the upcoming revision of ISO C++.   4. \nExperimental Results: Runtime In this section we evaluate the two standard solutions de\u00adscribed in Section \n2 against our proposal from the previous section. We denote the three competing database designs as: \n1. the iterator design (Section 2.2), 2. the comparator design (Section 2.3), and 3. the new design \n(Section 3.3).  We conduct a two-phase evaluation. In Section 4.1, we use microbenchmarks to characterize \nthe performance of each individual database operation. And in Section 4.2, we evalu\u00ad ate the overall \neffect on a real application. The experiments were conducted on a 2.4 GHz Intel Core 2 Duo machine equipped \nwith 4GB memory and run\u00adning lenny/sid Debian (Linux 2.6.20 kernel). The bench\u00admarks were compiled with \nGCC 4.3.2, using the -O2 .ag. While running, the benchmarks were pinned to a single core, and times were \nmeasured using the core s cycle counter; the reported results are averages over multiple runs. Except \nfrom the default Debian daemons, no other processes were present in the system while the measurements \ntook place. 4.1 Microbenchmarks We use four microbenchmarks to measure the duration of adding, deleting, \n.nding, and iterating through the items. Figure 12 displays the results. Durations are presented as a \nfunction of N (number of database items), and N is shown along the x axis. The add microbenchmark sequentially \nadds N different items to an empty database, where N is 2i for i =0, 1, 2, ..., 22. The y axis shows \nhow long it took to perform this work, normalized (divided) by N \u00b7K.(K was chosen to be 2, as shown in \nFigures 9 10.) The y axis thus re.ects the average time it takes to add one item to one container associated \nwith one sorting criterion. The other three microbenchmarks are similarly de.ned and normalized: delete \nsequentially erases the N items in the order by which they were inserted; .nd looks up each of the N \nitems within the database according to each of the K sorting criteria (and checks that the returned iterator \nis different than the corresponding end of sequence); and iterate traverses the N items (using the procedure \nshown in Figure 2) according to each of the K sorting criteria. The results coincide with our analysis \nfrom Section 3.4. Comparator vs. new Figure 12 illustrates that the new de\u00ad sign adds, deletes, and .nds \nitems faster than the comparator design. Indeed, these activities require repeated item com\u00adparisons \nalong the associated search tree paths; the compar\u00adisons translate to function invocations in the comparator \nde\u00adsign, but resolve into inlined code in the new design. Itera\u00adtion, on the other hand, involves no \ncomparisons, and so the performance of the comparator and new designs is similar. Figure 13(a) shows \nthe corresponding relative speedup, de.ned as the ratio of the duration it takes to perform each operation \nunder the two competing designs. (Values bigger than 1 indicate the new design is faster.) Initially, \nfor small N values, the comparator design may be faster. This happens because the new design utilizes \nthe Sorter t interface and thus induces one extra virtual function call (two in the case of the iterate \nbenchmark: begin and end). But when N is increased, the relative weight of this overhead decreases, as \nmore and more items must be compared ( iterate : must be traversed), such that beyond N =4 the speedup \nis always bigger than 1 ( iterate : equal to 1). We were initially surprised by the fact that the .nd \nspeedup is smaller than that of add and (sometimes) of delete . As the latter perform a lot more work \nthat does not involve comparisons (allocation, deallocation, and balanc\u00ading), we anticipated that the \nrelative weight of the compar\u00adisons would be smaller. It turns out that add and delete actually require \nmore comparisons, because the underlying (red black) search tree is generally less balanced while they \nexecute. The reason is that, when we repeatedly add items and monotonically grow the tree, we systematically \nencounter those cases that trigger the balancing activity, which occurs only when the tree is not balanced \nenough . (Monotonically deleting items has the same affect.) Such cases always involve an extra comparison, \nand .nd never encounters these cases because it does not alter the tree. Overall, the speedup behavior \nis the following. It goes up (for the reasons discussed above), reaches a kind of steady state that peaks \nat nearly 1.7, and then falls off a cliff to a level of around 1.15. We investigated the reason that \ncauses the fall and discovered that it is tightly connected to the size of the L2 cache. Figure 14 plots \nthe delete speedup curve and superimposes on it the associated resident set size (RSS) as reported by \nthe operating system through the proc .lesystem [44]; the RSS re.ects the size of physical memory the \nbenchmark utilized. On our testbed machine, the size of the L2 cache is 4MB, and according to Figure \n14, the biggest database size to .t within the L2 is N =64K. We can indeed see that immediately after \nthat N , the speedup drops. The reason is that memory accesses can no longer be served by the cache and \nrequire going to main memory. As such accesses may take hundreds of cycles, the relative bene.t of inlined \ncomparisons within the new design diminishes. Iterator vs. new By Figure 12, the time to add and delete \nitems by both designs is similar, which should come as no surprise because they utilize the same exact \ncode to perform these activities. The new design, however, .nds items and iterates through them faster \nthan the iterator design. The rea\u00adson is that, with the iterator design, both activities dynami\u00adcally \n(de)allocate iterator instances through new and delete; moreover, every operation applied to these instances \nis real\u00adized through an abstract interface and induces a virtual func\u00adtion call (as opposed to the new \ndesign that inlines these op\u00aderations). This was explained in detail in Section 3.4. add deletefinditerate \nThe associated speedup, shown in Figure 13(b), can therefore be explained as follows. Initially, for \nsmall N val\u00adues, the dynamic (de)allocation is the dominant part, as there are relatively few items to \nprocess. But as N increases, the price of dynamic (de)allocation is amortized across more items, causing \nthe speedup ratio to get smaller. The speedup then enters a steady state, until N =64K is reached and \nthe database no longer .ts in L2, at which point it drops to a level of around 1.75. Throughout the entire \nN range, the iterate speedup is higher than that of .nd , because the former involves an additional virtual \ncall ( .nd only compares the returned it\u00aderator to end, whereas iterate also increments the iterator). \n 4.2 Real Application To evaluate the new design in the context of a real applica\u00adtion, we use an in-house \nscheduler simulator, which is used for researching and designing the scheduling subsystem of supercomputers \nsuch as the IBM BlueGene machines. The simulator is capable of simulating the schedulers of most machines \nwithin the top-500 list [15], and it has been exten\u00ad sively used for research purposes [41, 21, 20, 50, \n46]. Others have implemented similar simulators [40, 16, 31]. size of database [number of items; log \nscaled] Figure 12. The results of the four microbenchmarks as achieved by the three competing database \ndesigns. a. new vs. comparator b. new vs. iterator new vs. comparator: delete size of database [number \nof items; log]size of database 1416 0.9 0.8 0.7 speedup 1.7 1.6 1.5 1.4 1.3 1.2 1.1 1 64 1416 256 64 \n2561K1K4K  16K 64K 256K 1M 4K16K64K 256K 1M 4M 4M 1 4 1416 64 16 64 256 2561K4K 1K 4K16K 64K 256K16K \n 64K256K1M 1M4M 4M 14RSS (log) 16642561K4K16K64K256K1M4M 1  4 1664256 1K 4K16K64K 256K 1M4M speedup \n The workload of supercomputers typically consists of a sequence of jobs submitted for batch execution. \nAccord\u00adingly, years-worth of logs that record such activity in real su\u00adpercomputer installations are \nused to drive the simulations. The logs are converted to a standard format [10] and are made available \nthrough various archives [42, 43]. Each log includes a description of the corresponding machine and the \nsequence of submitted jobs; each job is characterized by at\u00adtributes such as its arrival time, runtime, \nand the number of processors it used. The simulator reads the log, simulates the activity under the design \nthat is being evaluated, and out\u00adputs various performance metrics. For the purpose of perfor\u00admance evaluation, \neach log is considered a benchmark. The simulator is a discrete event-driven program. Events can be, \ne.g., job arrivals and terminations. Upon an event, the scheduler utilizes two main data structures: \nthe wait queue and the runlist. It inserts arriving jobs to the wait queue and removes terminating jobs \nfrom the runlist. It then scans the runlist to predict resources availability, and it scans the wait \nqueue to .nd jobs that can make use of these resources. According to various dynamic considerations, \nthe order of the job-scanning may change; the algorithm that makes use iterator vs. comparator vs. new \n 2 2 1.8 1.8 1.6 1.6 1.4 1.4 1.2 1.2 1 1 0.8 0.8 0.6 0.6 0.4 0.4 0.2 0.2 0 0 a. iterator vs. new b. \ncomparator vs. new 2 1.8 1.6 1.4 1.2 1 0.8 0.6 0.4 0.2 0 normalizedexeuction time logs  logs logs iterator \ncomparator new iterate add/del other Figure 15. Normalized execution time of the three Figure 16. Breaking \nthe execution time from Figure 15 to three disjoint com\u00ad simulator versions, when simulating six activity \nlogs.ponents: addition/deletion of items, traversal through the items, and the rest. of the scanning \nis otherwise the same. Adequate candidates are removed from the wait queue and inserted to the runlist. \nIt follows that the main data structures of the simulator must support functionality similar to that \nof the database we have developed earlier. Originally, the simulator was implemented using the classic \niterator design pattern. We have modi.ed the simulator and implemented the other two competing designs, \nsuch that the one being used is chosen through a compilation .ag. The evaluated scheduler required four \nsorting criteria for the wait queue (job arrival time, run\u00adtime, user estimated runtime, and system predicted \nruntime) and three for the runlist (termination time based on: real run\u00adtime, user estimated runtime, \nand system predicted runtime). The data structures store job IDs (integers) that serve as in\u00addices to \na vector that is initialized at start-up and holds all the jobs (and, thus, comparators refer to this \nvector). Figure 15 shows the time it takes to complete the sim\u00ad ulation when simulating six workload \nlogs.6 All execution times are normalized by that of the new design, on a per-log basis. We can see that \nthe execution time of the iterator de\u00adsign is x1.7 to x2.1 slower than that of the new design, and that \nthe execution time of the comparator design is x1.2 to x1.3 slower, depending on the log. Figure 16 breaks \nthe execution time to three disjoint com\u00ad ponents: cycles that were spent on adding or deleting jobs \nto/from the wait queue and runlist, cycles that were spent on traversing the jobs, and all the rest. \n(The simulator does not utilize the .nd operation.) We have shown in Section 4.1 that the new design \nis superior to the comparator design in terms of adding and deleting; and indeed, the majority of the \ndifference in their executions times is caused by addition and deletion. Likewise, we have shown that \nthe new design is superior to the iterator design in terms of traversing; and indeed, the majority of \nthe difference in executions times be\u00adtween these two designs is caused by iteration. 6 The logs span \nmonths to years and contain tens to hundreds of thousands of jobs submitted by hundreds of users operating \nwithin different sites under different load conditions; further details can be found in the Parallel \nWorkload Archive site [42].  A minor part of the difference is caused by the other op\u00aderations, which \nare identical across all designs. We speculate that this is caused by caching effects that are triggered \nby the less ef.cient parts of the program.  5. Techniques to Reduce Code Bloat Compilers generate object \ncode. In this section, we focus on how the code s size (in bytes) can be affected by re\u00adducing unneeded \ndependencies between the members and type parameters of a generic class. To this end, we continue to \nuse STL containers and iterators. But the discussion no longer revolves around the three designs from \nthe previous section. Rather, it focuses on the impact of using multiple type parameters to instantiate \na single class, e.g., as is done in Figure 9, where we use two comparator types (lt and gt) as type parameters. \nIn Figure 10, we only use one type pa\u00adrameter (CmpFunc t) and so our discussion does not apply. The more \ntype parameters that are used, the bigger the ob\u00adject code that is emitted. This increase in code is \nsometimes referred to as bloat, and this section is about reducing it. 5.1 What We Have Already Achieved \nLet us reconsider Figure 11. On its left, the iterator is inner and thus depends on the comparator and \nallocator. The de\u00adsign on its right de.nes the iterator outside and removes the unneeded dependencies. \nWe denote these two designs as in\u00adner and outer , respectively. In Section 4, we have shown how to leverage \nthe outer design to write faster programs. Here, we additionally argue that it also allows for reducing \nthe bloat. To see why, consider the following snippet that copies two integer std::sets into two matching \narrays. std::set<int,lt> u;// assume uholdsN elements std::set<int,gt> v;// vholdsN elementstoo int arr1[N]; \nint arr2[N]; std::copy( u.begin(), u.end(), arr1);// copy uto arr1 std::copy( v.begin(), v.end(), arr2);// \ncopy vto arr2 Suppose we (1) compile this snippet with an STL that uti\u00adlizes the inner design, (2) generate \nan executable called a.exe, and (3) run the following shell command, which prints how many times the \nsymbol std::copy is found in a.exe: nm demangle a.exe |grep -c std::copy The result would be 2, indicating \nthat the function std::copy was instantiated twice. In contrast, if we use an STL that utilizes the outer \ndesign, the result would be 1, re.ecting the fact that there is only one instantiation. The reason for \nthis difference is that, like many other standard C++ algorithms, std::copy is parameterized by the iterators \ntype: template<typename Src Iter, typename Dst Iter> Dst Iter std::copy(Src Iterbegin,Src Iter end,Dst \nItertarget); With the inner design, the iterator types of u and v are differ\u00adent due to their different \ncomparators, which means there are two Src Iter types, resulting in two instantiations of copy. The outer \ndesign has an independent iterator, yielding only one Src Iter and, hence, only one instantiation. The \nsame argument holds when using several allocator types. We conclude that, when unneeded dependencies \nexist, ev\u00adery additional type parameter associated with these depen\u00addencies results in another instantiation \nof the algorithm. This type of bloat is unnecessary and can be avoided by following the principle we \nadvocate and eliminating the said depen\u00addencies. Thus, in addition to allowing for faster code, our proposal \nalso allows for code that is more compact.  5.2 What We Can Achieve Further Our above understandings \nregarding bloat and how to reduce it can be generalized to have a wider applicability as follows. The \nouter design is successful in reducing the bloat of standard generic algorithms like std::copy, because \nsuch al\u00adgorithms suffer from no unneeded dependencies. This is true because (1) every type parameter \nthat is explicitly associated with any such algorithm is a result of careful consideration and unavoidable \nnecessity; and because (2) such algorithms are global routines that reside in no class and hence are \nnot subject to implicit dependencies. The latter statement does not apply to algorithms that are methods \nof a generic class. For example, all the mem\u00adber methods of std::set<T,C,A> implicitly depend on the \nkey type T, the comparator type C, and the allocator type A. We observe that this triple dependency occurs \neven if, logi\u00adcally, it should not. And we note that this is exactly the same observation we have made \nregarding member classes (iter\u00adators). We contend that this observation presents a similar opportunity \nto reduce the bloat.  5.3 Hoisting Others have already taken the .rst step to exploit this oppor\u00adtunity, \ntargeting the case where a method of a generic class is logically independent of all the generic parameters. \nFor ex\u00adample, the size method that returns the number of elements stored by the std::set7     template<typename \nT, typename C, typename A> size type set<T,C,A>::size() const {return this->count; } This method just \nreturns an integer data member (the alias size type is some integer type) and so its implementation is \nindependent of T, C, and A. Yet, for every T/C/A combina\u00adtion, the compiler emits another identical instantiation. \nThe well-known and widely used solution to this problem is template hoisting [8]. The generic class is \nsplit into a non\u00ad generic base class and a generic derived class, such that all the members that do not \ndepend on any of the type parame\u00adters are moved, hoisted , to the base class. In our example, these members \nare size and count, and their hoisting ensures that size is instantiated only once. Importantly, hoisting \nin\u00adduces no performance penalty, as none of the methods are made virtual and no runtime polymorphism \nis involved. Most STL implementations use hoisting to implement the standard associative containers. \nThese are set, multiset, map, and multimap. (Sets hold keys, maps hold key/data pairs, and multi-containers \ncan hold non-unique keys.) All the libraries listed in Table 3 implement these containers using one generic \nred-black tree class. Henceforth, we only consider the latter. As explained in Section 3.1, iteration\u00ad \nrelated code and the balancing code of the tree need not depend on T, C, and A, because they are strictly \nbased on the structure of the tree. And indeed, these routines are typically hoisted and operate on pointers \nto the non-generic base class of the tree s node. Thus, there is only one instantiation of the tree rebalance \nmethod for all the associative containers. 5.4 Generalized Hoisting We contend that hoisting can be \ngeneralized to reduce the bloat more effectively. Our claim is motivated by the fol\u00adlowing analysis. \nWe have examined the code of the generic red-black tree class of GCC and found that nearly all of its \nmethods either: (1) exclusively depend on T, T/C, or T/A; or (2) can be be trivially modi.ed to have \nthis type of depen\u00addency. We therefore propose to decompose the tree in a way that removes the other \ndependencies. Figure 17 roughly illustrates this idea. On the left, the red\u00ad black tree is de.ned using \none class, so when, e.g., the bal\u00adancing code is required, every change in T, C, or A will re\u00adsult in \nanother duplicate instantiation of rebalance. The mid\u00addle of the .gure recti.es this de.ciency by creating \na non\u00adgeneric base class and by hoisting rebalance. There will now be just one such instance, regardless \nof how many T/C/A combinations there are. The right of the .gure takes the next step and eliminates the \nremaining unneeded dependencies. 7 In practice, size is inlined; we assume it is not to allow for a short \nexample.   The erase routine needs the comparator to .nd an item, and it needs the allocator to deallocate \nit, so it depends on T/C/A. This is not the case for the .nd routine, which only depends on T/C, as it \nmerely compares items. The clear routine systematically deallocates the entire tree and does not care \nabout the order dictated by the comparator; it thus depends on only T/A. Finally, the majority of the \ncode of the swap routine (which swaps the content of two trees in a way that does not involve actual \ncopying of items) depends on only T. (The reminder of swap s code will be shortly discussed.) Like swap, \nas we have discussed in much detail, the nested iterator class only depends on T. In Figure 11 we have \nsuggested to move its de.nition to an external scope to eliminate its dependency on C/A. Generalized \nhoisting is an alternative way to achieve this goal.  6. Experimental Results: Code Bloat We have refactored \nRb tree, the red-black tree underlying all associative containers of GCC s STL, according to the generalized \nhoisting design principle. This section describes our experience and evaluates its success. We note that \nwe have intentionally constrained ourselves to only applying trivial changes to Rb tree, even though, \nin some cases, a more intrusive change would have been more effective. 6.1 Applying Generalized Hoisting \nto the STL of GCC Bloat reduction is only applicable to methods that are (1) not inlined or (2) inlined, \nbut invoke methods that are not inlined (directly or indirectly). Henceforth, we refer to such methods \nas noninlined. The code of the remaining methods (that are not noninlined) is re-emitted for each invocation; \nthis is inherent to inlining and is unrelated to generics bloat. All of GCC s STL associative containers \nare inlined wrappers of Rb tree, and their methods translate to invoca\u00adtions of Rb tree s public methods; \nthis wrapping code leaves no trace after inlining, and is optimized out in its entirety. Out of the 46 \npublic methods of Rb tree, 29 are nonin\u00adlined as listed in Table 4 (we are currently only interested \nin the method-name column; the other columns are addressed in Section 6.2). We have refactored Rb tree \nusing three addi\u00adtional classes: Rb base(depends on only T), Rb alloc (T/A), and Rb cmp (T/C), such that \nRb alloc derives std::allocator     by default (or the user s allocator if speci.ed), Rb cmp de\u00adrives \nRb base, and Rb tree derives Rb cmp and has an in\u00ad  avoid the diamond-shape inheritance depicted in \nFigure 17 (virtual multiple inheritance), because it complicates the memory layout by introducing certain \nindirection layers that might incur a performance penalty. (We did not investigate what this penalty \nis.) Furthermore, beyond what is already in the original Rb tree, we categorically did not add calls \nto functions that are not completely inlined so as not to degrade the performance. Namely, we did not \nadd any indirection. Group i in Table 4 includes the methods that reside in Rb base (swap) or in an external \nscope (comparison oper\u00adators). The former is comprised of 40 lines of code, only 2 of which (swaping \nthe comparator and allocator) depend on C/A; we hoist the .rst 38 lines, and replace the origi\u00adnal Rb \ntree::swap with an inlined version that (1) calls the hoisted version and (2) invokes the remaining 2 \nlines. The comparison operators are inlined calls to the global std::lexicographical compare, which, \nlike std::copy, operate on our hoisted iterator, and so it depends on only T . Group ii includes the \nnoninlined methods that copy or destroy the tree, or destroy a range of iterators. None of these activities \nrequire the comparator, and so this functionality is moved to Rb alloc. Remaining in Rb tree is easily \nsplittable code like copying of the comparator (as in swap). Group iii includes the only two routines \nthat actually use noninlined methods from both Rb cmp and Rb alloc. These routines need to .nd a range \nof iterators associated with a key (there can be more than one in multi-containers) and use Rb cmp::equal \nrange for this purpose. Once found, the range is deleted with the Rb alloc::erase from Group ii. No refactoring \nwas needed in this case.       # method name original . . refactored . . goodness of .t (R2 ) \ndiff. b0 d0 b1 c1 a1 d1 original refactored i. Noninlined code from only Rb base, or external 1 swap \n1 461 369 0 0 45 0.999998 0.999745 48 2 operator >= 104 589 595 0 0 67 0.999998 0.999896 30 3 operator \n> 104 589 595 0 0 67 0.999998 0.999896 30 4 operator <= 104 589 595 0 0 67 0.999998 0.999896 30 5 operator \n< 104 589 595 0 0 67 0.999998 0.999896 30 ii. Noninlined code not from Rb cmp 6 erase(iterator,iterator) \n381 1004 382 0 944 79 1.000000 0.999992 -20 7 destructor 238 459 237 0 403 45 0.999998 0.999966 12 8 \nclear 236 542 236 0 402 128 0.999998 0.999987 12 9 operator = 471 1680 491 0 1114 534 1.000000 0.999997 \n11 10 copy constructor 87 1760 179 0 1019 643 0.995402 0.972517 4 iii. Noninlined code from both Rb cmp \nand Rb alloc 11 erase(key) 375 2080 386 530 942 583 1.000000 0.999999 14 12 erase(key*,key*) 376 2449 \n392 527 939 952 0.999999 0.999994 14 iv. Noninlined code not from Rb alloc 13 insert equal(iterator,value) \n187 1633 208 1556 0 88 0.999996 0.999999 -32 14 insert equal(value) 124 992 144 928 0 70 0.999990 0.999991 \n-25 15 insert equal(iterator,iterator) 177 1493 207 928 0 568 0.999999 0.999999 -32 16 insert unique(value) \n166 1144 212 496 0 580 0.999988 0.999999 21 17 insert unique(iterator,iterator) 239 1641 272 496 0 1060 \n0.999999 0.999999 51 18 insert unique(iterator,value) 188 1893 213 496 0 1363 0.999997 1.000000 8 v. \nLikewise + entirely contained in Rb cmp 19 count(key) const 0 1092 0 1010 0 42 0.999999 0.999980 39 \n20 count(key) 0 1092 0 1010 0 42 0.999999 0.999979 39 21 rb verify() const 0 681 0 667 0 28 0.999998 \n0.999916 -14 22 upper bound(key) const 0 343 0 269 0 50 0.999915 0.999946 23 23 upper bound(key) 0 341 \n0 268 0 50 0.999918 0.999942 23 24 lower bound(key) const 0 343 0 269 0 50 0.999915 0.999946 23 25 lower \nbound(key) 0 341 0 268 0 50 0.999918 0.999942 23 26 .nd(key) const 0 343 0 269 0 50 0.999915 0.999946 \n23 27 .nd(key) 0 341 0 268 0 50 0.999912 0.999939 22 28 equal range(key) const 0 699 0 508 0 131 0.999970 \n0.999637 60 29 equal range(key) 0 695 0 504 0 131 0.999970 0.999634 60          Table 4. The \nnoninlined methods of GCC s std::Rb tree. We model the associated object code size (in bytes) with s\u00af0 \n(x, y)= b0 + d0 xy (size of original tree) and s\u00af1 (x, y)= b1 +c1 x +a1 y +d1 xy (size of refactored \ntree). Fitting against the real data is done with the nonlinear least-squares Marquardt-Levenberg algorithm; \nthe resulting R2 values are nearly 1, indicating the .ts are accurate. The insertion functions in Group \niv do not require any Rb alloc code except from allocating a new node, which is done with a short pure \ninlined Rb alloc function. The .rst insert equal methods (13 14) are multi-container versions that add \nthe new item even if it is already found in the tree. We move these to Rb cmp and change them such that \ninstead of getting the new key as a parameter, they get an already allocated node holding that key; we \nmake the new Rb tree versions inlined calls to the associated Rb cmp versions, and we incorporate the \nnode allocation as part of the call. These were one-line changes. Method 15 repeatedly invokes method \n14 and so remains unchanged. The refactoring of the insert unique methods (16 18) was different because \nthey correspond to unique contain\u00aders (that allocate a new node only if the respective key is not already \nfound in the tree), and they therefore involve a more complex allocation logic. We initially left these \nmeth\u00adods nearly unchanged, but later realized that they included several calls to an internal function \nthat we wrapped in in\u00adlined code, and this repeated code contributed to the bloat. Fortunately, an easy \n.x was possible. The methods consist of a sequence of conditional branches, such that each branch ends \nwith a call to the internal function; we replace all these calls with a single call positioned after \nthe branches.     The remaining methods, in Group v, are query routines that only use the comparator \nand perform no (de)allocation. We move them in their entirety to Rb cmp. 6.2 Evaluation To evaluate \nthe effect of our design, we (1) .x T, (2) sys\u00adtematically vary C and A, and (3) measure the size of \nthe resulting object code on a per-method basis. Let T be an integer type, and let {C1 ,C2 , ..., Cn \n}and {A1 ,A2 , ..., An } be n different integer comparators and allocators, respec\u00adtively. Given a noninlined \nRb tree function f, let f j be one i invocation of Rb tree<T,Ci,Aj >::f (i.e., the instantiation of \nRb tree s f when using key type T, comparator type Ci , and allocator type Aj ). Given x, y .{1, 2, ..., \nn}, we de.ne s(x, y)to be the size, in bytes, of the .le that is comprised of the invocations f j for \ni =1, 2, ..., x and j =1, 2, ..., y.   i For example, s(1, 1) is the size of the object .le that only \ncontains the call to f 1 ; s(1, 2)contains two calls: f 1 and f 2; 1 11 and s(2, 2)contains f11 , f12 \n, f 1 , and f22 . 2 Figure 19 (left) shows s0 (x, y)(size of original swap) and s1 (x, y)(size of refactored \nswap), in kilobytes, as a function of the number of comparators x =1, ..., 5 and allocators y =1, ..., \n5 (a total of 5 \u00d7 5 =25 object .les). Most of swap (refactored version) resides in Rb base, and this \npart is instantiated only once. In contrast with the original version, the code is re-instantiated for \nevery additional comparator or allocator, which explains why s0 (x, y)becomes bigger than s1 (x, y)at \napproximately the same rate along both axes. We hypothesize that the size s1 (x, y)of each noninlined \nrefactored method f can be modeled as follows: s1 (x, y) s\u00af1 (x, y)= b1 + c1 x + a1 y + d1 xy where \nb1 is the size of f s code (in bytes) that depends on only T (or nothing) and is thus instantiated only \nonce; c1 is the size of f s Rb cmp code (depends on only C and re\u00ademitted for each additional comparator); \na1 is the size of f s Rb alloc code (depends on only A and re-emitted for each additional allocator); \nand d1 is the size of f s Rb tree code (depends on both Cand Aand re-emitted for each additional comparator \nor allocator). We likewise hypothesize that the size s0 (x, y) of each noninlined original method can \nbe modeled as s0 (x, y) s\u00af0 (x, y)= b0 + d0 xy (c0 = a0 =0, as the original Rb tree aggregates all the \ncode and so none of the code is solely dependent on Cor A.) If the models s\u00af0 (x, y) and s\u00af1 (x, y) are \naccurate, they would allow us to reason about the bloat more effectively. We .t the data (sizes of 2\u00d729\u00d725 \n=1450object .les) against the above two models for all 29 noninlined meth\u00adods. The results of swap, shown \nin the middle and right of Figure 19, demonstrate a tight .t. Table 4 lists the model pa\u00ad rameters of \nall noninlined methods along with the associated coef.cient of determination, R2 , which quanti.es the \ngood\u00adness of the .t. As R2 is nearly 1 in all cases, we conclude that the measurements consistently support \nour models. Hence\u00adforth we use the models to approximate the size. Carefully examining the parameters \nreveals the positive nature of the change induced by generalized hoisting. First, note that the sums \nof the coef.cients of the two trees (b0 +d0 vs. b1 +c1 +a1 +d1 ) are similar, as indicated by the diff \ncolumn that shows their difference. These sums are in fact s\u00af0 (1, 1) and s\u00af1 (1, 1), re.ecting exactly \none instantiation of the respective method. The sums should not differ, as they are associated with the \nsame code; our new design has an effect only when more instantiations are created. Since s(x, y) b + \ncx + ay + dxy, the real goal of the refactoring is to reduce d, the amount of bytes dependent on both \nC and A. We cannot make these bytes go away. But we can shift them to other parameters; preferably to \nb (bytes independent of both Cand A), but also to c or a (bytes depend on C or A, but not on both). And \nindeed, comparing d0 to d1 in Table 4 reveals that we have successfully done so, as d1 is signi.cantly \nsmaller than d0 across all the methods. In Group i, the bytes are shifted to b1 , in Group ii to a1 , \nin Groups iv and v to c1 , and in Group iii to both c1 and a1 . Let R(x, y)= s\u00af0 (x, y)/s\u00af1 (x, y)denote \nthe bloat ratio, expressing how much more code is emitted by the original implementation relative to \nthe refactored one. Let us focus on R(x, 1), which re.ects the relative price of adding one more comparator. \nR(x, 1) depends on x, but only up to a point, because systematically increasing x means R(x, 1) converges \nto Rc = limx.8 R(x, 1) = d0 /(c1 + d1 ). We thus de.ne Rc to be the comparator bloat ratio. We likewise \nde.ne Ra =limy.8 R(1,y)= d0 /(a1 +d1 )to be the allo\u00adcator bloat ratio, and we de.ne Rac =limx,y.8 R(x, \ny)= d0 /d1 to be the joint bloat ratio. The ratios allow us to quan\u00adtify the effectiveness of the new \ndesign in reducing the bloat; they are shown in Figure 20 (same order as in Table 4). There is no difference \nbetween the three ratios of methods in Group i (swap etc.), because most bytes have shifted to b1 , and \nnone exclusively depend on Cor A. We can see that, asymptotically, the original swap generates 10x more \nbloat than our refactored version. In Group ii, adding a comparator to the original design can be up \nto 13x more expensive; though adding an allocator is equally expensive (as all the code depends on the \nallocator even in the refactored design). The comparator and joint ratios are equal in Group ii, as c1 \n= 0. In Groups iv v, an added allocator can be up to 25x less expensive with the refactored version. \n(The allocator/joint ratios are equal because a1 =0.) Finally, Group iii is the only case where the joint \nratio is different, since both c1 and a1 are nonzero, namely, some bytes exclusively depend on noninlined \nRb cmp code, and some on Rb alloc code. 6.3 Drawbacks of Generalized Hoisting Unlike nested classes, \nwhich we merely need to move out\u00adside, generalized hoisting requires real refactoring. Still, the changes \nwe applied to the Rb tree were trivial, and we believe that they can be applied by average programmers. \nThe technique is certainly suitable for library implementers in terms of their expertise and the cost-effectiveness \nof their efforts, from which all the library users would bene.t. In our example, there were only three \ntype parameters involved, making the refactoring feasible. More parameters would make things challenging, \nand we are doubtful whether our approach would scale. We speculate, however, that the principles might \nstill apply, and we believe this subject mer\u00adits further research. One possible approach might be exter\u00adnalized \nhoisting: Similarly to nested classes, we can move any given member method f to an external scope and \nreplace  comparator allocator joint  Figure 20. Comparing the two designs with the comparator (Rc \n), allocator (Ra ), and joint (Rca) bloat ratios. it with an inlined version that invokes the now-external \nfunc\u00adtion; the type parameter list of the generic now-external f would be minimized to only include its \nreal dependencies. The drawback is losing the reference to this , and having to supply the relevant data \nmember as arguments.  7. Compiler and Language Support Some of the bloat reduction achieved through \ngeneralized hoisting can be achieved by compiler optimizations. We are aware of one compiler, Microsoft \nVC++, which employs heuristics to reuse functions that are identical at the assembly level. This, however, \ndoes not produce perfect results [4]. But more importantly, such heuristics are inherently limited to \nmethods like those from Group v (Table 4), that require no manual modi.cation; all the rest of the Rb \ntree methods are different at the assembly level for different type parameters (unless the generalized \nhoisting technique is applied). In Section 2, we have presented the conventional solu\u00adtions to the classic \nmulti-index database problem and noted that they are based on runtime polymorphism. In Section 3 we have \nutilized SCARY assignments to devise a new so\u00adlution that is largely based on compile-time polymorphism, \nand in Section 4 we have shown that this solution is faster. In certain cases, it is possible for the \ncompiler to automatically transform a solution that is based on runtime polymorphism to a solution that \nis based on compile-time polymorphism. But such transformations would require whole-program op\u00adtimization, \nwhich would make it inapplicable to most real\u00adworld C++ programs (which rely on dynamic linking).  Replacing \ninner classes with aliases and decomposing a class with generalized hoisting can be perceived as tricks \nwe must employ since the language does not directly support the notion of minimizing dependencies between \nthe mem\u00adbers and parameters of a generic class. Alternatively, we might support a general variant of \nSCARY in the language and type system by allowing programmers to explicitly spec\u00adify dependencies for \nclass and method members of a generic type. This would, e.g., be done as brie.y illustrated in Fig\u00adure \n21. We intend to investigate this issue in the future. 8. Generalizing to Other Languages Our .ndings \nare applicable to languages that, upon different type parameters, emit different instantiations of the \ngeneric class. Such languages can utilize compile-time polymor\u00adphism and the aggressive optimizations \nit makes possible, but at the same time, they are susceptible to bloat. C# is such a language. Unlike \nC++, C# emits instantia\u00adtions at runtime as needed, and if the parameters involved are references (pointer \ntypes), the emitted code coalesce to a common instantiation. (This is somewhat similar to the C++ template<typename \nX, typename Y, typename Z> struct C { void f1() utilizes X,Z { // only allowed to use X or Z, not Y } \n void f2() { // for backward compatibility, this is // equivalent to: void f2() utilizes X,Y,Z } class \nInner_t utilizes Y { // only allowed to use Y, not X nor Z }; }; Figure 21. With the utilizes keyword, \nprogrammers would be able to accurately express dependencies; compilers should be able to enforce and \nexploit this in a straightforward manner. void* pointer hoisting technique [49].) But if the parameters \nare structures (value types), then a just-in-time (JIT) spe\u00adcialization is emitted, compiled, and optimized, \nachieving performance almost matching hand-specialized code [36]. C#, however, provides a weaker support \nto type aliasing. Its using directive is similar to C++ s typedef , but the alias only applies to the \n.le it occurs in. This means that it is currently impossible to hide the fact that the dependencies were \nminimized and that the nested class was moved outside; users must be made aware and explicitly utilize \nthe now\u00adexternal type, possibly by changing their code. We note, however, that support for generic programming \nis improving. In 2003, Garcia et al. compared languages based on several generics-related desired properties \n(includ\u00ading type aliasing), and they generated a table that lists which language supports which property \n[24]. The table entries were 52% full . This table was revisited in 2007 [25] and in 2009 [47], and became \n57% and 84% full, respectively. (We only consider languages that appeared in more than one table version; \nC# s score was nearly tripled.) It is therefore not unlikely that type aliasing would be added to C# \nin the future. And this paper provides yet another incentive. We note in passing that C# s standard iterators \nfollow the classic design pattern (iterators implement an abstract inter\u00adface) and hence pay the price \nof runtime polymorphism; we have shown that the overheads can be signi.cant. However, there is no technical \ndif.culty preventing a C++-like imple\u00admentation. And, regardless, our .ndings are general and ap\u00adply \nto all generic classes, not just to iterators. Our ideas also apply to D [5]. If the nested class is \nstatic, moving it outside is like doing it in C++, as D supports type aliasing. But unlike C++ and C#, \nD also supports non-static nested classes, which can access the outer object s members. And so moving \nsuch classes outside means breaking this as\u00adsociation. While somewhat less convenient, we can resolve \nthis dif.culty by manually adding a data member referring to the outer object. This presents the designer \nwith a tradeoff of convenience vs. the bene.ts detailed in this paper. Haskell and standard ML are not \nobject oriented lan\u00adguages, but both can represent nested types within generic types [9]. Both languages \ncan be implemented in a way that utilizes multiple instantiations and compile-time polymor\u00adphism [34, \n52], in which case some of our .ndings apply (Section 5.1). Java utilizes generics for compile-time type \nsafety, not compile-time polymorphism. Thus, our results do not apply. 9. Related Work In statically-typed \nlanguages like C++, Java, and C#, the use of runtime polymorphism translates to indirect branches, where \naddresses of call targets are loaded from memory. In the early 1990s, Fisher argued that indirect function \ncalls are unavoidable breaks in control and there are few com\u00adpiler or hardware tricks that could allow \ninstruction-level parallelism to advance past them [22]. Not only does in\u00ad direct branching prevent inlining, \nbut it also hinders oppor\u00adtunities for other optimizations such as better register alloca\u00adtion, constant \nfolding, etc. [6]. In addition, pipelined proces\u00ad sors are bad at predicting such branches, in.icting \npipeline .ushes that further degrade the performance [33]. Conse\u00ad quently, the problem is the focus of \nnumerous studies. Devirtualization attempts to transform the indirect calls of a program to direct calls. \nStatic devirtualization, with whole program optimizers, was applied to language like C++ [6, 3] and Modula-3 \n[14]. But in recent years a lot of effort has been put into dynamic devirtualization in the context of \nJava and JIT compiling. The function call graph is inferred at runtime [2, 54], and, when appropriate, \nsuch information is used for inlining devirtualized calls [13, 1, 32, 27]. (This work is potentially \napplicable to also C# and the .NET environment.) In parallel, architecture researchers have designed \nindirect branch predictors in an attempt to elevate the problem [45, 37], and such specialized hardware \nis currently deployed in state-of-the-art processors, like the Intel Core2 Duo [28]. Despite all this \neffort, the problem is consistent and prevalent [7, 18, 39, 54, 33]. Compile-time polymorphism attacks \nthe problem in its root cause, by avoiding indirect branches. It is explicitly de\u00adsigned to allow generic \ncode to achieve performance com\u00adparable to that of hand-specialized code [48], a goal that is often achieved \n[51, 35, 36, 26]. The programming technique we propose makes compile-time polymorphism applicable to \na wider range of problems. To exemplify, we have shown how to utilize the prevalent classic iterator \ndesign pattern [23] in a way that nearly eliminates indirect branching. Another problem that has spawned \nmuch research is ex\u00adecutable compaction [3, 11, 30]. Section 5.1 described tem\u00ad plate hoisting [8], which \nis the dominant programming tech\u00ad nique to reduce code bloat caused by generic programming. We have generalized \nthis technique to reduce the bloat fur\u00adther. Bourdev and J\u00a8arvi proposed an orthogonal technique involving \nmetaprogramming and manual guidance [4]. 10. Conclusions We advocate a design principle whereby the \ndependencies between the members and the type parameters of a generic class should be minimized, we propose \ntechniques to realize this principle, and we show that the principle can be lever\u00adaged to achieve faster \nand smaller programs. Generic programming is utilized by several languages to produce more ef.cient code. \nThe full compile-time knowl\u00adedge regarding the types involved allows for compile-time polymorphism, which \nobviates the need for dynamic bind\u00ading and enables aggressive optimizations such as inlining. But the \napplicability of compile-time polymorphism is in\u00adherently limited to homogeneous settings, where the \ntypes involved are .xed. When programmers need to handle a set of heterogeneous types in a uniform manner, \nthey typically have to introduce an abstraction layer to hide the type dif\u00adferences. They therefore resort \nto traditional runtime poly\u00admorphism through inheritance and virtual calls, hindering the aforementioned \noptimizations. We show that the homogeneity limitation is not as con\u00adstraining as is generally believed. \nSpeci.cally, we target in\u00adner classes that nest in a generic class. We make the case that instantiating \nthe outer class multiple times (with multiple type parameters) does not necessarily mean that the types \nof the corresponding inner classes differ. We demonstrate that the resulting interchangeability of the \nlatter can be exploited to produce faster code. We do so by utilizing the canonical it\u00aderator design \npattern (which heavily relies on dynamic bind\u00ading) in a novel way that entirely eliminates dynamic bind\u00ading \nfrom the critical path. We evaluate the proposed design and demonstrate a x1.2 to x2.1 speedup. While \nour example concerns C++/STL iterators, our ideas are applicable to any generic class within any programming \nlanguage that realizes genericity with multiple instantiations (such as C# and D). We .nd that, for programmers, \nobtaining the runtime speedups is nearly effortless and only requires to use the language in a previously \nunconceived manner ( SCARY as\u00adsignments ) that exploits the interchangeability. But for this to be possible, \ntwo conditions must be met. The .rst is tech\u00adnical. The designers of a generic class should carefully \ncon\u00adsider the relationship between its type parameters and its nested classes; if an inner class does \nnot depend on all the type parameters, it should be moved outside and be replaced with an alias that \nminimizes the dependencies. This makes SCARY assignments legal under existing, unmodi.ed com\u00adpilers. \nThe designers should then declare that no other de\u00adpendencies exist and thereby allow users to safely \nexploit the consequent interchangeability. We thus propose to amend standard APIs like STL to re.ect \nthe suggested principle; the change will not break old code, but rather, allow for a new kind of code. \nThe second condition is overcoming the typical initial re\u00adaction of programmers when presented with SCARY \nassign\u00adments, .nding it hard to believe that such assignments con\u00adform to the type system and, if so, \nare a useful technique. In our experience, it is easy to change their minds. A positive outcome of minimized \ndependencies is re\u00adduced code bloat, as less algorithm instantiations are needed (regardless of whether \nSCARY assignments are utilized). We suggest a generalized hoisting programming paradigm that generalizes \nthis principle in order to further reduce the bloat. By this paradigm, a generic class is decomposed \ninto a hierarchy that minimizes the dependencies between its generic type parameters and all of its members \n(inner classes and methods), without introducing indirection that degrades performance. We apply this \ntechnique to GCC s STL and obtain up to 25x reduction in object code size. Similarly to our above suggestions, \nthe technique is useful for languages that realize genericity with multiple instantiations. We have submitted \na proposal [38] to the ISO C++ com\u00ad mittee to change the standard to re.ect our ideas in the up\u00adcoming \nC++ revision, C++0x.  Acknowledgments We thank the anonymous reviewers for their insightful com\u00adments. \nThe .rst author also thanks Kai-Uwe Bux (Cor\u00adnell U.), Dilma Da Silva (IBM), Ronald Garcia (Rice U.), \nRobert Klarer (IBM) Uri Lublin (Redhat), Nate Nystrom (IBM), David Talby (Amazon), Michael Wong (IBM), \nAmi\u00adram Yehudai (TAU), and Greta Yorsh (IBM) for providing much appreciated feedback. Finally, and most \nimportantly, the .rst author thanks Ziv Balshai (Intel) for asking the orig\u00adinal question that started \nthe whole thing: Why does this code compile under Linux but not under Windows? Hope\u00adfully, the code will \nsoon compile on both. References [1] M. Arnold, S. Fink, D. Grove, M. Hind, and P. F. Sweeney, Adaptive \noptimization in the Jalape\u00b4no JVM . In 15th ACM Conf. on Object Oriented Prog. Syst. Lang. &#38; App. \n(OOPSLA), pp. 47 65, 2000. [2] M. Arnold and D. Grove, Collecting and exploiting high-accuracy call graph \npro.les in virtual machines . In IEEE Int l Symp. Code Generation &#38; Optimization (CGO), pp. 51 62, \n2005. [3] D. F. Bacon and P. F. Sweeney, Fast static analysis of C++ virtual function calls . In 11th \nACM Conf. on Object Oriented Prog. Syst. Lang. &#38; App. (OOPSLA), pp. 324 341, 1996. [4] L. Bourdev \nand J. J\u00a8arvi, Ef.cient run-time dispatching in generic programming with minimal code bloat . In Symp. \non Library-Centric Software Design (LCSD), Oct 2006. [5] W. Bright, D programming language . http://www.digitalmars.com/d \n[6] B. Calder and D. Grunwald, Reducing indirect function call overhead in C++ programs . In 21st ACM \nSymp. on Principles of Prog. Lang. (POPL), pp. 397 408, 1994. [7] B. Calder, D. Grunwald, and B. Zorn, \nQuantifying behavioral differences between C and C++ programs . J. Prog. Lang. 2, pp. 313 351, 1994. \n[8] M. D. Carroll and M. A. Ellis, Designing and Coding Reusable C++. Addison-Wesley, 1995. [9] M. M. \nT. Chakravarty, G. Keller, S. P. Jones, and S. Marlow, Associated types with class . In 32nd ACM Symp. \non Principles of Prog. Lang. (POPL), pp. 1 13, Jan 2005.  [10] S. Chapin et al., Benchmarks and standards \nfor the evaluation of parallel job schedulers . In 5th Workshop on Job Scheduling Strategies for Parallel \nProcessing (JSSPP), pp. 67 90, Springer-Verlag, Apr 1999. Lect. Notes Comput. Sci. vol. 1659. [11] D. \nCitron, G. Haber, and R. Levin, Reducing program image size by extracting frozen code and data . In ACM \nInt l Conf. on Embedded Software (EMSOFT), pp. 297 305, 2004. [12] T. H. Cormen, C. E. Leiserson, R. \nL. Rivest, and C. Stein, Introduction to algorithms. MIT Press, 2nd ed., 2001. [13] D.DetlefsandO.Agesen, \nInliningofvirtualmethods .In European Conf. on Object-Oriented Prog. (ECOOP), pp. 258 278, 1999. [14] \nA. Diwan, K. S. McKinley, and J. E. B. Moss, Using types to analyze and optimize object-oriented programs \n. ACM Trans. on Prog. Lang. &#38; Syst. (TOPLAS) 23(1), pp. 30 72, 2001. [15] J. J. Dongarra, H. W. Meuer, \nH. D. Simon, and E. Strohmaier, Top500 supercomputer sites . http://www.top500.org/ [16] C. L. Dumitrescu \nand I. Foster, GangSim: A simulator for grid scheduling studies . In 5th IEEE Int l Symp. on Cluster \nComput. &#38; the Grid (CCGrid), pp. 1151 1158 Vol. 2, 2005. [17] A. Duret-Lutz, T. G\u00b4eraud, and A. Demaille, \nDesign patterns for generic programming in C++ . In 6th USENIX Conf. on Object-Oriented Technologies \n(COOTS), p. 14, 2001. [18] M. A. Ertl, T. Wien, and D. Gregg, Optimizing indirect branch prediction accuracy \nin virtual machine interpreters . In ACM Int l Conf. Prog. Lang. Design &#38; Impl. (PLDI), pp. 278 288, \n2003. [19] A. Ezust and P. Ezust, An Introduction to Design Patterns in C++ with Qt 4. Prentice Hall, \n2006. [20] D. G. Feitelson, Experimental analysis of the root causes of performance evaluation results: \na back.ll case study . IEEE Trans. Parallel Distrib. Syst. (TPDS) 16(2), pp. 175 182, Feb 2005. [21] \nD. G. Feitelson, Metric and workload effects on computer systems evaluation . IEEE Computer 36(9), pp. \n18 25, Sep 2003. [22] J. A. Fisher and S. M. Freudenberger, Predicting conditional branch directions \nfrom previous runs of a program . In 5th Arch. Support for Prog. Lang. &#38; Operating Syst. (ASPLOS), \npp. 85 95, 1992. [23] E. Gamma, R. Helm, R. Johnson, and J. Vlissides, Design Patterns: Elements of Reusable \nObject-Oriented Software. Addison-Wesley, 1995. [24] R. Garcia, J. J\u00a8arvi, A. Lumsdaine, J. G. Siek, \nand J. Willcock, A comparative study of language support for generic programming . In 18th ACM Conf. \non Object Oriented Prog. Syst. Lang. &#38; App. (OOPSLA), pp. 115 134, Oct 2003. [25] R. Garcia, J. J\u00a8arvi, \nA. Lumsdaine, J. G. Siek, and J. Willcock, An extended comparative study of language support for generic \nprogramming . J. Func. Prog. (JFP)) 17(2), pp. 145 205, Mar 2007. [26] J. Gerlach and J. Kneis, Generic \nprogramming for scienti.c computing in C++, Java, and C# . In 5th Advanced Parallel Processing Technologies \n(APPT), pp. 301 310, Sep 2003. Lect. Notes Comput. Sci. vol. 2834. [27] N. Glew and J. Palsberg, Type-safe \nmethod inlining . J. Sci. Comput. Program. 52(1-3), pp. 281 306, 2004. [28] S. Gochman et al., The Intel \nPentium M processor: Microarchitec\u00adture and performance . Intel Technology Journal 7(2), May 2003. [29] \nM. Hansen, How to reduce code bloat from STL containers . C++ Report 9(1), pp. 34 41, Jan 1997. [30] \nH. He, J. Trimble, S. Perianayagam, S. Debray, and G. Andrews, Code compaction of an operating system \nkernel . In IEEE Int l Symp. Code Generation &#38; Optimization (CGO), pp. 283 298, 2007. [31] A. Iosup, \nD. H. Epema, T. Tannenbaum, M. Farrellee, and M. Livny, Inter-operating grids through delegated matchmaking \n. In ACM/IEEE Supercomputing (SC), pp. 1 12, 2007. [32] K. Ishizaki, M. Kawahito, T. Yasue, H. Komatsu, \nand T. Nakatani, A study of devirtualization techniques for a Java just-in-time compiler . In ACM Conf. \non Object Oriented Prog. Syst. Lang. &#38; App. (OOPSLA), pp. 294 310, 2000. [33] J. A. Joao, O. Mutlu, \nH. Kim, R. Agarwal, , and Y. N. Patt, Improving the performance of object-oriented languages with dynamic \npredication of indirect jumps . In Arch. Support for Prog. Lang. &#38; Operating Syst. (ASPLOS), pp. \n80 90, 2008. [34] M. P. Jones, Dictionary-free overloading by partial evaluation . Lisp and Symbolic \nComputation 8(3), pp. 229 248, 1995. [35] C. E. Kees and C. T. Miller, C++ implementations of numerical \nmethods for solving differential-algebraic equations: design and optimization considerations . ACM Trans. \nMath. Softw. 25(4), pp. 377 403, 1999. [36] A. Kennedy and D. Syme, Design and implementation of generics \nfor the .NET common language runtime . In ACM Int l Conf. Prog. Lang. Design &#38; Impl. (PLDI), pp. \n1 12, 2001. [37] H. Kim, J. A. Joao, O. Mutlu, C. J. Lee, Y. N. Patt, and R. Cohn, VPC prediction: reducing \nthe cost of indirect branches via hardware-based dynamic devirtualization . In 34th Int l Symp. on Computer \nArchit. (ISCA), p. 2007, 424 435. [38] R. Klarer, B. Stroustrup, D. Tsafrir, and M. Wong, SCARY iterator \nassignment and initialization . ISO C++ standards committee paper N2913=09-0103, Jul 2009. Frankfurt, \nGermany. http://www.open\u00adstd.org/jtc1/sc22/wg21/docs/papers/2009/n2913.pdf [39] J. Lau, M. Arnold, M. \nHind, and B. Calder, Online performance auditing: using hot optimizations without getting burned . In \nACM Int l Conf. Prog. Lang. Design &#38; Impl. (PLDI), pp. 239 251, 2006. [40] A. Legrand, L. Marchal, \nand H. Casanova, Scheduling distributed applications: the SimGrid simulation framework . In IEEE Int \nl Symp. on Cluster Comput. &#38; the Grid (CCGrid), p. 138, 2003. [41] A. Mu alem and D. G. Feitelson, \nUtilization, predictability, workloads, and user runtime estimates in scheduling the IBM SP2 with back.lling \n. IEEE Trans. Parallel Distrib. Syst. (TPDS) 12(6), pp. 529 543, Jun 2001. [42] Parallel Workloads Archive \n. http://www.cs.huji.ac.il/labs/parallel/workload [43] Grid Workloads Archive . http://gwa.ewi.tudelft.nl \n[44] Proc(5): process info pseudo-.lesystem -Linux man page . http://linux.die.net/man/5/proc (Acceded \nMar 2009). [45] A. Roth, A. Moshovos, and G. S. Sohi, Improving virtual function call target prediction \nvia dependence-based pre-computation . In 13th ACM Int l Conf. on Supercomput. (ICS), pp. 356 364, 1999. \n[46] E. Shmueli and D. G. Feitelson, On simulation and design of parallel-systems schedulers: are we \ndoing the right thing? . IEEE Trans. on Parallel Distrib. Syst. (TPDS) 20(7), pp. 983 996, Jul 2009. \n[47] J. G. Siek and A. Lumsdaine, A language for generic programming in the large . J. Science of Comput. \nProgramming, 2009. To appear. [48] A. Stepanov, The standard template library: how do you build an algorithm \nthat is both generic and ef.cient? . Byte 10, Oct 1995. [49] B. Stroustrup, The C++ Programming Language. \nAddison-Wesley, 3rd ed., 1997. [50] D. Tsafrir, Y. Etsion, and D. G. Feitelson, Back.lling using system\u00adgenerated \npredictions rather than user runtime estimates . IEEE Trans. Parallel Distrib. Syst. (TPDS) 18(6), pp. \n789 803, Jun 2007. [51] T. L. Veldhuizen and M. E. Jernigan, Will C++ be faster than Fortran? . In Int \nl Sci. Comput. in Object-Oriented Parallel Environments (ISCOPE), 1997. [52] S. Weeks, Whole-program \ncompilation in MLton . In Workshop on ML, p. 1, ACM, 2006. [53] Wikibooks, C++ programming/code/design \npatterns . http://en.wikibooks.org/wiki/C++ Programming/Code/Design Patterns. [54] X. Zhuang, M. J. Serrano, \nH. W. Cain, and J-D. Choi, Accurate, ef.cient, and adaptive calling context pro.ling . In ACM Int l Conf. \nProg. Lang. Design &#38; Impl. (PLDI), pp. 263 271, 2006.  \n\t\t\t", "proc_id": "1640089", "abstract": "<p>Generic classes can be used to improve performance by allowing compile-time polymorphism. But the applicability of compile-time polymorphism is narrower than that of runtime polymorphism, and it might bloat the object code. We advocate a programming principle whereby a generic class should be implemented in a way that minimizes the dependencies between its members (nested types, methods) and its generic type parameters. Conforming to this principle (1) reduces the bloat and (2) gives rise to a previously unconceived manner of using the language that expands the applicability of compile-time polymorphism to a wider range of problems. Our contribution is thus a programming technique that generates faster and smaller programs. We apply our ideas to GCC's STL containers and iterators, and we demonstrate notable speedups and reduction in object code size (real application runs 1.2x to 2.1x faster and STL code is 1x to 25x smaller). We conclude that standard generic APIs (like STL) should be amended to reflect the proposed principle in the interest of efficiency and compactness. Such modifications will not break old code, simply increase flexibility. Our findings apply to languages like C++, C#, and D, which realize generic programming through multiple instantiations.</p>", "authors": [{"name": "Dan Tsafrir", "author_profile_id": "81337494165", "affiliation": "IBM T.J. Watson Research Center, Yorktown Heights, NY, USA", "person_id": "P1728799", "email_address": "", "orcid_id": ""}, {"name": "Robert W. Wisniewski", "author_profile_id": "81100616443", "affiliation": "IBM T.J. Watson Research Center, Yorktown Heights, NY, USA", "person_id": "P1728800", "email_address": "", "orcid_id": ""}, {"name": "David F. Bacon", "author_profile_id": "81100628167", "affiliation": "IBM T.J. Watson Research Center, Yorktown Heights, NY, USA", "person_id": "P1728801", "email_address": "", "orcid_id": ""}, {"name": "Bjarne Stroustrup", "author_profile_id": "81100106139", "affiliation": "Texas A&M University, College Station, TX, USA", "person_id": "P1728802", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1640089.1640121", "year": "2009", "article_id": "1640121", "conference": "OOPSLA", "title": "Minimizing dependencies within generic classes for faster and smaller programs", "url": "http://dl.acm.org/citation.cfm?id=1640121"}