{"article_publication_date": "10-25-2009", "fulltext": "\n A Type and Effect System for Deterministic Parallel Java * Robert L. Bocchino Jr. Vikram S. Adve Danny \nDig Sarita V. Adve Stephen Heumann Rakesh Komuravelli Jeffrey Overbey Patrick Simmons Hyojin Sung Mohsen \nVakilian Department of Computer Science University of Illinois at Urbana-Champaign dpj@cs.uiuc.edu Abstract \nToday s shared-memory parallel programming models are complex and error-prone. While many parallel programs \nare intended to be deterministic, unanticipated thread interleav\u00adings can lead to subtle bugs and nondeterministic \nsemantics. In this paper, we demonstrate that a practical type and ef\u00adfect system can simplify parallel \nprogramming by guarantee\u00ading deterministic semantics with modular, compile-time type checking even in \na rich, concurrent object-oriented language such as Java. We describe an object-oriented type and effect \nsystem that provides several new capabilities over previous systems for expressing deterministic parallel \nalgorithms. We also describe a language called Deterministic Parallel Java (DPJ) that incorporates the \nnew type system features, and we show that a core subset of DPJ is sound. We describe an experimental \nvalidation showing that DPJ can express a wide range of realistic parallel programs; that the new type \nsystem features are useful for such programs; and that the parallel programs exhibit good performance \ngains (coming close to or beating equivalent, nondeterministic multithreaded pro\u00adgrams where those are \navailable). Categories and Subject Descriptors D.1.3 [Software]: Concurrent Programming Parallel Programming; \nD.3.1 [Software]: Formal De.nitions and Theory; D.3.2 [Soft\u00adware]: Language Classi.cations Concurrent, \ndistributed, and parallel languages; D.3.2 [Software]: Language Class\u00adi.cations Object-oriented languages; \nD.3.3 [Software]: * This work was supported by the National Science Foundation under grants CCF 07-02724 \nand CNS 07-20772, and by Intel, Microsoft and the Univer\u00adsity of Illinois through UPCRC Illinois. Permission \nto make digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior speci.c permission and/or a fee. OOPSLA 2009, October \n25 29, 2009, Orlando, Florida, USA. Copyright c &#38;#169; 2009 ACM 978-1-60558-734-9/09/10. . . $10.00 \n Language Constructs and Features Concurrent Program\u00adming Structures General Terms Languages, Veri.cation, \nPerformance Keywords Determinism, deterministic parallelism, effects, effect systems, commutativity 1. \nIntroduction The advent of multicore processors demands parallel pro\u00adgramming by mainstream programmers. \nThe dominant model of concurrency today, multithreaded shared memory pro\u00adgramming, is inherently complex \ndue to the number of possi\u00adble thread interleavings that can cause nondeterministic pro\u00adgram behaviors. \nThis nondeterminism causes subtle bugs: data races, atomicity violations, and deadlocks. The parallel \nprogrammer today prunes away the nondeterminism using constructs such as locks and semaphores, then debugs \nthe program to eliminate the symptoms. This task is tedious, error prone, and extremely challenging even \nwith good de\u00adbugging tools. The irony is that a vast number of computational algo\u00adrithms (though not \nall) are in fact deterministic: a given input is always expected to produce the same output. Almost all \nscienti.c computing, encryption/decryption, sorting, com\u00adpiler and program analysis, and processor simulation \nalgo\u00adrithms exhibit deterministic behavior. Today s parallel pro\u00adgramming models force programmers to \nimplement such al\u00adgorithms in a nondeterministic notation and then convince themselves that the behavior \nwill be deterministic. By contrast, a deterministic-by-default programming model [9] can guarantee that \nany legal program produces the same externally visible results in all executions with a particular input \nunless nondeterministic behavior is explic\u00aditly requested by the programmer in disciplined ways. Such \na model can make parallel application development and main\u00adtenance easier for several reasons. Programmers \ndo not have to reason about notoriously subtle and dif.cult issues such as data races, deadlocks, and \nmemory models. They can start with a sequential implementation and incrementally add par\u00adallelism, secure \nin the knowledge that the program behavior will remain unchanged. They can use familiar sequential tools \nfor debugging and testing. Importantly, they can test an application only once for each input [19]. \nUnfortunately, while guaranteed determinism is available for some restricted styles of parallel programming \n(e.g., data parallel, or pure functional), it remains a challenging re\u00adsearch problem to guarantee determinism \nfor imperative, object-oriented languages such as Java, C++, and C#. In such languages, object references, \naliasing, and updates to muta\u00adble state obscure the data dependences between parts of a program, making \nit hard to prove that those dependences are respected by the program s synchronization. This is a very \nimportant problem as many applications that need to be ported to multicore platforms are written in these \nlanguages. We believe that a type and effect system [27, 26, 12, 30] is an important part of the solution \nto providing guaranteed deterministic semantics for imperative, object-oriented lan\u00adguages. A type and \neffect system (or effect system for short) allows the programmer to give names to distinct parts of the \nheap (we call them regions) and specify the kind of ac\u00adcesses to parts of the heap (e.g., read or write \neffects)in dif\u00adferent parts of the program. The compiler can then check, using simple modular analysis, \nthat all pairs of memory ac\u00adcesses either commute with each other (e.g., they are both reads, or they \naccess disjoint parts of the heap) or are prop\u00aderly synchronized to ensure determinism. A robust type \nand effect system with minimal runtime checks is valuable be\u00adcause it enables checking at compile time \nrather than run\u00adtime, eliminates unnecessary runtime checks (thus leading to less overhead and/or less \nimplementation complexity), and contributes to program understanding by showing where in the code parallelism \nis expressed and where code must be rewritten to make parallelism available. Effect annota\u00adtions can \nalso provide an enforceable contract at interface boundaries, leading to greater modularity and composabil\u00adity \nof program components. An effect system can be supple\u00admented with runtime speculation [23, 51, 38, 31, \n50] or other runtime checks [43, 20, 47, 6] to enable greater expressivity. In this paper, we develop \na new type and effect system for expressing important patterns of deterministic parallelism in imperative, \nobject-oriented programs. FX [33, 27] showed how to use regions and effects in limited ways for determin\u00adistic \nparallelism in a mostly functional language. Later work on object-oriented effects [26, 12, 30] and object \nowner\u00adship [16, 32, 14] introduced more sophisticated mechanisms for specifying effects. However, studying \na wide range of realistic parallel algorithms has shown us that some signi.\u00adcantly more powerful capabilities \nare needed for such algo\u00adrithms. In particular, all of the existing work lacks general support for fundamental \nparallel patterns such as parallel updates on distinct .elds of nested data structures, parallel array \nupdates, in-place divide and conquer algorithms, and commutative parallel operations. Our effect system \ncan support all of the above capa\u00adbilities, using several novel features. We introduce region path lists, \nor RPLs, which enable more .exible effect sum\u00admaries, including effects on nested structures. RPLs also \nallow more .exible subtyping than previous work. We in\u00adtroduce an index-parameterized array type that \nallows ref\u00aderences to provably distinct objects to be stored in an ar\u00adray while still permitting arbitrary \naliasing of the objects through references outside the array. We are not aware of any statically checked \ntype system that provides this capabil\u00adity. We de.ne the notions of subarrays (i.e., one array that shares \nstorage with another) and partition operations,that together enable in-place parallel divide and conquer \nopera\u00adtions on arrays. Subarrays and partitioning provide a natural object-oriented way to encode disjoint \nsegments of arrays, in contrast to lower-level mechanisms like separation logic [35] that specify array \nindex ranges directly. We also introduce an invocation effect, together with simple commutativity anno\u00adtations, \nto permit the parallel invocation of operations that may actually interfere at the level of reads and \nwrites, but still commute logically, i.e., produce the same .nal (logical) behavior. This mechanism supports \nconcurrent data struc\u00adtures such as concurrent sets, hash maps, atomic counters, etc. We have designed \na language called Deterministic Paral\u00adlel Java (DPJ) incorporating these features. DPJ is an ex\u00adtension \nto Java that enforces deterministic semantics via compile-time type checking. Because of the guaranteed \nde\u00adterministic semantics, existing Java code can be ported to DPJ incrementally. Furthermore, porting \nto DPJ will have minimal impact on program testing: developers can use the same tests and testing methodology \nfor the ported parallel code as they had previously used for their sequential code. The choice of Java \nfor our work is not essential; simi\u00adlar extensions could be applied to other object-oriented lan\u00adguages, \nand we are currently developing a version of the lan\u00adguage and compiler for C++. We are also exploring \nhow to extend our type system and language to provide disciplined support for explicitly nondeterministic \ncomputations. This paper makes the following contributions: 1. Novel features. We introduce a new region-based \ntype and effect system with several novel features (RPLs, index-parameterized arrays, subarrays, and \ninvocation effects) for expressing core parallel programming pat\u00adterns in imperative languages. These \nfeatures guarantee determinism at compile-time. 2. Formal de.nition. For a core subset of the type system, \nwe have developed a formal de.nition of the static and dynamic semantics, and a detailed proof that our \nsystem allows sound static inference about noninterference of ef\u00adfects. We present an outline of the \nformal de.nition and proof in this paper. The full details are in an accompany\u00ading technical report [10] \navailable via the Web [1].  3. Language De.nition. We have designed a language called DPJ that incorporates \nthe type and effect system into a modern O-O language (Java) in such a way that it supports the full \n.exibility of the sequential subset of Java, enables incremental porting of Java code to DPJ, and guarantees \nsemantic equivalence between a DPJ pro\u00adgram and its obvious sequential Java version. We have implemented \na prototype compiler for DPJ that performs the necessary type checking and then maps parallelism down \nto the ForkJoinTask dynamic scheduling frame\u00adwork. 4. Empirical evaluation. We study six real-world \nparallel programs written in DPJ. This experience shows that DPJ can express a range of parallel programming \npatterns; that all the novel type system features are useful in real programs; and that the language \nis effective at achieving signi.cant speedups on these codes on a commodity 24\u00adcore shared-memory processor. \nIn fact, in 3 out of 6 codes, equivalent, manually parallelized versions written to use Java threads \nare available for comparison, and the DPJ versions come close to or beat the performance of the Java \nthreads versions.  The rest of this paper proceeds as follows. Section 2 pro\u00advides an overview of some \nbasic features of DPJ, and Sec\u00adtions 3 5 explain the new features in the type system (RPLs, arrays, and \ncommutativity annotations). Section 6 summa\u00adrizes the formal results for a core subset of the language. \nSection 7 discusses our prototype implementation and evalu\u00adation of DPJ. Section 8 discusses related \nwork, and Section 9 concludes.  2. Basic Capabilities We begin by summarizing some basic capabilities \nof DPJ that are similar to previous work [33, 30, 26, 14, 15]. We re\u00adfer to the example in Figure 1, \nwhich shows a simple binary tree with three nodes and a method initTree that writes into the mass .elds \nof the left and right child nodes. As we describe more capabilities of DPJ, we will also expand upon \nthis example to make it more realistic, e.g., supporting trees of arbitrary depth. Region names. In DPJ, \nthe programmer uses named re\u00adgions to partition the heap, and writes method effect sum\u00admaries stating \nwhat regions are read and written by each method. A .eld region declaration declares a new name r (called \na .eld region name) that can be used as a region name. For example, line 2 declares names Links, L,and \nR, and these names are used as regions in lines 4 and 5.1 A.eld region name is associated with the static \nclass in which it is declared; this fact allows us to reason soundly about ef\u00ad 1 As explained in Section \n3, in general a DPJ region is represented as a region path list (RPL), which is a colon-separated list \nof elements such as Root:L:L:R that expresses the nested structure of regions. When a simple name r functions \nas a region, as shown in this section, it is short for Root:r. 1 class TreeNode<region P> { 2 region \nLinks, L, R; 3 double mass in P ; 4 TreeNode<L> left in Links ; 5 TreeNode<R> right in Links ; 6 void \nsetMass(double mass) writes P { this.mass = mass; } 7 void initTree(double mass) { 8 cobegin { 9 /* reads \nLinks writes L */ 10 left.mass = mass; 11 /* reads Links writes R */ 12 right.mass = mass; 13 } 14 } \n15 } Figure 1. Basic features of DPJ. Type and effect annota\u00adtions are italicized. Note that method initTree \n(line 7) has no effect annotation, so it gets the default effect summary of reads and writes the entire \nheap. TreeNode<L> double mass L TreeNode<L> left Links TreeNode<R> right Links TreeNode<R> double mass \nR TreeNode<L> left Links TreeNode<R> right Links Figure 2. Runtime heap typing from Figure 1 fects \nwithout alias restrictions or interprocedural alias anal\u00adysis. A .eld region name functions like an ordinary \nclass member: it is inherited by subclasses, and outside the scope of its de.ning class, it must be appropriately \nquali.ed (e.g., TreeNode.L). A local region declaration is similar and de\u00adclares a region name at local \nscope. Region parameters. DPJ provides class and method re\u00adgion parameters that operate similarly to \nJava generic pa\u00adrameters. We declare region parameters with the keyword region, as shown in line 1, so \nthat we can distinguish them from Java generic type parameters (which DPJ fully sup-ports).Whena region-parameterizedclassormethodis \nused, region arguments must be provided to the parameters, as shown in lines 4 5. Region parameters enable \nus to create multiple instances of the same class with their data in differ\u00adent regions. Disjointness \nconstraints. To control aliasing of region parameters, the programmer may write a disjointness con\u00adstraint \n[14] of the form P1 # P2,where P1 and P2 are pa\u00adrameters (or regions written with parameters; see Section \n3) that are required to be disjoint. Disjointness of regions is fully explained in Section 3; in the \ncase of simple names, it means the names must be different. The constraints are checked when instantiating \nthe class or calling the method. If the disjointness constraints are violated, the compiler is\u00adsues a \nwarning. Partitioning the heap. The programmer may place the keyword in after a .eld declaration, followed \nby the region, as shown in lines 3 5. An operation on the .eld is treated as an operation on the region \nwhen specifying and check\u00ading effects. This effectively partitions the heap into regions. See Figure \n2 for an illustration of the runtime heap typing, assuming the root node has been instantiated with Root. \n Method effect summaries. Every method (including all constructors) must conservatively summarize its \nheap effects with an annotation of the form reads region-list writes region-list, as shown in line 6. \nWrites imply reads. When one method overrides another, the effects of the superclass method must contain \nthe effects of the subclass method. For example, if a method speci.es a writes effect, then all methods \nit overrides must specify that same writes effect. This constraint ensures that we can check effects \nsoundly in the presence of polymorphic method invocation [30, 26]. The full DPJ language also includes \neffect variables [33], to support writing a subclass whose effects are unknown at the time of writing \nthe superclass (e.g., in instantiating a library or framework class); however, we leave the discussion \nof effect variables to future work. Effects on local variables need not be declared, because these effects \nare masked from the calling context. Nor must initialization effects inside a constructor body be declared, \nbecause the DPJ type and effect system ensures that no other task can access this until after the constructor \nreturns. Read effects on final variables are also ignored, because those reads can never cause a con.ict. \nA method or construc\u00adtor with no externally visible heap effects may be declared pure. To simplify programming \nand provide interoperability with legacy code, we adopt the rule that no annotation means reads and writes \nthe entire heap, as shown in Figure 1. This scheme allows ordinary sequential Java to work correctly, \nbut it requires the programmer to add the annotations in order to introduce safe parallelism. Expressing \nparallelism. DPJ provides two constructs for expressing parallelism, the cobegin block and the foreach \nloop. The cobegin block executes each statement in its body as a parallel task, as shown in lines 8 13. \nThe foreach loop is used in conjunction with arrays and is described in Section 4.1. Proving determinism. \nTo type check the program in Figure 1, the compiler does the following. First, check that the summary \nwrites P of method setMass (line 6) is correct (i.e., it covers all effect of the method). It is, because \n.eld mass is declared in region P (line 3), and there are no other effects. Second, check that the paral\u00adlelism \nin lines 8 13 is safe. It is, because the effect of line 10 is reads Links writes L; the effect of line \n12 is reads Links writes R;and Links, L,and R are distinct names. Notice that this analysis is entirely \nintraprocedural.  3. Region Path Lists (RPLs) An important concept in effect systems is region nesting, \nwhich lets us partition the heap hierarchically so we can ex\u00adpress that different computations are occurring \non different parts of the heap. For example, to extend the code in Fig\u00adure 1 to a tree of arbitrary depth, \nwe need a tree of nested re\u00adgions. As discussed in Section 4, we can also use nesting to express that \ntwo aggregate data structures (like arrays) are in distinct regions, and the components of those structures \n(like the cells of the arrays) are in distinct regions, each nested un\u00adder the region containing the \nwhole structure. Effect systems that support nested regions are generally based on object ownership [16, \n14] or use explicit declara\u00adtions that one region is under another [30, 26]. As discussed below, we use \na novel approach based on chains of elements called region path lists, or RPLs, that provides new capabil\u00adities \nfor effect speci.cation and subtyping. 3.1 Specifying Single Regions The region path list (RPL) generalizes \nthe notion of a simple region name r. Each RPL names a single region,orset of memory locations, on the \nheap. The set of all regions parti\u00adtions the heap, i.e., each memory location lies in exactly one region. \nThe regions are arranged in a tree with a special re\u00adgion Root as the root node. We say that one region \nis nested under (or simply under) another if the .rst is a descendant of the second in the tree. The \ntree structure guarantees that for any two distinct names r and r', the set of regions under r and the \nset of regions under r' have empty intersection, and we can use this guarantee to prove disjointness \nof memory accesses. Syntactically, an RPL is a colon-separated list of names, called RPL elements, beginning \nwith Root. Each element after Root is a declared region name r,2 for example, Root:A:B. As a shorthand, \nwe can omit the leading Root.In particular, a bare name can be used as an RPL, as illustrated in Figure \n1. The syntax of the RPL represents the nesting of region names: one RPL is under another if the second \nis a pre.x of the .rst. For example, L:R is under L. We write R1 = R2 if R1 is under R2. We may also \nwrite a region parameter, instead of Root, at the head of an RPL, for example P:A,where P is a param\u00adeter. \nWhen a class with a region parameter is instantiated at runtime, the parameter is resolved to an RPL \nbeginning with Root. Method region parameters are resolved similarly at method invocation time. Because \na parameter P is always bound to the same RPL in a particular scope, we can make sound static inferences \nabout parametric RPLs. For example, for all P, P:A = P,and P:A = P:B if and only if A= B. Figure 3 illustrates \nthe use of region nesting and class region parameters to distinguish different .elds as well as different \nobjects. It extends the example from Figure 1 by 2 As noted in Section 2, this can be a package-or class-quali.ed \nname such as C.r; for simplicity, we use r throughout. 1 class TreeNode<region P> { 2 region Links, \nL, R, M, F; 3 double mass in P:M ; 4 double force in P:F ; 5 TreeNode<L> left in Links ; 6 TreeNode<R> \nright in Links ; 7 void initTree(double mass, double force) { 8 cobegin { 9 /* reads Links writes L:M \n*/ 10 left.mass = mass; 11 /* reads Links writes L:F */ 12 left.force = force; 13 /* reads Links writes \nR:M */ 14 right.mass = mass; 15 /* reads Links writes R:F */ 16 right.force = force; 17 } 18 } 19 } \nFigure 3. Extension of Figure 1 showing the use of region nesting and region parameters. TreeNode<L> \nTreeNode<R> double mass L : M double mass R : M double force L : F double force R : F ... ... * : M \nvs. * : F L : * vs. R : * Figure 4. Graphical depiction of the distinctions shown in Figure 3. The * \ndenotes any sequence of RPL elements; this notation is explained further in Section 3.2. adding a force \n.eld to the TreeNode class, and by making the initTree method (line 7) set the mass and force .elds of \nthe left and right child in four parallel statements in a cobegin block (lines 9 16). To establish that \nthe parallelism is safe (i.e., that lines 9 16 access disjoint locations), we place .elds mass and force \nin distinct regions P:M and P:F, and the links left and right in a separate region Links (since they \nare only read). The parameter P appears in both regions and P is bound to different regions (L and R) \nfor the left and right sub\u00adtrees, because of the different instantiations of the parametric type TreeNode \nfor the .elds left and right. Because the names L and R used in the types are distinct, we can distin\u00adguish \nthe effects on left (lines 10 12) from the effects on right (lines 14 16). And because the names M and \nF are distinct, we can distinguish the effects on the different .elds within an object i.e., lines 10 \nvs. 14 and lines 12 vs. 16, from each other. Figure 4 shows this situation graphically.  3.2 Specifying \nSets of Regions Partially speci.ed RPLs. To express recursive parallel algo\u00adrithms, we must specify effects \non sets of regions (e.g., all regions under R ). To do this, we introduce partially speci\u00ad.ed RPLs. A \npartially speci.ed RPL contains the symbol * 1 class TreeNode<region P> { 2 region Links, L, R, M, F; \n3 double mass in P:M ; 4 double force in P:F ; 5 TreeNode<P:L> left in Links ; 6 TreeNode<P:R> right \nin Links ; 7 TreeNode<*> link in Links ; 8 void computeForces() reads Links, *:M writes P:*:F { 9 cobegin \n{ 10 /* reads *:M writes P:F */ 11 this.force = (this.mass * link.mass) * R_GRAV; 12 /* reads Links, \n*:M writes P:L:*:F */ 13 if (left != null) left.computeForces(); 14 /* reads Links, *:M writes P:R:*:F \n*/ 15 if (right != null) right.computeForces(); 16 } 17 } 18 } Figure 5. Recursive computation showing \nthe use of par\u00adtially speci.ed RPLs for effects and subtyping. ( star ) as an RPL element, standing in \nfor some unknown sequence of names. An RPL that contains no * is fully spec\u00adi.ed. For example, consider \nthe code shown in Figure 5. Here we are operating on the same TreeNode shown in Figs. 1 and 3, except \nthat we have added (1) a link .eld (line 7) that points to some other node in the tree and (2) a computeForces \nmethod (line 8) that recursively descends the tree. At each node, computeForces follows link to another \nnode, reads the mass .eld of that node, computes the force between that node and this one, and stores \nthe result in the force .eld of this node. This computation can safely be done in parallel on the subtrees \nat each level, because each call writes only the force .eld of this, and the operations on other nodes \n(through link) are all reads of the mass, which is distinct from force. To write this computation, we \nneed to be able to say, for example, that line 13 writes only the left subtree, and does not touch the \nright subtree. Distinctions from the left. In lines 11 15 of Figure 5, we need to distinguish the write \nto this.force (line 11) from the writes to the force .elds in the subtrees (lines 13 and 15). We can \nuse partially speci.ed RPLs to do this. For example, line 8 says that computeForces may read all regions \nunder Links and write all regions under P that end with F. If RPLs R1 and R2 are the same in the .rst \nn places, they differ in place n +1, and neither contains a * in the .rst n +1 places, then (because \nthe regions form a tree) the set of regions under R1 and the set of regions under R2 have empty intersection. \nIn this case we say that R1:* and R2:* are disjoint, and we know that effects on these two RPLs are noninterfering. \nWe call this a distinction from the left, because we are using the distinctness of the names to the left \nof any star to infer that the region sets are non-intersecting. For example, a distinction from the left \nestablishes that the region sets P:F, P:L:*:F,and P:R:*:F (shown in lines 10\u00ad15) are disjoint, because \nthe RPLs all start with P and differ in the second place. Distinctions from the right. Sometimes it \nis important to specify all .elds x in any node of a tree. For example, in lines 10 15, we need to show \nthat the reads of the mass .elds are distinct from the writes to the force .elds. We can make this kind \nof distinction by using different names after the star: if R1 and R2 differ in the nth place from the \nright, and neither contains a * in the .rst n places from the right, then a simple syntactic argument \nshows that their region sets are disjoint. We call this pattern a distinction from the right, because \nthe names that ensure distinctness appear to the right of any star. For example, in lines 10 15, we can \ndistinguish the reads of *:M from the writes to P:L:*:F and P:R:*:F. More complicated patterns. More \ncomplicated RPL pat\u00adterns like Root:*:A:*:B are supported by the type system. Although we do not expect \nthat programmers will need to write such patterns, they sometimes arise via parameter sub\u00adstitution when \nthe compiler is checking effects.  3.3 Subtyping and Type Casts Subtyping. Partially speci.ed RPLs are \nalso useful for sub\u00adtyping. For example, in Figure 5, we needed to write the type of a reference that \ncould point to a TreeNode<P>,for any binding to P. With fully speci.ed RPLs we cannot do this, because \nwe cannot write a type to which we can assign both TreeNode<L> and TreeNode<R>. The solution is to use \na partially speci.ed RPL in the type, e.g., TreeNode<*>,as shown in line 7 of Figure 5. Now we have a \ntype that is .ex\u00adible enough to allow the assignment, but retains soundness by explicitly saying that \nwe do not know the actual region. The subtyping rule is simple: C<R1> is a subtype of C<R2> if the set \nof regions denoted by R1 is included in the set of regions denoted by R2. We write R . R2 to denote set \ninclusion for the corresponding sets of regions. If R1 and R2 are fully speci.ed, then R1 . R2 implies \nR = R2. Note that nesting and inclusion are related: R1 = R2 implies R1 . R2:*. However, nesting alone \ndoes not imply inclusion of the corresponding sets. For example, A:B = A,but A:B . A, because A:B and \nA denote distinct regions. In Section 6 we discuss the rules for nesting, inclusion, and disjointness \nof RPLs more formally. Figure 6 illustrates one possible heap typing resulting from the code in Figure \n5. The DPJ typing discipline ensures the object graph restricted to the left and right references is \na tree. However, the full object graph including the link references is more general and can even include \ncycles, as illustrated in Figure 6. Note how our effect system is able to prove that the updates to different \nsubtrees are distinct, even though (1) non-tree edges exist in the graph; and (2) those edges are followed \nto do possibly overlapping reads. Type casts. DPJ allows any type cast that would be legal for the types \nobtained by erasing the region variables. This approach is sound if the region arguments are consistent. \nFor example, given class B<region R> extends A<R>, a cast from A<r> to B<r> is sound, because either \nthe ref\u00aderence is B<r>, or it is not any sort of B, which will cause Figure 6. Heap typing from Figure \n5. Reference values are shown by arrows; tree arrows are solid, and non-tree arrows are dashed. Notice \nthat all arrows obey the subtyping rules.  a ClassCastException at runtime. However, a cast from Object \nto B<r1> is unsound and could violate the deter\u00adminism guarantee, because the Object could be a B<r2>, \nwhich would not cause a runtime exception. The compiler allows this cast, but it issues a warning.  \n4. Arrays DPJ provides two novel capabilities for computing with arrays: index-parameterized arrays and \nsubarrays. Index\u00adparameterized arrays allow us to traverse an array of object references and safely update \nthe objects in parallel, while subarrays allow us to dynamically partition an array into disjoint pieces, \nand give each piece to a parallel subtask. 4.1 Index-Parameterized Arrays A basic capability of any language \nfor deterministic paral\u00adlelism is to operate on elements of an array in parallel. For a loop over an \narray of values, it is suf.cient to prove that each iteration accesses a distinct array element (we call \nthis a unique traversal). For a loop over an array of references to mutable objects, however, a unique \ntraversal is not enough: we must also prove that any memory locations updated by following references \nin distinct array cells (possibly through a chain of references) are distinct. Proving this property \nis very hard in general, if assignments are allowed into refer\u00adence cells of arrays. No previous effect \nsystem that we are aware of is able to ensure disjointness of updates by follow\u00ading references stored \nin arrays, and this seriously limits the ability of those systems to express parallel algorithms. In \nDPJ, we make use of the following insight: Insight 1. We can de.ne a special array type with the re\u00adstriction \nthat an object reference value o assigned to cell n (where n is a natural number constant) of such an \narray has a runtime type that is parameterized by n. If accesses through cell n touch only region n (even \nby following a chain 1 class Body<region P> { 2 region Link, M, F ; 3 double mass in P:M ; 4 double \nforce in P:F ; 5 Body<*> link in Link ; 6 void computeForce() reads Link, *:M writes P:F { 7 force = \n(mass * link.mass) * R_GRAV; 8 } 9 } 10 11 final Body<[_]> []<[_] > bodies = new Body<[_]> [N]<[_]> \n; 12 foreach (int i in 0, N) { 13 /* writes [i] */ 14 bodies[i] = new Body<[i]> (); 15 } 16 foreach (int \ni in 0, N) { 17 /* reads [i], Link, *:M writes [i]:F */ 18 bodies[i].computeForce(); 19 } Figure 7. Example \nusing an index-parameterized array. of references), then the accesses through different cells are guaranteed \nto be disjoint. We call such an array type an index-parameterized array. To represent such arrays, we \nintroduce two language con\u00adstructs: 1. An array RPL element written [e],where e is an integer expression. \n 2. An index-parameterized array type that allows us to write the region and type of array cell e using \nthe array RPL element [e]. For example, we can specify that cell e resides in region Root:[e] and has \ntype C<Root:[e]>.  At runtime, if e evaluates to a natural number n, then the static array RPL element \n[e] evaluates to the dynamic array RPL element [n]. The key point here is that we can distinguish C<[e1]> \nfrom C<[e2]> if e1 and e2 always evaluate to unequal val\u00adues at runtime, just as we can distinguish C<r1> \nfrom C<r2>, where r1 and r2 are declared names, as discussed in Sec\u00adtion 3.1. Obviously, the compiler \ns capability to distinguish such types will be determined by its ability to prove the in\u00adequality of \nthe symbolic expressions e1 and e2. This is pos\u00adsible in many common cases, for the same reason that \nar\u00adray dependence analysis is effective in many, though not all, cases [24]. The key bene.t is that the \ntype checker has then proved the uniqueness of the target objects, which would not follow from dependence \nanalysis alone. In DPJ, the notation we use for index-parameterized ar\u00adrays is T []<R>#i,where T is a \ntype, R is an RPL, #i de\u00adclares a fresh integer variable i in scope over the type, and [i] may appear \nas an array RPL element in T or R (or both). This notation says that array cell e (where e is an in\u00adteger \nexpression) has type T [i . e] and is located in region R[i . e]. For example, C<r1:[i]>[]<r2:[i]>#i \nspeci\u00ad.es anarraysuch that cell e has type C<r1:[e]> and resides in region r2:[e].If T itself is an array \ntype, then nested index variable declarations can appear in the type. However, the most common case is \na single-dimensional array, which needs only one declaration. For that case, we provide a sim\u00ad 10 90 \n Figure 8. Heap typing from Figure 7. The type of array cell i is parameterized by i. Cross-links are \npossible, but if any links are followed to access other array cells, the effects are visible. pli.ed \nnotation: the user may omit the #i and use an un\u00adderscore ( ) as an implicitly declared variable. For \nexample, C<[ ]>[]<[ ]> is equivalent to C<[i]>[]<[i]>#i. Figure 7 shows an example, which is similar \nin spirit to the Barnes-Hut force computation discussed in Section 7. Lines 1 9 declare a class Body. \nLine 11 declares and creates an index-parameterized array bodies with N cells, such that cell i resides \nin region [i] and points to an object of type Body<[i]>. Figure 8 shows a sample heap typing, for some \nparticular value n of N. Lines 12 15 show a foreach loop that traverses the in\u00addices i . [0,n - 1] in \nparallel and initializes cell i with a new object of type Body<[i]>. The loop is noninterfer\u00ading because \nthe type of bodies says that cell bodies[i] resides in region [i], so distinct iterations i and j write \ndisjoint regions [i] and [j]. Lines 16 19 are similar, ex\u00adcept that the loop calls computeForce on each \nof the objects. In iteration i of this loop, the effect of line 16 is reads [i], because it reads bodies[i], \ntogether with reads Link, *:M writes [i]:F, which is the declared effect of method computeForce (line \n6), after substituting [i] for P. Again, the effects are noninterfering for i = j. To maintain soundness, \nwe just need to enforce the in\u00advariant that, at runtime, cell A[i] never points to an object of type \nC<[j]>,if i = j. The compiler can enforce this in\u00advariant through symbolic analysis, by requiring that \nif type C<[e1]> is assigned to type C<[e2]>,then e1 and e2 must always evaluate to the same value at \nruntime; if it cannot prove this fact, then it must conservatively disallow the as\u00adsignment. In many \ncases (as in the example above) the check is straightforward. Note that because of the typing rules, \nno two distinct cells of an index-parameterized array can point to the same object. However, it is perfectly \nlegal to reach the same object by following chains of references from distinct array cells, as shown \nin Figure 8. In that case, in a parallel traversal over the array, either the common object is not updated, \nin which case the parallelism is safe; or a write effect on the same region appears in two distinct iterations \nof a parallel loop, in which case the compiler can catch the error. Note also that while no two cells \nin an index-parameterized array can alias, references may be freely shared with other   1 class QSort<region \nP> { 2 DPJArrayInt<P> A in P ; 3 QSort(DPJArray<P> A) pure { this.A = A; } 4 void sort() writes P:* { \n5 if (A.length <= SEQ_LENGTH) { 6 seqSort(); 7 } else { 8 /* Shuffle A and return pivot index */ 9 int \np = partition(A); 10 /* Divide A into two disjoint subarrays at p */ 11 final DPJPartitionInt<P> segs \n= 12 new DPJPartitionInt<P> (A, p, OPEN); 13 cobegin { 14 /* writes segs:[0]:* */ 15 new QSort<segs:[0]:*>(segs.get(0)).sort(); \n16 /* writes segs:[1]:* */ 17 new QSort<segs:[1]:*>(segs.get(1)).sort(); 18 } 19 } 20 } 21 } Figure \n9. Writing quicksort with the partition operation. DPJArrayInt and DPJPartitionInt are specializations \nto int values. In line 12, the argument OPEN indicates that we are omitting the partition index from \nthe subarrays, i.e., they are open intervals. variables (including cells in other index-parameterized \nar\u00adrays), unlike linear types [26, 12, 13]. For example, if cell i of a particular array has type C<[i]>, \nthe object it points to could be referred to by cell i of any number of other ar\u00adrays (with the same \ntype), or by any reference of type C<*>. Thus, when we are traversing the array, we get the bene.t of \nthe alias restriction imposed by the typing, but we can still have as many other outstanding references \nto the objects as we like. The pattern does have some limitations: for example, we cannot move an element \nfrom position i to position j in the array C<[i]>[]#i. However, we can copy the references into a different \narray C<*>[] and shuf.e those references as much as we like, though we cannot use those references to \nupdate the objects in parallel. We can also make a new copy of element i with type C<[j]> and store the \nnew copy into position j. This effectively gives a kind of reshuf.ing, although the copying adds performance \noverhead. Another limitation is that our foreach currently only allows regular array traversals (including \nstrided traversals), though it could be extended to other unique traversals.  4.2 Subarrays A familiar \npattern for writing divide and conquer recursion is to partition an array into two or more disjoint pieces \nand give each array to a subtask. For example, Figure 9 shows a standard implementation of quicksort, \nwhich divides the array in two at each recursive step, then works in parallel on the halves. DPJ supports \nthis pattern with three novel features, which we illustrate with the quicksort example. First, DPJ provides \na class DPJArray that wraps an ordi\u00adnary Java array and provides a view into a contiguous seg\u00adment of \nit, parameterized by start position S and length L.In Figure 9, the QSort constructor (line 3) takes \na DPJArray object that represents a contiguous subrange of the caller s array. We call this subrange \na subarray. Notice that the DPJArray object does not replicate the underlying array; it stores only a \nreference to the underlying array, and the val\u00adues of S and L.The DPJArray object translates access to \nelement i into access to element S + i of the underlying ar\u00adray. If i< 0 or i = L, then an array bounds \nexception is thrown, i.e., access through the subarray must stay within the speci.ed segment of the original \narray. Second, DPJ provides a class DPJPartition,represent\u00ading an indexed collection of DPJArray objects, \nall of which point into mutually disjoint segments of the original ar\u00adray. To create a DPJPartition, \nthe programmer passes a DPJArray object into the DPJPartition constructor, along with some arguments \nthat say how to do the splitting. Lines 11 12 of Figure 9 show how to split the DPJArray A at index p, \nand indicate that position p is to be left out of the resulting disjoint segments. The programmer can \naccess segment i of the partition segs by saying segs.get(i), as shown in lines 15 and 17. Third, to \nsupport recursive computations, we need a slight extension to the syntax of RPLs (Section 3). Notice \nthat we cannot use a simple region name, like r, for the type of a partition segment, because different \npartitions can divide the same array in different ways. Instead, we allow a final local variable z (including \nthis) of class type to appear at the head of an RPL, for example z:r.The variable z stands in for the \nobject reference o stored into the variable at runtime, which is the actual region. Using the object \nreference as a region insures that different partitions get different regions, and making the variable \nfinal ensures that it always refers to the same region. We make these z regions into a tree as follows. \nIf z s type is C<R,...>,then z is nested under R; the .rst region parameter of a class functions like \nthe owner parameter in an object ownership system [18, 16]. In the particular case of DPJPartition, if \nthe type of z is DPJPartition<R>,then the type of z.get(i) is z:[i]:*,where z = R. Internally, the get \nmethod uses a type cast to generate a DPJArray of type this:[i]:* that points into the underlying array. \nThe type cast is not sound within the type system, but it is hidden from the user code in such a way \nthat all well-typed uses of DPJPartition are noninterfering. In Figure 9, the sequence of recursive sort \ncalls creates a tree of QSort objects, each in its own region. The cobegin in lines 13 17 is safe because \nDPJPartition guarantees that the segments segs.get(0) and segs.get(1) passed into the recursive parallel \nsort calls are disjoint. In the user code, the compiler uses the type and effect annota\u00adtions to prove \nnoninterference as follows. First, from the type of QSort and the declared effect of sort (line 4), the \ncompiler determines that the effects of lines 15 and 17 are writes segs:[0]:* and writes segs:[1]:*,as \nshown. Second, the regions segs:[0]:* and segs:[1]:* are dis\u00adjoint, by a distinction from the left (Section \n3.2). Finally, the effect writes P:* in line 4 correctly summarizes the ef\u00adfects of sort, because lines \n6 and 9 write P, lines 15 and 17 write under segs,and segs is under P, as explained above. Notice that \nDPJPartition can create multiple refer\u00adences to overlapping data with different regions in the types. \nThus, there is potential for unsoundness here if we are not careful. To make this work, we must do two \nthings. First, if z1 and z2 represent different partitions of thesamearray, then z1.get(0) and z2.get(1) \ncould overlap. Therefore, we must not treat them as disjoint. This is why we put * at the end of the \ntype z:[i]:* of z.get(i); otherwise we could incorrectly distinguish z1:[0] from z2:[1], using a distinction \nfrom the right. Sec\u00adond, if z has type DPJPartition<R>,then z.get(i) has type DPJArray<z:[i]:*> and points \ninto a DPJArray<R>. Therefore, we must not treat z:[i]:* as disjoint from R. Here, we simply do not include \nthis distinction in our type system. All we say is that z:[i]:* = R. See Section 6.3 and Appendix C.2 \nfor further discussion of the disjointness rules in our type system.  5. Commutativity Annotations \nSometimes to express parallelism we need to look at inter\u00adference in terms of higher-level operations \nthan read and write [29]. For example, insertions into a concurrent Set can go in parallel and preserve \ndeterminism even though the or\u00adder of interfering reads and writes inside the Set implemen\u00adtation is \nnondeterministic. Another such example is comput\u00ading connected components of a graph in parallel. In \nDPJ, we address this problem by adding two fea\u00adtures. First, classes may contain declarations of the \nform m ' commuteswith m ',where m and m are method names, in\u00addicating that any pair of invocations of \nthe named methods may be safely done in parallel, regardless of the read and write effects of the methods. \nSee Figure 10(a). In effect, the commuteswith annotation says that (1) the two invocations are atomic \nwith respect to each other, i.e., the result will be as if one occurred and then the other; and (2) either \norder of invocation produces the same result. The commutativity property itself is not checked by the \ncompiler; we must rely on other forms of checking (e.g., more complex program logic [52] or static analysis \n[42, 4]) to ensure that methods declared to be commutative are really commutative. In practice, we anticipate \nthat commuteswith will be used mostly by library and framework code that is written by experienced programmers \nand extensively tested. Our effect system does guarantee deterministic results for an application using \na commutative operation, assuming that the operation declared commutative is indeed commutative. Second, \nour effect system provides a novel invocation ef\u00adfect of the form invokes m with E. This effect records \nthat an invocation of method m occurred with underlying effects E. The type system needs this information \nto represent and 1 class IntSet<region P> { 2 void add(int x) writes P { ... } 3 add commuteswith add; \n4 } (a) Declaration of IntSet class with commutative method add 1 IntSet<R> set = new IntSet<R> (); 2 \nforeach (int i in 0, N) 3 /* invokes IntSet.add with writes R */ 4 set.add(A[i]); (b) Using commuteswith \nfor parallelism 1 class Adder<region P> { 2 void add(IntSet<P> set, int i) 3 invokes IntSet.add with \nwrites P { 4 set.add(i); 5 } 6 } 7 IntSet<R> set = new IntSet<R> (); 8 Adder<R> adder = new Adder<R> \n(); 9 foreach (int i in 0, N) 10 /* invokes IntSet.add with writes R */ 11 adder.add(set, A[i]); (c) \nUsing invokes to summarize effects Figure 10. Illustration of commuteswith and invokes. check effects \nsoundly in the presence of commutativity an\u00adnotations: for example, in line 4 of Fig. 10(b), the compiler \nneeds to record that add was invoked there (so it can dis\u00adregard the effects of other add invocations) \nand that the un\u00adderlying effect of the method was writes R (so it can verify that there are no other \ninterfering effects, e.g., reads or writes of R, in the invoking code). When there are one or more intervening \nmethod calls be\u00adtween a foreach loop and a commutative operation, it may also be necessary for a method \neffect summary in the pro\u00adgram text to specify that an invocation occurred inside the method. For example, \nin Figure 10(c), the add method is called through a wrapper object. We could have correctly speci.ed \nthe effect of Adder.add as writes P, but this would hide from the compiler the fact that Adder.add com\u00admutes \nwith itself. Of course we could use commuteswith for Adder.add, but this is highly unsatisfactory: it \njust propa\u00adgates the unchecked commutativity annotation out through the call chain in the application \ncode. The solution is to specify the invocation effect invokes IntSet.add with writes P,as shown. Notice \nthat the programmer-speci.ed invocation effect exposes an internal implementation detail (i.e., that \na par\u00adticular method was invoked) at a method interface. However, we believe that such exposure will \nbe rare. In most cases, the effect invokes C.m with E will be conservatively summa\u00adrized as E (Section \n6.1 gives the formal rules for covering effects). The invocation effect will only be used for cases where \na commutative method is invoked, and the commu\u00adtativity information needs to be exposed to the caller. \nWe believe these cases will generally be con.ned to high-level public API methods, such as Set.add in \nthe example given in Figure 10. Meaning Symbol De.nition Programs program region* class* e Regions Classes \nRPLs Fields region class R .eld region r class C<P > { .eld* method* comm * }Root | P | z | R : r | R \n:[i] | R : * Tf in Rf Types Methods Effects Expressions T method E e C<R> | T []<R>#i Tm(Tx) E { e }\u00d8| \nreads R | writes R |invokes C.m with E | E . E let x = e in e | this.f = z | this.f |z[n]= z | z[n] | \nz.m(z) | z | new C<R> |new T [n]<R>#i Variables Commutativity z comm this | x m commuteswith m Figure \n11. Core DPJ syntax. C, P , f, m, x, r,and i are identi.ers, and n is a natural number. Rf denotes a \nfully speci.ed RPL (i.e., containing no *).   6. The Core DPJ Type System We have formalized a subset \nof DPJ, called Core DPJ.To make the presentation more tractable and to focus attention on the important \naspects of the language, we make the fol\u00adlowing simpli.cations: 1. We present a simple expression-based \nlanguage, omitting more complicated aspects of the real language such as statements and control .ow. \n 2. Our language has classes and objects, but no inheritance. 3. Region names r are declared at global \nscope, instead of at class scope. Every class has one region parameter, and every method has one formal \nparameter. 4. To avoid dealing with integer variables and expressions, we require that array indices \nare natural number literals.  Removing the .rst simpli.cation adds complexity but raises no signi.cant \ntechnical issues. Adding inheritance raises standard issues for formalizing an object-oriented language. \nWe omit those here in order to focus on the novel aspects of our system, but we describe them in [10]. \nRemoving simpli.cations 3 and 4 is mostly a matter of bookkeeping. To handle arrays in the full language, \nwe need to prove equivalence and non-equivalence of array index expressions, but this is a standard compiler \ncapability. We have chosen to make Core DPJ a sequential language, in order to focus on our mechanisms \nfor expressing effects and noninterference. In Section 6.4, we discuss how to ex\u00adtend the formalism to \nmodel the cobegin and foreach con\u00adstructs of DPJ. 6.1 Syntax and Static Semantics Figure 11 de.nes the \nsyntax of Core DPJ. The syntax con\u00adsists of the key elements described in the previous sections (RPLs, \neffects, and commutativity annotations) hung upon a toy language that is suf.cient to illustrate the \nfeatures yet reasonable to formalize. A program consists of a number of region declarations, a number \nof class declarations, and an expression to evaluate. Class de.nitions are similar to Java s, with the \nrestrictions noted above. (a) Programs t program Valid program t class Valid class de.nition tG Valid \nenvironment G t .eld Valid .eld G t method Valid method G t comm Valid commutativity annotation (b) RPLs \nG tR Valid RPL G tR = R' R under R' G tR . R' R included in R' (c) Types G tT Valid type G tT = T ' \nT a subtype of T ' (d) Effects G tE Valid effect G tE . E' E a subeffect of E' (e) Expressions G te \n: T,E e has type T and effect E in G Figure 12. Core DPJ type judgments. We extend the judg\u00adments to \ngroups of things (e.g., G . .eld*) in the obvious way. We de.ne the static semantics of Core DPJ with \nthe judgments stated in Figure 12. The judgments are de.ned with respect to an environment G, where each \nelement of G is one of the following: A binding z . T stating that variable z has type T . These elements \ncome into scope when a new variable (let variable or formal parameter) is introduced.  A constraint \nP . R stating that region parameter P is in scope and included in region R. These elements come into \nscope when we capture the type of a variable used for an invocation (see the discussion of expression \ntyping judgments below).  An integer variable i. These elements come into scope when we are evaluating \nan array type or new array ex\u00adpression.  The formal rules for making the judgments are stated in full \nin Appendix A. Below we brie.y discuss each of the .ve groups of judgments. Programs. These judgments \nstate that a program and its top-level components (classes, methods, etc.) are valid. Most rules just \nrequire that the component s components are valid in the surrounding environment. The rule for valid \nmethod de.nitions (METHOD) requires that the method body s type and effect are a subtype and subeffect \nof the return type and declared effect. These constraints ensure that we can use the method declaration \nto reason soundly about a method s return type and effect when we are typing method invocation expressions. \nRPLs. These judgments de.ne validity, nesting, and in\u00adclusion of RPLs. Most rules are a straightforward \nformal translation of the relations that we described informally in Section 3.2. The key rule states \nthat if R is under R ' in some environment, then R is included in R ' :* in that en\u00advironment: (INCLUDE-STAR) \nG tR = R ' G tR . R ' : * Types. These de.ne when one type is a subtype of an\u00adother. The class subtyping \nrule is just the formal statement of the rule we described informally in Section 3.3: (SUBTYPE-CLASS) \nG tR . R ' G tC<R> = C<R ' > The array subtyping rule is similar: ' (SUBTYPE-ARRAY) G .{i} tR . R ' [i \n' . i] T = T ' G tT []<R>#i = T []<R ' >#i ' Here = means identity of element types up to the names of \ninteger variables i. More .exible element subtyping is not possible without sacri.cing soundness. We \ncould allow unsound assignments and check for them at runtime (as Java does for class subtyping of array \nelements), but this would require that we retain the class region binding information at runtime. Effects. \nThese judgments de.ne when an effect is valid, and when one effect is a subeffect of another. Intuitively, \nE is a subeffect of E ' means that E ' contains all the effects of E, i.e., we can use E ' as a (possibly \nconservative) summary of E. The rules for reads, writes, and effect unions are standard [16, 33], but \nthere are two new rules for invocation effects. First, if E ' covers E, then an invocation of some method \nwith E ' covers an invocation of the same method with E: (SE-INVOKES-1) G tE . E ' G t invokes C.m with \nE . invokes C.m with E ' Second, we can conservatively summarize the effect invokes C.m with E as just \nE: (SE-INVOKES-2) G t invokes C.m with E . E Expressions. These judgments tell us how to compute the \ntype and effect of an expression. They also ensure that the types of component expressions (for example \nat assignments and method parameter bindings) match in a way that guar\u00adantees soundness. The rules for \n.eld and array access and assignment, variable lookup, and new classes and arrays are straightforward. \nIn the rule for let x = e in e ' , we type e, bind x to the type of e, and type e '.If x appears in the \ntype or effect of e ', we replace it with R:* to generate a type and effect for the whole expression \nthat is valid in the outer scope. In the rule for method invocation (INVOKE), we translate the type Tx \nof the method formal parameter to the current context by creating a fresh region parameter P included \nin the region R of z s type. This technique is similar to how Java handles the capture of a generic wildcard. \nNote that simply substituting R for param(C) in translating Tx would not be sound; see [10] for an explanation \nand an example. Meaning Symbol De.nition RPLs Types dR dT Root | o | dR : r | dR :[i] | dR :[n] | dR \n: * C<dR> Effects dE \u00d8| reads dR | writes dR |invokes C.m with dE | dE . dE Figure 13. Dynamic syntax \nof Core DPJ. dRf denotes a fully-speci.ed dynamic RPL (i.e., containing no *).  We also check that the \nactual argument type is a subtype of the declared formal parameter type, and we report the invocation \nof the method with its declared effect. 6.2 Dynamic Semantics The syntax for entities appearing in the \ndynamic semantics is shown in Figure 13. At runtime, we have dynamic regions (dR), dynamic types (dT \n) and dynamic effects (dE), cor\u00adresponding to static regions (R), types (T ) and effects (E) respectively. \nDynamic regions and effects are not recorded in a real execution, but here we thread them through the \nex\u00adecution state so we can formulate and prove soundness re\u00adsults [16]. We also have object references \no, which are the actual values computed during the execution. The dynamic execution state consists of \n(1) a heap H, which is a function taking values to objects; and (2) a dy\u00adnamic environment dG, which \nis a set of elements of the form z . o (variable z is bound to value o)or P . dR (region parameter P \nis bound to region dR). dG de.nes a natural substitution on RPLs, where we replace the variables with \nvalues and the region parameters with regions as speci\u00ad.ed in the environment. We denote this substitution \non RPL R as dG(R), and we extend this notation to types and ef\u00adfects in the obvious way. Notice that \nwe get the syntax of Figure 13 by applying the substitution dG to the syntax of Figure 11. An object \nis a partial function taking .eld names to object references. If the function is unde.ned on all .eld \nnames, then wesay it is a null object. We use null objects because we need to track the actual types \nof null references to establish soundness. Since the actual implementation does not need to do this tracking, \nit can just use the single value null.Every object reference o . Dom(H) has a type, determined when the \nobject is created, and we write H.o : dT to mean that the reference o has type dT with respect to heap \nH. We write the evaluation rules in large-step semantics no\u00adtation, using the following evaluation function: \n' (e, dG,H) . (o,H , dE), where e is an expression to evaluate, dG and H give the dynamic context for \nevaluation, o is the result of the eval\u00ad ' uation, H is the updated heap, and dE represents the ef\u00adfects \nof the evaluation. A program evaluates to reference o with heap H and effect dE if its main expression \nis e and (e, \u00d8, \u00d8) . (o, H, dE). Section B of the Appendix states the rules for program evaluation. The \nrules are standard for an imperative lan\u00adguage, except that we record read effects in DYN-FIELD\u00adACCESS \nand DYN-ARRAY-ACCESS and write effects in DYN-FIELD-ASSIGN and DYN-ARRAY-ASSIGN. Rules DYN-LET and DYN-INVOKE \naccumulate the effects of the component expressions. Note that when we evaluate new T we eliminate any \n* from T in the dynamic type of the new reference, e.g., new C<Root:*> is the same as new C<Root>; this \nrule ensures that all object .elds are al\u00adlocated in fully speci.ed regions. This rule is sound for the \nsame reason that assigning C<Root> to a variable of type C<Root:*> is sound.  6.3 Soundness Our key \nsoundness result is that we can de.ne and check a static property of noninterference of effect between \nexpres\u00adsions in the language, such that static noninterference im\u00adplies dynamic noninterference. Appendix \nC states the major steps of the proof in formal terms. We divide the steps into three groups: type and \neffect preservation (Section C.1), dis\u00adjointness (Section C.2), and noninterference of effect (Sec\u00adtion \nC.3).We provide further explanation and a full proof in our technical report [10]. Type and effect preservation. \nIn Section C.1, we assert some preliminary de.nitions and the preservation result. A dynamic environment \ndG is valid (De.nition 1) if the types and RPLs appearing on the right of its bindings are valid, and \nit is internally consistent. A heap H is valid (De.ni\u00adtion 2) if the reference stored in every object \n.eld or array cell of H is consistent with the declared type of the .eld or cell, translated to dG. A \ndynamic environment dG instanti\u00adates a static environment G (De.nition 3) if the bindings to variables \nin dG are consistent with the bindings to the corre\u00adsponding variables in G, after translation to dG. \nTheorem 1 establishes that we can use the static types and effects (Section 6.1) to reason soundly about \ndynamic types and effects (Section 6.2). It states that if we type an expression e in environment G, \nand we evaluate e in dynamic environment dG,where dG instantiates G,then (a) the evaluation takes a valid \nheap to a valid heap; (b) the static type of e bounds the dynamic type of the value o that results from \nthe evaluation; and (c) the static effect of e bounds the dynamic effect that results from the evaluation. \nDisjoint RPLs. In Section C.2, we formally de.ne a dis\u00adjointness relation on pairs of RPLs (G R # R '). \nThe relation formalizes distinctions from the left and right, as discussed informally in Section 3.2. \nDe.nition 4 formally expresses how to interpret a dynamic RPL as a set of fully-speci.ed RPLs (i.e., \nregions). De.nition 5 shows how to associate every object .eld and array cell with a region of the heap. \nProposition 1 states that disjoint RPLs imply disjoint sets of fully speci.ed regions, i.e., disjoint \nsets of locations. Propo\u00adsition 2 states that at runtime, disjoint fully-speci.ed regions imply disjoint \nlocations. Noninterference. In Section C.3, we formally de.ne a noninterference relation on pairs of \nstatic effects (G E # E '). The rules express four basic facts: (1) reads com\u00admute with reads; (2) writes \ncommute with reads or writes if the regions are disjoint; (3) invocations commute with other effects \nif the underlying effects are disjoint; and (4) two in\u00advocations commute if the methods are declared \nto commute, regardless of interference between the underlying effects. Theorem 2 expresses the main \nsoundness property of Core DPJ, which is that the execution order of noninterfering expressions does \nnot matter. It states that in a well-typed '' program, if e and e are expressions with types T and T \nand effects E and E ',and E and E ' are noninterfering, then ' either order of evaluating e and e produces \nthe same values o and o ', the same effects dE and dE ', and the same .nal heap H. The claim is true \nfor dynamic effects from the commuta\u00adtivity of reads, the disjointness results of Section C.2, and the \nassumed correctness of the commutativity speci.cations for methods. The claim is true for static effects \nby the type and effect preservation property above. See [10] for the formal proof. 6.4 Deterministic \nParallelism As discussed in Sections 2 and 4, the actual DPJ language in\u00adcludes foreach for parallel \nloops and cobegin for a block of parallel statements. We brie.y discuss how to extend the formalism to \nmodel these constructs. We can easily simulate cobegin by adding a parallel ' composition operator e|e \n', which says to execute e and e in the same environment, in an unspeci.ed order, with an implicit join \nat the end of the execution. We can simulate foreach by allowing an induction variable i to appear in \nexpressions inside the scope of a foreach, mapping i to n over the index range of the foreach, and evaluating \nall en in unspeci.ed order. In both cases we can extend the static ' typing rules to say that for any \npair of expressions e and e as to which the order of execution is unspeci.ed, then the ' effects of \ne and e must be noninterfering. It follows directly from Theorem 2 that parallel composi\u00adtion of noninterfering \nexpressions produces the same result as sequential composition of those expressions. This guaran\u00adtees \ndeterminism of execution regardless of the order of par\u00adallel execution. The formalization of this property \nis straight\u00adforward, and we omit it from our technical report.  7. Evaluation We have carried out a \npreliminary evaluation of the language and type system features presented in this paper. Our evalu\u00adation \naddressed the following questions: Expressiveness. Can the type system express important parallel algorithms \non object-oriented data structures? When does it fail to capture parallelism and why?  Coverage. Are \neach of the new features in the DPJ type system important to express one or more of these algo\u00adrithms? \n  Performance. For each of the algorithms, what increase in performance is realized in practice? This \nis a quan\u00adtitative measure of how much parallelism the type sys\u00adtem can express for each algorithm (note \nthat the runtime overheads introduced by DPJ are negligible). To do the evaluation, we extended Sun s \njavac compiler so that it compiles DPJ into ordinary Java source. We built a runtime system for DPJ using \nthe ForkJoinTask frame\u00adwork that will be added to the java.util.concurrent standard library in Java 1.7 \n[2]. ForkJoinTask supports dy\u00adnamic scheduling of lightweight parallel tasks, using a work\u00adstealing scheduler \nsimilar to that in Cilk [8]. The DPJ com\u00adpiler automatically translates foreach to a recursive com\u00adputation \nthat successively divides the iteration space, to a depth that is tunable by the programmer, and it translates \na cobegin block into one task for every statement. Code using ForkJoinTask is compatible with Java threads \nso an existing multithreaded Java program can be incrementally ported to DPJ. Such code may still have \nsome guarantees, e.g., the DPJ portions will be guaranteed deterministic if the explic\u00aditly threaded \nand DPJ portions are separate phases that do not run concurrently. Using the DPJ compiler, we studied \nthe following pro\u00adgrams: Parallel merge sort, two codes from the Java Grande parallel benchmark suite \n(a Monte Carlo .nancial simula\u00adtion and IDEA encryption), the force computation from the Barnes-Hut n-body \nsimulation [45], k-means clustering from the STAMP benchmarks [34], and a tree-based collision de\u00adtection \nalgorithm from a large, real-world open source game engine called JMonkey (we refer to this algorithm \nas Col\u00adlision Tree). For all the codes, we began with a sequential version and modi.ed it to add the \nDPJ type annotations. The Java Grande benchmarks are explicitly parallel versions us\u00ading Java threads \n(along with equivalent sequential versions), and we compared DPJ s performance against those. We also \nwrote and carefully tuned the Barnes-Hut force computation using Java threads as part of understanding \nperformance is\u00adsues in the code, so we could compare Java and DPJ for that one as well. 7.1 A Realistic \nExample We use the Barnes-Hut force computation to show how to write a realistic parallel program in \nDPJ. Figure 14 shows a simpli.ed version of this code. The main simpli.cation is that the Vector objects \nare immutable, with final .elds (so there are no effects on these objects), whereas our actual implementation \nuses mutable objects. The class Node repre\u00adsents an abstract tree node containing a mass and position. \nThe mass and position represent the actual mass and position of a body (at a leaf) or the center of mass \nof a subtree (at an inner node). The Node class has two subclasses: InnerNode, representing an inner \nnode of the tree, and storing an array of children; and Body, representing the body data stored at the \nleaves, and storing a force. The Tree class stores the tree, 1 /* Abstract class for tree nodes */ 2 \nabstract class Node<region R> { 3 region MP; /* Region for mass and position */ 4 double mass in R:MP; \n/* Mass */ 5 Vector pos in R:MP; /* Position */ 6 } 7 8 /* Inner node of the tree */ 9 class InnerNode<region \nR> extends Node<R> { 10 region Children; 11 Node<R:*>[]<R:Children> children in R:Children; 12 } 13 \n14 /* Leaf node of the tree */ 15 class Body<region R> extends Node<R> { 16 region Force; /* Region for \nforce */ 17 Vector force in R:Force; /* Force on this body */ 18 19 /* Compute force of entire subtree \non this body */ 20 Vector computeForce(Node<R:*> subtree) 21 reads R:*:Children, R:*:MP { ... } 22 } \n23 24 /* Barnes-Hut tree */ 25 class Tree<region R> { 26 region Tree; /* Region for tree */ 27 Node<R> \nroot in R:Tree; /* Root */ 28 Body<R:[i]>[]<R:[i]>#i bodies in R:Tree; /* Leaves */ 29 30 /* Compute \nforces on all bodies */ 31 void computeForces() writes R:* { 32 foreach (int i in 0, bodies.length) { \n33 /* reads R:Tree, R:*:Node.Children, R:[i], 34 R:*:Node.MP writes R:[i]:Node.Force */ 35 bodies[i].force \n= bodies[i].computeForce(root); 36 } 37 } 38 } Figure 14. Using DPJ to write the Barnes-Hut force com\u00adputation. \ntogether with an array of Body objects pointing to the leaves of the tree. The method Tree.computeForces \ndoes the force com\u00adputation by traversing the array of bodies and calling the method Body.computeForce \non each one, to compute the force between the body this and subtree.If subtree is a body, or is suf.ciently \nfar away that it can be approximated as a point mass, then Body.computeForce computes and returns the \npairwise interaction between the nodes. Other\u00adwise, it recursively calls computeForce on the children \nof subtree, and accumulates the result. We use a region parameter on the node classes to distin\u00adguish \ninstances of these nodes. Class Tree uses the param\u00adeters to create an index-parameterized array of references \nto distinct body objects; the parallel loop in computeForces iterates over this array. This allows distinctions \nfrom the left for operations on bodies[i] (Section 3). We also use dis\u00adtinct region names within each \nclass (in particular, for the force, masses and positions, and the children array) to en\u00adable distinctions \nfrom the right. The key fact is that, from the effect summary in line 21 and the code in line 35, the \ncompiler infers the effects shown in lines 33 34. Using distinctions from the left and right, the compiler \ncan now prove that (1) the updates are distinct for distinct iterations of the foreach; and (2) all the \nupdates are distinct from the reads. Notice also how the nested RPLs allow us to describe the entire \neffect of computeForces as writes R:*. That is, to the outside world, computeForces just writes under \nthe region parameter of Tree. Thus with careful use of RPLs, we can enforce a kind of encapsulation of \neffects, which is important for modular software design.  7.2 Expressiveness and Coverage We used DPJ \nto express all available parallelism (except for vector parallelism, which we do not consider here) for \nMerge Sort, Monte Carlo, IDEA, K-Means, and Collision Tree. For Barnes-Hut, the overall program includes \nfour major phases in each time step: tree building; center-of-mass computation; force calculations; and \nposition calculations. Expressing the force, center of mass, and position calculations is straightfor\u00adward, \nbut we studied only the force computation (the domi\u00adnant part of the overall computation) for this work. \nDPJ can also express the tree-building phase, but we would have to use a divide-and-conquer approach, \ninstead of inserting bod\u00adies from the root via hand-over-hand locking, as in in [45]. Brie.y, we parallelized \neach of the codes as follows. MergeSort uses subarrays (Section 4.2) to perform in-place parallel divide \nand conquer operations for both merge and sort, switching to sequential merge and sort for subproblems \nbelow a certain size. Monte Carlo uses index-parameterized arrays (Section 4.1) to generate an array \nof tasks and com\u00adpute an array of results, followed by commutativity anno\u00adtations (Section 5) to update \nto globally shared data inside a reduction loop. IDEA uses subarrays to divide the input array into disjoint \npieces, then uses foreach to operate on each of the pieces. Section 7.1 describes our parallel Barnes-Hut \nforce computation. Collision Tree recursively walks two trees, reading the trees and collecting a list \nof intersecting tri\u00adangles. At each node, a separate triangle list is computed in parallel for each subtree, \nand then the lists are merged. Our implementation uses method-local regions to distinguish the writes \nto the left and right subtree lists. K-Means uses com\u00admutativity annotations to perform simultaneous \nreductions, one for each cluster. Table 1 summarizes the novel DPJ ca\u00adpabilities used for each code. \nTable 1. Capabilities Used In The Benchmarks 1. Index-parameterized array; 2. Distinctions from the left; \n3. Distinctions from the right; 4. Recursive subranges; 5. Commutativity annotations. Benchmark 12345 \nMergeSort -Y-Y- MonteCarlo Y Y --Y IDEA -Y-Y\u00adBarnes-Hut Y Y Y -- Collision Tree -Y --- KMeans ----Y \n Our evaluation and experience showed some interesting limitations of the current language design. To \nachieve good cache performance in Barnes-Hut, the bodies must be re\u00adordered according to their proximity \nin space on each time step [45]. As discussed in Section 7.1, we use an index- Num Monte Carlo IDEA Barnes \nHut Cores DPJ Java DPJ Java DPJ Java 2 2.00 1.80 1.95 1.99 1.98 1.99 3 2.82 2.50 2.88 2.97 2.96 2.94 \n 4 3.56 3.09 3.80 3.91 4.94 3.88 7 5.53 4.65 6.40 6.70 6.79 7.56 12 8.01 6.46 9.99 11.04 11.4 13.65 \n17 10.02 7.18 12.70 14.90 15.3 19.04 22 11.50 7.98 18.70 17.79 23.9 23.33  Table 2. Comparison of DPJ \nvs. Java threads performance for Monte Carlo, IDEA encryption, and Barnes Hut. parameterized array to \nupdate the bodies in parallel. As dis\u00adcussed in Section 4.1, this requires that we copy each body with \nthe new destination regions at the point of re-insertion. As future work, we believe we can ease this \nrestriction by adding a mechanism for disjointness checking at runtime. 7.3 Performance We measured \nthe performance of each of the benchmarks on a Dell R900 multiprocessor running Red Hat Linux with 24 \ncores, comprising four six-core Xeon processors, and a total of 48GB of main memory. For each data point, \nwe took the minimum of .ve runs on an idle machine. We studied multiple inputs for each of the benchmarks \nand also experimented with different limits for recursive codes. We present results for the inputs and \nparameter values that show the best performance, since our main aim is to evaluate how well DPJ can express \nthe parallelism in these codes. The sensitivity of the parallelism to input size and/or recursive limit \nparameters is a property of the algorithm and not a consequence of using DPJ. Figure 15 presents the \nspeedups of the six programs for p .{1, 2, 3, 4, 7, 12, 17, 22} processors. All speedups are relative \nto an equivalent sequential version of the program, with no DPJ or other multithreaded runtime overheads.All \nsix codes showed moderate to good scalability for all val\u00adues of p. Barnes-Hut and Merge Sort showed \nnear-ideal per\u00adformance scalability, with Barnes-Hut showing a superlinear increase for p =22 due to \ncache effects. Notably, as shown in Table 2, for the three codes where we have manually parallelized \nJava threads versions avail\u00adable, the DPJ versions achieved speedups close to (IDEA and Barnes Hut), \nor better than (Monte Carlo), the Java ver\u00adsions, for the same inputs on the same machines. We believe \nthe Java threads codes are all reasonably well tuned; the two Java Grande benchmarks were tuned by the \noriginal authors and the Barnes Hut code was tuned by us. The manually parallelized Monte Carlo code \nexhibited a similar leveling off in speedup as the DPJ version did beyond about 7 cores because both \nhave a signi.cant sequential component that makes copies of a large array for each parallel task. Over\u00adall, \nin all three programs, DPJ is able to express the avail\u00adable parallelism as ef.ciently as a lower-level \nhand coded parallel programming model that provides no guarantees of determinism or even race-freedom. \n Our experience so far has shown us that DPJ itself can be very ef.cient, even though both the compiler \nand runtime are preliminary. In particular (except for very small runtime costs for the dynamic partitioning \nmechanism for subarrays), our type system requires no runtime checks or speculation and therefore adds \nnegligible runtime overhead for achiev\u00ading determinism. On the other hand, it is possible that the type \nsystem may constrain algorithmic design choices. The limitation on reordering the array of bodies in \nBarnes-Hut, explained in Section 7.2, is one such example.  7.4 Porting Effort Table 3 shows the number \nof source lines changed and the number of annotations, relative to the program size. Program size is \ngiven in non-blank, non-comment lines of source code, counted by sloccount. The next column shows how \nmany LOC were changed when annotating. The last four columns show (1) the number of declarations using \nthe region keyword (i.e., .eld regions, local regions, and re\u00adgion parameters); (2) the number of RPLs \nappearing as ar\u00adguments to in, types, methods, and effect summaries; (3) the number of method effect \nsummaries, counting reads and writes separately; and (4) the number of commutativ\u00adity annotations. As \nthe table shows, the fraction of lines of code changed was not large, averaging 10.7% of the original \nlines. Most of the changed lines were due to writing RPL arguments when instantiating types (represented \nin column four), followed by writing method effect summaries (column .ve). More importantly, we believe \nthat the overall effort of writing, testing, and debugging a program with any paral\u00adlel programming model \nis dominated by the time required to understand the parallelism and sharing patterns (includ\u00ading aliases), \nand to debug the parallel code. The regions and effects in DPJ provide concrete guidance to the program\u00admer \non how to reason about parallelism and sharing.Once Total Annotated Region Effect Program SLOC SLOC \nDecls RPLs Summ. Comm. MergeSort 295 38 (12.9%) 15 41 7 0 Monte Carlo 2877 220 (7.6%) 13 301 161 1 IDEA \n228 24 (10.5%) 8 22 2 0 Barnes-Hut 682 80 (11.7%) 25 123 38 0 CollisionTree 1032 233 (22.6%) 82 408 58 \n0 K-means 501 5 (1.0%) 0 3 3 1 Total 5615 600 (10.7%) 143 898 269 2 Table 3. Annotation counts for the \ncase studies. the programmer understands the sharing patterns, he or she explicitly documents them in \nthe code through region and ef\u00adfect annotations, so other programmers can gain the bene.t of his or her \nunderstanding. Further, programming tools can alleviate the burden of writing annotations. We have developed \nan interactive port\u00ading tool, DPJIZER [49], that infers many of these annota\u00adtions, using iterative constraint \nsolving over the whole pro\u00adgram. DPJIZER is implemented as an Eclipse plugin and correctly infers method \neffect summaries for a program that is already annotated with region information. We are cur\u00adrently extending \nDPJIZER to infer RPLs, assuming that the programmer declares the regions. In addition, a good set of \ndefaults can further reduce the amount of manually written annotations. For example, if the programmer \ndoes not annotate a class .eld, its default re\u00adgion could be the RPL default-parameter:.eld-name.This \ndefault distinguishes both instances of the same class and .elds within a class. The programmer can override \nthe de\u00adfaults if she needs further re.nements.  8. Related Work We group the related work into .ve broad \ncategories: ef\u00adfect systems (not including ownership-based systems); own\u00adership types (including ownership \nwith effects); unique ref\u00aderences; separation logic; and runtime checks. Effect Systems: The seminal \nwork on types and effects for concurrency is FX [33, 27], which adds a region-based type and effect system \nto a Scheme-like, implicitly parallel lan\u00adguage. Leino et al. [30] and Greenhouse and Boyland [26] .rst \nadded effects to an object-oriented language. None of these systems can represent arbitrarily nested \nstructures or array partitioning, and they cannot specify arbitrarily large sets of regions. Also, the \nlatter two systems rely on alias re\u00adstrictions and/or supplementary alias analysis for soundness of effect, \nwhereas DPJ does not. Ownership Types: Some ownership-based type systems have been combined with effects \nto enable reasoning about noninterference. Both JOE [16, 46] and MOJO [14] have sophisticated effect \nsystems that allow nested regions and effects. However, neither has the capabilities of DPJ s array partitioning \nand partially speci.ed RPLs, which are crucial to expressing the patterns addressed in this paper. JOE \ns under effect shape is similar to DPJ s *, but it cannot do the equivalent of our distinctions from \nthe right. JOE allows slightly more precision than our rule LET when a type or effect uses a local variable \nthat goes out of scope, but we have found that this precision is not necessary for express\u00ading deterministic \nparallelism. MOJO has a wildcard region speci.er ?, but it pertains to the orthogonal capability of multiple \nownership, which allows objects to be placed in multiple regions. Leino s system also has this capability, \nbut without arbitrary nesting. Lu and Potter [32] show how to use effect constraints to break the owner \ndominates rule in limited ways while still retaining meaningful guarantees. The any context of [32] is \nidentical to Root:* in our system, but we can make more .ne-grained distinctions. For example, we can \nconclude that a pair of references stored in variables of type C<R1:*> and C<R2:*> can never alias, if \nR1:* and R2:* are disjoint. Several researchers [11, 3, 28] have described effect sys\u00adtems for enforcing \na locking discipline in nondeterministic programs, to prevent data races and deadlocks. Because they \nhave different goals, these effect systems are very different from ours, e.g., they cannot express arrays \nor nested effects. Finally, an important difference between DPJ and most ownership systems is that we \nallow explicit region declara\u00adtions, like [33, 30, 26], whereas ownership systems gen\u00aderally couple region \ncreation with object creation. We have found many cases where a new region is needed but a new object \nis not, so the ownership paradigm becomes awkward. Supporting .eld granularity effects also is dif.cult \nwith own\u00adership. Unique References: Boyland [13] shows how to use alias restrictions to guarantee determinism \nfor a simple language with pointers. Terauchi and Aiken [48] have extended this work with a type inference \nalgorithm that simpli.es the type annotations and elegantly expresses some simple patterns of determinism. \nAlias restrictions are a well-known alternative to effect annotations for reasoning about heap access, \nand in some cases they can complement effect annotations [26, 12]. However, alias restrictions severely \nlimit the expressivity of an object-oriented language. It is not clear whether the techniques in [13, \n48] could be applied to a robust object\u00adoriented language. Clarke and Wrigstad s external unique\u00adness \n[17] is better suited to an object-oriented style, but it is not clear whether external uniqueness is \nuseful for determin\u00adistic parallelism. Separation Logic: Separation logic [40] (SL) is a poten\u00adtial alternative \nto effect systems for reasoning about shared resources. O Hearn [35] and Gotsman et al. [25] use SL to \ncheck race freedom, though O Hearn includes some simple proofs of noninterference. Parkinson [37] has \nextended C# with SL predicates to allow sound inference in the presence of inheritance. Raza et al. [39] \nshow how to use separation logic together with shape analysis for automatic paralleliza\u00adtion of a sequential \nprogram. While SL is a promising approach, applying it to realis\u00adtic programs poses two key issues. \nFirst, SL is a low-level speci.cation language: it generally treats memory as a sin\u00adgle array of words, \non which notions of objects and linked data structures must be de.ned using SL predicates [40, 35]. Second, \nSL approaches generally either require heavyweight theorem proving and/or a relatively heavy programmer \nan\u00adnotation burden [37] or are fully automated, and thereby lim\u00adited by what the compiler can infer [25, \n39]. In contrast, we chose to start from the extensive prior work on regions and effects, which is more \nmature than SL for OO languages. As noted in [40], type systems and SL systems have many common goals \nbut have developed largely in parallel; as future research it would be useful to understand better the \nrelationship between the two. Runtime Checks: A number of systems enforce some form of disciplined parallelism \nvia runtime checks. Jade [43] and Prometheus [5] use runtime checks to guarantee determin\u00adistic parallelism \nfor programs that do not fail their checks. Jade also supports a simple form of commutativity annota\u00adtion \n[41]. Multiphase Shared Arrays [20] and PPL1 [47] are similar in that they rely on runtime checks that \nmay fail if determinism is violated. None of these systems checks non\u00adtrivial sharing patterns at compile \ntime. Speculative parallelism [7, 23, 51] can achieve deter\u00adminism with minimal programmer annotations, \ncompared to DPJ. However, speculation generally either incurs signif\u00adicant software overheads or requires \nspecial hardware [38, 31, 50]. Grace [7] reduces the overhead of software-only speculation by running \nthreads as separate processes and us\u00ading commodity memory protection hardware to detect con\u00ad.icts at \npage granularity. However, Grace does not ef.ciently support essential sharing patterns such as (1) .ne-grain \nac\u00adcess distinctions (e.g., distinguishing different .elds of an object, as in Barnes-Hut); (2) dynamically \nscheduled .ne\u00adgrain tasks (e.g., ForkJoinTask); or (3) concurrent data struc\u00adtures, which are usually \n.nely interleaved in memory. Fur\u00adther, unlike DPJ, a speculative solution does not document the parallelization \nstrategy or show how the code must be rewritten to expose parallelism. Kendo [36] and DMP [21] use runtime \nmechanisms to guarantee equivalence to some (arbitrary) serial interleaving of tasks; however, that interleaving \nis not necessarily obvi\u00adous from the program text, as it is in DPJ. Further, Kendo s guarantee fails \nif the program contains data races, and DMP requires special hardware support. SharC [6] uses a combi\u00adnation \nof static and dynamic checks to enforce race freedom, but not necessarily deterministic semantics, in \nC programs. Finally, a determinism checker [44, 22] instruments code to detect determinism violations \nat runtime. This approach is not viable for production runs because of the slowdowns caused by the instrumentation, \nand it is limited by the cover\u00adage of the inputs used for the dynamic analysis. However, it is sound \nfor the observed traces.  9. Conclusion We have described a novel type and effect system, together with \na language called DPJ that uses the system to enforce deterministic semantics. Our experience shows that \nthe new type system features are useful for writing a range of pro\u00adgrams, achieving moderate to excellent \nspeedups on a 24\u00adprocessor system with guaranteed determinism. Our future goals are to exploit region \nand effect anno\u00adtations for optimizing memory hierarchy performance; to add runtime support for more \n.exible operationson index\u00adparameterized arrays; to add support for object-oriented par\u00adallel frameworks; \nand to add support for explicitly nondeter\u00administic algorithms.  Acknowledgments The following persons \nprovided invaluable insight at various stages of this work: Ralph Johnson, Christopher Rodrigues, Marc \nSnir, Dan Grossman, Brad Chamberlain, John Brant, Rajesh Karmani, and Maurice Rabb.  References [1] \nhttp://dpj.cs.uiuc.edu. [2] http://gee.cs.oswego.edu/dl/concurrency-interest. [3] M. Abadi, C. Flanagan, \nand S. N. Freund. Types for safe locking: Static race detection for Java. TOPLAS, 2006. [4] F. Aleen \nand N. Clark. Commutativity analysis for software parallelization: letting program transformations see \nthe big picture. ASPLOS, 2009. [5] M. D. Allen, S. Sridharan, and G. S. Sohi. Serialization sets: A dynamic \ndependence-based parallel execution model. PPOPP, 2009. [6] Z. Anderson, D. Gay, R. Ennals, and E. Brewer. \nSharC: Checking data sharing strategies for multithreaded C. PLDI, 2008. [7] E. D. Berger, T. Yang, T. \nLiu, and G. Novark. Grace: Safe Multithreaded Programming for C/C++. OOPSLA, 2009. [8] R.D.Blumofe, C.F.Joerg, \nB.C.Kuszmaul, C.E.Leiserson, K. H. Randall, and Y. Zhou. Cilk: An ef.cient multithreaded runtime system. \nPPOPP, 1995. [9] R. Bocchino, V. Adve, S. Adve, and M. Snir. Parallel pro\u00adgramming must be deterministic \nby default. First USENIX Workshop on Hot Topics in Parallelism (HotPar), 2009. [10] R. L. Bocchino and \nV. S. Adve. Formal de.nition and proof of soundness for Core DPJ. Technical Report UIUCDCS-R\u00ad2008-2980, \nU. Illinois, 2008. [11] C. Boyapati, R. Lee, and M. Rinard. Ownership types for safe programming: Preventing \ndata races and deadlocks. OOPSLA, 2002. [12] J. Boyland. The interdependence of effects and uniqueness. \nWorkshop on Formal Techs. for Java Programs, 2001. [13] J. Boyland. Checking interference with fractional \npermis\u00adsions. SAS, 2003. [14] N. R. Cameron, S. Drossopoulou, J. Noble, and M. J. Smith. Multiple ownership. \nOOPSLA, 2007. [15] P. Charles, C. Grothoff, V. Saraswat, C. Donawa, A. Kielstra, K. Ebcioglu, C. von \nPraun, and V. Sarkar. X10: An object\u00adoriented approach to non-uniform cluster computing. OOP-SLA, 2005. \n [16] D. Clarke and S. Drossopoulou. Ownership, encapsulation and the disjointness of type and effect. \nOOPSLA, 2002. [17] D. Clarke and T. Wrigstad. External uniqueness is unique enough. ECOOP, 2003. [18] \nD. G. Clarke, J. M. Potter, and J. Noble. Ownership types for .exible alias protection. OOPSLA, 1998. \n[19] J. Dennis. Keynote address. PPOPP, 2009. [20] J. DeSouza and L. V. Kal\u00b4e. MSA: Multiphase speci.cally \nshared arrays. LCPC, 2004. [21] J. Devietti, B. Lucia, L. Ceze, and M. Oskin. DMP: Determin\u00adistic Shared \nMemory Multiprocessing. ASPLOS, 2009. [22] M. Feng and C. E. Leiserson. Ef.cient detection of determi\u00adnacy \nraces in Cilk programs. SPAA, 1997. [23] C. Flanagan and M. Felleisen. The semantics of future and its \nuse in program optimization. POPL, 1995. [24] R. Ghiya, D. Lavery, and D. Sehr. On the importance of \npoints-to analysis and other memory disambiguation methods for C programs. PLDI, 2001. [25] A. Gotsman, \nJ. Berdine, B. Cook, and M. Sagiv. Thread\u00admodular shape analysis. PLDI, 2007. [26] A. Greenhouse and \nJ. Boyland. An object-oriented effects system. ECOOP, 1999. [27] R. T. Hammel and D. K. Gifford. FX-87 \nperformance measurements: Data.ow implementation. Technical Report MIT/LCS/TR-421, 1988. [28] B. Jacobs, \nF. Piessens, J. Smans, K. R. M. Leino, and W. Schulte. A programming model for concurrent object\u00adoriented \nprograms. TOPLAS, 2008. [29] M. Kulkarni, K. Pingali, B. Walter, G. Ramanarayanan, K. Bala, and L. P. \nChew. Optimistic parallelism requires ab\u00adstractions. PLDI, 2007. [30] K. R. M. Leino, A. Poetzsch-Heffter, \nand Y. Zhou. Using data groups to specify and check side effects. PLDI, 2002. [31] W. Liu, J. Tuck, L. \nCeze, W. Ahn, K. Strauss, J. Renau, and J. Torrellas. POSH: a TLS compiler that exploits program structure. \nPPOPP, 2006. [32] Y. Lu and J. Potter. Protecting representation with effect encapsulation. POPL, 2006. \n[33] J. M. Lucassen and D. K. Gifford. Polymorphic effect sys\u00adtems. POPL, 1988. [34] C. C. Minh, J. Chung, \nC. Kozyrakis, and K. Oluko\u00adtun. STAMP: Stanford transactional applications for multi\u00adprocessing. IISWC, \n2008. [35] P. W. O Hearn. Resources, concurrency, and local reasoning. Theor. Comp. Sci., 2007. [36] \nM. Olszewski, J. Ansel, and S. Amarasinghe. Kendo: Ef.cient Deterministic Multithreading in Software. \nASPLOS, 2009. [37] M. J. Parkinson and G. M. Bierman. Separation logic, abstrac\u00adtion and inheritance. \nPOPL, 2008. [38] M. K. Prabhu and K. Olukotun. Using thread-level specula\u00adtion to simplify manual parallelization. \nPPOPP, 2003. [39] M. Raza, C. Calcagno, and P. Gardner. Automatic paralleliza\u00adtion with separation logic. \nESOP, 2009. [40] J. C. Reynolds. Separation logic: A logic for shared mutable data structures. Symp. \non Logic in Comp. Sci., 2002. [41] M. C. Rinard. The design, implementation and evaluation of Jade: A \nportable, implicitly parallel programming language. PhD thesis, Stanford University, 1994. [42] M. C. \nRinard and P. C. Diniz. Commutativity analysis: A new analysis technique for parallelizing compilers. \nTOPLAS, 1997. [43] M. C. Rinard and M. S. Lam. The design, implementation, and evaluation of Jade. TOPLAS, \n1998. [44] C. Sadowski, S. N. Freund, and C. Flanagan. SingleTrack: A dynamic determinism checker for \nmultithreaded programs. ESOP, 2009. [45] J. P. Singh, W.-D. Weber, and A. Gupta. SPLASH: Stanford parallel \napplications for shared-memory. Technical report, Stanford University, 1992. [46] M. Smith. Towards an \neffects system for ownership domains. ECOOP, 2005. [47] M. Snir. Parallel Programming Language 1 (PPL1), \nV0.9 Draft. Technical Report UIUCDCS-R-2006-2969, U. Illinois, 2006. [48] T. Terauchi and A. Aiken. \nA capability calculus for concur\u00adrency and determinism. TOPLAS, 2008. [49] M. Vakilian, D. Dig, R. Bocchino, \nJ. Overbey, V. Adve, and R. Johnson. Inferring Method Effect Summaries for Nested Heap Regions. ASE, \n2009. To appear. [50] C. von Praun, L. Ceze, and C. Cas\u00b8caval. Implicit parallelism with ordered transactions. \nPPOPP, 2007. [51] A. Welc, S. Jagannathan, and A. Hosking. Safe futures for Java. OOPSLA, 2005. [52] \nK. Zee, V. Kuncak, and M. Rinard. Full functional veri.cation of linked data structures. PLDI, 2008. \nA. Static Semantics Rules We divide the static semantics in to .ve parts: rules for valid program elements \n(Figure 16), rules for validity, nesting, and inclusion of RPLs ( Figure 17), rules for valid types and \nsubtypes (Figure 18), rules for valid effects and subeffects (Figure 19), and rules for typing expressions \n(Figure 20).  B. Dynamic Semantics Rules Figure 21 gives the rules for evaluating programs. If f : A \n. B is a function, then f .{x . y} is the function ' f : A .{x}. B .{y} de.ned by f '(a)= f(a) if a = \nx and f '(x)= y.new(C) is the function taking each .eld of class C with type T to a null reference of \ntype dG(T ),and ' new(T [n]) is the function taking each index n . [0,n - 1] to a null reference of \ntype dG(T )[i . n ']. The rules for dynamic RPLs, types, and effects are nearly identical to their static \ncounterparts. Instead of writing out all the rules, we describe how to generate them via simple substitution \nfrom the rules given in Section A. For every rule given there except RPL-VAR, RPL-PARAM, UNDER-VAR, INCLUDE-PARAM, \nand INCLUDE-FULL, do the following: (1) append DYN-to the front of the name; (2) replace G with H and \n[i] with [n]; and (3) replace R with dR, T with dT ,and E with dE. For example, here are the rules for \ndynamic class subtyping, generated by the substitution above from the rule SUBTYPE-CLASS: (DYN-SUBTYPE-CLASS) \nHt dR . dR ' HtC<dR> = C<dR ' > Then add the following rules: (DYN-RPL-REF) Hto : dT (DYN-UNDER-REF) \nHto : C<dR> Hto Hto = dR (DYN-TYPE-ARRAY) Ht dT [i . n] Ht dR[i . n] Ht dT []<dR>#i  C. Soundness C.1 \nType and Effect Preservation De.nition 1 (Valid dynamic environments). A dynamic en\u00advironment dG is valid \nwith respect to heap H (H dG)if the following hold: (1) for every binding z . o . dG, H.o : dT ; (2) \nfor every binding P . dR . dG, H dR; and (3) if this . o . dG,then H.o : C<dR>, and param(C) . dR . dG. \nDe.nition 2 (Valid heaps). Aheap H is valid ( .H)if for each o . Dom(H), one of the following holds: \n1. (a) H . o : C<dR> and (b) H.C<dR> and (c) for each .eld Tf in Rf . def(C),if H(o)(f) is de.ned, then \nH.H(o)(f): dT and H dT and H dT = T [o . this][dR . param(C)];or 2. (a) H.o : dT []<dR>#i and (b) H \ndT []<dR>#i and  ' (c) if H(o)(n) is de.ned, then H.H(o)(n): dT and ' H dT and H dT = dT [i . n]. De.nition \n3 (Instantiation of static environments). Ady\u00adnamic environment dG instantiates a static environment \nG (H dG = G)if G, .H, and H dG; the same variables appear in Dom(G) as in Dom(dG); and for each pair \nz . T . G and z . o . dG, H.v : dT and H dT = dG(T ). Theorem 1 (Preservation). For a well-typed program, \nif ' G e : T,E and H dG = G and (e, dG,H) . (o,H , dE), ' ' then (a) H '; and (b) H dT = dG(T ),where \nHo : dT ; '' and (c) H dE; and (d) H dE . dG(E). * (PROGRAM) tclass* \u00d8 te : T,E (CLASS) {this . C<P \n>} t .eld* method* comm (ENV) .z . T . G.G tT .P . R . G.G tR tclass* etclass C<P > { .eld* method* comm \n* } tG '' G ' G ' G ' G ' G tT f in R G tTr m(Tx x) E { e } (FIELD) G tT G tR (METHOD) G tTr,Tx,E =G \n.{x . Tx} te : T ,E ' tT = Tr tE ' . E (COMM) this . C<P > . G .def(C.m), def(C.m ' ) ' G tm commuteswith \nm Figure 16. Rules for valid program elements. def(C.m) means the de.nition of method m in class C. (RPL-ROOT) \n(RPL-VAR) z . C<R> . G (RPL-PARAM) this . C<P > . G . P . R . G (RPL-NAME) G tR region r . program G \nt Root G tz G tP G tR : r (RPL-INDEX) G tR i . G (RPL-STAR) G tR (UNDER-ROOT) (UNDER-VAR) z . C<R> . \nG G tR :[i]G tR : * G tR = Root G tz = R (UNDER-NAME) G tR = R ' (UNDER-INDEX) G tR = R ' (UNDER-STAR) \nG tR = R ' (UNDER-INCLUDE) G tR . R ' G tR : r = R ' G tR :[i] = R ' G tR : *= R ' G tR = R ' (INCLUDE-NAME) \nG tR . R ' (INCLUDE-INDEX) G tR . R ' (INCLUDE-STAR) G tR = R ' G tR : r . R ' : r G tR :[i] . R ' :[i]G \ntR . R ' : * (INCLUDE-PARAM) P . R . G (INCLUDE-FULL) G tR . Rf G tP . R G tRf . R Figure 17. Rules for \nvalid RPLs, nesting of RPLs, and inclusion of RPLs. The nesting and inclusion relations are re.exive \nand transitive (obvious rules omitted). (TYPE-CLASS) .def(C)G tR (TYPE-ARRAY) G .{i} tT,R G tC<R> G tT \n[]<R>#i ' (SUBTYPE-CLASS) G tR . R ' (SUBTYPE-ARRAY) G .{i} tR . R ' [i ' . i] T = T ' G tC<R> = C<R \n' > G tT []<R>#i = T []<R ' >#i ' ' ' Figure 18. Rules for valid types and subtypes. def(C) means the \nde.nition of class C. T = T means that T and T are identical up to the names of variables i. (EFFECT-EMPTY) \n(EFFECT-READS) G tR (EFFECT-WRITES) G tR (EFFECT-INVOKES) .def(C.m)G tE G t \u00d8 G t reads R G t writes \nR G t invokes C.m with E (EFFECT-UNION) G tE G tE ' (SE-EMPTY) (SE-READS) G tR . R ' (SE-WRITES) G tR \n. R ' G tE . E ' G t \u00d8. E G t reads R . reads R ' G t writes R . writes R ' (SE-READS-WRITES) G tR . \nR ' (SE-INVOKES-1) G tE . E ' G t reads R . writes R ' G t invokes C.m with E . invokes C.m with E ' \n(SE-INVOKES-2) (SE-UNION-1) G tE . E ' . G tE . E '' (SE-UNION-2) G tE ' . E G tE '' . E . E '' . E '' \n G t invokes C.m with E . E G tE . E ' G tE ' . E Figure 19. Rules for valid effects and subeffects. \n'' (LET) G te : C<R>,E G .{x . C<R>} te : T ,E ' (FIELD-ACCESS) Tf in Rf . def(C) this . C<param(C)> \n. G '' G t let x = e in e : T [x . R : *],E . E ' [x . R : *] G t this.f : T, reads Rf ' ' (FIELD-ASSIGN) \nthis . C<param(C)> . G z . T . G Tf in Rf . def(C)G tT = T G t this.f = z : T, writes Rf ' (ARRAY-ACCESS) \nz . T []<R>#i . G (ARRAY-ASSIGN) {z . T []<R>#i, z ' . T ' }. GG tT = T [i . n] '' G tz[n]: T [i . n], \nreads R[i . n] G tz[n]= z : T, writes R[i . n] ' (INVOKE) {z . C<R>,z . T }. G Tr m(Tx x) E { e }. def(C)G \n.{P . R} tT = Tx[this . z][param(C) . P ] ' G tz.m(z ): Tr[this . z][param(C) . R], invokes C.m with \nE[this . z][param(C) . R] (VAR) z . T . G (NEW-CLASS) G tC<R> (NEW-ARRAY) G tT []<R>#i G tz : T, \u00d8 G \nt new C<R> : C<R>, \u00d8 G t new T [n]<R>#i : T []<R>#i, \u00d8 Figure 20. Rules for typing expressions. param(C) \nmeans the parameter of class C. '' (DYN-LET) (e, dG,H) . (o, H ' , dE)(e, dG .{x . o},H ' ) . (o ,H \n'' , dE ' ) (DYN-VAR) z . o . dG '' (let x = e in e, dG,H) . (o ,H '' ,dE . dE ' )(z, dG,H) . (o, H, \n\u00d8) (DYN-FIELD-ACCESS) this . o . dG Hto : C<dR> Tf in Rf . def(C) (this.f, dG,H) . (H(o)(f),H, reads \ndG(Rf )) ' (DYN-FIELD-ASSIGN) {this . o, z . o }. dG Hto : C<dR> Tf in Rf . def(C) ' (this.f = z, dG,H) \n. (o ,H .{o . (H(o) .{f . o ' })}, writes dG(Rf )) (DYN-ARRAY-ACCESS) z . o . dG Hto : dT []<dR>#i (z[n], \ndG,H) . (H(o)(n),H, reads dR[i . n]) ' (DYN-ARRAY-ASSIGN) {z . o, z . o ' }. dG Hto : dT []<dR>#i '' \n' (z[n]= z, dG,H) . (o ,H .{o . (H(o) .{n . o })}, writes dR[i . n]) ' '' (DYN-INVOKE) H f o : C<dR> \nTr m(Tx x) E { e }. def(C)(e, {this . o, param(C) . dR, x . o },H) . (o ,H ' , dE) '''' (z.m(z ), {z \n. o, z . o ' }. dG,H) . (o ,H ' , invokes C.m with dE) (DYN-NEW-CLASS) o . Dom(H) H ' = H .{o . new(C)} \nH ' to : C<dG(R[: *. E])> (new C<R>, dG,H) . (o, H ' , \u00d8) (DYN-NEW-ARRAY) o . Dom(H) H ' = H .{o . new(T \n[n])} H ' to : dG(T )[]<dG(R[: *. E])> (new T [n]<R>#i, dG,H) . (o, H ' , \u00d8) Figure 21. Rules for program \nevaluation. '' (DISJOINT-LEFT-NAME) r = r G tR = Rf : r G tR ' = Rf : r G tR # R ' (DISJOINT-LEFT-INDEX) \ni G tR = Rf :[i]G tR ' = Rf :[i ' ] = i ' G tR # R ' (DISJOINT-LEFT-NAME-INDEX) G tR = Rf : r G tR ' \n= Rf :[i] (DISJOINT-RIGHT-NAME) (DISJOINT-RIGHT-INDEX) G tR # R ' ' r = r ' G tR : r # R ' : r i = i \n' G tR :[i]# R ' :[i ' ] (DISJOINT-RIGHT-NAME-INDEX) G tR : r # R ' :[i] (DISJOINT-NAME) G tR # R ' G \ntR : r # R ' : r (DISJOINT-INDEX) G tR # R ' G tR :[i]# R ' :[i] Figure 22. Rules for disjointness of \nRPLs. The disjointness relation is symmetric (obvious rule omitted). C.2 Disjointness Figure 22 gives \nthe rules for concluding that two static RPLs are disjoint; we extend them to dynamic RPLs as in Sec\u00adtion \nB. De.nition 4 (Set interpretation of dynamic RPLs). Let H and H dR.Then S(dR, H) is de.ned as follows: \n(1) S(dRf ,H)= {dRf };(2) S(dR : r, H)= {dRf : r|dRf . S(dR, H)};(3) S(dR :[n],H)= {dRf :[n]|dRf . S(dR, \nH)}; and (4) S(dR : *,H)= {dRf |H dRf = dR}. De.nition 5 (Region of a .eld or array cell). If H.o : C<dR> \nand Tf in Rf . def(C),then region(o, f, H)= Rf [this . o][param(C) . dR].If H.o : dT []<dR>#i, then region(o, \nn, H)= dR[i . n]. (NI-READ) G t reads R # reads R ' (NI-READ-WRITE) G tR # R ' G t reads R # writes R \n' (NI-WRITE) G tR # R ' G t writes R # writes R ' (NI-INVOKES-1) G tE # E ' G t invokes C.m with E # \nE ' ' (NI-INVOKES-2) m commuteswith m . def(C) G t invokes C.m with E # invokes C.m ' with E ' (NI-EMPTY) \nG t \u00d8 # E # E '' (NI-UNION) G tE # E '' G tE ' # E '' G tE . E ' Figure 23. The noninterference relation \non effects. Nonin\u00adterference is symmetric (obvious rule omitted). Proposition 1 (Disjointness of region \nsets). If H dR # dR ' , then S(dR, H) n S(dR ' ,H)= \u00d8. Proposition 2 (Distinctness of disjoint regions). \nIf H ' region(o, f, H)#region(o ' ,f ' ,H), then either o = o or ' f = f '; and if H region(o, n, H)#region(o \n,n ' ,H),then '' either o = o or n = n . C.3 Noninterference of Effect Figure 23 gives the noninterference \nrelation on static effects. We extend this relation to dynamic effects as in Section B. Theorem 2 (Soundness \nof noninterference). If G e : T,E ' and G e : T ' ,E ' and G E # E ' and H dG = '' G and (e, dG,H) \n. (o,H , dE) and (e, dG,H ') . '' ' (o ' ,H , dE '), then there exists H ''' such that (e, dG,H) . ''' \n'' (o ' ,H , dE ') and (e, dG,H ''') . (o,H , dE).   \n\t\t\t", "proc_id": "1640089", "abstract": "<p>Today's shared-memory parallel programming models are complex and error-prone.While many parallel programs are intended to be deterministic, unanticipated thread interleavings can lead to subtle bugs and nondeterministic semantics. In this paper, we demonstrate that a practical <i>type and effect system</i> can simplify parallel programming by <i>guaranteeing deterministic semantics</i> with modular, compile-time type checking even in a rich, concurrent object-oriented language such as Java. We describe an object-oriented type and effect system that provides several new capabilities over previous systems for expressing deterministic parallel algorithms.We also describe a language called Deterministic Parallel Java (DPJ) that incorporates the new type system features, and we show that a core subset of DPJ is sound. We describe an experimental validation showing thatDPJ can express a wide range of realistic parallel programs; that the new type system features are useful for such programs; and that the parallel programs exhibit good performance gains (coming close to or beating equivalent, nondeterministic multithreaded programs where those are available).</p>", "authors": [{"name": "Robert L. Bocchino", "author_profile_id": "81323487933", "affiliation": "University of Illinois at Urbana-Champaign, Urbana, IL, USA", "person_id": "P1728727", "email_address": "", "orcid_id": ""}, {"name": "Vikram S. Adve", "author_profile_id": "81100524180", "affiliation": "University of Illinois at Urbana-Champaign, Urbana, IL, USA", "person_id": "P1728729", "email_address": "", "orcid_id": ""}, {"name": "Danny Dig", "author_profile_id": "81100198424", "affiliation": "University of Illinois at Urbana-Champaign, Urbana, IL, USA", "person_id": "P1728730", "email_address": "", "orcid_id": ""}, {"name": "Sarita V. Adve", "author_profile_id": "81100524186", "affiliation": "University of Illinois at Urbana-Champaign, Urbana, IL, USA", "person_id": "P1728731", "email_address": "", "orcid_id": ""}, {"name": "Stephen Heumann", "author_profile_id": "81444608840", "affiliation": "University of Illinois at Urbana-Champaign, Urbana, IL, USA", "person_id": "P1728732", "email_address": "", "orcid_id": ""}, {"name": "Rakesh Komuravelli", "author_profile_id": "81361590765", "affiliation": "University of Illinois at Urbana-Champaign, Urbana, IL, USA", "person_id": "P1728733", "email_address": "", "orcid_id": ""}, {"name": "Jeffrey Overbey", "author_profile_id": "81315491236", "affiliation": "University of Illinois at Urbana-Champaign, Urbana, IL, USA", "person_id": "P1728734", "email_address": "", "orcid_id": ""}, {"name": "Patrick Simmons", "author_profile_id": "81444596796", "affiliation": "University of Illinois at Urbana-Champaign, Urbana, IL, USA", "person_id": "P1728735", "email_address": "", "orcid_id": ""}, {"name": "Hyojin Sung", "author_profile_id": "81444596778", "affiliation": "University of Illinois at Urbana-Champaign, Urbana, IL, USA", "person_id": "P1728736", "email_address": "", "orcid_id": ""}, {"name": "Mohsen Vakilian", "author_profile_id": "81444608515", "affiliation": "University of Illinois at Urbana-Champaign, Urbana, IL, USA", "person_id": "P1728728", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1640089.1640097", "year": "2009", "article_id": "1640097", "conference": "OOPSLA", "title": "A type and effect system for deterministic parallel Java", "url": "http://dl.acm.org/citation.cfm?id=1640097"}