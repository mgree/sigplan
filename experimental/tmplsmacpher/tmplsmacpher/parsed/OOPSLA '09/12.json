{"article_publication_date": "10-25-2009", "fulltext": "\n The DesignofaTaskParallel Library Daan Leijen,Wolfram Schulte, and Sebastian Burckhardt Microsoft Research \n {daan,schulte,sburckha}@microsoft.com Abstract void MatrixMult(int size, double[,] m1 TheTaskParallel \nLibrary (TPL)isa library for .NET that makes it easy to take advantage of potential parallelism in a \nprogram. The library relies heavily on generics and del\u00adegate expressions to provide custom control structures \nex\u00adpressing structured parallelism such as map-reduce in user programs.The libraryimplementationisbuilt \naroundthe no\u00adtion ofa task asa .nite CPU-bound computation.To capture the ubiquitous apply-to-all pattern \nthe library also introduces the novel concept of a replicable task. Tasks and replica\u00adble tasks are assigned \nto threads using work stealing tech\u00adniques,but unlike traditional implementations based on the THE protocol, \nthe library uses a novel data structure called a duplicating queue . A surprising feature of duplicating \nqueues is that they have sequentially inconsistent behavior on architectures with weak memory models,but \ncapture this non-determinism in a benign way by sometimes duplicating elements. TPL ships as partof the \nMicrosoftParallel Exten\u00adsions for the .NET framework 4.0, and forms the foundation ofParallelLINQqueries(however,notethatthe \nproductized TPL library may differ in signi.cant ways from the basic design described in this article). \nCategories and Subject Descriptors D.3.3[Programming Languages]: Concurrent ProgrammingStructures General \nTerms Languages, Design Keywords Domain speci.c languages, parallelism, work stealing 1. Introduction \nMany existing applications can be naturally decomposed so that they canbeexecutedin parallel.Take forexample \nthe following (na\u00a8ive) method for multiplyingtwo matrices: Permission to make digital or hard copies \nof all or part of this work for personal or classroom use is granted without fee provided that copies \nare not made or distributed for pro.t or commercial advantage and that copies bear this notice and the \nfull citation on the .rst page.To copyotherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. OOPSLA 2009, October 25 29, 2009, Orlando, \nFlorida, USA. Copyright c &#38;#169; 2009ACM 978-1-60558-734-9/09/10... $10.00 ,double[,] m2, double[,] \nresult){ for(int i = 0; i < size; i++){ for(int j = 0; j < size; j++){ result[i, j] = 0; for(int k = \n0; k < size; k++){ result[i, j] += m1[i, k] * m2[k, j]; } } } } In this example, the outer iterations \nare independent of each other and can potentially be done in parallel. A straight\u00adforward way to parallelize \nthis algorithm would be to use size threads, whereeach threadwouldexecutethetwo inner loops with its \ncorresponding iteration index butwait: that would be prohibitively expensive, since each thread needs \na stackandotherbookkeeping information.Itis bettertouse a small pool of threads, and assign to them a \nset of tasks to execute, where a task is a .nite CPU bound computation, like the body of a for loop. \nSo instead of size threads we should create size tasks, each of them executing the two in\u00adnerloopsforitsiterationindex.The \ntaskswouldbeexecuted by n threads, where n is typically the number of processors. Using tasks instead \nof threads has manybene.ts not only are they more ef.cient, theyalso abstract away from the the underlying \nhardware and the OS speci.c thread scheduler. TheTaskParallel Library (TPL) enables programmers to easily \nintroduce this potential taskparallelism in a program. UsingTPL we can replace the outer for loop of \nthe matrix multiplicationwithacalltothe static Parallel.For method: void ParMatrixMult(int size, double[,] \nm1 ,double[,] m2, double[,] result){ Parallel.For(0, size, delegate(int i){ for(int j = 0; j < size; \nj++){ result[i, j] = 0; for(int k = 0; k < size; k++){ result[i, j] += m1[i, k] * m2[k, j]; } }); }); \n}  The Parallel.For constructisastatic methodofthe Parallel class that takes three arguments: the .rst \ntwo arguments specify the iteration domain (between 0 and size), and the last argument is a delegate \nexpression that is calledfor each index in the domain. The delegate represents the task to execute. A \ndelegate expression is the C# equivalent of an anonymous function de.nition or lambda expression. In \nthis case, the delegate takes the iteration index as its .rst argu\u00adment and executes the unchanged loop \nbody of the origi\u00adnal algorithm. Note that in C# the delegate passed to the Parallel.For loop automatically \ncaptures m1, m2, result, and size. This automatic capture of free variables makes it particularly easy \nto experiment with introducing concur\u00adrencyinexisting programs.In Section6.1weshowto writea parallel \nmatrix computation by hand without using the TPL and compare its performance. TPL can be viewed as a \nsmall embedded domain speci.c language and its methods behave like custom control struc\u00adturesinthe language(Hudak1996).Wearguethatthe \nneces\u00adsary ingredients for such library approach in strongly typed languages are parametric polymorphism \n(generics) and .rst\u00adclass anonymous functions(delegates). Moreover, through the abstraction provided \nby these two ingredients, we can implement all the high level control structures in terms of just two \nprimitive concepts tasks and replicable tasks. This guarantees that the library abstractions have consistent \nsemantics and behave regularly. The parallel abstractions offeredby the library only ex\u00adpress potential \nparallelism, but they do not guarantee it. For example, on a single-processor machine,Parallel.For loopsareexecutedsequentially,closely \nmatching the perfor\u00admance of strictly sequential code. On a dual-core machine, however,thelibrary might \nusetwoworker threadstoexe\u00adcute the loop in parallel, depending on the workload and con.guration. Since \nno concurrency is guaranteed, the li\u00adbrary is speci.cally intended for speeding up CPU bound computations, \nand not for asynchronous programming. TPL also does not help to correctly synchronize parallel code that \nuses shared memory. Other mechanisms, such as locks, are needed to protect concurrent modi.cations to \nshared mem\u00adory, and it is the programmer s responsibility to ensure that the code can be safely executedin \nparallel. Under the hood, tasks and replicable tasks are assigned by the runtime to special worker threads. \nTPL uses standard work-stealing for this work distribution (Frigo et al. 1998) where tasks areheldina \nthread local task queue. Whenthe task queue of a thread becomes empty, the thread will try to steal tasks \nfrom the task queue of another thread. The per\u00adformanceofwork stealing algorithmsisinalargepart deter\u00adminedbytheef.ciencyof \ntheir task queueimplementations. We providea novel implementationofthese task queues, called duplicating \nqueues. A surprising feature of the du\u00adplicating queue is that it behaves in a sequentially incon\u00adsistent \nway on weak memory models. However, the non\u00addeterminism of the queue is carefully captured in a benign \nway by sometimes duplicating elements in the queue (but never losing or inventing elements, and still \nensuring that tasks are only executed once). Of course, exploiting weak memory models is playing with \n.re. That s why we veri.ed the correctness of the duplicatingqueue formally using the Checkfence tool \n(Burckhardtet al. 2007). Initially developed at Microsoft Research, TPL is ship\u00adpingasamajorpartofthe \nMicrosoftParallel Extensionsto the.NET frameworkand currentlyavailablefordownloadas a Community Technology \nPreview (Microsoft 2008). TPL forms the foundationofParallel LINQ queriesand has al\u00adready been used for \nparallelizing various applications (we will report on some of them). Note though that the produc\u00adtized \nlibrary has changed in many ways including the inte\u00adgration with the new thread pool implementation of \n.NET 4.0. Consequently the design of TPL has evolved, and the productized library may differ in signi.cant \nways from the libraryas describedinthis paper(asanexample,the produc\u00adtized TPL also contains support \nfor asynchronous program\u00adming and I/O bound computations). To summarize, we make the following main contribu\u00adtions: \n We show how we can embed a realistic parallel frame\u00adwork as a library in a strongly typed language, \nwhere the combination of generics and delegates proved vital to de\u00ad.ne custom control structures.  We \nshow that the library needs just two primitives: tasks and replicable tasks, where the latter forms a \nconvenient abstraction to capture parallel iteration and aggregation.  A duplicating queue is a novel \ndata structure for im\u00adplementing non-blocking task queues that can work ef\u00ad.ciently on architectures \nwith weak memory models. These queues are sequentially inconsistent but capture the resulting non-determinism \nin a benign way by some\u00adtimes duplicating elements.  We veri.ed the correctness of the duplicating queue \nfor\u00admally using Checkfence.  We provide a detailed assessment of TPLs performance and discuss its use \nin a Microsoft product.  The rest of the paper is structured as follows: \u00a7 2 intro\u00adduces example abstractions \nfor parallel programming pro\u00advided by the TPL; \u00a7 3de.nes these abstraction in terms of the underlying \ntask primitives; \u00a7 4describes TPL s strategy for workdistribution; \u00a7 5 introduces the duplicating queue \nand provides correctness arguments; \u00a7 6 reports on experi\u00adence by early adopters and provides performance \nnumbers; \u00a7 7discusses related work;\u00a7 8concludes. 2. Using the TPL This section introduces some basic \nabstractions for paral\u00adlelism in the TPL. One of the most basic patterns is standard fork-join parallelism.As \nanexample, consider the following sequential quicksort implementation:  static void SeqQuickSort<T>(T[] \ndom, int lo, int hi) where T : IComparable<T> { if(hi -lo <= Threshold) InsertionSort(dom, lo, hi); int \npivot = Partition(dom, lo, hi); SeqQuickSort(dom, lo, pivot -1); SeqQuickSort(dom, pivot + 1, hi); } \nThe algorithm is generic in the element type T and only re\u00adquires that theycan be compared. Under a certain \nthreshold, the algorithm falls back on insertion sort which performs better for a small number of elements. \nOtherwise, we parti\u00adtion the input array in twoparts and quick sort both parts sep\u00adarately. These two \nsorts can be performed in parallel since each sortworks ona distinct partof the array.We canex\u00adpress \nthis conveniently using the Parallel.Do method: static void ParQuickSort<T>(T[] dom, int lo, int hi) \nwhere T : IComparable<T> { if(hi -lo <= Threshold) InsertionSort(dom, lo, hi); int pivot = Partition(dom, \nlo, hi); Parallel.Do( delegate{ ParQuickSort(dom, lo, pivot -1); }, delegate{ ParQuickSort(dom, pivot \n+ 1, hi); } ); } The Parallel.Do methodisastatic methodthattakestwoor more delegates as arguments and \npotentially executes them in parallel. Since quick sort is recursive, a lot of parallelism is exposed \nsince every invocation introduces more parallel tasks. Depending on the available number of threads how\u00adever, \nthe library might actually execute only some of the tasksin parallelandthe remaining ones sequentially.Wewill \nlater investigate whythat is a good strategy. The abovecode also shows howTPL acts as an embedded domain \nspeci.c language: the Parallel.Do method is very close to extending the language with a parallel do statement. \nHaving .rst-class anonymous functionsis essentialto writea paralleldo conveniently.C# automatically captures \nthe free variables dom, lo, pivot, and hi. In Java programers need to simulate anonymous functions using \ninner classes which is less convenient and only variables marked as .nal can be captured. In C++, the \nsituation is very cumbersome as we need to introduce a new class with the free variables as its .elds. \n2.1 Map-reduce parallelism Often, a for loop is used to iterate over a domain and ag\u00adgregate thevalues \nintoa single result.Take forexample the following iteration that sums the prime numbers less than 10000: \nint sum= 0; for(int i = 0; i < 10000; i++){ if(isPrime(i)) sum += i; } Unfortunately we cannot parallelize \nthis loop as it stands since theiterations are not independent due to the modi.\u00adcation of the shared \nsum variable.Acorrect parallelversion would protect the addition with a lock, as in: int sum= 0; Parallel.For(0, \n10000, delegate(int i){ if(isPrime(i)){ lock (this){ sum += i; } } }); This program nowsuffers fromaperformance \nproblem since all parallel iterations contend for both the samelock and the same memory location. We \ncan avoid this if each worker thread maintains a thread local sum and only adds to the global sum at \nthe end of the loop. In TPL this particular pattern is captured by the Parallel.Aggregate operation, \nand we can rewrite the example as: int sum = Parallel.Aggregate( 0, 10000, // domain 0, // initial value \ndelegate(int i){ return (isPrime(i) ? i : 0) }, delegate(int x, int y){ return x+y; } ); The aggregate \noperation takes .ve arguments. The .rst two specify the input domain, which can also be an enumerator. \nThe next argument is the initial value for the result. The next two arguments are two delegate functions. \nThe .rst function is applied to each element, and the other is used to combine the element results. The \nlibrary will automatically useathread localvariableto computethe thread local results without anylocking, \nonly using a lock to combine the .nal thread local results. If the aggregation is done in parallel, it \nis possiblethat elementsare combinedinadifferentorderthan a sequential aggregation. Therefore, it is \nrequired that the combining delegate function is associativeand commutative, and where the initial value \nis its unit element. 2.2 Non-structured parallelism For cases where there is no standard control abstraction, \nTPL also allowsthe immediate useof tasksor futures.Afutureis a task wherethe associated action returnsa \nresult.We show the useof futures withanai\u00a8 ve Fibonacci example. Assume wehaveasequential Fib function. \nIts parallelversion, called ParFib, looks as follows:  static int ParFib(int n){ if(n<= 8) return Fib(n); \nvar f2 = new Future<int>(() => ParFib(n-2)); int f1 = ParFib(n-1); return (f1 + f2.Value); } The Future<int> \ncall takes the int returning delegate () => ParFib(n-2) as a parameter. The result of the future is re\u00adtrieved \nthrough Value. Since ParFib is recursive, an expo\u00adnential amountof parallelismis introduced:every ParFib(n-2) \nbranch of the call tree creates a subtask which can be done in parallel. The call to f2.Value waits until \nthe result is available. Futures are a nice example why embedded do\u00admain speci.c languages need both \n.rst-class functionsbut also parametric polymorphism (generics) in order to achieve enough abstraction \nto de.ne custom control structures. The future abstraction works well with symbolic code that is less \nstructured than loops. Since futures are true .rst\u00adclass values, one can use futures to introduce parallelism \nbetween logically distinct parts of a program. For exam\u00adple, one can store future values in data structures \nwhere an\u00adother distinct phase will actually request the values of these futures. An interesting application \ndomain is game devel\u00adopment. One phase calculates the new health of all actors as a future, while the \nfollowing phases use the values of those health futures. Another example might be a just-in\u00adtime compiler \nthat startsoffby generating code forthe main procedure, and storing the code for allother procedures \nin a future value. When calling other procedures, the code may now have been generated alreadyby a parallel \ntask. 3. Task primitives When building a parallel library, it is important to have just few primitive \nconcepts in order to ensure that opera\u00adtions haveregular and consistent semantics. Surprisingly,we canbuild \nthe entire library on just two primitive concepts, namely tasks and replicable tasks. All other abstractions, \nlike futures and parallel forloops are expressed in terms of these two primitives. 3.1 Tasks Atask represents \na lightweight .nite CPUbound computa\u00adtion. It is de.ned as: delegate void Action(); class Task{ Task(Action \naction); void Wait(); bool IsCompleted{ get;} ... } A task is created by supplying an associated action \nthat canpotentiallybeexecutedin parallel, somewhere between the creation time of the task, and the .rst \ncall to the Wait method. The associated action couldpotentially be executed on another thread that created \nthe task,but it is guaranteed that actions do not migrate among threads. This is an impor\u00adtant guarantee \nsince it enables us to safely use thread af.ne operations. The Wait method returns whenever the associated \naction has .nished. Any exception that is raised in the action is stored, and re-raised whenever Wait \nis called which ensures that exceptions are never lost and properly propagated to dependent tasks. Taking \nit all together, we can see tasks as an improved thread pool where work items return a handle that can \nbe waited upon, and where exceptions are propagated. On top of these basic tasks we can easily add more \nad\u00advanced abstractions. The parallel Do method, for example, can be implemented as follows: static void \nDo(Action action1, Action action2){ Task t = new Task(action1); // do potentially in parallel action2(); \n// call action2 directly t.Wait(); // wait for action1 } Futures are a variation of tasks, where the \nassociated action computes a result: delegate T Func<T>(); class Future<T> : Task{ Future (Func<T> function); \nT Value{ get;} // does an implicit wait } Afuture is constructed with a delegate having the Func<T> type \nwhere T is the return type of the delegate. The result of the future is retrieved through the Value property, \nwhich calls Wait internally to ensure that the task has completed and the resultvalue has been computed. \nSince Wait is called, calling Value will throw any exception that was raiseddur\u00adingthe computationofthe \nfuturevalue.Onecanviewfutures as returning either a result value or an exceptional value. Futures are \nan old concept already implemented in multi\u00adlisp (Halstead jr. 1985). Our notion of a future is not safe \n, in the sense that that the programmer is responsible for properly accessing shared memory. This is \nin contrast with some earlier approaches like Moreau (Moreau 1996) where the action of a future is wrapped \nautomatically in a memory transaction.  3.2 Replicable tasks The second primitive concept of our library \nare replicable tasks which form the basic abstractionto implement parallel distribution. A replicable \ntask can potentially execute its associated action itselfin parallel.Itis de.ned as:  class ReplicableTask \n: Task{ ReplicableTask(Action action); } The constructor takes an action that can potentially be exe\u00adcuted \nin parallel. If an exception is raised in anyof those ex\u00adecutions, only oneof themis stored and re-thrownby \nWait. Replicable tasks are a new concept and have proved to be invaluable for the implementation of the \nmany parallel iteration abstractions that the library implements. In particu\u00adlar, all the Parallel.For \nvariations of the library are imple\u00admentedusingreplicable tasks.Hereisforexampleastraight\u00adforward implementation \nof the basic Parallel.For abstrac\u00adtion1: static void For(int from, int to, Action<int> body){ int index \n= from; var rtask = new ReplicableTask(delegate{ int i; while ((i = InterLocked.Increment(ref index)) \n<= to){ body(i-1); } }); rtask.Wait(); } In thisexample, the action delegate associated with the repli\u00adcable \ntask captures a shared index variable index. This ac\u00adtion can potentially execute in parallel by multiple \nthreads and access to this shared index is coordinated through in\u00adterlocked operations, where each execution \nclaims one in\u00addexata time.Notehowwe completelyabstractedawayover work distribution and only express the \nessential details of the algorithm. As we will see in Section 4.2, the schedul\u00ading algorithm in the library \nautomatically ensures that idling processors will participate in doing the work for a replicable task, \nand that nested parallel for loops are scheduled ef.\u00adciently. Replicable futures are replicable tasks \nthat returnavalue. They are de.ned as: class ReplicableFuture<T> : Future<T>{ ReplicableFuture<T> (Func<T> \nfunction ,Func<T,T,T> combine); } A replicable future takes as arguments a function that re\u00adturns a result, \nand a combining function that is commutative and associative. Just like replicable tasks, the function \nar\u00adgument can potentially be executed in parallel by multiple processors, and in that case the combine \nfunction is used to combine the parallel results. Just as replicable tasks are a good abstraction for \nimplementing parallel iteration, replica\u00adble futures are a good abstraction for implementing parallel \n1An Action<T> delegate is an action that returns a value of type T. aggregation. As an example, here \nis an implementation for the Parallel.Aggregate method2: static T Aggregate<T>(int from, int to, T init, \nFunc<T> body, Func<T,T,T> combine){ int index = from; var rfuture = new ReplicableFuture<T>( delegate{ \nint i; T acc = init; while ((i = Interlocked.Increment(ref index)) <= to){ acc = combine(acc, body(i-1)); \n} return acc; }, combine ); return rfuture.Value; } Replicable tasks and futures are a powerful abstraction \nto implement different parallel iteration strategies. However, theyneed to be used with care and we mostly \nsee them as a tool for writing extension libraries that capture domain spe\u00adci.c parallel patterns. Also \nnote that only tasks and replica\u00adble tasks are really primitive to the TPL, since both futures and replicable \nfutures can be implemented in terms of tasks or replicable tasks. 4. Work distribution Aswehave seenTPLisconvenienttouse,butitcanonlybe \nsuccessful if besides elegance, it is also performant. In this section we focus on important high-level \ndesign decisions and focus later on how these decisions inform the design of the work stealing implementation. \n 4.1 Designfor ef.ciency The most important contributor for ef.ciencyis the decision to give no concurrency \nguarantees. Parallel tasks are only potentially run in parallel. The library speci.es that a task is \nexecuted between its creation and the .rst call to Wait. This means that we can give a valid implementation \nthat is fully sequential.Indeed,thereisaspecialdebugmodewhere all tasks are executed sequentially when \nWait is called (and will therefore never have race conditions). Effectively, there are nofairness guarantees \nfor parallel tasks.In contrast with OS threads, parallel tasks are only good for .nite cpu-bound tasks,but \nnot for asynchronous programming. This means that an implementation has a lot of freedom in choosing \nthe most ef.cient scheduling thatprocesses all tasksin the quickestway possible.Forexample, ona sin\u00ad \n2AFunc<R,A,B> delegate is a function that takes two arguments of type A and B respectively, and returns \na value of type R.  gle core machine, a parallel for loop can just be executed sequentially. The ability \nto execute tasks sequentially also enables an ef.cient implementation of waiting. Let s look at the last \nline of of the ParFib function from \u00a7 2.2 to see why. When a thread executes f2.Value it has to distinguish \nthree cases: 1. The task f2 has already beenexecuted byanotherworker thread in parallel. This is the \nideal case and f2.Value immediately returns with the result value and we achieve a (signi.cant) speedup \nrelative to single core execution. 2. The task f2 is still being executed by another worker thread in \nparallel. This is the worst case, and there is no other choice thantowait untilthe resultisavailable.In \nthiscase, anotherworkerthreadwillbe scheduledonthat particular core in order to maintain the ideal concurrency \nlevel. 3. The task f2 has not been started yet. This is actually a very common scenario and always the \ncase on single core machines. In this case, a valid strategy is to call the asso\u00adciated action of f2 \ndirectly and compute it sequentially!3  Case3isthemajorsourceofef.ciencyforthe libraryand we put most \nof our efforts in trying to optimize this case. In particular, this lead us to abandon the usual implementation \nof work-stealing queues (Herlihyand Shavit 2008) and de\u00advelop duplicating queues instead (see \u00a7 5). Also \nnote that in case 3, we execute the associated action of the task on the calling thread. If this action \ndoes a thread af.ne operation, like changing the locale or other thread lo\u00adcalstate,thiseffectisseen \nafterwards.Wefeelthatthisisfair since we consider a call to Value as similar to a method call. Moreover, \nsince we know what code is potentially executed, we can still reason about the thread local state. Similarly, \nwhen we end up in case 2, we always com\u00adpletely block the calling thread and do not use the thread toexecute \nother pending tasks.We schedulea freshworker thread on that core instead. This is important for tworeasons. \nFirst, unknown pending tasks could execute thread af.ne operations and prevent us from reasoning about \nthe thread\u00adlocal state. Second, it is vital to prevent deadlocks: if a task needstowaiton anothertask,andwewould \ncontinueexecu\u00adtiononthe same stack,wemayendupina situation wherea task is waiting (indirectly) for a \ntask that is higher up on the stack and where no further progress is possible even if the tasks are not \nin a circular dependencythemselves.  4.2 Work stealing The TPL runtime reuses the ideas of the well \nknown work stealing techniques as implemented for example in CILK (Danaher et al. 2005; Frigo et al. \n1998) and the Java fork\u00ad 3Contrast this with waiting on operating system signals, where it is not possibleto \nmakethesignal happen andtheonlythingthatcanbedoneis blocking the calling thread. join framework (Lea \n2000).We shortly discuss its principles and focus on why our implementation differs. The runtime system \nuses one worker group per processor. Each group contains one ormore worker threads where only one of \nthem is running and where all others are blocked. Whenever this running worker thread becomes blocked \ntoo (due to the previous case 2 for example) an extra running worker thread is added to that group such \nthat the processor isalwaysbusy.A worker thread can alsobe retired andgive control to another worker \nin its group when that worker can be unblocked (because the task on which it was waiting has completed \nfor example). Eachworker groupkeeps tasksin itsown doubled-ended queue. The task queue provides the operations \nPush, Pop and Take. When a task is created, the running worker thread pushes the task onto the local \nqueue of its group. If it .n\u00adishes its currenttask, it will try to pop a task from the group queue and \ncontinue with that. This way, a task is always pushed and popped locally, which bene.ts both the data \nlo\u00adcality of the task and reduces the amount of synchronization needed. If a worker thread .nds there \nare no more tasks in its queue (and none of the other workers in its group can be unblocked), it becomes \na thief : it chooses another task queue of another worker group at random and tries to steal a task or \nreplicable task from the (non-local) end of the task queue using Take.For manyparallel conquer-and-divide \nal\u00adgorithms, this ensures that the largest tasks are stolen .rst. For loops, which are typically implemented \nvia replicable tasks, it means that tasks are only replicated on demand, i.e. when another thread starts \nidling. Unfortunately,at this point we haveto do more synchronization for our queues, since we have multiple \nparties that can try to steal at the same time from the same queue, and we can have interaction between \ntheworker thread associated with that queue and the stealing thread. The performance of work stealing \nis largely dependent on the performance of its task queue implementation. In particular, we need to ensure \nthat each pushed task is only popped or taken once such that each task is only executed once.To achieve \natomic take and popoperations, most im\u00adplementations, like (Arora et al. 1998b), rely on the THE protocol \n(Dijkstra 1965), which allow the common Push and Pop operations to be implemented without using expen\u00adsive \natomic compare-and-swap operations. Unfortunately, this only works on machines with a sequentiallyconsistent \nmemory model, and in practice there are just few architec\u00adturesthat supportthis.Onthex86forexample,oneneedsto \ninserta memorybarrierin Pop operation which is almost as expensive as taking a lock in the .rst place. \nMoreover, there is an even bigger disadvantage to queues based on the THE protocol which is more subtle. \nAs shown in Section 4.1, whenever f2.Value is called where the future f2 has not yet started, we should \nexecute it directly on the calling thread. But if we use the THE protocol, there is no mechanism to get \nf2 exclusively: we can only Pop it from our queue if it happens to be the last task that was pushed, \notherwise one cannot determine whether f2 is on the local queue, or resides in a queue of another worker \ngroup.Withoutgainingexclusive accessin some otherway, we cannot execute the task directly or otherwise \nwe may execute a task more than once since it can concurrently be taken by another worker, or popped \nif it resided on a non\u00adlocal task queue.  Since this is exactly the common case that we need to optimize, \nwe opted for another approach where each task uses internally an atomic compare-and-swap operation to \nensure that it only executes once. We assign to each task an associated state, Init, Running, and Done. \nThe internal Run method onatask performsan atomic compare-and-swap operation to try to switch from Init \nto Running. If the atomic operation succeeds,the associated actionisexecutedandthe state is set to Done \nafterwards: void Task.Run(){ if(CompareAndSwap(ref state, Init, Running)){ Execute(); // execute the \nassociated action state = Done; } } Effectively,thisensuresthateachtaskisonlyexecuted once, or stated \ndifferently: running a task is an idempotent opera\u00adtion. Unfortunately, if we would use task queues based \non the THE protocol we now use two mechanisms for mutual exclusion, and execute both a memory barrier \ninstruction on a Pop operation, and an interlocked instruction when calling Run. Since these are expensive \ninstructions that require syn\u00adchronization among the processors, we would like to avoid these. As shown \nin the previous paragraphs, the interlocked instruction in the Run method is essential and since we now \nensure exclusivity on the task level, it is possible to use a weaker data structure for the task queues. \nIn particular, this leads to the development of the duplicating queue. Note that onlytasks need this \nadditional check; for repli\u00adcating tasks thereisnoneedtodothe atomic compare-and\u00adswap operation since \ntheyalready implement their own mu\u00adtual exclusion. 5. Duplicating queues Aduplicating queueisa double-ended \nqueue that potentially returns a pushed element more than once. In particular, the Push and Pop operations \nbehave like normal, but the Take operation is allowed to either take an element (and remove it from the \nqueue), or to just duplicate an element in the queue. While this nondeterminism might be dangerous for \nmany clients,itis .nefor ourusageofthe duplicating queue:the Task s Run method is idempotent and ReplicableTask \ns ex\u00adpecttobeexecutedin parallel. Other propertiesofaduplicat\u00ading queues are as usual: a duplicating \nqueue never loses an element, and returns all pushed elements after a .nite num\u00adber of Pop (and Take)operations. \nByallowing duplicationwe canavoid anexpensive mem\u00adory barrier instruction in the Pop operation on the \nx86 archi\u00adtecture. More generally, our duplicating queue is designed speci.callytobe correctonarchitectures \nsatisfyingtheTo\u00adtal Store Order (TSO) memory model (Sindhu et al. 1991) or strongermodel.Toourknowledge,thisisoneofthe.rst \ndata structures that takes speci.c advantage of weaker mem\u00adory models to avoid memory barriers. An interesting \naspect is that the non-determinism thatisintroducedby the weaker memory model is captured in a benign \nway where the num\u00adber of duplicated elements is non-deterministically deter\u00admined. 5.1 Total Store Order \nBefore explaining and verifying the algorithm, we .rst give a short speci.cation of the TSO memory model \nalong the lines of (Collier 1992; Gharachorloo et al. 1992; Sindhu et al. 1991). We write LA for load \nat address A, and SA for a store at address A. The memoryorder is an order on all the memory transactions \nfrom the processors, and we write op1 <m op2 if instruction op1 precedes instruction op2 in a particular \nmemory order. Similarly, we can de.ne a program order whichisthe orderof instructionsasexecuted by the \nprocessors. The order is total for each processor, while the order on all instructions is a partial order \nde.ned by an interleaving of the total order on each processor.We write op1 <p op2 if an instruction \nop1 precedes another instruction op2 in program order. We can now de.ne the TSO memory model as a set \nof axioms on the possible memory order: 1. In TSO, the stores are in a total memory order, i.e. .SS'. \nS<m S' . S' <m S 2. If two stores occur in program order, then these stores are in memory order too: \nS<p S' . S<m S' Informally, this means thata local storebuffer ona pro\u00adcessoriskeptin FIFO order where \nmemory stores cannot be reordered. 3. If a load occurs before another operation in program order, it \nalso precedes it in memory order: L<p op . L<m op Together with the previous axiom, this effectively \nmeans that only a load that succeeds a store in program order, may be reordered to precede it in the \nglobal memory order. 4. Finally, loads will always return the last value stored where stores that precede \nit in program order are seen even if such store occurs after the load in memory order. Writing val(op) \nfor the value that is stored or loaded, we formalize this as:  val(LA)= val(maxm({SA | SA <m LA}. {SA \n| SA <p LA})) where maxm is the maximum store under the memory order relation(<m).Informally,this axiom \ndescribes that processors always see their local stores where a load snoops the storebuffer. The TSO \nmemory model is for example supported by the Sparc architecture. The widelyavailable x86 architecture \nal\u00admost implements TSO except that stores do not always form a total order (axiom 1). In particular, \nwith four processors it is possible that two stores by two processors are seen in an opposite order by \nthe other two processors, i.e. stores to memory may be seen at different times by different pro\u00adcessors \ndepending on the physical layout of the processors. However, as we will see below, we will arrange that \nthe du\u00adplicating queue is only accessed by at most two concurrent processes, and therefore the x86 behaves \njust like a TSO ar\u00adchitecture for our particular purposes.  5.2 Implementation We describe the duplicating \nqueue usingC# code.We have chosen to present not the simplest possibleimplementation, buttouseafairly \nrealistic implementationthat representsthe queue as an array and uses wrap around to prevent shifting \nthe elements. The members of the queue are de.ned as: class DupQueue{ Task[] tasks; int size; int tailMin \nvolatile int tail volatile int head = = = 8; 0; 0; // size ? 2 ... } The tasks array contains the elements \nof the queue, where size is the size of the array. The tailMin member is essential to prevent losing \nelements and is discussedlater. The head and tail are indices in the array andrepresent the head and \ntail of the queue. When head == tail the queue is empty. There are three main operations on the queue: \nthe Take operationis calledby otherworker threadsto steal elements from the head of the queue by incrementing \nthe head index. The Push and Pop operations are calledby theworker thread thatowns the queue and pushes \nand pops elements to the tail of the queue,byrespectively incrementing and decrementing the tail index.Bydeclaring \nthe head and tail as volatile we prevent the compiler from rearranging accesses to those members. The \nTake operationis calledby thiefs andis the simplest and least often performed operation. Its de.nition \nis given public Task Take(){ lock(this){ if(head < tail ){ Task task = tasks[head%size]; head = head \n+ 1; return task; } else{ return null; } } } Figure 1. The Take operation is called by a thief in Figure \n1. Since multiple thiefs may try to steal a task at the same time, the code .rst takes a lock. This allows \nus to reason about the algorithm as if there are only two concurrent threads: a thief that calls Take, \nand a worker thread that calls Push and Pop (to make tasks available for stealing). If the queue is not \nempty,the head element of the queue is returned.For ef.ciency, the elements are accessed modulo the size \nof the array. This way, we do not have to shift the elements in the array once the end of the array is \nreachedbut can simply wrap around. The thief is the only thread that modi.es the head .eld (and reads \ntasks). The worker is the only thread that modi\u00ad.es the tail .eld and the tasks array. Since these .elds \nare modi.ed without using atomic operations, this means that on a TSO architecture the thief may see \nan outdated tail .eld or element, while the worker thread may see an out\u00addated head .eld.To make this \nmoreexplicitin the code, we denote a potentially outdated tail .eld in a thief thread as tail , and a \npotentially outdated head .eld in the worker thread as head . More formally we can have the order: Lw \nLt Sw Sw Lw <m and tail <m tail tail <p tailtail where we use the superscript w for operations in the \nworker t thread, and for operations in the stealing thread. In this case, the load Lw sees the value \nstored by Sw due to tail tail the program order (and axiom 4), while the load executed in the thief Lt \nsees the old value of the tail .eldbefore tail the Sw store happened. tail Since the head .eld is only \nincremented we have the following invariants: A. head ? head Also, in the Take operation, we can assume: \nB. Any elem[i%size] where head : i < tail is valid or null. Note that tail ? tail does not hold since \nthe tail can be decremented by the Pop operation (as described below).  public Task Pop(){ tail = tail \n-1; // can we pop safely? if( head <= Math.Min(tailMin,tail)){ if(tailMin > tail) tailMin = tail; Task \ntask = tasks[tail%size]; tasks[tail%size] = null; return task; } else{ lock (this){ // adjust head and \nreset tailMin if(head > tailMin) head = tailMin; tailMin = 8; // try to pop again if(head <= tail){ Task \ntask = tasks[tail%size]; tasks[tail%size] = null; return task; } else{ tail = tail + 1; // restore tail \nwhen empty return null; } } } } Figure 2. The Pop operation is called by a worker Informally this means \nthat Take can return an arbitrary element of the queue. Also head might be overwritten, i.e. the returned \nelementis notdeleted. Figures2and3de.ne the Pop and Push operations.Both of these operations are only \ncalled from the same worker threadand thereforedonotneedtotakealock.The tailMin .eld contains the minimal \nindex at which an element was popped. The Pop operation decrements the tail .eld opportunis\u00adtically and \ntests whether the queue still contains elements (head <= tail)and if the head .eld is smaller or equal \nto the tailMin. This test is written as head <= Math.Min(tailMin, tail) to combine both tests using \na single load of the head .eld (as a subsequent load may give a different (higher) value). If the test \nsucceeds, the tailMin .eld is updated and an elementis popped.Toprevent space leaksa null value is written \nto the popped location. If the testfails, wefall back on a safe routine and take an explicit lock. In \nthis case, there are no concurrent thiefs and the head value is current due to the implicit memory barrier \nof a lock, and we can safely determine whether the queue was empty or not. The initial test head <= tail \nensures we only pop ele\u00adments that havepreviously been pushed. Theoretically,many public void Push(Task \ntask) { if(task == null) return; // queue not full and no index over.ow? if(tail < Math.Min(tailMin, \nhead )+size &#38;&#38; tail < int.MaxValue/2){ tasks[tail%size] = task; tail = tail + 1; } else{ lock(this){ \nif(head > tailMin) head = tailMin; tailMin = 8; // adjust the indices to prevent over.ow int count = \nMath.Max(0,tail -head); head = head % size; tail = head + count; } // just run this task eagerly task.Run(); \n} } Figure 3. The Push operationis calledbyaworker steals could have happened where head is much larger \nthan head . However, it is still safe to pop those elements: i.e. re\u00ad turnan elementboth froma Take and \nfrom a Pop operation even though the element has onlybeen pushed once-the Run method, which consumes \nthe returned task takes care that a duplicate task is executed only once. The second initial test head \n<= tailMin prevents losing elements. In particular, on a TSO architecture the following sequence of events \ncould occur: 1)We push an element A and tail and tail are 1. 2) There is steal of A and head =1 but head \nis still 0. 3) There is a pop of A and tail and tailMin become 0. 4) There is a push of B where tail \nbecomes 1 again. 5) By now the value of head becomes equal to head (i.e. 1). Suppose we have a Pop operation \nnow. If we would only test for head <= tail, we would assume that the queue is empty and the B element \nwould be lost! This situation is prevented by the tailMin .eld. Since the test head <= Math.Min(tailMin, \ntail) fails, the safe path is taken. After the lock is taken, the Pop operation potentially resets the \nhead .eld to the tailMin .eld, and resets the tailMin .eld to 8. In the above scenario, head becomes \n0 again and the B element is immediately popped.With the addition of the tailMin .eld, wehave the followinginvariants: \n C. Any elem[i%size] where min( head , tailMin, tail) i< tail is valid or null. D. tailMin ? head 0 \nwhere head 0 is the .rst head value loaded since initialization or taking a lock. The Push operationin \nFigure2alsoavoidsa lockin almost all cases. If a null value is pushed the method returns imme\u00addiately. \nIt then tests whether there is still room in the queue. One may think that the test tail < head + size \nwould suf\u00ad.ce but that would not be strong enough. Indeed, as cap\u00adtured in invariant C, the tailMin .eld \ncould be lower as head andwe shouldbe carefulnottooverwritethosevalues. Therefore,the push operation \ntests whether the tail .eld is smaller as min(tailMin, head) + size. Also, to prevent in\u00adtegerover.owaftermanystealsandpushes,thereisatestto \nprevent the tail value from getting too large. If there is still room in the queue, the element is pushed \nand the tail is incremented. Otherwise, a lock is taken to prevent concurrent thiefs. Just like in Pop, \nthe head value potentially is set to the tailMin .eld, and tailMin is set to 8.To prevent integer over.ow, \nthe head and tail .eld are both adjusted to their minimal values. Finally, the pushed task is just run \neagerly: indeed, pushing a task is only done to enable it to be stolen by other worker threads. If the \nwork queue is full, there is no reason to enlarge it further since thereare still enough tasksthat canbe \nstolenandwe canjust as well execute it directly.  5.3 Veri.cation As apparent from the above description, \nthe individual oper\u00adationsofthe duplicating queue are simple,butthe concurrent interaction between themisvery \nsubtle.Togain con.dence in the correctness ofthe algorithms, we applied CheckFence (Burckhardt et al. \n2007) to verify the data structure formally under the TSO memory model. CheckFence takes three inputs: \n(1) a sequential reference implementation of the duplicating queue which serves as a speci.cation, (2) \na suite of concurrent unit tests, and (3) an axiomatic speci.cation of the memory model. It then uses \na SAT solver to verify that all possible concurrent execu\u00adtions of the tests on the given memory model \nare obser\u00advationally equivalent to some interleaved, atomic execution of the reference implementation. \nOur experience con.rmed thatthismethodologyisveryeffectivefor.nding subtle con\u00adcurrencybugs.We had to \nrevise our initialimplementation several times based on the counterexample traces produced by CheckFence. \nThe speci.c test suite we used is described in Appendix A. To specify the permitted behaviors of the \nduplicating queue, we expressed the duplication by introducing non\u00addeterminism into the Take operation. \nOur speci.cation op\u00aderates like a normal double-ended queue insofar Push and Pop operations are concerned \n(they simply add/remove el\u00adements at the tail end of the queue). In contrast, the Take operation nondeterministically \nchooses between two behav\u00adiors, either (1) removing an element from the head end of the queue as usual, \nor (2) returning a nondeterministicallycho\u00adsen element of the queue without removing it. Clearly, this \nspeci.cation is strong enough to ensure correct work steal\u00ading: no elements are lost or invented, and \nrepeated Pop calls will reliably empty the queue. CheckFence indeed veri.ed that our queue algorithm \nnever produces answers outside of the speci.cation. Even though the correctness proof is limited to test \nsequences de\u00adscribed in the appendix, they are veri.ed under all possible interleavingsallowedby the \nTSO memory model.Inthis re\u00adspect our methodology goes well beyond common practice, andgiventhe complexityof \nreasoning about concurrentdata structures under weak memory models, we feel that this is a strong result. \n6. Performance Given the continuous improvements in virtual machines and compilers for managed code, \nwe feel that absolute compar\u00adisons against other languages and frameworks is of limited value. Therefore, \nwe measure only intrinsic properties of our library like scalability andrelative speedups. 6.1 Matrix \nmultiplication revisited TPL can only succeed if it is simple to use and well\u00adperforming. So let s compare \nthe performance of the in\u00adtroductory matrix-multiplication with a hand written solu\u00adtion using the standard \nthread pool to introduce parallelism. Figure 4 shows a fairly sophisticated implementation: for instance, \nthe code statically divides the loop into chunks depending on the number of processors, creating twice \nas many as necessary to adapt better to dynamic workloads; also to minimize the number ofkernel transitions \nthe code introduces a counter together with a single wait handle. Clearly, this code is harder to write, \nand more error prone thanthe Parallel.For method. Perhaps it performs better? Eventhoughitishandtunedand \nusesanearoptimaldivision of work, it performs generally worse than Parallel.For. Figure5shows the speedups \nobtained on an eight core ma\u00adchine relative to running the sequential for loop. Note how the Parallel.For \nversion is just slightly slower ( 99%) than a direct forloop on a single core machine. Note also that \nthis hand-optimizedcode is not composi\u00adtional, for instance you have to do a major rewrite if you wanted \nto parallelize both outer loops; TPL however is composable, so you can simply exchange both outer loops \nwith Parallel.Fors. Also, in contrast to TPL, the hand\u00adoptimized version does not propagate exceptions \nraised in the loop body.  void ThreadMatrixMul(int size, double[,] m1 ,double[,] m2, double[,] res) \n{ int N = size; int P = 2 * Environment.ProcessorCount; int Chunk = N / P; // size of a work chunk AutoResetEvent \nsig = new AutoResetEvent(false); int counter = P; for(int c=0;c<P; c++){ // for each chunk ThreadPool.QueueUserWorkItem( \ndelegate(Object o){ int lc= (int)o; for(int i = lc * Chunk; // for each item  i <(lc+1== P?N:(lc+1)*Chunk); \ni++){ // original inner loop body for(int j = 0; j < size; j++){ res[i, j] = 0; for(int k = 0; k < size; \nk++){ res[i, j] += m1[i, k] * m2[k, j]; } } } // use ef.cient interlocked instructions if(Interlocked.Decrement(ref \ncounter) == 0){ sig.Set(); //kernel transition only when done } }, c); } sig.WaitOne(); } Figure 4. \nParallel matrix multiplication without using the TPL leads to static work distribution and explicit synchro\u00adnization. \n 6.2 Scaling The library should also scale well with common parallel pat\u00adterns.We useda collectionof \ncommon benchmarks adapted from the CILK and Java fork/join library benchmarks: Fib: The .bonacciof44 \nusingathresholdof 18.  Tree: Summation of nodes in a balanced tree of depth 28 with a threshold of 11. \n Matrix: Na\u00a8ive matrix multiplication of a 1024x1024 ma\u00adtrix of doubles.  Quad: Divide-and-conquer \nquad matrix multiplication of a 1024x1024 matrix of doubles.  2i-1 Integrate: RecursiveGaussianquadratureof \n(2i-1)xsumming over odd values of 1 i 6 andintegrating from -11 to 12. Figure5. Relativespeedup of parallelizing \nthe outer loop of a matrix multiplication with 750x750 elements, where1 is the running time of a normal \nfor-loop. The tests were run on a 4-socket dual-core Intel Xeon machine with 3Gb memory runningWindowsVista. \n benchmarks were run on a 4-socket dual-core Intel Xeon machine with 32Gb running Windows Server 2008. \nNote that Quick Sort cannot be fully parallelized. Jacobi: Iterative mesh with 100 steps of nearest \nneighbor averaging on a 4096x4096 matrix of doubles.  Quick Sort:Traditional quick sort algorithm ona \n1.6 mil\u00adlion element array with a sequential quick sort threshold of 4000 elements. This algorithm will \nnot scale linearly when run in parallelbut the parallel speedup is bounded  P log2(N) by (Thornley1995). \n2P -2+log2(N/P ) LU: Matrix decomposition of a 4096x4096 matrix of doubles. Figure 6 shows that all these \nalgorithms have good linear speedups when run on multiple processors. The exception is the traditional \nquick sort benchmark which does not scale linearly but we come very close to the ideal speedup. An\u00adother \nimportant point is that all of the benchmarks run (al\u00admost) asfast ona single processor as their sequential \ncoun\u00adgraphsusingMSAGL.Thetest wererunona8-socketdual\u00adcore Intel Xeon machine with 4Gb memory running \nWin\u00addowsVista.  terparts which is important when runningparallelized soft\u00adware on older machines. Even \nthough the above benchmarks show great speedups, we believe that for most realistic programs the speedups \nwill be much more modest. According to Amdahl s law the se\u00adquential partofa program willeventually dominatethe \nrun\u00adningtime.Forexample,even whenjust10%of your appli\u00adcation is intrinsically sequential, we can at most \nmake this program run 10 times as fast when perfectly parallelizing the other 90%. In practice this means \nthat especially the .rst 8cores can make a substantial difference in running times, but adding more cores \nwill have little effect. Microsoft product groups have started using TPL and have observed this effect. \nFor instance, Figure 7 describes the relative speedup of the Microsoft Automatic Graph Lay\u00adout library \n(MSAGL) (Nachmanson and Powers 2008) for threeextremegraphs.MSAGLisalargeandcomplexlibrary and we just \nreplaced a single for loop with a Parallel.For to parallelize MSAGL s spline computations. Unfortunately, \nonly part of the layout algorithm can be parallelized this easily, as the other parts currently consist \nof intrinsically sequential algorithms, like a sequential simplex algorithm. When looking at the .gure, \nwe see thatthe sequential parts dominate and the speedups are modest, between 1.2 to 2.2 timesfaster \non8processors. Still, since MSAGLis used for exampleto layoutlargeXML diagramsin real timeinVisual Studio, \nthe speedup is often noticeable in practice. Of course, in many cases we can take advantage of ever more \ncores, as long as we can make the parallel part rela\u00adtively larger by just adding more data. This happens \nin par\u00adticularin domains likegames or speech analysis.  6.3 Duplicating queues Measuring the performance \nof the duplicating queue in iso\u00adlation is not very useful. The reason is that the main perfor\u00admance bene.t \ndoes not come from the duplicating queue,but from being able to guarantee mutual exclusivity in the tasks \nthemselves, such that a task can be executed directly when Wait is called and the task has not started \nyet. Since the task queues no longer need to guarantee mutual exclusivity, the duplicating queue is mostly \nan optimization to avoid using too many expensive interlocked instructions. Therefore, it is better to \nmeasure the bene.tof being able to directlyexecutea task for which Wait is called (and which hasnot startedyet).We \ncreatedtwoversionsofthe library: one based on a duplicating queue with direct execution of tasks, andatraditional \nimplementation based on the standard THE protocol. The standard .bonacci benchmark represents one of \nthe worst-case examples since most waits are for tasks that were just pushed on the stack (and reside \non the topof the stack).We ran this benchmark ona4 processor machine which used 196418 tasks( 500.000 \ntasks per second). The implementation based on the duplicating queue was 1.4 times faster when using \nall processors. Here are some statistics, where DUP refers to the duplicating queue implementation, and \nTHE to the implementation based onthe THE protocol. speedup steal switch migrate workers DUP 3.89 29 \n20 6 20 THE 2.78 37 2452 9596 53 As we can see, the performance of THE mostly suffered because there \nwere many more switches between threads, and many more thread migrations. This happened precisely when \nWait was called for a task that had not yet started. Since one cannot reliably determine whether this \ntask was on the local queue, a fresh worker thread was needed to execute the task resulting in a thread \nswitch. Of course, this automatically also lead to more migration of threads where ready worker threads \nwere stolen by another worker group. Interestingly, the number of task steals are about the same as this \nis mostly determined by the particular algorithm and sizes of the tasks. Of course, for most fork-join \nparallelism (including the .bonacci benchmark), the task to be waited upon is often right on the top \nof the local task queue. When Wait is called on a task that has not started yet, we can optimize for \nthis case in the THE implementation by simply popping the top of the stack if that happens to be our \ntask and execute it directly. When applying this simple optimization, both implementations perform very \nsimilar for this benchmark. Of course, the DUP implementation outperforms again when the parallelism \nis less structured using futures for example, and in general at anytime when Wait is called on a task \nthat is not on the top of the stack. 7. Related work There is a wealth of research into parallel scheduling \nalgo\u00adrithms, data structures, and language designs, and we neces\u00adsarily restrict this section to work \nthat is directly relevant to work stealing and embeddedlibrary designs.  The idea of duplicating queues \nhas recently been descibed byMagedMichaeletal(2009)as idempotent queues.We were not aware of this work \nat the time of writing this paper and arrived at our results independently (doing our .rst im\u00adplementation \nin January 2008). Their general motivation and the semantics of the idempotent queue seem largely identi\u00adcal,but \nthe implementation is quite different. Their elegant implementation packs .elds together in a memory \nword and relies strictly on atomic compare-and-swap and memory or\u00addering instructions. In contrast, our \nimplementation uses a simple lock on allbut the critical paths which can simplify many implementation \naspects and also removes a level of indirection on the criticalpath. Futures are a well known concept \nand were already im\u00adplemented for multi-lisp (Halstead jr. 1985). It is important to distinguish safe \nfutures from the futures as described in this article. Safe futures have no observable side effect and \ncan therefore always be safely evaluated in parallel. In con\u00adtrast, TPL futures have no such restrictions \nand program\u00admers have to be careful when accessing shared state. Safe fu\u00adturesinthe contextofJavahavebeen \nstudiedforexampleby Pratikakisetal.(2004)andWelcetal.(2005).The semantics of futures with side effects \nand exceptions has been studied by Moreau (Moreau 1996) andby Flanagan and Felleisenin the contextof \noptimization(Flanagan and Felleisen 1995). CILK (Randall 1998) championed lightweight task exe\u00adcution \nframeworks for C. The CILK runtime used the THE protocol to implement task queues (Frigo et al. 1998; \nDijk\u00adstra 1965).Tasks are created using the spawn primitive, and synchronized with the sync keyword. \nIn contrast to TPL, the liveness of CILK tasks is determined by their lexical scope which enables the \nCILK compiler to allocate tasks on the stack instead of the heap which can be more ef.cient. On the other \nhand,it makes tasks second-class citizens and re\u00adstricts how they can be used. More recent work on a \nJava based CILK implementation studies the interaction of Java exceptions with the CILK sync and spawn \nprimitives (Dana\u00adher et al. 2005, 2006). There also exist quite a few task based libraries for light\u00adweight \nparallel programming in standardCand C++ based on workstealing. Some of the more well-known include StackThreads \n(Taura et al. 1999), the Filaments library (En\u00adgler et al. 1993), and Hood (Blumofe and Papadopoulos \n1999; Blumofe et al. 1995). Blumofe and Leiserson (Blumofe and Leiserson 1999) proved that the work-stealing \nalgorithm is ef.cient with re\u00adspect to time, space, and communication for the class of fully strict multithreaded \ncomputations. Arora, Blumofe, and Plaxton (Arora et al. 1998a) extended the time bound re\u00adsult to arbitrary \nmultithreaded computations. In addition, Acar, Blelloch, and Blumofe (Acar et al. 2000) show that work-stealing \nschedulers are ef.cient with respect to cache misses for jobs with nested parallelism. Agrawal looked \ninto improving the heuristics of work stealing through adaptive scheduling (Agrawal et al.2006). There \nexist many languages with light weight task par\u00adallelism. For instance, the Fortress language (Steele \n2006; Allen et al. 2007; Aditya et al. 1995) makes parallelism the default execution mode, and internally \nuses work stealing to schedule tasks ef.ciently. Our main inspiration is the widely used Java fork-join \nframework by Doug Lea (Lea 2000, 1999) which is now part ofthe standard Java libraries. Just like CILK, \ntasks are restricted to fork-join style parallelism only and should not escape theirlexical scope. Thisisnot \nenforced though which canbe dangerous asthe library can deadlock when tasks are used outside the lexical \nscope (even if there is no circular dependencyamong tasks). 8. Conclusion We described the lessons learned \nin the design of a library for parallel programming. TPL is an example of the possi\u00adbilities of an embedded \ndomain speci.c language that relies heavily on parametric polymorphism and .rst-class anony\u00admous functions, \nand we hope to applythis to other domains as well. Tothe best of our knowledge, the duplicating queue \nis one of the .rst data structures that explicitly takes the properties of weakmemory models into account, \nand it is surprising we can capture the resulting non-determinism in a benign way without for example \nlosing or inventing elements. It would be interesting to see if we can adapt the structure such that \nit can be applied for other parallel algorithms too. Acknowledgments The authorswouldliketo thankVance \nMorrison,JoeDuffy, and StephenToub for their feedbackand help in the design and implementation of TPL. \nReferences Umut A. Acar, Guy E. Blelloch, and Robert D. Blumofe. The data locality of work stealing. \nIn SPAA 00: Proceed\u00adings of the twelfth annual ACM symposium on Parallel algorithms and architectures, \npages 1 12, 2000. Shail Aditya, Arvind, Lennart Augustsson, Jan-Willem Maessen, and Rishiyur S. Nikhil. \nSemantics of pH: A Parallel Dialect of Haskell. InPaul Hudak, editor, Proc. HaskellWorkshop, LaJolla, \nCA USA, pages 35 49, June 1995. KunalAgrawal,YuxiongHe,WenJingHsu,and CharlesE. Leiserson. Adaptive scheduling \nwith parallelism feed\u00adback. In PPoPP 06: Proceedings of the eleventh ACM SIGPLAN symposium on Principles \nand practice of par\u00adallel programming, pages 100 109, 2006.  Eric Allen, David Chase, Christine Flood, \nVictor Luchangco, Jan-Willem Maessen, Sukyoung Ryu, and Guy L. Steele Jr. Project fortress: A multicore \nlanguage for multicore processors. In Linux Magazine, September 2007. Nimar S. Arora, Robert D. Blumofe, \nand C. GregPlaxton. Thread scheduling for multiprogrammedmultiprocessors. In ACM Symposium onParallel \nAlgorithms andArchitec\u00adtures, pages 119 129, 1998a. Nimar S. Arora, Robert D. Blumofe, and C. GregPlaxton. \nThread scheduling for multiprogrammedmultiprocessors. In SPAA 98: Proceedings of the tenth annualACM \nsym\u00adposium on Parallel algorithms and architectures, pages 119 129, 1998b. Robert D. Blumofe and Charles \nE. Leiserson. Scheduling multithreaded computations by work stealing. Journal of theACM, 46(5):720 748, \nSeptember 1999. Robert D. Blumofe and DionisiosPapadopoulos. Hood:A user-level threads library for multiprogrammed \nmultipro\u00adcessors. Technical report, University of Texas, Austin, 1999. RobertD. Blumofe, ChristopherF. \nJoerg, BradleyC.Kusz\u00admaul, Charles E. Leiserson, Keith H. Randall, and Yuli Zhou. Cilk: an ef.cient multithreaded \nruntime system. SIGPLAN Not., 30(8):207 216, 1995. S. Burckhardt, R. Alur, and M. Martin. CheckFence: \nCheck\u00ading consistencyof concurrent data types on relaxed mem\u00adory models. In Programming Language Design \nand Im\u00adplementation (PLDI), pages 12 21, 2007. WilliamW. Collier.Reasoning aboutParallelArchitectures. \nPrentice Hall, 1992. John S. Danaher, I-Ting Angelina Lee, and Charles E. Leis\u00aderson. The jcilk language \nfor multithreaded comput\u00ading. In Synchronization and Concurrency in Object-Oriented Languages (SCOOL), \nSan Diego, California, October 2005. John S. Danaher, I-Ting Angelina Lee, and Charles E. Leis\u00aderson. \nProgramming with exceptions in jcilk. Science of Computer Programming (SCP), 63(2):147 171, Decem\u00adber \n2006. E. W. Dijkstra. Solution of a problem in concurrent pro\u00adgramming control. Commun.ACM, 8(9):569, \n1965. DawsonR. Engler,DawsonR. Engler,GregoryR. Andrews, Gregory R. Andrews, David K. Lowenthal, and \nDavid K. Lowenthal. Filaments: Ef.cient support for .ne-grain parallelism. Technical Report TR 93-13a, \nUniversity of Arizona, 1993. Cormac Flanagan and Matthias Felleisen. The semantics of future and its \nuse in program optimization. In Rice University, pages 209 220, 1995. Matteo Frigo, Charles E. Leiserson, \nandKeith H. Randall. The implementation of the Cilk-5 multithreaded language. In Proceedings of the ACM \nSIGPLAN 98 Conference on Programming Language Design and Implementation, pages 212 223, Montreal, Quebec, \nCanada, June 1998. Proceedings publishedACM SIGPLAN Notices,Vol. 33, No. 5, May, 1998. Kourosh Gharachorloo, \nSarita V. Adve, Anoop Gupta, John L. Hennessy, and Mark D. Hill. Programming for different memory consistencymodels. \nJournal ofParallel and Distributed Computing, 15:399 407, 1992. R.H. Halsteadjr. Multilisp:Alanguage \nfor concurrent sym\u00adbolic computation. ACMTransactions on Programming Languages and Systems, 7(4):501 \n538, October 1985. Maurice Herlihyand Nir Shavit. The Art of Multiprocessor Programming. Morgan Kaufmann, \nMarch 2008. Paul Hudak.Building domain-speci.c embedded languages. ACM Comput. Surv., page 196, 1996. \nDoug Lea. Concurrent Programming inJava. Second Edi\u00adtion: Design Principles and Patterns. Addison-Wesley \nLongman Publishing Co., Inc., Boston, MA, USA, 1999. ISBN 0201310090. Doug Lea. A java fork/join framework. \nIn Java Grande, pages 36 43, 2000. Maged M. Michael, Martin T. Vechev, and Vijay A. Saraswat. Idempotent \nwork stealing. In PPoPP 09: Pro\u00adceedingsof the 14thACM SIGPLAN symposiumon Prin\u00adciples and practice of \nparallel programming, pages 45 54, 2009. Microsoft. Parallel extensions to .NET. June 2008. URL http://msdn.microsoft.com/en-us/concurrency. \nLuc Moreau. The semantics of scheme with future. In In In ACM SIGPLAN International Conference on Functional \nProgramming (ICFP 96, pages 146 156, 1996. Lev Nachmanson and Lynn Powers. Microsoft automatic graph \nlayout library (msagl). research.microsoft. com/en-us/projects/msagl, 2008. Polyvios Pratikakis, Jaime \nSpacco, and Michael Hicks. Transparent proxies for java futures. SIGPLAN Not., 39 (10):206 223, 2004. \nISSN 0362-1340. Keith H. Randall. Cilk: Ef.cient Multithreaded Comput\u00ading. PhD thesis, Department of \nElectrical Engineering and Computer Science, Massachusetts Institute ofTech\u00adnology, May 1998. Pradeep \nS. Sindhu, Jean-Marc Frailong, and Michel Cek\u00adleov. Formal speci.cation of memory models. Techni\u00adcal \nReport CSL-91-11, XeroxPalo Alto Research Center, December 1991. Guy Steele. Parallel programming and \nparallel abstractions in fortress. In Invited talk at the Eighth International Sym\u00ad  posium on Functional \nand Logic Programming (FLOPS), April 2006. KenjiroTaura,KunioTabata, and AkinoriYonezawa. Stack\u00adthreads/mp: \nintegrating futures into calling standards. SIGPLAN Not., 34(8):60 71, 1999. John Thornley. Performance \nof a class of highly\u00adparallel divide-and-conquer algorithms. Technical Re\u00adport 1995.cs-tr-95-10, California \nInstitute ofTechnology, 1995. Adam Welc, Suresh Jagannathan, and Antony Hosking. Safe futures for java. \nIn OOPSLA 05: Proceedings of the 20th annual ACM SIGPLAN conference on Object\u00adoriented programming, systems, \nlanguages, and applica\u00adtions, pages 439 453, 2005. A. Veri.cation test sequences We modelled the following \ntest sequences in checkfence, where i2 and i4 initializea queueof size2 and4 respec\u00adtively, ps is a push \noperation, pp a pop operation, tk a take operation, and the bar(|)operator composes operation se\u00adquences \nin parallel: i2 (ps pp| tk) i4(pspspp |tktk) i4(pspspp |tk) i4(pspppp |tk|tk) i4(pspspp pp|tk) i4(psppps \npp|tk |tk tk) i4(pspspp pp|tk |tk) i4(pspspp pp|tk tk) i4(pspsps pppppp|tk) i4(psppps pspppp|tk) i4(psppps \nppppps|tk|tk) i4(pspsps pppppp|tk|tktk) i4(pspspp pspspppppp| tktk) B. Differences with the productized \nTPL At the time of writing, there are the following API differ\u00adenceswith the productizedTPL (.NET Framework \n4.0, Beta 1): A task is started as Task.Factory.StartNew(action), and a future as Task.Factory.StartNew(function). \n The value of a future is retrieved as future.Result.  Parallel.Aggregate is only available through \nPLINQ.  Replicable tasks are not exposed.     \n\t\t\t", "proc_id": "1640089", "abstract": "<p>The Task Parallel Library (TPL) is a library for .NET that makes it easy to take advantage of potential parallelism in a program. The library relies heavily on generics and delegate expressions to provide custom control structures expressing structured parallelism such as map-reduce in user programs. The library implementation is built around the notion of a task as a finite CPU-bound computation. To capture the ubiquitous apply-to-all pattern the library also introduces the novel concept of a replicable task. Tasks and replicable tasks are assigned to threads using work stealing techniques, but unlike traditional implementations based on the THE protocol, the library uses a novel data structure called a 'duplicating queue'. A surprising feature of duplicating queues is that they have sequentially inconsistent behavior on architectures with weak memory models, but capture this non-determinism in a benign way by sometimes duplicating elements. TPL ships as part of the Microsoft Parallel Extensions for the .NET framework 4.0, and forms the foundation of Parallel LINQ queries (however, note that the productized TPL library may differ in significant ways from the basic design described in this article).</p>", "authors": [{"name": "Daan Leijen", "author_profile_id": "81100572466", "affiliation": "Microsoft Research, Redmond, WA, USA", "person_id": "P1728765", "email_address": "", "orcid_id": ""}, {"name": "Wolfram Schulte", "author_profile_id": "81100279001", "affiliation": "Microsoft Research, Redmond, WA, USA", "person_id": "P1728766", "email_address": "", "orcid_id": ""}, {"name": "Sebastian Burckhardt", "author_profile_id": "81350574118", "affiliation": "Microsoft Research, Redmond, WA, USA", "person_id": "P1728767", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1640089.1640106", "year": "2009", "article_id": "1640106", "conference": "OOPSLA", "title": "The design of a task parallel library", "url": "http://dl.acm.org/citation.cfm?id=1640106"}