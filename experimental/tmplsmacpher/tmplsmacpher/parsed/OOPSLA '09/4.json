{"article_publication_date": "10-25-2009", "fulltext": "\n Grace: Safe Multithreaded Programming for C/C++ EmeryD. Berger TingYang Tongping Liu GeneNovark Dept. \nof Computer Science University of Massachusetts, Amherst Amherst, MA 01003 {emery,tingy,tonyliu,gnovark}@cs.umass.edu \nAbstract The shift from single to multiple core architectures means that programmers must write concurrent, \nmultithreaded pro\u00adgrams in order to increase application performance. Unfortu\u00adnately, multithreaded applications \nare susceptible to numer\u00adous errors, including deadlocks, race conditions, atomicity violations, and \norder violations. These errors are notoriously dif.cult for programmers to debug. This paper presents \nGrace, a software-only runtime sys\u00adtem that eliminates concurrency errors for a class of mul\u00adtithreaded \nprograms: those based on fork-join parallelism. By turning threads into processes, leveraging virtual \nmem\u00adory protection, and imposing a sequential commit proto\u00adcol, Grace provides programmers with the appearance \nof deterministic, sequential execution, while taking advantage of available processing cores to run code \nconcurrently and ef.ciently. Experimental results demonstrate Grace s ef\u00adfectiveness: with modest code \nchanges across a suite of computationally-intensive benchmarks (1 16 lines), Grace can achieve high scalability \nand performance while prevent\u00ading concurrency errors. Categories and Subject Descriptors D.1.3 [Software]: \nConcurrent Programming Parallel Programming; D.2.0 [Software Engineering]: Protection mechanisms General \nTerms Performance, Reliability Keywords Concurrency, determinism, deterministic con\u00adcurrency, fork-join, \nsequential semantics 1. Introduction While the past two decades have seen dramatic increases in processing \npower, the problems of heat dissipation and Permission to make digital or hard copies of all or part \nof this work for personal or classroom use is granted without fee provided that copies are not made or \ndistributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page.To copyotherwise, to republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. OOPSLA 2009, October 25 29, 2009, Orlando, Florida, USA. Copyright \nc &#38;#169; 2009ACM 978-1-60558-734-9/09/10... $10.00 energy consumption now limit the ability of hardware \nman\u00adufacturers to speed up chips by increasing their clock rate. This phenomenon has led to a major shift \nin computer ar\u00adchitecture, where single-core CPUs have been replaced by CPUs consisting of a number of \nprocessing cores. The implication of this switch is that the performance of sequential applications is \nno longer increasing with each new generation of processors, because the individual processing components \nare not gettingfaster.On the other hand, appli\u00adcations rewritten to use multiple threads can take advantage \nof these available computing resources to increase their per\u00adformance by executing their computations \nin parallel across multiple CPUs. Unfortunately, writing multithreaded programs is chal\u00adlenging. Concurrent \nmultithreaded applications are suscep\u00adtible to a wide range of errors that are notoriously dif.cult to \ndebug [29].Forexample, multithreaded programs thatfail to employ a canonical locking order can deadlock \n[16]. Be\u00adcause the interleavings of threads are non-deterministic, pro\u00adgrams that do not properly lock \nshared data structures can suffer from race conditions [30].Arelated problemis atom\u00adicity violations, \nwhere programs may lock and unlock indi\u00advidual objects but fail to ensure the atomicity of multiple object \nupdates [14]. Another class of concurrency errors is order violations, wherea program dependsona sequenceof \nthreads that the scheduler may not provide [26]. This paper introduces Grace, a runtime system that elim\u00adinates \nconcurrency errors for a particular class of multi\u00adthreaded programs: those that employ fully-structured, \nor fork-join based parallelism to increase performance. While fork-join parallelism does not capture \nall pos\u00adsible parallel programs, it is a popular model of paral\u00adlel program execution: systems based \nprimarily on fork\u00adjoin parallelism include Cilk, Intel s Threading Building Blocks [35], OpenMP, and \nthe fork-join framework pro\u00adposed for Java [24]. Perhaps the most prominent use of fork\u00adjoin parallelism \ntoday is in Google s Map-Reduce frame\u00adwork,alibrary thatis usedto implementanumberof Google services \n[9, 34]. However,none of these prevent concurrency errors, which are dif.cult even for expert programmers \nto avoid [13]. Grace manages the execution of multithreaded programs with fork-join parallelism so that \ntheybecome behaviorally equivalent to their sequential counterparts: every thread spawn becomes a sequential \nfunction invocation, and locks become no-ops. Thisexecution model eliminates most concurrencyerrors that \ncan arise due to multithreading (seeTable 1).By con\u00adverting lock operations to no-ops, Grace eliminates \ndead\u00adlocks. By committing state changes deterministically, Grace eliminates race conditions. By executing \nthreads in program order, Grace eliminates atomicity violations and greatly re\u00adduces the risk of order \nviolations. Finally, by enforcing se\u00adquential semantics and thus sequential consistency, Grace eliminates \nthe need for programmers to reason about com\u00adplex underlying memory models. Toexploitavailable computing \nresources (multiple CPUs or cores), Grace employs a combination of speculative threadexecution, together \nwithasequential commit protocol that ensures sequential semantics.By replacing threads with processes \nand providing appropriate shared memory map\u00adpings, Grace leverages process isolation, page protection \nand virtual memory mappings to provide isolation and full support for speculative execution on conventional \nhardware. Under Grace, threadsexecute optimistically, writing their updates speculativelybut locally. \nAs long as the threads do not con.ict, that is, theydo not haveread-write dependencies on the same memory \nlocation, then Grace can safely commit theireffects.In caseofa con.ict, Grace commitsthe earliest thread \nin program order from the con.icting set of threads. Rather thanexecutingthreads atomically, Grace usesevents \nlike thread spawns and joins as commit points that divide execution into pieces of work, and enforces \na deterministic execution that matches a sequential execution. This deterministic execution model allows \nprogrammers to reason about their programs as if they were serial pro\u00adgrams, making them easier to understand \nand debug [2]. Traditionally, when programmers reorganize thread interac\u00adtions to obtain reasonable performance \n(e.g.,by selecting an appropriate grain size, reducing contention, and minimizing the size of critical \nsections), theyrun risk of introducing new, dif.cult-to-debug concurrency errors. Grace not only lifts \nthe burden of using locks or atomic sections on program\u00admers,but also allows them to optimize performance \nwithout the risk of compromising correctness. We evaluate Grace s performance on a suite of CPU\u00adintensive, \nfork-join based multithreaded applications, as well as a microbenchmark we designed to explore the space \nof programs for which Grace will be most effective.We also evaluate Grace s ability to avoid a selection \nof concurrency bugs taken from the literature. Experimental results show that Grace ensures the correct \nexecution of otherwise-buggy concurrent code. While Grace does not guarantee concur\u00ad // Run f(x) and \ng(y) in parallel. t1 = spawn f(x); t2 = spawn g(y); // Wait for both to complete. sync; Figure 1. A multithreaded \nprogram (using Cilk syntax for clarity). // Run f(x) to completion, then g(y). t1 = spawn f(x); t2 = \nspawn g(y); // Wait for both to complete. sync; Figure 2. Its sequential counterpart (elided operations \nstruck out). rencyfor unchanged programs, we found that minor changes (1 16 lines of source code) were \nenough to allow Grace to achieve comparable scalability and performance to the stan\u00addard (unsafe) threads \nlibrary across most of our benchmark suite, while ensuring safe execution. The remainder of this paper \nis organized as follows. Sec\u00adtion2outlines the sequential semantics that Grace provides. Section3describes \nthe software mechanisms that Grace uses to enable speculativeexecution withlowoverhead. Section4 presents \nthe commit protocol that enforces sequential se\u00admantics, and explains how Grace can support I/O together \nwith optimistic concurrency. Section5describes ourexper\u00adimental methodology. Section 6 then presents \nexperimen\u00adtal results across a benchmark suite of concurrent, multi\u00adthreaded computation kernels, a microbenchmark \nthat ex\u00adplores Grace s performance characteristics, and a suite of concurrency errors. Section 7 surveys \nrelated work, Sec\u00adtion8describes future directions, and Section9concludes. 2. Sequential Semantics To \nillustratetheeffectof running Grace,we usetheexample shown in Figure 1, which for clarity uses Cilk-style \nthread operations rather than the subset of the pthreads API that Grace supports. Here, spawn createsa \nthread toexecute the argument function, and sync waits for all threads spawned in the current scope to \ncomplete. Thisexample programexecutesthetwofunctions f and g asynchronously(as threads),andwaitsforthemto \ncomplete. If f and g share state, thisexecution could resultin atomicity violations or race conditions; \nif these functions acquire locks in different orders, then theycould deadlock. Now consider the version \nof this program shown in Figure 2, where calls to spawn and sync (struck out) are ignored. The second \nprogram is the serial elision [5] of the .rst all parallel function calls have been elided. The result \nis a serial program that,byde.nition, cannot suffer from concur\u00ad Concurrency Error Cause Prevention \nby Grace Deadlock cyclic lock acquisition locks converted to no-ops Race condition unguarded updates \nall updates committed deterministically Atomicity violation unguarded, interleaved updates threads run \natomically Order violation threads scheduled in unexpected order threads execute in program order Table \n1. The concurrency errors that Grace addresses, their causes, and how Grace eliminates them. rencyerrors. \nBecause the executions of f(x) and g(y) are not interleaved and execute deterministically, atomicity \nvio\u00adlations or race conditions are impossible. Similarly, the or\u00addering of execution of these functions \nis .xed, so there can\u00adnot be order violations. Finally, a sequential program does not need locks, so \neliding them prevents deadlock. 2.1 Programming Model Grace enforces deterministic execution of programs \nthat rely on fully structured or fork-join parallelism, such as master-slave parallelism or parallelized \ndivide-and-conquer, whereeachdivisionstep forksoffchildren threadsandwaits for themto complete. These \nprogramshaveastraightforward sequential counterpart: the serial elision described above. For convenience, \nGrace exports its operations as a subset of the popular POSIX pthreads API, although it does not support \nthe full range of pthreads semantics. Grace s current target class of applications is applica\u00adtions running \nfork-join style, CPU-intensive operations. At present, Grace is not suitable for reactive programs like \nserver applications, and does not support programs with concurrency control through synchronization primitives \nlike condition variables, or other programs that are inherently concurrent: that is, their serial elision \ndoes not result in a program that exhibits the same semantics. Note that while Graceis abletopreventa \nnumberof con\u00adcurrency errors, it cannot eliminate errors that are external to the program itself.Forexample, \nGrace does not attempt to detect or prevent errors like .le system deadlocks (e.g., through flock())or \ndue to message-passing dependencies on distributed systems. 3. Supportfor Speculation Grace achieves \nconcurrent speedup of multithreaded pro\u00adgrams by executing threads speculatively, then committing their \nupdates in program order (see Section 4).Akeychal\u00adlenge is how to enable low-overhead thread speculation \nin C/C++. One possible candidate would be some form of transac\u00adtional memory [17, 36]. Unfortunately, \nno existing or pro\u00adposed transactional memory system provides all of the fea\u00adtures that Grace requires: \n full compatibility withCand C++ and commodity hard\u00adware,  full support for long-lived transactions, \n  complete isolation of updates from other threads, i.e., strong atomicity [6],  support for irrevocable \nactions including I/O and memory management, and  extremely low runtime and space overhead.  Existing \nsoftware transactional memory (STM) systems are optimized for short transactions, generally demarcated \nwith atomic clauses. These systems do not effectively support long-lived transactions, which either abort \nwhen\u00adever con.icting shorter-lived transactions commit their state .rst, or must switch to single-threaded \nmode to ensurefair progress. They also often preclude the use of irrevocable actions (e.g., I/O) inside \ntransactions [40]. Most importantly, STMs typically incur substantial space and runtime overhead (around \n3X) for fully-isolated mem\u00adory updates inside transactions. While compiler optimiza\u00adtions can reduce \nthis cost on unshared data [37], transactions must still incur this overhead on shared data. In the absence \nof sophisticated compiler analyses, we found that the overheads of conventional log-based STMs are unacceptable \nfor the long transactions that Grace targets. We attempted to employ Sun s state-of-the-art TL2 STM system \n[11] using Pin [28] to instrument reads and writes that call the appropriate TL2 function (transactional \nreads and writes). Unlike most programs using TL2 (including the STAMP transaction benchmark suite), \nthe transactions here comprise every read and write. In all of our tests, the length of the logs becomes \nexcessive, causing TL2 to run out of memory. To meet its requirements, Grace employs a novel virtual\u00admemory \nbased software transactional memory with a num\u00adber ofkey features. First, it supports fully-isolated \nthreads of arbitrary length (in terms of the number of memory ad\u00addresses read or written). Second, its \nperformance overhead is amortized over the length of a thread s execution rather than being incurred \non every access, so that threads that run for more than a few milliseconds effectively run at full speed. \nThird, it supports threads with arbitrary opera\u00adtions, including irrevocable I/O calls (see Section 4). \nFinally, Graceworks withexistingCand C++ applications running on commodity hardware. 3.1 Processes as \nThreads Ourkeyinsight is that we can implement ef.cient software transactional memory by treating threads \nas processes: in\u00adFigure3. Anoverviewofexecutionin Grace. Processes emulate threads (Section3.1)withprivate \nmappingsto mmapped .les that hold committed pages andversion numbers for globals and the heap (Sections \n3.2 and 3.3). Threads run concurrentlybut are committed in sequential order: each thread waits until \nits logical predecessor has terminated in order to preserve sequential semantics (Section 4). Grace then \ncompares the version numbers of the read pages to the committed versions. If theymatch, Grace commits \nthe writes and increments version numbers; otherwise, it discards the pages and rolls back.  stead \nof spawning new threads, Grace forks off new pro\u00adcesses. Because each thread isinfacta separate process, \nit is possible to use standard memory protection functions and signal handlers to track reads and writes \nto memory. Grace tracks accesses to memory at a page granularity, trad\u00ading imprecision of object tracking \nfor speed. Crucially, be\u00adcause only the .rst read or write to each page needs to be tracked, all subsequent \noperations proceed at full speed. To create the illusion that these processes are executing in a shared \naddress space, Grace uses memory mapped .les to share the heap and globals across processes. Each pro\u00adcess \nhas two mappings to the heap and globals: a shared mapping that re.ects the latest committed state, and \na lo\u00adcal (per-process), copy-on-write mapping that each process uses directly. In addition, Grace establishes \na shared and lo\u00adcal map of an array of version numbers. Grace uses these version numbers one for each \npagein the heap and global area to decide when it is safe to commit updates.  3.2 Globals Grace uses \na .xed-size .le to hold the globals, which it lo\u00adcatesinthe programimagethroughlinker-de.nedvariables. \nIn ELF executables, the symbol end indicates the .rst ad\u00address after uninitialized global data. Grace \nuses an ld-based linker script to identify the area that indicates the start of the global data. In addition, \nthis linker script instructs the linker to page align and separate read-only and global areas of memory. \nThis separation reduces the riskoffalse sharing by ensuring that writes to a global object never con.ict \nwith reads of read-only data.  3.3 Heap Organization Grace also uses a .xed-size mapping (currently \n512MB) to hold the heap. It embeds the heap data structure into the be\u00adginning of the memory-mapped .le \nitself. This organization elegantly solves the problem of rolling back memory allo\u00adcations. Grace rolls \nback memory allocations just as it rolls back anyother updates to heap data. Anycon.ict causes the heap \nto revert to an earlier version. However, a na\u00a8ive implementation of the allocator would give rise to \nan unacceptably large number of con.icts: any threads that perform memory allocationswould con.ict.For \nexample, consider a basic freelist-based allocator. Any al\u00adlocation or deallocation updates a freelist \npointer. Thus, any time twothreads both invoke malloc or free on the same\u00adsized object, one thread will \nbe forced to roll back because both threads are updating the page holding that pointer. To avoid this \nproblem of inadvertent rollbacks, Grace usesa scalable per-thread heaporganizationthatis loosely based \non Hoard [3] andbuilt with Heap Layers [4]. Grace divides the heap into a .xed number of sub-heaps (currently \n16). Each thread uses a hash of its process id to obtain the indexof the heap it uses for all memory \noperations(malloc and free). This isolation of each thread s memory operations from the other s allows \nthreads to operate independently most of the time. Each sub-heap is initially seeded with a page\u00adaligned \n64K chunk of memory. As long as a thread does not exhaust its own sub-heap s pool of memory, it will \nop\u00aderate independently from anyother sub-heap. If it runs out of memory, it obtains another 64K chunk \nfrom the global allocator. This allocation only causes a con.ict with another thread if that thread also \nruns out of memory during the same period of time. This allocation strategy has two bene.ts. First, it \nmini\u00admizes the number of false con.icts created by allocations from the main heap. Second, it avoids \nan important source offalse sharing. Because each thread uses different pages to satisfy object allocation \nrequests, objects allocated by one thread are unlikely to be on the same pages as objects al\u00ad  located \nby another thread (except when both threads hash to void atomicBegin (void) { the samesub-heap). This \nheap organization ensures that con\u00ad // Roll back to here on abort. .icts only arise when allocated memory \nfrom aparent thread // Saves PC, registers, stack. is passed to children threads, or when objects allocated \nby context.commit(); one thread are then accessed by another, later thread. // Reset pages seen (for \nsignal handler). To further reduce false sharing, Grace s heap rounds up pages.clear(); large object \nrequests (8K or larger) to a multiple of the // Reset global and heap protection. system page size (4K), \nensuring that large objects never globals.begin(); overlap, regardless of which thread allocated them. \nheap.begin(); } 3.4 Thread Execution Figure 3 presents an overview of Grace s execution of a Figure 4. \nPseudo-code for atomic begin. thread. This example is simpli.ed: recall that Grace does not always execute \nentire threads atomically. Atomic execution begins at program startup (main()), and whenever a new checks \nto see whether the read set is empty, at which point thread is spawned. It ends (is committed) not only \nwhen a it can safely commit. While this situation may appear to thread ends, but also when a thread spawns \na child or joins be unlikely, it is common when multiple threads are being (syncs) a previously-spawned \nchild thread. created inside a for loop, and thus the application is only Before the program begins, \nGrace establishes shared and reading local variables from registers. Allowing commits in local mappings \nfor the heap and globals. It also establishes this case is an important optimization, because otherwise, \nthe mappings for the version numbers associated with each Grace would have to pause the thread until \nits immediate page in both the heap and global area. Because these pages predecessor the last thread \nit has spawned has commit\u00ad are zero-.lled on-demand, this mapping implicitly initializes ted. As Section \n4 explains, this step is required to provide the version numbers to zero. A page s version number is \nsequential semantics. incremented only on a successful commit, so it is equivalent to its total number \nof successful commits to date. Committing Initialization Once a thread has .nished executing and anylogically \npre\u00adceding threads have already completed, Grace establishes Grace initializes state tracking at the \nbeginning of pro\u00ad locks on all .les holding memory mappings using inter\u00ad gram execution and at the start \nof every thread by invoking process mutexes (in the call to lock())and proceeds to atomicBegin (Figure \n4). Grace .rst saves the execution check whether it is safe to commit its updates. Notice that context \n(program counter, registers, and stack contents) and this serialization only occurs during commits; thread \nexecu\u00ad sets the protection of every page to PROT NONE, so that any tion is entirely concurrent. access \ntriggers a fault. It also clears both its read and write Grace .rst performs a consistencycheck, comparing \nthe sets, which hold the addresses of every page read or written. version numbers for every page in the \nread set against the Execution committed versions both for the heap and the globals. If they all match, \nit is safe for Grace to commit the writes, Grace tracks accesses to pages by handling SEGV protec\u00ad which \nit does by copying the contents of each page into the tion faults. The .rst access to each page is treated \nas a read. corresponding page in the shared images. It then relinquishes Grace adds the page address \nto the read set, and then sets the .le locks and resumes execution. the protection for the page to read-only. \nIf the application If, however, any of the version numbers do not match, later writes to the page, Grace \nadds the page to the write Grace invokes atomicAbort to abort the current execu\u00ad set, and then removes \nall protection from the page. Thus, tion (Figure 5). Grace issues a madvise(MADV DONTNEED) in the worst \ncase, a thread incurs two minor page faults for call to discard any updates to the heap and globals, \nwhich every page that it visits. While protection faults and signals forces all new accesses to use memory \nfrom the shared are expensive, their cost is quickly amortized even for rel\u00ad (committed) pages. It then \nunlocks the .le maps and re\u00ad atively short-lived threads (e.g., a millisecond or more), as executes, \ncopying the saved stack over the current stack and Section 6.2 shows. then jumping into the previously \nsaved execution context. Completion At the end of each atomically-executed region the end 4. Sequential \nCommit of main() or an individual thread, right before a thread Grace provides strong isolation of threads, \nensuring that they spawn, and right before joining another thread Grace in\u00ad do not interfere with each \nother when executing specula\u00ad vokes atomicEnd (Figure 5), which attempts to commit tively. However, this \nisolation on its own does not guarantee all updates by calling atomicCommit (Figure 6). It .rst sequential \nsemantics because it does not prescribe anyorder.  void atomicEnd (void){ if (!atomicCommit()) atomicAbort(); \n} void atomicAbort (void){ // Throw away changes. heap.abort(); globals.abort(); // Jump back to saved \ncontext. context.abort(); } Figure 5. Pseudo-code for atomic end and abort. bool atomicCommit (void){ \n// If haven t read or written anything, // we don t have to wait or commit; // update local view of memory \n&#38; return. if (heap.nop() &#38;&#38; globals.nop()) { heap.updateAll(); globals.updateAll(); return \ntrue; } // Wait for immediate predecessor // to complete. waitExited(predecessor); // Now try to commit \nstate. Iff we succeed, // return true. // Lock to make check &#38; commit atomic. lock(); bool committed \n= false; // Ensure heap and globals consistent. if (heap.consistent() &#38;&#38; globals.consistent()) \n{ // OK, all consistent: commit. heap.commit(); globals.commit(); xio.commit(); // commits buffered \nI/O committed = true; } unlock(); return committed; } Figure 6. Pseudo-code for atomic commit. Toprovidethe \nappearanceof sequentialexecution, Grace not only needs to provide isolation of each thread,but also must \nenforce a particular commit order. Grace employs a simple commit algorithm that provides the effect of \na se\u00adquential execution. Grace s commit algorithm implements the following pol\u00adicy: a thread is only \nallowed to commit after all of its logi\u00adcal predecessorshave completed.Itmight appear that sucha commit \nprotocol would be costly to implement, possibly re\u00ad void * spawnThread (threadFunction * fn, void arg) \n{ * // End atomic section here. atomicEnd(); // Allocate shared mem object // to hold thread s return \nvalue. ThreadStatus t = * new (allocateStatus()) ThreadStatus; // Use fork instead of thread spawn. int \nchild = fork(); if (child) { // I m the parent (caller of spawn). // Store the tid to allow later sync \n// on child thread. t->tid = child; // The spawned child is new predecessor. predecessor = child; // \nStart new atomic section // and return thread info. atomicBegin(); return (void *) t; } else { // I \nm the child. // Set thread id. tid = getpid(); // Execute thread function. atomicBegin(); t->retval = \nfn(arg); atomicEnd(); // Indicate that process has ended // to alert its successor (parent) // that \nit can continue. setExited(); // Done. _exit (0); } } Figure 7. Pseudo-code for thread creation. Note \nthat the actual Grace library wraps thread creation and joining with a pthreads-compatible API. quiring \nglobal synchronization and complex data structures. Instead, Grace employs a simple and ef.cient commit \nalgo\u00adrithm, which threads the tree of dependencies through all the executing threads to ensure sequential \nsemantics. Executing threads form a tree, where the post-order traversal speci.es the correct commit \norder. Parents must wait for their last-spawned child, children wait either for their preceding sibling \nif it exists, or the parent s previous sibling. Grace threads the tree of dependencies through all the \nexecuting threads to ensure sequential semantics. Thekeyis that only thread spawnsaffect commit depen\u00addence, \nand then only affect those of the newly-spawned child and parent processes. Each new child always appears \nimme\u00addiately before its parent in the post-order traversal. Updat\u00ading the predecessor values is akin \nto inserting the child pro\u00ad  void joinThread (void * v, void ** result) { ThreadStatus t = (ThreadStatus \n*) v; * // Wait for a particular thread // (if argument non-NULL). if (v != NULL) { atomicEnd(); // \nWait for thread to terminate. if (t->tid) waitExited (t->tid); // Grab thread result from status. if \n(result != NULL) { *result = t->retval; // Reclaim memory. freeStatus(t); } atomicBegin(); } } Figure \n8. Pseudo-code for thread joining. cess into a linked list representing this traversal. Each child sets \nits predecessor to the parent s predecessor (which hap\u00adpens automatically because of the semantics of \nfork), and then the parent sets its predecessor to the child s ID (see Fig\u00adure 7). The parent then continues \nexecution until the next com\u00admitpoint(theendofthe thread,anewthreadspawn,orwhen it joins another thread). \nAt this time, if the parent thread has read anymemory from the heap or globals(see Section 3.4), it then \nwaits on a semaphore that the child thread sets when itexits (see Figures7and8). 4.1 Transactional I/O \nGrace s commit protocol not only enforces sequential se\u00admanticsbut also has an additional important bene.t. \nBecause Grace imposes an order on thread commits, there is always one thread running that is guaranteed \nto be able to commit its state: the earliest thread in program order. This property en\u00adsures that Grace \nprograms cannot suffer from livelock caused byafailureofanythreadtomake progress,a problem with some \ntransactional memory systems. Thisfactallows Gracetoovercomeaneven moreimpor\u00adtant limitation of most \nproposed transactional memory sys\u00adtems: it enables the execution of I/O operations in a system with optimistic \nconcurrency. Because some I/O operations are irrevocable (e.g., network reads after writes), most I/O \noperations appear to be fundamentally at odds with specula\u00adtive execution. The usual approach is to ban \nI/O from spec\u00adulative execution, or to arbitrarily pick a winner to obtain a global lock prior to executing \nits I/O operations. In Grace, each threadbuffers its I/O operations and com\u00admits them at the same time \nit commits its updates to memory, as shown in Figure 6. However, if a thread attempts to exe\u00adcute an \nirrevocable I/O operation, Grace forces it to wait for its immediate predecessor to commit. Grace then \nchecks to make sure that its current state is consistent with the com\u00admitted state. Once both of these \nconditions are met, the cur\u00adrent thread is then guaranteed to commit when it terminates. Grace then allows \nthe thread to perform the irrevocable I/O operation, which is now safe because the thread s execution \nis guaranteed to succeed. 5. Methodology We perform our evaluation on a quiescent 8-core system (dual \nprocessor with4cores), and 8GB of RAM. Each pro\u00adcessor is a 4-core 64-bit Intel Xeon running at 2.33 \nGhz with a 4MB L2 cache. We compare Grace to the Linux pthreads library (NPTL), on Linux 2.6.23 with \nGNU libc version 2.5. 5.1 CPU-Intensive Benchmarks Weevaluate Grace s performance on real computationker\u00adnels \nwith a range of benchmarks, listed in Table 2. One benchmark, matmul a recursive matrix-matrix multi\u00adply \nroutine comes from the Cilk distribution. We hand\u00adtranslated this program to use the pthreads API (es\u00adsentially \nreplacing Cilk calls like spawn with their coun\u00adterparts). We performed the same translation for the \nre\u00admaining Cilk benchmarks, but because they use unusu\u00adally .ne-grained threads, none of them scaled \nwhen using pthreads. The remaining benchmarks are from the Phoenix bench\u00admark suite [34]. These benchmarks \nrepresentkernel compu\u00adtations and were designed to be representative of compute\u00adintensivetasks fromarangeof \ndomains, including enterprise computing, arti.cial intelligence, and image processing.We use the pthreads-basedvariantsof \nthese benchmarks with the largest available inputs. In addition to describing the benchmarks, Table 2 \nalso presents detailed benchmark characteristics measured from their execution with Grace, including \nthe total number of commits and rollbacks, together with the average number of pages read and written \nand average wall-clock time per atomicregion.With theexceptionof matmul and kmeans, the benchmarks read \nand write from relatively few pages in each atomic region. matmul has a coarse grain size and large footprint,but \nhas no interference between threads due to the regular structure of its recursive decomposition. On the \nother hand, kmeans has a benign race which forces Grace to trigger numerous rollbacks (see Section 6.1). \n 5.1.1 Modi.cations All of these programs run correctly with Grace out of the box ,but as weexplain below, \nthey required slight tweak\u00ading to allow them to scale (with no modi.cations, none of the programs scale). \nThese changes were typically short and local, requiring one or two lines of new code, and re\u00adquired no \nunderstanding of the application itself. Several of  Benchmark Description histogram Analyzes images \nRGB components kmeans Iterative clustering of 3-D points linear regression Computes best .t line for \nset of points matmul Recursive matrix-multiply pca Principal component analysis on matrix 4.3 191.1 \nTable 2. CPU-intensive multithreaded benchmark suite and detailed characteristics (see Section 5.1). \n9 0 7.3 5.9 1512.3 6273 4887 404.5 2.3 8.7 2.2 0.204 string match Searches .le for encrypted word these \nchanges could be mechanically applied by a compiler, thoughwehavenotexploredthis.(Wenotethatthe modi.\u00adcation \nof benchmarks to explore new programming models is standard practice, e.g., in papers exploring software \ntrans\u00adactional memory or map-reduce.) Thread-creation hoisting/argument padding: In most of the applications, \nthe only modi.cation we made was to the loop that spawned threads. In the Phoenix benchmarks, this loop \nbody typically initializes each thread s arguments before spawning the thread.False sharing on these \nupdates causes Grace to serialize all of threads, precluding scala\u00adbility.We resolved this eitherby hoisting \nthe initialization (initializing thread arguments .rst in a separate loop and then spawning the threads), \nor, where possible, by padding the thread argument data structures to 4K. In one case, for the kmeans \nbenchmark, the benchmark erroneously reuses the same thread arguments for each thread, which not only \ncauses Graceto serializethe programbutalsoisa race con\u00addition.We .xed the codeby creatinga new heap-allocated \nstructure to hold the arguments for each thread. Page-size base case:We made a one-line change to the \nmatmul benchmark, where we increased the base matrix sizeofthe recursiontoamultipleofthesizeofapagetopre\u00adventfalse \nsharing. Interestingly,this modi.cationwas bene\u00ad.cial not only for Gracebut alsofor the pthread version. \nIt not only reducesfalse sharing across the threadsbut also im\u00adprovesthe baseline performanceofthe benchmarkbyaround \n8%by improving its cache utilization. Changed concurrency structure: Our most substantial change (16 \nlines of code) was to pca, where we changed the way that the program manages concurrency. The origi\u00adnal \nbenchmark divided work dynamically across a number of threads, with each thread updating a global variable \nto in\u00addicate which row of a matrix to process next: with Grace, the .rst thread performed allof the computations.To \nenable pca to scale, we statically partitioned the work by provid\u00ading each thread with a range of rows. \nThis modi.cation had little impact on the pthreads versionbut dramatically im\u00adproved the scalability \nwith Grace. Summary: The vast majority of the code changes were local, purely mechanical and required \nminimal programmer intervention, primarilyin the thread creation loop.In almost every case, the modi.cations \nrequired no knowledge of the (average per atomic region) Commits Rollbacks Pages Read Pages Written \nRuntime (ms) 4.8 1024.0 1865 2359.4 Figure 9. Performance of multithreaded benchmarks run\u00adning with pthreads \nand Grace on an8core system (higher is better). Grace generally performs nearly as well as the pthreads \nversion while ensuring the absence of concur\u00adrency errors. underlying application. The reordering or \nmodi.cation in\u00advolved a small number of lines of code (1 16). 6. Evaluation Our evaluation answers the \nfollowing questions: 1. How well does Grace perform on real applications? 2. What kind of applications \nwork best with Grace? 3. How effective is Grace against a range of concurrency errors?  6.1 RealApplications \nFigure9shows the resultof running our benchmark suiteof applications, graphed as their speedup over a \nserial execu\u00adtion. The Grace-based versions achieve comparable perfor\u00admance while at the same time guaranteeing \nthe absence of concurrency errors. The average speedup for Grace is 6.2X, while the average speedup for \npthreads is 7.13X. There are two notable outliers. The .rst one is pca, which exhibits superlinear speedups \nboth for Grace and pthreads. The superlinear speedup is due to improved cache locality caused by the \ndivision of the computation into smaller chunks across multiple threads.   While the kmeans benchmark \nachievesamodest speedup with pthreads (3.65X), it exhibits no speedup with Grace (1.02X), which serializes \nexecution. This benchmark itera\u00adtively clusters points in 3D space. Until it makes no further modi.cations, \nkmeans spawns threads to .nd clusters (set\u00adting a cluster id for each point), and then spawns threads \nto computeand store meanvaluesina shared array.Itwouldbe straightforward to eliminate all rollbacks for \nthe .rst threads by simply rounding up the number of points assigned to each thread, allowing each thread \nto work on independent regions of memory. However, kmeans does not protect ac\u00adcesses or updates to the \nmean value array and instead uses benign races as a performance optimization. Grace has no way of knowing \nthat these races are benign and serializes its execution to prevent the races.  6.2 Application Characteristics \nWhile the preceding evaluation shows that Grace performs well on a range of benchmarks, we also developed \na mi\u00adcrobenchmark to explore a broader space of applications. In particular,our microbenchmarkallowsustovarythefollow\u00ading \nparameters: grain size, the running time of each thread; footprint,the numberof pages updatedbya thread;and \ncon\u00ad.ict rate, the likelihood of con.icting updates by a thread. These parameters isolate Grace s overheads. \nFirst, the shorter a thread s execution (the smaller its grain), the more the increased cost of thread \nspawns in Grace (actually pro\u00adcess creation) should dominate. Second, increasing the num\u00adber of pages \naccessed by a thread (its footprint) stresses the cost of Grace s page protection and signal handling. \nThird, increasing the number of con.icting updates forces Grace to rollback and re-execute code more \noften, degrading perfor\u00admance. Grain size: We .rst evaluate the impact of the length of thread execution \non Grace s performance. We execute a range of tests, where each thread runs for some .xed number of milliseconds \nperforming arithmetic operations in a tight loop. Notice that this benchmark only exercises the CPU and \nthe cost of thread creation and destruction, because it does not reference heap pages or global data. \nEach experiment is con.gured to run for a .xed amount of time: nTh \u00d7 len \u00d7 nIter = 16 seconds, where \nnTh is the number of threads (16), len is the thread running time, and nIter is the number iterations. \n Figure 10 shows the effect of thread running time on performance. Because we expected the higher cost \nof thread spawns to degrade Grace sperformance relativeto pthreads, we were surprisedtoviewthe oppositeeffect.Wediscovered \nthat the operating system s scheduling policy plays an im\u00adportant role in this set of experiments. When \nthe size of each thread is extremely small, neither Grace nor pthreads makeeffectiveuseofavailable CPUs. \nIn both cases, the processes/threads .nish so quickly that the load balancer is not triggered and so \ndoes not run them on different CPUs. As the thread running time becomes larger, Grace tends to make better \nof CPU resources, sometimes up to 20% faster. We believe this is because the Linux CPU scheduler attempts \nto put threads from the same process on one CPU to exploit cache locality, which limits its ability to \nuse more CPUs,but is more liberal in its placement of pro\u00adcesses across CPUs. However, once thread running \ntime be\u00adcomeslargeenough(over50ms)fortheload balancertotake effect, both Grace and pthreads scale well. \nFigure 10(b) shows that Grace has competitive performance compared to pthreads, and the overhead of process \ncreation is never larger than 2%. Footprint:In order to evaluate the impact of per-thread footprint, \nwe extend the previous benchmark so that each thread also writes a value onto a number of private pages, \nwhich only exercises Grace s page protection mechanism without triggering rollbacks.We conduct anextensive \nsetof tests, ranging thread footprint from1 pages to 1024 pages (4MB).Thisexperimentistheworstcase scenarioforGrace, \nsince each write triggers two pagefaults. Figure 11 summarizes the effect of thread footprint over three \nrepresentative thread running time settings: small (10ms), medium (50ms) and large (200ms). When the \nthread footprint is not too large(= 64 pages), Grace has compara\u00adble performance to pthreads, with no \nmore than a 5% slowdown. As the thread footprint continues to grow, the performance of Grace starts to \ndegrade due the overhead of page protectionfaults.However,even when each thread dirties one megabyte \nof memory (256 pages), Grace s per\u00adformance is within an acceptable range for the medium and large thread \nruntime settings. The overhead of page protec\u00adtionfaults only becomes prohibitively large when the thread \nfootprint is large relative to the running time, which is un\u00adlikely to be representative of compute-intensive \nthreads. Con.ict rate: We next measure the impact of con.icting updateson Grace sperformancebyhavingeach \nthreadinthe microbenchmark updatea globalvariable withagiven prob\u00adability,which the result that anyother \nthread reading or writ\u00ading that variable will need to rollback and re-execute. Grace makes progress even \nwith a 100% likelihood of con.icts be\u00adcause its sequential semantics provide a progress guarantee: the \n.rst thread in commit order is guaranteed to succeed without rolling back Figure 12 shows the resulting \nimpact on speedup (where each thread runs for 50 milliseconds). When the con.ict rate is low, Grace \ns performance re\u00admains close to that of pthreads. Higher con.ict rates de\u00adgrade Grace s performance, \nthough to a diminishing extent: a 5% con.ict rate leads to a 6-way speedup, while a 100% con.ict rate \nmatches the performance of a serial execution. In this benchmark, one processor is always performing \nuse\u00adful work, so performance matches the serial baseline. In a program with many morethreads than processors,however, \na 100% con.ict rate under Grace would result in a slow\u00addown. Summary: This microbenchmark demonstrates \nthat the use of processes versus threads in Grace has little impact on performance for threads that run \nas little as 10ms, adding no more than 2% overhead and actually providing slightly better scalability \nthan pthreads in some cases. Memory protection overhead is minimal when the number of pages dirtied is \nnot excessively large compared to the grain size (e.g., up to 2MB for 50ms threads). Rollbacks triggered \nby con.icting memory updates have the largest impact on performance. While Grace can provide scalability \nfor high con.ict rates, the con.ict rate should bekept relatively low to ensure reasonable performance \nrelative to pthreads.  6.3 Concurrency Errors We illustrate Grace s ability to eliminate most concurrency \nbugsby compilingabug suite primarily drawn from actual bugs describedinpreviousworkon error detectionand \nlisted inTable3 [25, 26, 27]. Because concurrency errors areby their nature non-deterministic and occur \nonly for particular thread interleavings, we inserted delays (via the usleep function call) at key points \nin the code. These delays dra\u00admatically increase the likelihood of encountering these er\u00adrors, allowing \nus to compare the effect of using Grace and pthreads.  Bug type Benchmark description deadlock Cyclic \nlock acquisition race condition Race condition example, Lucia et al. [27] atomicity violation Atomicity \nviolation from MySQL [26] order violations Order violation from Mozilla 0.8 [25] Table 3. Error benchmark \nsuite. // Deadlock. thread1 () { lock (A); // usleep(); lock (B); // ...do something unlock (B); unlock \n(A); } thread2 () { lock (B); // usleep(); lock (A); // ...do something unlock (A); unlock (B); } Figure \n13. Deadlock example. This code has a cyclic lock acquisition pattern that triggers a deadlock under \npthreads while running to completion with Grace. 6.3.1 Deadlocks Figure 13 illustrates a deadlock error \ncaused by cyclic lock acquisition. This example spawns two threads that each at\u00adtempt to acquire two \nlocks A and B,butin different orders: thread 1 acquires lock A then lock B, while thread 2 ac\u00adquires \nlock B then lock A. When using pthreads, these threads deadlock if both of them manage to acquire their \n.rst locks, becauseeachofthe threadsiswaitingtoacquirealock heldby the other thread. Inserting usleep \nafter these locks makes this program deadlock reliably under pthreads. However, because Grace s atomicity \nand commit protocol lets it treat locks as no-ops, this program never deadlocks with Grace. 6.3.2 Race \nconditions We next adapt an example from Lucia et al. [27], removing the lock in the original example \nto trigger a race. Figure 14 shows two threads both executing increment, which in\u00adcrementsa sharedvariable \ncounter. However,because ac\u00adcess to counter is unprotected, both threads could read the samevalueandsocanlosean \nupdate. Runningthisexample under pthreads with an injected delay exhibits this race, printing 0,0,1,1. \nBy contrast, Grace prevents the race by // Race condition. int counter = 0; increment() { print (counter); \nint temp = counter; temp++; // usleep(); counter = temp; print (counter); } thread1() { increment(); \n} thread2() { increment(); } } Figure 14. Race condition example: the race is on the vari\u00adable counter, \nwhere the .rst update can be lost. Under Grace, both increments always succeed. // Atomicity violation. \n// thread1 S1: if (thd->proc_info) { // usleep(); S2: fputs (thd->proc_info,..) } // thread2 S3: thd->proc_info \n= NULL; Figure 15. An atomicity violation from MySQL [26]. A faulty interleaving can cause this code \nto trigger a segmen\u00adtation fault due to a NULL dereference, but by enforcing atomicity, Grace prevents \nthis error. executing each thread deterministically, and invariably out\u00adputs the sequence 0,1,1,2. 6.3.3 \nAtomicityViolations To verify Grace s ability to cope with atomicity violations, we adapted an atomicity \nviolationbug taken from MySQL s InnoDB module, describedbyLuetal.[26].Inthisexample, shown in Figure \n15, the programmer hasfailed to properly protect access to the global variable thd. If the scheduler \nexecutes the statement labeled S3 in thread2 immediately after thread 1 executes S1, the program will \ndereference NULL andfail. Inserting a delay between S1 and S2 causes every exe\u00adcution of this code with \npthreads to segfault because of a NULL dereference.With Grace, threads appeartoexecute atomically, so \nthe program always performs correctly.  6.3.4 Order violations Finally, we consider order violations, \nwhich were recently identi.ed as a common class of concurrency errors by Lu et  // Order violation. \nchar proc_info;* thread1() { // ... // usleep(); proc_info = malloc(256); } thread2() { // ... strcpy(proc_info,\"abc\"); \n} main() { spawn thread1(); spawn thread2(); } Figure 16. An order violation.If thread2 executes before \nthread 1, it writes into unallocated memory. Grace ensures that thread 2 always executes after thread \n1, avoiding this error. al. [26]. An order violation occurs when the program runs correctly under one \norderingof threadexecutions,but incor\u00adrectly underadifferent schedule. Notice that order violations are \northogonal to atomicity violations: an order violation can occur even when the threads are entirely atomic. \nFigure 16 presents a case where the programmer s in\u00adtendedorderisnot guaranteedtobeobeyedbythe scheduler. \nHere,if thread2managesto write into proc info before it has been allocatedby thread1,it will causeasegfault.How\u00adever, \nbecause the scheduler is unlikely to be able to sched\u00adule thread2before thread1hasexecuted the allocation \ncall, this code will generally work correctly. Nonetheless, it will occasionallyfail, and injecting usleep() \nforces it tofail reliably.With Grace, this microbenchmarkalways runs cor\u00adrectly, because Grace ensures \nthat the spawned threads ex\u00adhibit sequentialsemantics. Thus, thread2 can commit only after thread1completes, \npreventing the order violation. Interestingly,while Grace prescribes the order of program execution,Figure17showsthattheexpected \nordermightnot be the order that Grace enforces. In this example, modeled after an order violationbug \nfrom Mozilla, the pthreads version is almost certain to execute statement S2 immedi\u00adately after S1; that \nis, well before the scheduler is able to run thread1. The .nal value of foo (once thread1 ex\u00adecutes) \nwill therefore almost always be 0. However, in the rare event that a context switch occurs immediately \nafter S1, the thread may get a chance to run .rst, leaving the value of foo at1and causing the assertion \ntofail.Suchabugwouldbe unlikelytoberevealedduring testingand could leadtofailuresinthe .eld thatwouldbe \nexceedingly dif.cult to locate. // Order violation. int foo; thread1() { foo = 0; } main() { S1: spawn \nthread1(); // usleep(); S2: foo=1; // ... assert (foo == 0); } Figure 17. An order violation. Here, \nthe intended effect violates sequential semantics, so the error is not .xed but occurs reliably. However, \nwith Grace, the .nal value of foo will always be 1, because that result corresponds to the result of \na se\u00adquential execution of thread1. While this result might not have been the one that the programmer \nexpected, using Grace would have made the error both obvious and repeat\u00adable, and thus easier to .x. \n7. RelatedWork The literature relating to concurrent programming is vast. We brie.y describe the most \nclosely-related work here. 7.1 Transactional memory The area of transactional memory, .rst proposed \nby Herlihy and Moss for hardware [17] and for software by Shavit and Touitou [36], is now a highly active \narea of research. Larus and Rajwar s book provides an overview of recent work in the area [23]. We limit \nour discussion here to the most closely related software approaches that run on commodity hardware. Transactional \nmemory eliminates deadlocksbut does not address other concurrency errors like races and atomicity, leaving \nthe burden on the programmer to get the atomic sections right. Worse, software-based transactional mem\u00adory \nsystems (STM) typically interact poorly with irrevoca\u00adble operations like I/O and generally degrade performance \nwhen compared to their lock-based counterparts, especially those that provide strong atomicity [6]. STMs \nbased on weak atomicity can provide reasonable performance but expose programmers to a range of new and \nsubtle errors [37]. Fraser and Harris s transaction-based atomic blocks [15] are a programming construct \nthat has been the model for manysubsequent language proposals. However, the seman\u00adtics of these language \nproposals are surprisingly complex. For example, Shpeisman et al. [37] show that proposed weak transactions \ncan give rise to unanticipated and un\u00adpredictable effects in programs that would not have arised when \nusing lock-based synchronization. With Grace, pro\u00adgram semantics are straightforward and unsurprising. \n Welc et al. introduce support for irrevocable transactions in the McRT-STM system for Java [40]. Like \nGrace, their system supports one active irrevocable transaction at a time. McRT-STM relies on a lock \nmechanism combined with compiler-introduced read and write barriers, while Grace s support forI/Ofalls \nout for free from its commit protocol. The McRT system for C++ also includes a malloc imple\u00admentation \ncalled McRT-malloc, which resembles Hoard [3] but is extended to support transactions [19]. Ni et al. \npresent the design and implementation of a transactional extension to C++ that enable transactional use \nof the system memory allocator by wrapping all memory management functions and providing custom commit \nand undo actions [31]. These approaches differ substantially from Grace s memory allo\u00adcator, which employsafar \nsimpler design thatleverages the fact that in Grace, all code, including malloc and free, execute transactionally. \nGrace also takes several additional steps that reduce the riskoffalse sharing.  7.2 Concurrent programming \nmodels We restrict our discussion of programming models here to imperative rather than functional programming \nlanguages. Cilk[5]isa multithreadedextensionoftheCprogramming language. Like Grace, Cilk uses a fork-join \nmodel of paral\u00adlelism and focuses on the use of multiple threads for CPU intensive workloads, rather \nthan server applications. Unlike Grace,whichworkswithCorC++ binaries,Cilkis currently restricted to C. \nCilk also relies on programmers to avoid race conditions and other concurrency errors; while there has \nbeen work on dynamic tools to locate these errors [8], Grace automatically prevents them. A proposed \nvariant of Cilk called Transactions Everywhere adds transactions to Cilkbyhavingthe compiler insert cutpoints \n(transaction end and begin) at various points in the code, including at the end of loop iterations. While \nthis approach reduces expo\u00adsure to concurrencyerrors, it does not prevent them, and data race detection \nin this model has been shown to be an NP\u00adcomplete problem [18]. Concurrencyerrors remain common even \nin fork-join programs: Feng and Leiserson report that their Nondeterminator race detector for Cilk found \nraces in several Cilk programs written by experts, as well as in half the submitted implementations of \nStrassen s matrix-matrix multiply in a class at MIT [13]. Intel s Threading Building Blocks(TBB)isaC++ \nlibrary that provides lightweight threads ( tasks ) executing on a Cilk-like runtime system [35]. TBB \ncomprisesa non-POSIX compatible API, primarilybuilding ona fork-join program\u00adming model with concurrent \ncontainers and high-level loop constructs like parallel do that abstract away details liketask creation \nand barrier synchronization (although TBB also includes support for pipeline-based parallelism, which \nGracedoesnot).TBB reliesonthe programmertoavoid con\u00adcurrency errors that Grace prevents. Automatic mutual \nexclusion, or AME, is a recently\u00adproposed programming model developed at Microsoft Re\u00adsearch Cambridge. \nIt is a language extension to C# that as\u00adsumes that all shared state is private unless otherwise indi\u00adcated \n[20]. These guarantees are weaker than Grace s, in that AME programmers can still generate code with \nconcurrency errors. AME has a richer concurrent programming model than Grace that makes it more .exible,but \nits substantially more complex semantics preclude a sequential interpreta\u00adtion [1]. By contrast, Grace \ns semantics are straightforward and thus likely easier for programmers to understand. von Praunetal. \npresent ImplicitParallelismwith Ordered Transactions (IPOT), that describes a programming model, like \nGrace, that supports speculative concurrency and en\u00adforces determinism [38]. However, unlike Grace, IPOT \nre\u00adquiresacompletely newprogramming language, withawide range of constructs including variable type annotations \nand constructs to support speculative and explicit parallelism. In addition, IPOTwould require special \nhardware and compiler support, while Grace operates on existing C/C++ programs that use standard thread \nconstructs. Welc et al. present a future-based model for Java pro\u00adgramming that, like Grace, is safe \n[39].Afuture denotes an expression that may be evaluated in parallel with the rest of the program; when \nthe program uses the expression s value, it waits for the future to complete execution before continuing. \nAs with Grace s threads, safe futures ensure that the concurrent execution of futures provides the same \neffect asevaluatingtheexpressions sequentially.However,thesafe future system assumes that writes are \nrare in futures (by con\u00adtrast with threads), and uses an object-based versioning sys\u00adtem optimized for \nthis case. It also requires compiler support and currently requires integration withagarbage-collected \nenvironment, making it generally unsuitable for use with C/C++. Grace s use of virtual memory primitives \nto support spec\u00adulation is a superset of the approach used by behavior\u00adoriented parallelism (BOP) [12]. \nBOP allows programmers to specify possibly parallelizable regions of code in sequen\u00adtial programs, and \nuses a combination of compiler analysis and the strong isolation properties of processes to ensure that \nspeculative execution never prevents a correct execution. While BOP seeks to increase the performance \nof sequen\u00adtial code by enabling safe, speculative parallelism, Grace provides sequential semantics for \nconcurrently-executing, fork-join based multithreaded programs.  7.3 Deterministic thread execution \nA number of runtime systems have recently appeared that are designed to provide a measure of deterministic \nexecu\u00adtion of multithreaded programs. Isolator uses a combination of programmer annotation, custom memory \nallocation, and virtual memory primitives to ensure that programs follow a locking discipline [33]. Isolator \nworks on existing lock\u00adbased codes, but does not address issues like atomicity or deadlock.Kendo alsoworks \non stock hardware and provides deterministicexecution,but onlyof the orderof lock acqui\u00adsitions [32]. \nIt also requires data-race free programs. DMP uses hardware support to provide a total ordering on multi\u00adthreadedexecution, \nwhich aims to ensure that programs reli\u00adably exhibit the same errors, rather than attempting to elimi\u00adnate \nconcurrency errors altogether [10].  In concurrentwork, Bocchinoetal. present Deterministic Parallel \nJava (DPJ), a dialect of Java that adds two parallel constructs(cobegin and foreach)[21].Aprogrammer \nusing DPJ provides region annotations to describe accesses to disjoint regions of the heap. DPJ s type \nand effect system then veri.es the soundness of these annotations at compile\u00adtime, allowing it to execute \nnon-interfering code in parallel with the guarantee that the parallel code executes with the same semantics \nas a sequential execution (although it re\u00adlies on the correctness of commutativity annotations). Un\u00adlike \nGrace, DPJ does not rely on runtime support, but re\u00adquires programmer-supplied annotations and cannot \nprovide correctness guarantees for ordinary multithreaded code out\u00adside the parallel constructs.  7.4 \nOther uses of virtual memory A number of distributed shared memory (DSM) systems of the early 90 s also \nemployed virtual memory primitives to detect reads and writes and implement weaker consis\u00adtency models \ndesigned to improve DSM performance, in\u00adcluding Munin [7] andTreadMarks [22]. While both Grace and these \nDSM systems rely on these mechanisms to trap reads and writes, the similarities end there. Grace executes \nmultithreaded shared memory programs on shared memory systems, rather than creating the illusion of shared \nmemory onadistributedsystem,wheretheoverheadsofmemorypro\u00adtection and pagefault handling are negligible \ncompared to the costs of network transmission of shared data. 8. FutureWork In this section, we outline \nseveral directions for future work for Grace, including extending its range of applicability and further \nimproving performance. We intend to extend Grace to support other models of concurrency beyond fork/join \nparallelism. One potential classof applicationsis request/response servers, whereasin\u00adgle controller \nthread spawns manymostly-indepedent child threads. For these programs, Grace could guarantee isola\u00adtion \nfor child threads while maintaining scalability. This ap\u00adproach would require modifying Grace s semantics \nto allow the controller thread to spawn new children without commit\u00adting in order to allow it to handle \nthe side-effects of socket communication without serializing spawns of child threads. While con.icts \ncause rollbacks, theyalso provide poten\u00adtially useful information that can be fed back into the run\u00adtime \nsystem. We are building enhancements to Grace that will both report memory areas that are the source \nof frequent con.icts and act on this information. This information can guide programmers as they tune \ntheir programs for higher performance. More importantly, we are currently develop\u00ading a tool that will \nallow this data to be used by Grace to automatically prevent con.icts (without programmer inter\u00advention) \nby padding or segregating con.icting heap objects from different call sites. While we have shown that \nprocess invocation is surpris\u00adingly ef.cient, we would like to further reduce the cost of threads. While \nwe do not evaluate it here, we recently de\u00adveloped a technique that greatly lowers the cost of thread \ninvocationby taking advantageof the followingkeyinsight. Once a divide-and-conquer application has spawned \na large enough number of threads to take advantage of available processors, it is possible to practically \neliminate the cost of thread invocation at deeper nesting levels by directly exe\u00adcuting thread functions \ninstead of spawning new processes. While this approach has no impact on our benchmark suite, it dramatically \ndecreases the cost of thread spawns, running at under 2X the cost of Cilk s lightweight threads. Another \npossible use of rollback information would be for scheduling: the runtime system could partition threads \ninto con.icting sets, and then only schedule the .rst thread (in serial order) from each of these sets. \nThis algorithm would maximize the utilization of available parallelism by preventing repeated rollbacks. \nWe are also investigating the use of compiler optimiza\u00adtions to automatically transform code to increase \nscalabil\u00adity. For example, Grace s sequential semantics could en\u00adable cross-thread optimizations, such \nas hoisting con.icting memory operations out of multiple threads. 9. Conclusion This paper presents Grace, \na runtime system for fork-join based C/C++ programs that, by replacing the standard threads library with \na system that ensures deterministic execution, eliminates a broad class of concurrency errors, including \ndeadlocks, race conditions, atomicity violations, and order violations. With modest source code modi.ca\u00adtions \n(1 16 lines of code in our benchmark suite), Grace generally achieves good speed and scalability on multicore \nsystems while providing safety guarantees. The fact that Grace makes multithreaded program executions \ndeterminis\u00adtic and repeatable also has the potential to greatly simplify testing and debugging of concurrent \nprograms, even where deploying Grace might not be feasible. 10. Acknowledgements The authors would like \nto thank Ben Zorn for his feedback during the development of the ideas that led to Grace, to Luis Ceze \nfor graciously providing benchmarks, and to Cliff Click, Dave Dice, Sam Guyer, and Doug Lea for their \nin\u00advaluable comments on earlier drafts of this paper.We also thank Divya Krishnan for her assistance. \nThis material is based upon work supported by Intel, Microsoft Research, and the National ScienceFoundation \nunder CAREERAward CNS-0347339 and CNS-0615211. Any opinions, .ndings, and conclusions or recommendations \nexpressed in this ma\u00adterial are those of the author(s) and do not necessarily re.ect the viewsof the \nNational ScienceFoundation.  References [1] M. Abadi, A. Birrell,T. Harris, and M. Isard. Semantics \nof transactional memory and automatic mutual exclusion. In POPL 08: Proceedings of the 35th annualACM \nSIGPLAN-SIGACT symposium on Principles of programming languages, pages 63 74,NewYork,NY, USA, 2008.ACM. \n[2] D.F. Bacon and S. C. Goldstein. Hardware-assisted replay of multiprocessor programs. In PADD 91: \nProceedings of the 1991ACM/ONR workshop onParallel and distributed debugging,pages 194 206,NewYork,NY,USA, \n1991.ACM. [3]E.D.Berger,K.S. McKinley,R.D. Blumofe,andP.R.Wil\u00adson. Hoard:Ascalable memory allocator for \nmultithreaded applications. In Proceedings of the International Conference on Architectural Support for \nProgramming Languages and Operating Systems (ASPLOS-IX), pages 117 128,NewYork, NY, USA,Nov. 2000.ACM. \n[4] E. D. Berger, B. G. Zorn, and K. S. McKinley. Composing high-performance memory allocators. In Proceedings \nof the 2001ACM SIGPLAN Conference onProgramming Language Design and Implementation (PLDI 2001), pages \n114 124, NewYork,NY, USA, June 2001.ACM. [5] R.D. Blumofe,C.F.Joerg,B.C.Kuszmaul,C.E. Leiserson, K. H. \nRandall, andY. Zhou. Cilk: an ef.cient multithreaded runtime system. J.Parallel Distrib.Comput., 37(1):55 \n69, 1996. [6] C. Blundell, E. C. Lewis, and M. M. K. Martin. Deconstruct\u00ading transactions: The subtleties \nof atomicity. In WDDD 05: 4thWorkshop on Duplicating, Deconstructing, and Debunk\u00ading, June 2005. [7] \nJ.B. Carter,J.K. Bennett, andW.Zwaenepoel. Implementa\u00adtion and performance of Munin. In SOSP 91: Proceedings \nof the ThirteenthACM Symposium on Operating Systems Prin\u00adciples, pages 152 164,NewYork,NY, USA, 1991.ACM. \n[8] G.-I. Cheng, M. Feng, C. E. Leiserson, K. H. Randall, and A.F. Stark. Detecting data races in Cilk \nprograms that use locks. In SPAA 98: Proceedings of the tenth annualACM symposium onParallel algorithms \nand architectures, pages 298 309,NewYork,NY, USA, 1998.ACM. [9] J. Dean and S. Ghemawat. MapReduce: simpli.ed \ndata processing on large clusters. In OSDI 04: Proceedings of the 6th conference on Symposium on Opearting \nSystems Design &#38;Implementation, pages 10 10, Berkeley, CA, USA, 2004. USENIX Association. [10] J. \nDevietti, B. Lucia, L. Ceze, and M. Oskin. DMP: deterministic shared memory multiprocessing. In ASPLOS \n09: Proceedings of the 14th International Conference on Architectural Support for Programming Languages \nand Operating Systems, pages 85 96,NewYork,NY, USA, 2009. ACM. [11] D. Dice, O. Shalev, and N. Shavit. \nTransactional locking ii. In S. Dolev, editor, DISC, volume 4167 of LectureNotes in Computer Science, \npages 194 208. Springer, 2006. [12]C.Ding,X.Shen,K.Kelsey,C.Tice,R. Huang,andC. Zhang. Software behavior \noriented parallelization. In PLDI 07: Proceedings of the 2007 ACM SIGPLAN conference on Programming language \ndesign and implementation, pages 223 234,NewYork,NY, USA, 2007.ACM. [13] M. Feng and C. E. Leiserson. \nEf.cient detection of determinacyraces in cilk programs. In SPAA 97: Proceedings of the ninth annualACM \nsymposium onParallel algorithms and architectures, pages 1 11, NewYork, NY, USA, 1997. ACM. [14] C. Flanagan \nand S. Qadeer. A type and effect system for atomicity. In PLDI 03: Proceedings of theACM SIGPLAN 2003 \nconference on Programming language design and implementation, pages 338 349,NewYork,NY, USA, 2003. ACM. \n[15] T. Harris and K. Fraser. Language support for lightweight transactions. In OOPSLA 03: Proceedings \nof the 18th annual ACM SIGPLAN conference on Object-oriented programing, systems, languages, and applications, \npages 388 402, New York,NY, USA, 2003.ACM. [16]J.W.Havender.Avoiding deadlockin multitasking systems. \nIBM SystemsJournal, 7(2):74 84, 1968. [17] M. Herlihy and J. E. B. Moss. Transactional memory: architectural \nsupport for lock-free data structures. In ISCA 93: Proceedings of the 20th annual international symposium \non Computer architecture, pages 289 300, NewYork, NY, USA, 1993.ACM. [18] K. Huang. Data-race detection \nin transactions-everywhere parallel programming. Master s thesis, Department of Electrical Engineering \nand Computer Science, Massachusetts InstituteofTechnology, June 2003. [19] R. L. Hudson, B. Saha, A.-R. \nAdl-Tabatabai, and B. C. Hertzberg. Mcrt-malloc: a scalable transactional memory allocator. In ISMM 06: \nProceedings of the 5th International Symposium on Memory Management, pages 74 83, New York,NY, USA, 2006.ACM. \n[20] M. Isard and A. Birrell. Automatic mutual exclusion. In HotOS XI: 11th Workshop on Hot Topics in \nOperating Systems, Berkeley, CA, May 2007. [21] R. L. B. Jr., V. S. Adve, D. Dig, S. Adve, S. Heumann, \nR. Komuravelli, J. Overbey, P. Simmons, H. Sung, and M. Vakilian. A type and effect system for deterministic \nparallel Java. In OOPSLA 09: Proceedings of the 24th ACM SIGPLAN Conference on Object-oriented Programming \nSystems, Languages, and Applications,NewYork,NY, USA, 2009.ACM. [22]P.Keleher,A.L. Cox,S.Dwarkadas, andW.Zwaenepoel. \nTreadmarks: Distributed shared memory on standard work\u00adstations and operating systems. In WTEC 94: Proceedings \nof the USENIXWinter 1994Technical Conference, pages 10 10, Berkeley, CA, USA, 1994. USENIX Association. \n[23] J. R. Larus and R. Rajwar. Transactional Memory. Morgan&#38; Claypool, 2006.  [24] D. Lea.AJavafork/join \nframework.In JAVA 00: Proceedings of theACM 2000 conference onJava Grande, pages 36 43, NewYork,NY, USA, \n2000.ACM. [25]S.Lu,S.Park,C.Hu,X.Ma,W. Jiang,Z.Li,R.A.Popa,and Y. Zhou. MUVI: automatically inferring \nmulti-variable access correlations and detecting related semantic and concurrency bugs. In SOSP 07: Proceedings \nof theTwenty-FirstACM SIGOPS Symposium on Operating Systems Principles, pages 103 116,NewYork,NY, USA, \n2007.ACM. [26] S. Lu, S. Park, E. Seo, and Y. Zhou. Learning from mistakes: a comprehensive study on \nreal world concurrency bug characteristics. In ASPLOS XIII: Proceedings of the 13th international conference \non Architectural support for programming languages and operating systems, pages 329 339,NewYork,NY, USA, \n2008.ACM. [27] B. Lucia, J. Devietti, K. Strauss, and L. Ceze. Atom-Aid: Detecting and surviving atomicity \nviolations. In ISCA 08: Proceedings of the 35th Annual International Symposium on Computer Architecture, \nNew York, NY, USA, June 2008. ACM Press. [28] C.-K.Luk,R.Cohn,R.Muth,H.Patil,A. Klauser,G.Lowney, S.Wallace,V. \nJ. Reddi, and K. Hazelwood. Pin:building customized program analysis tools with dynamic instrumen\u00adtation. \nIn PLDI 05:Proceedingsof the 2005ACM SIGPLAN conference on Programming language design and implemen\u00adtation, \npages 190 200,NewYork,NY, USA, 2005.ACM. [29]C.E.McDowellandD.P. Helmbold. Debugging concurrent programs. \nACM Comput. Surv., 21(4):593 622, 1989. [30]R.H.B. NetzerandB.P. Miller. What are race conditions?: Some \nissues and formalizations. ACM Lett. Program. Lang. Syst., 1(1):74 88, 1992. [31]Y.Ni,A.Welc, A.-R. Adl-Tabatabai,M. \nBach,S. Berkow\u00adits, J. Cownie, R. Geva, S.Kozhukow, R. Narayanaswamy, J. Olivier, S. Preis, B. Saha, \nA. Tal, and X. Tian. Design and implementation of transactional constructs for C/C++. In OOPSLA 08: Proceedings \nof the 23rdACM SIGPLAN Conference on Object-oriented Programming Systems, Lan\u00adguages, and Applications, \npages 195 212, NewYork, NY, USA, 2008.ACM. [32] M. Olszewski, J. Ansel, and S. Amarasinghe. Kendo: ef.cient \ndeterministic multithreading in software. In ASPLOS 09: Proceedings of the 14th International Conference \non Architectural Support for Programming Languages and Operating Systems, pages 97 108, New York, NY, \nUSA, 2009.ACM. [33] S. Rajamani, G. Ramalingam, V. P. Ranganath, and K. Vaswani. ISOLATOR: dynamically \nensuring isolation in comcurrent programs. In ASPLOS 09: Proceeding of the 14th International Conference \non Architectural Support for Programming Languages and Operating Systems, pages 181 192,NewYork,NY, USA, \n2009.ACM. [34] C. Ranger, R. Raghuraman, A. Penmetsa, G. Bradski, and C. Kozyrakis. Evaluating MapReduce \nfor multi-core and multiprocessor systems. In Proceedings of the 13th Intl. Symposium on High-Performance \nComputer Architecture (HPCA), feb 2007. [35] J. Reinders. Intel Threading Building Blocks: Out.tting \nC++ for Multi-core ProcessorParallelism. O Reilly Media, Inc., 2007. [36]N.ShavitandD.Touitou. Softwaretransactional \nmemory. In PODC 95: Proceedings of the fourteenth annualACM symposium on Principles of distributed computing, \npages 204 213,NewYork,NY, USA, 1995.ACM. [37]T. Shpeisman,V. Menon, A.-R. Adl-Tabatabai,S. Balensiefer, \nD. Grossman, R. L. Hudson, K. F. Moore, and B. Saha. Enforcing isolation and ordering in STM. In PLDI \n07: Proceedings of the 2007 ACM SIGPLAN conference on Programming language design and implementation, \npages 78 88,NewYork,NY, USA, 2007.ACM. [38] C. von Praun, L. Ceze, and C. Cas\u00b8caval. Implicit parallelism \nwith ordered transactions. In PPoPP 07: Proceedings of the 12thACM SIGPLAN Symposium on Principles and \nPractice ofParallel Programming, pages 79 89,NewYork,NY, USA, 2007.ACM. [39] A.Welc, S. Jagannathan, \nand A. Hosking. Safe futures for Java. In OOPSLA 05:Proceedingsofthe 20th annualACM SIGPLAN Conference \non Object oriented Programming, Systems, Languages, and applications, pages 439 453, New York,NY, USA, \n2005.ACM. [40] A. Welc, B. Saha, and A.-R. Adl-Tabatabai. Irrevocable transactions and their applications. \nIn SPAA 08: Proceedings of the Twentieth Annual Symposium on Parallelism in Algorithms and Architectures, \npages 285 296, New York, NY, USA, 2008.ACM.     \n\t\t\t", "proc_id": "1640089", "abstract": "<p>The shift from single to multiple core architectures means that programmers must write concurrent, multithreaded programs in order to increase application performance. Unfortunately, multithreaded applications are susceptible to numerous errors, including deadlocks, race conditions, atomicity violations, and order violations. These errors are notoriously difficult for programmers to debug.</p> <p>This paper presents Grace, a software-only runtime system that eliminates concurrency errors for a class of multithreaded programs: those based on fork-join parallelism. By turning threads into processes, leveraging virtual memory protection, and imposing a sequential commit protocol, Grace provides programmers with the appearance of deterministic, sequential execution, while taking advantage of available processing cores to run code concurrently and efficiently. Experimental results demonstrate Grace's effectiveness: with modest code changes across a suite of computationally-intensive benchmarks (1-16 lines), Grace can achieve high scalability and performance while preventing concurrency errors.</p>", "authors": [{"name": "Emery D. Berger", "author_profile_id": "81100228645", "affiliation": "University of Massachusetts, Amherst, Amherst, MA, USA", "person_id": "P1728723", "email_address": "", "orcid_id": ""}, {"name": "Ting Yang", "author_profile_id": "81406592384", "affiliation": "University of Massachusetts, Amherst, Amherst, MA, USA", "person_id": "P1728724", "email_address": "", "orcid_id": ""}, {"name": "Tongping Liu", "author_profile_id": "81444607968", "affiliation": "University of Massachusetts, Amherst, Amherst, MA, USA", "person_id": "P1728725", "email_address": "", "orcid_id": ""}, {"name": "Gene Novark", "author_profile_id": "81350568767", "affiliation": "University of Massachusetts, Amherst, Amherst, MA, USA", "person_id": "P1728726", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1640089.1640096", "year": "2009", "article_id": "1640096", "conference": "OOPSLA", "title": "Grace: safe multithreaded programming for C/C++", "url": "http://dl.acm.org/citation.cfm?id=1640096"}