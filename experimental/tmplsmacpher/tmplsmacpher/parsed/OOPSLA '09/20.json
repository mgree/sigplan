{"article_publication_date": "10-25-2009", "fulltext": "\n NUMA-Aware Memory Manager with Dominant-Thread-Based Copying GC Takeshi Ogasawara IBM Research Tokyo \nYamato, Kanagawa 242-8502, Japan takeshi@jp.ibm.com Abstract We propose a novel online method of identifying \nthe pre\u00adferred NUMA nodes for objects with negligible overhead during the garbage collection time as \nwell as object allo\u00adcation time. Since the number of CPUs (or NUMA nodes) is increasing recently, it \nis critical for the memory manager of the runtime environment of an object-oriented language to exploit \nthe low latency of local memory for high perfor\u00admance. To locate the CPU of a thread that frequently \nac\u00adcesses an object, prior research uses the runtime information about memory accesses as sampled by \nthe hardware. How\u00adever, the overhead of this approach is high for a garbage col\u00adlector. Our approach \nuses the information about which thread can exclusively access an object, or the Dominant Thread (DoT). \nThe dominant thread of an object is the thread that often most accesses an object so that we do not require \nmemory access samples. Our NUMA-aware GC performs DoT based object copying, which copies each live object \nto the CPU where the dominant thread was last dispatched before GC. The dominant thread information is \nknown from the thread stack and from objects that are locked or reserved by threads and is propagated \nin the object reference graph. We demonstrate that our approach can improve the per\u00adformance of benchmark \nprograms such as SPECpower ssj2008, SPECjbb2005, and SPECjvm2008. We prototyped a NUMA\u00adaware memory manager \non a modi.ed version of IBM Java VM and tested it on a cc-NUMA POWER6 machine with eight NUMA nodes. \nOur NUMA-aware GC achieved per\u00adformance improvements up to 14.3% and 2.0% on average over a JVM that \nonly used the NUMA-aware allocator. The Permission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior \nspeci.c permission and/or a fee. OOPSLA 2009, October 25 29, 2009, Orlando, Florida, USA. Copyright c \n&#38;#169; 2009 ACM 978-1-60558-734-9/09/10. . . $10.00 total improvement using both the NUMA-aware \nallocator and GC is up to 53.1% and 10.8% on average. Categories and Subject Descriptors D.3.4 [Processors]: \nMemory management (garbage collection) General Terms Algorithms, Measurement, Performance Keywords cc-NUMA, \nJava 1. Introduction Modern servers exploit cache-coherent non-uniform mem\u00adory access (cc-NUMA) architectures. \nFor example, widely used processors such as Opteron [1], POWER6 [19], Ne\u00adhalem [17], and T2plus [33] \ncan be con.gured to use cc-NUMA. A cc-NUMA server has multiple CPUs. Each CPU can ef.ciently access the \ndata in local memory, which is at\u00adtached to that CPU, with minimal latency. In contrast, the la\u00adtency \nfor remote memory, which is attached to another CPU, is at least twice as long. A NUMA-aware memory manager \nis critical for the best performance of multi-threaded object-oriented programs running on a cc-NUMA \nserver. Prior research on Java vir\u00adtual machines [35, 9] showed the high promise of mem\u00adory location \noptimization for objects. Programs written in object-oriented languages usually allocate large numbers \nof objects. If the memory manager does not consider the CPUs where the objects are placed, threads can \nsuffer from the large latency of the remote memory because the objects may be badly distributed among \nthe CPUs. A problem in prior research is the cost of calculations to .nd the memory locations that are \npreferred for the live objects at garbage collection time. For object-oriented lan\u00adguages, the memory \nlocation optimization should be per\u00adformed at the object level rather than at the page level [35]. To \nperform the memory location optimization at runtime, prior research collects samples of memory accesses \nand summarizes them to calculate the preferred location for each object, or the CPU that most accesses \nthe object, for each op\u00adtimized object [35]. There are two problems that can degrade the performance \nof programs with this approach. The .rst problem is the cost of post-processing the event traces. This \napproach combines two separate traces to .nd the preferred locations: memory access samples and allocated \nobjects. It is dif.cult for the event handler invoked by each memory\u00adaccess event to locate the object \ncorresponding to the current memory access and to update the number of events directly inside the object \n(e.g., in the object header). Memory ac\u00adcesses do not always occur at object boundaries. It is also hard \nto locate the object that includes the data address that caused the current event. Therefore the samples \nare usually maintained outside the heap, typically as sequential traces. In the post-processing, the \ngarbage collector, which knows the object boundaries, must traverse this sequential trace data to map \nthe memory accesses to the objects. The second problem is the cost of looking up the optimal location \nof each object that is moved by garbage collection. Since the garbage collector moves a large number \nof live objects, the extra cost when moving each object signi.cantly increases the total GC overhead. \nWhen the garbage collector moves an object to its preferred location, it has to .nd the preferred location \nin another data structure such as a hash map that keeps the pair of each object and its preferred location. \nThe GC time is signi.cantly increased with this approach [35]. It is a challenge for a NUMA-aware memory \nmanager of an object-oriented language to ef.ciently determine the pre\u00adferred locations without any additional \ninformation, with\u00adout degrading the performance of programs. Another ap\u00adproach uses the connectivity \nbetween objects in the reference graph to optimize the object layouts within the cache lines [14]. However, \nsuch snapshot information does not show the strength of the connectivity between objects and threads. \nFor instance, an object can be pointed at by multiple threads. If a garbage collector .nds that a thread \nthat rarely accesses the object is reaching an object, it may wrongly move that ob\u00adject to a remote memory \nrelative to a thread that frequently accesses the object. We propose a novel approach for a NUMA-aware \ngarbage collector. Our approach uses the dominant-thread (DoT)in\u00adformation of objects as to heuristically \ndecide on the pre\u00adferred memory locations for objects. A modern Java virtual machine saves the dominant-thread \ninformation in the object header, such as the thread id that acquired the lock of the ob\u00adject [2] or \nthat reserved the object [18]. If objects have no dominant-thread information, we use the dominant-thread \ninformation of objects that directly or indirectly reference the objects. The cost of calculating the \npreferred memory location for each object by using this dominant-thread infor\u00admation is very small. Therefore, \nour garbage collector can perform the memory location optimization with negligible overhead. In summary, \nthe contributions of our paper are: A technique for a NUMA-aware garbage collector to ef\u00ad.ciently .nd \na CPU for each object where the dominant thread for that object is running. Our garbage collector collects \nsuch a dominant-thread information and propa\u00adgates it in the object reference graph. Our approach can \n.nd a dominant thread even for an object that is refer\u00adenced by multiple threads as well as for a thread-local \nobject. A performance evaluation of our approach by using stan\u00addard benchmark programs such as SPECpower \nssj2008, SPECjbb2005, and SPECjvm2008. Our prototype was implemented on a modi.ed version of the IBM \nJava VM. Our NUMA-aware garbage collector achieved additional performance improvements up to 14.3% and \n2.0% on av\u00aderage over a JVM that only has a NUMA-aware allocator. The rest of the paper is organized \nas follows. Section 2 discusses related work. Section 3 explains the memory management of a NUMA-unaware \nJava VM. Section 4 dis\u00adcusses the problem of remote memory accesses on a NUMA\u00adunaware Java VM. Section \n5 discusses the NUMA-aware memory management in a Java VM. Section 6 presents ex\u00adperimental results. \nSection 7 concludes and summarizes this paper. 2. Related Work Tikir et al. showed that the memory location \noptimization should be .ne-grained for each object in a JVM [35]. They showed that a page-level optimization, \nwhich is often used in non-object-oriented programming environments such as OpenMP [36, 23, 34], cannot \nimprove the performance of a Java application. They move objects to the CPUs that most access the objects \nby using a modi.ed garbage collector. To recognize these CPUs, they collect samples of the memory accesses \nby using the hardware during the program execu\u00adtion. When a garbage collection occurs, they summarizes \nthe samples to .nd the CPU that most accessed the object. Their approach and ours allocate objects in \nthe local memory and trigger garbage collection when there is no free memory for one of the CPUs. Also, \nboth approaches use a garbage collector for the memory location optimization for the live objects. In \ncontrast to their approach, we do not rely on memory access samples, since they can increase the over\u00adhead \nof garbage collection. In addition, their survivor space is not NUMA-aware while ours is NUMA-aware. \nA NUMA\u00adaware survivor space is important since we found the perfor\u00admance improvement in some benchmarks \nin which several GCs moved objects to the survivor space. Thread-local objects have been extensively \nstudied in prior research on JVMs. Thread-local objects are objects that are pointed at by only a single \nthread. A thread and its thread-local objects should be placed on the same CPU of a cc-NUMA server. Steensgarrd \nproposed thread-local heaps based on thread escape analysis [29]. Thread escape analy\u00adsis conservatively \nrecognizes thread-local objects [20, 21]. Thread-local objects are placed in thread-local heaps. Do\u00admani \net al. proposed thread-local heaps with local garbage collection [11]. An object is initially allocated \nas thread\u00adlocal and becomes global when a non-local object points at the thread-local object. Cohen et \nal. extended these ap\u00adproaches to reduce the memory footprint by clustering the heaps [6]. Our approach \ncurrently does not but could use the thread locality of objects for determining the preferred CPUs for \nthe objects. However, our experiments showed that the memory locations of the objects that can be reached \nfrom multiple threads should be optimized. Our approach can handle such optimizations. Optimization \nof the data layout of objects has been stud\u00adied to optimize the data locality within cache lines [14]. \nTheir approaches and ours can coexist and simultaneously improve the performance of the memory accesses. \nOperating systems provide several memory af.nity poli\u00adcies such as striped and local for memory allocation \n[5, 7, 31, 4]. The local policy reserves memory in the local mem\u00adory for a thread that is requesting \nmemory when the thread .rst accesses the memory. Then migrating the memory pages to the CPU of the thread \nthat next touches the memory is useful for some programs [34]. With our NUMA-aware al\u00adlocator, threads \n.rst allocate objects on the local CPU for themselves. Then our NUMA-aware garbage collector may migrate \nthe objects to preferred CPUs. The schedulers of operating systems that support cc-NUMA machines will \nrepeatedly dispatch a thread on the same CPU [8, 31, 4]. Our approach assumes that our JVM runs with \nsuch a NUMA-af.nity scheduler.  3. Memory Management of NUMA-Unaware JVM This section explains the memory \nmanagement of a JVM that does not perform memory location optimization for NUMA. We consider a generational \nheap, which is widely used on JVM implementations. This section also explains how the pages of the heap \nare randomly distributed among theCPUsonsuchaJVM. 3.1 Generational Heap The pause time during which \na GC stops applications (GC pause time) is a major response-time criterion for many ap\u00adplications written \nin object-oriented languages. In particular, for server applications that require a large heap to handle \na large number of clients, the GC pause time should be mini\u00admized to improve the QoS (quality-of-service). \nGenerational GC has been proposed [22, 37] and is widely used on many production Java virtual machines \n[3, 32, 16]. Applications written in object-oriented lan\u00adguages create a large number of objects as data \nstructures. However, most of the created objects die quickly (are short\u00adlived) in many applications [37, \n28]. To exploit the characteristics of short-lived objects for ef\u00ad.cient GC, the heap is split into two \nspaces on a generational GC, a young generation and an old generation,asshown in Figure 1. Each thread \nallocates new objects in the young H Old Generation Figure 1. Generational Heap generation. The GC \nalso tries to insure the young genera\u00adtion only has short-lived objects by moving any long-lived objects \nto the old generation (promotion). The young gener\u00adation is itself split into two spaces: an allocation \nspace and a survivor space. The allocation space is used by application threads to reserve memory for \nnewly created objects. The survivor space is used by GC threads as storage in which the GC threads save \nthe live but short-lived objects. Our Java VM .ips the allocation space and the survivor space after \nevery GC [15], as explained in a later section. Also, our Java VM maximizes the size of the allocation \nspace for the young generation to reduce the frequency of minor GCs by using a technique called tilting. \nInitially, the allocation space and the survivor space each consume 50% of the young generation. After \neach .ip, the allocation space can be adjusted (with up to 90% in the young generation) until the size \nof the survivor space is suf.cient enough for the live short-lived objects. 3.2 Object Allocation Threads \nreserve memory for objects from the allocation space. Threads can reserve two types of memory: allocation \nbuffers and objects too large to .t in the allocation buffers. Modern Java VMs use the allocation buffers \nto improve the performance of object allocation [10, 30]. Each thread has its own allocation buffer and \nallocates most of its objects from that buffer. The performance of object allocation with the buffer \ncan scale, since all of the threads can use their buffers in parallel without mutual exclusion. When \nthere is no free space in the buffer when allocating an object, a thread reserves another buffer from \nthe allocation space. Threads allocate memory for large objects directly from the allocation space. The \nallocation space is consumed linearly from beginning to end in the address order without reclaiming any \nspace until the entire space is consumed. The free space in the allocation space will be a single large \nblock after GC. When a thread reserves some new memory from the allocation space, it seeks the global \nlock for mutual exclusion and obtains a block that can satisfy the current memory request by adjusting \na pointer that maintains the top of the free space [27]. 3.3 Garbage Collection Two types of GCs occur \nin a generational heap: minor and major. The minor GC is limited to the young generation and the major \nGC is used for the entire heap. Most of GCs are minor since most of objects die quickly as large numbers \nof short-lived objects are allocated. The overhead of the minor GC is much lower than that of the major \nGC since the major GC has to scan more live objects than the minor GC. A minor GC occurs when there \nis no free space and an attempt is made to reserve memory from the allocation space. A minor GC .nds \nthe live objects in the allocation space and moves them to the survivor space. When all of the live objects \nhave been evacuated from the allocation space, the allocation space is empty and the survivor space contains \nthe live objects. For the objects that survived minor GCs multiple times, a minor GC considers these \nobjects as old and moves them to the old generation space instead of the survivor space. Our Java VM \n.ips the allocation space and the survivor space in each minor GC [15]. A major GC occurs when there \nis no free space in the old space. A major GC scans for the live objects in the old generation. Our JVM \nperforms this marking phase while the application threads continue running to reduce the pause time of \nthe major GC [25]. The sweeping of the dead objects is performed by multiple GC threads in parallel but \nthe application threads must pause.  3.4 Page Mapping Each page in the virtual address space of the \nJava heap is mapped to a physical page of the memory attached to a CPU. If a JVM does not explicitly \ncontrol the mapping between pages and CPUs, the memory locations of the pages depend on the memory allocation \npolicy of the operating system and the characteristics of the memory management of the JVM. In the following \nsections, we .rst explain what services operating systems provide so that programs can use the local \nmemory on NUMA servers and then discuss the page mapping of the Java heap. 3.4.1 Default NUMA Af.nity \nfrom Operating Systems For programs that do not explicitly control the NUMA af.n\u00adity, operating systems \nsupport standard memory af.nity poli\u00adcies, which decide on the CPU when reserving memory in the system: \nlocal-CPU or all-CPU [7, 31, 4]. The selection of a default policy depending on the characteristics of \nthe memory usage can allow programs to tune the NUMA af.n\u00adity without introducing any additional complexity \nfor man\u00adaging the memory locations of the data. With the local-CPU policy, when a thread .rst touches \na page, the memory for that page is reserved from the CPU where the thread is running. This policy is \nuseful for appli\u00adcations with certain characteristics and many programs can bene.t from this policy. \nFirst, both the thread that reserves some memory and the thread that uses it should continue to run on \nthe same CPU. Typically, these threads are the same thread. Second, the amount of memory reserved should \nbe smaller than the memory attached to the CPU. With the all-CPU policy, the memory is reserved from \nthe CPUs in some manner, such as randomly or cyclically. This kind of policy is useful for the applications \nthat can\u00adnot bene.t from the local-CPU policy. For example, a con\u00adtroller thread may reserve memory and \ninitialize the data as the worker threads work in parallel on separate CPUs sharing the data. In such \na scenario, the local-CPU policy would cause all of the data to be reserved on the CPU of the controller \nthat initializes the data, and only the worker threads running on that same CPU would be able to access \nthe data locally. The all-CPU policy can address this imbal\u00adance among the threads by obtaining the memory \nfrom each of the CPUs for the data. In addition to these memory-location policies, an operat\u00ading system \nwill have a NUMA-af.nity scheduler, which tries to dispatch a thread to the CPU where the thread was \nlast dis\u00adpatched. [8, 31, 4]. The NUMA-af.nity scheduler will sup\u00adport the default memory allocation \npolicies, in particular, the local-CPU policy so that the local memory that is reserved for a thread \nwill continue to be local to the thread. 3.4.2 Page Mapping of NUMA-Unaware Java Heap The pages of the \nJava heap tend to be randomly distributed among the CPUs, not only with the all-CPU policy, but even \nwith the local-CPU policy on a NUMA-unaware JVM, since there is no memory-location optimization for the \nobjects. The all-CPU policy is intended to distribute the pages among the CPUs, but with the local-CPU \npolicy, the page mapping depends on how each page is .rst touched by a thread1. With the local-CPU policy, \neach page of the Java heap is mapped to a CPU when some data is .rst written to that page by a thread. \nSuch data includes newly created objects in the allocation space and surviving objects in the survivor \nspace or the old generation space. A thread that .rst writes the data to a page is an application thread \nfor the allocation space and a GC thread for the survivor space or the old generation space. The thread \nthat .rst writes the data to a page raises a page fault. Then the operating system maps the page to the \nmemory of the CPU where the thread is running. The destination CPUs for the mapped pages of the heap \ntend to randomly distributed across the CPUs because of certain characteristics of object allocation. \nFor example, the time intervals between the requests to allocate memory for buffers and large objects \nare in general constantly .uctuating on each thread, since the rate of object allocation is not constant \nfor all of the phases of execution. Also, the memory requests that are performed in parallel on separate \nCPUs are serialized because of mutual exclusion. The page mapping of the Java heap is determined once \nthe pages have been mapped to the CPUs. If a thread reserves memory from a page that was already mapped \nto a CPU, then the thread cannot change the page mapping even if it is running on a remote CPU relative \nto that page. This sit\u00aduation occurs frequently, since multiple threads can reserve 1 We assume that \na JVM does not touch the entire heap during initialization to avoid allocating all of the heap memory \nfrom a speci.c CPU.  Buf0 Buf1 Buf2 Buf3 Buf4 Buf5 Buf6 Buf7 Allocation Space Page Page Boundary Boundary \nCPU Memory Figure 2. Object Allocation on Non-NUMA-Aware JVM memory from the same page, especially if \nlarge pages such as 16-MB pages are used in the system.   4. Remote Memory Accesses to Objects This \nsection discusses the problems of remote memory ac\u00adcesses to Java objects. We explain why objects tend \nto be placed in remote memory unless memory allocator and GC are NUMA-aware. We .rst discuss the problems \nfor newly created objects and then the problems for surviving objects. 4.1 Object Allocation in Remote \nMemory The memory location where an object is allocated is criti\u00adcal to the performance of applications. \nAs explained in Sec\u00adtion 3.2, the allocation space is consumed linearly from end to end in address order \nwithout reclaiming any space un\u00adtil the entire space is consumed. Therefore, newly created objects tend \nto be fetched from the physical memory rather than from the data cache. If a thread allocates an object \nin a remote memory, it will suffer from the long latency of re\u00admote memory access whenever it accesses \nthe object. In spite of this problem, threads tend to allocate objects in remote memory on a NUMA-unaware \nJava VM. We can explain the reason as follows. This is because successive requests tend to be issued \nby threads running in parallel on separate CPUs. These requests are satis.ed with neighboring blocks \nin the allocation space, since the allocation space is used linearly in the address order. These neighboring \nblocks are included in various memory pages, especially if the page size is large (such as 16 MB). Since \nthe large page is mapped to only one CPU, all of these threads obtain their memory blocks from the same \nCPU. however, though the memory blocks exist on the same CPU, the threads that use the blocks are running \non separate CPUs. Therefore, most of the threads wind up obtaining the blocks from remote CPUs. Figure \n2 illustrates an example. Eight threads running on separate CPUs obtain allocation buffers from the same \nCPU. Allocation buffers (Buf0, ..., Buf7 in the .gure) are sequentially reserved from a page of the allocation \nspace that is mapped to the memory of CPU0. Only the thread running on CPU0 (12.5% of the total threads) \ncan obtain the buffer from the local memory. This percentage decreases as the server has more CPUs. We \ncannot address this problem of object allocation in re\u00admote memory by using any memory af.nity policy \nprovided by an operating system. No memory af.nity policy can con\u00adtrol the CPU mapping of the data within \na memory page. The Old Generation Young Generation Survivor Space Allocation Space  Figure 3. NUMA-aware \nheap layout memory af.nity policy only .nds the CPU that provides the memory in units of memory pages. \n 4.2 Surviving Objects in Remote Memory The source and destination CPUs for surviving objects can be \nremote relative to the GC threads. When a GC thread ob\u00adtains a set of objects to be scanned, these objects \ncan be in a remote memory relative to the GC thread. In addition, the destinations of the moved objects \nmay also on the remote memory. Since GC is memory-intensive work, remote mem\u00adory accesses when moving \nobjects signi.cantly affect the performance of GC. User threads can access surviving objects in remote \nmem\u00adory. For long-lived objects, the memory locations of these objects do not change within the old generation \nspace un\u00adtil the next major GC. Therefore, if these objects are moved from remote CPUs to a user thread, \nthe user thread always suffers from the cost of remote memory access to the objects. In contrast, the \nmemory locations of short-lived objects can be randomly changed within the survivor area during every \nminor GC. Therefore, the remote access penalty is more crit\u00adical for long-lived objects.  5. NUMA-Aware \nJVM This section .rst explains the layout of the Java heap that is used by our NUMA-aware JVM. Then this \nsection explains the NUMA-ware allocator and the NUMA-aware GC. 5.1 NUMA-Aware Heap Layout Figure 3 shows \nthe layout of our NUMA-aware heap. Our heap is based on the generational heap explained in Section 3.1. \nEach generation is split into segments. The size of each segment is constant and is .xed in each generation \nat the startup of the JVM. Each generation can have its own seg\u00adment size. For example, the segment size \nfor the old gener\u00adation can be 1 GB while that for the young generation is 16 MB. Each segment is mapped \nto the memory on a speci.ed CPU. Since an operating system performs the CPU mapping of the memory in \nunits of pages, each segment consists of a page or of multiple pages. The segment-to-CPU mapping is performed \nby using the NUMA API provided by the operating system. A group of N contiguous segments is a segment \ngroup, where N is the number of CPUs. The CPU mapping of a segment group is divided among the CPUs: the \ni-th segment is mapped to the i-th CPU. The old generation consists of one segment group. The young generation, \nin contrast, consists of multiple segment groups. Figure 3 shows that the young generation consists of \n10 segment groups (the shaded area shows one segment group). In this example, the survivor space has \nbeen reduced to the minimum size, or 10% of the young genera\u00adtion. The allocation space consists of nine \nsegment groups while the survivor space consists of one segment group. We allocate at least one segment \ngroup for the minimized sur\u00advivor space. If the minimized survivor space includes a seg\u00adment group, then \nthe GC can move the live objects to their preferred CPUs in the survivor space. Therefore, we need ten \nsegment groups for the young generation in this exam\u00adple. Otherwise, the GC cannot .nd the local CPU \nin the sur\u00advivor space. For example, suppose that the young genera\u00adtion was initialized with two segment \ngroups (one for the young space and the other for the survivor space). Each seg\u00adment group consists of \neight segments. In this case, the size of each segment (12.5% of the young generation) is larger than \nthe minimized survivor space (10% of the young gen\u00aderation).  5.2 NUMA-Aware Allocator The NUMA-aware \nallocator obtains a requested block of memory from a speci.ed CPU. Such a CPU is called a preferred CPU. \nThe allocator is called by user threads for object allocation, but at the same time it is also used by \nGC threads. The user threads use the allocator to allocate memory for allocation buffers and large objects \nfrom the allocation space. The GC threads use the allocator to reserve memory for the surviving objects \nfrom either the survivor space or the old generation. How a GC thread determines the preferred CPU is \nexplained in the next section. A user thread sets the preferred CPU to the current CPU where the thread \nis running, when it calls the allocator. This policy was studied in prior research and works effectively \nfor short-lived objects [35, 9]. In Java, a thread that allocates an object is the .rst thread that accesses \nthe object. An allocating thread zeros out and initializes the newly created object immediately after \nreserving the memory for the new object. Also, only the allocating thread accesses the new object for \nthe majority of new objects. Many objects are used as working data and die quickly without being transferred \nto another thread. In addition, a NUMA-af.nity scheduler in an operating system dispatches threads to \nthe CPUs where they were last dispatched. The current CPU id can be obtained Segment groupi Free spacej \n  CPU0 CPU1 CPU2 CPU3 (a) Before Allocation Segment groupi Free spacej Free spacej   CPU0 CPU1 CPU2 \nCPU3 (b) After Allocation Figure 4. Reserve a memory block from a free space through a system call or \non some systems by directly reading a special register that shows the CPU id. The allocator .rst checks \nif there is a free space in the heap that includes a segment that is mapped to the preferred CPU. If \nthe allocator .nds such a free space, it reserves memory from the segment within the free space and recycles \nthe remaining portions of the free space. Figure 4 illustrates how the allocator reserves memory from \nthe free space. The .rst and second .gures (Figures 4 (a) and (b)) show the heap status before and after \nreserving the memory block, respec\u00adtively. The .rst small portion of the .rst segment and all of the \nfourth segment are already in use for this segment group. In this example, memory owned by the preferred \nCPU, CPU1, is requested. The allocator .nds a free space, Freej , that includes a segment that is mapped \nto the pre\u00adferred CPU within a segment group, Segment groupi (Fig\u00adure 4 (a)). The free space extends \nfrom the middle of the .rst segment to the end of the third segment of Segment groupi. The allocator \nreserves memory from the second segment (the shaded box in Figure 4 (b)) and recycles the remaining por\u00adtions, \nF ree spacejand F ree spacejjj . j Our current design maintains the free spaces in an address\u00adordered \nfree list in each generation. The allocator searches the free list in the address order. There is a single \nlarge free space in the list after every minor GC. The length of the free list remains short as long \nas the user or GC threads consume the memory at a similar rate on each CPU. In that case, since the heap \nwill be consumed linearly in units of segment groups, only one or a few segment groups will have fragmented \nfree spaces as shown in Figure 4. The reserved memory can extend from the preferred-CPU segment to the \nnext segment in our current design. When the memory is reserved past the end of a segment, the next segment \nwill be used if the .rst segment cannot satisfy the memory request. In particular, we need to use multiple \n  Figure 5. An object reference graph segments to reserve memory for arrays that are larger than one \nsegment.  5.3 NUMA-Aware Garbage Collector The NUMA-aware GC moves the live objects to the mem\u00adory attached \nto their preferred CPUs. The NUMA-aware GC uses the NUMA-aware allocator to reserve memory that will \nbe the destination of live objects. Similar to the NUMA\u00adunaware GC, a GC thread reserves memory for a \nnew sur\u00advivor buffer or for a large live object. The GC thread uses a survivor buffer for small live \nobjects. A GC thread can have multiple survivor buffers each of which is reserved on a separate CPU. \nWhen a GC thread moves a live object to a preferred CPU, it uses a survivor buffer that is owned by the \npreferred CPU. A GC thread .nds the preferred CPU for each live object based on the dominant-thread (DoT) \ninformation. The domi\u00adnant thread is identi.ed in the reference graph in three ways: from the thread \nstacks, from the owned or reserved objects, and from the parent objects. First, a GC thread sets the \npre\u00adferred CPU for an object that is directly pointed at from the stack of a user thread to the CPU where \nthe user thread was last dispatched. The dominant thread for the object can be the user thread whose \nstack points directly at the object. We assume that such objects are accessed mostly by that thread. \nSimilar to the NUMA-unaware GC, the GC starts from the thread stacks when traversing an object reference \ngraph. Fig\u00adure 5 shows an example of the object reference graph. There are three threads, T1, T2, and \nT3. There are live objects that are reached from the thread stacks on the graph. A GC thread obtains \na thread from the current thread list and scans the ob\u00adject pointers on the stack of that thread. On \nmany operating systems, it is dif.cult for a GC thread to obtain the CPU id where any given thread was \nlast dis\u00adpatched. In our current design, a user thread periodically ob\u00adtains the current CPU id by calling \na system call and updates its thread CPU id with the sampled CPU id. The GC thread uses this sampled \nCPU id as the CPU where the user thread was last dispatched. Such a sampled CPU is probably correct since \nthe NUMA-af.nity scheduler tries to dispatch a thread to the CPU where the thread was last dispatched. \nFigure 6. Object reference graph and thread CPU ids Second, a GC thread uses two types of the object \nheader information for the dominant-thread information: locking threads and reserving threads. When an \nobject is locked by a user thread, many JVMs store the information that identi.es that thread in the \nobject s header [2]. An object is usually accessed only by the thread that has locked the object. Therefore, \nwe set the preferred CPU to the CPU where the locking thread was last dispatched. In addition, on modern \nJVMs, an object that was frequently locked by a speci.c thread is reserved by the thread [18, 24, 26]. \nSuch object reservation leaves the thread id in the object header even when the lock has been released. \nSimilar to a locked object, a reserved object is also accessed only by the reserving thread. Therefore, \nwe can set the preferred CPU to the CPU where the reserving thread was last dispatched. Third, a GC thread \ncan propagate the preferred CPU through the reference graph. When a GC thread .nishes moving the current \nobject, it seeks one of the objects that are pointed at by the current object as its next target. The \nGC thread sets the default preferred CPU for the next object to that of the current object. Then the \nGC thread checks if any dominant thread information is available in the object header of the next target \nobject. If available, the GC thread sets the preferred CPU of the next target to the thread CPU id of \nthe dominant thread. Otherwise, a GC thread uses the default preferred CPU. This example shows how the \npreferred CPUs are deter\u00admined while traversing the reference graph. Figure 6 shows an object reference \ngraph when GC starts. There are three threads: T1, T2, and T3. Each thread has a thread CPU id. T1, T2, \nand T3 were last dispatched to CPU0, CPU1, and CPU2, respectively. Two objects have the dominant thread \ninformation, T1 and T2. The objects labeled by T1 and its child can be reached from T2 and T3 as well \nas T1. Also, the object labeled by T2 and its child can be reached from T3 as well as T2. Figure 7 illustrates \nhow the preferred CPUs are determined for this example graph. A triangle shows that the preferred CPU \nof an object is initialized based on the thread stack. A diamond shows that the preferred CPU of an object \nwas overridden based on the locking or reserving thread in\u00adformation. A dashed arrow shows that the preferred \nCPU of a child object is set to that of its parent. T1 T2 T3 Thread CPU id Th read Sta cks  stack \n Ob ject s loc k / default reserve  Figure 7. Preferred CPUs determined from a reference graph T1 T2T3 \nThread CPU id CPU0 CPU1 CPU2 Thread Stacks Objects default Figure 8. Incorrect preferred CPUs are propagated \nif the dominant thread information is ignored Using the dominant thread for determining the preferred \nCPUs is the key to ef.ciently determining the preferred CPUs. In some applications, objects are reachable \nfrom mul\u00adtiple threads in the reference graph, while each disjoint sub\u00adset of the objects is accessed \nby a single thread. When us\u00ading thread locality analysis (such as thread escape analy\u00adsis [29] and thread-local \nheaps [11]), these objects cannot be distinguished from the objects whose owners change fre\u00adquently. \nA typical example is an application in which a con\u00adtroller thread dispatches jobs to worker threads. \nThe con\u00adtroller thread and worker threads communicate with each other by sharing objects and can reach \nthose objects in the reference graph. Each worker accesses its own subset of the objects by controlling \nthe mutual exclusion. Sharing objects among threads is widely used in server applications such as SPECpower \nssj2008. Without using the dominant thread for the preferred CPU, incorrect preferred CPUs can be propagated, \nespe\u00adcially when using parallel GC. In practice, we observed that most of the live objects were wrongly \nmoved to the CPU of a speci.c thread in our experiments if the dominant thread was ignored. In parallel \nGC, the JVM starts N GC threads where N is usually the number of hardware threads. These GC threads traverse \nthe reference graph in parallel [15]. With parallel GC in our example, three GC threads, GC1, GC2, and \nGC3, will start traversal of the reference graph from the stacks of T1, T2, and T3, respectively. Figure \n8  HW/SW Processor POWER6 4.7GHz Total CPUs 8 Total Memory 64 GB Threads per CPU 4 Memory per CPU 8GB \nOperating System AIX 6.1 Java VM 64-bit IBM J9 1.6.0 SR1 Total Heap Size 20 GB Table 1. H/W and S/W \ncomponents shows that, without the dominant thread information, GC3 .nished its traversal before GC1 \nand GC2 traversed the la\u00adbeled objects. The preferred CPUs of the objects T1 and T2 and their children \nobjects are wrongly set to CPU2 even though they are mainly accessed by other threads. This situ\u00adation \ncan frequently occur since GC3 cannot know whether or not a target object can be reached from T1 or T2. \nAfter determining the preferred CPU for the current ob\u00adject, a GC thread will move the object to the \nmemory at\u00adtached to the preferred CPU. Since the thread will be dis\u00adpatched within the same CPU by the \nNUMA-af.nity sched\u00aduler, the thread can continue to access the object on the local memory.  6. Experimental \nResults To evaluate our approach, we compare the throughputs of Java programs with the base JVM and with \na modi.ed ver\u00adsion using our prototype software. This section .rst explains the methodology of our experiments. \nThen this section presents and discusses the experimental results of bench\u00admark programs. 6.1 Methodology \nThis section .rst describes the Java VM that we used for our prototype. Then this section explains the \nhardware and soft\u00adware components used for our experiments. Table 1 summa\u00adrizes these components. 6.1.1 \nJava Virtual Machine We prototyped our approach on a modi.ed version of the IBM Developer Kit and Runtime \nEnvironment, Java Tech\u00adnology Edition, Version 6, Service Release 1 (64-bit version) [13, 15]. in the \nexperiments, we used 20 GB as the total heap, which consisted f 16 GB of young generation and 4 GB of \nold generation memory. The JVM uses the large (16 MB) page feature of the operating system to reduce \nthe cost of lo\u00adcal to physical address translation. For the JIT compiler, we enabled the lock reservation \noptimization [18]. Lock reser\u00advation is usually activated for hot code [12]. For our experi\u00adments, we \nactivated the lock reservation for the code that was compiled at the default optimization level in addition \nto the hot code on both the base version and our modi.ed version of the JVM.  We modi.ed the memory \nallocator and the generational garbage collector of the JVM. Both the code compiled by the just-in-time \n(JIT) compiler, which compiles the Java bytecode into the native code, and the interpreter used the modi.ed \nmemory allocator during the experiments with the prototype. For each segment of the Java heap, we speci.ed \nthe CPU to which the segment was mapped by using the resource set API of the operating system. The segment \nsize was constant for each generation. For our experiments, the segment size was 16 MB for the young \ngeneration and 512 MB for the old generation. The segment size was .xed at the startup time of the JVM. \nLarge arrays that consumed more than sev\u00aderal megabytes could be allocated across multiple segments since \nthe page mapping changed every 16 MB in the young generation. We allowed objects to stretch across segments. \nOur prototype triggers a GC if no free space is found in the heap segments mapped to the local CPU, similar \nto the approach used in prior research [35, 9]. This aggressive NUMA-af.nity allocation can increase \nthe utilization of the local memory, though it can also increase the frequency of garbage collection. \n 6.1.2 Hardware and Operating System Our hardware environment was a POWER6-based shared\u00admemory multiprocessor \nserver with the AIX 6.1 operating system. The server has eight CPUs. Each CPU has two POWER6 cores [19], \nrunning at 4.7 GHz. Each core runs two hardware threads by using the Simultaneous Multi-Threading (SMT) \nfeature. The total number of hardware threads is 32. The 64-KB L1 I-cache and 64-KB L1 D-cache are integrated \nin each core. A private 4-MB L2 cache is attached to each core within a CPU. The memory con.guration \nis cc-NUMA. Each CPU has 8 GB as a local memory. The remote memory can be accessed through the SMP fabric \nbus. A remote access is two times slower than a local access according to the NUMA distances de.ned in \nthe system table2. 6.1.3 Benchmark Programs We used many benchmark programs that tested a vari\u00adety of \nscenarios, including business applications, com\u00adpiler, crypto, mpegaudio, numerical computing, xml, render\u00ading, \nand others. The programs came from standard bench\u00admark programs: SPECpower ssj2008, SPECjbb2005, and \nSPECjvm2008. All of the programs are multi-threaded and can fully use the available CPUs. 2 ACPI (Advanced \nCon.guration and Power Interface) SLIT (System Lo\u00adcality Information Table) read by the numactl tool \non Linux  6.1.4 Measurement Methodology We ran the programs with three versions of the JVM: the base, \nthe version that includes only the NUMA-aware al\u00adlocator, and the version that includes the NUMA-aware \nmemory manager, which consists of both the NUMA-aware allocator and the NUMA-aware GC. By comparing the \nthroughput between the NUMA-aware allocator only and the NUMA-aware memory manager, we can evaluate the \nadditional performance improvements of the NUMA-aware GC. These two prototypes used the same NUMA-aware \nal\u00adgorithms except for moving live objects during the GC time. The NUMA-aware allocator uses the same \nalgorithm as used in the base JVM, which reserves the memory in address or\u00adder as explained in Section \n3. We ran each program with a warm-up run and a measure\u00adment run. We took the best throughput of .ve \nruns for each JVM. We used the following workloads. For SPECpower ssj2008, we used the 100% load. For \nSPECjbb2005, we used the same number of warehouses as that of hardware threads. The mea\u00adsurement times \nfor these programs were 4 minutes. For the programs of SPECjvm2008, the warm-up time was 30 sec\u00adonds \nand the measurement time was 1 minute. Also, we used the command-line options for the JIT to compile \nmethods at their .rst invocations and to disable upgrade compilation in order to reduce the .uctuations \nin the throughput.  6.2 Experimental Results We discuss the experimental results with our approach in \nthis section. We also discuss how the GC overhead is affected by the aggressive NUMA-af.nity allocation \npolicy. 6.2.1 Performance Improvements from NUMA-Aware JVM Figure 9 shows the relative performance improvements \nwith our NUMA-aware memory manager over the base JVM for each benchmark program. The .rst bar in each \npair shows the improvement by the NUMA-aware allocator compared to the base JVM. The second bar shows \nthe improvement with both the NUMA-aware allocator and the NUMA-aware GC compared to the base JVM. In \nsummary, the relative performance improvements by using both the NUMA-aware allocator and GC are up to \n53.1% and 10.8% on average. We observed signi.cant im\u00adprovements of more than 20% for .ve out of the \neighteen programs including SPECpower ssj2008, SPECjbb2005, scimark.fft.default, scimark.lu.default, \nand scimark.sparse. default. We observed a 10% improvement for compiler. compiler. For the programs compiler.sun.ow, \ncrypto.aes, crypto.signverify, scimark.sor.default, sun.ow, xml.transform, and xml.validation, we observed \n2%-8% improvements. For the remaining programs, compress, crypto.rsa, mpegaudio, scimark.monte carlo, \nand serial, we observed no signi.cant improvements. 60% 50% ssj2008 (1 min)jbb2005 (1 min)compiler.compilercompiler.sunflowcompresscrypto.aescrypto.rsacrypto.signverifympegaudioscimark.fft.defaultscimark.lu.defaultscimark.sor.defaultscimark.sparse.defaultscimark.monte_carlosunflowxml.transformxml.validationAverage \n Relative Performance Improvement 40% 30% 20% 10% 0% scimark.sor.default scimark.sparse.default scimark.monte_carlo \n Base JVM Normalized NUMA-Aware Alloc &#38; GC - Figure 11. GC count jects that were moved by each GC. \nThe NUMA-aware GC NUMA-Aware Allocator NUMA-Aware Allocator &#38; GC decided on preferred CPUs for each \nof these objects. Some Figure 9. Relative performance improvements programs, such as scimark.sor.default, \ndid not cause any GCs, but this result does not always mean that new objects are rarely created by these \nprograms. They might have GC  if we ran the programs for longer periods. 6.2.2 GC Overhead Our prototype \nperforms aggressive NUMA-af.nity alloca\u00ad tion, which can trigger a GC earlier than the base JVM. This \nallocation policy triggers a GC if no free space is found in the heap segments mapped to the local CPU, \nsimilar to the approach used in prior research [35, 9], and as mentioned in Section 6.1.1. This is intended \nto improve the performance by allocating objects only in local memory. However, it can increase the frequency \nof garbage collection. Therefore, we measured the GC counts for the benchmark programs. Figure 11 shows \nthe GC counts during one-minute measure\u00adments. The .rst bar shows the GC count with the base JVM. The \nsecond bar shows the GC count with the NUMA-aware allocator and GC. The GC count with the NUMA-aware \nal\u00adlocator and GC is normalized to the throughput of the base JVM, since the GC count is proportional \nto the throughput. ssj2008 jbb2005 compiler.compiler compiler.sunflow compress crypto.aes crypto.rsa \ncrypto.signverify mpegaudio scimark.fft.default scimark.lu.default scimark.sor.default scimark.sparse.default \nscimark.monte_carlo sunflowxml.transformxml.validation Average Figure 10. The average number and size \nof live objects per GC We calculated the normalized GC count by using the for\u00admula, The NUMA-aware GC \nproduced additional improve\u00adments for seven out of the eighteen programs. Focusing on performance improvements \nover the base JVM, the NUMA\u00adaware GC added 10% and 14% to the improvements of the NUMA-aware allocator \nfor SPECpower ssj2008 and SPECjbb2005, respectively. Also, the NUMA-aware GC added 3%, 5%, 2%, 1%, and \n3% for compiler.compiler, sci\u00admark.fft.default, scimark.lu.default, scimark.sparse.default, and xml.validation, \nrespectively. For the other programs, we observed no additional improvements. Overall, the NUMA\u00adaware \nGC added an average 2% performance improvement. The bars in Figure 10 show the average numbers of live \nob- GCCountnuma * (T hroughputbase/T hroughputnuma). The aggressive NUMA-af.nity allocation did not \nsignif\u00adicantly increase the GC count as shown in Figure 11. This means that, when there is no free space \non one CPU, the other CPUs also have little free memory. These programs allocate objects at similar object \nallocation rates in each thread. For such programs, this allocation policy works ef.\u00adciently and does \nnot degrade the performance. Next, we compared the average pause time per GC of the base JVM and the \nJVM with NUMA-aware allocator and GC to evaluate the overhead of our approach. Figure 12 shows the relative \nincrease in the GC pause time. Y %  on the y-axis shows that the average GC pause time was increased \nby Y % in the NUMA-aware JVM compared to the base JVM. We excluded the programs that did not have GC \nfrom this .gure. We observed negligible increases, no increase, or even decreases in the pause time for \nnine of the total four\u00adteen programs: ssj2008, jbb2005, compiler.compiler, com\u00adpiler.sun.ow, compress, \ncrypto.aes, crypto.signverify, sci\u00admark. sparse.default, and xml.validation. Since half of these programs \nmove a large number of small objects, as shown in Figure 10, the GC pause times would have been signi.\u00adcantly \nincreased if the additional cost of our approach were large. From these results, we can conclude that \nthe cost of our approach is negligible for GC. For the remaining programs, the observed increases in \nthe overhead were not due to our approach but due to limita\u00adtions of our current implementation. By investigating \nsci\u00admark.lu.default and xml.transform, we found that the in\u00adcreases are due to signi.cantly increased \nexecution time for moving large objects. We also found that both the source and destination were in remote \nmemory for a large number of objects. As a result, the time for moving objects from and to remote memory \nwas 11 times longer than for local memory for scimark.lu.default. In our current implementa\u00adtion, the \nGC threads can suffer from the remote memory ac\u00adcesses, since they can obtain the target objects from \nremote memory. Overall, even with these performance limitations, the average increase in the GC pause \ntime was only 14%.   7. Conclusions We have created a novel online method for identifying the preferred \nCPUs for objects with negligible overhead dur\u00ading the garbage collection time as well as during object \nallocation time. Our approach uses the information about which thread can dominantly access an object, \nor a domi\u00adnant thread. The dominant thread of an object is probably the thread that most often accesses \nan object. Also, the dominant thread can be recognized from the thread stack and object headers while \ntraversing the object reference graph during GC. Therefore, we do not need the memory access samples \nthat increase the GC overhead, as done in prior research. Our NUMA-aware GC performs object copying \nbased on the dominant-thread (DoT), which copies each live object to the CPU where the dominant thread \nwas last dispatched be\u00adfore the GC. The dominant thread information is propagated from the thread stack \nand from objects that are locked or reserved by threads through the object reference graph. We showed \nthat our approach can improve the perfor\u00ad mance of various benchmark programs from SPECpower ssj2008, \nSPECjbb2005, and SPECjvm2008. We prototyped our NUMA\u00ad aware memory manager on a modi.ed version of IBM \nJava VM and tested it on a cc-NUMA POWER6 machine with eight NUMA nodes. Our NUMA-aware GC achieved ad\u00additional \nperformance improvements up to 14.3% and 2.0% on average. The total improvements using both the NUMA\u00ad \naware allocator and GC were up to 53.1% and 10.8% on average.  References [1] Advanced Micro Devices, \nInc. Performance guidelines for AMD AthlonTM64 and AMD OpteronTMccNUMA multiprocessor systems, 2006. \n[2] David F. Bacon, Ravi Konuru, Chet Murthy, and Mauri\u00adcio Serrano. Thin locks: featherweight synchroniza\u00adtion \nfor Java. In PLDI 98: Proc. ACM SIGPLAN 1998 Conf. on Programming language design and implemen\u00adtation, \npages 258 268, 1998. [3] BEA Systems, Inc. BEA JRockit, r27.5, command\u00adline reference. http://e-docs.bea.com/jrockit/ \njrdocs/pdf/refman.pdf, January 2008. [4] Martin J. Bligh, Matt Dobson, Darren Hart, and Gerrit Huizenga. \nLinux on NUMA systems. In Proceedings of the Linux Symposium, pages 295 306, 2004. [5] B. Brock, G. Carpenter, \nE. Chiprout, E. Elnozahy, M. Dean, D. Glasco, J. Peterson, R. Rajamony, F. Raw\u00adson, R. Rockhold, and \nA. Zimmerman. Windows NT in a ccNUMA system. In WINSYM 99: Proceedings of the 3rd conference on USENIX \nWindows NT Sympo\u00adsium, pages 7 7, Berkeley, CA, USA, 1999. USENIX Association. [6] Myra Cohen, Shiu Beng \nKooi, and Witawas Srisa-an. Clustering the heap in multi-threaded applications for improved garbage collection. \nIn GECCO 06: Proceed\u00adings of the 8th annual conference on Genetic and evo\u00adlutionary computation, pages \n1901 1908, New York, NY, USA, 2006. ACM. ISBN 1-59593-186-4. doi: http://doi.acm.org/10.1145/1143997.1144314. \n[7] IBM Corp. IBM @server pSeries 650 performance and tuning. http://www-03.ibm.com/systems/ resources/systems_p_hardware_whitepapers_ \np650_perf.pdf, 2002.  [8] IBM Corp. AIX 5L differences guide version 5.3 edition. http://www.redbooks.ibm.com/ \nredpieces/pdfs/sg247463.pdf, 2004. [9] David Dagastine and Paul Hohensee. High per\u00adformance Java technology \nin a multi-core world. http://developers.sun.com/learning/ javaoneonline/2007/pdf/TS-2885.pdf, 2007. \n[10] Robert Dimpsey, Rajiv Arora, and Kean Kuiper. Java server performance: a case study of building \nef.cient, stable JVMs. IBM Systems Journal, 39(1):151 174, 2000. [11] Tamar Domani, Gal Goldshtein, Elliot \nK. Kolodner, Ethan Lewis, Erez Petrank, and Dafna Sheinwald. Thread-local heaps for Java. In ISMM 02: \nProceed\u00adings of the 3rd International Symposium on Memory Management, pages 76 87, New York, NY, USA, \n2002. ACM. ISBN 1-58113-539-4. doi: http://doi.acm.org/ 10.1145/512429.512439. [12] Nikola Grcevski. \nEffective method for Java lock reser\u00advation for Java virtual machines that have cooperative multithreading. \nIn 6th Workshop on Compiler-Driven Performance, Associated with CASCON2007, 2007. [13] Nikola Grcevski, \nAllan Kielstra, Kevin Stoodley, Mark G. Stoodley, and Vijay Sundaresan. Java just-in\u00adtime compiler and \nvirtual machine improvements for server and middleware applications. In Proc. the 3rd Virtual Machine \nResearch and Technology Symposium, pages 151 162, 2004. [14] Martin Hirzel. Data layouts for object-oriented \npro\u00adgrams. In SIGMETRICS 07: Proceedings of the 2007 ACM SIGMETRICS International Conference on Mea\u00adsurement \nand Modeling of Computer Systems, pages 265 276, New York, NY, USA, 2007. ACM. ISBN 978-1-59593-639-4. \ndoi: http://doi.acm.org/10.1145/ 1254882.1254915. [15] IBM Corp. IBM developer kit and runtime envi\u00adronment, \nJava technology edition, version 6 diag\u00adnostics guide. http://download.boulder.ibm. com/ibmdl/pub/software/dw/jdk/diagnosis/ \ndiag60.pdf,. [16] IBM Corp. WebSphere Application Server Network Deployment, version 6.1, tuning Java \nvirtual machines. http://publib.boulder.ibm.com/infocenter/ wasinfo/v6r1/index.jsp?topic=/com.ibm. websphere.nd.doc/info/ae/ae/tprf_tunejvm_ \nv61.html,. \u00ae 64 and IA-32 architectures opti\u00admization reference manual, 2009. [17] Intel Corp. Intel \nR [18] Kiyokuni Kawachiya, Akira Koseki, and Tamiya On\u00adodera. Lock reservation: Java locks can mostly \ndo with\u00adout atomic operations. In Proc. ACM SIGPLAN Conf. on Object-Oriented Programming Systems, Languages \nand Applications, pages 130 141, 2002. [19] Hung Q. Le, William J. Starke, J. Stephen Fields, Fran\u00adcis \nP. O Connell, Dung Q. Nguyen, Bruce J. Ronchetti, Wolfram M. Sauer, Eric M. Schwarz, and Michael T. (Mike) \nVaden. IBM POWER6 microarchitecture. IBM J. Res. Dev., 51(6):639 662, 2007. ISSN 0018\u00ad8646. [20] Kyungwoo \nLee and Samuel P. Midkiff. A two-phase escape analysis for parallel Java programs. In PACT 06: Proceedings \nof the 15th International Confer\u00adence on Parallel Architectures and Compilation Tech\u00adniques, pages 53 \n62, New York, NY, USA, 2006. ACM. ISBN 1-59593-264-X. doi: http://doi.acm.org/ 10.1145/1152154.1152166. \n[21] Kyungwoo Lee, Xing Fang, and Samuel P. Midkiff. Practical escape analyses: how good are they? In \nVEE 07: Proceedings of the 3rd International Confer\u00adence on Virtual Execution Environments, pages 180 \n190, New York, NY, USA, 2007. ACM. ISBN 978-1-59593-630-1. doi: http://doi.acm.org/10.1145/ 1254810.1254836. \n[22] Henry Lieberman and Carl Hewitt. A real-time garbage collector based on the lifetimes of objects. \nCommun. ACM, 26(6):419 429, 1983. ISSN 0001-0782. doi: http://doi.acm.org/10.1145/358141.358147. [23] \nJaydeep Marathe and Frank Mueller. Hardware pro.le\u00adguided automatic page placement for ccnuma systems. \nIn PPoPP 06: Proceedings of the eleventh ACM SIG-PLAN symposium on Principles and practice of paral\u00adlel \nprogramming, pages 90 99, New York, NY, USA, 2006. ACM. ISBN 1-59593-189-9. doi: http://doi.acm. org/10.1145/1122971.1122987. \n[24] Takeshi Ogasawara, Hideaki Komatsu, and Toshio Nakatani. TO-lock: Removing lock overhead using the \nowners temporal locality. In PACT 04: Proc. 13th In\u00adternational Conf. on Parallel Architectures and Compi\u00adlation \nTechniques, pages 255 266, 2004. [25] Mattias Persson. Java technology, IBM style: Garbage collection \npolicies, part 1. http://http://www.ibm. com/developerworks//library/j-ibmjava2/, 2006. [26] Kenneth \nRussell and David Detlefs. Eliminating synchronization-related atomic operations with biased locking \nand bulk rebiasing. In Proc. ACM SIGPLAN Conf. on Object-Oriented Programming Systems, Lan\u00adguages and \nApplications, pages 263 272, 2006. [27] Ryan Sciampacone. A production world view of garbage collection \nfor the Java ME and SE spaces. http://www.mm-net.org.uk/school/ programme.html. [28] Ye.m Shuf, Manish \nGupta, Rajesh Bordawekar, and Jaswinder Pal Singh. Exploiting proli.c types for mem\u00adory management and \noptimizations. In POPL 02: Pro\u00adceedings of the 29th ACM SIGPLAN-SIGACT sympo\u00adsium on Principles of programming \nlanguages, pages 295 306, New York, NY, USA, 2002. ACM. ISBN 1\u00ad58113-450-9. doi: http://doi.acm.org/10.1145/503272. \n503300. [29] Bjarne Steensgaard. Thread-speci.c heaps for multi\u00adthreaded programs. In ISMM 00: Proceedings \nof the 2nd International Symposium on Memory Manage\u00adment, pages 18 24, New York, NY, USA, 2000. ACM. \nISBN 1-58113-263-8. doi: http://doi.acm.org/10.1145/ 362422.362432. [30] Sun Microsystems, Inc. Ergonomics \nin the 5.0 JavaTMvirtual machine. http://java.sun.com/ docs/hotspot/gc5.0/ergo5.html. [31] Sun Microsystems, \nInc. SolarisTMmemory placement optimization and Sun FireTMservers. http://www.sun.com/servers/wp/docs/mpo_ \nv7_CUSTOMER.pdf, 2003. [32] Sun Microsystems, Inc. Java SE 6 HotSpotTMvirtual machine garbage collection \ntuning. http: //java.sun.com/javase/technologies/ hotspot/gc/gc_tuning_6.html, 2008. \u00ae [33] Sun Microsystems, \nInc. Sun SPARC Renterprise T5440 server architecture, 2008. [34] Christian Terboven, Dieter an Mey, \nDirk Schmidl, Henry Jin, and Thomas Reichstein. Data and thread af.nity in openmp programs. In MAW 08: \nProceed\u00adings of the 2008 workshop on Memory access on fu\u00adture processors, pages 377 384, New York, NY, \nUSA, 2008. ACM. ISBN 978-1-60558-091-3. doi: http: //doi.acm.org/10.1145/1366219.1366222. [35] Mustafa \nM. Tikir and Jeffery K. Hollingsworth. NUMA-aware Java heaps for server applications. In IPDPS 05: Proceedings \nof the 19th IEEE Interna\u00adtional Parallel and Distributed Processing Symposium (IPDPS 05) -Papers, page \n108.2, Washington, DC, USA, 2005. IEEE Computer Society. ISBN 0-7695\u00ad2312-9. doi: http://dx.doi.org/10.1109/IPDPS.2005. \n299. [36] Mustafa M. Tikir and Jeffrey K. Hollingsworth. Us\u00ading hardware counters to automatically improve \nmem\u00adory performance. In SC 04: Proceedings of the 2004 ACM/IEEE conference on Supercomputing, page 46, \nWashington, DC, USA, 2004. IEEE Computer Society. ISBN 0-7695-2153-3. doi: http://dx.doi.org/10.1109/ \nSC.2004.64. [37] David Ungar. Generation scavenging: A non\u00addisruptive high performance storage reclamation \nalgo\u00adrithm. In SDE 1: Proceedings of the .rst ACM SIG-SOFT/SIGPLAN software engineering symposium on \nPractical software development environments, pages 157 167, New York, NY, USA, 1984. ACM. ISBN 0\u00ad89791-131-8. \ndoi: http://doi.acm.org/10.1145/800020. 808261.  \n\t\t\t", "proc_id": "1640089", "abstract": "<p>We propose a novel online method of identifying the preferred NUMA nodes for objects with negligible overhead during the garbage collection time as well as object allocation time. Since the number of CPUs (or NUMA nodes) is increasing recently, it is critical for the memory manager of the runtime environment of an object-oriented language to exploit the low latency of local memory for high performance. To locate the CPU of a thread that frequently accesses an object, prior research uses the runtime information about memory accesses as sampled by the hardware. However, the overhead of this approach is high for a garbage collector.</p> <p>Our approach uses the information about which thread can exclusively access an object, or the <i>Dominant Thread</i> (DoT). The dominant thread of an object is the thread that often most accesses an object so that we do not require memory access samples. Our NUMA-aware GC performs DoT based object copying, which copies each live object to the CPU where the dominant thread was last dispatched before GC. The dominant thread information is known from the thread stack and from objects that are locked or reserved by threads and is propagated in the object reference graph.</p> <p>We demonstrate that our approach can improve the performance of benchmark programs such as SPECpower ssj2008, SPECjbb2005, and SPECjvm2008.We prototyped a NUMAaware memory manager on a modified version of IBM Java VM and tested it on a cc-NUMA POWER6 machine with eight NUMA nodes. Our NUMA-aware GC achieved performance improvements up to 14.3% and 2.0% on average over a JVM that only used the NUMA-aware allocator. The total improvement using both the NUMA-aware allocator and GC is up to 53.1% and 10.8% on average.</p>", "authors": [{"name": "Takeshi Ogasawara", "author_profile_id": "81100012455", "affiliation": "IBM Research - Tokyo, Yamato, Japan", "person_id": "P1728792", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1640089.1640117", "year": "2009", "article_id": "1640117", "conference": "OOPSLA", "title": "NUMA-aware memory manager with dominant-thread-based copying GC", "url": "http://dl.acm.org/citation.cfm?id=1640117"}