{"article_publication_date": "01-25-2012", "fulltext": "\n Programming with Binders and Indexed Data-Types Andrew Cave Brigitte Pientka McGill University {acave1,bpientka}@cs.mcgill.ca \nAbstract We show how to combine a general purpose type system for an existing language with support for \nprogramming with binders and contexts by re.ning the type system of ML with a restricted form of dependent \ntypes where index objects are drawn from contextual LF. This allows the user to specify formal systems \nwithin the logical framework LF and index ML types with contextual LF objects. Our language design keeps \nthe index language generic only requiring decidability of equality of the index language providing a \nmodular design. To illustrate the elegance and effectiveness of our language, we give programs for closure \nconversion and normalization by evaluation. Our three key technical contribution are: 1) We give a bi\u00addirectional \ntype system for our core language which is centered around re.nement substitutions instead of constraint \nsolving. As a consequence, type checking is decidable and easy to trust, although constraint solving \nmay be undecidable. 2) We give a big-step en\u00advironment based operational semantics with environments \nwhich lends itself to ef.cient implementation. 3) We prove our language to be type safe and have mechanized \nour theoretical development in the proof assistant Coq using the fresh approach to binding. Categories \nand Subject Descriptors D.3.3 [Programming Lan\u00adguages]: Language Constructs and Features Data types and \nstruc\u00adtures General Terms Design, Languages Keywords Logical frameworks, higher-order abstract syntax, \nde\u00adpendent types, recursive types 1. Introduction To reason about the runtime behavior of software, we \nroutinely de\u00adsign and use formal systems, given by axioms and inference rules, such as logics to reason \nabout access control [Abadi et al. 1993, 1999; Garg and Pfenning 2006] and information .ow [Miyamoto \nand Igarashi 2004], logics to reason about memory access [Nanevski et al. 2008a] or simply the scope \nof names [Pottier 2007]. Over the last decade we have come closer to narrowing the gap between programming \nsoftware systems and reasoning about them [Chen and Xi 2005; Sheard 2004; Westbrook et al. 2005]. The \ngeneral mantra is to design rich type systems which allow programmers to specify and enforce statically \npowerful invariants about their pro- Permission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior \nspeci.c permission and/or a fee. POPL 12, January 25 27, 2012, Philadelphia, PA, USA. Copyright c &#38;#169; \n2012 ACM 978-1-4503-1083-3/12/01. . . $10.00 grams. Yet, existing approaches lack rich abstractions that \nallow users to describe formal systems and proofs on a high-level, factor out common and recurring issues, \nmake it easy to use, and at the same time have a small trusted kernel. In this paper, we extend a general \npurpose language to support programming with formal systems and ultimately proofs. This is achieved by \nindexing types with contextual LF objects [Pientka 2008]. Contextual LF extends the logical framework \nLF [Harper et al. 1993] with the power of contextual objects . .M of type A[.]. M denotes an object which \nmay refer to the bound variables listed in . and has type A in the context . (see also [Nanevski et al. \n2008b]). It also supports .rst-class contexts and allows us to ab\u00adstract over contexts. This allows the \nuser to specify formal systems within the logical framework LF and obtaining support for repre\u00adsenting \nand managing binders, renaming, fresh name generation, and capture-avoiding substitutions. Contextual \nLF allows program\u00admers to pack open LF objects together with the context in which they are meaningful \nthereby obtaining closed objects which can be passed and manipulated. In particular, contextual LF objects \ncan be used to index types and track rich formal properties. We demon\u00adstrate the advantages of combining \ncontextual LF with data-types by discussing implementations of closure conversion and normal\u00adization \nby evaluation. These examples have been a good benchmark in comparing systems and demonstrate the bene.ts \nand elegance of our approach. Fundamentally, our approach follows the tradition of indexed types (see \nXi and Pfenning [1999]; Zenger [1997]) choosing as an index domain contextual LF which allows us to express \nproperties about open objects, the scope of variables and contexts which previ\u00adous systems such as ATS/LF \n[Xi 2004] lack. Instead of generating and propagating constraints which is common in indexed type sys\u00adtems, \nwe will associate patterns with re.nements substitution and work only with constraints in solved form. \nThis leads to a small trusted kernel. Compared to languages such as Beluga [Pientka and Dun.eld 2010] \nor Delphin [Poswolsky and Sch\u00a8urmann 2009, 2008] which support programming with binders already, the \nlanguage we pro\u00adpose in this paper supports recursive types which the former lan\u00adguages lack; this extension \nis key to tackle important problems such as normalization by evaluation which are typically out of reach \nfor these languages. Having the ability to de.ne data-types is con\u00advenient; it is also more ef.cient \nthan supporting data-types via a Church-encoding which is in principle already possible. In contrast \nto Licata and Harper [2009] which supports mixing binding and computation, we keep the separation of \ndata and com\u00adputations. This has several advantages: Because our index language remains pure, it is straightforward \nto establish adequacy of the for\u00admal system and its encoding using standard techniques (see for ex\u00adample \nHarper and Licata [2007]). This allows us to maintain all the good properties of contextual objects, \nnamely strong normaliza\u00adtion and decidable equality. Moreover, our computation language remains close \nto traditional ML-like languages and is designed to be modular in the index domain. As a consequence, \none can easily replace contextual LF with for example a higher-order logic with inductive types without \naffecting the computations. Compared to other systems with a uniform language for computations and types \nsuch as the Calculus of Construction or Martin L\u00a8  of type theory, our computation language can easily \nbe combined with imperative features, allows non-terminating computation, and requires fewer annotations \nto make type checking decidable. The main technical contributions of this paper are: We present a core \nlanguage with dependent types where we separate the types from computations. One can think of this core \nlanguage as the target of elaborating a surface language where we may omit implicit indices via type \nreconstruction. Our language is a conservative extension of a general purpose language where types are \nindexed by contextual LF objects and contexts and at the same time supports pattern matching on its index \nobjects. Because it may be viewed as an extension of Beluga with recursive types, we call our language \nBeluga\u00b5. However, we emphasize that our design of the computation language is generic and we can replace \nthe index language which in our case is contextual LF with any other language where equality between \ntwo index objects is decidable.  We present a bi-directional type system for our core language. Theoretically, \nwe model dependently typed recursive types as .xpoints with explicit equality constraints on contextual \nob\u00adjects. This is similar to Xi et al. [2003] and Sulzmann et al. [2007] where recursive types are endowed \nwith equalities be\u00adtween types to model GADTs. However, instead of accumulat\u00ading and solving constraints \nduring type checking, our approach relies on re.nement substitutions in branches. Although con\u00adstraint \nsolving may be undecidable in our setting, type check\u00ading based on re.nements remains decidable. Type \nchecking is hence easier to trust.  We give a big-step environment-based operational semantics. Because \nwe allow matching on computation-level expressions and index objects, we distinguish between the environment \nfor computation-level values and index objects and show that types are preserved. By extending our operational \nsemantics to also track diverging computations following Cousot and Cousot [1992]; Leroy and Grall [2009], \nwe prove progress.  We have mechanized the typing rules, the operational semantics and the type safety \nproof (preservation and progress) in the proof assistant Coq. We use the fresh approach to binding by \nPouillard and Pottier [2010] to model variables in our language.  The proposed programming language \nwith support for contex\u00adtual objects and data-types is a prime candidate for programming code transformations \nand certi.ed programming. More generally, it provides a foundation for programming with domain-speci.c \nlog\u00adics and demonstrates how to endow a general purpose program\u00adming language with direct support for \nprogramming with logics and proofs. The remainder of the paper is organized as follows. To illustrate \nthe main idea of de.ning types indexed with contextual objects, we discuss in detail three examples (Section \n2): closure-based evalu\u00adator, closure conversion and normalization by evaluation. We then introduce Beluga\u00b5, \na language which supports contextual objects and computation-level data-types in Section 3. We begin \nby intro\u00adducing our index domain in Section 3.1 which in our case is con\u00adtextual LF [Pientka 2008]. This \nwill review and summarize previ\u00adous work in this area. We then present the computation language in Section \n4 which includes indexed types, recursive types, vari\u00adants and general pattern matching. We will leave \nout polymorphism and imperative features which are orthogonal issues and which are straightforward to \nadd. The typing rules for Beluga\u00b5 and an environment-based big-step semantics together with the type \nsafety proof are presented in Section 4.4. We explain our mechanization of the type safety proof in Section \n5. The rest of the paper is con\u00adcerned with some related work, current status and future research directions. \n2. Motivating Examples In this section, we discuss three examples which illustrate the utility of a language \ncombining contextual types with indexed recursive types. We use an informal surface syntax inspired by \nboth Beluga and Agda [Norell 2007]. This surface syntax is intended to elabo\u00adrate to the core language \npresented in Section 4. 2.1 Closure-based evaluator for the lambda-calculus We begin with a demonstration \nof a closure-based evaluator for the untyped lambda-calculus which is directly comparable to that of \nLicata and Harper [2009]. This allows us to explain contextual objects in a simple setting. We .rst represent \nthe untyped lambda calculus in LF with higher order abstract syntax (HOAS). lam: (tm . tm) . tm. app: \ntm . tm . tm. To implement an evaluator, we must analyze and pattern match on lambda-terms and consequently \nwe must be able to handle open objects. Hence, the evaluator will be parameterized with a context . which \nkeeps track of variables of type tm and manipulates contextual objects of type tm[.]. To type contexts \nwe de.ne a context schema ctx as follows: schema ctx = tm; To express that . contains only variables \nof type tm we write .:ctx. The highlight of this example, when compared to Beluga or Del\u00adphin, is that \nwe can write the closure of a term under an environ\u00adment of bindings as a computation-level data-type \nwith a single constructor cl as follows: datatype clos : ctype = cl : {.:ctx} . tm[.,x:tm] . (#tm[.] \n. clos) . clos type envr . = #tm[.] . clos We write tm[.] for the type of terms whose free variables \ncome only from the context .. We write #tm[.] for the type of variables of type tm in context . i.e. \nelements of .. We overload . using it for LF types and also computation-level types; since datatype is \npart of the computation language, the arrow in #tm[.] . clos does not indicate the LF function space, \nbut rather the usual computational function space. Hence this datatype represents the bodies of lambda \nabstractions in some context together with a binding for each of the free variables minus one. Note that \nwe wrap arguments in {} to indicate that they are passed implicitly. We can now proceed to evaluate a \nterm in a context . and in an environment which provides bindings for the variables in .. rec eval : \n{.:ctx} tm[.] . envr . . clos = fn e . fn env . case e of | .. #p .. . env (.. #p ..) | .. lam (.x. E \n.. x) . cl (.,x:exp . E .. x) env | .. app (E1 ..) (E2 ..) . let cl (f,x:tm. E..x)env =eval(.. E1 ..) \nenv let v = eval (.. E2 ..) env in in eval (f,x:tm.E..x) (fn var . case var of | f,x:tm . x . v | f,x:tm \n. #p .. . env (f. #p .. )) When we pattern match on a contextual object e : tm[.], we might obtain a \nvariable in ., which we write as .. #p .. . The item #p is a parameter variable, standing for a position \nin .. Its association with the identity substitution for . (written .. ) turns it from a position into \na genuine tm[.]. In this case, we look it up in the environment. A lambda abstraction simply evaluates \nto a closure. We think of the . in .. lam (.x. E..x) as binding all the free variables of lam (.x. E..x). \nWe explicitly apply . s identity substitution .. to E in the pattern to indicate that the variables in \n. are permitted to occur in E. Conversely, writing the pattern as .. lam (.x.E x) attempts to strengthen \nthe variables of . out of E, which is not the intention.  An application evaluates the function position \nto obtain a body E in the possibly different context f with an associated env providing bindings for \nthe variables in f. We now evaluate the body E in the environment env extended with the appropriate binding \nfor x. This implementation is quite close to that of Licata and Harper [2009]. One noticeable difference \nis that contexts appear explicitly in our de.nition of clos, which is arguably more readable.  2.2 Relating \nContexts: Closure Conversion When implementing a transformation between languages, as is common in compilers, \nwe often need the resulting terms to be in a different but related context. In systems such as Twelf \n[Pfenning and Sch\u00a8urmann 1999] and Beluga, the solution is somewhat un\u00adsatisfactory. The programmer states \nresults in a combined context, and relies on sophisticated subordination and subsumption mecha\u00adnisms. \nHowever, by encoding this relation on contexts as an induc\u00adtive predicate, we can express this directly \nand do away with world subsumption. We demonstrate this idea with an implementation of clo\u00adsure conversion. \nGuillemette and Monnier [2007] provide a good overview of closure conversion. Our implementation resembles \ntheirs, although we use HOAS for our term representations. The particulars of closure conversion are \nnot tremendously important here. We wish primarily to demonstrate how data-types comple\u00adment contextual \ntypes. The source language is the language of Section 2.1. The target language is augmented with constructs \nfor explicit closures, shown in part below: clam : (envr . ctm) . ctm. proj : envr . nat . ctm. close \n: ctm . envr . ctm. nil : envr. create : envr . ctm. snoc : envr . ctm . envr An envr is an arbitrary \nlength tuple of terms. Closure conversion will turn contexts into explicit ctm objects such that the \nbodies of lambda abstractions will only refer to their envr and no other variables. The reason is that \nwe want to be able to hoist them to the top level. close constructs an explicit closure which binds all \nbut the last argument. projE N projects out the Nth component of the environment E. We must now do as \nwe promised and turn the open bodies of lambdas into closed terms which instead project from their envi\u00adronment. \nTo characterize the free variables in the open bodies we de.ne the context cctx. We depart from Guillemette \nand Monnier [2007] by performing substitutions instead of let-bindings, which is easy thanks to HOAS. \nschema cctx = ctm; rec addProjs : (f:cctx) (N:nat[]) (M:cexp[f,e:envr]) . cexp[e:envr] = .f . . N . . \nM . case f of | [] . [e:envr]. M e | f,x:ctm . addProjs f (s N) (f,e.M..(proje N) e) We write [] for \nthe empty context and [e:envr] for a singleton context containing a single variable of type envr. Of \ncourse if we insist that contexts be passed explicitly as envi\u00adronments, we had better be able to turn \ncontexts into environments: rec ctxToEnv : (f:cctx) envr[f]= .f . case f of | [] . []. nil | f,x:ctm \n. let f. env .. = ctxToEnv f in f,x:ctm. snoc (env ..) x So far this is more or less standard Beluga \ncode. The raison d etre for this example is the recursive relation on contexts: datatype ctx_rel : ctx \n. cctx . ctype | rnil : ctx_rel [] [] | rsnoc : {.f} . ctx_rel .f . ctx_rel (.,x:tm) (f,x:ctm) We freely \nomit types where they might reasonably be inferred. ctx_rel .f states only that . and f are the same \nlength. We illustrate more sophisticated relations later. The closure conversion function takes vanilla \nterms in a context . into target language terms in a related context f. We only explain the cases for \nvariables and lambda-abstraction. The full implemen\u00adtation can be found in the appendix. rec conv : {.:ctx} \n(f:cctx) . ctx_rel .f . tm[.] . ctm[f]= .f . fn cr . fn m . case m of Variables are taken to corresponding \nvariables. When we learn that . is non-empty, we learn by inspecting cr that so too is f. The {f = ...} \nsyntax passes the implicit argument f explicitly. | . ,x:tm. x . let rsnoc {f=f ,x:ctm} _ = cr in f ,x. \nx | . ,x:tm. #p .. . let rsnoc {f=f ,x:ctm} cr = cr in let f .M .. =conv_cr (. . #p ..) in f ,x. M .. \nTo closure convert a lambda, we closure convert the body in the extended context .,x using an extended \ncontext relation rsnoc cr. We close it and return an explicit closure. For simplicity, we cheat and do \nnot compute the set of variables occuring in the body: we instead close over the entire context. | .. \nlam (.x. M .. x) . let f, x:ctm. M .. x = conv _ (rsnoc cr) (.,x. M..x) in let [ev:envr]. M ev = addProjs \n_ 0 (f, x:ctm, ev:envr. M .. x) in let f. Env .. = ctxToEnv f in f. close (clam (.ev. M ev)) (Env ..) \nWe omit here the case for applications which is straightforward. A more sophisticated example of a context \nrelation appears if we wish to express that a transformation is type preserving. In type-preserving closure \nconversion (see Minamide et al. [1996] and Guillemette and Monnier [2007]), there is also a non-trivial \nrelation between source language types and target language types. Assuming now that we use intrinsically-typed \nterms, we might wish to use a relation such as the following: datatype ctx_rel : ctx . cctx . ctype = \n| rnil : ctx_rel [] [] | rsnoc : {.f} {T:tp[]} {S:ctp[]} . ctx_rel .f . tp_rel T S . ctx_rel (., x:tm \nT) (f, x:ctm S)  2.3 Logical Relations: Normalization by Evaluation Implementing normalization proofs \nor normalization by evaluation (NbE) in a system such Beluga has been dif.cult to do directly. Here we \ndemonstrate an implementation of typed normalization by evaluation [Berger and Schwichtenberg 1991] in \nour language which supports both contextual types and indexed data-types. The essence of normalization \nby evaluation is to normalize object level terms by reusing the evaluation of the computation level. \nThere are therefore two levels of terms: object language terms using the LF function space, and computation \nlevel (semantic) terms using the computation level function space. Normalization proceeds by interpreting \nobject level terms as semantic terms and reifying the result. The source language is tm: a standard (intrinsically) \nsimply\u00adtyped family of LF-level terms with lam and app as constructors.  We have an open type atomic_tp \nof atomic (base) types which our implementation is essentially parametric over. The target is simply\u00adtyped \nterms in \u00df-normal .-long form: nlam : (neut T . norm S) . norm (arr T S). rapp : neut (arr T S) . norm \nT . neut S. embed : neut (atomic P) . norm (atomic P). To characterize the free variable context of the \nsource language, we de.ne schema ctx = some [T:tp] tm T. The context of the tar\u00adget language is described \nby schema tctx = some [T:tp] neut T. Semantic terms must be de.ned as a datatype, because we must use \nthe computation-level function space: type sub .f = {T:tp[]} . #(neut T)[.] . #(neut T)[f] datatype sem \n: ctx . tp[] . ctype = | syn :{.} {P:atomic_tp[]} . (neut (atomic P))[.] . sem . (atomic P) | slam : \n{. A B} ({f} . sub .f . sem f A . sem f B) . sem . (arr A B)  The type {T:tp[]} . #(neut T)[.] . #(neut \nT)[f] is read as the type of (type preserving) substitutions of variables in . for variables in f. The \nneed for this will become clear in our imple\u00admentations of substitution and rei.cation. Observe that \nwe restrict the embedding of neut into sem to atomic types. This enforces .-longness. We must manually \nimplement substitution of variables for vari\u00adables in sem, since we do not provide substitution for free \nfor datatypes. In fact, it is substantially different from LF substitution. Licata [2011] explains this \ndifference in depth. The interesting case is slam. It makes essential use of the quan\u00adti.cation over \nsubstitutions in slam. Similar mechanisms appear in both Licata and Harper [2009] and Pouillard and Pottier \n[2010]. rec subst:{.f S} . sub .f . sem . S . sem f S= fn s . fn e . case e of | syn (.. R ..) . nsubst \ns (.. R ..) | slam f . slam (fn s . fn s . f(s . s) s) rec nsubst : {.f S} . sub .f . (neut S)[.] . \n(neut S)[f] = ... Where nsubst performs substitution on syntactic neutral terms. We show the full code \nfor this example in the appendix. Embedding neut into sem is only possible at atomic types, so we must \n.-expand in the general case: rec reflect : (. A) (R:(neut A)[.]) . sem . A= .. . . A . . R . case []. \nA of | []. atomic P . syn (.. R ..) | []. arrTS . slam (. {f} . fn s . fn s . let f. R .. = nsubst s \n(.. R ..) in let f.N..=reify _T s in reflect _ S (f. rapp (R ..) (N ..))) We can then reify semantic \nterms as object level terms by calling the computation level functions on fresh variables: rec reify \n: (. A) . sem . A . (norm A)[.]= .. . . A . fn s . case []. A of | []. atomic P . let syn (.. R..)=s \nin .. embed (R ..) | []. arrTS . let slamf= s in let .,x:tm T. E..x = reify (f weaken (reflect _ T (.,x:neut \nT. x))) in .. nlam (.x.E ..x) Where weaken is the weakening substitution of type: {.:ctx} {S:tp[]} . \nsub . (.,x:tm S) We can now implement evaluation with the help of an environ\u00adment of bindings. The lam \ncase evaluates in the extended environ\u00adment. In the application case, the evaluation of E1 must produce \nan slam since syn is only applicable to atomic types. rec eval : {.f S} . . ({T} #(tm T)[.] . sem f T) \n . (tm S)[.] . sem f S= fn r . fn s . fn e . case e of | .. #p .. . s (. . #p .. ) | .. lam (.x. E .. \nx) . slam (fn s . fn s .  eval (extend ((subst s ) . s) s) (.,x.E .. x) | .. app (E1 ..) (E2 ..) . let \nslam f = eval s (.. E1 ..) in f id (eval s (.. E2 ..)) We have used extend to extend the domain of the \nenvironment in the lam case. Its type is shown below. rec extend : {.:tctx} {f:ctx} {S} . ({T} . #(tm \nT)[.] . sem f T) . sem f S . ({T} . #(tm T)[.,x:tm S] . sem f T)  Normalization is then simply evaluation \nfollowed by rei.cation: rec nbe : {A} . (tm A)[ ] . (norm A)[ ] = fn e . reify [] A (eval (fn y . impossible \ny) e) Notably, this implementation cleanly enforces .-longness and type preservation by employing dependent \ntypes. Expressing these invariants together with a clean approach to variable binding in NbE is rarely \nfound in other work. Licata and Harper [2009] and Shinwell et al. [2003] lack dependent types. Pouillard \nand Pottier [2010] have this ability, although their implementation is untyped NbE, and hence does not \nemploy it. Further, this illustrates that arbitrary mixing of computation and LF function spaces is not \ncrucial to NbE, as suggested by Licata and Harper [2009]. Their framework, however, obtains weakening \nfor free for sem while we have to do a modicum of work to implement it. From a logic perspective, this \ncan be seen as a (partial) com\u00adpleteness and consistency proof for a natural deduction system by the \nmethod of logical relations. We say partial because we post\u00adpone the issues of totality checking to future \nwork. We anticipate that this can be scaled to normalization proofs for the simply-typed lambda calculus \nwithout any dif.culty. The addition of indexed data-types hence brings substantial bene.t to programming \nand proof systems such as Beluga, since it makes such systems capa\u00adble of proofs by logical relations. \n3. A Review of Contextual LF Here we describe the index domain, which in our case is contextual LF [Pientka \n2008, 2011] which builds on contextual types which were .rst introduced in Nanevski et al. [2008b]. \n3.1 Contextual LF Contextual LF extends the logical framework LF [Harper et al. 1993] with the power \nof contextual objects . .M of type A[.]. M denotes an object which may refer to the bound variables listed \nin . and has type A in the context . (see also [Nanevski et al. 2008b]). . can be obtained from the context \n. by simply dropping the type annotations and keeping only the declared variable names. We characterize \nonly objects in \u00df. normal form, since these are the only meaningful objects in LF. Furthermore, we concentrate \nhere on characterizing well-typed terms, but de.ning kinds and kinding rules for types is straightforward \nand omitted. Atomic types P, Q ::= a M Types A, B ::= P | .x:A.B Heads H ::= x | c | p[s] Neutral Terms \nR ::= H | RN | u[s] Normal Terms M, N ::= R | .x.M Substitutions s ::= \u00b7| id. | s, M | s; H Contexts \n. ::= \u00b7| . | .,x:A  Normal objects may contain ordinary bound variables which are used to represent \nobject-level binders and are bound by .\u00adabstraction or in a context .. They may also contain meta-variables \nu[s] and parameter variables p[s] which we call contextual vari\u00adables. Contextual variables are associated \nwith a post-poned sub\u00adstitution s. The meta-variable u stands for a contextual object . .R where . describes \nthe ordinary bound variables which may occur in R. This allows us to rename the free variables occurring \nin R when necessary. The parameter variable p stands for a contextual object . .R where R must be either \nan ordinary bound variable from . or another parameter variable. In the simultaneous substitutions s, \nwe do not make its domain explicit. Rather we think of a substitution together with its domain . and \nthe i-th element in s corresponds to the i-th declaration in .. We have two different ways of building \na substitution: either by using a normal term M or a variable x. Note that a variable x is only a normal \nterm M if it is of base type. However, as we push a substitution s through a .-abstraction .x.M, we need \nto extend s with x. The resulting substitution s, x may not be well-typed, since x may not be of base \ntype and in fact we do not know its type. Hence, we allow substitutions not only to be extended with \nnormal terms M but also with variables x. Without loss of generality we require that meta-variables have \nbase type. A bound variable context . contains bound variable declara\u00adtions in addition to context variables. \nA context may only contain at most one context variable and it must occur at the left. This will make \nit easier to ensure bound variable dependencies are satis.ed in the dependently typed setting. Following \nPientka [2008] we use a bi-directional type system where we check normal terms against a type and synthesize \na type for neutral terms. LF objects may depend on variables declared in the context . and the meta-context \n. which contains contextual variables such as meta-variables u, parameter variables p and con\u00adtext variables \n.. We introduce . more formally in the next section. All typing judgments have access to both contexts \nand a well-typed signature S where we store constants together with their types and kinds. .; . f M . \nA Normal term M checks against type A .; . f R . A Neutral term R synthesizes type A .; . f s . .1 Substitution \ns has domain .1 and range .. The bi-directional typing rules are mostly straightforward and are presented \nin Figure 1. We will tacitly rename bound variables, and maintain that contexts and substitutions declare \nno variable more than once. Note that substitutions s are de.ned only on ordinary variables x and not \ncontextual variables. Moreover, we require the usual conditions on bound variables. For example in the \nrule for .-abstraction the bound variable x must be new and cannot already occur in the context .. This \ncan be always achieved via a-renaming. Similarly, in meta-terms we tacitly apply a-renaming. As is common, \nwe rely on hereditary substitutions, written as [N/x]A(B) (or [s].(B)) to guarantee that when we substitute \na term N which has type A for the variable x in the type B, we obtain a type B1 which is in normal form. \nHereditary substitutions continue to substitute, if a redex is created; for example, when replacing naively \nx by .y.c y in the object xz, we would obtain (.y.c y) z which is not in normal form and hence not a \nvalid term in our grammar. Hereditary substitutions continue to substitute z for y in cy to obtain cz \nas a .nal result. For a more detailed description of hereditary substitution, we refer the reader to \nfor example Nanevski et al. [2008b]. Finally, we remark on equality checking. When checking A = B we \nmust take into account .-contraction, because we have two Neutral Terms .; . f R . A .(x)= A .(p)=#A[F] \n.;. f s . F .; . f x . A .; . f p[s] . [s]FA S(c)= A .(u)= P [F] .;. f s . F .; . f c . A .; . f u[s] \n. [s]FP .; . f R . .x:A.B .; . f M . A .; . f RM . [M/x]AB Normal Terms .; . f M . A .; . f R . PP = \nQ .; .,x:A f M . B .; . f R . Q .; . f .x.M . .x:A.B Substitutions .; . f s . .1 .; . f s . F .;. f H \n. BB =[s]FA .; . f\u00b7.\u00b7 .; . f s; H . F,x:A .; . f s . F .;. f M . [s]FA .; ., . f id. . . .; . f s, M \n. F,x:A Figure 1. Typing for contextual LF ways to build substitutions. If x has type .y:A.B then we \nmay have written s; x or s, .y.x y.  3.2 Meta-Objects and Meta-types We lift contextual LF objects to \nmeta-types and meta-objects to treat abstraction over meta-objects uniformly. Meta-objects are ei\u00adther \ncontextual objects written as . .R or contexts .. These are the index objects which can be used to index \ncomputation-level types. There are three different meta-types: P [.] denotes the type of a meta-variable \nu and stands for a general contextual object . .R. #A[.] denotes the type of a parameter variable p and \nit stands for a variable object, i.e. either . .x or . .p[p] where p is a variable substitution. A variable \nsubstitution p is a special case for gen\u00aderal substitutions s; however unlike p[s] which can produce \na gen\u00aderal LF object, p[p] guarantees we are producing a variable. G de\u00adscribes the schema (i.e. type) \nof a context. The tag # on the type of parameter variables is a simple syntactic device to distinguish \nbe\u00adtween the type of meta-variables and parameter variables. It does not introduce a subtyping relationship \nbetween the type #A[.] and the type A[.]. The meta-context in which an LF object appears uniquely determines \nif X denotes a meta-variable, parameter vari\u00adable or context variable. We use the following convention: \nif X denotes a meta-variable we usually write u or v; if it stands for a parameter-variable, we write \np and for context variables we use .. ---. ---. Context schemas G ::= .(x:A).B | G + .(x:A).B Meta Objects \nC ::= ..R | . Meta Types U ::= P [.] | #A[.] | G Meta substitutions . ::= \u00b7| ., C/X Meta-context . ::= \n\u00b7| .,X:U ---. Context schemas consist of different schema elements .(x:A).B which are built using +. \nIntuitively, this means a concrete declara\u00adtion in a context must be an instance of one of the elements \nspec\u00adi.ed in the schema. For example, a context x:exp nat,y:exp bool will check against the schema .T \n:tp.exp T .  Meta Terms . f C . U .(.)= G . f\u00b7. G . f . . G . f . . G - ---. - --. .(x : B1).B . G .; \n. f s . (x:B1) A =[s]- --.B (x:B1) . f .,x:A . G .; . f s . F .;. f R . P .(x)= A . f . .s . F[.] . f \n. .R . P [.] . f . .x . #A[.] .(p)=#A[F] .;. f p . F[p]F(A)= B . f . .p[p] . #B[.] . f . . .1 . f . . \n.1 . f C . [ .] .1 (U) Meta-Substitutions . f\u00b7.\u00b7 . f ., C/X . .1,X:U Figure 2. Typing for meta-terms \nThe uniform treatment of meta-terms, called C, and meta\u00adtypes, called U , allows us to give a compact \nde.nition of meta\u00adsubstitutions . and meta-contexts .. We omit here the rules stating when meta-types \nand meta\u00adcontexts are well-formed and show only the typing rules for meta\u00adterms and meta-substitutions \nin Figure 2. A consequence of the uniform treatment of meta-terms is that the design of the computation \nlanguage is modular and parame\u00adterized over meta-terms and meta-types. This has two main advan\u00adtages: \nFirst, we can in principle easily extend meta-terms and meta\u00adtypes without affecting the computation \nlanguage; in particular, it is straightforward to add substitution variables which were present in Pientka \n[2008] or allow for richer context schemas. Second, it will be key to a modular, clean design of computations. \nThe single meta-substitution, written as [ C/X] U (*) where * stands for A, M, R, s, ., is de.ned inductively \non the struc\u00adture of the given object. (see for example Pientka [2011] or the appendix). We only discuss \nbrie.y here some of the fundamen\u00adtal ideas. Let us .rst consider the case where X stands for a meta-variable \nu and C is a meta-object . .R. We note that there are no capture issues when we push [ . .R/u] through \na lambda\u00adexpression and the only interesting issue arises when we encounter an object u[s]. In this case, \nwe apply [ . .R/u] to s to obtain s1 . Subsequently, we apply s1 to R to obtain the .nal result. Next, \nwe consider the case where X stands for a parameter vari\u00ad able p and C is a meta-object . .x or ..q[p]. \nThe only interest\u00ading case is when we encounter p[s]. Similar to the case for meta\u00advariables, we apply \nthe meta-substitution to s to obtain s1 and sub\u00adsequently apply s1 to x or q[p]. There is however a small \ncaveat: since s1 is an arbitrary substitution, applying it to x, may yield a normal object M. Hence, \nsimply returning M may produce a non\u00adnormal term which is not meaningful in our grammar. The solution \nto this problem is to de.ne meta-substitutions hereditarily; hence we index the meta-substitution with \nits domain. Finally, the case where X stands for a context variable . and C is a meta-object .. There \nare two interesting cases: 1) when we encounter the identity substitution id., we unroll . and create \nat the same time a concrete identity substitution which maps all variables from . to themselves. 2) when \nwe encounter a context variable . in a context, then we simply replace it with the concrete context .. \nThe full de.nition of meta-substitutions is given in the appendix and has been previously described in \n[Nanevski et al. 2008b; Pientka 2008, 2011]. The simultaneous meta-substitution, written as [ .] , is \na straight\u00adforward extension of the single substitution. Theorem 3.1 (Meta-substitution property). If \n.1 f . . . and .; . f J then .1;[ .] .. f [ .] .J. 4. Beluga\u00b5: a language with binding support and recursive \ntypes We present in this section a dependently typed programming lan\u00adguage Beluga\u00b5 along the lines of \nMini-ML, including recursive types, variants and general pattern matching which is critical in practice \nand whose theory in this setting is non-trivial. Polymor\u00adphism, on the other hand, is largely orthogonal \nand therefore post\u00adponed. The type index objects are drawn from the domain of meta\u00adobjects presented \nin the previous section, but we emphasize that this language is parametric over the index domain, requiring \nonly decidable equality. 4.1 Types and Kinds Our type language supports function types (written as T1 \n. T2), -. products (written as T1 \u00d7 T2), labelled sums (written as (l:Th), de\u00adpendent function types \n(written as .X:U.T ) and dependent prod\u00aduct types (written as SX:U.T ). We only allow dependencies on \nmeta-terms not on arbitrary computation-level expressions. The novel part in our type language is our \nde.nition of recursive types together with the equality constraint which may be associated with a given \ntype. The recursive type is written as \u00b5Z.. M X.T ; while Z denotes a type variable, .M X.T describes \na type-level function which expects meta-terms.  Kinds K ::= ctype | .X:U.K --. Types T ::= Unit | Z \n| T1 . T2 | T1 \u00d7 T2 |(l : T h | .X:U.T | SX:U.T | C1 = C2 . T M | \u00b5Z.. MC | U X.T | T Context G ::= \u00b7| \nG,Z : K | G,x : T We note that we can directly refer to meta-types and embed them in our computation-level \ntypes. Hence meta-objects can be directly analyzed and manipulated by our computation language. This \nis convenient in our setting, however, it also prevents a naive erasure of all the meta-objects. We also \nnote that equalities cannot occur just by themselves in our grammar. The reason is that equalities are \ntreated silently during type checking and equality proofs which establish C1 = C2 do not pollute our \ncomputation-level expressions. In fact, equalities typically occur inside a recursive type and they are \ntrivially true once we have chosen the correct instantiation for the existentially quanti.ed variable. \nWe illustrate this idea shortly. To illustrate, we give here three examples. Example 1 The type of a \nvector of booleans which keeps track of their length can be de.ned as follows. We assume an LF signature \nwhich declares nat:type together with two constants, 0:nat and s: nat . nat. Because all index objects \nto the recursive type V are closed, we omit writing the empty context and simply write nat for nat[] \nand Y for \u00b7.Y . Vec = \u00b5Vec..X. ( nil : X =0 . Unit , cons :SY :nat.X = s Y . bool \u00d7 Vec Y h  Example \n2 The relation between contexts from Section 2.2 can be written as follows: \u00b5Ctx rel....f. ( rnil : . \n= \u00b7. f = \u00b7. Unit , rsnoc:S.1:ctx.Sf1:cctx. . = .1 ,x:tm . f = f1 ,y:ctm . Ctx rel .1 f1 h Our treatment \nof recursive types with equalities is similar to Xi et al. [2003] and Sulzmann et al. [2007] where recursive \ntypes are endowed with equalities between types to model GADTs. Recently .xed points with equalities \nbetween terms have appeared in Licata [2011] and in Baelde et al. [2010] for example. In our setting, \nwe treat equality between contextual objects. Next, we give the kinding rules for computation-level types. \nWell-kinded computation-level types .; G f T : K --. --. .,X:U ;G,Z :.X:U.K f T : K --. .- .; G f \u00b5Z. \n.X . T :.X:U.K .; G f T1 : ctype .; G f T2 : ctype * .{., \u00d7} .; G f T1 * T2 : ctype .; G f U . mtype \n.,X:U ;G f T : ctype .; G f .X:U.T : ctype .; G f U . mtype .,X:U ;G f T : ctype .; G f SX:U.T : ctype \n. f C1 . U . f C2 . U .; G f T : ctype .; G f C1 = C2 . T : ctype . f U . mtype .; G f U : ctype .; G \nf Unit : ctype G(Z)= K .; G f T :.X:U.K . f C : U .; G f Z : K .; G f TC :[ C/X] K  4.2 Computations \nOur language of computations includes recursion (written as rec f.E), nameless functions (written as \nfn x.E) and dependent functions (written as .X. E). We also include pairs (written as (E1,E2)) and dependent \npairs (written as pack (C, E)). Finally, we include labeled variants (written as (l = Eh) and a fold \ncon\u00adstructor for recursive types. Expressions I ::= y | IE | IC | (E : T ) (synth.) Expressions E ::= \nI | C | fn y.E | .X. E | rec f.E | unit (checked) | fold E |(l = Eh| pack (C, E) | (E1,E2) | case I of \nBMPattern pat ::= x | C | unit | fold pat |(l = path | pack (C, pat) | (pat1, pat2) Branch B ::= .;G \n. pat : . . E Contexts G ::= \u00b7| G,y:T Our language is split into expressions for which we synthesize \ntypes and expressions which are checked against a type. This min\u00adimizes the necessary type annotations \nand provides a syntax di\u00adrected recipe for a type checker. Intuitively, the expressions which introduce \na type are expressions which are checked and expres\u00adsions which eliminate a type are in the synthesis \ncategory. We have two different kinds of function applications, one for applying computation-level functions \nto an expression and the other to apply a dependent function to a meta-object C. Pairs and dependent \npairs are analyzed by pattern matching. .; G f I . T Expression I synthesizes type T y:T . G .;G f I \n. C = C . T .; G f y . T .; G f I . T .; G f I . T2 . T .; G f E . T2 .; G f IE . T .; G f I . .X:U.T \n. f C . U .; G f E . T .; G f IC . [ C/X] T .; G f (E : T ) . T .; G f E . T Expression E checks against \ntype T --. .; G f Ei . Ti where li : Ti . l : T .; G,f : T f E . T --. .; G f(li = Ei).(l : T ) .; G \nf rec f.E . T .; G,y:T1 f E . T2 .,X:U;G f E . T .; G f fn y.E . T1 . T2 .; G f .X. E . .X:U.T .; G f \nE1 . T1 .; G f E2 . T2 .; G f I . TT = T 1 .; G f (E1,E2) . T1 \u00d7 T2 .; G f I . T 1 . f C . U .; G f E \n. [ C/X] T . f C . U .; G f pack (C, E) . SX:U.T .; G f C . U .; G f E . [\u00b5Z..X . S/Z][[X C/X ] S .; \nG f E . T .; G f fold E . (\u00b5Z..X . S) CX.; G f E . C = C . T .; G f I . S for all i .; G f Bi . S . T \n.; G f case I of X B . T .; G f B . S . T Branch B with pattern of S checks against T .i f .i . ..i;Gi \nf pat . [ .i] S .i;[ .i]]G, Gi f E . [ .i] T .; G f .i;Gi . pat : .i . E . S . T Figure 3. Typing for \ncomputations Branches are modelled by .; G . pat : . . E where . describes the meta-variables occurring \nin the pattern which are often left implicit in the surface language, while G corresponds to the explicit \narguments. The re.nement substitution . describes how the type of the scrutinee is instantiated so the \ngiven branch is applicable. Example 3 We show next the elaboration of a simple program to compute the \ntail of a vector and its elaboration: rec fn tail : {N:nat[]} . vec (s N) . vec N = l . case l of cons \nh t . t Which can be elaborated into: rec tail..N. fn l. case l of | M : nat ; h : bool, t:Vec M . fold \n( cons = pack (M, (h, t) ) h : M/N . t We insert the length argument to cons which was left implicit \nin the source-level program and the re.nement M/N which guarantees that the type of the pattern is compatible \nwith the type of the scrutinee. We also list explicitly the type of the index variable M as well as the \ntype of the arguments h and t.  4.3 Typing rules Next, we give the typing rules for computations in \nFigure 3. We present bi-directional typing rules for computations which will minimize the amount of typing \nannotations. We distinguish be\u00adtween typing of expressions and branches. In the typing judgment, we will \ndistinguish between the context . for contextual variables from our index domain and the context G which \nincludes decla\u00adrations of computation-level variables. Contextual variables will be introduced via .-abstraction. \nThe contextual variables in . are also introduced in the branch of a case-expression. Computation-level \nvariables in G are introduced by recursion or functions and in addi\u00adtion in branches. We use the following \njudgments:  .; G fE. T Expression E checks against type T .; G fI . T Expression I synthesizes type \nT .; G fB. S . T Branch B with pattern of type S checks against T The typing rules are given in Figure \n3. We will tacitly rename bound variables, and maintain that contexts declare no variable more than once. \nMoreover, we require the usual conditions on bound variables. For example in the rule for .-abstraction \nthe contextual variable X must be new and cannot already occur in the context .. This can be always achieved \nvia a-renaming. Similarly, in the rule for recursion and function abstraction, the variable x must be \nnew and cannot already occur in G. The rules which synthesize and check are mostly standard and we only \npoint out a few rules. We have two rules for applications: to synthesize the type S of a non-dependent \napplication (IE), we synthesize the type for I to be T . S and check E against T . For the dependent \napplication IC, we synthesize the type .X:U.T for I and check that C is a well-typed meta-object of type \nU. Note that we drop the computation context G when we transition to type check a meta-object, since \nmeta-objects cannot refer to computations. The .nal type for IC is [ C/X] T . If we have synthesized \na type T together with a trivial equality constraint, we simply drop the constraint and return T . To \ncheck a .-abstraction against .X:U.T , we add X:U to the meta-context . and continue to check that the \nbody of the abstraction has type T . To check that a function fn y.E has type T1 . T2, we add the assumption \ny:T1 to the computation context G and continue to check the body E against T2. For checking a non-dependent \npair (E1,E2) against the type T1 \u00d7 T2, we check each part of the pair against their respective type. \nFor checking a dependent pair pack (C, E) against SX:U.T , we check that C is a well-typed meta-object \nof type U switching to the typing rules for the meta-level and dropping the context G. In addition, we \ncheck that E has type [ C/X] T . When we check a contextual meta-object C against a type U, we simply \nconvert to the type checking rules for meta-objects and forget about the computation-level context G. \nOur typing rules will ensure that meta-objects are pure objects and do not contain any computation-level \nexpressions. When checking an expression against C = C . T , we can simply drop the constraint C = C, \nsince it is trivially true. We check (fold E) against the recur\u00ad sive type (\u00b5Z.. MCMby unrolling the \n.xed point de.nition X.S) and checking E against [\u00b5Z.. MC/ M X. S/Z][[ MX] S. The difference to simply-typed \nrecursive types is that dependently typed recursive types are applied to index objects CM.  To illustrate \nthat our data carries enough information to ensure that the equality constraints are trivially true, \nif the term is well\u00adtyped, we show the typing derivation for cons (true,nil) in Fig\u00adure 4. In the rule \nfor case expressions, we .rst infer the type S of the scrutinee and then proceed to check that each branch \nBi has a pattern compatible with S and its body has a type compatible with T . To check a branch .i;Gi \nf pati : .i . Ei, we check that the re.nement substitution .i provides instantiations from the outer \nmeta-context . to the current meta-context .i. Moreover, the pattern has type [ .i] S, i.e. it is compatible \nwith the type of the scrutinee, and only refers to the local variables .i and Gi. We then proceed to \ncheck the body Ei against [ .i] T . Because the pattern may re.ne the types, we must make sure to apply \n.i to the appropriate parts and extend the computation context [ .i]]G with the bindings Gi introduced \nin the branch. The typing rules for patterns are given in Fig. 5. They duplicate some of the type checking \nrules for tuples, dependent pairs, meta\u00adobjects, recursive types, and variants. We ensure that the compu\u00adtation \nvariables occurring in patterns occur uniquely and we split the computation context in the rule for tuples. \nThe meta-context on the other hand remains. As a consequence, contextual variables may occur more than \nonce, which is also often necessary to obtain well-typed expressions, but we enforce linearity for computation \nvariables occurring in patterns. Theorem 4.1 (Decidability of Type Checking). Type-checking computation-level \nexpressions is decidable. Proof. The typing judgments are syntax-directed and therefore clearly decidable. \n 4.4 Big-step operational semantics Next, we de.ne the operational semantics for computations in Fig. \n7. We adopt an environment-based approach where we do not eagerly propagate values. Recall that we distinguish \nbetween meta-variables in . and program variables in G. To work elegantly with re.nement substitutions \nin branches, we hence de.ne two environments: . denotes the instantiation for meta-variables in .; . \nprovides instantiations for program variables in G. Values V ::= F [. ; .] | unit | fold V |(l = V h \n| pack (C, V ) | (V1,V2) Function Values F ::= fn y.E | .X. E Extended Values W ::= V | (rec f.E)[. ; \n.] Closures L ::= E [. ; .] Environments . ::= \u00b7| ., W/y Values are either meta-terms C, unit, pairs \n(V1,V2), dependent pairs pack (C, V ), variants (l = V h, fold V , or functions as clo\u00adsures. Since we \nhave non-dependent and dependent functions, we have two corresponding closures. Closures are snapshots \nof com\u00adputation inside an environment. The environment is represented by the two suspended substitutions \n. and . for each of the two contexts . and G respectively. We write E[.; .] for a closure consisting \nof the expression E and the suspended meta-substitution . and the program environment .. The intended \nmeaning is that .rst meta\u00adsubstitution . is applied to E and then ordinary substitution . to the result. \nFor clari.cation, we show the typing for environments and values in Figure 6. We give a big-step semantics \nfor computations in Figure 7. To evaluate a variable y in the environment . and ., we simply look up \nits binding in the computation environment .. Since . contains extended values, in particular y may be \nbound to a recursive func\u00adtion which in itself is not a valid result, we continue to evaluate the extended \nvalue we retrieve from . to a proper value. When we encounter a meta-object C in the environment . and \n., we apply . to C to compute a closed meta-object. unit simply evaluates to it\u00adself regardless of the \nenvironment. Evaluating a function fn y.E in the environment . and . simply returns the closure fn y.E \n[. ; .]. When evaluating a recursive function rec f.E in an environment . and ., we evaluate the body \nE and extend the computation envi\u00adronment . binding f to itself. Evaluating a tuple (E1,E2) in the environment \n. and . is straightforward: we evaluate E1 in the environment . and . and we proceed similarly to evaluate \nE2. The evaluation rules for fold and variants are straightforward.  () . Unit () . 0=0 . Unit f( nil \n= ()h.(nil :0=0 . Unit , cons :SY :nat.0= s Y . bool \u00d7 Vec Y h f true . bool f fold ( nil = ()h. Vec \n0 f (true, fold ( nil = () h ) . bool \u00d7 Vec 0 f 0 . nat f (true, fold ( nil = () h ) . s 0= s 0 . bool \n\u00d7 Vec 0 f pack (0, (true, fold ( nil = () h )) . SY :nat.s 0= s Y . bool \u00d7 Vec Y f( cons = pack (0, (true, \nfold ( nil = () h )) h.(nil : s 0=0 . Unit , cons :SY :nat.s 0= s Y . bool \u00d7 Vec Y h f fold ( cons = \npack (0, (true, fold ( nil = () h )) h. Vec (s 0) Figure 4. Typing derivation for vector cons (true, \nnil) := fold ( cons = pack (0, (true, fold ( nil = () h )) h .; G f pat . T Pattern checks against type \nT T = \u00b5Z.. M.; G f pat . [T/Z][[ MX] S X.S C/ M G(x)= T . f C . U .; G f pat . T .; G f x . T .; G f \nC . U .; G f unit . Unit .; G f fold pat . TC .; G f pat . C = C . T M --. .; G f pati . Ti where li \n: Ti . l : T . f C . U .; G f pat . [ C/X] T .; G1 f pat1 . T1 .; G2 f pat2 . T2 --. .; G f(li = patih.(l \n: T h .; G f pack (C, pat) . SX:U.T .; G1, G2 f (pat1, pat2) . T1 \u00d7 T2 Figure 5. Typing rules for patterns \nE [. ; .] . V Expression E in environment [.; .] evaluates to value V I1 [. ; .] . (fn y.E)[.1 ; .1] \nE2 [. ; .] . V2 E [.1 ; .1,V2/y] . VI [. ; .] . (.X. E)[.1 ; .1] E [.1, [ .] C/X ; .1] . V (I1 E2)[. \n; .] . V (IC)[. ; .] . V .(y)= V.(y)=(rec f.E)[.1 ; .1] .(y) . VE [. ; .] . VE [. ; .] . V C [. ; .] \n. [ .] C unit [. ; .] . unit y [. ; .] . Vy [. ; .] . V (E : T )[. ; .] . V (fold E)[. ; .] . fold V \nE [. ; .] . VE [. ; ., (rec f.E)[. ; .]/f] . V (l = E) [. ; .] .(l = V ) (fn y.E)[. ; .] . (fn y.E)[. \n; .](.X. E)[. ; .] . (.X. E)[. ; .](rec f.E)[. ; .] . V . .i f . case I of X = .i B [. ; .] . V pack \n(C, E)[. ; .] . pack ([[.] C, V )(E1,E2)[. ; .] . (V1,V2) case I of (.i;Gi . pat : .i . Ei | BX)[. ; \n.] . V . E [. ; .] . VE1 [. ; .] . V1 E2 [. ; .] . V2 . . . i.i1 =.i f . i.i1 I [. ; .] . V1 case I \nof BX[. ; .] . VI [. ; .] . V1 Ei [[[.11] .1 ; ., .1] . V .i f . = .i/(.1;.1 ) ;[ .1]]Gi f V1 [ .1] \npat = .i/(.1;.1 ) ;[ .1]]Gi f V1 =[ .1] pat/(.11; .1) case I of (.i;Gi . pat : .i . Ei | BX)[. ; .] . \nV case I of (.i;Gi . pat : .i . Ei | BX)[. ; .] . V Figure 7. Big-step semantics We have two rules for \nevaluating applications: the .rst is for a non-dependent application I1 E2 in an environment [.; .]. \nWe .rst evaluate I1 in the given environment obtaining a closure (fn y.E)[.1 ; .1]. Then we evaluate \nE2 to a value V2 and .nally proceed to evaluate E in the extended environment [.1 ; .1,V2/y] where the \nmeta-substitution .1 remains unchanged. On the other hand, when evaluating a dependent application IC \nin an environ\u00adment [. ; .], we evaluate I to a closure (.X. E)[.1 ; .1]. We now extend the meta-substitution \n. with the binding [ .] C/X and evaluate E in the extended environment [.1, [ .] C/X ; .1] where the \ncomputation environment .1 remains unchanged. The most interesting cases are those for case-expressions. \nA branch may be skipped if either the type of the scrutinee and the type of the pattern are not compatible, \ni.e. the current meta\u00adsubstitution and the re.nement substitution in the given branch do not unify, or \nif the types are compatible then the scrutinee itself may still be incompatible with the pattern of the \ncurrent branch. Evaluating a case expression, we .rst evaluate the scrutinee I in the current environment \n[. ; .] to some value V1. Next, we check that the current meta-substitution . is uni.able with the re.nement \nsubstitution .i of the given branch. This is written as . .i f . = .i/(.1 ;.i1 ) and .1 is the result \nof unifying . with .i s.t. . =[ .1] .i and .1 is a substitution which maps contextual variables from \n.i to .1 i. Unifying the contextual substitutions ensures that the type of the scrutinee and the type \nof the pattern are compatible. Next, we check that the pattern is compatible with the  V : T Value V \nhas type T \u00b7f . . . . :[ .]]G .;G f F . T \u00b7f C . U F [. ; .]: [ .] TC : U unit : Unit where li : Ti . \n--. C/ X--. V : Ti l : TT = \u00b5Z..X.SXV :[T/Z][[ XX] S X (li = V ) : (l : T ) fold V : TC V : T \u00b7 f C . \nU V : [ C/X] T V1 : T1 V2 : T2 V : C = C . T pack (C, V ) : SX:U.T (V1, V2) : T1 \u00d7 T2 L : T Closure L \nhas type T \u00b7f . . . . :[ .]]G .;G f E . T or .; G f E . T E [. ; .] : [ .] T . : G Environment . has \ndomain G . : G W : T \u00b7 : \u00b7 (., W/y) : G, y:T Figure 6. Value and closure typing value of the scrutinee. \nBefore matching the value of the scrutinee against the the pattern, we apply the contextual substitution \n.1 to the pattern pat and also to the variables listed Gi which occur in the pattern. The result will \nbe a contextual substitution .11 for the meta-context .1 i and a substitution .1 for actual pattern variables \nfrom [ .1]]Gi. Finally, the body of the branch Ei is evaluated. Recall that if the overall case-expression \nhas type T in a meta-context . and computation context G, then Ei has type [ .i] T in a meta\u00adcontext \n.i and computation context [ .i]]G, Gi. Therefore, we will now evaluate Ei in the contextual environment \n[ .11] .1 and extend the computation environment . with the new bindings in .1 . We now proceed to prove \nsubject reduction which guarantees that types are preserved during evaluation. Theorem 4.2 (Subject reduction). \nLet L : T . If L . V then V : T . Proof. Structural induction on L . V . To prove progress, we follow \nCousot and Cousot [1992] and Leroy and Grall [2009] and extend our big-step operational seman\u00adtics to \nallow for non-terminating computations. In addition to the judgment E [. ; .] . V we also allow for diverging \ncomputation using the judgment E [. ; .] .8. For example, the diverging eval\u00aduation rules for products \nare shown below: E1 [. ; .] .8 E1 [. ; .] . V1 E2 [. ; .] .8 (E1,E2)[. ; .] .8 (E1,E2)[. ; .] .8 These \nrules should be read coinductively. We note that matching . described by = and the substitution operation \ndo not lead to non\u00adtermination in our operational semantics. Lemma 4.3 (Canonical Forms). 1. If V : T \n. S then V is of the form: (fn y.E)[. ; .] 2. If V :.X:U.T then V is of the form: (.X. E)[. ; .]  Proof. \nBy inversion on value typing. Assuming that patterns cover all cases, we .nally can state and prove progress. \nTheorem 4.4. If L : T and for all values V , \u00acL . V then L .8 Proof. (Classical) By coinduction and case \nanalysis on the typing derivation, appealing to canonical forms. Corollary 4.5 (Progress). If L : T then \neither L . V or L .8 . 5. Mechanization We have mechanized the proofs of the subject reduction and progress \ntheorems presented in Section 4.4 in the Coq proof assis\u00adtant. The development is approximately 800 lines \nof speci.cation and under 500 lines of proof. We refer the interested reader to the supplementary material \nfor this paper for the Coq proofs. We do not formalize contextual LF. Rather, the proofs are ab\u00adstract \nover the domain of meta-terms C and meta-types U. We need only assume that they behave well with meta-substitution \nand that pattern matching for C is decidable. This demonstrates our point that the computation language \nforms a general core for dependently typed languages parametric over the domain C. We use the fresh look \napproach to binding due to Pouillard and Pottier [2010], which is an abstract interface to well-scoped \nde Brujin indices. Informally we found that the additional abstraction pushed us towards a more high-level \nalgebraic approach relying heavily on simultaneous substitutions in place of low level de Bruijn shifting. \nA natural question to ask is why we did not choose to for\u00admalize the language in a system with built-in \nsupport for binding and substitution such as Beluga. One answer is that Beluga is cur\u00adrently lacking \nthe recursive types we propose here. We would ar\u00adgue that the computation language is best represented \nas a recur\u00adsive type, since the syntax contains meta-substitutions ., which are conveniently represented \nas computational functions #tm[.] -> tm[f] which cannot be written in LF. In fact we never perform substitutions \non computation level expressions, hence the lack of substitution for free is moot. 6. Related Work Over \nthe past two decades, programming language researchers have been investigating language-based approaches \nto design safe and reliable software. Our work draws on and combines two domains: programming with binders \nand programming with indexed types. Our work follows the tradition of indexed types [Xi and Pfen\u00adning \n1999; Zenger 1997] where we separate the index domain of types from the computation-language. This has \nseveral known advantages: it is easy to allow state, exceptions, and polymor\u00adphism. Moreover, we are \nnot restricted to total functions as in full dependently-typed languages such as Coq [Bertot and Cast\u00b4eran \n2004] or Agda [Norell 2007]. However, since we can pattern match on index objects, we cannot naively \nerase them. Most closely related to our work is the work by Chen and Xi [2005]; Xi [2004] and Sarkar \n[2009]. The ATS system designed by Xi and collaborators [Xi 2004] allows programmers to specify formal \nsystems within the logical framework LF and embed LF ob\u00adjects as indices in computation-level types (see \n[Donnelly and Xi 2005]). The programmer can then supply her own proofs witness\u00ading the equality between \ntwo objects when automatic constraint solving in ATS fails. However, the major challenge when manip\u00adulating \nand traversing LF objects is that we will encounter open LF objects, i.e. LF objects which may contain \nfree variables. Unfor\u00adtunately, ATS does not provide support for manipulating such open LF objects. To \nsupport certi.ed programming, Sarkar [2009] pro\u00adposed ML/LF which extended an ML-like language with LF \nas an index domain. To allow programming with open LF objects, he pro\u00adposes to reify the dynamic assumptions \nwhich arise when travers\u00ading a binder and manipulate their representatives explicitly. This requires \nan extension of LF with Sigma-types and unit to model contexts and their dependencies. In contrast, our \nwork builds on contextual LF [Pientka 2008] and explicitly supports contexts and parameter variables \nand types for them. This allows us to express strong invariants by for example stating that we map variables \nfrom a context to variables in another context which seems dif.cult in Sarkar s approach, since there \nis no guarantee by the underlying type system that we are only storing and manipulating variables in \nthe rei.ed context.  Westbrook et al. [2005] also suggests to index types with LF objects in a type-safe \nfunctional language to support programming with proofs in the presence of unrestricted recursion and \nimperative features while retaining decidable type checking. However their work restricts LF to the .rst-order \nfragment and explicitly forbids .-abstractions. As a consequence, encodings based on higher-order abstract \nsyntax are not supported. In recent years, we have also made substantial advances in programming with \nbinders. We build on the idea of contextual types which is central to Beluga [Pientka 2008]; however \nso far, Beluga and similar systems such as Delphin [Poswolsky and Sch\u00a8 urmann 2008] are limited to only \nmanipulating contextual LF objects. We take it in this paper one step further of allowing contextual \nobjects and contexts as indices to computation-level types. Recently, Licata and Harper [2009] have proposed \na system where one can mix computation functions and binding abstractions; this builds on their earlier \nideas in Licata et al. [2008]. A prototype based on these ideas is implemented as a library within Agda \nand has been used to for example implement normalization by evaluation. Structural properties such as \nweakening or substitution do however not hold in general, but they can be implemented generically. While \nLicata and Harper [2009] demonstrate convincingly that their library within Agda elegantly supports programming \nwith binders, it is less clear whether their prototype will scale to support dependent types and meta-reasoning. \nIt is also remarkable that we do not need to fully mix LF function space with the function space for \ncomputations to implement normalization-by-evaluation. Taking a broader view on programming with binders, \nour work also seems super.cially similar to Pouillard and Pottier [2010] where the authors describe an \nAgda library to support safe pro\u00adgramming with names. The fundamental idea is to index terms with a world \nwhich names inhabit. When traversing a binding construct, we build up a chain of worlds which is similar \nto our context. How\u00adever, it is unclear whether their work scales to support dependent types and hence \nencoding proofs and meta-reasoning. We hope that the presented work will enable us to shed further light \non the rela\u00adtionship between nominal systems and LF-style encodings. An interesting application of the \npresented work is its use as a tactic language for an interactive theorem prover. Stampoulis and Shao \n[2010] for example propose a language VeriML where we write computations about .HOLind, a higher-order \nlogic with in\u00adductive de.nitions. At this point, VeriML only allows direct pat\u00adtern matching on .HOLind \nobjects, but does not allow in general computation-level types to be indexed with .HOLind objects. As \na consequence, recursive types in VeriML cannot be dependently typed. Our approach of adding dependently-typed \ninductive types to the computation language is directly applicable to their work and would add additional \n.exibility to their language. Our work takes inspiration from the work on inductive de.\u00adnitions in proof \ntheory, in particular the work by McDowell and Miller [2002] and more recently Baelde et al. [2010]; \nGacek et al. [2008] which targets reasoning about higher-order abstract syntax representations. Our addition \nof recursive types in Beluga\u00b5 gives us effectively the same power as .xpoint de.nitions in their work. \nTo facilitate reasoning about HOAS representations, Miller and his collaborators have extended the logic \nitself with the V-quanti.er which allows generic quanti.cation. Contexts must be modelled and reasoned \nabout explicitly. In contrast, our work allows us to re\u00admain in .rst-order logic by generalizing the \nterm language to allow for contextual meta-objects and contexts. This allows us to parame\u00adterize our \nrecursive types over contexts and provide explicit support for reasoning about contexts. We believe the \ndescribed work is an important step of understanding the differences and similarities be\u00adtween the approaches \nbased on proof theory on the one hand and the approaches grounded in type theory on the other hand. 7. \nConclusion and future work We presented a type-theoretic foundation for programming with binders and \nindexed data-types. In particular, we have shown how to add indexed recursive types to the Beluga language \nand proven the extension to be type safe. We have also mechanized the type safety proof in Coq. There \nare however more general lessons: we have streamlined earlier presentations of Beluga by separating meta-objects \nfrom computations. This has two important consequence: .rst, our com\u00adputation language becomes modular; \nwe can in fact easily replace contextual objects by another decidable index domain. Second, our modular \napproach lays the groundwork for adding contextual ob\u00adjects to other languages richer than the ML-like \ncomputation lan\u00adguage we used in this paper, such as Agda. In this paper, we have concentrated on programming \nwith binders and indexed data-types, however frameworks such as Bel\u00aduga are also proof development environments. \nTo use the presented language as a core language for a proof assistant, we need to guar\u00adantee that the \nimplemented functions are total, i.e. all cases are covered and the functions themselves are terminating. \nWe believe coverage checking can be solved by extending prior work on cov\u00aderage checking contextual objects \n[Dun.eld and Pientka 2009; Sch\u00a8urmann and Pfenning 2003]. Termination checking requires us to identity \na suitable notion of acceptable inductive datatypes, e.g. based on strict positivity as in Coq [Paulin-Mohring \n1993], and we plan to adapt sized types as for example in Abel [2007, 2008] in the future. References \nMart\u00b4in Abadi, Michael Burrows, Butler W. Lampson, and Gordon D. Plotkin. A calculus for access control \nin distributed systems. ACM Transaction on Programming Language Systems, 15(4):706 734, 1993. Martin \nAbadi, Anindya Banerjee, Nevin Heintze, and Jon G. Riecke. A core calculus of dependency. In 26th ACM \nSIGPLAN-SIGACT Symposium on Principles of Programming Languages (POPL 99), pages 147 160. ACM Press, \n1999. Andreas Abel. Mixed inductive/coinductive types and strong normaliza\u00adtion. In Zhong Shao, editor, \n5th ASIAN Symposium on Programming Languages and Systems (APLAS 07), volume 4807 of Lecture Notes in \nComputer Science, pages 286 301. Springer, 2007. Andreas Abel. Polarized subtyping for sized types. Mathematical \nStruc\u00adtures in Computer Science, 18(5):797 822, 2008. Special issue on sub\u00adtyping, edited by Healfdene \nGoguen and Adriana Compagnoni. David Baelde, Zach Snow, and Dale Miller. Focused inductive theorem proving. \nIn J\u00a8urgen Giesl and Reiner Haehnle, editors, 5th International Joint Conference on Automated Reasoning \n(IJCAR 10), Lecture Notes in Arti.cial Intelligence (LNAI 6173), pages 278 292. Springer, 2010. Ulrich \nBerger and Helmut Schwichtenberg. An inverse of the evaluation functional for typed lambda-calculus. \nIn Logic in Computer Science, pages 203 211, 1991. Yves Bertot and Pierre Cast\u00b4eran. Interactive Theorem \nProving and Pro\u00adgram Development. Coq Art: The Calculus of Inductive Constructions. Springer, 2004. \n Chiyan Chen and Hongwei Xi. Combining programming with theorem proving. In Olivier Danvy and Benjamin \nC. Pierce, editors, 10th Inter\u00adnational Conference on Functional Programming, pages 66 77, 2005. Patrick \nCousot and Radhia Cousot. Inductive de.nitions, semantics and abstract interpretations. In 19th ACM SIGPLAN-SIGACT \nSymposium on Principles of Programming Languages (POPL 92), pages 83 94. ACM, 1992. Kevin Donnelly and \nHongwei Xi. Combining higher-order abstract syntax with .rst-order abstract syntax in ats. In Randy Pollack, \neditor, Work\u00adshop on Mechanized Reasoning about Languages with Variable Binding (MERLIN 05), pages 58 \n63. ACM, 2005. Joshua Dun.eld and Brigitte Pientka. Case analysis of higher-order data. In International \nWorkshop on Logical Frameworks and Meta-Languages: Theory and Practice (LFMTP 08), volume 228 of Electronic \nNotes in Theoretical Computer Science (ENTCS), pages 69 84. Elsevier, June 2009. Andrew Gacek, Dale Miller, \nand Gopalan Nadathur. Combining generic judgments with recursive de.nitions. In F. Pfenning, editor, \n23rd Sym\u00adposium on Logic in Computer Science. IEEE Computer Society Press, 2008. D. Garg and F. Pfenning. \nNon-interference in constructive authorization logic. In Proceedings of the 19th IEEE Computer Security \nFoundations Workshop (CSFW 19). IEEE Computer Society Press, 2006. Louis-Julien Guillemette and Stefan \nMonnier. A type-preserving closure conversion in Haskell. In Proceedings of the ACM SIGPLAN Workshop \non Haskell, Haskell 07, pages 83 92, 2007. Robert Harper and Daniel R. Licata. Mechanizing Metatheory \nin a Logical Framework. Journal of Functional Programming, 17(4-5):613 673, 2007. Robert Harper, Furio \nHonsell, and Gordon Plotkin. A framework for de.ning logics. Journal of the ACM, 40(1):143 184, January \n1993. Xavier Leroy and Herv Grall. Coinductive big-step operational semantics. Information and Computation, \npages 284 304, 2009. Daniel R. Licata. Dependently Typed Programming with Domain-Speci.c Logics. PhD \nthesis, Carnegie Mellon University, 2011. Daniel R. Licata and Robert Harper. A universe of binding and \ncompu\u00adtation. In Graham Hutton and Andrew P. Tolmach, editors, 14th ACM SIGPLAN International Conference \non Functional Programming, pages 123 134. ACM Press, 2009. Daniel R. Licata, Noam Zeilberger, and Robert \nHarper. Focusing on binding and computation. In F. Pfenning, editor, 23rd Symposium on Logic in Computer \nScience, pages 241 252. IEEE Computer Society Press, 2008. Raymond C. McDowell and Dale A. Miller. Reasoning \nwith higher-order abstract syntax in a logical framework. ACM Transactions on Computa\u00adtional Logic, 3(1):80 \n136, 2002. ISSN 1529-3785. Yasuhiko Minamide, Greg Morrisett, and Robert Harper. Typed closure conversion. \nIn In Twenty-Third ACM Symposium on Principles of Pro\u00adgramming Languages, pages 271 283. ACM Press, 1996. \nKenji Miyamoto and Atsushi Igarashi. A modal foundation for secure information .ow. In A. Sabelfeld, \neditor, Workshop on Foundations of Computer Security (FCS 04), pages 187 203, 2004. Aleksandar Nanevski, \nJ. Gregory Morrisett, and Lars Birkedal. Hoare type theory, polymorphism and separation. Journal of Functional \nProgram\u00adming, 18(5-6):865 911, 2008a. Aleksandar Nanevski, Frank Pfenning, and Brigitte Pientka. Contextual \nmodal type theory. ACM Transactions on Computational Logic, 9(3): 1 49, 2008b. Ulf Norell. Towards a \npractical programming language based on dependent type theory. PhD thesis, Department of Computer Science \nand Engineer\u00ading, Chalmers University of Technology, September 2007. Technical Report 33D. Christine \nPaulin-Mohring. Inductive de.nitions in the system coq -rules and properties. In Marc Bezem and Jan Friso \nGroote, editors, Inter\u00adnational Conference on Typed Lambda Calculi and Applications(TLCA 93), volume \n664 of Lecture Notes in Computer Science, pages 328 345. Springer, 1993. Frank Pfenning and Carsten Sch\u00a8urmann. \nSystem description: Twelf a meta-logical framework for deductive systems. In H. Ganzinger, editor, 16th \nInternational Conference on Automated Deduction (CADE-16), volume 1632 of Lecture Notes in Arti.cial \nIntelligence, pages 202 206. Springer, 1999. Brigitte Pientka. A type-theoretic foundation for programming \nwith higher\u00adorder abstract syntax and .rst-class substitutions. In 35th Annual ACM SIGPLAN-SIGACT Symposium \non Principles of Programming Lan\u00adguages (POPL 08), pages 371 382. ACM Press, 2008. Brigitte Pientka. \nProgramming proofs: A novel approach based on contex\u00adtual types. submitted, 2011. Brigitte Pientka and \nJoshua Dun.eld. Beluga: a framework for program\u00adming and reasoning with deductive systems (System Description). \nIn J\u00a8urgen Giesl and Reiner Haehnle, editors, 5th International Joint Con\u00adference on Automated Reasoning \n(IJCAR 10), Lecture Notes in Arti.cial Intelligence (LNAI 6173), pages 15 21. Springer-Verlag, 2010. \nAdam Poswolsky and Carsten Sch\u00a8urmann. System description: Delphin a functional programming language \nfor deductive systems. In Interna\u00adtional Workshop on Logical Frameworks and Meta-Languages: Theory and \nPractice (LFMTP 08), volume 228 of Electronic Notes in Theoreti\u00adcal Computer Science (ENTCS), pages 135 \n141. Elsevier, 2009. Adam B. Poswolsky and Carsten Sch\u00a8urmann. Practical programming with higher-order \nencodings and dependent types. In 17th European Sym\u00adposium on Programming (ESOP 08), volume 4960, pages \n93 107. Springer, 2008. Franc\u00b8ois Pottier. Static name control for FreshML. In 22nd IEEE Sym\u00adposium on \nLogic in Computer Science (LICS 07), pages 356 365. IEEE Computer Society, July 2007. Nicolas Pouillard \nand Franois Pottier. A fresh look at programming with names and binders. In 15th ACM SIGPLAN International \nConference on Functional Programming (ICFP 2010), pages 217 228, 2010. Susmit Sarkar. A Dependently Typed \nProgramming Language, with appli\u00adcations to Foundational Certi.ed Code Systems. PhD thesis, Carnegie \nMellon University, 2009. CMU-CS-09-128. Carsten Sch\u00a8urmann and Frank Pfenning. A coverage checking algorithm \nfor LF. In D. Basin and B. Wolff, editors, Proceedings of the 16th International Conference on Theorem \nProving in Higher Order Logics (TPHOLs 03), pages 120 135. Springer, 2003. Tim Sheard. Languages of the \nfuture. SIGPLAN Notices, 39(12):119 132, 2004. Mark R. Shinwell, Andrew M. Pitts, and Murdoch J. Gabbay. \nFreshML: programming with binders made simple. In 8th International Con\u00adference on Functional Programming \n(ICFP 03), pages 263 274. ACM Press, 2003. Antonis Stampoulis and Zhong Shao. VeriML: typed computation \nof log\u00adical terms inside a language with effects. In Paul Hudak and Stephanie Weirich, editors, 15th \nACM SIGPLAN International Conference on Functional Programming (ICFP 10), pages 333 344. ACM, 2010. Martin \nSulzmann, Manuel M. T. Chakravarty, Simon Peyton Jones, and Kevin Donnelly. System f with type equality \ncoercions. In ACM SIGPLAN International Workshop on Types in Languages Design and Implementation (TLDI \n07), pages 53 66. ACM, 2007. E. Westbrook, A. Stump, and I. Wehrman. A Language-based Approach to Functionally \nCorrect Imperative Programming. In Olivier Danvy and Benjamin C. Pierce, editors, 10th International \nConference on Func\u00adtional Programming (ICFP05), pages 268 279. ACM, 2005. Hongwei Xi. Applied type system. \nIn TYPES 2003, volume 3085 of Lecture Notes in Computer Science, pages 394 408. Springer, 2004. Hongwei \nXi and Frank Pfenning. Dependent types in practical program\u00adming. In 26th ACM SIGPLAN-SIGACT Symposium \non Principles of Pro\u00adgramming Languages (POPL 99), pages 214 227. ACM Press, 1999. Hongwei Xi, Chiyan \nChen, and Gang Chen. Guarded recursive datatype constructors. In 30th ACM SIGPLAN-SIGACT Symposium on \nPrinciples of Programming Languages (POPL 03), pages 224 235. ACM Press, 2003. Christoph Zenger. Indexed \ntypes. Theoretical Computer Science, 187(1-2): 147 165, 1997.    \n\t\t\t", "proc_id": "2103656", "abstract": "<p>We show how to combine a general purpose type system for an existing language with support for programming with binders and contexts by refining the type system of ML with a restricted form of dependent types where index objects are drawn from contextual LF. This allows the user to specify formal systems within the logical framework LF and index ML types with contextual LF objects. Our language design keeps the index language generic only requiring decidability of equality of the index language providing a modular design. To illustrate the elegance and effectiveness of our language, we give programs for closure conversion and normalization by evaluation.</p> <p>Our three key technical contribution are: 1) We give a bi-directional type system for our core language which is centered around refinement substitutions instead of constraint solving. As a consequence, type checking is decidable and easy to trust, although constraint solving may be undecidable. 2) We give a big-step environment based operational semantics with environments which lends itself to efficient implementation. 3) We prove our language to be type safe and have mechanized our theoretical development in the proof assistant Coq using the fresh approach to binding.</p>", "authors": [{"name": "Andrew Cave", "author_profile_id": "81496650900", "affiliation": "McGill University, Montreal, PQ, Canada", "person_id": "P2991437", "email_address": "acave1@cs.mcgill.ca", "orcid_id": ""}, {"name": "Brigitte Pientka", "author_profile_id": "81100506891", "affiliation": "McGill University, Montreal, PQ, Canada", "person_id": "P2991438", "email_address": "bpientka@cs.mcgill.ca", "orcid_id": ""}], "doi_number": "10.1145/2103656.2103705", "year": "2012", "article_id": "2103705", "conference": "POPL", "title": "Programming with binders and indexed data-types", "url": "http://dl.acm.org/citation.cfm?id=2103705"}