{"article_publication_date": "01-25-2012", "fulltext": "\n Higher-Order Functional Reactive Programming in Bounded Space Neelakantan R. Krishnaswami Nick Benton \nJan Hoffmann MPI-SWS Microsoft Research Yale University neelk@mpi-sws.org nick@microsoft.com jan.ho.mann@yale.edu \n Abstract Functional reactive programming (FRP) is an elegant and success\u00adful approach to programming \nreactive systems declaratively. The high levels of abstraction and expressivity that make FRP attrac\u00adtive \nas a programming model do, however, often lead to programs whose resource usage is excessive and hard \nto predict. In this paper, we address the problem of space leaks in discrete\u00adtime functional reactive \nprograms. We present a functional reac\u00adtive programming language that statically bounds the size of the \ndata.ow graph a reactive program creates, while still permitting use of higher-order functions and higher-type \nstreams such as streams of streams. We achieve this with a novel linear type theory that both controls \nallocation and ensures that all recursive de.nitions are well-founded. We also give a denotational semantics \nfor our language by combining recent work on metric spaces for the interpretation of higher-order causal \nfunctions with length-space models of space\u00adbounded computation. The resulting category is doubly closed \nand hence forms a model of the logic of bunched implications. Categories and Subject Descriptors D.3.2 \n[Data.ow Languages] General Terms languages, design, theory Keywords functional reactive programming, \ndata.ow, space\u00adbounded computation, linear logic, bunched implications 1. Introduction Reactive systems \nengage in an ongoing interaction with their en\u00advironment, consuming input events and producing corresponding \noutput events. Examples of such systems range from embedded controllers and sensor networks up to complex \ngraphical user interfaces, web applications, games and simulations. Program\u00adming reactive systems in \na general-purpose imperative language can be unpleasant, as different parts of the program interact not \nby structured control .ow, but by dynamically registering state\u00admanipulating callback functions with \none another. The complexity of writing and reasoning about programs written in such a higher\u00adorder imperative \nstyle, as well as the critical nature and resource requirements of many reactive systems, has inspired \nextensive re- Permission to make digital or hard copies of all or part of this work for personal or classroom \nuse is granted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. POPL \n12 January 25 27, 2012, Philadelphia, PA, USA. Copyright c &#38;#169; 2012 ACM 978-1-4503-1083-3/12/01. \n. . $10.00 search into domain-speci.c languages (DSL), libraries and analysis techniques for reactive \nprogramming. Synchronous data.ow languages, such as Esterel [3], Lustre [4], and Lucid Synchrone [21], \nimplement a domain-speci.c compu\u00adtational model deriving from Kahn networks. A program corre\u00adsponds to \na .xed network of stream-processing nodes that commu\u00adnicate with one another, each consuming and producing \na statically\u00adknown number of primitive values at every clock tick. Synchronous languages have precise, \nanalysable semantics, provide strong guar\u00adantees about bounded usage of space and time, and are widely \nused in applications such as hardware synthesis and embedded control software. Functional reactive programming \n(FRP), as introduced by El\u00adliott and Hudak [8], also works with time-varying values (rather than mutable \nstate) as a primitive abstraction, but provides a much richer model than the synchronous languages: signals \n(behaviours) can vary continuously as well as discretely, values can be higher\u00adorder (including both \n.rst-class functions and signal-valued sig\u00adnals), and the overall structure of the system can change \ndynami\u00adcally. FRP has been applied in problem domains including robotics, animation, games, web applications \nand GUIs. However, the ex\u00adpressivity and apparently simple semantics of the classic FRP model come at \na price. Firstly, the intuitively appealing idea of modelling A-valued signals as elements of the stream \ntype A. (or, in the continuous case, AR) and reactive systems as stream functions Input. . Output. does \nnot rule out systems that violate causality (the output today can depend upon the input tomorrow) or \nreactivity (ill-founded feedback can lead to unde.ned behaviour). Secondly, as the model is highly expressive \nand abstracts entirely from resource usage, the time and space behaviour of FRP pro\u00adgrams is hard to \npredict and, even with sophisticated implementa\u00adtion techniques, can often be poor. It is all too easy \nto write FRP programs with signi.cant space leaks, caused by, for example, in\u00adadvertently accumulating \nthe entire history of a signal.1 Subsequent research has attempted to reduce junk and allevi\u00adate performance \nproblems by imposing restrictions on the classic FRP model. The Yale Haskell Group s Yampa [11, 19], \nfor exam\u00adple, is an embedded DSL for FRP that constructs signal process\u00ading networks using Hughes s arrow \nabstraction [12]. Signals are no longer .rst-class, and signal-processing functions must be built from \nwell-behaved casual primitives by causality-preserving com\u00adbinators. Arrowized FRP allows signals to \ncarry complex values but is essentially .rst-order (there is no exponential at the level of sig\u00adnal functions), \nthough certain forms of dynamism are allowed via built in switching combinators. Yampa does not enforce \nreactiv\u00ad 1 Closely related are time leaks , which occur when sampling a time\u00addependent value can invoke \nan arbitrarily lengthy computation to catch up with the current time.  ity or provide resource guarantees \nbut, empirically at least, makes certain kinds of leaks less likely. Krishnaswami and Benton [13] recently \ndescribed a semantic model for higher-order, discrete-time functional reactive programs based on ultrametric \nspaces, identifying causal functions with non\u00adexpansive maps and interpreting well-founded feedback via \nBa\u00adnach s .xpoint theorem. They gave an associated language, fea\u00adturing a Nakano-style [18] temporal \nmodality for well-founded re\u00adcursion, and showed the correctness of an implementation using an imperatively-updated \ndata.ow graph. This implementation is much more ef.cient than directly running the functional semantics, \nbut nothing prevents the data.ow graph from growing unboundedly as a program executes, leading to undesirable \nspace leaks. In this pa\u00adper, we solve the problem of such leaks by extending the ultramet\u00adric approach \nto FRP with linearly-typed resources that represent the permission to perform heap-allocation, following \nthe pattern of Hofmann s work [9, 10] on non-size-increasing computation. We give a denotational model \nfor bounded higher-order reac\u00adtive programming in terms of complete ultrametric length spaces , which \ncarry both an ultrametric distance measure and a size func\u00adtion. Maps between such spaces must be non-expansive \nand non\u00adsize-increasing. Intuitively, the metric is used to enforce good tem\u00adporal behaviour (causality \nand productivity of recursive de.ni\u00adtions), whilst the size measure enforces good spatial behaviour, \nbounding the number of cells in the data.ow graph. The category of complete ultrametric length spaces \nis doubly-closed, forming a model of the logic of bunched implications [20] and exposing a (perhaps) \nsurprising connection between the type theory of stream programming and separation logic. We de.ne a \nterm language, with a rather novel type theory, that corresponds to our model and allows us to write \nbounded reactive programs. Judgements are all time-indexed, with the successor operation on times internalized \nin a modality A. Terms are typed in three contexts, one carrying linear (actually af.ne) resources of \ntype ., giving permission to allocate; one binding pure, resource\u00adfree variables; and one binding potentially \nresourceful variables. Resource-freedom is internalized via a !A modality, in the style of linear logic. \nWe give the language an interesting staged operational semantics, which separates the normalizing reduction \nthat takes place within each time step from the transitions that take place when the clock advances, \nand show that this is soundly modelled in the denotational semantics. The operational semantics uses \nterms of the language itself to encode the heap context within which evaluation takes place. We also \ngive a number of examples in our language that illus\u00adtrate properties of the model and the applicability \nof our approach, showing that one can work with recursively-de.ned higher-order functions and streams \nin a natural way while still ensuring causal\u00adity, productivity and bounded space usage. To improve readability, \nwe invert the logical order of presen\u00adtation: Section 2 gives an informal account of the language that \nsuf.ces for presenting some of the motivating examples. Section 3 formally de.nes the language and the \ntype system. We then, in Sec\u00adtion 4, de.ne the operational semantics and, in Section 5 and 6, present \nthe details of the denotational model. Finally, in Section 7, we discuss our work and relate it to existing \nresearch. 2. Programming Language The language is essentially a simply-typed .-calculus with a type constructor \nS(-) for in.nite streams, extended with three non\u00adstandard notions: delay types, resource types, and \npure types. We treat streams as much as possible as mathematical sequences, but want to ensure that we \ncan also interpret them as the successive values of signals generated by implementable clocked systems. \nThus a de.nition (in a generic functional syntax for now)  ... t =1 ... t =2 ... t =3 ... t =4 ... t \n=5 Figure 1. Memory Usage of Streams nats : N. S(N) nats n = cons(n, nats (n+1)) denotes a parameterised \nin.nite stream but can also be understood as a stateful process ticking out successive natural numbers \nas time advances. It is necessary to restrict recursive de.nitions to ensure that signals are well-de.ned \nat all times. The recursion above is clearly guarded: the recursive call to nats only occurs underneath \na cons constructor, so successively unfolding the stream at each clock tick is productive. That is, we \nalways discover at least one new cons constructor which we can examine to .nd the head and the tail of \nthe stream at the current time. However, simple syntactic guardedness checks (used by lan\u00adguages as varied \nas Lucid Synchrone, Agda and Coq) do not inte\u00adgrate well with higher-order. For example, one might want \na stream functional that abstracts over the constructor: higher order f v = f(v, higher order f (v + \n1)) The guardedness of higher order now depends on the de.nition of f, which is an unknown parameter. \nAs in our earlier work [13], we instead use a next-step modality A to track the times at which values \nare available in their types. A value of type A is a computation that will yield a value of type A when \nexecuted on the next clock tick. The tail function has type tail : S(A) . S(A), expressing that the tail \nof a stream only becomes available in the future. Similarly the type of cons is re.ned to cons : A \u00d7 \nS(A) . S(A), capturing that streams are constructed from a value today and a stream tomorrow. By giving \nthe .xed point combinator the type .x :( A . A) . A, we ensure that all recursive de.nitions are well-founded \nwithout restricting their syntactic form. If we care about space usage, however, this use of types to \ntrack guardedness still admits too many programs. The problem is to limit the amount of data that must \nbe buffered to carry it from one time tick to the next. In the case of nats above, it is clear that the \ncurrent state of the counting process can always be held in a single natural number. But consider a similar \nde.nition at a higher type: constantly leak : S(A) . S(S(A)) constantly leak xs = cons(xs, constantly \nleak xs) The call constantly leak xs yields a stream of streams which is constantly xs. This is a perfectly \nwell-founded functional de.nition but, as a stateful process, requires the whole accumulated history \nof xs to be buffered so it can be pushed forward on each time step. Figure 1 shows the evolution of a \nstream as time passes. The gray nodes denote the nodes already produced, the doubly-circled nodes show \nthe current value of the head of the stream, and the white nodes mark the elements yet to be produced. \nAt each time step, one node moves into the past (becomes gray), and the current head advances one step \nfarther into the stream.  As constantly leak xs retains a reference to its argument, it needs to buffer \nall of the gray nodes to correctly enumerate the elements of the streams to be produced in future. Running \nconstantly leak xs for n time steps thus requires O(n) space, which is unreasonable. To see this more \nclearly, consider the following function: f : S(A) . S(S(A)) . N. N. S(A) f xs yss nm = let (x, xs ) \n= (head xs, tail xs) in let (ys , yss ) = (head yss, tail yss ) in if n = 0 then cons(x, f (head yss \n) yss m (m+1)) else cons(x, f xs yss (n-1) m) diag : S(A) . S(A) diag xs = f xs ( constantly leak xs) \n01 Now, diag ( nats 0) yields (0, 0, 1, 0, 1, 2, 0, 1, 2, 3,...), enumer\u00adating the pre.xes of nats 0. \nAs each pre.x appears in.nitely of\u00adten, the whole history of the initial stream must be saved as the \nprogram executes. There are de.nitions with the same type as constantly leak that can be implemented \nin constant space: tails :: S(A) . S(S(A)) tails xs = cons(xs, tails ( tail xs )) Since tails returns \nthe successive tails of its stream argument, we do not need to remember gray nodes (see Figure 1), and \nhence can implement the function without any buffering. Substituting tails for constantly leak turns \ndiag into the (space-ef.cient) identity. To account for the memory usage associated with creating and \nbuffering stream data, we adapt a variant of the linear (af.ne) resource types of Hofmann s LFPL [9, \n10]. The type . represents a permission to create one new stream; the tensor product R . S is the permission \nto do both R and S; and the linear function space R -A builds an A, consuming the resources in R. We \nfurther re.ne the construction of streams of type S(A) to take three '' arguments using the syntactic \nform cons(u,e,u. e). The term u : . is a permission to allocate a cons cell and the term e : A is ' the \nhead of the stream. The tail, e: S(A), is de.ned in scope of the variable u', which re-binds the allocation \npermission that will be freed up on the next time step. We still permit sharing of stream values without \nrestriction since data.ow programs gain ef.ciency precisely from the ability to share. Therefore, we \nalso support a context of unrestricted variables and an intuitionistic2 function space A . B. (Hofmann \ns original language featured a strictly linear type discipline.) Function clo\u00adsures can also need buffering \nif they capture streams in their envi\u00adronment, so it is useful to introduce the type constructor !A, \nwhich classi.es those A-values that need no buffering and may be freely carried forward in time. 2.1 \nExamples Our language makes the intuition of bounded resource consump\u00adtion explicit, while remaining \nclose to standard functional program\u00adming style. We begin with the de.nition of two classic stream func\u00adtions, \nnats and .b, in our language: nats :!N. .-S(!N) nats = .x loop :!N. .-S(!N) . xu. let !n = x in cons(u,!n, \nu . loop !( n+1) u ) .bs :!N. !N. .-S(!N) .bs = .x loop :!N. !N. .-S(!N). . xyu. let !n = x in let !m \n= y in cons(u,!n, u . loop !m !(n+m) u ) 2 We do not decompose the intuitionistic function space as !A \n-B. The function nats differs from the standard de.nition in two ways. First, we include an extra resource \nargument of type ., which con\u00adtains a permission used as the extra argument to the cons operator. Furthermore, \nwe require the argument of the function to be of the modal type !N. Since the variable n is used in both \nthe head and the tail of the cons-expression, we need to know that n can be used at different times without \nrequiring additional space usage. This is exactly what the type !A allows: an expression let !a =a in \n... binds the modal value a' to a variable a which can be used at any current or future time. The introduction \nform !e for !A constructs a value of type A, which may only mention resource-free variables. The de.nition \nof the function .bs is similar. Similarly, we give the de.nition of the constant function, which returns \na constant stream of values. constant :!A . .-S(!A) constant = .x loop :!A . .-S(!A). . a u. let !a = \na in cons(u, a , v. loop !av) The modal type !A in the typing of constant prevents one de.ning constantly \nleak in terms of constant, since stream values are never resource-free (as cons-expressions include a \nresource argument). However, we can still de.ne functions constructing higher-order streams, so long \nas space usage is bounded. In this, we improve upon much previous work on ef.cient implementations of \nFRP. An example is the tails function that we described earlier; in our language, this is programmed \nas follows: tails : S(A) . .-S(S(A)) tails = .x tails : S(A) . .-S(S(A)). . xs u. let xs = tail (xs) \nin cons(u, xs , u . tails xs u ) Higher type streams also let us de.ne many of the switching combi\u00adnators \nof FRP libraries, without having to build them in as primitive operations. For example: switch : S(bool) \n. S(S(A)) . .-S(A) switch = let loop = .x loop : S(bool) . S(S(A)) . S(A) . .-S(A). . bs xss current \nu. let yss = tail ( xss ) in let bs = tail (bs) in if head(bs) then let zs = tail (head xss) in cons(u, \nhead(head xss), u . loop bs yss zs u ) else let zs = tail ( current ) in cons(u, head(current ), u . \nloop bs yss zs u ) in . bs xss u. loop bs xss (head xss) u The function switch takes a stream of boolean \nevents and a stream of streams. It then yields the elements of the head of the stream of streams until \nthe boolean stream yields true, at which point it starts generating the elements of the current stream \nfrom the stream of streams. In this way, it is easy to go beyond simple static data.ow programs, without \nhaving to contort programs to .t a .xed set of combinators. However, with the full resources of a higher-order \nprogramming language available, it is often convenient to de.ne programs in terms of familiar stream \nfunctionals such as map. map : !( A . B) . S(A) . .-S(B) map h = let ! f = h in .x loop : S(A) . .-S(B). \n. xs u. let ys = tail xs in cons(u, f (head xs ), u . loop ys u )  This function illustrates a common \npattern of higher-order pro\u00adgramming in our language. We often wish to use functional argu\u00adments at many \ndifferent times: in this case, we want to apply the argument to each element of the input stream. Therefore, \nthe func\u00adtional arguments in higher-order functions often need to be under the time-independence modality \n!(A . B). The unfold function provides another nice example of higher\u00adorder programming: unfold : !( \nX . A \u00d7 X) . X . .-S(A) = unfold h = let ! f = h in .x loop : X . .-S(A). . xu. let (a, d) = f(x) in \nlet x = d in cons(u, a, v. loop x v) Using unfold, one can directly translate deterministic state ma\u00adchines \ninto stream programs, passing in the state transformer func\u00adtion and the initial state as the arguments. \nThis function is also the .rst example with the delay modality our state transformer takes a state, \nand returns a value and the state to use on the next timestep. The elimination form let y =e in . . . \ntakes an expression of type A and binds it to a variable y of type A, one tick in the future. This lets \nus use it in the third argument to cons, since the tail of a stream is also an expression one tick in \nthe future. Next, we illustrate the importance of being able to use streams intuitionistically, even \nas we track resources linearly. zip : S(A) \u00d7 S(B) . .-S(A \u00d7 B) zip = .x zip : S(A) \u00d7 S(B) . .-S(A \u00d7 B). \n. (xs , ys) u. let xs = tail (xs) in let ys = tail (ys) in cons(u,(head xs, head ys ), v. zip (xs , ys \n) v) sum : S(N) \u00d7 S(N) . (. . .) -S(N) = . (xs , ys)(u, v ). map !+ ( zip (xs , ys) u) v double : S(N) \n. (. . .) -S(N) double ns (u, v) = sum(ns, ns)(u, v) We .rst de.ne the function zip, which takes two \nstreams and returns a stream of pairs, and the function sum, which takes two streams of natural numbers \nand pointwise sums their elements. These two functions are then used to de.ne double, which takes a stream \nof numbers and returns a new stream of elements each of which is twice the size of the input. Note that \ndouble works by passing the sum function the same stream in both arguments, decisively violating linearity. \nThe map and zip functions (together with unzip, which we do not de.ne here) witness that S(\u00b7) is a Cartesian \nfunctor. We can also de.ne maps illustrating other semantic properties of streams. cokleisli : !(S(A) \n. B) . S(A) . .-S(B) cokleisli g = let ! f = g in .x loop : S(A) . .-S(B). . xs u. let ys = tail (xs) \nin cons(u, f xs , u . loop ys u ) .ip : S( A) . (.-S(A)) .ip = .x .ip : S( A) . (.-S(A)) . xs . let x \n= head(xs) in let xs = tail (xs) in ( let f = .ip xs in . u. cons(u, x , u . fu )) un.ip : S(A) . .-S( \nA) un.ip = .x un.ip : S(A) . .-S( A). . xs u. let ys = xs in cons(u, (head(ys )), u . let ys = tail \n(ys) in un.ip ( ys ) u ) The cokleisli function lifts a function from streams S(A) to a type B to a function \nfrom streams of A to streams of B, giving a comonad structure [24] to the S(\u00b7) functor. The .ip and un.ip \nfunctions de.ne an isomorphism between streams of delayed val\u00adues and delayed streams of values. These \noperations make use of the ability to explicitly delay expressions e until the next tick with the e introduction \nform for A. In our earlier work, we had general delay operators dA : A . A, which shifted values of any \ntype forward one tick into the fu\u00adture. However, a given piece of data may represent different val\u00adues \nas time passes, and so we do not want delay maps of type dA : S(A) . S(A), since this type does not capture \nthe addi\u00adtional storage needed to move the argument forward one step in the future. However, it is possible \nin our system to de.ne delay oper\u00adators with types such as S(!N) . .- S(!N), which explicitly represent \nthe buffering in the type: buffer :!N. S(!N) . .-S(!N) buffer b xs u = let !y = head(xs) in // let head \nof y be usable later let ys = tail (xs) in // ysattime1 cons(u, b, v. buffer !y ys v) // now call buffer \nat time 1 The bu.er function prepends a number to the front of a stream and can be used to construct \na delay operator: delaynats : S(!N) . .- S(!N) delaynats xs u = let !y = head(xs) in // let head of y \nbe usable later let ys = tail (xs) in // tail ys is at time 1 ( buffer !y ys u) // now call buffer at \ntime 1 with y,ys This gives a delay operator for streams, but additionally asks for a resource with which \nto construct the delayed stream. The def\u00adinition illustrates our design philosophy that expensive crosstime \noperations should be programmed explicitly. We conclude with a traditional example of stream program\u00adming, \nthe sieve of Eratosthenes: sieve : !( N. bool) . S(!N) . .-S(!(option N)) sieve pxsu = let !n = head \nxs in let ns = tail (xs) in let !pred = p in if pred n then let q = !(. j . pred j . j mod n= 0) in cons(u, \n!( Some n), v. sieve qnsv) else cons(u,!None, v. sieve pnsv) primes : . . .-S(N) primes (u,v) = sieve \n!( is odd )( nats 2 u) v The sieve function incrementally constructs a .lter predicate that tests each \nelement of the stream for divisibility by all the primes seen so far. Since the number of primes is in.nite, \nthe size of the sieve predicate s lambda-term grows without bound, but is nevertheless accepted by our \nlanguage s type system. By design, we allow all ordinary call-by-value functional programs (which have \nreasonable compositional cost semantics [23]), and only use typing to track the unusual memory leaks \nof FRP. Hence our types only track the size of the data.ow graph (i.e., the number of live cons cells). \n 3. Syntax and Typing We give the types and syntax of the programming language in Figure 2. The types \ninclude base types P , stream types S(A), the next-step modality A, ordinary functions A . B, the purity \nmodality !A, and the linear function space R -A. Resources R include the allocation permission ., and \nthe tensor product R . S. For space reasons, we do not include products A \u00d7 B or sums A + B here. (Since \nwe require coproducts to de.ne switching combinators, we emphasise that this is purely for space reasons: \nthere are no technical complications associated with sum types.) The typing rules are de.ned in Figure \n3. The two type judge\u00adments of our type theory are T f t :i R and T; .; G f e :i A. Both judgements are \ntime-indexed, in the sense that the type sys\u00adtem judges a term to have a type at a particular time i. \nFurthermore, each hypothesis in each context is indexed by the time at which they can be used. (As usual, \nwe take contexts to be unordered, and implicitly assume alpha-renaming to ensure that all variables are \ndistinct.) The judgement T f t :i R states that in the context of af.ne resource variables T, the term \nt has resource type R, at time i. The resource terms are built from af.ne pairs of the ground type . \nand are permissions to allocate one or more cons cells. The judgement T; .; G f e :i A has three contexts: \nthe af.ne resource context T contains again permissions to allocate cons cells; the intuitionistic context \n. contains pure hypotheses (i.e., variables in . bind non-state-dependent values); and the intuition\u00adistic \ncontext G binds arbitrary value types, and permits unrestricted sharing and reuse of variables. Under \nthese three contexts, we judge a term e to be an expression of type A at time i. There are only two rules \nfor the af.ne resource term calculus in Figure 3. The RHYP rule allows a resource to be used at any time \nafter the context says it is available. The tensor rule R.I lets one form an af.ne pair (t, t '), dividing \nthe resources in the context between the two components. Observe that these rules allow weakening but \nnot contraction. The rule PHYP lets us use a pure hypothesis at any time after the time index in the \nvariable context. In contrast, the rule EHYP only permits using a variable x at exactly its time index \nin G. This difference is one of the keys to accurately tracking space usage: we may substitute values \nwhich require buffering for variables in G, and by disallowing implicit transport of values across time, \nwe ensure that the programmer uses explicit buffering whenever needed. The rules .I and .E introduce \nand eliminate intuitionistic functions. The introduction rule does not permit the body to use any resources, \nsince we can call functions multiple times. (The presence of T in the conclusion of .I and the other \nvalue forms builds in weakening, so that we do not have to give a separate structural rule.) The elimination \nrule does allow expressions to use resources, since we will use a call-by-value evaluation strategy that \nwill evaluate the two terms (using up their resource permissions) before substituting a value into a \nlambda-term. The rules -I and -E introduce and eliminate linear functions. The introduction rule only \npermits the body to use the resources it receives in the argument, since we need to ensure that the function \ncan be safely called multiple times. As a result, our typing rules do not permit currying of linear functions \n(R -S -A i R .S -A), even though our underlying semantic model does permit it. If our type theory had \nthe tree-structured contexts of the logic of bunched implications [20], then currying linear functions \nwould be syntactically expressible. However, type checking for bunched calculi is still a dif.cult problem, \nand so in this work we restrict our attention to a linear fragment. The rules SI, SE-HEAD and SE-TAIL \nare the introduction and elimination rules for streams. The syntactic form cons(t,e,u. e ' ) takes three \narguments: the expression t is a permission to create a cons cell, the expression e is the head of the \nstream, and e ' is the tail of the stream. The tail subterm e ' occurs underneath a binder for the resource \nvariable u '. The intuition is that each stream takes up one unit of space at each successive time step, \nand u ' names the permission t, after one time step has elapsed. This lets us pass the permission to \nuse t to functions on subsequent time steps in the body of e ' . The rule SE-HEAD is straightforward: \ngiven a stream of type S(A), we get a value of type A, at the same time. The rule SE-TAIL uses the form \nlet x = tail(e) in e ' to bind the tail of e to the variable x in e '. The tail of a stream at time i \nlives at time i +1 and we choose a binding elimination form to maintain the invariant that no term of \ntime i contains any subterms at any earlier time. The rules I and E introduce and eliminate delay terms. \nThe rule I says that if e is a term of type A at time i +1, then e is a term of type A at time i. Like \nthe other value introduction forms, it prevents e from using any resources, so that e can be substituted \nfreely. The rule E gives a binding elimination let x = e in e ' for the next-step modality. We use a \nbinding elimination for the same reason as in the rule SE-TAIL we do not want terms of time i to contain \nsubterms of time <i. The rule !I introduces a term of pure type !A. It does so by typing the body of \na term !A at type A with an empty resource and shared context. Since e can only refer to hypotheses in \n., which are pure, it follows that e itself must be pure. The rule !E types the elimination form let \n!x = e in e ', which binds the value e to the variable x in the pure context .. The rule .E is the elimination \nform for the tensor. Given a term t of type R . S, the expression let (u, v) = t in e binds the components \nto the variables u and v for the scope of e. The rule LET introduces local let-bindings. The introduced \nbinding x = e must be at the same time i as the overall expression. The rule FIX types .xed points .x \nx : A. e. The body e is typed at time i with the recursive hypothesis x :i+1 A one tick later. The one-step \ndelay ensures the guardedness of recursive de.nitions. Furthermore, the typing of the expression e is \nderived under an empty resource context and with an empty shared context. Since e may unfold multiple \ntimes, giving e resources would violate linearity. Furthermore, the unrollings can happen at different \ntimes, which means that using any variables in the shared context might require buffering. We now state \nthe structural and substitution principles. Lemma 1. (Admissibility of Weakening and Contraction) 1. \nIf T f t :i R then T, T ' f t :i R . 2. If T; .; G f e :i A then T, T ' ;., . ' ;G, G ' f e :i A . \n3. If T; .; G,x :i A, y :i A f e :i B then T; .; G,x :i A f [x/y]e :i A .  4. If T; .,x :i A, y :j A;G \nf e :i B and i = j then T; .; G,x :i A f [x/y]e :i A .  Theorem 1. (Substitution) We have that: 1. \nIf T f t :i R and T ' ,u :i R f t ' :j R ', then T, T ' f [t/u]t ' :j R ' . 2. If T f t :i R and T,u \n:i R; .; G f e :j A, then T, T ' ; .; G f [t/u]e :j A. 3. If \u00b7; .; \u00b7f e :i A and T; .,x :j A;G f e ' \n:k B and i = j, then T; .; G f [e/x]e ' :k B . 4. If \u00b7; .; G f e :i A and T; .; G,x :i A f e ' :j B \n, then T; .; G f [e/x]e ' :j B .  Lemma 1 and Theorem 1 can be proved by structural inductions on the \ntype derivations in the respective premises.  Types General A ::= P | A . A | S(A) | A | !A | R -A Resource \nR ::= . | R . R Terms General e ::= x | .x : A. e | ee ' | .u : R. e | et | cons(t,e,u. e ' ) | head(e) \n| let x = tail(e) in e ' | e | let x = e in e ' | !e | let !x = e in e ' | let (u, v) = t in e | .x x \n: A. e | let x = e in e ' Resource t ::= u |(t, t ' ) Values v ::= .x : A. e | .u : R. e | !v | e | x \nContexts General G ::= \u00b7| G,x :i A Pure . ::= \u00b7| .,x :i A Resource T ::= \u00b7| T,u :i R Evaluation Contexts \nC ::= D | let x = tail(y) in C | let x = cons(u, v, u ' .e) in C Figure 2. Syntax 4. Operational Semantics \nTheorem 1 formulates the substitution principles so that a general expression e that replaces a variables \nhas to be typed without resource variables. That is, the substitution [e/x]e ' is only sound if the substituted \nterm e uses no resource variables. On the other hand, the typing rule for cons cells demands the use \nof a resource, raising the question: what is the operational semantics of cons cells? A purely substitution-based \noperational semantics cannot be correct, because it does not account for the sharing of cons cells. Consider \nthe following expression that is well-typed in our system. let xs = cons(u, ...) in // u is the linear \nallocation permission sum (xs , xs) v Here, we construct xs once, but use it twice in the call to sum. \nWe cannot simply substitute the cons into the body of the let in our system, as that would duplicate \nthe linear variable u. One approach for managing permissions is to introduce a heap for cons cells, and \nrefer to streams indirectly by reference. How\u00adever, adding a heap moves us away from our functional intuitions \nand makes it more dif.cult to connect to our denotational model. Instead, we retain the idea of referring \nto streams by reference, but use variables for the indirect reference, by de.ning evaluation to put terms \ninto let-normal form, such as: let xs = cons(u1, e1 , v1 . e1 ) in let ys = cons(u2, e2 , v2 . e2 ) in \nlet xs = tail (xs) in ... let zs = cons(u3, e3 , v2 . e3 ) in let ys = tail (ys) in Now, the nested let-bindings \nact as our heap. The value v may contain many references to individual streams (such as xs), but since \neach stream is bound only once, we can respect the linearity constraint on the allocation permissions \nui. (Taking the tail of streams, as in the de.nition xs ' and ys ', also needs to be in let\u00adnormal form, \nsince we cannot cut out the tail of a cons cell until T; .; G f e :i AT f t :i R u :i R . T i = j RHYP \nT f u :j R T '' :i R ' T f t :i R f t T, T ' f t, t ' :i R . R ' R.I x :i A . . i = jx :i A . G PHYP \nEHYP T; .; G f x :j A T; .; G f x :i A \u00b7; .; G,x :i A f e :i B .I T; .; G f .x : A. e :i A . B T; .; \nG f e :i A . B T ' ; .; G f e ' :i A T, T ' ; .; G f ee ' :i B .E u :i R; .; G f e :i A -I T; .; G f \n.u : R. e :i R -A T; .; G f e :i R -A T ' f t :i R -E T, T ' ; .; G f et :i A T f t :i . ' ''' T ;.;G \nf e :i A T ,u :i+1 .; .; G f e :i+1 S(A) SI ''' ' T, T , T ;.;G f cons(t,e,u. e ):i S(A) T; .; G f e \n:i S(A) SE-HEAD T; .; G f head(e):i A T; .; G f e :i S(A) T ' ; .; G,y :i+1 S(A) f e ' :i B SE-TAIL T, \nT ' ; .; G f let y = tail(e) in e ' :i B \u00b7; .; G f e :i+1 A I T; .; G f e :i A T; .; G f e :i A T ' \n; .; G,x :i+1 A f e ' :i B T, T ' ; .; G f let x = e in e ' :i B E \u00b7; .; \u00b7f e :i A !I T; .; G f !e :i \n!A T; .; G f e :i !A T ' ;.,x :i A;G f e ' :i B T, T ' ; .; G f let !x = e in e ' :i B !E T f t :i R \n. S T ' ,u :i R, v :i S; .; G f e :i C .E T, T ' f let (u, v) = t in e :i C \u00b7;.,x :i+1 A; \u00b7f e :i A FIX \nT; .; G f .x x : A. e :i A T; .; G f e :i A T ' ; .; G,x :i A f e ' :i B T, T ' ; .; G f let x = e in \ne ' :i B LET Figure 3. Typing Rules  the next tick.) Using bindings to represent sharing will make it \neasier to continue using our denotational model to interpret the resulting terms. The scoping rules for \nlet-binding also restrict us to a DAG dependency structure, an invariant that imperative reactive programming \nimplementations based on dependency graphs must go to some lengths to implement and maintain. Let-normalizing \ncons cells has a second bene.t: we can advance the global clock by taking the tails of each cons cell \nin the context. let xs = [u1/v1]e1 in let ys = [u2/v2]e2 in let xs = xs in ... let zs = [u3/v3]e3 \nin let ys = ys in v Since we know where all of the cons cells are, we can rewrite them to model the passage \nof time. Advancing the clock for tail expres\u00adsions simply drops the tail. Intuitively, yesterday they \npromised a tail stream today, and after the step the binding they refer to con\u00adtains that tail stream. \nOur operational semantics has two phases: the within-step oper\u00adational semantics, which puts an expression \ninto let-normal form, and the step semantics, which advances the clock by one tick by rewriting the cons \ncells to be their tails. 4.1 Within-Step Operational Semantics The syntax of values and evaluation contexts \nis given in Figure 2 and the typing and auxilliary operations are given in Figure 5. We de.ne the reduction \nrelation in Figure 6, in big-step style. We write S Ci Oi f C -Oi ' for the evaluation-context typing \njudgement. The context S is a resource context T that consists only of . hypotheses. Similar, the contexts \nOi are restricted forms of general contexts G consisting only of stream variables at time i or i +1. \nBoth are de.ned in Figure 4. The judgement S Ci Oi f C -O ' i reads as the evaluation context C creates \nthe bindings in O ' i, uses the resources in S to do so, and may refer to the bindings in Oi . The context-concatenation \noperation C . C ' appends two eval\u00aduation contexts C and C '. It is de.ned in Figure 5 and satis.es the \nfollowing properties. Lemma 2. (Context Concatenation) We have that: . is associative with unit D . ' \n-O '' If S Ci Oi f C -O ' i and S ' Ci Oi, O ' i f C i , then ' i, O '' S, S ' Ci O f C . C -O ' i . \nIf S Ci Oi f C1 . C2 -O ' i, then there exist S1, S2 and O1 i , O2 i such that S=S1, S2 and O ' i =O1 \ni , O2 i and S1 Ci Oi f C1 \u00adO1 i and S2 Ci Oi, Oi 1 f C2 -O2 i . These properties all follow from routine \ninductions. In Figure 6, we give the context semantics, evaluating an expres\u00adsion in a context into a \nvalue in a larger context. Note that the value forms for streams are variables, since we need to preserve \nshar\u00ading for them, and we can use variable names as pointers into the evaluation context. For most expression \nforms, the context seman\u00adtics works as expected; it evaluates each subexpression in context, building \na value in a larger context. The rule CONSE is one of the two rules that extend the context. It evaluates \na cons cell, creates a binding to a fresh variable, and returns the fresh variable as the value for that \nstream. The other rule that extends the context is TAILE. It adds a binding to the context naming the \ntail it constructs. The rule HEADE, on the other hand, uses the C@x . v relation that is de.ned in Figure \n5 to .nd the head of the cons cell bound to the variable x. The rule FIXE looks entirely conventional. \nWe simply unfold the .xed point and continue. We are nevertheless able to prove a normalization result \nfor the within-step operational semantics since the .xed point substitutes for a variable at a future \ntime. We begin the metatheory with a type preservation proof. Theorem 2. (Type Preservation) If we have \nthat S ' '' S Ci \u00b7f C -Oi, ; \u00b7;Oi f e :i A, and C[e] . C [v], then there is an O ' i and C ' such that \n''' ' C = C . C , S ' Ci Oi f C -O ' i, and \u00b7; \u00b7;Oi, O ' i f v :i A. This theorem follows from a routine \nstructural induction. To show soundness, we will prove termination via a Kripke logical relations argument. \nSince we evaluate terms e in contexts C, and return a value v in some larger context C ', we take our \nKripke worlds to be the closed contexts. That is, worlds are those C such that S Ci \u00b7f C -Oi. We de.ne \nthe ordering C ' . C on worlds so that there should be some C1 such that C ' = C . C1. Thus, a future \nworld is one in which more bindings are available. In Figure 4, we de.ne the logical relation by induction \non types. There is one clause VA(C) for each type A, de.ning a subset of the well-typed expressions of \ntype A, closed save for the variables bound by C. The expression relation EA(S; C) consists of the expressions \nthat use resources in S and evaluate to a value in VA(C) in the context C. The de.nition of the logical \nrelation for streams states that a variable x is in VS(A)(C) if x binds a cons cell with v in its head, \nand v is in the A-relation. As expected, the relation for functions consists of lambdas such that in \nany future world, applying a value in the A-relation should result in a term in the expression relation \nat type B. In the case of the delay modality we allow any well-typed value, since the within-step evaluation \nrelation does not evaluate any terms at time i, and the body of a delay is at time i +1. The !A relation \nconsists of values !v such that v is in the A-relation in the empty world D, since we want values of \ntype !A to not depend on the stream values C binds. Last, the relation at R -A consists of those lambda-terms \n.u : R. e such that for any resource t of type R in context S, the expression [t/u]e is in the expression \nrelation for A with resources S. To prove the fundamental property, we de.ne some auxilliary predicates \nin Figure 4. The predicate Good(Oi) picks out those contexts in which all of the bindings in Oi at time \ni contain true streams, according to the stream relation. The V(G=i; C) and V!(.=i) sets extend the value \nrelation to substitutions rather than single values, and S f . :T de.nes linear substitutions. The no\u00adtations \n.=i and G=i mean contexts where every variable is at time i or later. In these substitutions, we only \nrequire substitutands for variables at time i to lie in the logical relation, and require only well-typededness \nfor other variables, since within-step evaluation only affects the current tick. Theorem 3. (Fundamental \nProperty of Logical Relations) Suppose T; .=i;Oi, G=i f e :i A. Furthermore, suppose that C . Good(O) \nand S f . :T and p .V!(.=i) and . .V(G=i; C). Then (. . p . .)(e) .EA(S; C). As is usual for logical \nrelations, this theorem follows from a struc\u00adtural induction on the derivation of e : A. The fundamental \nproperty suf.ces to prove normalization, once we observe that typing derivations satisfy the following \nhistory independence property: Lemma 3. (History Independence) If T; .; G,x :i A f e :j B and i<j, then \nT; .; G f e :j B.  If T; .,x :i A;G f e :j B and i<j, then T; .,x :j A;G f  e :j B. If T,u :i R; .; \nG f e :j B and i<j, then T,u :j R; .; G f e :j B.  Parameter of the logical relation: i Base Contexts \nS ::= \u00b7| S,u :0 . Oi ::= \u00b7| Oi,x :i S(A) | Oi,x :i+1 S(A) VA(S Ci \u00b7f C -Oi) .{v |\u00b7; \u00b7;Oi f v :i A } EA(S; \nS ' Ci \u00b7f C -Oi) .{e | S; \u00b7;Oi f e :i A } VS(A)(C)= {x |.v. C@x . v . v .VA(C) }VA.B (C)= {.x : A. e \n|.C ' ,v .VA(C . C ' ). [v/x]e .EB (\u00b7; C . C ' ) }V A(S Ci \u00b7f C -Oi)= { e |\u00b7; \u00b7;Oi f e :i A }V!A(C)= \n{!v | v .VA(D) }VR.A(C)= {.u :R. e |.C ' , S, t. S f t :i R . [t/u]e .EA(S; C . C ' ) } EA(S; C)= '' \n' {e : A |.C ,v. C[e] . (C . C )[v] . v .VA(C . C ) } Good(Oi)=C .x :i S(A) . Oi.x .VS(A)(C) V(\u00b7; C)= \n{\u00b7}V(G,x :i A; C)= {(., [v/x]) | . .V(G; C) . v .VA(C) }V(G,x :j>i A;S Ci \u00b7f C -Oi)= {(., [e/x]) | . \n.V(G; C) .\u00b7; \u00b7;Oi f e :j A } V!(\u00b7)= {\u00b7} V!(.,x :i A)= {(., [v/x]) | . .V!(G) . v .VA(D) } V!(.,x :j>i \nA)= {(., [e/x]) | . .V!(G) .\u00b7; \u00b7; \u00b7f e :j A } S f . :T S f t :i R S ' f . :T \u00b7f\u00b7 : \u00b7 S, S ' f (., [t/u]) \n: (T,u :i R) Figure 4. Logical Relation for Termination These properties are all proved by structural \ninduction. The syntax ensures expressions at time j>i do not depend on a variable of time i. As a result, \nwe only need to consider contexts in which . and G contain variables no younger than the current time. \nNormalization immediately follows: Corollary 1. (Normalization) Suppose S; \u00b7; \u00b7f e :i A. Then D[e] . \nC[v]. Finally note that we are considering normalization of open terms, since we have no constants of \ntype .. The non-existence of such constants is, of course, what ensures that the language respects space \nbounds. Theorem 4. (Space Bounded Evaluation) Suppose S; \u00b7; \u00b7f e :i A and D[e] . C[v]. Then the size \nof C the number of cons cells it binds is bounded by the size of S. Given type preservation, this theorem \nis straightforward. Each cons cell in the context needs a distinct resource variable, so the number of \ncons cells in C is clearly bounded by the size of S.  4.2 Next-Step Operational Semantics Recall that \nwhen we advance time, we want to replace the tails of stream variables with just the variable. We de.ne \nthe necessary operations in Figure 7. To tick the clock, we de.ne the relation C[x] . e, which takes \na stream in context C[x] and constructs a new expression e by cutting the tails of the streams in C, \nand sending each tail expressions tail(y) in C to y. This models the effect of advancing time by one \ntick, so that all the streams in the expression context become their tails, and all references to tails \nbecome references to the updated stream variable. To show the type-correctness of this operation, we \nintroduce the Step(Oi) operation, which tells us how the typing changes: it sends all streams at time \ni to time i +1 (and leaves streams at time i +1 alone). Now, we can prove the following type soundness \ntheorem. Theorem 5. (Ticking the Clock) If S; \u00b7;Oi f C[x]:i S(A) and C[x] . e, then S; \u00b7; Step(Oi) f \ne :i+1 S(A). This theorem follows from a routine structural induction, and establishes that our semantics \nis space-bounded. Since S is the same before and after the tick from C[x] . e, it follows that evalu\u00adating \ne will never construct more streams than S permits. Together with the within-step type preservation property, \nour clock-ticking theorem gives a (purely syntactic) proof of the space-boundedness of our language. \nHowever, while our de.nition of advancing the clock is intu\u00aditively plausible, and even makes proving \nmemory safety easy, it is still unclear in what sense it is correct. To illustrate the issues involved, \nobserve that our language contains lambda-expressions (which may contain free variable references), and \nthat the tick op\u00aderator essentially imperatively updates stream values. It is impor\u00adtant to prove that \nticking the clock cannot change the meaning of a lambda-term in a way which violates the purely functional \ncharac\u00adter of functional reactive programming. To show this, we will .rst give a denotational semantics \nof stream programs. Then we will show that if C[x] . e, then the meaning of e does in fact equal the \ntail of the denotational meaning of C[x], showing that ticking the clock really does advance the clock \nin the way we expect. This is morally a standard adequacy proof, applied to an unusual operational semantics. \n5. Semantic Intuitions 5.1 Causality and Ultrametric Spaces The intuition underpinning reactive programming \nis the stream transformer, a function which takes a stream of inputs and gen\u00aderates a stream of outputs. \nBut not all functions on streams are im\u00adplementable reactive programs in order to be implementable at \nall, reactive programs must respect the causality condition. That is, the .rst n outputs of a stream \nfunction may depend on at most its .rst n inputs. Writing LxsJn for the n-element pre.x of the stream \nxs, we formalize causality as follows: De.nition 1. (Causality) A stream function f : A. . B. is causal, \nwhen for all n and all streams as and as ', we have that if LasJ= Las 'Jthen Lf asJ= Lf as 'J. nn nn \nFurthermore, reactive programs often de.ne streams by feed\u00adback. If a stream transformer can produce \nthe .rst value of its out\u00adput without looking at its input, then we can constructing a .xed point via \nfeedback, taking the n-th output and supplying it as the input at time n +1. So as long as we can generate \nmore than n out\u00adputs from the .rst n inputs, we can .nd a .xed point. Formalizing this gives us a de.nition \nof guardedness for de.ning .xed points: De.nition 2. (Guardedness) A function f : A. . B. is guarded, \nwhen there exists k> 0 such that for all n and all streams as and as ', if LasJ= Las 'Jthen Lf asJ= Lf \nas 'J nn n+kn+k.  S Ci Oi f C -Oi ' EXTNIL S Ci Oi f D -Oi \u00b7; \u00b7;Oi f v :i A S ' ,u ' :i+1 .; \u00b7;Oi f \ne :i+1 S(A) S Ci Oi,x :i S(A) f C -O ' i EXTCONS S,u :i ., S ' Ci Oi f let x = cons(u, v, u ' .e) in \nC -Oi' ,x :i S(A) y :i S(A) . Oi S Ci Oi,x :i+1 S(A) f C -O ' i EXTTAIL S Ci Oi f let x = tail(y) in \nC -Oi' ,x :i+1 S(A) C . C ' '' D . C \" C (let x = tail(y) in C) . C ' \" let x = tail(y) in (C . C ' ) \n(let x = cons(u, v, u ' .e) in C) . C ' \" let x = cons(u, v, u ' .e) in (C . C ' ) C@x . v C = C ' . \nlet y = tail(z) in D C ' @x . v LOOKUPTAIL C@x . v ' '' C = C . let y = cons(u,v ,u .e) in D C ' @x . \nv LOOKUPNEXT C@x . v C = C ' . let x = cons(u, v, u ' .e) in D LOOKUPCURR C@x . v Figure 5. Context \nTyping and Operations However, these de.nitions apply only to stream functions, and real programs need \nmore types than just the stream type. So we need generalisations of causality which work at other types \nsuch as streams of streams and higher-order functions. To generalize these de.nitions, we follow our \nearlier work [13] by moving to a category of metric spaces. A complete 1-bounded bisected ultrametric \nspace A (which we will simply call ultrametric space ) is a pair (|A|,d), where |A| is a set and d .|A|\u00d7|A|. \n[0, 1] is a distance function satisfying the following properties: 1. d(x, y)=0 iff x = y 2. d(x, y)= \nd(y, x) 3. d(x, z) = max(d(x, y),d(y, z)) 4. d(x, y)=0 or 2-n for some n 5. All Cauchy sequences have \nlimits  We take the morphisms between ultrametric spaces to be the non\u00adexpansive maps f : A . B. These \nare the set-theoretic functions f .|A|.|B| such that: For all a, a ' .|A|, we have dB(f a,f a ' ) = dA(a, \na ' ) That is, a morphism between A and B is a function f such that it takes any two points in A to two \npoints in B that are at least as close it is a non-distance-increasing function. C[e] . C ' [v] VALE \nC[v] . C[v] C[e] . C ' [.x : A. e '' ] '' '' '' '' '''' C [e ] . C [v] C [[v/x]e ] . C [v ] APPE ' ''' \n' C[ee ] . C [v ] ' ' ''' C[e] . C [.u : R. e ' ] C [[t/u]e ] . C [v] LAPPE C[et] . C '' [v] C[e] . C \n' [v] ''' ' x fresh C = C . let x = cons(t,v,u. e ) in D CONSE C[cons(t,e,u. e ' )] . C '' [x] C[e] . \nC ' [x] C ' @x . cons(t,v,u. e) HEADE C[head(e)] . C ' [v] ' ' ''' C[e] . C [y](C . let x = tail(y) \nin D)[e ] . C [v] TAILE C[let x = tail(e) in e ' ] . C '' [v] ' ' ' '''' C[e] . C [ e1] C [[e1/x]e ] \n. C [v ] E ' '' '' C[let x = e in e ] . C [v ] C[[.x x : A. e/x]e] . C ' [v] C[.x x : A. e] . C ' [v] \nFIXE ' ' ' '''' C[e] . C [v] C [[v/x]e ] . C [v ] LETE ' '' '' C[let x = e in e ] . C [v ] ''' ' C[[t1/u, \nt2/v]e ] . C [v ] C[e] . C [v] .E !I ''' ' C[let (u, v) = (t1,t2) in e ] . C [v ] C[!e] . C [!v] ' ' \n' '''' C[e] . C [!v] C [[v/x]e ] . C [v ] !E ' '' '' C[let !x = e in e ] . C [v ] Figure 6. Within-Step \nOperational Semantics The category of ultrametric spaces is useful for two reasons. First, the causal \nstream functions are exactly the nonexpansive maps between spaces of streams with the Cantor metric (i.e., \nthe distance between two streams is 2-n, where n is the .rst position at which they disagree). Since \nthe category of 1-bounded complete ultrametric spaces is Cartesian closed, we have our higher-type generalisation \nof causality one which would be very dif.cult to .nd from purely operational considerations. Second, \nnonempty metric spaces satisfy Banach s theorem, which lets us de.ne .xed points at arbitrary types: \nProposition 1. (Banach s Contraction Map Theorem) If A is a nonempty complete metric space, and f : A \n. A is a strictly contractive function, then f has a unique .xed point.  5.2 Modeling Space Bounds with \nLength Spaces As noted earlier, simple casuality still allows undesirable functions. Requiring a stream \nfunction to depend only on its history does not prevent it from depending on its whole history.  C[z] \ne D[z] x C[z] e let x = tail(y) in C[z] let x = y in e C[z] e '' '' let x = cons(u,v,u . e ) in C[z] \nlet x =[u/u ]e in e Step(Oi) Step(\u00b7)= \u00b7 Step(Oi,x :i S(A)) = Step(Oi),x :i+1 S(A) Step(Oi,x :i+1 S(A)) \n= Step(Oi),x :i+1 S(A) StepOi (.) . [[Oi] i . [ Step(Oi)]]i Step(())= () Step\u00b7Oi,x:iS(A)(., vs) =(StepOi \n(.), tail(vs)) StepOi,x:i+1S(A)(., vs)=(StepOi (.), vs) Figure 7. The Next Step Operator To deal with \nthis issue, we adapt the length spaces of Hofmann [10], which give a model of space-bounded computation. \nThe idea behind this model is to start with a partially ordered resource monoid R representing space \nresources (N in the original work). One then constructs the category of length spaces as follows. A length \nspace A is a pair (|A|,sA : |A|. R), consisting of a set of elements |A| and a size function s which \nassigns a size sA(a) to each element a .|A|. A morphism of length spaces f : A . B is a non-size-increasing \nfunction. That is, it is a set\u00adtheoretic function f .|A|.|B| with the property that: .a .|A|.sB (fa) \n= sA(a) The programming language intuition is that a morphism A . B is a term of type B with a free variable \nin A, and so a term cannot use more memory than it receives from its environment. To model the permission \nto allocate, we can de.ne a length space of type .\" (1,.(). 1). The space . is uninteresting computationally \n(its set only has the unit in it), but it brings a permission to allocate with it. So we can model computations \nwhich do allocation by giving them permission elements, thereby controlling the allocation performed. \n6. The Denotational Semantics 6.1 The Resource Model In the synchronous data.ow model, there is a global, \nambient notion of time. Furthermore, higher-order reactive programs can create a data.ow graph dynamically, \nby waiting for an event before choosing to build cons cells to do some computation. So we need a resource \nstructure capable of modelling space usage over time. Therefore we take resources to be the monoidal \nlattice R = (Time . Space, ., max, T, min, 0, ., =), where Time = N, and Space = Nl{8} (the vertical \nnatural numbers with a topmost element). Intuitively, time is discrete, and measured in ticks. Space \ncounts the number of cons cells used in the program, and may be in.nite (obviously, we cannot implement \nsuch programs). We de.ne the lattice operations as follows: 1. . = .k. 0 2. T = .k. 8 3. 0= .k. 0 \n4. max(c, d)= .k. max(ck,dk) 5. min(c, d)= .k. min(ck,dk) 6. c . d = .k. ck + dk 7. c = d iff .k . \nTime, we have ck = dk  Essentially, we lift the lattice structure of the vertical natural num\u00adbers pointwise \nacross time (with (0, +) as the monoidal structure), so that a resource c . R describes the number of \ncons cells that are used at each time step. We then turn R into an ultrametric space by equipping it \nwith the Cantor metric: dR(c, d)=2-n where n = min {k . Time | ck = dk } 6.2 The Category of Complete \nUltrametric Length Spaces A complete 1-bounded bisected ultrametric length space A (which we will gloss \nas metric length space ) is a tuple (|A|, d, s), where (|A|,d) is a complete 1-bounded bisected ultrametric \nspace, and sA : |A|. R is a size function giving each element of |A| a size drawn from R. Furthermore, \nthe size function s : |A|. R must be a nonex\u00adpansive map between (|A|,d) and (R, dR). Nonexpansiveness \nen\u00adsures that we cannot tell if the memory usage requirements of two elements of |A| differs until we \nknow that the elements themselves differ. In addition to being intuitively reasonable, this requirement \nensures that limits of Cauchy sequences will be well-behaved with respect to size, which we need to ensure \nthe completeness of the size-0 subspace of A that we use to interpret !A. The morphisms of this category \nare the nonexpansive size\u00adpreserving maps f : A . B, which are the set-theoretic functions f .|A|.|B| \nsuch that: For all a, a ' .|A|, we have dB(f a,f a ' ) = dA(a, a ' ) For all a .|A|, we have sB(fa) = \nsA(a) That is, the morphisms we consider are the functions which are both causal and space-bounded. \n 6.3 Categorical Structure Metric length spaces and nonexpansive size-preserving maps form a category \nthat we use to interpret our programming language. First, it forms an intuitionistic bicartesian BI category, \nwhich is a doubly\u00adclosed category with both cartesian and monoidal closed structure, as well as supporting \ncoproduct structure. Second, this category also models the resource types . of Hofmann [10], as well \nas a resource-freedom modality !A, which is comonadic in the usual fashion of linear logic. Third, it \nsupports a version of the delay modality of our earlier work [13], which lets us interpret guarded recursion \nvia Banach s .xed point theorem. We give the de.nitions of all of these objects below. In Figure 9, we \nde.ne the distance and size functions, and in Figure 8, we give the natural transformations associated \nwith the objects. 1=({*},d1,s1)  A + B =(|A| + |B|,dA+B,sA+B)  A \u00d7 B =(|A|\u00d7|B|,dA\u00d7B,sA\u00d7B)  A . B \n=(|A|.|B|,dA.B ,sA.B)  A + B =(|A| + |B|,dA+B,sA+B   A*B =(|A|\u00d7|B|,dA;B,sA;B )  A -*B =(|A|.|B|,dA-;B \n,sA-;B) A =(|A|,d A,s A)  S(A)=(|A|.,dS(A),sS(A))  !A =({a . A | sA(a)=0 } ,s!A)  . =({*},d1,s.) \n0 =({*},d1,s<) The construction of Cartesian and monoidal products closely fol\u00adlows that of Hofmann [10]. \nThe Cartesian product is a sharing product , in which the associated resources are available to both \ncomponents (this explains the use of max), and the monoidal prod\u00aduct is a disjoint product , in which \nthe resources are divided be\u00adtween the two components (explaining the use of . in the size func\u00adtion). \nThe best intuition for the closed structure comes from imple\u00admenting .rst-class functions as closures: \nthe monoidal exponential A -*B takes an argument which does not share with the captured environment, \nand the Cartesian exponential A . B which does. A difference between our work and earlier work on length \nspaces is our heavy use of the category s Cartesian closed structure. Indeed, dal Lago and Hofmann [15] \nuse a realizability model to re\u00admove the Cartesian closed structure from their semantics they wished \nto prevent duplicated variables in lambda-terms from en\u00adabling large increases in the size of a lambda-term \nunder reduction, since this makes establishing strict resource bounds more dif.cult. As we only want \nto track the allocation of cells, but wish to allow free sharing otherwise, the CCC structure takes on \na central role in our model. Our next-step modality s metric d A is the same as in Krish\u00adnaswami and \nBenton [13], but the size function s A (which shifts all sizes 1 timestep into the future relative to \nsA), means that (A . B) i A . B. This breaks with Krishnaswami and Benton [13] and Nakano [18], signi.cantly \nchanging the elimina\u00adtion rules. As mentioned earlier, this limitation is intentional: we do not want \ndelay operators at types for which delay would be expen\u00adsive. Our semantics rules out such maps with \nthe size function for streams plus the requirement that morphisms are non-size\u00adincreasing. The size function \nfor streams gives a size for the stream as 1 to account for the size of the stream itself, plus the maxi\u00admum \nspace usage of all the values the stream takes on. Intuitively, a stream can seen as taking the space \nfor an in.nitary Cartesian prod\u00aduct A \u00d7 A \u00d7 2A \u00d7 ..., plus a constant for the stream cell itself. This \nis the only place where we increment the size of a value rel\u00adative to its components, which justi.es \nthe idea that sizes measure the number of cons cells. Since delaying a stream shifts its time us\u00adage \nby one step, we have no a priori reason to expect that a delay map will exist at all types. However, \nfor types such as N, !A, and ., there do exist maps A . A, which is why time subsumption is justi.ed \nfor the linear and pure contexts. Furthermore, all types whose values are all of size zero have maps \nA .!A. As a result, we can introduce con\u00adstants corresponding to such maps for these types, allowing \ntypes such as numbers and booleans be promoted to the pure fragment. (In fact, our implementation applies \nthese coercions implicitly, pro\u00adviding a slightly friendlier syntax than presented here.) Our semantic \nmodel contains a space . to interpret the resource type ., which gives 1 unit of space at every time \ntick. Our model uses the additional metric length space 0, which gives 1 unit of space at time 0, and \nno units of space at any other time. This lets us give a nice type to cons : 0 * (A \u00d7 S(A)) . S(A). Note \nthat the type A \u00d7 S(A) lacks the space to form a stream we need 1 unit of space at time 0, which neither \nthe A nor the S(A) provide. 1: A . I = .a. * p1 : A \u00d7 B . A = .(a, b).a p2 : A \u00d7 B . B = .(a, b).b (f, \ng) : A . B \u00d7 C = .a. (f a,g a) where f : A . B, g : A . C . (f) : A . B . C = .a. .b. f(a, b) where f \n: A \u00d7 B . C eval :(A . B) \u00d7 A . B = .(f, a).f a f *g : A*B . C*D = .(a, b). (f a,g b) where f : A . C, \ng : B . D a :(A*B) *C . A* (B*C)= .((a, b),c). (a, (b, c)) a-1 : A* (B*C) . (A*B) *C = .(a, (b, c)). \n((a, b),c) . : A*B . B*A = .(a, b). (b, a) . : A*I . A = .(a, *).a .-1 : A*I . A = .a. (a, *) . (f) : \nA . B -*C = .a. .b. f(a, b) where f : A*B . C eval-; :(A -*B) *A . B = .(f, a).f a E :!A . A = .a. a \nf :!A .!B = .a.f a where f :!A . B !f :!A .!B = .a.f a where f : A . B d :!A . !A = .a. a f : A . B = \n.a.f a where f : A . B . :! A . !A = .a. a .-1 : !A . A = .a. a head : S(A) . A = .(x \u00b7 xs).x tail : \nS(A) . S(A)= .(x \u00b7 xs). xs cons : 0 * (A \u00d7 S(A) . S(A)= .(*, (x, xs)).x \u00b7 xs split : . .0 * . = .*. (*, \n*) split-1 : 0 * . . . = .(*, *). * .x : !( !A .!A) .!A = .f. \u00b5(f) . : A \u00d7 B . (A \u00d7 B)= .(a, b). (a, \nb) .-1 : (A \u00d7 B) . A \u00d7 B = .(a, b). (a, b) .; : A* B . (A*B)= .(a, b). (a, b) .-1 ; : (A*B) . A* B = \n.(a, b). (a, b) . : A*B . A \u00d7 B = .(a, b). (a, b) s : A\u00d7!B . A*!B = .(a, b). (a, b) . : !(A \u00d7 B) .!A*!B \n= .(a, b). (a, b) .-1 :!A*!B .!(A \u00d7 B)= .(a, b). (a, b) Figure 8. Categorical Combinators  6.4 Denotational \nInterpretation We give the interpretation of types and contexts in Figure 10. The interpretation of types \noffers no surprises, but the interpretation of contexts is relative to the current time. The interpretations \nof T and . keeps hypotheses at times earlier than the current time, but G simply drops all earlier hypotheses. \nThis corresponds to the difference between the type rules RHYP and PHYP on the one hand, and the rule \nEHYP on the other. In all three cases, future hypotheses are interpreted with the delay modality. In \nFigure 11, we give a time-indexed interpretation function for expressions, [[T; .; G f e :i A] i which \nhas the type [[T]]i*![[.]]i * [[G]]i . [ A] . The interpretation of I makes use of the functoriality \nof A to interpret the body of the delay in the future, and then bring it back to the past, with the necessary \naction on contexts de.ned in Figure 10. The other rules are as expected, with the resource context managed \nin a single-threaded way and the other contexts duplicated freely. We can then show that this semantics \nis sound with respect to substitution.  . . dA(x, x ' ) if v = inl x . v ' = inl x ' dA+B = .(v, v ' \n). . dB (y, y ' ) 1 if v = inr y . v ' = inr y ' otherwise d1 dA\u00d7B = = .(() , ()). 0 .((a, b), (a ' , \nb ' )). max(dA(a, a ' ), dB (b, b ' )) dA;B = .((a, b), (a ' , b ' )). max(dA(a, a ' ), dB (b, b ' )) \ndA.B dA-;B d A dS(A) d!A = = = = = .(f, g). max {dB(f a, g a) | a . |A| }.(f, g). max {dB(f a, g a) | \na . |A| }.(a, a ' ). 1 2 dA(a, a ' ) .(xs, ys). max 2-n \u00b7 dA(xsn, ysn) | n . N .(a, a ' ). dA(a, a ' \n) d. d< = = .(() , ()). 0 .(() , ()). 0 sA+B = .v. sA(a) sB (b) if v = inl a if v = inr b s1 sA\u00d7B = \n= .(). 0 .(a, b). max(sA(a), sB (b)) sA;B = .(a, b). sA(a) . sB (b) sA.B sA-;B s A = = = .f. min {k . \nR | .a . |A|. sB(f a) = max(k, sA(a)) }.f. min {k . R | .a . |A|. sB(f a) = k . sA(a) } sA sS(A) s. s< \ns!A = = = = .xs. .k. 1 . max ( isA)(xsi) | i . Time .(). .k. 1 .(). .k. if k = 0 then 1 else 0 .a. 0 \ns = .a. .k. if k = 0 then 0 else s(a)(k - 1) Figure 9. The Distance and Size Functions Theorem 6. (Semantic \nSubstitution) Suppose . . [[T]]i, . ' . [[T ' ] i, p . [[.]]i and . . [[G]]i. Then 1. If T f t :i R and \nT ' ,u :i R f t ' :j R ', then [[T, T ' f [t/u]t ' :j R ' ] (., . ' ) is equal to ' :j R ' [[T ' ,u \n:i R f t ] (. ' , [[T f t :i R] .). 2. If T f t :i R and T ' ,u :i R; .; G f e :j A, then [[T, T ' ; \n.; G f [t/u]e :j A] ((., . ' ), p, .) equals [[T ' ,u :i R; .; G f e :j A] ((. ' , [[T f t :i R] .), \np, .)  3. If \u00b7; .; \u00b7f e :i A and T; .,x :j A;G f e ' :k B and i = j, then [[T; .; G f [e/x]e ' :k B] \n(., p, .) equals [[T; .,x :j A;G f e ' :k B] (., (p, [ \u00b7; .; \u00b7f e :i A] (() , p, ())),.) 4. If \u00b7; .; \nG f e :i A and T; .; G,x :i A f e ' :j B, then T; .; G f [e/x]e ' :j B equals [[T; .; G,x :i A f e ' \n:j B] (., p, (., [ \u00b7; .; G f e :i A] (() , p, .)))  These theorems follow from structural induction \non the typing derivation of the term being substituted into. Figure 12, gives the interpretation of contexts, \nusing the ex\u00adpression semantics to de.ne the meaning of each stream bound by the context. This interpretation \nis sound with respect to the hole\u00ad.lling of terms in contexts: Theorem 7. (Context Soundness) If S Ci \nOi f C -O ' i and S ' ; \u00b7;Oi, O ' i f e :i A and s . [[S]]i and s ' . [[S ' ] i and . . [[Oi] i, then \n[[S, S ' ; \u00b7;Oi f C[e]:i A] ((s, s ' ),.) is equal to [[S ' ; \u00b7;Oi, O ' i f e :i A] (s ' , () , (., [[S \nCi Oi f C -O ' i] (s, .)). Now we can show that the within-step operational semantics is sound with respect \nto the denotational model: Theorem 8. (Soundness of Within-Step Semantics) Let S; \u00b7; \u00b7f C[e]:i A and \nC[e] . C ' [v]. Then [[S; \u00b7; \u00b7f C[e]:i A] equals [[S; \u00b7; \u00b7f C ' [v]:i A] . Finally, we can show that \nadvancing the clock has the expected semantics: Theorem 9. (Soundness of Advancing the Clock) Let S; \n\u00b7;Oi f C[x]:i S(A), and s . [[S]]i and . . [[Oi] i, and suppose C[x] e. Then we have that tail([[S; \u00b7;Oi \nf C[x]:i S(A)]] (s, () ,.)) is equal to [[S; \u00b7; Step(Oi) f e :i+1 S(A)]](s ' ,(),. ' )) where s ' = Nexti \nS s and . ' = StepOi (.). This theorem connects the operation of stepping the heap with the denotational \ninterpretation each time we advance the clock, each stream in a closed context C will become its tail. \nSo advancing the clock successively enumerates the elements of the stream. 7. Discussion In this paper, \nwe have introduced an expressive type system for writing stream programs, and given an operational semantics \nre\u00adspecting the space-ef.ciency claims of the type system. Our seman\u00adtic model is one of the primary \ncontributions of this paper, since it lets us reason about space usage without surrendering the sets-and\u00adfunctions \nview of FRP. Also, our model contains many operations which are not currently expressible in our language: \nfor example, in the future we might want richer types in the af.ne context and function space, so that \noperations like in-place map can be typed !(A . B) . S(A) -*S(B). Our language contains an explicit delay \nmodality (as in logi\u00adcal presentations of step-indexing [2, 7, 18]) and an update-based operational semantics. \nThe reader may .nd it surprising that, al\u00adthough our operational semantics does make use of a form of \nmu\u00adtable higher-order store, the logical relation we give is not step\u00adindexed. The reason this is possible \nis essentially that FRP is a syn\u00adchronous model of computation, in which all the updates happen in a \nseparate phase from ordinary functional evaluation. This explains why we were able to present a two-phase \nsemantics, and since no heap modi.cations take place during ticks, there is no need for step\u00adindexing. \nWan et al. [25] introduced real-time FRP; a restricted subset of FRP sharing many of the same design \nchoices of synchronous data.ow languages. It is essentially .rst-order (streams can carry general values \nof the host language, but these values can not them\u00adselves refer to streams), and makes use of a novel \ncontinuation typ\u00ading to ensure that all recursive signals are tail-recursive. As a result, the language \nrequires only constant-stack and constant-time FRP reductions. Event-driven FRP [26] is similar, but \nrelaxes FRP s tim\u00ading constraints by dropping the global clock. None of our examples using higher-order \nfunctions or nested streams can be programmed in real-time FRP, which is carefully engineered to avoid \nexposing a .rst-class behaviour type, and so cannot express higher-order operations on streams. All productive \nreal-time FRP programs can be written in our language, since we can de.ne all of real-time FRP s stream \noperators (switching, delays, multimodal recursion) as ordinary user-level programs. Liu et al. s causal \ncommutative arrows [17] are another attempt to control the memory and space usage of reactive programs. \nThis work takes advantage of the fact that pure arrowized FRP (i.e., without switching) builds .xed data.ow \ngraphs, allowing programs to be optimized into single-loop code via a transformation reminis\u00adcent of \nthe Bohm-Jacopini theorem. Our language does not seem to support such an elegant normal form, because \nof the presence  [ A . B] =[ A] . [ B] [ S(A)]] = S([[A] ) [ A] = [ A] [[!A] = ![[A] [ R -A] =[ R] -*[ \nA] [ .] = . [ R . S] =[ R] * [ S] [[T]]i [[.]]i [[G]]i [[T]]i . Obj [ \u00b7] i = I [[T,x :n R] i = [[T]]i \n* n-i[ R] when n = i [[T,x :n R] i = [[T]]i * [ R] when n<i [[.]]i . Obj [ \u00b7] i = I [[.,x :n A] i = [[.]]i \n\u00d7 n-i[ A] when n = i [[.,x :n A] i = [[.]]i \u00d7 [ A] when n<i [[G]]i . Obj [ \u00b7] i = I [[G,x :n A] i = [[G]]i \n\u00d7 n-i[ A] when n = i [[G,x :n A] i = [[G]]i when n<i i i NextT . [[T]]i . [[T]]i+1 Next. . [[.]]i . [[.]]i+1 \nNextG i . [[G]]i . [[G]]i+1 Nexti \u00b7 () = ()Nexti T,x:nR (., r) = (Nexti T ., d(r)) if n = i Nexti T,x:nR \n(., r) = (Nexti T ., r) if n>i Nexti \u00b7 () = ()Nexti .,x:nA(p, v) = (Nexti . p, d(v)) if n = i NextiA(p, \nv) = (Nexti if n>i . p, v) .,x:n Nexti \u00b7 () = () Nexti = G if n = i G,x:nA(.,v) Nexti Nexti G,x:nA(.,v) \n= (Nexti G .,v) if n>i Figure 10. Interpretation of Types and Contexts of higher-order streams and functions, \nbut it would nonetheless be interesting to investigate related optimizations in our setting. Sculthorpe \nand Nilsson s work [22] on safe functional program\u00adming uses Agda s types to ensure productivity, by \nhaving depen\u00addent types track whether signal processors have delays before per\u00admitting feedback. Our \nguardedness modality is simpler but less .exible, since it cannot depend on the values a signal produces. \nHowever, the advantage of our modality is that it works smoothly at higher-order. Cooper and Krishnamurthi \n[6] described the FrTime system, which embeds FRP into the PLT Scheme (now Racket) implemen\u00adtation. One \ncommonality between FrTime and our work is that switching does not come from special primitives, but \nfrom or\u00addinary conditionals and case statements. Unlike our denotational model, Cooper s operational \nsemantics [5] exempli.es the imper\u00adative FRP tradition, in which reactivity is modelled explicitly as \n[[T f t :i R] i . [[T]]i . [ R] [[T; .; G f e :i A] i . [[T]]i *![[.]]i * [[G]]i . [ A] [[T f u :j \nR] i . = .(u) [[T, T ' f (t, t ' ):j R . S] i (., . ' ) = ([[t] ., [ t ' ] . ' ) [[T; .; G f x :j A] \ni (., p, .)= p(x)(if x :i A . .) [[T; .; G f x :i A] i (., p, .)= .(x)(if x :i A . G) [[T; .; G f .x \n: A. e :i A . B] i (., p, .)= .v. [ \u00b7; .; G,x :i A f e :i B] i (() , p, (.,v)) [[T, T ' ; .; G f ee ' \n:i B] i ((., . ' ), p, .)= let f =[ e : A . B] i (., p, .) in let v =[ e ' : A] i (. ' , p, .) in eval(f, \nv) [[T; .; G f !e :i !A] i (., p, .)= [ \u00b7; .; \u00b7f e :i A] (() , p, ()) i [[T, T ' ; .; G f let !x = e \nin e ' :i B] i (., p, .)= [ e ' ] i (. ' ,.-1(p, [ e] i (. ' , p, .)),.) , T '' '' [[T, T ' ; .; G f \ncons(t, e, u .e ):i S(A)]]i ((., . ' ,. '' ), p, .)= let (d, r)= split([[t] i .) in let h =[ e] i (. \n' , p, .) in ' . '' let t = [ e ] i+1 ((Nexti ,r), Nexti G .) in T!! . p, Nexti cons(d, (h, t)) [[T; \n.; G f head(e):i A] i (., p, .)= head([[T; .; G f e :i S(A)]]i (., p, .)) [[T, T ' ; .; G f let x = tail(e) \nin e ' :i B] i ((., . ' ), p, .)= let vs = [[T]]i.GeS(A)i in (., p, .) [[T ' ; .; G,x :i+1 S(A) f e ' \n:i B] i (. ' , p, (., tail(vs))) [[T; .; G f e :i A] i (., p, .)= [ \u00b7; .; G f e :i+1 A] i+1 (() , Nexti \nG .). p, Nexti [[T, T ' ; .; G f let x = e in e ' :i B] i ((., . ' ), p, .)= let v = [[T; .; G f e :i \nA] i (., p, .) in [[T ' ; .; G,x :i+1 A f e ' :i B] i (. ' , p, (., v)) [[T; .; G f .u : R. e :i R -A] \ni (., p, .)= .r. [ u :i R; .; G f e :i A] (r, p, .) [[T, T ' ; .; G f et :i A] i ((., . ' ), p, .)= eval-;([[e] \n(., p, .), [ t] . ' ) [[T, T ' ; .; G f let (u, v)= t in e :i A] i ((., . ' ), p, .)= let (r, s) = [[T \nf t :i R . S] . in [[T ' ,u :i R, v :i S; .; G f e :i A] ((. ' , r, s), p, .) [[T; .; G f .x x : A. e \n:i A] i (., p, .)= A = .-1 let p ' (p, .-1 v) in let f = .v. in [ \u00b7;.,x :i+1 A; \u00b7f e :i A] i (() ,p ' \n, ()).x(f) [[T, T ' ; .; G f let x = e in e ' :i B] i ((., . ' ), p, .)= let v = [[T; .; G f e :i A] \ni (., p, .) in [[T ' ; .; G,x :i A f e ' :i B] i (. ' , (p, v),.) Figure 11. Denotational Semantics of \nTerms  [[S Ci Oi f C -Oi ' ] . [[S]]i * [[Oi] . [[Oi ' ] [[S Ci Oi f D -\u00b7] (., .)= () [[S Ci Oi f let \ny = tail(x) in C -y :i+1 S(A), O ' i] (., .)= let v = tail(.(x)) in let . ' = [[S Ci Oi,y :i+1 S(A) f \nC -Oi ' ] (., (., v)) in (. ' ,v) , S '' ' [[S, S ' Ci Oi f let x = cons(u,v,u . e) in C -Oi' ,x :i+1 \nS(A)]] ((., . ' ,. '' ),.)= let (r, d) = [[S ' f u :i .] . ' in let h =[ \u00b7; \u00b7;Oi f v :i A] (() , () ,.) \nin let .1 '' = (Nexti S!! . '' ,d) in let .1 = NextO. i in let t = [[S '' ,u ' :i+1 .; \u00b7;Oi f e :i+1 \nS(A)]] (.1 '' , () ,.1' ) in let vs = cons(r, (h, t)) in let . ' = [[S Ci Oi,x :i S(A) f C -Oi ' ] (., \n(., vs)) in (. ' , vs) Figure 12. Interpretation of Contexts a kind of heap update in the operational \nsemantics. We think the operational semantics in this paper, which is quasi-imperative in .avour, is \nclose enough to both a denotational model and Cooper s semantics that it makes sense to study how to \nreunify the pure and the imperative .avours of FRP. Krishnaswami and Benton [14] have also presented \na language for writing reactive GUI programs. This language also makes use of linear types, but in this \ncase they are used to track the allocation of new GUI widgets. It is not yet clear to us how one might \ncombine space-boundedness with this kind of dynamism: we may need to add a dynamic allocation monad to \nour model to integrate the two lines of work. Supporting other dynamic data structures (not necessarily \nstreams) suggests looking at the work of Acar et al. [1], who have studied adding user-controlled incrementalization \nto self-adjusting computation, which shares many implementation issues with FRP. Conversely, it will \nbe worth investigating whether our denotational semantics can be adapted to provide a useful categorical \ncost se\u00admantics for self-adjusting computation[16]. References [1] U. A. Acar, G. E. Blelloch, R. Ley-Wild, \nK. Tangwongsan, and D. T\u00a8 urkoglu. Traceable data types for self-adjusting computation. In PLDI, pages \n483 496, 2010. [2] A. W. Appel, P.-A. Melli`es, C. D. Richards, and J. Vouillon. A very modal model of \na modern, major, general type system. In M. Hofmann and M. Felleisen, editors, POPL, pages 109 122. ACM, \n2007. ISBN 1-59593-575-4. [3] G. Berry and L. Cosserat. The ESTEREL synchronous programming language \nand its mathematical semantics. In Seminar on Concurrency, pages 389 448. Springer, 1985. [4] P. Caspi, \nD. Pilaud, N. Halbwachs, and J. Plaice. LUSTRE: A declar\u00adative language for real-time programming. In \nPOPL, 1987. [5] G. Cooper. Integrating data.ow evaluation into a practical higher\u00adorder call-by-value \nlanguage. PhD thesis, Brown University, 2008. [6] G. Cooper and S. Krishnamurthi. Embedding dynamic data.ow \nin a call-by-value language. Programming Languages and Systems, pages 294 308, 2006. [7] D. Dreyer, A. \nAhmed, and L. Birkedal. Logical step-indexed logical relations. In LICS, pages 71 80. IEEE, 2009. [8] \nC. Elliott and P. Hudak. Functional reactive animation. In ICFP, 1997. [9] M. Hofmann. A type system \nfor bounded space and functional in-place update. Nordic Journal of Computing, 7(4), 2000. [10] M. Hofmann. \nLinear types and non-size-increasing polynomial time computation. Information and Computation, 183(1), \n2003. [11] P. Hudak, A. Courtney, H. Nilsson, and J. Peterson. Arrows, robots and functional reactive \nprogramming. In Advanced Functional Pro\u00adgramming, volume 2638 of LNCS. Springer, 2003. [12] J. Hughes. \nGeneralizing monads to arrows. Sci. Comput. Program., 37(1-3), 2000. [13] N. Krishnaswami and N. Benton. \nUltrametric semantics of reactive programs. In LICS. IEEE, 2011. [14] N. Krishnaswami and N. Benton. \nA semantic model of graphical user interfaces. In ICFP, 2011. [15] U. D. Lago and M. Hofmann. Realizability \nmodels and implicit complexity. Theor. Comput. Sci., 412(20):2029 2047, 2011. [16] R. Ley-Wild, U. A. \nAcar, and M. Fluet. A cost semantics for self\u00adadjusting computation. In Z. Shao and B. C. Pierce, editors, \nPOPL, pages 186 199. ACM, 2009. ISBN 978-1-60558-379-2. [17] H. Liu, E. Cheng, and P. Hudak. Causal commutative \narrows and their optimization. In ICFP, 2009. [18] H. Nakano. A modality for recursion. In LICS, pages \n255 266, 2000. [19] H. Nilsson, A. Courtney, and J. Peterson. Functional reactive pro\u00adgramming, continued. \nIn ACM Haskell Workshop, pages 51 64. ACM, 2002. [20] P. W. O Hearn and D. J. Pym. The logic of bunched \nimplications. Bulletin of Symbolic Logic, 5(2), 1999. [21] M. Pouzet. Lucid Synchrone, version 3. Tutorial \nand reference man\u00adual. Universit\u00b4e Paris-Sud, LRI, 2006. [22] N. Sculthorpe and H. Nilsson. Safe functional \nreactive programming through dependent types. In ICFP, 2009. [23] D. Spoonhower, G. E. Blelloch, R. Harper, \nand P. B. Gibbons. Space pro.ling for parallel functional programs. In J. Hook and P. Thiemann, editors, \nICFP, pages 253 264. ACM, 2008. ISBN 978-1-59593-919\u00ad 7. [24] T. Uustalu and V. Vene. The essence of \ndata.ow programming. In K. Yi, editor, APLAS, volume 3780 of Lecture Notes in Computer Science, pages \n2 18. Springer, 2005. ISBN 3-540-29735-9.  [25] Z. Wan, W. Taha, and P. Hudak. Real-time FRP. In ICFP, \npages 146 156, 2001. [26] Z. Wan, W. Taha, and P. Hudak. Event-driven FRP. In PADL, pages 155 172, 2002. \n   \n\t\t\t", "proc_id": "2103656", "abstract": "<p>Functional reactive programming (FRP) is an elegant and successful approach to programming reactive systems declaratively. The high levels of abstraction and expressivity that make FRP attractive as a programming model do, however, often lead to programs whose resource usage is excessive and hard to predict. In this paper, we address the problem of space leaks in discrete-time functional reactive programs. We present a functional reactive programming language that statically bounds the size of the dataflow graph a reactive program creates, while still permitting use of higher-order functions and higher-type streams such as streams of streams. We achieve this with a novel linear type theory that both controls allocation and ensures that all recursive definitions are well-founded.</p> <p>We also give a denotational semantics for our language by combining recent work on metric spaces for the interpretation of higher-order causal functions with length-space models of space-bounded computation. The resulting category is doubly closed and hence forms a model of the logic of bunched implications.</p>", "authors": [{"name": "Neelakantan R. Krishnaswami", "author_profile_id": "81320491252", "affiliation": "Max Planck Institute for Software Systems, Saarbruecken, Germany", "person_id": "P2991336", "email_address": "neelk@mpi-sws.org", "orcid_id": ""}, {"name": "Nick Benton", "author_profile_id": "81100165244", "affiliation": "Microsoft Research, Cambridge, United Kingdom", "person_id": "P2991337", "email_address": "nick@microsoft.com", "orcid_id": ""}, {"name": "Jan Hoffmann", "author_profile_id": "81100074262", "affiliation": "Yale University, New Haven, CT, USA", "person_id": "P2991338", "email_address": "jan.hoffmann@yale.edu", "orcid_id": ""}], "doi_number": "10.1145/2103656.2103665", "year": "2012", "article_id": "2103665", "conference": "POPL", "title": "Higher-order functional reactive programming in bounded space", "url": "http://dl.acm.org/citation.cfm?id=2103665"}