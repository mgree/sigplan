{"article_publication_date": "01-25-2012", "fulltext": "\n A Rely-Guarantee-Based Simulation for Verifying Concurrent Program Transformations Hongjin Liang Xinyu \nFeng Ming Fu School of Computer Science and Technology, University of Science and Technology of China \nHefei, Anhui 230026, China lhj1018@mail.ustc.edu.cn xyfeng@ustc.edu.cn fuming@ustc.edu.cn Abstract Verifying \nprogram transformations usually requires proving that the resulting program (the target) re.nes or is \nequivalent to the original one (the source). However, the re.nement relation between individual sequential \nthreads cannot be preserved in general with the presence of parallel compositions, due to instruction \nreordering and the different granularities of atomic operations at the source and the target. On the \nother hand, the re.nement relation de.ned based on fully abstract semantics of concurrent programs assumes \narbitrary parallel environments, which is too strong and cannot be satis.ed by many well-known transformations. \nIn this paper, we propose a Rely-Guarantee-based Simulation (RGSim) to verify concurrent program transformations. \nThe rela\u00adtion is parametrized with constraints of the environments that the source and the target programs \nmay compose with. It considers the interference between threads and their environments, thus is less \npermissive than relations over sequential programs. It is composi\u00adtional w.r.t. parallel compositions \nas long as the constraints are sat\u00adis.ed. Also, RGSim does not require semantics preservation under all \nenvironments, and can incorporate the assumptions about en\u00advironments made by speci.c program transformations \nin the form of rely/guarantee conditions. We use RGSim to reason about opti\u00admizations and prove atomicity \nof concurrent objects. We also pro\u00adpose a general garbage collector veri.cation framework based on RGSim, \nand verify the Boehm et al. concurrent mark-sweep GC. Categories and Subject Descriptors D.2.4 [Software \nEngineer\u00ading]: Software/Program Veri.cation Correctness proofs, Formal methods; F.3.1 [Logics and Meanings \nof Programs]: Specifying and Verifying and Reasoning about Programs General Terms Theory, Veri.cation \nKeywords Concurrency, Program Transformation, Rely-Guarantee Reasoning, Simulation 1. Introduction Many \nveri.cation problems can be reduced to verifying program transformations, i.e., proving the target program \nof the transforma\u00adtion has no more observable behaviors than the source. Below we give some typical examples \nin concurrent settings: Permission to make digital or hard copies of all or part of this work for personal \nor classroom use is granted without fee provided that copies are not made or distributed for pro.t or \ncommercial advantage and that copies bear this notice and the full citation on the .rst page. To copy \notherwise, to republish, to post on servers or to redistribute to lists, requires prior speci.c permission \nand/or a fee. POPL 12, January 25 27, 2012, Philadelphia, PA, USA. Copyright c &#38;#169; 2012 ACM 978-1-4503-1083-3/12/01. \n. . $10.00 Correctness of compilation and optimizations of concurrent programs. In this most natural \nprogram transformation veri.ca\u00adtion problem, every compilation phase does a program transfor\u00admation T, \nwhich needs to preserve the semantics of the inputs.  Atomicity of concurrent objects. A concurrent \nobject or library provides a set of methods that allow clients to manipulate the shared data structure \nwith abstract atomic behaviors [15]. Their correctness can be reduced to the correctness of the transforma\u00adtion \nfrom abstract atomic operations to concrete and executable programs in a concurrent context.  Verifying \nimplementations of software transactional memory (STM). Many languages supporting STM provide a high-level \natomic block atomic{C}, so that programmers can assume the atomicity of the execution of C. Atomic blocks \nare imple\u00admented using some STM protocol (e.g., TL2 [11]) that allows very .ne-grained interleavings. \nVerifying that the .ne-grained program respects the semantics of atomic blocks gives us the correctness \nof the STM implementation.  Correctness of concurrent garbage collectors (GCs). High\u00adlevel garbage-collected \nlanguages (e.g., Java) allow program\u00admers to work at an abstract level without knowledge of the un\u00adderlying \nGC algorithm. However, the concrete and executable low-level program involves interactions between the \nmutators and the collector. If we view the GC implementation as a trans\u00adformation from high-level mutators \nto low-level ones with a concrete GC thread, the GC safety can be reduced naturally to the semantics \npreservation of the transformation.  To verify the correctness of a program transformation T,we follow \nLeroy s approach [19] and de.ne a re.nement relation . between the target and the source programs, which \nsays the target has no more observable behaviors than the source. Then we can formalize the correctness \nof the transformation as follows: Correct(T) . .C, C.C = T(C)=. C . C . (1.1) That is, for any source \nprogram C acceptable by T, T(C) is a re.nement of C. When the source and the target are shared\u00adstate \nconcurrent programs, the re.nement . needs to satisfy the following requirements to support effective \nproof of Correct(T): Since the target T(C) may be in a different language from the source, the re.nement \nshould be general and independent of the language details.  To verify .ne-grained implementations of \nabstract operations, the re.nement should support different views of program states and different granularities \nof state accesses at the source and the target levels.  When T is syntax-directed (and it is usually \nthe case for par\u00adallel compositions, i.e., T(C I C')= T(C) I T(C')), a com\u00ad   positional re.nement \nis of particular importance for modular veri.cation of T. However, existing re.nement (or equivalence) \nrelations cannot sat\u00adisfy all these requirements at the same time. Contextual equiva\u00adlence, the canonical \nnotion for comparing program behaviors, fails to handle different languages since the contexts of the \nsource and the target will be different. Simulations and logical relations have been used to verify compilation \n[4, 16, 19, 21], but they are usu\u00adally designed for sequential programs (except [21, 25], which we will \ndiscuss in Section 8). Since the re.nement or equivalence re\u00adlation between sequential threads cannot \nbe preserved in general with parallel compositions, we cannot simply adapt existing work on sequential \nprograms to verify transformations of concurrent pro\u00adgrams. Re.nement relations based on fully abstract \nsemantics of concurrent programs are compositional, but they assume arbitrary program contexts, which \nis too strong for many practical transfor\u00admations. We will explain the challenges in detail in Section \n2. In this paper, we propose a Rely-Guarantee-based Simulation (RGSim) for compositional veri.cation \nof concurrent transforma\u00adtions. By addressing the above problems, we make the following contributions: \n RGSim parametrizes the simulation between concurrent pro\u00adgrams with rely/guarantee conditions [17], \nwhich specify the interactions between the programs and their environments. This makes the corresponding \nre.nement relation composi\u00adtional w.r.t. parallel compositions, allowing us to decompose re.nement proofs \nfor multi-threaded programs into proofs for individual threads. On the other hand, the rely/guarantee \ncon\u00additions can incorporate the assumptions about environments made by speci.c program transformations, \nso RGSim can be applied to verify many practical transformations.  Based on the simulation technique, \nRGSim focuses on com\u00adparing externally observable behaviors (e.g., I/O events) only, which gives us considerable \nleeway in the implementations of related programs. The relation is mostly independent of the lan\u00adguage \ndetails. It can be used to relate programs in different languages with different views of program states \nand different granularities of atomic state accesses.  RGSim makes relational reasoning about optimizations \npossi\u00adble in parallel contexts. We present a set of relational reason\u00ading rules to characterize and justify \ncommon optimizations in a concurrent setting, including hoisting loop invariants, strength reduction \nand induction variable elimination, dead code elimi\u00adnation, redundancy introduction, etc..  RGSim gives \nus a re.nement-based proof method to verify .ne-grained implementations of abstract algorithms and con\u00adcurrent \nobjects. We successfully apply RGSim to verify con\u00adcurrent counters, the concurrent GCD algorithm, Treiber \ns non\u00adblocking stack and the lock-coupling list.  We reduce the problem of verifying concurrent garbage \ncollec\u00adtors to verifying transformations, and present a general GC ver\u00adi.cation framework, which combines \nunary Rely-Guarantee\u00adbased veri.cation [17] with relational proofs based on RGSim.  We verify the Boehm \net al. concurrent garbage collection algo\u00adrithm [7] using our framework. As far as we know, it is the \n.rst time to formally prove the correctness of this algorithm.  In the rest of this paper, we .rst analyze \nthe challenges for com\u00adpositional veri.cation of concurrent program transformations, and explain our \napproach informally in Section 2. Then we give the ba\u00adsic technical settings in Section 3 and present \nthe formal de.nition of RGSim in Section 4. We show the use of RGSim to reason about local r1; local \nr2; x:= 1; y:= 1; r1 := y; . r2 := x; if(r1=0)then if(r2=0)then critical region critical region (a) Dekker \ns Mutual Exclusion Algorithm x := x+1; . x := x+1; vs. local r1; local r2; r1:= x; . r2 := x; x:= r1 \n+1; x :=r2+1; (b) Different Granularities of Atomic Operations Figure 1. Equivalence Lost after Parallel \nComposition optimizations in Section 5, verify atomicity of concurrent objects in Section 6, and prove \nthe correctness of concurrent GCs in Sec\u00adtion 7. Finally we discuss related work and conclude in Section \n8. 2. Challenges and Our Approach The major challenge we face is to have a compositional re.nement relation \nbetween concurrent programs, i.e., we should be able to know T(C1) IT(C2) C1 I C2 if we have T(C1) C1 \nand T(C2) C2. 2.1 Sequential Re.nement Loses Parallel Compositionality Observable behaviors of sequential \nimperative programs usually re\u00adfer to their control effects (e.g., termination and exceptions) and .nal \nprogram states. However, re.nement relations de.ned cor\u00adrespondingly cannot be preserved after parallel \ncompositions. It has been a well-known fact in the compiler community that sound optimizations for sequential \nprograms may change the behaviors of multi-threaded programs [5]. The Dekker s algorithm shown in Figure \n1(a) has been widely used to demonstrate the problem. Re\u00adordering the .rst two statements of the thread \non the left preserves its sequential behaviors, but the whole program can no longer en\u00adsure exclusive \naccess to the critical region. In addition to instruction reordering, the different granularities of \natomic operations between the source and the target programs can also break the compositionality of program \nequivalence in a concurrent setting. In Figure 1(b), the target program at the bottom behaves differently \nfrom the source at the top (assuming each statement is executed atomically), although the individual \nthreads at the target and the source have the same behaviors.  2.2 Assuming Arbitrary Environments is \nToo Strong The problem with the re.nement for sequential programs is that it does not consider the effects \nof threads intermediate state accesses on their parallel environments. People have given fully abstract \nse\u00admantics to concurrent programs (e.g., [1, 8]). The semantics of a program is modeled as a set of execution \ntraces. Each trace is an interleaving of state transitions made by the program itself and ar\u00adbitrary \ntransitions made by the environment. Then the re.nement between programs can be de.ned as the subset \nrelation between the corresponding trace sets. Since it considers all possible envi\u00adronments, the re.nement \nrelation has very nice compositionality, but unfortunately is too strong to formulate the correctness \nof many well-known transformations, including the four classes of transfor\u00admations mentioned before: \n  Many concurrent languages (e.g., C++ [6]) do not give seman\u00adtics to programs with data races (like \nthe examples shown in Figure 1). Therefore the compilers only need to guarantee the semantics preservation \nof data-race-free programs.  When we prove that a .ne-grained implementation of a concur\u00adrent object \nis a re.nement of an abstract atomic operation, we can assume that all accesses to the object in the \ncontext of the target program use the same set of primitives.  Usually the implementation of STM (e.g., \nTL2 [11]) ensures the atomicity of a transaction atomic{C} only when there are no data races. Therefore, \nthe correctness of the transformation from high-level atomic blocks to .ne-grained concurrent code assumes \ndata-race-freedom in the source.  Many garbage-collected languages are type-safe and prohibit operations \nsuch as pointer arithmetics. Therefore the garbage collector could make corresponding assumptions about \nthe mu\u00adtators that run in parallel.  In all these cases, the transformations of individual threads are \nallowed to make various assumptions about the environments. They do not have to ensure semantics preservation \nwithin all contexts.  2.3 Languages at Source and Target May Be Different The use of different languages \nat the source and the target levels makes the formulation of the transformation correctness more dif\u00ad.cult. \nIf the source and the target languages have different views of program states and different atomic primitives, \nwe cannot directly compare the state transitions made by the source and the target pro\u00adgrams. This is \nanother reason that makes the aforementioned subset relation between sets of program traces in fully \nabstract semantics infeasible. For the same reason, many existing techniques for prov\u00ading re.nement or \nequivalence of programs in the same language cannot be applied either. 2.4 Different Observers Make \nDifferent Observations Concurrency introduces tensions between two kinds of observers: human beings (as \nexternal observers) and the parallel program con\u00adtexts. External observers do not care about the implementation \nde\u00adtails of the source and the target programs. For them, intermediate state accesses (such as memory \nreads and writes) are silent steps (unobservable), and only external events (such as I/O operations) \nare observable. On the other hand, state accesses have effects on the parallel program contexts, and \nare not silent to them. If the re.nement relation relates externally observable event traces only, it \ncannot have parallel compositionality, as we ex\u00adplained in Section 2.1. On the other hand, relating all \nstate ac\u00adcesses of programs is too strong. Any reordering of state accesses or change of atomicity would \nfail the re.nement.  2.5 Our Approach In this paper we propose a Rely-Guarantee-based Simulation (RGSim) \n: between the target and the source programs. It es\u00adtablishes a weak simulation, ensuring that for every \nexternally ob\u00adservable event made by the target program there is a corresponding one in the source. We \nchoose to view intermediate state accesses as silent steps, thus we can relate programs with different \nimple\u00admentation details. This also makes our simulation independent of language details. To support parallel \ncompositionality, our relation takes into account explicitly the expected interference between threads \nand their parallel environments. Inspired by the Rely-Guarantee (R-G) veri.cation method [17], we specify \nthe interference using rely/guarantee conditions. In Rely-Guarantee reasoning, the rely condition R of \na thread speci.es the permitted state transitions that its environment may have, and its guarantee G \nspeci.es the possi\u00adble transitions made by the thread itself. To ensure parallel threads can collaborate, \nwe need to check the interference constraint, i.e., the guarantee of each thread is permitted in the \nrely of every others. Then we can verify their parallel composition by separately veri\u00adfying each thread, \nshowing its behaviors under the rely condition indeed satisfy its guarantee. After parallel composition, \nthe threads should be executed under their common environment (i.e.,the in\u00adtersection of their relies) \nand guarantee all the possible transitions made by them (i.e., the union of their guarantees). Parametrized \nwith rely/guarantee conditions for the two levels, our relation (C, R, G) : (C, R, G) talks about not \nonly the target C and the source C, but also the interference R and G between C and its target-level \nenvironment, and R and G between C and its en\u00advironment at the source level. Informally, (C, R, G) : \n(C, R, G) says the executions of C under the environment R do not exhibit more observable behaviors than \nthe executions of C under the en\u00advironment R, and the state transitions of C and C satisfy G and G respectively. \nRGSim is now compositional, as long as the threads are composed with well-behaved environments only. \nThe paral\u00adlel compositionality lemma is in the following form. If we know (C1, R1, G1) : (C1, R1, G1) \nand (C2, R2, G2) : (C2, R2, G2), and also the interference constraints are satis.ed, i.e., G2 .R1, G1 \n.R2, G2 . R1 and G1 . R2, we could get (C1 IC2, R1 nR2, G1 .G2) : (C1 IC2, R1 n R2, G1 . G2) . The compositionality \nof RGSim gives us a proof theory for concur\u00adrent program transformations. Also different from fully abstract \nsemantics for threads, which assumes arbitrary behaviors of environments, RGSim allows us to instantiate \nthe interference R, G, R and G differently for different assumptions about environments, therefore it \ncan be used to verify the aforementioned four classes of transformations. For instance, if we want to \nprove that a transformation preserves the behaviors of data-race-free programs, we can specify the data-race-freedom \nin R and G. Then we are no longer concerned with the examples in Figure 1, both of which have data races. \n3. Basic Technical Settings In this section, we present the source and the target programming languages. \nThen we de.ne a basic re.nement , which naturally says the target has no more externally observable event \ntraces than the source. We use as an intuitive formulation of the correctness of transformations. 3.1 \nThe Languages Following standard simulation techniques, we model the seman\u00adtics of target and source \nprograms as labeled transition systems. Before showing the languages, we .rst de.ne events and labels \nin Figure 2(a). We leave the set of events unspeci.ed here. It can be instantiated by program veri.ers, \ndepending on their interest (e.g., input/output events). A label that will be associated with a state \ntransition is either an event or t, which means the corresponding transition does not generate any event \n(i.e., a silent step). The target language, which we also call the low-level language, is shown in Figure \n2(b). We abstract away the forms of states, ex\u00adpressions and primitive instructions in the language. \nAn arithmetic expression E is modeled as a function from states to integers lifted with an unde.ned value \n.. Boolean expressions are modeled sim\u00adilarly. An instruction is a partial function from states to sets \nof la\u00adbel and state pairs, describing the state transitions and the events it generates. We use P() to \ndenote the power set. Unsafe executions  (Events) e ::= ... (Labels) o ::= e | t (a) Events and Transition \nLabels (LState) s ::= ... (LExpr) E . LState . Int. (LBExp) B . LState .{true, false}. (LInstr) c . LState \n. P((Labels \u00d7 LState) .{abort}) (LStmt) C ::= skip | c | C1; C2 | if (B) C1 else C2 | while (B) C | C1 \n.C2 (LStep) -.L .P((LStmt/{skip}\u00d7 LState) \u00d7 Labels \u00d7((LStmt \u00d7 LState) .{abort})) (b) The Low-Level Language \n(HState) S ::= ... (HExpr) E . HState . Int. (HBExp) B . HState .{true, false}. (HInstr) c . HState . \nP((Labels \u00d7 HState) .{abort}) (HStmt) C ::= skip | c | C1;; C2 | if B then C1 else C2 | while B do C \n| C1 1C2 (HStep) -.L .P((HStmt/{skip}\u00d7 HState) \u00d7 Labels \u00d7((HStmt \u00d7 HState) .{abort})) (c) The High-Level \nLanguage Figure 2. Generic Languages at Target and Source Levels lead to abort. Note that the semantics \nof an instruction could be non-deterministic. Moreover, it might be unde.ned on some states, making it \npossible to model blocking operations such as acquiring a lock. Statements are either primitive instructions \nor compositions of them. skip is a special statement used as a .ag to show the end of executions. A single-step \nexecution of statements is modeled as a labeled transition -.L , which is a triple of an initial program \ncon.guration (a pair of statement and state), a label and a resulting con.guration. It is unde.ned when \nthe initial statement is skip.The step aborts if an unsafe instruction is executed. The high-level language \n(source language) is de.ned similarly in Figure 2(c), but it is important to note that its states and \nprimitive instructions may be different from those in the low-level language. The compound statements \nare almost the same as their low-level counterparts. C1;; C2 and C11C2 are sequential and parallel com\u00adpositions \nof C1 and C2 respectively. Note that we choose to use the same set of compound statements in the two \nlanguages for simplic\u00adity only. This is not required by our simulation relation, although the analogous \nprogram constructs of the two languages (e.g., paral\u00adlel compositions C1 IC2 and C11C2) make it convenient \nfor us to discuss the compositionality later. Figure 3 shows part of the de.nition of -.H , which gives \nthe high-level operational semantics of statements. We often omit the subscript H (or L)in -.H (or -.L \n) and the label on top of the arrow when it is t. The semantics is mostly standard. We only show the \nrules for primitive instructions and parallel com\u00adpositions here. Note that when a primitive instruction \nc is blocked at state S (i.e., S . dom(c)), we let the program con.guration reduce to itself. For example, \nthe instruction lock(l) would be blocked when l is not 0, making it be repeated until l becomes 0; whereas \nunlock(l) simply sets l to 0 at any time and would never be blocked. Primitive instructions in the high-level \nand low\u00adlevel languages are atomic in the interleaving semantics. Below we use -. * for zero or multiple-step \ntransitions with no events e generated, and -. * for multiple-step transitions with only one event e \ngenerated.  3.2 The Event Trace Re.nement Now we can formally de.ne the re.nement relation that relates \nthe set of externally observable event traces generated by the target and the source programs. A trace \nis a sequence of events e, and may end with a termination marker done or a fault marker abort. (EvtTrace) \nE ::= e | done | abort | e::E De.nition 1 (Event Trace Set). ETrSetn(C, s) represents a set of external \nevent traces produced by C in n steps from the state s: ETrSet0(C, s) {e} ;  ETrSetn+1(C, s) {E | (C, \ns) -. (C ' ,s ' ) .E.ETrSetn(C ' ,s ' )  e . (C, s) -. (C ' ,s ' ) .E ' .ETrSetn(C ' ,s ' ) .E =e:: \nE ' . (C, s) -. abort .E =abort . C =skip .E =done} .  We de.ne ETrSet(C, s) as n ETrSetn(C, s). We \noverload the notation and use ETrSet(C, S) for the high-level language. Then we de.ne an event trace \nre.nement as the subset relation between event trace sets, which is similar to Leroy s re\u00ad.nement property \n[19]. De.nition 2 (Event Trace Re.nement). We say (C, s) is an e\u00adtrace re.nement of (C, S), i.e., (C, \ns)(C, S), if and only if ETrSet(C, s) . ETrSet(C, S) . The re.nement is de.ned for program con.gurations \ninstead of for code only because the initial states may affect the behaviors of programs. In this case, \nthe transformation T should translate states as well as code. We overload the notation and use T(S) to \nrepresent the state transformation, and use C T C for .s, S.s = T(S) =. (C, s)(C, S) , then Correct(T) \nde.ned in formula (1.1) can be reformulated as Correct(T) .C, C.C = T(C)=. C T C . (3.1) 4. The RGSim \nRelation The e-trace re.nement is de.ned directly over the externally ob\u00adservable behaviors of programs. \nIt is intuitive, and also abstract in that it is independent of language details. However, as we explained \nbefore, it is not compositional w.r.t. parallel compositions. In this section we propose RGSim, which \ncan be viewed as a composi\u00adtional proof technique that allows us to derive the simple e-trace re.nement \nand then verify the corresponding transformation T. 4.1 The De.nition Our co-inductively de.ned RGSim \nrelation is in the form of (C, s, R, G) :a;. (C, S, R, G), which is a simulation between program con.gurations \n(C, s) and (C, S). It is parametrized with the rely and guarantee conditions at the low level and the \nhigh level, which are binary relations over states: R, G.P(LState \u00d7 LState) , R, G .P(HState \u00d7 HState) \n. The simulation also takes two additional parameters: the step in\u00advariant a and the postcondition ., \nwhich are both relations between the low-level and the high-level states. a, ., . .P(LState \u00d7 HState) \n.  (t, S ' ) . c S (e, S ' ) . c S abort . c S S . dom(c) (c, S) -. (skip, S ' ) (c, S) e-. (skip, S \n' ) (c, S) -. abort (c, S) -. (c, S) (C1, S) -. (C ' 1, S ' ) (C2, S) -. (C ' 2, S ' ) (skip1skip, S) \n-. (skip, S) (C1 1C2, S) -. (C ' 1 1C2, S ' ) (C1 1C2, S) -. (C1 1C ' 2, S ' ) (C1, S) e-. (C ' 1, S \n' ) (C2, S) e-. (C ' 2, S ' ) (C1, S) -. abort or (C2, S) -. abort (C1 1C2, S) e-. (C ' 1 1C2, S ' ) \n(C1 1C2, S) e-. (C1 1C ' 2, S ' ) (C1 1C2, S) -. abort Figure 3. Operational Semantics of the High-Level \nLanguage aa\u00df s S s. S R R R R RM * a a \u00df s ' S ' s ' . ' S ' (a) a-Related Transitions (b) The Side \nCondition of TRANS Figure 4. Related Transitions Before we formally de.ne RGSim in De.nition 4, we .rst \nintroduce the a-related transitions as follows. De.nition 3 (a-Related Transitions). (R, R)a {((s, s \n' ), (S, S ' )) | (s, s ' ) .R. (S, S ' ) . R . (s, S) . a . (s ' , S ' ) . a} . (R, R)a represents a \nset of the a-related transitions in R and R, putting together the corresponding transitions in R and \nR that can be related by a, as illustrated in Figure 4(a). (G, G)a is de.ned in the same way. De.nition \n4 (RGSim). Whenever (C, s, R, G) :a;. (C, S, R, G), then (s, S) .a and the following are true: 1. if \n(C, s) -. (C ' ,s ' ), then there exist C ' and S ' such that (C, S) -. * (C ' , S ' ), ((s, s ' ), (S, \nS ' )) .(G, G *)a and (C ' ,s ' , R, G) :a;. (C ' , S ' , R, G); e 2. if (C, s) -. (C ' ,s ' ), then \nthere exist C ' and S ' such that e (C, S) -. * (C ' , S ' ), ((s, s ' ), (S, S ' )) .(G, G *)a and (C \n' ,s ' , R, G) :a;. (C ' , S ' , R, G); 3. if C = skip, then there exists S ' such that (C, S) -. * (skip, \nS ' ), ((s, s), (S, S ' )) .(G, G *)a, (s, S ' ) . . and . . a;  4. if (C, s) -. abort,then (C, S) -. \n* abort; 5. if ((s, s ' ), (S, S ' )) .(R, R *)a,then (C, s ' , R, G) :a;. (C, S ' , R, G).  Then, \n(C, R, G) :a;.. (C, R, G) iff for all s and S,if (s, S) . .,then (C, s, R, G) :a;. (C, S, R, G). Here \nthe precondition . is used to relate the initial states s and S. Informally, (C, s, R, G) :a;. (C, S, \nR, G) says the low-level con.guration (C, s) is simulated by the high-level con.guration (C, S) with \nbehaviors G and G respectively, no matter how their environments R and R interfere with them. It requires \nthe follow\u00ading hold for every execution of C: Starting from a-related states, each step of C corresponds \nto zero or multiple steps of C, and the resulting states are a\u00adrelated too. If an external event is produced \nin the step of aa (C, s)(C, S) (C, s)(C, S) G e G e R R * * a (C ' ,s ' )(C ' , S ' )(C, s ' ) a(C, \nS ') (a) Program Steps (b) Environment Steps Figure 5. Simulation Diagrams of RGSim C, the same event \nshould be produced by C. We show the simulation diagram with events generated by the program steps in \nFigure 5(a), where solid lines denote hypotheses and dashed lines denote conclusions, following Leroy \ns notations [19]. The a relation re.ects the abstractions from the low-level ma\u00adchine model to the high-level \none, and is preserved by the re\u00adlated transitions at the two levels (so it is an invariant). For instance, \nwhen verifying a .ne-grained implementation of sets, the a relation may relate a concrete representation \nin memory (e.g., a linked-list) at the low level to the corresponding abstract mathematical set at the \nhigh level.  The corresponding transitions of C and C need to be in (G, G *)a. That is, for each step \nof C, its state transition should satisfy the guarantee G, and the corresponding transition made by the \nmultiple steps of C should be in the transitive closure of G. The guarantees are abstractions of the \nprograms behaviors. As we will show later in the PAR rule in Figure 7, they will serve as the rely conditions \nof the sibling threads at the time of parallel compositions. Note that we do not need each step of C \ntobe in G, although we could do so. This is because we only care about the coarse-grained behaviors (with \nmumbling) of the source that are used to simulate the target. We will explain more by the example (4.1) \nin Section 4.2.  If C terminates, then C terminates as well, and the .nal states should be related by \nthe postcondition .. We require . . a, i.e., the .nal state relation is not weaker than the step invariant. \n C is not safe only if C is not safe either. This means the trans\u00adformation should not make a safe high-level \nprogram unsafe at the low level.  Whatever the low-level environment R and the high-level one R do, \nas long as the state transitions are a-related, they should not affect the simulation between C and C, \nas shown in Fig\u00adure5(b). Hereastepin R may correspond to zero or multiple steps of R. Note that different \nfrom the program steps, for the environment steps we do not require each step of R to corre\u00adspond to \nzero or multiple steps of R. On the other hand, only requiring that R be simulated by R is not suf.cient \nfor parallel compositionality, which we will explain later in Section 4.2.   InitRelT(.) .s, S.s = \nT(S) =. (s, S) . . B . B {(s, S) | Bs = B S} B. B {(s, S) | Bs . B S} Intuit(a) .s, S,s ' , S ' . (s, \nS).a . s .s ' . S.S ' =. (s ' , S ' ) . a . # a (. n a) . (. 1 a) \u00df . a {(s, S) |... (s, .) . a . (., \nS) . \u00df} a 1 \u00df {(s1 1 s2, S1 1 S2) | (s1, S1) . a . (s2, S2) . \u00df} Id {(s, s) | s . LState} True {(s, s \n' ) | s, s ' . LState} RM isMidOf (a, \u00df; R, R) .s, s ' , S, S ' . ((s, s ' ), (S, S ' )) .(R, R)\u00df.a =.... \n(s, .) . a . (., S) . \u00df =... ' . ((s,s ' ), (.,. ' )).(R, RM)a . ((.,. ' ), (S,S ' )).(RM, R)\u00df Figure \n6. Auxiliary De.nitions for RGSim Then based on the simulation, we hide the states by the precon\u00addition \n. and de.ne the RGSim relation between programs only. By the de.nition we know . . a if (C, R, G) :a;.. \n(C, R, G), i.e., the precondition needs to be no weaker than the step invariant. RGSim is sound w.r.t. \nthe e-trace re.nement (De.nition 2). That is, (C, s, R, G) :a;. (C, S, R, G) ensures that (C, s) does \nnot have more observable behaviors than (C, S). Theorem 5 (Soundness). If there exist R, G, R, G, a and \n. such that (C, s, R, G) :a;. (C, S, R, G),then (C, s)(C, S). The soundness theorem can be proved by \n.rst strengthening the relies to the identity transitions and weakening the guarantees to the universal \nrelations. Then we prove that the resulting simulation under identity environments implies the e-trace \nre.nement. For program transformations, since the initial state for the target program is transformed \nfrom the initial state for the source, we use InitRelT(.) (de.ned in Figure 6) to say the transformation \nT over states ensures the binary precondition .. Corollary 6. If there exist R, G, R, G, a, . and . such \nthat InitRelT(.) and (C, R, G) :a;.. (C, R, G),then C T C.  4.2 Compositionality Rules RGSim is compositional \nw.r.t. various program constructs, includ\u00ading parallel compositions. We present the compositionality \nrules in Figure 7, which gives us a relational proof method for concurrent program transformations. As \nin the R-G logic [17], we require that the pre-and post\u00adconditions be stable under the interference from \nthe environments. Here we introduce the concept of stability of a relation . w.r.t. aset of transition \npairs . .P((LState \u00d7 LState) \u00d7 (HState \u00d7 HState)). De.nition 7 (Stability). Sta(., .) holds iff for all \ns, s ' , S and S ' , if (s, S) . . and ((s, s ' ), (S, S ' )) . .,then (s ' , S ' ) . .. Usually we \nneed Sta(., (R, R * )a), which says whenever . holds initially and R and R * perform related actions, \nthe resulting states still satisfy .. By unfolding (R, R * )a,wecouldseethat a itself is stable w.r.t. \nany a-related transitions, i.e., Sta(a, (R, R * )a). Another simple example is given below, where both \nenvironments could increment x and the unary stable assertion { x = 0} is lifted to the relation .: . \n{(s, S) | s(x)=S(x) = 0} a {(s, S) | s(x)=S(x)} R{(s, s ' ) | s ' = s{x . s(x)+1}} R {(S, S ' ) | S ' \n=S{x . S(x)+1}} We can prove Sta(., (R, R * )a). Stability of the pre-and post\u00adconditions under the \nenvironments interference is assumed as an implicit side condition at every proof rule in Figure 7, e.g.,weas\u00adsume \nSta(., (R, R * )a) in the SKIP rule. We also require implicitly that the relies and guarantees are closed \nover identity transitions, since stuttering steps will not affect observable event traces. In Figure \n7, the rules SKIP, SEQ, IF and WHILE reveal a high degree of similarity to the corresponding inference \nrules in Hoare logic. In the SEQ rule, . serves as the postcondition of C1 and C1 and the precondition \nof C2 and C2 at the same time. The IF rule requires the boolean conditions of both sides to be evaluated \nto the same value under the precondition .. We give the de.nitions of the sets B . B and B . B in Figure \n6. The rule also requires the precondition . to imply the step invariant a.Inthe WHILE rule, the . relation \nis viewed as a loop invariant preserved at the loop entry point, and needs to ensure B . B. Parallel \ncompositionality. The PAR rule shows parallel compo\u00adsitionality of RGSim. The interference constraints \nsay that two threads can be composed in parallel if one thread s guarantee im\u00adplies the rely of the other. \nAfter parallel composition, they are ex\u00adpected to run in the common environment and their guaranteed \nbe\u00adhaviors contain each single thread s behaviors. Note that, although RGSim does not require every step \nof the high-level program to be in its guarantee (see the .rst two condi\u00adtions in De.nition 4), this \nrelaxation does not affect the parallel compositionality. This is because the target could have less \nbehav\u00adiors than the source. To let C11C2 simulate C1 IC2,weonlyneed a subset of the interleavings of \nC1 and C2 to simulate those of C1 and C2. Thus the high-level relies and guarantees need to ensure the \nexistence of those interleavings only. Below we give a simple example to explain this subtle issue. We \ncan prove (x:=x+2, R, G) :a;.. (x:=x+1;x:=x+1, R, G) , (4.1) where the relies and the guarantees say \nx can be increased by 2 and a, . and . relate x of the two sides: R = G{(s, s ' ) | s ' = s . s ' = s{x \n. s(x)+2}} ; R = G {(S, S ' ) | S ' =S . S ' =S{x . S(x)+2}} ; a = . = . {(s, S) | s(x)=S(x)} . Note \nthat the high-level program is actually .ner-grained than its guarantee, but to prove (4.1) we only need \nthe execution in which it goes two steps to the end without interference from its environment. Also we \ncan prove (print(x), R, G) :a;. (print(x), R, G). . Then by the PAR rule, we get (x:=x+2 .print(x), R, \nG) a;.. ((x:=x+1;x:=x+1)1print(x), R, G) , which does not violate the natural meaning of re.nements. \nThat is, all the possible external events produced by the low-level side can also be produced by the \nhigh-level side, although the latter could have more external behaviors due to its .ner granularity. \nAnother subtlety in the RGSim de.nition is with the .fth con\u00addition over the environments, which is crucial \nfor parallel composi\u00adtionality. One may think a more natural alternative to this condition is to require \nthat R be simulated by R: If (s, s ' ) .R, then there exists S ' such that ' (4.2) (S, S ' ) . R * and \n(C, s ' , R, G) :a;.. (C, S ' , R, G) . We refer to this modi.ed simulation de.nition as : ' . Unfortu\u00adnately, \n: ' does not have parallel compositionality. As a counter\u00adexample, if the invariant a says the left-side \nx is not greater than the right-side x and the precondition . requires x of the two sides are equal, \ni.e., a {(s, S) | s(x) = S(x)} . {(s, S) | s(x)=S(x)} ,  (C1, R, G)(C1, R, G)(C2, R, G)(C2, R, G) . \n. a a;.. a;.. (SKIP)(SEQ) (skip, R, Id)(skip, R, Id)(C1; C2, R, G)(C1;; C2, R, G) a;.. a;.. (C1, R, G)(C1, \nR, G)(C2, R, G)(C2, R, G) a;.1 .a;.2 . . . (B . B) .1 =(. n (B. B)) .2 =(. n (\u00acB..\u00acB)) . . a (IF) (if \n(B) C1 else C2, R, G)(if B then C1 else C2, R, G) a;.. (C, R, G)(C, R, G) . . (B . B) .1 =(. n (B. B)) \n.2 =(. n (\u00acB..\u00acB)) a;.1 . (WHILE) (while (B) C, R, G)(while B do C, R, G) a;..2 (C1, R1, G1)(C1, R1, \nG1)(C2, R2, G2)(C2, R2, G2) a;..1 a;..2 G1 .R2 G2 .R1 G1 . R2 G2 . R1 (PAR) (C1 .C2, R1 nR2, G1 .G2) \na;. (C1 1C2, R1 n R2, G1 . G2) (.1n.2) (C, R, G)(C, R, G)(C, R, G)(C, R, G) a;.. a;.. (. . .) . a ' . \na Sta(a ' , (G, G *)a) a . a ' Sta(a, (R, R *)a/ ) (STREN-a)(WEAKEN-a) (C, R, G)(C, R, G)(C, R, G)(C, \nR, G) a/;.. a/;.. ' (C, R, G)(C, R, G) . . .. . . ' . a R ' .R R ' . R G.G ' G . G ' a;.. (CONSEQ) \n(C, R ' , G ' ) ./ (C, R ' , G ' ) a;./ (C, R, G)(C, R, G)(C, R, G) a;.. (M, RM, GM) a;.. . . \u00df Intuit({a, \n., ., \u00df, ., R, R, R1, R1})(M, RM, GM) \u00df;d. (C, R, G) . # {.,.,a} Sta(., {(G, G *)a, (R1, R *)\u00df})RM isMidOf \n(a, \u00df; R, R *) 1 (FRAME)(TRANS) (C, R1R1, G1G1)(C, R 1 R1, G 1 G1)(C, R, G)(C, R, G) a \u00df;(. .)(. .) \n\u00df.a;(d..)(...) Figure 7. Compositionality Rules for RGSim we could prove the following: ' (x:=x+1, Id, \nTrue)(x:=x+2, Id, True); a;.a ' (print(x), True, Id)(print(x), True, Id) . a;.a Hereweuse Id and True \n(de.ned in Figure 6) for the sets of identity transitions and arbitrary transitions respectively, and \noverload the notations at the low level to the high level. However, the following re.nement does not \nhold after parallel composition: ' (x:=x+1 .print(x), Id, True) a;.a (x:=x+21print(x), Id, True) . This \nis because the rely R (or R) is an abstraction of all the per\u00admitted behaviors in the environment of \na thread. But a concrete sibling thread that runs in parallel may produce less transitions than R (or \nR). To obtain parallel compositionality, we need to en\u00adsure that the simulation holds for all concrete \nsibling threads. With our de.nition :, the re.nement (print(x), True, Id) :a;.a (print(x), True, Id) \nis not provable because, after the environ\u00adments a-related transitions, the target may print a value \nsmaller than the one printed by the source. Other rules. We also develop some other useful rules about \nRGSim. For example, the STREN-a rule allows us to replace the invariant a by a stronger invariant a '. \nWe need to check that a ' is indeed an invariant preserved by the related program steps, i.e., Sta(a \n' , (G, G * )a) holds. Symmetrically, the WEAKEN-a rule re\u00adquires a to be preserved by environment steps \nrelated by the weaker invariant a '. As usual, the pre/post conditions, the relies and the guarantees \ncan be strengthened or weakened by the CONSEQ rule. The FRAME rule allows us to use local speci.cations. \nWhen ver\u00adifying the simulation between C and C, we need to only talk about the locally-used resource \nin a, . and ., and the local relies and guarantees R, G, R and G. Then the proof can be reused in con\u00adtexts \nwhere some extra resource . is used, and the accesses of it respect the invariant \u00df and R1, G1, R1 and \nG1.Wegivetheauxil\u00adiary de.nitions in Figure 6. The disjoint union l between states is lifted to state \npairs. An intuitionistic state relation is monotone w.r.t. the extension of states. The disjointness \n. # a says that any state pair satisfying both . and a can be split into two disjoint state pairs satisfying \n. and a respectively. For example, let . {(s, S) |s(y)=S(y)} and a {(s, S) | s(x)=S(x)}, then both . \nand a are intuitionistic and . # a holds. We also require . to be stable under interference from the \nprograms (i.e., the programs do not change the extra resource) and the extra environments. We use . # \n{., ., a} as a shorthand for (. # .) . (. # .) . (. # a). Similar representations are used in this rule. \nFinally, the transitivity rule TRANS allows us to verify a trans\u00adformation by using an intermediate level \nas a bridge. The interme\u00addiate environment RM should be chosen with caution so that the (\u00df . a)-related \ntransitions can be decomposed into \u00df-related and a-related transitions, as illustrated in Figure 4(b). \nHere . de.nes the composition of two relations and isMidOf de.nes the side con\u00addition over the environments, \nas shown in Figure 6. We use . for a middle-level state. Soundness of all the rules in Figure 7 is proved \nin the techni\u00adcal report [20], showing that for each rule the premises imply the conclusion. The proofs \nare also mechanized in the Coq proof assis\u00adtant [10]. Instantiations of relies and guarantees. We can \nderive the se\u00adquential re.nement and the fully-abstract-semantics-based re.ne\u00adment by instantiating the \nrely conditions in RGSim. For example, the re.nement (4.3) over closed programs assumes identity envi\u00adronments, \nmaking the interference constraints in the PAR rule un\u00adsatis.able. This con.rms the observation in Section \n2.1 that the sequential re.nement loses parallel compositionality. (C, Id, True) :a;.. (C, Id, True) \n(4.3)  The re.nement (4.4) assumes arbitrary environments, which makes theinterferenceconstraintsinthe \nPAR ruletriviallytrue.Butthisas\u00adsumption is too strong: usually (4.4) cannot be satis.ed in practice. \n(C, True, True) :a;.. (C, True, True) (4.4)  4.3 A Simple Example Below we give a simple example to \nillustrate the use of RGSim and its parallel compositionality in verifying concurrent program transformations. \nThe high-level program C1 1C2 is transformed to C1 I C2, using a lock l to synchronize the accesses of \nthe shared variable x. We aim to prove C1 IC2 T C11C2. That is, although x:=x+2 is implemented by two \nsteps of incrementing x in C2,the parallel observer C1 will not print unexpected values. Here we view \noutput events as externally observable behaviors. print(x); 1 x:=x+2; . lock(l); lock(l); print(x); . \nx := x+1; x := x+1; unlock(l); (unlock(l); X:=x; ) To facilitate the proof, we introduce an auxiliary \nshared vari\u00adable X at the low level to record the value of x at the time when releasing the lock. It \nspeci.es the value of x outside every critical section, thus should match the value of the high-level \nx after every corresponding action. Here (C) means C is executed atomically. By the soundness and compositionality \nof RGSim, we only need to prove simulations over individual threads, providing appropriate relies and \nguarantees. We .rst de.ne the invariant a, which only cares about the value of x when the lock is free. \na {(s, S) | s(X)=S(x) . (s(l)=0 =. s(x)= s(X))} . We let the pre-and post-conditions be a as well. The \nhigh-level threads can be executed in arbitrary environ\u00adments with arbitrary guarantees: R = G True. \nThe transfor\u00admation uses the lock to protect every access of x, thus the low-level relies and guarantees \nare not arbitrary: R{(s, s ' ) | s(l)=cid =. s(x)=s ' (x) . s(X)=s ' (X) . s(l)=s ' (l)} ; G{(s, s ' \n) | s ' =s . s(l)=0 . s ' =s{l cid} . s(l)=cid . s ' =s{x } . s(l)=cid . s ' =s{l 0, X }} . Every low-level \nthread guarantees that it updates x only when the lock is acquired. Its environment cannot update x or \nl if the current thread holds the lock. Here cid is the identi.er of the current thread. When acquired, \nthe lock holds the id of the owner thread. Following the de.nition, we can prove (C1, R, G) :a;aa (C1, \nR, G) and (C2, R, G) :a;a (C2, R, G). By applying the a PAR rule and from the soundness of RGSim (Corollary \n6), we know C1 IC2 T C1 1 C2 holds for any T that respects a. Perhaps interestingly, if we omit the lock \nand unlock operations in C1,then C1 IC2 would have more externally observable behav\u00adiors than C1 1 C2. \nThis does not indicate the unsoundness of our PAR rule (which is sound!). The reason is that x might \nhave dif\u00adferent values on the two levels after the environments a-related transitions, so that we cannot \nhave (print(x), R, G) :a;aa (print(x), R, G) with the current de.nitions of a, R and G,even though the \ncode of the two sides are syntactically identical. More discussions. RGSim ensures that the target program \npre\u00adserves safety properties (including the partial correctness) of the source, but allows a terminating \nsource program to be transformed to a target having in.nite silent steps. In the above example, this \nal\u00adlows the low-level programs to be blocked forever (e.g., at the time when the lock is held but never \nreleased by some other thread). Proving the preservation of the termination behavior would require liveness \nproofs in a concurrent setting (e.g., proving the absence of deadlock), which we leave as future work. \nIn the next three sections, we show more serious examples to demonstrate the applicability of RGSim. \n5. Relational Reasoning about Optimizations As a general correctness notion of concurrent program transforma\u00adtions, \nRGSim establishes a relational approach to justify compiler optimizations on concurrent programs. Below \nwe adapt Benton s work [3] on sequential optimizations to the concurrent setting. 5.1 Optimization Rules \nUsually optimizations depend on particular contexts, e.g.,the as\u00adsignment x := E can be eliminated only \nin the context that the value of x is never used after the assignment. In a shared-state con\u00adcurrent \nsetting, we should also consider the parallel context for an optimization. RGSim enables us to specify \nvarious sophisticated requirements for the parallel contexts by rely/guarantee conditions. Based on RGSim, \nwe provide a set of inference rules to character\u00adize and justify common optimizations (e.g., dead code \nelimination) with information of both the sequential and the parallel contexts. Due to the space limit, \nwe only present some interesting rules here and leave other rules in the technical report [20]. Note \nin this sec\u00adtion the target and the source programs are in the same language. Sequential skip Law (C1, \nR1, G1)(C2, R2, G2) a;.. (skip; C1, R1, G1)(C2, R2, G2) a;.. (C1, R1, G1) a;.. (C2, R2, G2) (C1, R1, \nG1) a;. (skip; C2, R2, G2) . Plus the variants with skip after the code C1 or C2.Thatis, skips could \nbe arbitrarily introduced and eliminated. Common Branch .s1,s2. (s1,s2) . . =. Bs2 =. (C, R, G)(C1, R \n' , G ' ) .1 =(. n (true. B)) a;.1 . (C, R, G)(C2, R ' , G ' ) .2 =(. n (true..\u00acB)) a;.2 . (C, R, G)(if \n(B) C1 else C2, R ' , G ' ) a;.. This rule says that, when the if-condition can be evaluated and both \nbranches can be optimized to the same code C, we can transform the whole if-statement to C without introducing \nnew behaviors. Dead While . =(. n (true..\u00acB)) . . a Sta(., (R1, R*)a) 2 (skip, R1, Id)(while (B){C}, \nR2, Id) a;.. We can eliminate the loop, if the loop condition is false (no matter how the environments \nupdate the states) at the loop entry point. Dead Code Elimination (skip, Id, Id)(C, Id, G) Sta({.,.}, \n(R1, R*)a) a;.. 2 (skip, R1, Id)(C, R2, G) a;.. Intuitively (skip, Id, Id) :a;. (C, Id, G) says that \nthe code C . can be eliminated in a sequential context where the initial and the .nal states satisfy \n. and . respectively. If both . and . are stable w.r.t. the interference from the environments R1 and \nR2,thenthe code C can be eliminated in such a parallel context as well.  Redundancy Introduction (c, \nId, G)(skip, Id, Id) Sta({.,.}, (R1, R* 2)a) a;.. (c, R1, G)(skip, R2, Id) a;.. As we lifted sequential \ndead code elimination, we can also lift se\u00adquential redundant code introduction to the concurrent setting, \nso long as the pre-and post-conditions are stable w.r.t. the environ\u00adments. Note that here c is a single \ninstruction, because we should consider the interference from the environments at every interme\u00addiate \nstate when introducing a sequence of redundant instructions.  5.2 An Example of Invariant Hoisting With \nthese rules, we can prove the correctness of many traditional compiler optimizations performed on concurrent \nprograms in ap\u00adpropriate contexts. Here we only give a small example of hoisting loop invariants. More \nexamples (e.g., strength reduction and induc\u00adtion variable elimination) can be found in the technical \nreport [20]. Target Code (C1) Source Code (C) local t; local t; t:=x+1; while(i<n){ . while(i<n){ t:=x+1; \ni:=i+t; i:=i+t; }} When we do not care about the .nal value of t, it s not dif.\u00adcult to prove that the \noptimized code C1 preserves the sequential behaviors of the source C [3]. But in a concurrent setting, \nsafely hoisting the invariant code t:=x+1 also requires that the environ\u00adment should not update x nor \nt. R{(s, s ' ) | s(x)= s ' (x) . s(t)= s ' (t)} . The guarantee of the program can be speci.ed as arbitrary \ntransi\u00adtions. Since we only care about the values of i, n and x, the invari\u00adant relation a can be de.ned \nas: a {(s1,s) | s1(i)= s(i) . s1(n)= s(n) . s1(x)= s(x)} . We do not need special pre-and post-conditions, \nthus the correct\u00adness of the optimization is formalized as follows: (C1, R, True) :a;aa (C, R, True) \n. (5.1) We could prove (5.1) directly by the RGSim de.nition and the operational semantics of the code. \nBut below we give a more con\u00advenient proof using the optimization rules and the compositional\u00adity rules \ninstead. We .rst prove the following by the Dead-Code-Elimination and Redundancy-Introduction rules: \n(t:=x+1, R, True)(skip, R, True); a;a. (skip, R, True)(t:=x+1, R, True) , a;.. where . and . specify \nthe states at the speci.c program points: .a n{(s1,s) | s1(t)= s1(x)+1} ; .. n{(s1,s) | s(t)= s(x)+1} \n. After adding skipsto C1 and C to make them the same shape , we can prove the simulation by the compositionality \nrules SEQ and WHILE. Finally, we remove all the skips and conclude (5.1), i.e., the correctness of the \noptimization in appropriate contexts. Since the relies only prohibit updates of x and t, we can execute \nC1 and C concurrently with other threads which update i and n or read x, still ensuring semantics preservation. \n6. Proving Atomicity of Concurrent Objects A concurrent object provides a set of methods, which can be \ncalled in parallel by clients as the only way to access the object. RGSim gives us a re.nement-based \nproof method to verify the atomicity ADD(e): RMV(e): 0 atom { 0 atom { S:=S . {e}; S:=S - {e}; }} (a) \nAn Abstract Set add(e): rmv(e): local x,y,z,v; local x,y,z,u; 0 <x := Head;> 0 <x := Head;> 1 lock(x); \n1 lock(x); 2 <y := x.next;> 2 <z := x.next;> 3 <v := y.data;> 3 <u := z.data;> 4 while (v < e) { 4 while \n(u < e) { 5 lock(y); 5 lock(z); 6 unlock(x); 6 unlock(x); 7 x:=y; 7 x:=z; 8 <y := x.next;> 8 <z := x.next;> \n9 <v := y.data;> 9 <u := z.data;> } } 10 if(v=e){ 10 if(u!=e){ 11 lock(y); 11 y := new(); 12 <z := y.next;> \n12 y.lock := 0; 13 <x.next := z;> 13 y.data := e; 14 unlock(x); 14 y.next := z; 15 free(y); 15 <x.next \n:= y;> } else { } 16 unlock(x); 16 unlock(x); } (b) The Lock-Coupling List-Based Set Figure 8. The Set \nObject of implementations of the object: we can de.ne abstract atomic operations in a high-level language \nas speci.cations, and prove the concrete .ne-grained implementations re.ne the correspond\u00ading atomic \noperations when executed in appropriate environments. For instance, in Figure 8(a) we de.ne two atomic \nset operations, ADD (e) and RMV(e). Figure 8(b) gives a concrete implementation of the set object using \na lock-coupling list. Partial correctness and atomicity of the algorithm has been veri.ed before [28, \n29]. Here we show that its atomicity can also be veri.ed using our RGSim by proving the low-level methods \nre.ne the corresponding abstract operations. We will discuss the key difference between the previous \nproofs and ours in Section 8. We .rst take the generic languages in Figure 2, and instantiate the high-level \nprogram states below. (HMem) Ms,Ml . (Loc . PVar) HVal (HThrds). . ThrdID . HMem (HState)S . HThrds \u00d7 \nHMem The state consists of shared memory Ms (where the object resides) and a thread pool ., which is \na mapping from thread identi.ers (t . ThrdID) to their memory Ml. The low-level state s is de.ned similarly. \nWe use ms, ml and p to represent the low-level shared memory, thread-local memory and the thread pool \nrespectively. To allow ownership transfer between the shared memory and thread-local memory, we use atom{C}A \n(or (C)A at the low level) to convert the shared memory to local and then execute C (or C) atomically. \nFollowing RGSep [29], an abstract transition A . P(HMem\u00d7HMem) (or A.P(LMem\u00d7LMem))isusedtospecify the \neffects of the atomic operation over the shared memory, which allows us to split the resulting state \nback into shared and local when we exit the atomic block. The atomic blocks are instantiations of the \ngeneric primitive operations c (or c) in Figure 2. Their semantics is shown in the technical report [20]. \nWe omit the annotations A and  ms |= list(x, A) shared map(ms,Ms) local map(ml,Ml) a Glock(t) Gunlock(t) \nGadd(t) Grmv(t) Glocal(t) G(t) Gadd(t) Grmv(t) Glocal(t) G(t) '' ' (ms = f . x = null . A = E) . (.m..v..y..A \n' .ms = m 1{x ( ,v,y)}. A = v ::A ' . m |= list(y, A ' )) ss s '' ' .m..A..x. ms = m 1{Head x}. (m |= \nlist(x, MIN VAL ::A::MAX VAL)) . sorted(A) . (elems(A)=Ms(S)) ss s '' ml(e)= Ml(e) ..m .ml = m 1{x , \ny , z , u , v } ll {((p, ms), (.,Ms)) | shared map(ms,Ms) ..t . dom(.). local map(p(t), .(t))} ' {((p, \nms), (p, m ' )) |.x,v,y. ms(x)=(0,v,y) . m = ms{x (t,v,y)}} ss ' {((p, ms), (p, m ' )) |.x,v,y. ms(x)=(t,v,y) \n. m = ms{x (0,v,y)}} ss '' {((p 1{t ml},ms), (p 1{t m },m )) ls |.x,y,z,u,v,w. ms(x)=(t,u,z) . ms(z)=( \n,w, ) '' . m = ms{x (t,u,y)}1{y (0,v,z)}. (m 1{y (0,v,z)} = ml) . u<v<w} sl '' {((p 1{t ml},ms), (p \n1{t m },m )) ls |.x,y,z,u,v. ms(x)=(t,u,y) . ms(y)=(t,v,z) '' . m 1{y (t,v,z)} = ms{x (t,u,z)}. m = \nml 1{y (t,v,z)}. v< MAX VAL} sl '' {((p 1{t ml},ms), (p 1{t ml},ms)) | p . (ThrdID . LMem) . m l,ml,ms \n. LMem}Glock(t) .Gunlock(t) .Gadd(t) .Grmv(t) .Glocal(t) R(t) t/= t G(t ' ) '' ' {((. 1{t Ml},Ms), (. \n1{t M },M )) |.e. M = Ms{S Ms(S).{e}}} lss '' ' {((. 1{t Ml},Ms), (. 1{t M },M )) |.e. M = Ms{S Ms(S)-{e}}} \nlss '' {((. 1{t Ml},Ms), (. 1{t Ml },Ms)) | . . (ThrdID . HMem) . Ml,Ml ,Ms . HMem} Gadd(t) . Grmv(t) \n. Glocal(t) R(t) t/G(t ' ) =t Figure 9. Useful De.nitions for the Lock-Coupling List A in Figure 8, \nwhich are the same as the corresponding guarantees in Figure 9, as we will explain below. In Figure 8, \nthe abstract set is implemented by an ordered singly-linked list pointed to by a shared variable Head, \nwith two sentinel nodes at the two ends of the list containing the values MIN VAL and MAX VAL respectively. \nEach list node is associated with a lock. Traversing the list uses hand-over-hand locking: the lock on \none node is not released until its successor is locked. add(e) inserts a new node with value e in the \nappropriate position while holding the lock of its predecessor. rmv(e) redirects the predecessor s pointer \nwhile both the node to be removed and its predecessor are locked. We de.ne the a relation, the guarantees \nand the relies in Fig\u00adure 9. The predicate ms |= list(x, A) represents a singly-linked list in the shared \nmemory ms at the location x, whose values form the sequence A. Then the mapping shared map between the \nlow\u00adlevel and the high-level shared memory is de.ned by only concern\u00ading about the value sequence on \nthe list: the concrete list should be sorted and its elements constitute the abstract set. For a thread \nt s local memory of the two levels, we require that the values of e are the same and enough local space \nis provided for add(e) and rmv(e), as de.ned in the mapping local map.Then a relates the shared memory \nby shared map and the local memory of each thread t by local map. The atomic actions of the algorithm \nare speci.ed by Glock, Gunlock, Gadd, Grmv and Glocal respectively, which are all parame\u00adterized with \na thread identi.er t. For example, Grmv(t) says that when holding the locks of the node y and its predecessor \nx, we can transfer the node y from the shared memory to the thread s local memory. This corresponds to \nthe action performed by the code of line 13 in rmv (e). Every thread t is executed in the environment \nthat any other thread t ' can only perform those .ve actions, as de\u00ad.ned in R(t). Similarly, the high-level \nG(t) and R(t) are de.ned according to the abstract ADD(e) and RMV(e). The relies and guar\u00adantees are \nalmost the same as those in the proofs in RGSep [28]. We can prove that for any thread t, the following \nhold: (t.add(e), R(t), G(t)) :a;aa (t.ADD(e), R(t), G(t)) ; (t.rmv(e), R(t), G(t)) :a;aa (t.RMV(e), R(t), \nG(t)) . We give the detailed proofs in the technical report [20], which are done operationally based \non the de.nition of RGSim. By the compositionality and the soundness of RGSim, we know that the .ne-grained \noperations (under the parallel environment R) are simulated by the corresponding atomic operations (under \nthe high-level environment R), while R and R say all accesses to the set must be done through the add \nand remove operations. This gives us the atomicity of the concurrent implementation of the set object. \nMore examples. In the technical report [20], we also show the use of RGSim to prove the atomicity of \nother .ne-grained algorithms, including the non-blocking concurrent counter [27], Treiber s stack algorithm \n[26], and a concurrent GCD algorithm (calculating great\u00adest common divisors). 7. Verifying Concurrent \nGarbage Collectors In this section, we explain in detail how to reduce the problem of verifying concurrent \ngarbage collectors to transformation veri.ca\u00adtion, and use RGSim to develop a general GC veri.cation \nframe\u00adwork. We apply the framework to prove the correctness of the Boehm et al. concurrent GC algorithm \n[7]. 7.1 Correctness of Concurrent GCs A concurrent GC is executed by a dedicate thread and performs \nthe collection work in parallel with user threads (mutators), which access the shared heap via read, \nwrite and allocation operations. To ensure that the GC and the mutators share a coherent view of the \nheap, the heap operations from mutators may be instrumented with extra operations, which provide an interaction \nmechanism to allow arbitrary mutators to cooperate with the GC. These instrumented heap operations are \ncalled barriers (e.g., read barriers, write barriers and allocation barriers). The GC thread and the \nbarriers constitute a concurrent garbage collecting system, which provides a higher-level user-friendly \npro\u00adgramming model for garbage-collected languages (e.g.,Java).In this high-level model, programmers \nfeel they access the heap using regular memory operations, and are freed from manually disposing objects \nthat are no longer in use. They do not need to consider the implementation details of the GC and the \nexistence of barriers.  We could verify the GC system by using a Hoare-style logic to prove that the \nGC thread and the barriers satisfy their speci.cations. However, we say this is an indirect approach \nbecause it is unclear if the speci.ed correct behaviors would indeed make the mutators happy and generate \nthe abstract view for high-level programmers. Usually this part is examined by experts and then trusted. \nHere we propose a more direct approach. We view a concurrent garbage collecting system as a transformation \nT from a high-level garbage-collected language to a low-level language. A standard atomic memory operation \nat the source level is transformed into the corresponding barrier code at the target level. In the source \nlevel, we assume there is an abstract GC thread that magically turns unreachable objects into reusable \nmemory. The abstract collector AbsGC is transformed into the concrete GC code Cgc running concurrently \nwith the target mutators. That is, T(tgc.AbsGC1t1.C1 1...1tn.Cn) tgc.Cgc It1.T(C1)I... Itn.T(Cn) , where \nT(C) simply translates some memory access instructions in C into the corresponding barriers, and leaves \nthe rest unchanged. Then we reduce the correctness of the concurrent garbage col\u00adlecting system to Correct(T), \nsaying that any mutator program will not have unexpected behaviors when executed using this system. \n 7.2 A General Framework The compositionality of RGSim allows us to develop a general framework to prove \nCorrect(T), which cannot be done by mono\u00adlithic proof methods. By the parallel compositionality of RGSim \n(the PAR rule in Figure 7), we can decompose the re.nement proofs into proofs for the GC thread and each \nmutator thread. Verifying the GC. The semantics of the abstract GC thread can be de.ned by a binary state \npredicate AbsGCStep: (S, S ' ) . AbsGCStep (tgc.AbsGC, S) -. (tgc.AbsGC, S ' ) That is, the abstract \nGC thread always makes AbsGCStep to change the high-level state. We can choose different AbsGCStep for \ndiffer\u00adent GCs, but usually AbsGCStep guarantees not modifying reach\u00adable objects in the heap. Thus for \nthe GC thread, we need to show that Cgc is simulated by AbsGC when executed in their environments. This \ncan be re\u00adduced to unary Rely-Guarantee reasoning about Cgc by proving Rgc; Ggc .{pgc}Cgc{qgc} in a standard \nRely-Guarantee logic with proper Rgc, Ggc, pgc and qgc, as long as Ggc is a concrete repre\u00adsentation \nof AbsGCStep. The judgment says given an initial state satisfying the precondition pgc, if the environment \ns behaviors sat\u00adisfy Rgc, then each step of Cgc satis.es Ggc, and the postcondition qgc holds at the \nend if Cgc terminates. In general, the collector never terminates, thus we can let qgc be false. Ggc \nand pgc should be pro\u00advided by the veri.er, where pgc needs to be general enough that can be satis.ed \nby any possible low-level initial state. Rgc encodes the possible behaviors of mutators, which can be \nderived, as we will show below. Verifying mutators. For the mutator thread, since T is syntax\u00addirected \non C, we can reduce the re.nement problem for arbitrary mutators to the re.nement on each primitive instruction \nonly, by the compositionality of RGSim. The proof needs proper rely/guarantee conditions. Let G(t.c) \nand G(t.T(c)) denote the guarantees of the source instruction c and the target code T(c) respectively. \nThen we can de.ne the general guarantees for a mutator thread t: G(t) c G(t.c); (7.1) G(t) c G(t.T(c)) \n. Its relies should include all the possible guarantees made by other threads, and the GC s abstract \nand concrete behaviors respectively: R(t) AbsGCStep . G(t ); t/=t ' (7.2) R(t) Ggc .G(t ' ) . t/=t The \nRgc used to verify the GC code can now be de.ned below: Rgc t G(t) . (7.3) The re.nement proof also \nneeds de.nitions of binary a, . and . relations. The invariant a relates the low-level and the high-level \nstates and needs to be preserved by each low-level step. In general, a high-level state S can be mapped \nto a low-level state s by giving a concrete local store for the GC thread, adding additional struc\u00adtures \nin the heap (to record information for collection), renaming heap cells (for copying GCs), etc.. For \neach mutator thread t,the relations .(t) and .(t) need to hold at the beginning and the end of each basic \ntransformation unit (every high-level primitive instruc\u00adtion in this case) respectively. We let .(t) \nbe the same as .(t) to support sequential compositions. We require InitRelT(.(t)) (see Figure 6), i.e., \n.(t) holds over the initial states. In addition, the tar\u00adget and the source boolean expressions should \nbe evaluated to the same value under related states, as required in the IF and WHILE rules in Figure \n7. GoodT(.(t)) InitRelT(.(t))..B..(t) . (T(B) . B) (7.4) Theorem 8 (Verifying Concurrent Garbage Collecting \nSystems). If there exist Rgc, Ggc, pgc, R(t), R(t), .(t) and a such that (7.1), (7.2), (7.3), (7.4) and \nthe following hold: 1. (Veri.cation of the GC code) Rgc; Ggc .{pgc}Cgc{false}; 2. (Correctness of T on \nmutator instructions) .c. (t.T(c), R(t), G(t)) :a;.(t) .(t) (t.c, R(t), G(t)); 3. (Side conditions) Ggc \n. a-1 . a-1 . (AbsGCStep) * ; .s, S.s = T(S) =. pgc s; then Correct(T). That is, to verify a concurrent \ngarbage collecting system, we need to do the following: De.ne the a and .(t) relations, and prove the \ncorrectness of T on high-level primitive instructions. Since T preserves the syn\u00adtax on most instructions, \nit s often immediate to prove the target instructions are simulated by their sources. But for instructions \nthat are transformed to barriers, we need to verify the barriers that they implement both the source \ninstructions (by RGSim) and the interaction mechanism (shown in their guarantees).  Find some proper \nGgc and pgc, and verify the GC code by R-G reasoning. We require the GC s guarantee Ggc should not con\u00adtain \nmore behaviors than AbsGCStep (the .rst side condition), and Cgc can start its execution from any state \ns transformed from a high-level one (the second side condition).   7.3 Application: Boehm et al. Concurrent \nGC Algorithm We illustrate the applications of the framework (Theorem 8) by proving the correctness of \na mostly-concurrent mark-sweep garbage collector proposed by Boehm et al. [7]. Variants of the algorithm \nhave been used in practice (e.g., by IBM [2]). Due to the space limit, we only describe the proof sketch \nhere. Details are presented in the technical report [20]. Overview of the GC algorithm. The top-level \ncode of the GC thread is shown in Figure 10. In each collection cycle, after an  { wfstate} 0 Collection() \n{ 1 local mstk: Seq(Int); Loop Invariant: { wfstate * (ownnp(mstk) . mstk = E)} 2 while (true) { 3 Initialize(); \n{ (wfstate . reach inv) * (ownnp(mstk) . mstk = E)} 4 Trace(); { (wfstate . reach inv) * (ownnp(mstk) \n. mstk = E)} 5 CleanCard(); { (wfstate . reach inv) * (ownnp(mstk) . mstk = E)} atomic{ 6 ScanRoot(); \n{{.X.(wfstate . reach rtnw stk(X) . stk black(X)) *(ownnp(mstk) . mstk = X)} 7 CleanCard(); } { (wfstate \n. reach black) * (ownnp(mstk) . mstk = E)} 8 Sweep(); } } { false} Figure 10. Outline of the GC Code \nand Proof Sketch update(x.id, E) { // id . {pt1, ..., ptm} atomic{ x.id := E; aux := x; } atomic{ x.dirty \n:= 1; aux := 0; } } Figure 11. The Write Barrier for Boehm et al. GC initialization process, the GC enters \nthe concurrent mark-phase (line 4) and traces the objects reachable from the roots (i.e.,the mutators \nlocal pointer variables that may contain references to the heap objects). A mark stack (mstk) is used \nto do a depth-.rst tracing. During the tracing, the connectivity between objects might be changed by \nthe mutators, thus a write barrier is required to notify the collector of those modi.ed objects by dirtying \nthe objects tags (called cards). When the tracing is done, the GC suspends all the mutators and re-traces \nfrom the dirty objects that have been marked (called card-cleaning, line 6 and 7). The stop-the-world \nphase is implemented by atomic{C}. Finally, all the reachable objects are ensured marked and the GC performs \nthe concurrent sweep-phase (line 8), in which unmarked objects are reclaimed. Usually in practice, there \nis also a concurrent card-cleaning phase (line 5) before the stop-the-world card-cleaning to reduce the \npause time. The full GC code Cgc is given in the technical report [20]. The write barrier is shown in \nFigure 11, where the dirty .eld is set after modifying the object s pointer .eld. Here we use a write-only \nauxiliary variable aux for each mutator thread to record the current object that the mutator is updating. \nWe add aux only for the purpose of veri.cation, so that we can easily specify the .ne-grained property \nof the write barrier in the guarantees that immediately after updating the pointer .eld, the thread would \ndo nothing else except setting the corresponding dirty .eld. The GC does not use read barriers nor allocation \nbarriers. We .rst present the high-level and low-level program state mod\u00adels in Figure 12. The behaviors \nof the high-level abstract GC thread are de.ned as follows: AbsGCStep {((., H), (., H' )) |.l. reachable(l)(., \nH)=. H(l)= H' (l)} , saying that, the mutator stores and the reachable objects in the heap are remained \nunmodi.ed. Here reachable(l)(., H) means the object at the location l is reachable in H from the roots \nin .. (HStore) S . PVar HVal (HHeap) H . Loc HObj (HThrds). . MutID . HStore (HState)S . HThrds \u00d7 HHeap \n(LStore) s . PVar LVal\u00d7{0, 1} (LHeap) h . [1..M] LObj (LThrds) p . ThrdID . LStore (LState) s . LThrds \n\u00d7 LHeap Figure 12. High-Level and Low-Level State Models The transformation. The transformation T is \nde.ned as follows. For code, the high-level abstract GC thread is transformed to the GC thread shown \nin Figure 10. Each instruction x.id := E in mutators is transformed to the write barrier, where id is \na pointer .eld of x. Other instructions and the program structures of mutators are unchanged. The following \ntransformations are made over initial states. First we require the high-level initial state to be well-formed: \nwfstate(., H) .l. reachable(l)(., H)=. l . dom(H) . That is, reachable locations cannot be dangling pointers. \n High-level locations are transformed to integers by a bijective function Loc2Int : Loc . [0..M] satisfying \nLoc2Int(nil)=0.  Variables are transformed to the low level using an extra bit to preserve the high-level \ntype information (0 for non-pointers and 1 for pointers).  High-level objects are transformed to the \nlow level by adding the color and dirty .elds with initial values WHITE and 0 re\u00adspectively. Other addresses \nin the low-level heap domain [1..M] are .lled out using unallocated objects whose colorsare BLUE and \nall the other .elds are initialized by 0.Hereweuse BLACK and WHITE for marked and unmarked objects respectively, \nand BLUE for unallocated memory.  The concrete GC thread is given an initial store.  The formal de.nition \nof T is included in the technical report [20]. To prove Correct(T) in our framework, we apply Theorem \n8, prove the re.nement between low-level and high-level mutators, and verify the GC code using a unary \nRely-Guarantee-based logic. Re.nement proofs for mutator instructions. We .rst de.ne the a and .(t) relations. \na {((p l{tgc },h), (., H)) |.t . dom(.). store map(p(t), .(t)) . heap map(h, H) . wfstate(., H)} . In \na, the relation between low-level and high-level stores and heaps are enforced by store map and heap \nmap respectively. Their de.\u00adnitions re.ect the state transformations we describe above, ignoring the \nvalues of those high-level-invisible structures. It also requires the well-formedness of high-level states. \nFor each mutator thread t,the .(t) relation enforced at the beginning and the end of each transformation \nunit (each high-level instruction) is stronger than a. It requires that the value of the auxiliary variable \naux (see Figure 11) be a null pointer (0p): .(t) a n{((p, h), (., H)) | p(t)(aux)=0p} . The re.nement \nbetween the write barrier at the low level and the pointer update instruction at the high level is formulated \nas: (t.update(x.id, E), R(t), Gt write barrier) :a;.(t) .(t) (t.(x.id := E), R(t), Gt write pt) , where \nGt and Gt write pt are the guarantees of the two-step write barrier and the high-level atomic write operation \nrespectively. write barrier  Since the transformation of other high-level instructions is identity, \nthe corresponding re.nement proofs are simple. Rely-Guarantee reasoning about the GC code. The unary \npro\u00adgram logic we use to verify the GC thread is a standard Rely-Guarantee logic adapted to the target \nlanguage. We describe states using separation logic assertions, as shown below: p, q ::= B | t.ownp(x) \n| t.ownnp(x) | E1.id . E2 | p * q | ... Following Parkinson et al. [23], we treat program variables as \nre\u00adsource and use t.ownp(x) and t.ownnp(x) for the thread t s owner\u00adships of pointers and non-pointers \nrespectively. We omit the thread identi.ers if these predicates hold for the current thread. We .rst \ngive the precondition and the guarantee of the GC. The GC starts its executions from a low-level well-formed \nstate, i.e., pgc wfstate. Just corresponding to the high-level wfstate de.\u00adnition, the low-level wfstate \npredicate says none of the reachable objects are BLUE.Wede.ne Ggc as follows: Ggc {((p 1{tgc s},h), (p \n1{tgc s ' },h ' )) |.n. reachable(n)(p, h) =.lh(n)J = lh ' (n)J . h(n).color = BLUE . h ' (n).color = \nBLUE} . The GC guarantees not modifying the mutator stores. For any mutator-reachable object, the GC \ndoes not update its .elds coming from the high-level mutator, nor does it reclaim the object. Here LJ \nlifts a low-level object to a new one that contains mutator data only. The proof sketch is given in Figure \n10. One of the key invariants used in the proof is reach inv, which says any WHITE reachable object can \neither be traced from a root object in a path on which every object is WHITE, or be reachable from a \nBLACK object whose pointer .eld was updated and dirty bitwas setto 1. Since the proof is done in the \nunary logic, the details here are orthogonal to our simulation-based proof (but it is RGSim that allows \nus to derive Theorem 8, which then links proofs in the unary logic with relational proofs). We give the \ncomplete proofs in the technical report [20]. 8. Related Work and Conclusion There is a large body of \nwork on re.nements and veri.cation of program transformations. Here we only focus on the work most closely \nrelated to the typical applications discussed in this paper. Verifying compilation and optimizations \nof concurrent programs. Compiler veri.cation for concurrent programming languages can date back to work \nby Wand [31] and Gladstein et al. [14], which is about functional languages using message-passing mechanisms. \nRecently, Lochbihler [21] presents a veri.ed compiler for Java threads and prove semantics preservation \nby a weak bisimulation. He views every heap update as an observable move, thus does not allow the target \nand the source to have different granularities of atomic updates. To achieve parallel compositionality, \nhe requires the relation to be preserved by any transitions of shared states, i.e., the environments \nare assumed arbitrary. As we explained in Section 2.2, this is a too strong requirement in general for \nmany transformations, including the examples in this paper. Burckhardt et al. [9] present a proof method \nfor verifying con\u00adcurrent program transformations on relaxed memory models. The method relies on a compositional \ntrace-based denotational seman\u00adtics, where the values of shared variables are always considered arbitrary \nat any program point. In other words, they also assume arbitrary environments. Following Leroy s CompCert \nproject [19], Sev.c.\u00b4ik et al. [25] verify compilation from a C-like concurrent language to x86 by simulations. \nThey focus on correctness of a particular compiler, and there are two phases in their compiler whose \nproofs are not compo\u00adsitional. Here we provide a general, compiler-independent, compo\u00adsitional proof \ntechnique to verify concurrent transformations. We apply RGSim to justify concurrent optimizations, following \nBenton [3] who presents a declarative set of rules for sequential optimizations. Also the proof rules \nof RGSim for sequential com\u00adpositions, conditional statements and loops coincide with those in relational \nHoare logic [3] and relational separation logic [32]. Proving linearizability or atomicity of concurrent \nobjects. Fil\u00adipovic\u00b4et al. [13] show linearizability can be characterized in terms of an observational \nre.nement, where the latter is de.ned similarly to our Correct(T). There is no proof method given to \nverify the linearizability of .ne-grained object implementations. Turon and Wand [27] propose a re.nement-based \nproof method to verify concurrent objects. They .rst propose a simple re.nement based on Brookes fully \nabstract trace semantics [8], which is com\u00adpositional but cannot handle complex algorithms (as discussed \nin Section 2.2). Their fenced re.nement then uses rely conditions to .lter out illegal environment transitions. \nThe basic idea is similar to ours, and the re.nement can also be used to verify Treiber s stack algorithm. \nHowever, it is not a congruence for parallel composi\u00adtion . In their settings, both the concrete (.ne-grained) \nand the ab\u00adstract (atomic) versions of object operations need to be expressed in the same language. They \nalso require that the .ne-grained im\u00adplementation should have only one update action over the shared \nstate to correspond to the high-level atomic operation. These re\u00adquirements and the lack of parallel \ncompositionality limit the ap\u00adplicability of their method. It is unclear if the method can be used for \ngeneral veri.cation of transformations, such as concurrent GCs. Elmas et al. [12] prove linearizability \nby incrementally rewrit\u00ading the .ne-grained implementation to the atomic abstract speci\u00ad.cation. Their \nbehavioral simulation used to characterize lineariz\u00adability is an event-trace subset relation with requirements \non the orders of method invocations and returns. Their rules heavily rely on movers (i.e., operations \nthat can commute over any operation of other threads) and always rewrite programs to instructions, thus \nare designed speci.cally for atomicity veri.cation. In his thesis [28], Vafeiadis proves linearizability \nof concurrent objects in RGSep logic by introducing abstract objects and abstract atomic operations as \nauxiliary variables and code. The re.nement between the concrete implementation and the abstract operation \nis implicitly embodied in the unary veri.cation process, but is not spelled out formally in the meta-theory \n(e.g., the soundness). Verifying concurrent GCs. Vechev et al. [30] de.ne transforma\u00adtions to generate \nconcurrent GCs from an abstract collector. After\u00adwards, Pavlovic et al. [24] present re.nements to derive \nconcrete concurrent GCs from speci.cations. These methods focus on de\u00adscribing the behaviors of variants \n(or instantiations) of a correct ab\u00adstract collector (or a speci.cation) in a single framework, assuming \nall the mutator operations are atomic. By comparison, we provide a general correctness notion and a proof \nmethod for verifying con\u00adcurrent GCs and the interactions with mutators (where the barriers could be \n.ne-grained). Furthermore, the correctness of their trans\u00adformations or re.nements is expressed in a \nGC-oriented way (e.g., the target GC should mark no less objects than the source), which cannot be used \nto justify other transformations. Kapoor et al. [18] verify Dijkstra s GC using concurrent sepa\u00adration \nlogic. To validate the GC speci.cations, they also verify a representative mutator in the same system. \nIn contrast, we reduce the problem of verifying a concurrent GC to verifying a transfor\u00admation, ensuring \nsemantics preservation for all mutators. Our GC veri.cation framework is inspired by McCreight et al. \n[22], who propose a framework for separate veri.cation of stop-the-world and incremental GCs and their \nmutators, but their framework does not handle concurrency.  Conclusion and future work. We propose RGSim \nto verify con\u00adcurrent program transformations. By describing explicitly the inter\u00adference with environments, \nRGSim is compositional, and can sup\u00adport many widely-used transformations. We have applied RGSim to reason \nabout optimizations, prove atomicity of .ne-grained con\u00adcurrent algorithms and verify concurrent garbage \ncollectors. In the future, we would like to further test its applicability with more ap\u00adplications, such \nas verifying STM implementations and compilers. It is also interesting to explore the possibility of \nbuilding tools to automate the veri.cation process. Acknowledgments We would like to thank Matthew Parkinson \nand anonymous ref\u00aderees for their suggestions and comments on earlier versions of this paper. This work \nis supported in part by grants from National Natural Science Foundation of China (NSFC) under Grant No. \n60928004, 61073040 and 61103023. Xinyu Feng is also supported in part by NSFC under Grant No. 90818019, \nby Program for New Century Excellent Talents in Universities (NCET), and by the Fun\u00addamental Research \nFunds for the Central Universities. References [1] M. Abadi and G. Plotkin. A model of cooperative threads. \nIn Proc. 36th ACM Symp. on Principles of Prog. Lang. (POPL 09), pages 29 40. ACM Press, January 2009. \n[2] K. Barabash, O. Ben-Yitzhak, I. Goft, E. K. Kolodner, V. Leikehman, Y. Ossia, A. Owshanko, and E. \nPetrank. A parallel, incremental, mostly concurrent garbage collector for servers. ACM Trans. Program. \nLang. Syst., 27(6):1097 1146, 2005. [3] N. Benton. Simple relational correctness proofs for static analyses \nand program transformations. In Proc. 31th ACM Symp. on Principles of Prog. Lang. (POPL 04), pages 14 \n25. ACM Press, January 2004. [4] N. Benton and C.-K. Hur. Biorthogonality, step-indexing and compiler \ncorrectness. In Proc. 14th ACM Int l Conf. on Functional Prog. (ICFP 09), pages 97 108. ACM Press, September \n2009. [5] H.-J. Boehm. Threads cannot be implemented as a library. In Proc. 2005 ACM Conf. on Prog. Lang. \nDesign and Impl. (PLDI 05), pages 261 268. ACM Press, June 2005. [6] H.-J. Boehm and S. V. Adve. Foundations \nof the C++ concurrency memory model. In Proc. 2008 ACM Conf. on Prog. Lang. Design and Impl. (PLDI 08), \npages 68 78. ACM Press, June 2008. [7] H.-J. Boehm, A. J. Demers, and S. Shenker. Mostly parallel garbage \ncollection. In Proc. 1991 ACM Conf. on Prog. Lang. Design and Impl. (PLDI 91), pages 157 164. ACM Press, \nJune 1991. [8] S. D. Brookes. Full abstraction for a shared-variable parallel language. Inf. Comput., \n127(2):145 163, 1996. [9] S. Burckhardt, M. Musuvathi, and V. Singh. Verifying local transfor\u00admations \non relaxed memory models. In Proc. 19th Int l Conf. on Com\u00adpiler Construction (CC 10), volume 6011 of \nLecture Notes in Com\u00adputer Science, pages 104 123. Springer, March 2010. [10] Coq Development Team. The \nCoq proof assistant reference manual. The Coq release v8.3, October 2010. [11] D. Dice, O. Shalev, and \nN. Shavit. Transactional locking II. In Proc. 20th Int l Symp. on Distributed Computing (DISC 06), volume \n4167 of Lecture Notes in Computer Science, pages 194 208. Springer, September 2006. [12] T. Elmas, S. \nQadeer, A. Sezgin, O. Subasi, and S. Tasiran. Simplifying linearizability proofs with reduction and abstraction. \nIn Proc. 16th Int l Conf. on Tools and Algorithms for the Construction and Analysis of Systems (TACAS \n10), volume 6015 of Lecture Notes in Computer Science, pages 296 311. Springer, March 2010. [13] I. Filipovi\u00b4c, \nP. O Hearn, N. Rinetzky, and H. Yang. Abstraction for concurrent objects. In Proc. 18th European Symp. \non Prog. (ESOP 09), volume 5502 of Lecture Notes in Computer Science, pages 252 266. Springer, March \n2009. [14] D. S. Gladstein and M. Wand. Compiler correctness for concur\u00adrent languages. In Proc. 1st \nInt l Conf. on Coordination Languages and Models (COORDINATION 96), volume 1061 of Lecture Notes in Computer \nScience, pages 231 248. Springer, April 1996. [15] M. Herlihy and N. Shavit. The Art of Multiprocessor \nProgramming. Morgan Kaufmann, April 2008. [16] C.-K. Hur and D. Dreyer. A Kripke logical relation between \nML and assembly. In Proc. 38th ACM Symp. on Principles of Prog. Lang. (POPL 11), pages 133 146. ACM Press, \nJanuary 2011. [17] C. B. Jones. Tentative steps toward a development method for inter\u00adfering programs. \nACM Trans. Program. Lang. Syst., 5(4):596 619, 1983. [18] K. Kapoor, K. Lodaya, and U. Reddy. Fine-grained \nconcurrency with separation logic. J. Philosophical Logic, 40(5):583 632, 2011. [19] X. Leroy. A formally \nveri.ed compiler back-end. J. Autom. Reason., 43(4):363 446, December 2009. [20] H. Liang, X. Feng, and \nM. Fu. A rely-guarantee-based simulation for verifying concurrent program transforma\u00adtions. Technical \nreport (with Coq implementation), Uni\u00adversity of Science and Technology of China, October 2011. http://kyhcs.ustcsz.edu.cn/relconcur/rgsim. \n[21] A. Lochbihler. Verifying a compiler for java threads. In Proc. 20th European Symp. on Prog. (ESOP \n10), volume 6012 of Lecture Notes in Computer Science, pages 427 447. Springer, March 2010. [22] A. McCreight, \nZ. Shao, C. Lin, and L. Li. A general framework for certifying garbage collectors and their mutators. \nIn Proc. 2007 ACM Conf. on Prog. Lang. Design and Impl. (PLDI 07), pages 468 479. ACM Press, June 2007. \n[23] M. Parkinson, R. Bornat, and C. Calcagno. Variables as resource in Hoare logics. In Proc. 21th IEEE \nSymp. on Logic In Computer Science (LICS 06), pages 137 146. IEEE Computer Society, August 2006. [24] \nD. Pavlovic, P. Pepper, and D. R. Smith. Formal derivation of con\u00adcurrent garbage collectors. In Proc. \n10th Int l Conf. on Mathematics of Program Construction (MPC 10), volume 6120 of Lecture Notes in Computer \nScience, pages 353 376. Springer, June 2010. [25] J. .c\u00b4ik, V. Vafeiadis, F. Z. Nardelli, S. Jagannathan, \nand P. Sewell. Sev.Relaxed-memory concurrency and veri.ed compilation. In Proc. 38th ACM Symp. on Principles \nof Prog. Lang. (POPL 11), pages 43 54. ACM Press, January 2011. [26] R. K. Treiber. System programming: \ncoping with parallelism. Tech\u00adnical Report RJ 5118, IBM Almaden Research Center, 1986. [27] A. Turon \nand M. Wand. A separation logic for re.ning concurrent objects. In Proc. 38th ACM Symp. on Principles \nof Prog. Lang. (POPL 11), pages 247 258. ACM Press, January 2011. [28] V. Vafeiadis. Modular .ne-grained \nconcurrency veri.cation. Techni\u00adcal Report UCAM-CL-TR-726, University of Cambridge, Computer Laboratory, \nJuly 2008. [29] V. Vafeiadis and M. J. Parkinson. A marriage of rely/guarantee and separation logic. \nIn Proc. 18th Int l Conf. on Concurrency Theory (CONCUR 07), volume 4703, pages 256 271. Springer, 2007. \n[30] M. T. Vechev, E. Yahav, and D. F. Bacon. Correctness-preserving derivation of concurrent garbage \ncollection algorithms. In Proc. 2006 ACM Conf. on Prog. Lang. Design and Impl. (PLDI 06), pages 341 353. \nACM Press, June 2006. [31] M. Wand. Compiler correctness for parallel languages. In Proc. Conf. on Functional \nProg. Lang. and Computer Architecture (FPCA 95), pages 120 134. ACM Press, June 1995. [32] H. Yang. Relational \nseparation logic. Theoretical Computer Science, 375(1-3):308 334, 2007.    \n\t\t\t", "proc_id": "2103656", "abstract": "<p>Verifying program transformations usually requires proving that the resulting program (the target) refines or is equivalent to the original one (the source). However, the refinement relation between individual sequential threads cannot be preserved in general with the presence of parallel compositions, due to instruction reordering and the different granularities of atomic operations at the source and the target. On the other hand, the refinement relation defined based on fully abstract semantics of concurrent programs assumes arbitrary parallel environments, which is too strong and cannot be satisfied by many well-known transformations. In this paper, we propose a Rely-Guarantee-based Simulation (RGSim) to verify concurrent program transformations. The relation is parametrized with constraints of the environments that the source and the target programs may compose with. It considers the interference between threads and their environments, thus is less permissive than relations over sequential programs. It is compositional w.r.t. parallel compositions as long as the constraints are satisfied. Also, RGSim does not require semantics preservation under all environments, and can incorporate the assumptions about environments made by specific program transformations in the form of rely/guarantee conditions. We use RGSim to reason about optimizations and prove atomicity of concurrent objects. We also propose a general garbage collector verification framework based on RGSim, and verify the Boehm et al. concurrent mark-sweep GC.</p>", "authors": [{"name": "Hongjin Liang", "author_profile_id": "81496643592", "affiliation": "University of Science and Technology of China, Hefei, China", "person_id": "P2991450", "email_address": "lhj1018@mail.ustc.edu.cn", "orcid_id": ""}, {"name": "Xinyu Feng", "author_profile_id": "81539835456", "affiliation": "University of Science and Technology of China, Hefei, China", "person_id": "P2991451", "email_address": "xyfeng@ustc.edu.cn", "orcid_id": ""}, {"name": "Ming Fu", "author_profile_id": "81460653083", "affiliation": "University of Science and Technology of China, Hefei, China", "person_id": "P2991452", "email_address": "fuming@ustc.edu.cn", "orcid_id": ""}], "doi_number": "10.1145/2103656.2103711", "year": "2012", "article_id": "2103711", "conference": "POPL", "title": "A rely-guarantee-based simulation for verifying concurrent program transformations", "url": "http://dl.acm.org/citation.cfm?id=2103711"}