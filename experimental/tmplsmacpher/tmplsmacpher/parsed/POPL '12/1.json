{"article_publication_date": "01-25-2012", "fulltext": "\n Message of Thanks On the receipt of the 2011 ACM SIGPLAN Distinguished Achievement Award Tony Hoare \nPrincipal Researcher, Microsoft Research Ltd., Hon. Mem. Cambridge University Computer Laboratory. Categories \nand Subject Descriptors D.2.1 [Programming Lan\u00adguages]: Formal De.nitions and Theory; D.2.4 [Software \nEn\u00adgineering]: Software/Program Veri.cation; F.3.1 [Logics and Meanings of Programs]: Specifying and \nVerifying and Reason\u00ading about Programs; F.3.2 [Logics and Meanings of Programs]: Semantics of Programming \nLanguages General Terms Veri.cation, Reliability, Languages, Theory, Se\u00adcurity Even to one who has lived \nlong enough to receive many awards, the recognition of distinguished achievement from the scienti.c community \nin one s own research .eld is surprisingly welcome. I particularly treasure the SIGPLAN citation for \nmy award, which singles out exactly the modest achievements of which I am most proud. I have known nearly \nall the previous winners of this award, and they number among my professional colleagues, rivals, and \nfriends. I have derived inspiration from them all, and I am pleased to be regarded in their company. \nThe award prompts me to re.ect again on the origins and progress of my professional career: how did I \nselect the topics for my research? What methods did I consider appropriate for an academic researcher \nto pursue such research? What is the current level of maturity of research on these topics? What does \nthe fu\u00adture hold for it? Why do I regard my renewed interest in unifying theories as a contribution to \nthat future? Computer programs I have always regarded the computer program as a worthy topic for scienti.c \nstudy. Indeed, programs must surely be a central topic in computer science, both pure and applied. Having \nchosen this topic, I addressed myself to the fundamental questions that excite the curiosity of all scientists, \nno matter what their chosen topic of research. What does it do? To answer this question, we need a conceptual \nframework and language for describing the externally observable properties and behaviour the program. \nEqually important is a de\u00adscription of the environment in which the program is executed, in- Copyright \nis held by the author/owner(s). POPL 12, January 25 27, 2012, Philadelphia, PA, USA. ACM 978-1-4503-1083-3/12/01. \n cluding its users. In many branches of science, separate branches of mathematics have been developed \nto provide effective descrip\u00adtive frameworks for the topics relevant to that branch. Fortunately, logicians \nfrom around the beginning of the last century have shown that all branches of mathematics rest on a common \nfoundation. To preserve the greatest generality in describing the behaviour of a general-purpose computer, \nwe should exploit this common foun\u00addation. From it we get Boolean algebra, predicate calculus, and the \ntheory of sets. When required, it is easy to de.ne from these foundations the structures manipulated \nby our programs, for ex\u00adample functions, sums, products, types, relations, sequences, bags, etc. Experiments \nin application have shown that these concepts give concise and intuitive descriptions of computer systems, \nwhile pre\u00adserving a level of abstraction that is appropriate for humanly com\u00adprehensible speci.cations. \nHow does it work? For this we need to look inside the program, to identify its internal components and \nthe ways that they are con\u00adnected. Again, the foundations of mathematics provide a language for de.ning \nthe behaviour of each component, and the interfaces between them. In an idealised vision of rational \nsoftware engineer\u00ading, these interface speci.cations will be written in advance of the design. They will \nserve as a contract between the implementers of the various program components. This is an ideal that \nhas inspired much excellent pure research in the .eld, aimed at the design of the\u00adories that exploit \nmodularity (compositionality) of designs, even if it has to be sacri.ced later to ef.ciency of the eventually \nexecuted program. Why does it work? The answer to this deeper question must appeal to general principles, \nwhich apply not just to a particular program, but to a general range of similar programs, actual or hypothetical. \nThe principles should support a proof that the joint working of all the components of a program will \nnecessarily lead to the correct operation of the whole program, as described by its speci.cation. Then \nwe will really know both why and how the program works. The principles of programming are often codi.ed \nas a set of rules de.ning the semantics of the programming language in which the program is (or can be) \nexpressed. Several forms of semantics have been developed to serve different purposes. An operational \nsemantics provides a guide for an implementation of the program\u00adming language, and serves the programmer \nas a basis for diagnos\u00ading errors discovered in test. A deductive semantics gives the pat\u00adtern for proving \ncorrectness of programs, which is a more dif.cult but more certain way of ensuring that no errors will \ndetected in ser\u00advice. A denotational semantics constructs a mathematical model of the programming language, \nso that standard mathematical reason\u00ading can be applied directly to it. Each of these styles of semantics \nhas a clearly de.ned role. The last section of this message expresses the hope that a full theory of \nprogramming will eventually unify semantics expressed in all the styles, and so ensure their mutual consistency. \n How do we know the theory applies to the real world? This is the question of greatest concern to the \nscientist. It is answered by conducting a series of varied experiments, with collection and interpretation \nof their observed results. Every claim of discovery of a new theory, and every extension/correction to \nan old theory, must be supported by appeal to experiment even a thought experiment will do. Further \nand larger experiments are needed to explore the limits of applicability even of already well-supported \ntheories. For objectivity, these experiments should be conducted by scientists independent of the proposers \nof the theory. Often the theory needs modi.cation or extension to extend the range of its application. \nA unifying theory is one whose range of application is the widest of all. How are the results of the \nresearch transferred to engineering practice? Modern engineering depends utterly on automation of the \ndesign process. It is the computer that works out in detail the implications of every engineering design \ndecision, and checks the serviceability and safety of a product before it comes into service. An established \ndesign automation toolset provides a rapid means of world-wide transfer of new scienti.c results into \nevery\u00adday engineering practice. When scientists agree about the strength of the evidence of a new theory, \nor an amendment or extension of an old theory, the implementers and suppliers of the tools will compete \nto incorporate any consequential improvements in the next release of the toolset for which they have \ndeveloped a market share. Current state of the art In mature branches of science, it is the natural world \nthat sup\u00adplies the experimental observations that support, refute, or re.ne the theories of pure scientists. \nAstronomers and physicists (and most recently geneticists) have collaborated on long-term interna\u00adtional \nprojects to build the telescopes and reactors to conduct ex\u00adperiments, whose results populate the enormous \ndata bases of the world s computers. For an engineering science, the experiments are performed on the \nproducts made by man. In computer science, the programs developed by the open source movement already \ngive cheap and public access to experimental material, on a scale that is fortunately more than adequate \nfor our current research needs. The main task for modern data-based science is to interpret the enormous \nvolumes of experimental material, by relating it to the natural laws which are believed to explain them. \nScientists write computer signi.cant computer programs to analyse the mass of data automatically in the \nlight of current theories. This is now the only way to extract scienti.cally illuminating information \nfrom the data, and so to re.ne, extend, and even unify existing theories. Analogous tools for scienti.c \nanalysis of programs are used by the software industry to help in the engineering of critical parts of \nwidely used software. They are often used to support the exper\u00adimental side of research into the principles \nof programming. The last decade has seen an enormous increase in the power of these programming tools. \nMoore s law predicts every decade a roughly a thousand fold increase in the space-time performance of \ncommodity computer chips, often accompanied by a reduction in price. This rate of progress has been compounded \nby a compara\u00adble increase in the algorithmic ef.ciency of SAT and SMT solvers and model checkers. The \nadvance in software tools has been driven by regular scienti.c competitions. These are regularly organised \nand refereed by independent scientists, and the whole experimen\u00adtal community collaborates in the assembly \nof realistic challenge material for conduct of the competitions. A second .llip for program analysis \nhas been the totally un\u00adexpected phenomenon of the computer virus. Viruses or other mal\u00adware exploit \na programming error to damage or even take control of (perhaps millions of) computers which run the erroneous \nprogram. That is why leading software manufacturers are continuing to in\u00advest heavily in the development \nof program analysis tools. These are based on the best available current theories of programming, and \nthe best available SMT solvers and model checkers. The tools are now applied routinely to many millions \nof lines of commercial software before release. The future? I therefore predict an exciting future for \nfurther academic research on the principles of programming, and for further exploitation of its research \nresults. The research will take advantage of the most advanced available industrial program analysis \ntools to perform experiments, at ever increasing scales, on real and realistic soft\u00adware. The tools themselves \nwill evolve by exploiting experience of their use, both by scientists and by software developers. Con\u00adtinuous \ninteraction of theorists, tool-builders and experimental\u00adists will lead to an exponential increase in \nthe rate of scienti.c progress; perhaps computer science will match the recent spurt in the progress \nof physics, astronomy, and more recently biology, which has been achieved by integrating computerised \ntools into the scienti.c method and culture. Much of this progress will be made by collaboration between \nacademic researchers and industrial software developers, who al\u00adready welcome the opportunity of using \nprogram analysis tools to reduce the costs and the risks of programming error. At the same time, industry \nwill continue to pour their resources into more imme\u00addiately applicable tools, which concentrate on test \ncase generation of early detection of programming errors. Academic research should not be con.ned to \ncompeting with the better funded research of industrial users. It should also con\u00adtinue to pursue higher \nand longer term ideals, because this is the only way of ensuring a continued stream of new ideas and \neven breakthroughs to advance the state of the art. Ideals such as accu\u00adracy of measurement or purity \nof materials are the driving force of science. Even if the theory itself says that they can only be approxi\u00admated, \nthe approximations can be inde.nitely re.ned. In computer science, the relevant scienti.c ideal is total \ncorrectness of computer programs, guaranteed by proofs conducted with the assistance of computers during \ntheir design and implementation. It is for the en\u00adgineer to decide later in each case how far the ideals \nmust be com\u00adpromised to meet engineering constraints of cost and timescale. Uni.cation My own personal \nresearch has recently reverted to pursuit of a sci\u00adenti.c ideal, namely the uni.cation of theories of \nprogramming. Since this is mentioned in my citation for the distinguished achieve\u00adment award, I will \ndevote this last section of my message of thanks to explaining why I believe that uni.cation will make \na contribution to the future described in the previous section. In the natural sciences, the quest for \na unifying theory is an integral part of the scienti.c culture. The aim is to show that a single theory \napplies to a wide range of highly disparate phenom\u00adena. For example, the gravitational theory of Isaac \nNewton applies very accurately both to apples falling towards the earth and to plan\u00adets falling towards \nthe sun. In many cases, a more homogeneous subset of the phenomena is already covered by a more specialised \nscienti.c theory. In these cases, the specialised theory must be de\u00adrived mathematically from the claimed \nuni.ed theory. For example, Newton s theory of gravitation uni.es the elegant planetary theory of Kepler, \nas well as the less elegant Ptolemaic theories of astron\u00adomy.  The scienti.c bene.t of a uni.ed theory \nis that it is supported by all the evidence that has already been accumulated for all of the previous \ntheories separately. Furthermore, each of the previous theories then inherits the support given by the \ntotal sum of evidence contributed by all the other theories. The practicing engineer has different concerns \nfrom the scien\u00adtist, including deadlines and budgets for the current project. The engineer will therefore \ncontinue to use familiar more specialised theories that have been found from experience to be well adapted \nto the particular features of the current project, or the needs of the current client. Indeed, the innovative \nengineer will often specialise the theory even further, adapting it so closely to current needs that \nthere will never be an opportunity for repeated use. That is why the separate theories that are subsumed \nby a unifying theory often retain all their practical value, and they are in no way belittled or superseded \nby the uni.cation. The real practical value of uni.cation lies in its contribution to the transfer of \nthe results of scienti.c research into engineering practice. One of the main factors that inhibit the \nengineer (and the sensible manager) from adopting a scienti.c theory is that the scientists do not yet \nagree what that theory should be. Fortunately, there is an agreed method of resolving a scienti.c dispute. \nAn experiment is designed whose result is predicted differently by all the theories that are party to \nthe dispute. The engineer can then have increased con.dence in the winner. But sometimes, no such decisive \nexperiment can be discovered. This may be because, in spite of differences in their presentation, the \ntheories are in fact entirely consistent. In this case, the only way of resolving the issue is to .nd \na theory that uni.es them all. Quantum theory provides an example. Three separate mathematical presentations \nof quantum theory were put forward by Heisenberg, Schroedinger and Dirac. Then Dirac showed that they \nwere all derivable from a single uni.ed theory. This is what enabled the award of a Nobel prize to all \nthree of them. And quantum theory is now accepted as the nearest to a theory of everything that physics \nhas to offer. A second contribution of a uni.ed theory to the practicing engi\u00adneer is in the design and \nuse of a suite of software tools that assist in automation of the design process. Since every major engineering \nenterprise today combines a range of technologies, it is important that all the specialised members of \nthe tool suite should be based on a common theory, so that they can communicate consistently among each \nother on standard interfaces which are based upon the uni.cation. The standards also facilitate competition \namong the tools, and permit independent evolution of separate tools for joint use in a design automation \ntoolset. Finally, the education of the general scientist and engineer will surely be facilitated by reducing \nthe number of independently de\u00adveloped theories to a single theory, presented in a single coherent framework \nand notation. That in itself is suf.cient justi.cation for conduct by academics of research into uni.cation \nof theories.  \n\t\t\t", "proc_id": "2103656", "abstract": "", "authors": [{"name": "Tony Hoare", "author_profile_id": "81335491522", "affiliation": "Microsoft Research, Cambridge, United Kingdom", "person_id": "P2991324", "email_address": "thoare@microsoft.com", "orcid_id": ""}], "doi_number": "10.1145/2103656.2103659", "year": "2012", "article_id": "2103659", "conference": "POPL", "title": "Message of thanks: on the receipt of the 2011 ACM SIGPLAN distinguished achievement award", "url": "http://dl.acm.org/citation.cfm?id=2103659"}