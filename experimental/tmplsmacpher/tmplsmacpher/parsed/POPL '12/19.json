{"article_publication_date": "01-25-2012", "fulltext": "\n A Compiler and Run-time System for Network Programming Languages Christopher Monsanto NateFoster Rob \nHarrison * DavidWalker Princeton University Cornell University US Military Academy Princeton University \nAbstract Software-de.ned networks (SDNs) are a new kind of network architecture in which a controller \nmachine manages a distributed collection of switches by instructing them to install or uninstall packet-forwarding \nrules and report traf.c statistics. The recently formed Open Networking Consortium, whose members include \nGoogle, Facebook, Microsoft, Verizon, and others, hopes to use this architecture to transform the way \nthat enterprise and data center networks are implemented. In this paper, we de.nea high-level, declarative \nlanguage, called NetCore, for expressing packet-forwarding policies on SDNs. Net-Core is expressive, \ncompositional, and has a formal semantics. To ensure that a majority of packets are processed ef.ciently \non switches instead of on the controller we present new compila\u00adtion algorithms for NetCore and couple \nthem with a new run-time system that issues rule installation commands and traf.c-statistics queries \nto switches. Together, the compiler and run-time system generate ef.cient rules whenever possible and \noutperform the sim\u00adple, manual techniques commonly used to program SDNs today. In addition, the algorithms \nwe develop are generic, assuming only that the packet-matching capabilities available on switches satisfy \nsome basic algebraic laws. Overall, this paper delivers a new design for a high-level net\u00adwork programming \nlanguage; an improved set of compiler algo\u00adrithms; a new run-time system for SDN architectures; the .rst \nfor\u00admal semantics and proofs of correctness in this domain; and an implementation and evaluation that \ndemonstrates the performance bene.ts over traditional manual techniques. Categories and Subject Descriptors \nD.3.2 [Programming Lan\u00adguages]: Language Classi.cations Specialized application lan\u00adguages GeneralTerms \nLanguages, Design Keywords Software-de.ned Networking, OpenFlow, Frenetic, Network programming languages, \nDomain speci.c languages * The views expressed in this paper are those of the authors and do not re.ect \nthe of.cial policy or position of the US Military Academy, the De\u00adpartment of the Army, the Department \nof Defense, or the US Government. Copyright2012AssociationforComputing Machinery.ACM acknowledgesthatthis \ncontribution was authored or co-authored by a contractor or af.liate of the U.S. Government. As such, \nthe Government retains a nonexclusive, royalty-free right to publishor reproducethis article,ortoallowotherstodoso,forGovernmentpurposes \nonly. POPL 12, January25 27,2012, Philadelphia,PA,USA. Copyright&#38;#169;2012ACM978-1-4503-1083-3/12/01. \n. .$10.00 1. Introduction A network is a collection of connected devices that route traf\u00ad.c from one \nplace to another. Networks are pervasive: they con\u00adnect students and faculty on university campuses, \nthey send pack\u00adets between a variety of mobile devices in modern households, they route search requests \nand shopping orders through data cen\u00adters, they tunnel between corporate networks in San Francisco and \nHelsinki, and they connect the steering wheel to the drive train in your car. Naturally, these networks \nhave different pur\u00adposes, properties, and requirements.To service these requirements, companies like \nCisco, Juniper, and others manufacture a variety of devices including routers (which forward packets \nbased on IP addresses), switches (which forward packets based on MAC ad\u00addresses),NAT boxes (which translate \naddresses withina network), .rewalls (which squelch forbidden or unwanted traf.c), and load balancers \n(which distribute work among servers), to name a few. While each of these devices behaves differently, \ninternally they are all built on top of a data plane that buffers, forwards, drops, tags, rate limits, \nand collects statistics about packets at high speed. More complicated devices like routers also have \na control plane that run algorithms for tracking the topology of the network and computing routes through \nit. Using statistics gathered from the data plane and the results computed using the device s specialized \nalgorithms, the control plane installs or uninstalls forwarding rules in the data plane. The data plane \nisbuilt out offast, special-purpose hardware, capable of forwarding packets at the rate at which they \narrive, while the control plane is typically implemented in software. Remarkably, however, traditional \nnetworks appear to be on the verge of a major upheaval. On March 11th, 2011, Deutsche Telekom,Facebook, \nGoogle, Microsoft,Verizon, andYahoo!,own\u00aders of some of the largest networks in the world, announced \nthe for\u00admation of the Open NetworkingFoundation [19]. The foundation s proposal is extraordinarily simple: \neliminate the control plane from network devices. Instead of baking speci.c control software into each \ndevice, the foundation proposes a standard protocol that a separate, general-purpose machine called a \ncontroller can use to program and query the data planes of many cooperating devices. By moving the control \nplane from special-purpose devices onto stock machines, companies like Google will be able to buy cheap, \ncommodity switches, and write controller programs to customize and optimize their networks however theychoose. \nNetworks built on this new architecture, which arose from ear\u00adlier work on Ethane [4] and 4D [10], are \nnow commonly referred to as Software-De.ned Networks (SDNs). Already, several commer\u00adcial switch vendors \nsupport OpenFlow [17], a concrete realization of the switch-controller protocol required for implementing \nSDNs, and researchers have used OpenFlow to develop new network-wide algorithms for server load-balancing, \ndata center routing, energy\u00adef.cient network management, virtualization, .ne-grained access control, \ntraf.c monitoring, fault tolerance, denial of service detec\u00adtion, host mobility, and manyothers [8, 12 \n14, 18, 26]. Now the obvious question is: Why should programming lan\u00adguage researchers, and the POPL \ncommunity in particular, care about these developments? The answer is clear: Some of our most important \ninfrastructure our networks will soon be running an entirely new kind of program. Using our experience, \nprinciples, tools, and algorithms, our community has a unique opportunity to de.ne the languages these \nprograms will be written in and the in\u00adfrastructure used to implement them. We can have major impact, \nand help make future networks easier to program, more secure, more reliable, and more ef.cient. As a \nstep toward carrying out this agenda, we propose a high\u00adlevel language called NetCore, the Network Core \nProgramming Language, for expressing packet-forwarding policies. NetCore has an intuitive syntax based \non familiar set-theoretic operations that allows programmers to construct (and reason about!) rich policies \nin a natural way. NetCore s primitives for classifying packets in\u00adclude exact-match bit patterns and \narbitrary wildcard patterns. It also supports using arbitrary functions to analyze packets and his\u00adtorical \ntraf.c patterns. This feature makes it possible to describe complicated, dynamic policies such as authentication \nand load bal\u00adancing in a natural way, using ordinary functional programs. Unfortunately, compiling these \nrich policies is challenging. On the one hand, the controller machine has the computational power to \nevaluate arbitrary policies, but the switches do not: they can only implement simple kinds of bit matching \nrules. On the other hand, directing a packet to the controller for processing incurs orders of magnitude \nmore latency than processing it on a switch. Hence, despite the limited computational power of the switches, \nit is critical to .nd ways for them to perform most packet processing. The NetCore compiler and run-time \nsystem surmounts this chal\u00adlenge by analyzing programs and automatically dividing them into two pieces: \none that runs on the switches and another that runs on the controller. Moreover, this division of labour \ndoes not occur once at compile time; it occurs dynamically and repeatedly. Intuitively, when a packet \ncannot be handled by a switch, it is redirected to the controller. The controller partially evaluates \nthe packet with re\u00adspect to the current network policyand dynamically generates new switch-level rules \nthat handle said packet as well as others like it. The new rules are subsequently sent to the switches \nso similar pack\u00adets arriving in the future are handled in the networkfast path. Over time, more and more \nrules are added to the switches and less and lesstraf.cisdivertedtothe controller.Wecallthis iterative \nstrategy reactive specialization. Our strategy is inspired by the idiom commonly used for SDN applications \ntoday [12, 13, 27], in which an event-driven program manually installs a rule to handle future traf.c \nevery time a packet is diverted to the controller. However, manyprograms written man\u00adually in this style \nuse inef.cient exact-match rules rather than wild\u00adcard rules, because reasoning about the semantics of \noverlapping wildcards quickly becomes very complicated too complicated to do by hand. Hence, our strategy \nimproves on past work by (1) pro\u00adviding high-level abstractions that obviate the need for program\u00admers \nto deal with the low-level details of individual switches, (2) synthesizing ef.cient forwarding rules \nthat exploit the capabili\u00adties of modern switches including wildcard rules implemented by ternary content-addressable \nmemories (TCAMs), (3) automating the process of dynamically unfolding packet-processing rules on to switches \ninstead of requiring that programmers craft tricky, low\u00adlevel, event-based programs manually. To summarize: \nthe central contribution of this paper is a frame\u00adwork for implementing a canonical, high-level network \nprogram\u00adming language correctly and ef.ciently. More speci.cally: AuthServer A Controller C Figure 1. \nExample topology. We de.ne the syntax and semantics for NetCore (Section 3) and model the interaction \nbetween the NetCore run-time system and the network in a process calculus style (Section 4). This is \nthe .rst formal analysis of how a controller platform interacts with switches.  We develop novel algorithms \nfor compiling network programs and managing controller-switch interactions at run time, includ\u00ading classi.ergeneration \nand reactive specialization (Section 5).  We prove key correctness theorems (Section 6), establishing \nsimulation relations between our low-level, distributed imple\u00admentation strategy and our high-level NetCore \nsemantics. We also prove an important quiescence theorem showing that our implementation successfully \nrelocates computation from the controller onto switches.  We describe a prototype implementation and \nan evaluation on some simple benchmarks demonstrating the practical utility of our framework (Section \n7).  NetCore arose out of our previous work on Frenetic [9], an\u00adother high-level network programming \nlanguage. Frenetic has three main pieces: (1) an SQL-like query language for reading network state, (2) \na language for specifying packet-forwarding policies and, (3) a functional reactive glue language that \nprocesses the results of queries and generates streams of forwarding policies for the network. NetCore \nreplaces Frenetic s language for expressing for\u00adwarding policies with a signi.cantly more powerful language \nthat supports processing packets using arbitrary functions. In addition, NetCore also contains a minimalist \nquery language, as its predi\u00adcates can analyze traf.c history. The main contribution of this paper relative \nto earlier work on Frenetic is the design of new algorithms for compiling these rich policies and for \nmanaging the controller-switch interactions that arise as compiled policies are executed in a network. \nThese algorithms handle NetCore s new policy language, and the core elementsof Frenetic s old policylanguageeven \nbetter.In particular, the NetCore compiler generates ef.cient switch classi.ers by (1) using wildcard \nrules that process more packets on switches instead of simple exact-match rules, and (2) generating rules \nproactively (i.e., in advance of when they are needed), again to process more packets on switches, instead \nof strictly reactively (i.e., on demand). Finally, NetCore has a formal semantics and correctness proofs \nfor its core algorithms, whereas Frenetic had none. 2. NetCore Overview This section presents additional \nbackground on SDNs and NetCore, using examples to illustrate the main ideas. For concreteness, we focus \non the OpenFlow SDN architecture [20], but we elide and take liberty with certain inessential details. \nOur compiler does not assume the speci.cs of the current OpenFlow platform. OpenFlow overview. OpenFlow \nis based on a two-tiered archi\u00adtecture in which a controller manages a collection of subordinate switches. \nFigure 1 depicts a simple topology with a controller C managing a single switch S. Packets may either \nbe processed on switches or on the controller, but processing a packet on the con\u00adtroller increases its \nlatencyby several orders of magnitude. Hence, to ensure good performance, the controller typically installs \na clas\u00adsi.er consisting of a set of packet-forwarding rules on each switch. Each forwarding rule has \na pattern that identi.es a set of pack\u00adets, an action that speci.es how packets matching the pattern \nshould be processed, counters that keep track of the number and size of all packets processed using the \nrule, and an integer priority. When a packet arrives at a switch, it is processed in three steps: First, \nthe switch selects a rule whose pattern matches the packet. If it has no matching rules, then it drops \nthe packet, and if it has multiple matching rules, then it picks the one with the highest pri\u00adority. \nSecond, the switch updates the counters associated with the rule. Finally, the switch applies the action \nlisted in the rule to the packet. In this paper, we are concerned with two kinds of actions: (1) a forwarding \naction {l1 ,...,lk }, which forwards the packet to a set of (usually one, sometimes zero, rarely more \nthan one) adjacent network locations li, where each li may be the name of another switch, network, or \nhost,1 and (2) a controller action O, which forwards the packet to the controller for processing. NetCore:Asimple \nstaticforwarding policy. NetCore is a declar\u00adative language for specifying high-level packet-forwarding \npoli\u00adcies. The NetCore compiler and run-time system handle the details of translating these policies \nto switch-level rules and issuing com\u00admands to install the generated rules on switches. The simplest \nNetCore policies are speci.ed using a predicate e that matches some set of packets and a set S of locations \nto which those packets should be forwarded.We write these policies e . S. The simplest predicates match \nbits in a particular packet header .eld. For example, the predicate SrcAddr:10.0.0.0/8 speci.es that \nthe .rst octet of the packet s source address must be 10 (us\u00ading the standard notation for expressing \nIP pre.x patterns). More complex predicates arebuiltby taking the union(.), intersection (n), negation(\u00ac), \nor difference(\\) of simpler predicates. Analo\u00adgous set-theoretic operations may be used to compose more \ncom\u00adplex policies from simpler policies. As an example, consider the following policy. SrcAddr:10.0.0.0/8 \n\\ (SrcAddr:10.0.0.1 . DstPort:80) . {Switch 1} It states that packets from sources in subnet 10.0.0.0/8 \nshould be forwarded to switch 1, except for packets coming from 10.0.0.1 or going to a destination on \nport 80. The .rst challenge in compiling a high-level language such as NetCore to a low-level SDN framework \nsuch as OpenFlow arises from the relative lack of expressiveness in the switch packet\u00admatching primitives.For \ninstance, because switches cannotexpress the difference of two patterns in a single rule, this policy \nneeds to be implemented using three rules installed in a particular prioritized order: one that drops \npackets from 10.0.0.1, another that drops all packets going to port 80, anda .nal rule that forwards \nall remaining packets from 10.0.0.0/8 to Switch 1. The following switch\u00adlevel classi.er implements this \npolicy.We write these classi.ers with the highest priority rule .rst. Switch-level patterns are on the \nleft, actions are on the right, and a colon separates the two. SrcAddr:10.0.0.1 : {} DstPort:80 : {} \nSrcAddr:10.0.0.0/8 : {Switch 1} Next consider a similar high-level policy to the .rst: 1On real OpenFlow \nswitches, locations are actually integers corresponding to physical ports on the switch; in this paper \nwe model them symbolically. SrcAddr:10.2.0.0/16 \\ (SrcAddr:10.2.0.1 . DstPort:22) . {Switch 2} We can \ngenerate a classi.er for this policyin the same way: SrcAddr:10.2.0.1 : {} DstPort:22 : {} SrcAddr:10.2.0.0/16 \n: {Switch 2}  Now suppose that we want to generate a classi.er that implements the union of the two \npolicies.We cannot combine the classi.ers in a simpleway(e.g., by concatenating or interleaving them) \nbecause therules interactwitheachother.Forexample,ifweweretosimply concatenate the two lists of rules, \nthe rule that drops packets to port 80 would incorrectly shadow the forwarding rule for traf.c from 10.2.0.0/16. \nInstead, we need to perform a much more complicated translation that produces the following classi.er: \nSrcAddr:10.2.0.1, DstPort:80 : {}, SrcAddr:10.2.0.0/16, DstPort:80 : {Switch 2} SrcAddr:10.2.0.1 : {Switch \n1} SrcAddr:10.2.0.0/16, DstPort:22 : {Switch 1} SrcAddr:10.2.0.0/16 : {Switch 1,Switch 2} SrcAddr:10.0.0.1 \n: {} SrcAddr:10.0.0.0/8, DstPort:80 : {} SrcAddr:10.0.0.0/8 : {Switch 1} Dealing with these complexities \noften leads SDN programmers to use exact-match rules i.e., rules that fully specify every bit in every \nsingle header .eld. Exact-match rules, for instance, do not use wildcard patterns that match many values \nfor a single header .eld, such as 10.0.0.0/8, nor do they leave certain header .elds completely unconstrained. \nOur .rst implementation of Frenetic [9] used exact-match rules exclusively because such rules were far \neasier for its run-time system to reason about, particularly when it came to composing multiple user \npolicies. This paper presents new, general-purpose algorithms for syn\u00adthesizing low-level switch classi.ers \nthat use wildcard rules to the extent possible. These new algorithms result in far more ef.cient system \nthan the one we built in earlier work: in Frenetic s origi\u00adnal exact-match architecture, many more packets \nwound up being sent to the controller (suffering orders of magnitude increase in la\u00adtency) and many more \nrules had to be sent to switches. The results of our experiments, presented in Section 7, highlight the \nmagnitude of these differences. NetCore: Richer predicates and dynamic policies. The policies presented \nin the previous section were relatively simple they did nothing besides match bits in header .elds and \nforward packets ac\u00adcordingly. Such static policies can be expressed in Frenetic s sim\u00adple policy language, \nthough they are not be implemented nearly as ef.ciently as in the NetCore system. However, many applications \ndemand dynamic policies whose forwarding behavior depends on complex functions of traf.c history and \nother information. And these richer policies cannot be implemented by simply analyzing bits in header \n.elds. As anexample, suppose wewant tobuilda security application that implements in-network authentication \nfor the topology shown in Figure 1. The network N1 contains a collection of internal hosts, N2 represents \nthe upstream connection to the Internet, A is the server that handles authentication for hosts in N1 \n, and all three el\u00adements are connected to each other by the switch S. Informally, we want the network \nto perform routing and access control accord\u00ading to the following policy:Forward packets from unauthenticated \nhosts in N1 to A, from authenticated hosts in N1 to their intended destination in N2 , and from A and \nN2 back to N1 (although not from N2 to A). This policy can be described succinctly in NetCore as follows. \n (InPort:Network 1 n inspect ps auth . {Network 2}) . (InPort:Network 1 n\u00ac(inspect ps auth) . {Server \nA}) . (InPort:Server A . InPort:Network 2 . {Network 1}) where ps = InPort:Server A auth (S,s,p) = any \n(isAddr p) S isAddr p (_,p ) = p.SrcAddr == p .DstAddr This policy uses an inspector predicate to classify \ntraf.c from N1 as authenticated or unauthenticated. An inspector predicate inspect ef has two arguments: \na .lter predicate e over the net\u00adwork traf.c history and an (almost) arbitrary boolean-valued func\u00adtion \nf. The .lter predicate generates a controller state S, which is a collection of traf.c statistics, represented \nabstractly as a multiset of switch-packet pairs (the switch being the place where the packet was processed). \nThe boolean-valued function f receives the con\u00adtroller state as one its arguments and may analyze it \nas part of its decision-making process. In the example above, the .lter predicate ps selects all traf.c \ncoming from the authentication server. In this idealized example, we will treat an entity sending a packet \np as authenticated if the authentication server has ever sent it a packet at any point in the past. The \nfunction auth takes three arguments: the controller state S, the switch s that should be handling the \npacket, and the packet p to which the policy applies. Here, the auth function tests whether the SrcAddr \n.eld of the packet p being processed is equal to the DstAddr of any other packet p in the .ltered traf.c \nhistory (and because there is only one switch in this example, auth ignores its s argument). In other \nwords, it tests whether the authentication server has sent a packet to that sender in the past. If it \nhas, the inspector predicate is satis.ed; if not, it is not satis.ed. The auth function performs these \ntests using the auxiliary functions any,a built-in function that tests whether a boolean function is \ntrue of anyelement of a multiset, and isAddr, a user-de.ned function that tests whether one packet s \nDstAddr is equal to another packet s SrcAddr. This inspector is combined with the other set-theoretic \noperators to implement the overall packet-forwarding policy. Policies that use inspectors are easy to \nwrite because forwarding decisions can be expressed using arbitrary functional programs. These programs \ncan query past traf.c history or look up facts they need such as authentication status in a database. \nOn the other hand, these programs never have to manage the low-level details of generating or installing \nswitch-level rules the run-time system does that tedious, error-prone work for the programmer. Of course, \nthis expressiveness presents an extreme challenge for the compiler and run-time system. While it would \nbe easy to evaluate the results of such policies by sending all packets to the controller, doing so wouldbe \ntotally impractical.We must.ndawayto implementthe policywhile processing the majority of traf.c on switches. \nOur implementation strategy for such policies proceeds as fol\u00adlows. First, we compile the parts of the \npolicy that do not involve inspectors as effectively as we can: The system generates normal forwarding \nrules when it can, and rules that send packets to the con\u00adtroller otherwise. Next, whenever a packet \nthat cannot be handled by a switch arrives at the controller, the run-time system evaluates the packet \nagainst the current policy, which includes inspectors, producing a set of forwarding actions. There are \ntwo possibilities: 1. The policy with respect to this packet (and similar ones) is invariant. In other \nwords, every subsequent time the system evaluates the policy against this packet, it will return the \nsame set of forwarding actions. 2. The policy with respect to this packet (and similar ones) is volatile. \nIn other words, the set of forwarding actions to be applied to this policymay change in the future. \n In the .rst case, the system can install rules on the switch that as\u00adsociate packets similar to the \none just processed with the set of forwarding actions just computed. Because the set of computed ac\u00adtions \nwill never change, installing such rules on switches preserves the semantics of the policy. In the second \ncase, the system can not install rules on the switch the next packet might be forwarded differently, \nso the system will have to reevaluate it on the con\u00adtroller. In our example, once a host has been authenticated, \nit stays authenticated once the auth function evaluates to true it will con\u00adtinue to do so, and is therefore \ninvariant. Since inferring invariance automatically from an arbitrary program is a dif.cult problem, \nwe currently ask NetCore programmers to supply invariance informa\u00adtion to the compiler in the form of \nan auxiliary hand-written func\u00adtion. In this simple case, writing the invariance function auth_inv is \ntrivial it is true whenever auth is true: auth_inv (S,s,p) = auth (S,s,p) To effectively generate rules, \neven in the presence of inspector predicates, the run-time system must be able to determine when inspector \nreturns the same results on one packet as it does on another i.e., it must be able to calculate the similar \npackets re\u00adferred to above. Observe that an inspector always returns the same results on two different \npackets if those packets agree on all header .elds that the inspector function examines. Conversely, \nif the in\u00adspector does not examine a particular header .eld, the value of that .eld does not affect its \nresult. Hence, when generating a policy after evaluating it against a single packet, the run-time can \nsubsti\u00adtute wildcards for all header .elds that the policy does not inspect. Though it is likely possible \nto infer the set of headers anyinspector function examines (at least conservatively), our current implemen\u00adtation \nassumes that programmers supply this information explicitly. Overall, these techniques (1) run-time evaluation \nof policies against particular packets on the controller, (2) invariance, and (3) speci.cation of header \ninformation collaborate to turn the dif.\u00adcult problem of evaluating policies containing arbitrary functions \nback in to the simpler problem of compiling static forwarding poli\u00adcies ef.ciently.We call these techniques \nreactive specialization. 3. A Core Calculusfor Network Programming This section de.nes the syntax and \nsemantics of NetCore, a core calculus for high-level network programming. The calculus has two major \ncomponents: predicates, which describe sets of packets, and policies, which specify where to forward \nthose packets. Figure 2 presents the syntax of these constructs as well as various network values such \nas headers and packets. Notation. Throughout this paper, whenever we de.ne syntax, as in the grammar \nfor packets, we will use the grammar non-terminal (p) as a metavariable ranging over the objects being \nde.ned, the capitalizedversionof the non-terminal(P)as a metavariable rang\u00ading over sets or multisets \nof such objects, and vector notation(p.) for sequences of objects. We describe .nite sets using the notation \n{x1 ,...,xk } and combine sets using operations ., n, \u00ac, and \\ (union, intersection, complement, and \ndifference respectively). Typically, we give de.\u00adnitions for intersection and complement and leave union(S1 \n. S2 = \u00ac(\u00acS1 n\u00acS2 )) and difference (S1 \\ S2 = S1 n\u00acS2 ) as de\u00adrived forms. We also overload \u00ac and use \nit to negate booleans; its meaning will be clear from context. We write multisets us\u00ading the notation \n{1x1 ,...,xn1} and combine multisets using mul\u00adtiset union M1 . M2 . We write .nite maps using the notation \n{x1 . y1 ,...,xn . yn} and lookup elements of a .nite map m using function application m(xi). Network \nvalues. For simplicity, we only model a single kind of network entity to forward to, switches s. Packets \np are the basic NetworkValues Switch s Header h Switch Set S ::= {s1 ,...,sn} Header Set H ::= {h1 ,...,hn} \nBit b ::= 1 1 0 Packet p ::= {h1 ..b1 ,...,hn ..bn} State S ::= {1(s1 ,p1 ),..., (sn,pn)1} Language Snapshot \nx ::= (S,s,p) Wildcard w ::= 1 1 0 1 ? Inspector f . State[H1 ]\u00d7 Switch \u00d7 Packet[H2 ]. Bool Predicate \ne ::= h : w.Iswitch s Iinspect ef Ie1 n e2 I\u00ace Policy t ::= e . S It1 n t2 I\u00act Figure 2. NetCore syntax. \n[e] ={x1 ,...,xk } [h : w.] ={(S,s,p)Ip(h)matches w.} '' switch s ={(S,s,p)Is = s} ' [inspect ef] ={(S,s,p)If(S \n,s,p)} '' ' where S ' ={I(s ,p ' )I(s ,p ' ). Sand (S,s ,p ' ). [e] I}[e1 n e2 ] = [e1 ] n [e2 ] [\u00ace] \n=\u00ac [e] [t] (x)= S if x . [e] {S [e . S] (x)= \u00d8 otherwise [t1 n t2 ] (x)= [t1 ] (x)n [t2 ] (x)[\u00act] (x)=\u00ac \n[t] (x) Figure 3. NetCore semantics. values processed by programs, which we represent as a .nite map \nfrom headers h to bitstrings .b. We write p(h)for the bitstring associated with the header h in p. We \nassume all .elds have .xed, .nite length and therefore the set of complete packets is .nite. The controller \nstate (S) accumulates information about packets that arrive at each switch. We represent controller state \nas a multiset of switch-packet pairs. Predicates. Informally, predicates select sets of packets that \nare of interest in some forwarding policy. Formally, a predicate e de\u00adnotes a set of snapshots x comprising \na controller state S, a switch s, and a packet p located at s. The state component is essential for modeling \npredicates that depend upon historical traf.c patterns, such as the past load on particular links or \npackets sent and re\u00adceived from various locations. Figure 3 de.nes the semantics of predicates.We say \nthata snapshot x matches a predicate e when it belongs to the denotation of e. We sometimes say that \na packet p matches e, leaving the state and the switch implicit because they are . irrelevant or uninteresting.We \nalso say thata bitstring b matches a wildcard w.whenever the corresponding bits match. For example, 1111 \nand 0011 both match the wildcard ??11. Basic predicates have the form h : w..A packet pmatches h : w.if \np(h),(i.e., the h header of p) matches w.. For example, the predicate DstPort:1010000 matches all packets \nwith DstPort header .eld equal to 80 (as 1010000 is 80 in binary). Another basic predicate, switch s, \nmatches all packets (in any state) sent to s. More complex predicates are built up from simpler ones \nusing the intersection and complement operators. Additionalbuilding blocks such as True, False, e1 . \ne2 , or e1 \\ e2 can be implemented as derived forms. The most interesting component of the language is \nthe inspector predicate, inspect ef. The .rst component of an inspector is a .lter predicate e that selects \nswitch-packet pairs matching e from the current state S, creating a re.ned state S ' . In other words, \ne acts as a query over the network traf.c history. The second component, f, is an (almost) arbitrary \nBoolean-valued function over S ' and the switch-packet pair (s and p) in question. The authentication \nexample de.ned in the previous section used an inspector. Another example is(inspect filterWeb cond)where \nfilterWeb = DstPort:1010000 cond (S,s,p) = cardinality S < 10000 || p.SrcAddr == 10.0.0.1 Here, cardinality \nis a function that counts the number of ele\u00admentsina multiset. This inspectorextracts all web traf.c(DstPort \nis 1010000)from the current state. The inspector is satis.ed if the total number web packets sent is \nless than 10000 or the packet p comes froma particular sender(SrcAddr is 10.0.0.1). To make compilation \ntractable, two additional pieces of infor\u00admation are associated with inspector functions f. The .rst \npiece of information comes from the sets of headers mentioned in its in\u00addexed type, State[H1 ]\u00d7 Switch \n\u00d7 Packet[H2 ]. Bool . Such a type restricts f to only examine headers H1 of packets in the state and \nheaders H2 in the packet. For instance, the function cond above may be assigned a type wherein H1 is \nthe empty set (as summing packet counts requires looking at no headers) and H2 is {SrcAddr}(as cond only \nexamines the SrcAddr .eld of its packet argument). The second piece of information associated with f \ncomes from its invariance oracle. A function f is invariant on(S,s,p), written invariant ((S,s,p),f), \nif for all S ' , we have ' f(S. S ,s,p)= f(S,s,p). Intuitively, a function is invariant on a state when \nits result does not change, no matter what additional information is added to it. Again, as an example, \nthe cond function above is invariant on all snapshots involving packets from 10.0.0.1 as well as all \nsnapshots where cardinality S = 10000 once the total volume of web traf.c has crossed the threshold, \nthe function always returns true. In our implementation, the programmer writes invariance oracles by \nhand as simple Haskell functions. Together, the header sets in the inspector types, and the invari\u00adance \noracle, allow the compiler to generate effective switch-level rules even though the inspector function \nitself cannot be analyzed. However, the language of predicates does have one signi.cant lim\u00aditation: \nit depends upon permanent invariance of predicates. There are predicates that are invariant for a long \ntime, and hence could have rules installed on switches for that time, but are not perma\u00adnentlyinvariant.We \nbelieve our framework canbeextendedto han\u00addle such semi-permanent invariance properties, having the compiler \nuninstall rules at the end of a time period, or in response to a net\u00adworkevent,but deferaninvestigationof \nthis topicto futurework. Synchronous Machine Msync ::=(t,S,T)Asynchronous Machine Masync ::=(t,S,T1 \n,T2 ) o ' Msync .. Msync ' [t] (S,s,p)= S forward(S,p)= T s,p (t,S,T .{IT (s Ip)I}).. (t,S.{I(s,p)I},T \n. T ' ) o ' Masync .. Masync ' [t] (S,s,p)= S forward(S,p)= T ' '' T1 = T1 . TT2 = T2 .{IT (s Ip)I} s,p \n'' (t,S,T1 .{IT (s Ip)I},T2 ).. (t,S,T 1 ,T 2 ) (t,S,T1 ,T2 .{IT (s Ip)I}).(t,S.{I(s,p)I},T1 ,T2 ) Figure \n4. Reference machines. Policies. Policies t specify how packets should be forwarded through the network.Basic \npolicies, written e . S, say that pack\u00adets matching e should be forwarded to the switches in S. As with \npredicates, we build complex policies by combining simple poli\u00adcies using intersection and negation. \nFigure3de.nes the semantics of policies as a function from snapshots x to sets of switches S. Al\u00adthough \nthe policylanguage is syntactically simple, it is remarkably expressive. In particular, inspectors are \na powerful tool that can be used to express a wide range of behaviors including load balancing, .ne-grained \naccess control, and manystandard routing policies. Machines. To understand how the network behaves over \ntime, we de.ne two abstract machines. Both machines forward pack\u00adets according to t but they differ in \nhow often the switches syn\u00adchronize traf.c statistics with the controller. The synchronous ma\u00adchine de.nes \nan idealized implementation that, at all times, has perfect information about the traf.c sent over the \nnetwork. Of course, it would be impractical to implement this machine in a real network because, in general, \nit would require sending ev\u00adery packet to the controller if any packet were forwarded by a switch there \nwould be a delay between when the packet was for\u00adwarded and when the controller state was augmented with \ninfor\u00admation about that packet. The asynchronous machine de.nes a looser, more practical, implementation. \nLike the .rst machine, it is policy-compliant it forwards packets according to the policy but it updates \nits state asynchronously instead of in lockstep with each packet processed. Hence, it makes no guarantees \nabout what it knows about the network s traf.c. While the synchronous ma\u00adchine can be thought of as the \nbest possible policy-compliant ma\u00adchine, the asynchronous machine can be thought of as the worst policy-compliant \nmachine. Anyreasonable implementation will sit between the two. In other words, implementations should \nbe policy compliant,but users should notexpect perfect synchrony the cost of implementing it would be \nprohibitive. In practice, synchroniza\u00adtion with switches typically happens at periodic, timed intervals \n(modulo variances in the latency of communication) but for sim\u00adplicity, we do not model time explicitly. \nFigure4de.nes both reference machines. They use the function forward(S,p), which generates a multiset \nof transmissions, forward(S,p)={IT (s Ip)Is . SI}. The stateof the synchronous machine(Msync)includes \nthe NetCore policy t, the state S, and a multiset T of pending transmissions. At each step, the machine \nremoves a transmission from T, processes it using the policy, updates the machine state, and adds the \nnew transmissions generated by the policy to the multiset of pending transmissions. The state of the \nasynchronous machine (Masync) includes the program t, state S, and two multisets of transmissions: T1 \n, which represents transmissions waiting to be processed by the policy, and T2 , which represents transmissions \nthat have been processed by the policy but have not yet been added to the state. The .rst inference rule \nfor the second machine takes a transmission from T1 , processes it using the policy, and places it in \nT2 , the set of transmissions waiting to be incorporated into S;the second rule takes a transmission \nfrom T2 and adds it to S. 4. The Run-time System In this section we discuss how to implement NetCore \ns semantics on a software-de.ned network by giving an operational semantics to the NetCore run-time system \nand the underlying network de\u00advices. This operational semantics explains the basic interactions between \nthe controller and the switches. Switch classi.ers. Before we can present the run-time system, we need \na concrete representation of the rules that switches use to process packets. A classi.er r.is a sequence \nof rules r, each containing a switch-level pattern z and an action a. While our high\u00adlevel semantics \nuses sets, classi.ers are represented as sequences to model rule priority: within a classi.er, rules \non the left have higher priority than rules on the right. The pattern (z)component of a rule recognizes \na set of packets, and hence is similar to (but less general than) a predicate in Net-Core. We write p \n. z when packet m matches pattern z. We hold patterns abstract to model the variety of different matching \ncapabil\u00aditiesin today s switches.Forexample, OpenFlow switches support pre.x pattern matching on source \nand destination IP addresses(i.e., patterns like 0110*)but only exact or full unconstrained matching \non most other headers. Some switches support various other ex\u00adtended patterns, such as ranges of the \nform [n1 ,n2 ]. An action ais either a set of switches S, which forwards packets to each switch in the \nset, or O, which forwards packets to the controller. Most switches support other actions such as modifying \nheader .elds,but for simplicity we only model forwarding. Given a packet pand a classi.er r., we match \nthe packet against the classi.er by .nding the .rst rule whose pattern matches the packet. We write r..p \nz : a for the matching judgment. More formally, we de.ne classi.er matching as follows; note that it \nselects the highest priority (leftmost) matching rule: p. z1 . p. zi-1 p. zi (z1 : a1 ,...,zi-1 : ai-1 \n,zi : ai ,...,zn : an).p zi : ai Molecular machine. We formalize the operational semantics of the run-time \nsystem as a molecular machine, in the style of the chemical abstract machine [2]. The machine s components, \ncalled molecules,aregivenontheleftsideofFigure5.For simplicity,we assume that packets arriving at a switch \nmay be processed in any order and do not modelfailures that cause packets to be dropped. The molecule \nC (t IS)represents the controller machine run\u00adning the NetCore policy t in state S. The molecule S (s \nIr.IZ)represents switch s with packet classi.er r.and local switch state Z. The switch state records \nthe patterns of rules that have been used to match packetsbut not yet queried and processedby the controller. \nReal switches use integer counters as state; for simplicity, we rep\u00adresent these countersin unary usinga \nmultisetof patterns.A trans\u00admission molecule T (s Ip)represents a packet pen route to switch M .o . \nM ' Pattern z Switch Action a ::= Rule r ::= Classi.er r.::= Switch State Z ::= Molecule m ::= Machine \nM ::= Observation o ::= E-SWITCHPROCESS r .p ' .z : S forward(S,p)= T s,p ' S (s Ir.IZ),T (s Ip)... \nS (s Ir.IZ.{IzI}),T S IO E-SWITCHHELP z : a r..p z : O (r1 ,...,rn) S (s Ir.IZ),T (s Ip). S (s Ir.IZ),H \n(s Ip) {Iz1 ,...,znI} E-CONTROLLER '' C (t IS)I [t] (S,s,p)= S forward(S,p)= T Specialize((S,s,p),t)= \nr. S (s Ir.IZ)I s,p '' C (t IS),S (s Ir.IZ),H (s Ip)... C (t IS.{I(s,p)I}),S (s I(.r ,r.)IZ),T T (s \nIp)I H (s Ip) E-COLLECT {Im1 ,...,mnI} r..p z : a \u00b7Is,p C (t IS),S (s Ir.IZ.{IzI}). C (t IS.{I(s,p)I}),S \n(s Ir.IZ) E-STEP ' M .o . M o (M. M '' )..(M ' . M '' ) Figure 5. The run-time system. s. Finally, a \nhelp molecule H (s Ip)represents a request issued by switch s to the controller for assistance in processing \npacket p. The operational semantics of the molecular machine is de.ned by the inference rules on the \nright side of Figure 5. To lighten the notation in this .gure, we drop the multiset braces when writing \na collection of molecules. In other words, we write m1 ,m2 ,... instead of {Im1 ,m2 ,... I}. Each operational \nrule may optionally be labelled with an observation o, which records when transmissions are processed.We \nuse observations in Section6where we establish equivalences between the molecular machine and the reference \nmachines de.ned in the last section. The rules E-SWITCHPROCESS and E-SWITCHHELP model the work done by \nswitches to process packets. The former rule is invoked when a packet matches a rule with a non-controller \naction (a set of switches to forward to). In this case, the switch forwards the packet accordingly and \nrecords the rule pattern in its state. The latter rule is invoked when a packet matches a rule with a \ncontroller action. In this case, the switch generates a help molecule. The rule E-CONTROLLER models the \nwork done by the con\u00adtroller to process help molecules. The controller interprets the packet using its \nNetCore policy, generating new transmissions T ' to be sent into the network, and adds the packet to \nits state. In ad\u00addition, the controller uses the NetCore compiler to generate new rules to process future, \nsimilar packets on switches, instead of on the controller. The compiler is accessed through the call \nto the Specialize function, which generates the new rules for the switch in question.We hold the de.nitionof \nthis function abstract for now; it is de.ned precisely in the next section. The rule E-COLLECT models \nthe work done by the controller to transfer information about the packets that matched a particular switch-level \nrule from the switch to the controller state. More pre\u00adcisely, it chooses a pattern z from a switch state \nand then uses the lookup judgement r..p z : a to synthesize a packet p that might have matched the corresponding \nrule in the switch classi.er. The pair of the packet and the switch are then stored in the controller \nstate as a past transmission that might have occurred. The interesting part of this transfer is that \nthe controller stores full packets whereas switches only store sets of patterns and a pattern only speci.es \npart of a packet perhaps its IP address or VLAN tag, but not the packet itself. Hence the transfer operation \nmust fabricate those parts of the packets that are not speci.ed in the switch pattern, and the system \nas a whole must be correct no matter how the under-speci.ed parts of a packet are fabricated. This places \nan important constraint on the compiler: If the rules and their patterns are not speci.c enough then \nalthough one packet may have matched a rule on a switch, a completely different packet may befabricated \nand passed back to the controller. Consequently, the controller state will not model past network traf.c \nsuf.ciently accurately and forwarding policies that depend upon past network traf.c will not be implemented \ncorrectly. A second subtle issue with the E-COLLECT rule is that the pat\u00adterns of higher-priority rules \npartially overlap and take precedence over patterns from lower-priority rules. Hence, examining the pat\u00adtern \nof a low-priority rule in isolation does not provide suf.cient information to synthesize a packet that \nmight have matched that rule. One must take all of the rules of the classi.er, and their prior\u00adity order, \nin to account when synthesizing a packet that may have matched a pattern. The E-C OLLECT rule does this \nthrough the use of the full classi.er matching judgement. Finally, note that the implementation does \nnot actuallyfabricate all of these packets in practice, the switch passes integer coun\u00adters associated \nwith patterns back to the controller. Still, this non\u00addeterministic rule effectively captures a key correctness \ncriterion for the system: The controller program cannot distinguish between any of the packets that might \nbe synthesized by the E-COLLECT rule and must be correct no matter which one is fabricated. Of course, \nthis is also where the compiler s use of header information comes in to play: the fabricated packets \nare only different in .elds that inspector functions (and other predicates) do not analyze. 5. The NetCore \nAlgorithms The NetCore system performs two distinct tasks: Classi.er Generation: given a NetCore policy, \nconstruct a set of classi.ers, one for each switch in the network.  Primitive intermediate form u ::=(h1 \n: w.1 ).....(hn : w.n)Three-valued boolean b ::= True IMaybe IFalse Pattern intermediate form p ::=.u \n: z : b : H. Policyintermediate form . ::=.u : z : S1 ,S2 : H. I(s,e)= p. I(s,h : w.)=.(h : w.): O (h \n: w.): True : \u00d8., .. : . : False : \u00d8. {.. : . : True : \u00d8. if s = s I(s,switch s ' )= ' ' .. : . : False \n: \u00d8. if s . s I(s,inspect ef)=..ui : zi : Maybe :(Hi . H). i where f : State[H]\u00d7 Switch \u00d7 Packet[H ' \n]. Bool and (I(s,e))=.ui : zi : bi : Hi. i ''' ' I(s,e n e ' )=...ui . uj : zi . zj : bi . bj : Hi . \nHj . ij where (I(s,e))i =.ui : zi : bi : Hi. ''' ' and (I(s,e ' ))=.uj : zj : bj : Hj . j I(s,\u00ace)=..ui \n: zi :\u00acbi : Hi. i where (I(s,e))=.ui : zi : bi : Hi. i I(s,t)= ... .ui : zi : S,S : Hi. if bi = True \nI(s,e . S)= ... .ui : zi :\u00d8,S : Hi. if bi = Maybe i . . .ui : zi :\u00d8,\u00d8: Hi. if bi = False where (I(s,e))=.ui \n: zi : bi : Hi. ' ''' ' I(s,t n t ' )=...ui . uji : zi . zj : S1 ,S2 : Hi . Hj . ij where (I(s,t))=.ui \n: zi : S1i ,S2i : Hi . i '''' ' and (I(s,t ' ))=.uj : zj : S1j ,S2j : Hj . j and S1 ' = S1i n S1' j and \nS2 ' = S2i n S2' j I(s,\u00act)=..ui : zi :\u00acS2i ,\u00acS1i : Hi. i where (I(s,t))i =.ui : zi : S1i ,S2i : Hi . \nC(s,t)= r. .zi : S1i if S1i = S2i and Hi . headers(zi) C(s,t)= ... and consistent(..,i)i . . zi : O otherwise \nwhere I(s,t)= .. and (..)i =.ui : zi : S1i,S2i : Hi . and consistent(..,i)= .p..j....pz .ui : zi : S1i,S2i \n: Hi.. . .p .u .uj : zj : S1i ,S2i : Hj . and headers(z)={h Ip1 . z . p2 . z . p1 (h)= p2 (h)} Figure \n6. NetCore classi.er generation. Reactive Specialization: given a packet not handled by the current classi.er \ninstalled on a switch, generate additional rules that allow the switch to handle future packets with \nsimilar header .elds without consulting the controller. This section presents thekeyalgorithms that \nimplement these tasks. 5.1 Parameters The NetCore system is parameterized on several structures: a lattice \nof switch patterns, and two oracles that map primitive predicates onto switch-level and wildcard patterns \nrespectively. Abstracting some of the low-level details of compilation makes it possible to execute NetCore \npolicies on many diverse kinds of hardware and even use switches with different capabilities in the same \nnetwork. Formally, we assume that switch patterns form a bounded lattice. A pattern z1 sits lower than \n(or equal to) another pattern z2 , writ\u00adten z1 . z2 , when z1 matches a subset of the packets matched \nby z2 . The . element matches every packet and the 1 element matches none. Abusing notation slightly, \nwe write p . z to indicate that packet p matches pattern z. To ensure that intersections are com\u00adpiled \ncorrectly, we require that meets be exact, in the sense that p. z . z ' if and only if p. z and p. z \n' . The .rst oracle, called the compilation oracle O, maps primi\u00adtives h : w.into the pattern lattice. \nIn manycases, the pattern gener\u00adated by O(h : w.)will match the set of packets described by h : w.exactly, \nbut sometimes this is not possible. For example, Open-Flow switches only support pre.x wildcards for \nIP addresses, so the best approximation of the non-pre.x pattern SrcAddr : 1?1? is SrcAddr : 1???.We \ngive the oracle some .exibility in selecting pat\u00adterns and only require it to satisfy two conditions: \n(1) it must return an overapproximation of the primitive and (2) it must be mono\u00adtonic, in the sense \nit translates (semantically) larger primitives to larger patterns. Formally, the requirements on compilation \noracles are as follows: (1) (S,s,p). [h : w] implies p . O(h : w)and ' (2) [h : w] . [h ' : w ] implies \nO(h : w). O(h ' : w ' ). The second oracle, called the re.nement oracle U, takes a prim\u00aditive h : w.and \na packet p as arguments and produces a pattern ' h : w.. Unlike the compilation oracle, which overapproximates \npredicates, the re.nement oracle underapproximates predicates, al\u00adlowing the compilation infrastructure \nto generate effective switch\u00adlevel rules for a subset of the pattern of interest. While there is gen\u00aderally \none best overapproximation, there often exist many useful underapproximations; in such cases, we disambiguate \nby selecting the best underapproximation that matches p. For example, if we were to re.ne SrcAddr : 1?1?(which \ncan t be compiled exactly on OpenFlow) with a packet with source address 1111, we would gen\u00aderate the \nunderapproximation SrcAddr : 111?, yet if we re.ned the same predicate with a packet with source address \n1010, we would instead generate SrcAddr : 101?. 5.2 Classi.er Generation Ideally, given a policy, the \nNetCore compiler would generate a classi.er with the same semantics i.e., one that denotes the same function \non packets. But certain NetCore features, such as inspec\u00adtors and wildcard patterns (when not supported \nby the underlying hardware), cannot be implemented on switches. So in general, the generated classi.er \nwill only approximate the policy, and certain packets will have to be processed on the controller. The \nclassi.er generatorworksintwo phases.Inthe .rst phase, it translates high-level policies to an intermediate \nform containing switch-level patterns and actions, as well as precise semantic in\u00adformation about the \npolicy being compiled. In the second phase, it builds a classi.er by attaching actions to patterns, using \nthe seman\u00adtic information produced in the .rst phase to determine whether it is safe to attach forwarding \nactions to a pattern, or whether the special controller action O must be used instead. The grammars \nat the top of Figure 6 de.ne the syntax for the intermediate forms used in classi.er generation. The \nintermediate form for predicates .u : z : b : H.contains four values: an ideal pattern u; a switch pattern \nz; a three-valued boolean b; and a set of headers H. The ideal pattern u represents the pattern we would \ngenerate if the pattern lattice supported arbitrary wildcards. Ideal patterns are represented as a conjunction \nof header and wild\u00adcard pairs. We write . for the unconstrained ideal pattern i.e., an empty conjunction. \nThe switch pattern z represents the actual pattern generated by the compiler, which is an overapproximation \nof the ideal pattern in general. The three-valued boolean b indi\u00adcates whether packets matching the predicate \nshould de.nitely be accepted(True), rejected(False), or whether there is insuf.cient information and \na de.nitive answer must be made by the controller (Maybe). To combine three-valued booleans, we extend \nthe stan\u00addard boolean operators as follows: Maybe . False = False Maybe . True = Maybe Maybe . Maybe \n= Maybe \u00acMaybe = Maybe The set of headers H keeps track of the header .elds (within pack\u00adets in the controller \nstate) that inspector functions may examine. This header information is used to ensure the compiler generates \nsuf.ciently .ne-grained switch rules so that when information is transferred from the switch to the controller \n(using the E-C OLLECT rule discussed in the previous section), the information is precise enough to guarantee \nthe correctness of the inspectors. The intermediate form for policies .u : z : S1 ,S2 : H.is similar \nto the form for predicates, but instead of a three-valued boolean, it records lower and upper bounds \n(S1 and S2 ) on the sets of switches to which a packet might be forwarded. Intuitively, a proper forwarding \nrule can only be generated when we know exactly which switches to forward packets to (i.e., when S1 and \nS2 are equal). In other cases, the compiler will generate a rule that sends packets to the controller. \nPredicate translation. The heart of classi.er generation is the function I(s,e), presented in Figure \n6, which takes a predicate e and switch s as arguments and produces a sequence of interme\u00addiate predicates \nthat approximate e on s. One of the invariants of the algorithm is that it always generates a complete \nsequence i.e., intermediate predicates whose patterns collectively match ev\u00adery packet. In addition, \nthe algorithm attempts to produce a se\u00adquence whose patterns separate packets into two sets one with \npackets that match the predicate being compiled and another with those that do not. However, it does \nnot always succeed in doing so, for two fundamental reasons: (1) the algorithm cannot analyze the decisions \nmade by inspectors as far as the analysis is con\u00adcerned, inspectors are black boxes, and (2) certain \nprimitive predi\u00adcates cannot be expressed precisely using switch patterns. The in\u00adtermediate predicates \ncontain suf.cient information for the com\u00adpiler to reason about the precision of the rules it generates. \nWe write (I(s,e))=.ui : zi : bi : Hi .to indicate that compiling e i returns a sequence of intermediate \npredicates whose ith element is .ui : zi : bi : Hi., and ..ui : zi : bi : Hi. i to denote the sequence \nof intermediate predicates out of compo\u00adnents .ui : zi : bi : Hi .indexed by i. The .rst equation at \nthe top of Figure6 states that the compiler translates primitive predicates h : w.into two intermediate \npred\u00adicates: .(h : w.): O (h : w.): True : \u00d8., which contains the switch pattern produced by the compilation \noracle, and .. : . : False : \u00d8., which, by using the . pattern, ensures that the sequence is com\u00adplete. \nLike classi.ers, these sequences should be interpreted as a prioritized series of rules. Hence the second \nquadruple only rejects things the .rst does not match. The case for switch predicates switch s ' has \ntwo possible out\u00adcomes: If the switch s whose classi.er is being compiled is the same as s ' , then the \ncompiler generates an intermediate form that associates every packet with True. Otherwise, the compiler \ngener\u00adates an intermediate form that associates every packet with False. Intuitively, the classi.er generated \nfor inspect ef must satisfy three conditions. First, it should approximate the semantics of f. Because \nthe behavior of f is unknown at compile time, the approx\u00adimation cannot be exact. Hence, the intermediate \npredicates gener\u00adated for the inspector should contain Maybe, indicating that match\u00ading packets should \nbe sent to the controller for processing. Second, it should be structured so that it can identify packets \nmatched by the traf.c .lter predicate e i.e., the packets that must be present in the controller state \nto evaluate f. Third, it should also be suf.\u00adciently .ne-grained to provide information about the set \nof headers H mentioned in the type of f, which represent the headers of pack\u00adets in the state that f \nexamines.2 Hence, the compiler recursively generates a sequence of intermediate predicates from e, and \nthen it\u00aderates through it, replacing the three-valued boolean bi with Maybe and adding H to the set of \nheaders Hi in each .ui : bi : zi : Hi.. To generate intermediate forms for an intersection (e1 n e2 ), \nthe compiler combines each pair of intermediate predicates gener\u00adated for e1 and e2 . The resulting classi.er \ncaptures intersection in the following sense: if a packet matches zi in the .rst intermediate form and \nmatches z ' in the second intermediate form, it matches j the form with pattern zi .zj ' in the result, \nand likewise for ui and uj ' . Performing this construction naively would result in a combinato\u00adrial \nblowup. However, it is often possible to exploit algebraic prop\u00aderties of patterns to reduce the size \nof the sequence in practice see the examples below and also Section 7. Finally, the case for negated \npredicates: \u00ace iterates through the sequence generated by the compiler for e and negates the three\u00advalued \nboolean in each intermediate predicate. Predicate translation examples. To illustrate some of the details \nof predicate compilation, consider the translation of the inspector\u00adfree predicate (e1 n e2 )where e1 \nis (h1 : 0?)and e2 is (h2 : 11)and h1 and h2 are distinct headers. Assume that switch patterns support \nwildcards such as 0? on h1 . The left and right sides of the intersection generate the following intermediate \npredicates: I(s,e1 )=.(h1 : 0?):(h1 : 0?): True : \u00d8.,.. : . : False : \u00d8. I(s,e2 )=.(h2 : 11):(h2 : 11): \nFalse : \u00d8.,.. : . : True : \u00d8. Note that the negation in e2 .ips the parts of the intermediate forms \ndesignated as True i.e., it inverts the parts of the sequence that match and do not match the predicate. \nNext, consider compilation of the intersection, and note that we simplify the results slightly using \nidentities such as z .. = z and u .. = u and b. True = b. .(h1 : 0?. h2 : 11):(h1 : 0?. h2 : 11): True \n: \u00d8., .(h1 : 0?):(h1 : 0?): False : \u00d8., .(h2 : 11):(h2 : 11): False : \u00d8., .. : . : False : \u00d8. This classi.er \ncan then be simpli.ed further, as the last three rules overlap and are associated with the same three-valued \nboolean: .(h1 : 0?. h2 : 11):(h1 : 0?. h2 : 11): True : \u00d8., .. : . : False : \u00d8. Now suppose instead \nthat the switch only has limited support for wildcards and cannot represent h1 : 0?. In this case, the \ncompilation ' 2Note that the compiler does not add the setH mentioned in the type of f to Hi. This set \nis used during specialization to determine similar packets. oracle provides an overapproximation of \nthe pattern, say, .. Hence, the intermediate predicate for e1 above would be as follows: I(s,e1 )=.(h1 \n: 0?):.: True : \u00d8.,.. : . : False : \u00d8. For another example, consider compiling a predicate that in\u00adcludes \nan inspector such as (inspect (h1 : 00)f)n(h2 : 11). In this case, (h1 : 00)compiles similarly to the \nsimple clauses above: .(h1 : 00):(h1 : 00): True : \u00d8.,.. : . : False : \u00d8. If the set of headers f examines \non the state is H, the inspector inspect (h1 : 00)f compiles to the following: .(h1 : 00):(h1 : 00): \nMaybe : H.,.. : . : Maybe : H. Note that the de.nitive booleans above have been replaced with Maybe, \nindicating that the controller will need to determine whether packets match the predicate. However, when \nwe inter\u00adsect the results of compiling h2 : 11 with the results of compiling the inspector, we obtain \nthe following: .(h1 : 00. h2 : 11):(h1 : 00. h2 : 11): Maybe : H., .(h2 : 11):(h2 : 11): Maybe : H., \n.(h1 : 00):(h1 : 00): False : H., .. : . : False : H. Importantly, even though the inspector is uncertain \n(i.e., it has Maybe in each intermediate predicate), the result is not entirely un-certain.Because b.False \nis False even when bis Maybe, intersect\u00ading inspectors with de.nitive predicates can resolve uncertainty. \nLikewise, as b. True is True, compiling the union of a de.nitive clause with an inspector also eliminates \nuncertainty. And although the calculus does not represent unions explicitly, its encoding oper\u00adates asexpected \nafact weexploitin reactive specialization. Policy translation. The function I(s,t), which translates \na pol\u00adicy into intermediate form, is similar to the translation for pred\u00adicates. Figure 6 gives the formal \nde.nition of the translation. To translate a basic policy e . S, the compiler .rst generates a se\u00adquence \nfrom e, and then attaches a pair of actions representing lower and upper bounds for each rule. There \nare three cases: If the three-valued boolean bi is true, it uses S as both the upper and lower bounds. \nIf bi isfalse, it uses \u00d8 as the bounds. If bi is Maybe, it uses \u00d8 as the lower bound and S as the upper \nbound, which rep\u00adresents the range of possible actions. The translations of intersected and negated policies \nare analogous to the cases for predicates. Classi.er construction. The second phase of classi.er genera\u00adtion \nanalyzes the intermediate form of the policy and produces a bona .de switch classi.er. The C(s,t)function \nthat implements this phase is de.ned in Figure 6. It .rst uses I(s,t)to gener\u00adate a sequence of intermediate \npolicies, and then analyzes each.ui : S1i,S2i : zi : Hi.to generate a rule. There are two possible outcomes \nfor each intermediate policyin the sequence. First, if (1) the bounds S1i and S2i are tight, (2) zi is \nsuf.ciently .ne grained to collect information about all headers in Hi, and (3) we get the same switch \nbounds (S1i,S2i )regardless of whether we match pack\u00adets using the ideal primitives or the switch-level \npatterns, then it is safe for the compiler to throw away the high-level semantic in\u00adformation (bounds \nand ideal primitives) and emit an effective rule zi : S1i. Otherwise, it generates a rule with the controller \naction O. The formal conditions needed for this analysis are captured by consistent(..,i)and headers(z). \nThe predicate consistent(..,i)is satis.ed if looking up an arbitrary packet matching the ith switch pattern \nyields the same switch bounds as looking it up using the ideal pattern (whereweextend classi.er lookupto \nsequencesofin\u00adtermediate policies in the obvious way). The function headers(z)calculates the set of headers \nconstrained fully by z. Formal properties. For a classi.er to be sound, it must satisfy two properties: \nit must forward packets according to the policy, and its rules must encode enough information about the \npackets that match them to implement inspectors. The correctness of classi.er generation is captured \nin the following de.nition and lemma. De.nition 1 (Classi.er Soundness). A classi.er r.is sound on switch \ns with respect to t if the following two criteria hold: Routing soundness: for all snapshots (S,s,p), \nif r..p S then [t] (S,s,p)= S, and r .p1 Collection soundness: for all packets p1 and p2 , if .z : S \n' and r..p2 z : S, then for all snapshots (S,s ,p ' ), '' [t] (S.{I(s,p1 )I},s ,p ' )= [t] (S.{I(s,p2 \n)I},s ,p ' ). Lemma1 (Classi.er Generation Soundness). Classi.er C(s,t)is sound on switch s with respect \nto t. Intuitively, routing soundness ensures that the actions computed by looking up rules in the classi.er \nr.are consistent with t.Formally, the condition states that if looking up a packet p in r.on switch s \nproduces a set of switches S, then evaluating t on snapshots containing s and p also produces S. Note \nthat this condition does not impose any requirements if looking up p in r.yields O, as the packet will \nbe sent to the controller, which will evaluate t on p directly. Collection soundness ensures that the \nrules in r.are suf.ciently .ne grained so that when the controller collects traf.c statistics from switches, \nthe rule patterns contain enough information to implement the policy s inspectors. This is seen in the \nE-COLLECT rule in the molecular machine (Figure 5), which fabricates packets that match the rule being \ncollected. Collection soundness ensures that fabricated packets are correct. Formally, it requires that \nt behave the same on all snapshots in which the state S has been extended with arbitrary packets p1 and \np2 matching a given rule z : S. Lemma 1 states that the classi.ers generated by the NetCore compiler \nare sound. 5.3 Reactive Specialization The algorithm described in the preceding section generates classi\u00ad.ers \nthat can be installed on switches. But as we saw, it has some substantial limitations. Dynamic policies \nthat use inspectors cannot be analyzed. And even for purely static policies, if the switch has poor support \nfor wildcards, the classi.er needed to implement the policymay be large much larger than would be practical \nto gener\u00adate.To deal with these situations, we de.ne reactive specialization, a powerful generalization \nof the simple, reactive, strategy imple\u00admented manually by OpenFlow programmers. We de.ne reactive specialization \nusing two operations: program re.nement, which ex\u00adpands the policy relative to a new snapshot witnessed \nat the con\u00adtroller, and pruning, which extracts new, effective rules from the classi.er generated from \ntheexpanded policy. Program re.nement. When the controller receives a new packet that a switch could \nnot handle, it interprets the policy with respect to the packet, switch, and its current state. The idea \nin program re.nement is to augment the program with additional information gleaned from this packet that \ncan be used to build a specialized classi.er that handles similar packets on the switch in the future. \nFigure 7 de.nes the re.nement function. The key invariant of this program transformation is that the \nsemantics of the old and new policies are identical. However, syntactically, the new program will typically \nhave a different structure, as the transformation uses the packet to unfold primitives and inspectors. \nThis makes compilation more precise and the recompiled program more effective. The rules for re.ning \na predicate appear at the top of Figure 7. The .rst rule uses the re.nement oracle U to re.ne basic predi\u00adcates. \nUnlike the compilation oracle, which may overapproximate ' R(x,e)= e R((S,s,p),h : w.)=(h :.w,p) w). \nU (h :.R(x, switch s)= switch s R((S,s,p),inspect ef)= .(inspect e ' f). e '' if invariant (x,f). f(x) \n. ' '' .(inspect ef)\\ e if invariant (x,f).\u00acf(x) ' . inspect ef if \u00ac invariant (x,f) . where f : State[H]\u00d7 \nSwitch \u00d7 Packet[H ' ]. Bool and x =(S,s,p) ' and e = R(x,e).(R(x,e)n similar(s,p,H)) '' and e = similar(s,p,H \n' )R(x,e1 n e2 )=R(x,e1 )n R (x,e2 )R(x, \u00ace)=\u00acR(x,e) ' R(x,t)= t R(x,e . S)=R(x,e). S R(x,t1 n t2 )=R(x,t1 \n)nR (x,t2 )R(x, \u00act)=\u00acR(x,t) Specialize(x,t)= r. Specialize((S,s,p),t)= prune(.r,p) where r.=C(s,R((S,s,p),t)) \nFigure 7. NetCore re.nement. the predicate, the re.nement oracle underapproximates it, so that the rest \nof the compilation infrastructure will be able to generate an effective switch-level rule that matches \nthe given packet. Be\u00adcause the new predicate is the union of the old predicate and an underapproximation, \nthe overall semantics is unchanged. In some cases, especially if the switch-supported patterns are weak, \nthe best underapproximation the re.nement oracle can generate is an exact\u00admatch predicate. In many other \ncases, however, if the switch sup\u00adports pre.x matching or wildcards, the re.nement oracle will pro\u00adduce \na predicate that matches manymore packets. The second rule re.nes switch predicates switch s. Because \nthe switch predicate already reveals the maximum amount of informa\u00adtion, it cannot be re.ned further. \nThe rule for inspectors is the most interesting. It uses a simi\u00adlarity predicate that describes the set \nof packets sent to the same switch that agree on a set of headers H: similar(s,p,H)= switch s n .(h : \np(h)). h.H We .rst re.ne the traf.c .lter predicate e to add additional struc\u00adture for traf.c collection. \nTo ensure that the re.ned classi.er has suf.ciently .ne-grained rules to collect the packets in the controller \nstate examined by f, we form the union of the re.ned traf.c .lter and the similarity predicate similar(s,p,H), \nrestricted to the re\u00ad.ned traf.c .lter. Next, we add additional information about the inspector s decision \non the packet pto the policy. Recall that if f is invariant with respect to a snapshot x (which includes \nthe controller state, switch, and packet) then it will return the same decision on all similar packets \nin the future. In the .rst case, if the inspector is in\u00advariant and evaluates to true on the current \nsnapshot (S,s,p), then we re.ne it by taking the union of the inspector and the similarity predicate \nsimilar(s,p,H ' ). The second case is similar, except that the inspector does not evaluate to true, and \nhence we re.ne the in\u00adspector by subtracting the similarity predicate. Finally, in the third case, the \ninspector is not invariant so no sound re.nement exists the decision returned by the inspector may change \nin the future if the controller state changes. Hence, packets must continue being diverted to the controller \nuntil the inspector becomes invariant. The rules for re.ning intersection (e1 n e2 )and negation \u00ace \npredicates and policies are all straightforward. Pruning. In general, after a policy has been re.ned \nand recom\u00adpiled, some of the new rules will be useless they will not pro\u00adcess additional packets on the \nswitch.We pruneaway these useless rules using a function prune(.r,p)that removes rules from r.that (1) \nsend packets to the controller (adding such rules does not im\u00adprove the ef.ciency of the switch), (2) \nhave nothing to do with the packet p(meaning theyare irrelevant to specialization with respect to p), \nor (3) overlap with a rule we removed earlier (to preserve the semantics of the rules). Putting it all \ntogether. We de.ne reactive specialization (the function Specialize at the bottom of Figure 7), by composing \nre.nement, recompilation, and pruning to generate a specialized classi.er froma snapshot x and policyt. \nFormal properties. We .rst establish that specialization, and therefore reactive rule generation, is \nsound. Lemma 2 (Specialization Soundness). If r.is sound on switch s '' with respect to t and r.= Specialize((S,s,p),t), \nthen (.r ,r.)is sound on s with respect to t. To establish the other properties, we need a way of characterizing \nthe packets that go to the controller.We de.ne the controller set of a classi.er r.as follows: O(r.)={pIr..p \nO}. The second property we establish is that re.nement is monotonic. That is, if we append reactive \nrules to a switch s classi.er, the resulting classi.er does not send more packets to the controller that \nthe original one.Formally, Lemma 3 (Specialization Monotonicity). For all policies t and '' classi.ers \nr.and r.such that r.= Specialize((S,s,p),t)we have ' O((.r ,r.)). O(.r). The .nal property we establish \nis that under certain assump\u00adtions, appending reactive rulestoa classi.er resultsin strictlyfewer packets \ngoing to the controller.To make sucha guarantee, we need two conditions: First, the policy t must be \nrealizable intuitively, it must only use features that can be implemented on switches(e.g., on an OpenFlow \nswitch, the policymust not match on payloads). De.nition2(Realizable). Apolicyt is realizable if, for \nevery sub\u00adterm h : w.of t and p . [h : w], we have (S,s,p). [U(h : w,p)]if and only if p. O(U(h : w,p)). \nRealizability states that compiling an underapproximation of a high-level predicate with respect to a \npacket matching the predicate yields a switch-level rule that exactly corresponds to the predicate. Second, \nall inspectors in the policy must be determinate. We for\u00admalize this by extending the notion of invariance \nto full invariance: De.nition 3 (Fully Invariant). A policy t is fully invariant on S if for every subterm \nof t of the form inspect ef and we have invariant ((S,s,p),f)for all switches s and packets p. For policies \nsatisfying these conditions, we can guarantee that the packet used to re.ne and recompile the policywill \nnever be sent to the controller again. Lemma 4 (Specialization Progress). If t is realizable and fully \n' invariant on S, and r.= Specialize((S,s,p),t), then for any ' classi.er r., we have p. O((r.,r.)). \n6. System-wide Correctness Properties This section uses the tools developed in the previous section to \nde\u00adliver our two central theoretical results: (1) a proof of functional correctness for NetCore, and \n(2) a proof of quiescence, another fundamental theorem which establishes that, when inspectors are invariant, \nthe network eventually reaches a state in which all pro\u00adcessing occurs ef.ciently on its switches. Functional \ncorrectness. Recall in Section3 we de.ned two ide\u00adalized reference machines: the synchronous reference \nmachine, which at all times knows (and has recorded) information about every packet processed in the \nnetwork, and the asynchronous ref\u00aderence machine, which nondeterministically learns about packets processed \nin the network. To demonstrate the correctness of the NetCore compiler, we show that it inhabits the \nspace between the asynchronous and synchronous reference machines. More for\u00admally, we prove that the \nasynchronous reference machine simulates the NetCore molecular machine and the molecular machine simu\u00adlates \nthe synchronous reference machine. Given a set of switches S and a policy t, we initialize the molecular \nmachine as follows: Init(S,t)={IC (t I\u00d8)I}.{IS (s IC(s,t)I\u00d8)Is . SI}. The next theorem establishes the \nrelationship between the reference and molecular machines. Theorem1 (Functional Correctness). Given a \nset of switches S, an initial set of transmissions T such that T (s Ip). T implies s . S, and a molecular \nmachine M = Init(S,t). T, we have: The asynchronous machine (t,\u00d8,T,\u00d8)weakly simulates M.  M weakly \nsimulates the synchronous machine (t,\u00d8,T).  Proof sketch. We describe the .rst simulation only; the \nsecond is similar. The simulation relation between the asynchronous machine and the molecular machine \nsatis.es the following: (1) each switch s classi.er on the molecular machine is sound with respect to \nt, (2) there exists an observation-preserving bijection between pending transmissions in the asynchronous \nmachine and transmissions and help molecules in the molecular machine, and (3) there exists an observation \npreserving bijection between the processed transmis\u00adsions in the asynchronous machine and the switch \nstates in the molecular machine. The initial state satis.es these criteria by clas\u00adsi.er generation soundness. \nNow, consider taking a transition. If it forwards a packet, the .rst bijection is preserved by routing \ncor\u00adrectness; if it collects a pattern, the second bijection is preserved by collection soundness; .nally, \nif it generates reactive rules, they are sound by specialization soundness. Quiescence. The quiescence \ntheorem demonstrates that the Net-Core compiler effectively moves work offof the controller and onto \nswitches, even when the program is expressed in terms of patterns the switch cannot implement precisely \nand inspector functions the compiler cannot analyze. Formally, quiescence states that if all of the inspectors \nin the program are invariant, then the NetCore com\u00adpiler will eventually install rules on switches that \nhandle all future traf.c i.e., eventually, the system can reach a con.guration where no additional packets \nneed to be sent to the controller. Before we can state the quiescence theorem precisely, we need a few \nde.nitions and supporting lemmas. First, we say that M is derived from policy t if (Init(S,t). T)M, where \n.* .* is the re.exive, transitive closure of the single step judgement, ignoring observations. Second, \nwe lift the notion of full invariance to machines M; M is fully invariant if the controller s policy \nis fully invariant with respect to the controller s state. We also lift the notion of controller set \non classi.ers to machines M: O(M)={(s,p)IS (s Ir.IZ). M and p. O(r.)}. The .rst lemma, controller set \nmonotonicity, states that the set of packets that require processing on the controller never increases: \nLemma 5 (Controller Set Monotonicity). If M is derived from t o and M .. M ' , then O(M ' ). O(M). Proof. \nFollows from specialization monotonicity. Now we are ready to prove the key lemma needed for quies\u00adcence. \nThe controller set progress lemma states that if the con\u00adtroller program is realizable and has become \nfully invariant, then every time the controller processes a help molecule, the controller set becomes \nstrictly smaller. In other words, every help molecule contains enough information (and the compiler is \npowerful enough to exploit it) for the E-CONTROLLER rule to generate useful new classi.er rules. Lemma6 \n(Controller Set Progress). For every realizable policy t o . M ' and fully invariant M derived from \nt, if M .is an instance of E-CONTROLLER then O(M ' ). O(M). Proof. Follows from specialization progress. \nQuiescence follows from these lemmas, as the total number of possible packets is .nite. The precise statement \nof quiescence says that the run-time system may (as opposed to does) quiesce, because the machine may \nnon-deterministically choose to continue forwarding packets using the switches instead of processing \nthe remaining help molecules. Formally, a machine con.guration M may quiesce if there exists a con.guration \nM ' such that M .* M ' and the rule E-CONTROLLER is not used in any derivation in the operational semantics \nstarting from M ' . With this de.nition in hand, we can state quiescence. Theorem 2 (Quiescence). For \nevery realizable policy t and fully invariant M derived from t, we have that M may quiesce. 7. Implementation \nand Evaluation We have implemented a prototype NetCore compiler in Haskell us\u00ading the ideas presented \nin this paper. The core algorithms are for\u00admulated in terms of abstract type classes(e.g., lattices for \nswitch patterns) and oracles. This makes it easy to instantiate the com\u00adpiler for different switch architectures \none simply has to de.ne instances for a few type classes and provide the appropriate oracles. We have \nbuilt two back-ends, both targeting OpenFlow switches. The .rst generates coarse-grained wildcard rules. \nThe other back\u00adend, used for comparison, generates the kind of exact-match rules used in our earlier \nwork on Frenetic [9] and most hand-written NOXapplications [11]. Optimizations. The implementation uses \na number of heuristic optimizations to avoid the combinatorial blowup that would result from compiling \nclassi.ers naively.Forexample,it applies algebraic rewritings on-the-.y to remove useless patterns and \nrules and re\u00adduce the size of the intermediate patterns and classi.ers it needs to manipulate. The compilation \nalgorithms identify and remove pat\u00adterns completely shadowed by other patterns and patterns whose SPE \nSPQE IPE 10k 10k 10k 5k 5k 5k Classi.erSize SwitchMisses 10k 5k Classi.erSizeSwitchMisses Classi.erSize \nSwitchMisses  SPE SPQE IPE Full Compiler Switch Misses 0 7.5k 4.5k Classi.er Size 8 15.9k 0.3k \u00b5Flow \nCompiler Switch Misses 13.7k 13.7k 33.4k Classi.er Size 13.7k 13.7k 29.2k  25k 50k 75k 25k 50k 75k 25k \n50k 75k Packets Packets Packets 10k 5k 10k  5k 25k 50k 75k 25k 50k 75k 25k 50k 75k Packets Packets \nPackets Figure 8. Experimental results. effect is covered by a larger pattern with lower priority but \nthe same actions. Although these heuristics are simple, they go a long way toward ensuring reasonable \nperformance in our experience. Evaluation. To evaluate our implementation, we built an instru\u00admented \nversion of the run-time system that collects statistics about the sizes of the classi.ers generated by \nthe compiler and the amount of traf.c handled on switches (as opposed to the controller). Be\u00adcause space \nfor classi.ers is a limited resource on switches, and because the cost of diverting a packet to the controller \nslows down its processing by orders of magnitude, these metrics quantify some of the most critical performance \naspects of the system. We compared the performance of the full (which makes use of all OpenFlow rules, \nincluding wildcards) and \u00b5.ow (which only generates exact-match rules, also known as micro.ow rules) \ncompilers on the following programs: StaticPolicy Experiment (SPE): implements the simple static policydescribed \nat the beginning of Section 2. This benchmark measures the (in)ef.ciency of compilation strategies based \non generating exact-match rules.  Static Policy with Query Experiment (SPQE): forwards packets using \nthe same policy as in SPE but also collects traf\u00ad.c statistics for each host. Due to this collection, \nthis program cannot be directly compiled to a switch classi.er at least, not without expanding all 4.3 \nbillion possible hosts! Thus, this benchmark measures the ef.ciencyof reactive specialization.  InspectorPolicy \nExperiment (IPE):forwards packets and col\u00adlects traf.c statistics using the authentication application \npre\u00adsented in Section 2. This benchmark measures the performance of a more realistic application implemented \nusing inspectors.  To drive these experiments, we generated packets using fs [22], a tool that synthesizes \nrealistic packet traces from several statistical parameters.We ran eachexperiment on 100K packets in \ntotal.For the SPE and SPQE benchmarks, we generated traf.c with 1024 ac\u00adtive hosts sending packets to \nan external network for 30 seconds each. For the IPE benchmark, we generated traf.c with 254 hosts (a \nclassC network) sending traf.c to the authentication server and an external network for 30 seconds each. \nThe results of the exper\u00adiments are shown in Figure 8. The graphs on the top row show the number of packets \nthat missed and had to be sent to the controller against the total number of packets processed. Likewise, \nthe graphs on the bottom row show the size of the compiled classi.er, in terms of number of rules, versus \ntotal packets. The table at the right gives the .nal results after all 100K packets were processed. In \nterms of the proportion of packets processed on switches, the full OpenFlow compiler outperforms the \nmicro.ow-based com\u00adpiler on nearly all of the benchmarks. On the SPE benchmark, the full compiler generates \na classi.er that completely handles the policy, so no packets are sent to the controller. (The line for \nthe full compiler overlaps with the x-axis.) The micro.ow compiler, of course, diverts a packet to the \ncontroller for each distinct mi\u00adcro.ow, generating 13.7k rules in total. On the SPQE benchmark, the full \ncompiler generates wildcard rules (using reactive special\u00adization) that handle all future traf.c from \neach unique host after seeing a packet from it. These rules handle many more packets than the exact-match \nrule produced by the micro.ow compiler. On this benchmark, it is worth noting that the classi.ers produced \nby the full compiler are larger than the ones produced by the micro.ow compiler, especially initially. \nThis is due to the fact that the full compiler generates multiple rules in response to a single controller \npacket, attempting to cover a broad space of future similar pack\u00adets, whereas the micro.ow compiler predictably \ngenerates a single micro.ow for each controller packet. One can see that the work done by the full compiler \npays off in terms of the number of pack\u00adets that must be diverted to the controller. Moreover, over time, \nthe size of the micro.ow compiler-generated classi.er approaches that of the full compiler. Lastly, the \nIPE benchmark demonstrates that the full compiler generates more effective classi.ers than the mi\u00adcro.ow \ncompiler, even in the presence of inspector functions that it cannot analyze directly. Note that a large \nnumber of packets must be diverted to the controller in any correct implementation until they authenticate, \nthe inspector is not invariant for any host. How\u00adever, the full compiler quickly converges to a classi.er \nthat pro\u00adcesses all traf.c directly on the switch. 8. RelatedWork Building on ideas .rst proposed in \nEthane [4] and 4D [10], NOX [11] was the .rst concrete system to popularize what is cur\u00adrentlyknownas \nsoftware-de.ned networking.Itprovidesanevent\u00addriven interface to OpenFlow [17] and requires that programmers \nwrite reactive programs using callbacks and explicit, switch-level packet-processing rules. There are \nnumerous examples of network applications built on top of NOX using micro.ows [12, 13, 27], but relatively \nfew that use wildcard rules (thoughWang s load bal\u00adancer [26] is a nice example of the latter). Networking \nresearchers are now actively developing next\u00adgeneration controller platforms. Some of them, such as Beacon \n[1] (designed for Java) and Nettle [25] (designed for Haskell) pro\u00advide elegant OpenFlow interfaces for \nnew programming languages. Others, such as Onix [15], and Maestro [3] improve scalability and fault tolerance \nthrough parallelization and distribution. None of these systems automatically generate reactive protocols \nor provide formal semantics or correctness guarantees like NetCore does. Both NetCore and NDLog [16] \nuse high-level languages to pro\u00adgram networking infrastructure,but the similarities end there. ND-Log \nprograms are written in an explicitly distributed style whereas high-level NetCore programs are written \nas if the program has an omniscient, centralized view of the entire network. The NetCore implementation \nautomatically partitions work onto a distributed set of switches and synthesizes a reactive communication \nprotocol that simulates the semantics of the high-level language. Part of the job of the NetCore compiler \nis to generate ef.cient packet classi.ers. Most previous research in this area (seeTay\u00adlor [24] for a \nsurvey) focuses on static compilation. The NetCore compiler generates classi.ersin thefaceof non-static \npolicies, with unknown inspector functions, and synthesizes a distributed switch\u00adcontroller implementation. \nBro [21], Snortran [7], Shangri-La [5] and FPL-3E [6] compile rich packet-.ltering and monitoring pro\u00adgrams, \ndesigned to secure networks and detect intrusions, down to special packet-processing hardware and FPGAs. \nThe main differ\u00adence between NetCore and all of these systems is that they are lim\u00adited to a single device. \nTheydo not address the issue of how to pro\u00adgram complex, dynamic policies for a collection of interconnected \nswitches and theydo not synthesize the distributed communication patterns between the switches and controller. \nActive Networking, as in the SwitchWare project [23], shares many high-level goals with Software-De.ned \nNetworking, but the implementation strategy is entirely different. The former uses smart switches to \ninterpret programs encapsulated in packets, while the latter uses dumb switches controlled by a remote \nhost. Acknowledgments We wish to thank Ken Birman, Mike Freedman, Sharad Malik, Jen Rexford, Cole Schlesinger, \nand Alec Story and the anonymous POPL reviewers for discussions about this research and comments on drafts \nof this paper. Our work is supported in part by ONR grants N00014-09-1-0770 and N00014-09-1-0652 and \nNSF grants CNS\u00ad1111698 and CNS-1111520. Any opinions, .ndings, and recom\u00admendations are those of the \nauthors and do not necessarily re.ect the views of the ONR or NSF. References [1] Beacon: A java-based \nOpenFlow control platform., Nov 2010. See http://www.beaconcontroller.net. [2] G. Berry and G. Boudol. \nThe chemical abstract machine. In POPL, pages 81 94, 1990. [3]Z.Cai,A.Cox,andT.Ng. Maestro:A systemforscalable \nOpenFlow control. Technical Report TR10-08, Rice University, Dec 2010. [4] M. Casado, M. Freedman, J. \nPettit, J. Luo, N. Gude, N. McKeown, and S. Shenker. Rethinking enterprise network control. Trans. on \nNetworking., 17(4), Aug 2009. [5] M. Chen, X. Li, R. Lian, J. Lin, L. Liu, T. Liu, and R. Ju. Shangri\u00adla: \nAchieving high performance from compiled network applications while enabling ease of programming. In \nPLDI,Jun 2005. [6] M. Cristea, C. Zissulescu, E. Deprettere, and H. Bos. FPL-3E: To\u00adwards language support \nfor recon.gurable packet processing. In SAMOS, pages 201 212,Jul 2005. [7] S. Egorov and G. Savchuk. \nSNORTRAN: An Optimizing Compiler for Snort Rules. Fidelis Security Systems, 2002. [8] D. Erickson et \nal. A demonstration of virtual machine mobility in an OpenFlow network, Aug 2008. Demo at ACM SIGCOMM. \n[9] N. Foster, R. Harrison, M. Freedman, C. Monsanto, J. Rexford, A. Story, and D.Walker. Frenetic:A \nnetwork programming language. In ICFP, Sep 2011. [10] A. Greenberg, G. Hjalmtysson, D. Maltz, A. Myers, \nJ. Rexford, G. Xie, H. Yan, J. Zhan, and H. Zhang. A clean slate 4D approach to network control and management. \nSIGCOMM CCR, 35:41 54, Oc\u00adtober 2005. [11] N. Gude, T. Koponen, J. Pettit, B. Pfaff, M. Casado, N. McKeown, \nand S. Shenker. NOX: Towards an operating system for networks. SIGCOMM CCR, 38(3), 2008. [12] N. Handigol, \nS. Seetharaman, M. Flajslik, N. McKeown, and R. Jo\u00adhari. Plug-n-Serve: Load-balancing web traf.c using \nOpenFlow, Aug 2009. Demo at ACM SIGCOMM. [13] B. Heller, S. Seetharaman, P. Mahadevan, Y. Yiakoumis, \nP. Sharma, S. Banerjee, and N. McKeown. ElasticTree: Saving energy in data center networks. In NSDI, \nApr 2010. [14] L. Jose, M. Yu, and J. Rexford. Online measurement of large traf.c aggregates on commodity \nswitches. In Hot-ICE, Mar 2011. [15] T.Koponen, M. Casado, N. Gude,J. Stribling, L. Poutievski, M. Zhu, \nR. Ramanathan, Y. Iwata, H. Inoue, T. Hama, and S. Shenker. Onix: A distributed control platform for \nlarge-scale production networks. In OSDI, Oct 2010. [16] B. Loo, J. Hellerstein, I. Stoica, and R. Ramakrishnan. \nDeclarative routing: Extensible routing with declarative queries. In SIGCOMM, pages 289 300, 2005. [17] \nN. McKeown,T. Anderson,H. Balakrishnan,G.Parulkar,L. Peterson, J. Rexford, S. Shenker, andJ.Turner. Open.ow: \nEnabling innovation in campus networks. SIGCOMM CCR, 38(2):69 74, 2008. [18] A. Nayak, A. Reimers, N. \nFeamster, and R. Clark. Resonance: Dy\u00adnamic access control in enterprise networks. In WREN, Aug 2009. \n[19] The Open Networking Foundation, Mar 2011. See http:// www.opennetworkingfoundation.org/. [20] OpenFlow, \nNov 2010. See http://www.openflowswitch.org. [21] Vern Paxson. Bro: A system for detecting network intruders \nin real\u00adtime. Computer Networks, 31(23 24):2435 2463, Dec 1999. [22] J. Sommers, R. Bowden, B. Eriksson, \nP. Barford, M. Roughan, and N. Duf.eld. Ef.cient network-wide .ow record generation. In INFO-COM, pages \n2363 2371, 2011. [23] SwitchWare. http://www.cis.upenn.edu/~switchware, 1997. [24] D.Taylor. Survey \nand taxonomy of packet classi.cation techniques. ACM Comput. Surv., 37:238 275, September 2005. [25] \nA.Voellmy andP. Hudak. Nettle: Functional reactive programming of OpenFlow networks. In PADL,Jan 2011. \n[26] R. Wang, D. Butnariu, and J. Rexford. OpenFlow-based server load balancing gone wild. In Hot-ICE, \nMar 2011. [27] K. Yap, M. Kobayashi, R. Sherwood, T. Huang, M. Chan, N. Hand\u00adigol, and N. McKeown. OpenRoads: \nEmpowering research in mobile networks. SIGCOMM CCR, 40(1):125 126, 2010.  \n\t\t\t", "proc_id": "2103656", "abstract": "<p>Software-defined networks (SDNs) are a new kind of network architecture in which a controller machine manages a distributed collection of switches by instructing them to install or uninstall packet-forwarding rules and report traffic statistics. The recently formed Open Networking Consortium, whose members include Google, Facebook, Microsoft, Verizon, and others, hopes to use this architecture to transform the way that enterprise and data center networks are implemented.</p> <p>In this paper, we define a high-level, declarative language, called NetCore, for expressing packet-forwarding policies on SDNs. NetCore is expressive, compositional, and has a formal semantics. To ensure that a majority of packets are processed efficiently on switches---instead of on the controller---we present new compilation algorithms for NetCore and couple them with a new run-time system that issues rule installation commands and traffic-statistics queries to switches. Together, the compiler and run-time system generate efficient rules whenever possible and outperform the simple, manual techniques commonly used to program SDNs today. In addition, the algorithms we develop are generic, assuming only that the packet-matching capabilities available on switches satisfy some basic algebraic laws.</p> <p>Overall, this paper delivers a new design for a high-level network programming language; an improved set of compiler algorithms; a new run-time system for SDN architectures; the first formal semantics and proofs of correctness in this domain; and an implementation and evaluation that demonstrates the performance benefits over traditional manual techniques.</p>", "authors": [{"name": "Christopher Monsanto", "author_profile_id": "81488673230", "affiliation": "Princeton University, Princeton, NJ, USA", "person_id": "P2991382", "email_address": "chris@monsan.to", "orcid_id": ""}, {"name": "Nate Foster", "author_profile_id": "81444600818", "affiliation": "Cornell University, Ithaca, NY, USA", "person_id": "P2991383", "email_address": "jnfoster@cs.cornell.edu", "orcid_id": ""}, {"name": "Rob Harrison", "author_profile_id": "81477640650", "affiliation": "United States Military Academy, West Point, NY, USA", "person_id": "P2991384", "email_address": "Rob.Harrison@usma.edu", "orcid_id": ""}, {"name": "David Walker", "author_profile_id": "81100426485", "affiliation": "Princeton University, Princeton, NJ, USA", "person_id": "P2991385", "email_address": "dpw@cs.princeton.edu", "orcid_id": ""}], "doi_number": "10.1145/2103656.2103685", "year": "2012", "article_id": "2103685", "conference": "POPL", "title": "A compiler and run-time system for network programming languages", "url": "http://dl.acm.org/citation.cfm?id=2103685"}