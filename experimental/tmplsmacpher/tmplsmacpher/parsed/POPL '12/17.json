{"article_publication_date": "01-25-2012", "fulltext": "\n Analysis of Recursively Parallel Programs * Ahmed Bouajjani LIAFA, Universit\u00b4e Paris Diderot, France \n abou@liafa.jussieu.fr Abstract We propose a general formal model of isolated hierarchical parallel computations, \nand identify several fragments to match the concur\u00adrency constructs present in real-world programming \nlanguages such as Cilk and X10. By associating fundamental formal models (vector addition systems with \nrecursive transitions) to each fragment, we provide a common platform for exposing the relative dif.culties \nof algorithmic reasoning. For each case we measure the complexity of deciding state-reachability for \n.nite-data recursive programs, and propose algorithms for the decidable cases. The complexities which \ninclude PTIME, NP, EXPSPACE, and 2EXPTIME contrast with un\u00addecidable state-reachability for recursive \nmulti-threaded programs. Categories and Subject Descriptors D.2.4 [Software/Program Veri.cation]: Formal \nmethods; F.3.1 [Specifying and Verifying and Reasoning about Programs]: Mechanical veri.cation; D.3.2 \n[Lan\u00adguage Classi.cations]: Concurrent, distributed, and parallel lan\u00adguages; F.1.2 [Models of Computation]: \nParallelism and concur\u00adrency General Terms Algorithms, Reliability, Veri.cation Keywords Concurrency, \nParallelism, Veri.cation 1. Introduction Despite the ever-increasing importance of concurrent software \n(e.g., for designing reactive applications, or parallelizing compu\u00adtation across multiple processor cores), \nconcurrent programming and concurrent program analysis remain challenging endeavors. The most widely \navailable facility for designing concurrent applications is multithreading, where concurrently executing \nsequential threads nondeterministically interleave their accesses to shared memory. Such nondeterminism \nleads to rarely-occurring Heisenbugs which are notoriously dif.cult to reproduce and repair. To prevent \nsuch bugs programmers are faced with the dif.cult task of preventing undesirable interleavings, e.g., \nby employing lock-based synchro\u00adnization, without preventing benign interleavings otherwise the desired \nreactivity or parallelism is forfeited. *Partially supported by the project ANR-09-SEGI-016 Veridyc. \nSupported by a post-doctoral fellowship from the Fondation Sciences Math\u00b4 ematiques de Paris. 0Proofs \nto technical results are contained in an extended online report [3]. Permission to make digital or hard \ncopies of all or part of this work for personal or classroom use is granted without fee provided that \ncopies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. POPL 12, January 25 27, 2012, Philadelphia, \nPA, USA. Copyright c &#38;#169; 2012 ACM 978-1-4503-1083-3/12/01. . . $10.00 Michael Emmi LIAFA, Universit\u00b4e \nParis Diderot, France mje@liafa.jussieu.fr The complexity of multi-threaded program analysis seems \nto comply with the perceived dif.culty of multi-threaded program\u00adming. The state-reachability problem \nfor multi-threaded programs is PSPACE-complete [21] with a .nite number of .nite-state threads, and undecidable \n[29] with recursive threads. Current analysis ap\u00ad proaches either explore an underapproximate concurrent \nsemantics by considering relatively few interleavings [9, 22] or explore a coarse overapproximate semantics \nvia abstraction [13, 18]. Explicitly-parallel programming languages have been advocated to avoid the \nintricate interleavings implicit in program syntax [24], and several such industrial-strength languages \nhave been devel\u00adoped [2, 6, 7, 17, 25, 30, 32]. Such systems introduce various mech\u00ad anisms for creating \n(e.g., fork, spawn, post) and consuming (e.g., join, sync) concurrent computations, and either encourage \n(through recommended programming practices) or ensure (through static analyses or runtime systems) that \nparallel computations ex\u00adecute in isolation without interference from others, through data\u00adpartitioning \n[7], data-replication [6], functional programming [17], message passing [27], or version-based memory \naccess models [32], perhaps falling back on transactional mechanisms [23] when com\u00ad plete isolation is \nimpractical. Although few of these systems behave deterministically, consuming one concurrent computation \nat a time, many are sensitive to the order in which multiple isolated computa\u00adtions are consumed. Furthermore, \nsome allow computations creating an unbounded number of sub-computations, returning to their supe\u00adriors \nan unbounded number of handles to un.nished computations. Even without multithreaded interleaving, nondeterminism \nin the or\u00adder in which an unbounded number of computations are consumed has the potential to make program \nreasoning complex. In this work we investigate key questions on the analysis of interleaving-free programming \nmodels. Speci.cally, we ask to what extent such models simplify program reasoning, how those models compare \nwith each other, and how to design appropriate analysis algorithms. We attempt to answer these questions \nas follows: We introduce a general interleaving-free parallel programming model on which to express \nthe features found in popular parallel programming languages (Section 2).  We discover a surprisingly-complex \nfeature of some existing lan\u00adguages: even simple classes of programs with the ability to pass un.nished \ncomputations both to and from subordinate computa\u00adtions have undecidable state-reachability problems \n(Section 2.4).  We show that the concurrency features present in many real\u00adworld programming languages \nsuch as Cilk, X10, and Multilisp are captured precisely (modulo the possibility of interleaving) by various \nfragments of our model (Sections 4 and 6).  For fragments corresponding to real-world language features, \nwe measure the complexity of computing state-reachability for .nite-data programs, and provide, in most \ncases, asymptotically optimal state-reachability algorithms (Sections 5 and 7).   Our focus on .nite-data \nprograms without interleaving is a means to measuring complexity for the sake of comparison, required \nsince state-reachability for in.nite-data or multi-threaded programs is generally undecidable. Applying \nour algorithms in practice may rely on data abstraction [16], and separately ensuring isolation [23], \nor approximating possible interleavings [9, 13, 18, 22]; still, our handling of computation-order non-determinism \nis precise. The major distinguishing language features are whether a single or an arbitrary number of \nsubordinate computations are waited for at once, and whether the scope of subordinate computations is \ncon.ned. Generally speaking, reasoning for the single-wait case of Section 4 is less dif.cult than for \nthe multi-wait case of Section 6, and we demonstrate a range of complexities1 from PTIME, NP, EXPSPACE, \nand 2EXPTIME for various scoping restrictions in Sections 5 and 7. Despite these worst-case complexities, \na promising line of work has already demonstrated effective algorithms for practically-occurring EXPSPACE-complete \nstate-reachability problem instances based on simultaneously computing iterative under-and over-approximations, \nand rapidly converging to a .xed point [15, 19]. We thus present a classi.cation of concurrency constructs, \ncon\u00ad necting programming language features to fundamental formal mod\u00ad els, which highlight the sources \nof concurrent complexity resulting from each feature, and provide a platform for comparing the dif.\u00ad \nculty of formal reasoning in each. We hope that these results may be used both to guide the design of \nimpactful program analyses, as well as to guide the design and choice of languages appropriate for various \nprogramming problems. 2. Recursively Parallel Programs We consider a simple concurrent programming model \nwhere com\u00ad putations are hierarchically divided into isolated parallely executing tasks. Each task executes \nsequentially while maintaining regions (i.e., containers) of handles to other tasks. The initial task \nbegins without task handles. When a task t creates a subordinate (child) task u, t stores the handle \nto u in one of its regions, at which point t and u begin to execute in parallel. The task u may then \nrecursively create additional parallel tasks, storing their handles in its own regions. At some later \npoint when t requires the result computed by u, t must await the completion of u i.e., blocking until \nu has .nished at which point t consumes its handle to u. When u does complete, the value it returns is \ncombined with the current state of t via a programmer-supplied return-value handler. In addition to creating \nand consuming subordinate tasks, tasks can transfer ownership of their subordinate tasks to newly-created \ntasks by initially passing to the child a subset of task handles and to their superiors upon completion \nby .nally passing to the parent unconsumed tasks. This model permits vastly concurrent executions. Each \ntask along with all the tasks it has created execute completely in parallel. As tasks can create tasks \nrecursively, the total number of concurrently executing tasks has no bound, even when the number of handles \nstored by each task is bounded. 2.1 Program Syntax Let Procs be a set of procedure names, Vals a set \nof values, Exprs a set of expressions, Regs a .nite set of region identi.ers, and Rets . (Vals . Stmts) \na set of return-value handlers. The grammar of Figure 1 describes our language of recursively parallel \nprograms. We intentionally leave the syntax of expressions e unspeci.ed, though we do insist Vals contains \ntrue and false, and Exprs contains Vals and the (nullary) choice operator *. We 1In order to isolate \nconcurrent complexity from the exponential factor in the number of program variables, we consider a .xed \nnumber of variables in each procedure frame; this allows us a PTIME point-of-reference for state-reachability \nin recursive sequential programs [31]. P ::= ( proc p (var l: T ) s )* s ::= s; s | l := e | skip | assume \ne | if e then s else s | while e do s | call l := pe | return e | post r . p e rr d | ewait r | await \nr Figure 1. The grammar of recursively parallel programs. Here T is an unspeci.ed type, p ranges over \nprocedure names, e over expressions, r over regions, and d over return-value handlers. refer to the class \nof programs restricted to a .nite set of values as .nite-value programs, and to the class of programs \nrestricted to at most n . N (resp., 1) region identi.ers as n-region (resp., single\u00adregion) programs. \nA sequential program is a program without post, ewait, and await statements. Each program P declares \na sequence of procedures named p0 ...pi . Procs *, each p having single type-T parameter l and a top-level \nstatement denoted sp; as statements are built inductively by composition with control-.ow statements, \nsp describes the entire body of p. The set of program statements s is denoted Stmts. Intuitively, a post \nr . pe Tr d statement stores the handle to a newly-created task executing procedure p in the region r; \nbesides the procedure argument e, the newly-created task is passed a subset of the parent s task handles \nin regions Tr, and a return\u00advalue handler d. The ewait r statement blocks execution until some task whose \nhandle is stored in region r completes, at which point its return-value handler is executed. Similarly, \nthe await r statement blocks execution until all tasks whose handles are stored in region r complete, \nat which point all of their return-value handlers are executed, in some order. We refer to the call, \nreturn, post, ewait and await as inter-procedural statements, and the others as intra-procedural statements, \nand insist that return-value handlers are comprised only of intra-procedural statements. The assume e \nstatement proceeds only when e evaluates to true we use this statement in subsequent sections to block \nundesired executions in our encodings of other parallel programming models. Example 1. The Fibonacci \nfunction can be implemented as a single\u00adregion recursively parallel program as follows. proc fib (var \nn: N) var sum: N if n<2 then return 1 else post r . fib (n-1) e (.v. sum := sum + v); post r . fib (n-2) \ne (.v. sum := sum + v); await r; return sum Alternate implementations are possible, e.g., by replacing \nthe await statement by two ewait statements, or storing the handles to the recursive calls in separate \nregions. Note that in this implementation task-handles are not passed to child tasks (e speci.es the \nempty region sequence) nor to parent tasks (all handles are consumed by the await statement before returning). \nThe programming language we consider is simple yet expressive, since the syntax of types and expressions \nis left free, and we lose no generality by considering only a single variable per procedure. 2.2 Parallel \nSemantics with Task-Passing Unlike recursive sequential programs, whose semantics is de.ned over stacks \nof procedure frames, the semantics of recursively parallel programs is de.ned over trees of procedure \nframes. Intuitively, the frame of each posted task becomes a child of the posting task s frame. Each \nstep of execution proceeds either by making a single intra-procedural step of some frame in the tree, \ncreating a new frame by posting a task, or removing a frame by consuming a completed task; unconsumed \nsub-task frames of a completed task are added as children to the completed task s parent. A task (e, \ns, d) is a valuation e . Vals to the procedure-local variable l, along with a statement s to be executed, \nand a return\u00advalue handler d . Rets. (Here s describes the entire body of a procedure p that remains \nto be executed, and is initially set to p s top-level statement sp.) A tree con.guration c is a .nite \nunordered tree of task-labeled vertices and region-labeled edges, and the set of con.gurations is denoted \nCon.gs. Let M[Con.gs] denote the set of con.guration multisets. We represent con.gurations inductively, \nwriting (t, m) for the tree with t-labeled root whose child sub\u00adtrees are given by a region valuation \nm : Regs . M[Con.gs]: for r . Regs, the multiset m(r) speci.es the collection of sub\u00adtrees connected \nto the root of (t, m) by an r-edge. The initial def region valuation m\u00d8 is de.ned by m\u00d8(r)= \u00d8 for all \nr . Regs. The singleton region valuation (r . c) maps r to {c}, and r' . Regs \\{r} to \u00d8, and the union \nm1 . m2 of region valuations is def de.ned by the multiset union of each valuation: (m1 . m2)(r)= m1(r) \n. m2(r) for all r . Regs. The projection m |rof a region valuation m to a region sequence Tr is de.ned \nby m |r(r r')= m(r') ' when r' occurs in Tr, and m |r(r)= \u00d8 otherwise. r r For expressions without program \nvariables, we assume the existence of an evaluation function [\u00b7]e : Exprs . P(Vals) such that [*]e = \nVals. For convenience, we de.ne def def e((e, s, d))= e(e)= [e[e/l]]e as l is the only variable, the \nexpression e[e/l] has no free variables. To reduce clutter and focus on the relevant parts of transition \nrules in the program semantics, we introduce a notion of contexts. A con.guration context C is a tree \nwith a single o-labeled leaf, task-labeled vertices and leaves otherwise, and region-labeled edges. We \nwrite C[c] for the con.guration obtained by substituting a con\u00ad.guration c for the unique o-labeled leaf \nof C. We use con.guration contexts to isolate individual task transitions, writing, for instance ' C[(t, \nm)] . C[(t,m)] to indicate an intra-procedural transition of the task t. Similarly a statement context \nS = o; s1; ... ; si is a o-led sequence of statements, and we write S[s0] for the statement obtained \nby substituting a statement s0 for the unique occurrence of o as the .rst symbol of S, indicating that \ns0 is the next-to-be\u00adexecuted statement. A task-statement context T = (e, S, d) is a task with a statement \ncontext S in place of a statement, and we write T [s] to indicate that s is the next statement to be \nexecuted in the task (e, S[s] ,d). Finally, we write C[(T [s1] ,m)] . C[(T [s2] ,m')] to denote a transition \nof a task executing a statement s1 and replacing s1 by s2 normally s2 is the skip statement. Since the \ncurrent statement s of a task T [s] does not effect expression evaluation, we liberally write e(T ) to \ndenote the evaluation e(T [s]). We say a task t = (e, S[s] ,d) is completed when its next-to\u00adbe-executed \nstatement s is return e, in which case we de.ne def rvh(t)= {d(v): v . e(e)} as the set of possible return-value \nhan\u00addler statements for t; rvh(t) is unde.ned when t is not completed. Figure 2 and Figure 3 de.ne the \ntransition relation .rpp/p of recursively parallel programs as a set of operational steps on con.gurations. \nThe intra-procedural transitions .seq of individual tasks in Figure 2 are standard. More interesting \nare the inter\u00adprocedural transitions of Figure 3, which implicitly include a transition C[(t1,m)] .rpp/p \nC[(t2,m)] whenever t1 .seq t2. PP The POST-T rule creates a procedure frame to execute in parallel, and \nlinks it to the current frame by the given region, passing ownership of tasks in the speci.ed region \nsequence to the newly\u00adcreated frame. The .WAIT-T rule consumes the result of a single child frame in \nthe given region, and applies the return-value handler to update the parent frame s local valuation. \nSimilarly, the .WAIT- POST-T v . e(T ) m' = m \\ m |r.r .(v, sp,d) ,m |r rrrpp/p ' C[(T [post r . per \nd] ,m)] ----. CT [skip] ,m P .WAIT-T ' m1 =(r .(t2,m2)) . ms . rvh(t2) 1 rpp/p ' C[(T1[ewait r] ,m1)] \n----. CT1[s] ,m1 . m2 P .WAIT-NEXT-T ' m1 =(r .(t2,m2)) . ms . rvh(t2) 1 rpp/p ' C[(T1[await r] ,m1)] \n----. CT1[s; await r] ,m1 . m2 P .WAIT-DONE-T m(r)= \u00d8 rpp/p C[(T [await r] ,m)] ----. C[(T [skip] ,m)] \nP Figure 3. The tree-based transition relation for parallely-executing recursively parallel programs \nwith task-passing. NEXT-T and .WAIT-DONE-T rules consume the results of every child frame in the given \nregion, applying their return handlers in the order they are consumed. The semantics of call statements \nreduces to that of post and ewait: supposing an unused region identi.er rcall, we translate each statement \ncall l := pe into the sequence post rcall . pee dcall; ewait rcall, def where dcall(v)= l := v is the \nreturn-value handler which simply writes the entire return value v into the local variable l, and e denotes \nan empty sequence of region identi.ers. A parallel execution of a program P (from c0 to cj ) is a .rpp/p \n con.guration sequence c0c1 ...cj where ciP ci+1 for 0 = i<j. An initial condition . = (p0,e0) is a procedure \np0 . Procs along with a value e0 . Vals. A con.guration ((e0, s, d) ,m\u00d8) is called (p0,e0)-initial when \ns is the top-level statement of p0. A con.guration cf is called ef -.nal when there exists a context \nC such that cf = C[(t, m)] and l(t)= ef . We say a valuation e is reachable in P from . when there exists \nan execution of P from some c0 to cf , where c0 is .-initial and cf is e-.nal. Problem 1 (State-Reachability). \nThe state-reachability problem is to determine, given an initial condition . of a program P and a valuation \ne, whether e is reachable in P from .. 2.3 Sequential Semantics with Task-Passing Since tasks only exchange \nvalues at creation and completion-time, the order in which concurrently-executing tasks make execution \nsteps does not affect computed program values. In this section we leverage this fact and focus on a particular \nexecution order in which at any moment only a single task is enabled. When the currently enabled task \nencounters and ewait/await statement, suspending execution to wait for a subordinate task t, t becomes \nthe currently\u00adenabled task; when t completes, control returns to its waiting parent. At any moment only \nthe tasks along one path . in the con.guration tree have ever been enabled, and all but the last task \nin . are waiting for their child in . to complete. We encode this execution order into an equivalent \nstack-based operational semantics, which essentially transforms recursively parallel programs into sequential \nprograms with an unbounded auxiliary storage device used to store subordinate tasks. We interpret the \newait and await statements as procedure calls which compute the values returned by previously-posted \ntasks. ASSUME IF-THEN IF-ELSE SKIP true . e(T ) true . e(T ) false . e(T ) seqseqseqseq T [skip; s] \n--. T [s] T [assume e] --. T [skip] T [if e then s1 else s2] --. T [s1] T [if e then s1 else s2] --. \nT [s2] PP PP ASSIGN LOOP-DO LOOP-END C ' . e(C) true . e(T ) false . e(T ) seqseqseq (C, S[l := e],d) \n--. C ' ,S[skip],d T [while e do s] --. T [s; while e do s] T [while e do s] --. T [skip] PP P Figure \n2. The intra-procedural transition relation for recursively parallel programs. POST-S ' v . e(T ) m = \nm \\ m |. r .(v, sp,d) ,m | r r rpp/s ' (T [post r . per d] ,m) c - --. T [skip] ,m c P .WAIT-S ' m =(r \n. c0) . m rpp/s ' (T [ewait r] ,m) c - --. c0 T [skip] ,m c P .WAIT-NEXT-S ' m =(r . c0) . m rpp/s ' \n(T [await r] ,m) c - --. c0 T [skip; await r] ,m c P .WAIT-DONE-S m(r)= \u00d8 rpp/s (T [await r] ,m) c - \n--. (T [skip] ,m) c P RETURN-S s . rvh(t1) rpp/s (t1,m1)(T2[skip] ,m2) c - --. (T2[s] ,m1 . m2) c P Figure \n4. The stack-based transition relation for sequentially\u00adexecuting recursively parallel programs with \ntask-passing. We de.ne a frame to be a con.guration in the sense of the tree-based semantics of Section \n2.2, i.e., a .nite unordered tree of task-labeled vertices and region-labeled edges. (Here all non-root \nnodes in the tree are posted tasks that have yet to take a single step of execution.) In our stack-based \nsemantics, a stack con.guration c is a sequence of frames, representing a procedure activation stack. \nFigures 2 and 4 de.ne the sequential transition relation .rpp/s of recursively parallel programs as a \nset of operational steps on con\u00ad.gurations. The inter-procedural transitions of Figure 4 implicitly in\u00ad \nclude a transition (t1,m) c .rpp/s (t2,m) c whenever t1 .seq t2. PP Interesting here are the rules for \newait and await. The .WAIT-S rule blocks the currently executing frame to obtain the result for a single, \nnondeterministically chosen, frame c0 in the given re\u00adgion, by pushing c0 onto the activation stack. \nSimilarly, the .WAIT-NEXT-S and .WAIT-DONE-S rules block the currently executing frame to obtain the \nresults for every task in the given region, in a nondeterministically-chosen order. Finally, the RETURN-S \napplies a completed task s return-value handler to update the parent frame s local valuation. The de.nitions \nof sequential execution, initial, and reachable are nearly identical to their parallel counterparts. \nLemma 1. The parallel semantics and the sequential semantics are indistinguishable w.r.t. state reachability, \ni.e., for all initial conditions . of a program P , the valuation e is reachable in P from . by a parallel \nexecution if and only if e is reachable in P from . by a sequential execution.  2.4 Undecidability of \nState-Reachability with Task-Passing Recursively parallel programs allow pending tasks to be passed bidi\u00adrectionally: \nboth from completed tasks and to newly-created tasks. This capability makes the state-reachability problem \nundecidable even for the very simple cases recursive programs with at least one region, and for non-recursive \nprograms with at least two regions. Essentially, when pending tasks can be passed to newly-created tasks, \nit becomes possible to construct and manipulate unbounded task-chains by keeping a handle to most-recently \ncreated task, after having passed the handle of the previously-most-recently created task to the most-recently \ncreated task. We can then show that such unbounded chains of pending tasks can be used to simulate an \narbi\u00adtrary unbounded and ordered storage device. De.nition 1 (Task passing). A program which contains \na statement post r . peTr d, such that |Tr| > 0 is called task-passing. The task-depth of a program P \nis the maximum length of a sequence p1 ...pi of procedures in P such that each pj contains a statement \npost r . pi+j eTr d, for 0 <j<i, and some r . Regs, e . Exprs, Tr . Regs *, and d . Rets. Programs with \nunbounded task-depth are recursive, and are otherwise non-recrusive. Theorem 1. The state-reachability \nproblem for n-region .nite\u00advalue task-passing parallel programs is undecidable for (a) non-recursive \nprograms with n> 1, and (b) recursive programs with n> 0.  The proof of Theorem 1 is given by two separate \nreductions from the emptiness problem for Turing machines to single-wait programs, i.e., those using \newait statements but not await state\u00adments. In essence, as each task-handle can point to an unbounded \nchain of task-handles, we can construct an unbounded Turing ma\u00adchine tape by using one task-chain to \nstore the contents of cells to the left of the tape head, and another chain to store the contents of \ncells to the right of the tape head. If only one region is granted but recursion is allowed (i.e., as \nin (b)), we can still construct the tape using the task-chain for the cells right of the tape head, while \nusing the (unbounded) procedure-stack to store the cells left of the head. When only one region is granted \nand recursion is not allowed, neither of these reductions work. Without recursion we can bound the procedure \nstack, and then we can show that single-stack machine suf.ces to encode the single unbounded chain of \ntasks. 3. Programs without Task Passing Due to the undecidability result of Theorem 1 and our desire \nto compare the analysis complexities of parallel programming mod\u00adels, we consider, henceforth, unless \notherwise speci.ed, only non\u00adtask-passing programs, simplifying program syntax by writing post r . ped. \nWhen task-passing is not allowed, region valu\u00adations need not store an entire con.guration for each newly-posted \ntask, since the posted task s initial region valuation is empty. As this represents a signi.cant simpli.cation \non which our subsequent analysis results rely, we rede.ne here a few key notions. POST ' v . e(T ) m \n= m . (r .(v, sp,d)) rpp ' (T [post r . ped] ,m) c --. T [skip] ,m c P .WAIT ' m =(r . t2) . m rpp ' \n(T1[ewait r] ,m) c --. (t2, \u00d8) T1[skip] ,m c P .WAIT-NEXT ' m =(r . t2) . m rpp ' (T1[await r] ,m) c \n--. (t2, \u00d8) T1[skip; await r] ,m c P .WAIT-DONE m(r)= \u00d8 rpp (T [await r] ,m) c --. (T [skip] ,m) c P \nRETURN s . rvh(t1) rpp (t1,m1)(T2[skip] ,m2) c --. (T2[s] ,m1 . m2) c P Figure 5. The stack-based transition \nrelation for sequentially\u00adexecuting recursively parallel programs without task-passing. 3.1 Sequential \nSemantics without Task-Passing A region valuation is a (non-nested) mapping m : Regs . M[Tasks] from \nregions to multisets of tasks, a frame (t, m) is a task t . Tasks paired with a region valuation m, and \na con.guration c is a sequence of frames representing a procedure activation stack. The transition relation \n.rpp of Figures 2 and 5 implicitly include a transition (t1,m) c .rpp (t2,m) c whenever t1 .seq t2. The \nPP de.nitions of sequential execution, initial, and reachable are nearly identical to their task-passing \nparallel and sequential counterparts. Since pending tasks need not store initial region-valuations in \nnon\u00adtask-passing programs, this simpler semantics is equivalent to the previous stack-based semantics. \nLemma 2. For all initial conditions . non-task-passing programs P , the valuation e is reachable in P \nfrom . by a sequential execution with task-passing if and only if e is reachable in P from . by a sequential \nexecution without task-passing. Even with this simpli.cation, we do not presently know whether the state-reachability \nproblem for (.nite-value) recursively parallel programs is decidable in general. In the following sections, \nwe iden\u00adtify several decidable, and in some cases tractable, restrictions to the program model which \ncorrespond to the concurrency mechanisms found in real-world parallel programming languages.  3.2 Recursive \nVector Addition Systems with Zero-Test Edges Fix k . N.A recursive vector addition system (RVASS) A = \n(Q, d)of dimension k is a .nite set Q of states, along with a .nite set d = d1 l d2 l d3 of transitions \npartitioned into additive transitions d1 . Q\u00d7Nk \u00d7Nk \u00d7Q, recursive transitions d2 . Q\u00d7Q\u00d7Q\u00d7Q, and zero-test \ntransitions d3 . Q \u00d7 Q. We write n2 rn1r q'-.q ' whenq, Tn1, Tn2,q '. d1, and q1q2 q'-.q ' whenq, q1,q2,q \n'. d2. q'.q ' whenq, q '. d3. A (non-recursive) vector addition system (with states) (VASS) is a recursive \nvector addition system (Q, d) such that d contains only additive transitions. An (RVASS) frame (q, Tn) \nis a state q . Q along with a vector Tn . Nk, and an (RVASS) con.guration c . (Q \u00d7 Nk)+ is a non\u00adempty \nsequence of frames representing a stack of non-recursive sub-computations. The transition relation .rvas \nfor recursive vector addition systems is de.ned in Figure 6. The ADDITIVE rule updates the top frame \n(q, Tn) by subtracting the vector Tn1 from Tn, adding the vector Tn2 to the result, and updating the \ncontrol state to q '. The CALL rule pushes on the frame-stack a new frame (q1, 0) from which the RETURN \nrule will eventually pop at some point when the control state is q2; when this happens, the vector Tn1 \nof the popped frame is added to the vector Tn2 of the frame below. We describe an application of the \nCALL (resp., RETURN) rule as a call (resp., return) transition. Finally, the ZERO rule proceeds only \nwhen the top-most frame s vector equals 0. An execution of a RVASS A (from c0 to cj ) is a con.guration \n.rvas sequence c0c1 ...cj where ci ci+1 for 0 = i<j.A con.guration (q, Tn) is called q0-initial when \nq = q0 and Tn = 0, and a con.guration cf is called qf -.nal when cf = (qf , Tn) c for some con.guration \nc and Tn . Nk. We say a state qf is reachable in A from q0 when there exists an execution of A from some \nq0-initial con.guration c0 to some qf -.nal con.guration cf . The state-reachability problem for recursive \nvector addition systems is to determine whether a given state q is reachable from some q0. Recently Demri \net al. [8] have proved that state-reachability in branching vector addition systems (BVAS) a very similar \nformal model to which RVASS reduces is in 2EXPTIME. This immedi\u00adately gives us an upper-bound on computing \nstate-reachability in RVASS without zero-test edges. Though state-reachability in non\u00adrecursive systems \nis EXPSPACE-complete [26, 28], for the moment, we do not know matching upper and lower bounds for RVASS. \nLemma 3. The state-reachability problem for recursive (resp., non\u00adrecursive) vector addition systems \nwithout zero-test edges is EXPSPACE-hard, and in 2EXPTIME (resp., EXPSPACE). 3.3 Encoding Recursively \nParallel Programs as RVASSs When the value set Vals of a given program P is taken to be .nite, the set \nTasks also becomes .nite since there are .nitely many statements and return-value handlers occurring \nin P . As .nite\u00addomain multisets are equivalently encoded with a .nite number of counters (i.e., one \ncounter per element), we can encode each region valuation m . Regs . M[Tasks] by a vector Tn . Nk of \ncounters, where k = |Regs \u00d7 Tasks|. To clarify the correspondence, we .x an enumeration cn : Regs \u00d7 Tasks \n.{1,...,k}, and associate each region valuation m with a vector Tn such that for all r . Regs and t . \nTasks, m(r)(t)= Tn(cn(r, t)). Let Tni denote the unit vector of dimension i, i.e., Tni(i)=1 and T ni(j)=0 \nfor j= i. Given a .nite-data recursively parallel program P without task\u00adpassing, we associate a corresponding \nrecursive vector addition def system AP = (Q, d). We de.ne Q = Tasks . Tasks3, and de.ne d formally \nin Figure 7. Intra-procedural transitions translate directly to additive transitions. The call statements \nare handled by recursive transitions between entry and exit points t0 and tf of the called procedure. \nThe post statements are handled by additive transitions that increment the counter corresponding to a \nregion-task pair. The ewait statements are handled in two steps: .rst an additive transition decrements \nthe counter corresponding to region-task pair (r, t0), then a recursive transition between entry and \nexit points t0 and tf of the corresponding procedure is made, applying the return\u00advalue handler of tf \nupon the return. (Here we use an intermediate state (T [skip] ,t0,tf ). Q to connect the two transitions, \nin order to differentiate the intermediate steps of other ewait transitions.) The await statements are \nhandled similarly, except the await statement must be repeated again upon the return. Finally, a zero-test \ntransition allows AP to eventually step past each await statement. ADDITIVE q rn1rn2 '-. q ' rn = rn1 \nCALL q q1q2 '-.q ' RETURN q q1q2 '-.q ' ZERO q'.q ' (q, rn) c rvas- -. q ' , rn 8 rn1 . rn2 c (q, rn) \nc rvas- -. (q1, 0) (q, rn) c (q2, rn1) (q, rn2) c rvas- -. q ' , rn1 . rn2 c (q, 0) c rvas- -. q ' , \n0 c transition originating from each state, i.e., for all q . Q, Figure 6. The transition relation \nfor recursive vector addition systems. To simplify presentation, we assume that there is at most one \nrecursive d2 n ({q}\u00d7 Q3) = 1. We denote by 0 the vector (0, 0,..., 0), and by . and e mm mm the usual \nvector addition and subtraction operators. v0 . e(T ) i = cn(r, (v0,sp,d)) 0r niT [await r] '-.T [skip] \nT [post r . ped] ' -.T [skip] v0 . e(T ) t0 = (v0,sp, dcall) (l := vf ) . rvh(tf ) t0tf T [call l := \npe] '-.T l := vf seq t1 --. t2 P i = cn(r, t0) s . rvh(tf ) 00 rt0tf ni0 t1'-.t2 T [ewait r] 'T [skip] \n,t0,tf-. '-.T [s] i = cn(r, t0) s . rvh(tf ) rni0t0tf T [await r] 'T [skip] ,t0,tf '-.T [s; await r] \n-. Figure 7. The transitions of the RVASS AP encoding the behavior of a .nite-data recursively parallel \nprogram P . Notice that ignoring intermediate states (t1,t2,t3). Q, the frames (t, Tn) of AP correspond \ndirectly to frames (t, m) of the given program P , given the correspondence between vectors and region \nvaluations. This correspondence between frames indeed extends to con.gurations, and ultimately to the \nstate-reachability problems between AP and P . Lemma 4. For all programs P without task-passing, procedures \np0 . Procs, and values e0,e . Vals, e is reachable from (e0,p0)in P if and only if there exist s . Stmts \nand d0,d . Rets such that (e, s, d) is reachable from (e0,sp0 ,d0) in AP . Our analysis algorithms in \nthe following sections use Lemma 4 to compute state-reachability of a program P without task-passing \nby computing state-reachability on the corresponding RVASS AP . In general, our algorithms compute sets \nof region valuation vectors def rvas sms(t0,tf ,P )= {Tn : (t0, 0) --. *(tf , Tn)}, AP summarizing the \nexecution of a procedure between an entry point t0 and exit point tf , where we write .rvas * to denote \nzero or more AP applications of .rvas . Given an effective way to compute such a AP function, we could \nsystematically replace inter-procedural program steps (i.e., of the call, ewait, and await statements) \nwith intra\u00adprocedural edges performing their net effect. Note however that even if the set of tasks is \n.nite, the set sms(t0,tf , AP ) of summaries between t0 and tf need not be .nite; the ability to compute \nthis set is thus the key to our summarization-based algorithms in the following sections. 4. Single-Wait \nPrograms De.nition 2 (Single wait). A single-wait program is a program which does not contain the await \nstatement. Single-wait programs can wait only for a single pending task at any program point. Many parallel \nprogramming constructs can be modeled as single-wait programs. 4.1 Parallel Programming with Futures \nThe future annotation of Multilisp [17] has become a widely adopted parallel programming construct, included, \nfor example, in X10 [7] and in Leijen et al. [25] s task parallel library. Flanagan and Felleisen [12] \nprovide a principled description of its semantics. The future construct leverages the procedural program \nstructure for parallelism, essentially adding a lazy procedure call which immediately returns control \nto the caller with a placeholder for a value that may not yet have been computed, along with an operation \nfor ensuring that a given placeholder has been .lled in with a computed value. Syntactically, futures \nadd two statements, future x := pe touch x, where x ranges over program variables, p . Procs, and e \n. Exprs. Though it is not necessarily present in the syntax of a source language with futures, we assume \nevery use of a variable assigned by a future statement is explicitly preceded by a touch statement. Semantically, \nthe future statement creates a new process in which to execute the given procedure, which proceeds to \nexecute in parallel with the caller and all other processes created in this way. The touch statement \non a variable x blocks execution of the current procedure until the future procedure call which assigned \nto x completes, returning a value with which is copied into x. Even though each procedure can only spawn \na bounded number of parallel processes i.e., one per program variable there is in general no bound on \nthe total number of parallelly-executing processes, since procedure calls even parallel ones are recursive. \nExample 2. The Fibonacci function can be implemented as a parallel algorithm using futures as follows. \nproc fib (var n: N) var x, y: N if n<2 then return 1 else future x := fib (n-1); future y := fib (n-2); \ntouch x; touch y; return x+y As opposed to the usual (na\u00a8ive) sequential implementation operating in \ntime O(n 2), this parallel implementation runs in time O(n). The semantics of futures is readily expressed \nwith task-passing programs using the post and ewait statements. Assuming a region identi.er rx and return \nhandler dx for each program variable x, we encode future x := pe as post rx . peTrdx touch x as ewait \nrx def where dx(v)= x := v simply assigns the return value v to the variable x, and the vector Tr contains \neach ry such that the variable y appears in e.  4.2 Parallel Programming with Revisions Burckhardt et \nal. [6] s revisions model of concurrent programming proposes a mechanism analogous to (software) version \ncontrol systems such as CVS and subversion, which promises to naturally and easily parallize sequential \ncode in order to take advantage of multiple computing cores. There, each sequentially executing process \nis referred to as a revision. A revision can branch into two revisions, each continuing to execute in \nparallel on their own separate copies of data, or merge a previously-created revision, provided a programmer-de.ned \nmerge function to mitigate the updates to data which each have performed. Syntactically, revisions add \ntwo statements, x := rfork s join x, where x ranges over program variables, and s . Stmts. Seman\u00adtically, \nthe rfork statement creates a new process to execute the given statement, which proceeds to execute in \nparallel with the invoker and all other processes created in this way. The assign\u00adment stores a handle \nto the newly-created revision in a revision variable x. The join statement on a revision variable x blocks \nexecution of the current revision until the revision whose handle is stored in x completes; at that point \nthe current revision s data is updated according to a programmer-supplied merge function m :(Vals \u00d7 Vals \n\u00d7 Vals) . Vals: when v0, v1 are, resp., the initial and .nal data values of the merged revision, and \nv2 is the current data value of the current revision, the current revisions data value is updated to \nm(v0,v1,v2). The semantics of revisions is readily expressed with task-passing programs using the post \nand ewait statements. Assuming a region identi.er rx for each program variable x, and a programmer\u00adsupplied \nmerge function m, we encode x := rfork s as post rx . ps l Trd join x as ewait rx where ps is a procedure \ndeclared as proc ps (var l: T ) var l0 := l s; return (l0,l) def and d((v0,v1))= l := m(v0,l,v1) updates \nthe current local valuation based on the joined revision s initial and .nal valuations v0,v1 . Vals, \nand the joining revision s current local valuation stored in l. The vector Tr contains each ry for which \nthe revision variable y is accessed in s.2  4.3 Programming with Asynchronous Procedures Asynchronous \nprograms [14, 19, 33] are becoming widely-used to build reactive systems, such as device drivers, web \nservers, and graphical user interfaces, with low-latency requirements. Essentially, a program is made \nup of a collection of short-lived tasks running one\u00adby-one and accessing a global store, which post other \ntasks to be run at some later time. Tasks are initially posted by an initial procedure, and may also \nbe generated by external system events. An event loop repeatedly chooses a pending task from its collection \nto execute to completion, adding the tasks it posts back to the task collection. Syntactically, asynchronous \nprograms add two statements, async pe eventloop such that eventloop is invoked only once as the last \nstatement of the initial procedure. Semantically, the async statement initializes a 2Actually r must \nin general be chosen non-deterministically, as each revision handle may be joined either by the parent \nrevision or its branch. procedure call and returns control immediately, without waiting for the call \nto return. The eventloop statement repeatedly dispatches pending i.e., called but not yet returned procedures, \nand execut\u00ading them to completion; each procedure executes atomically making both synchronous calls, \nas well as an unbounded number of addi\u00adtional asynchronous procedure calls. The order in which procedure \ncalls are dispatched is chosen non-deterministically. We encode asynchronous programs as (non-deterministic) \nre\u00adcursively parallel programs using the post and ewait statements. Assuming a single region identi.er \nr0, we encode async pe as post r0 . p ' e d eventloop as while true do ewait r0 . Supposing p has top-level \nstatement s accessing a shared global variable g (besides the procedure parameter l), we declare p ' \nas ' proc p (var l: T ) var g0 := * var g := g0 s; return (g0,g). def Finally d ((v0,v1))= assume l= \nv0;l := v1 models the atomic update p performs from an initial (guessed) shared global valuation v0. \nGuessing allows us to simulate the communication of a shared global state g, which is later ensured to \nhave begun with v0, which the previously-executed asynchronous task had written. 5. Single-Wait Analysis \nThe absence of await edges in a program P implies the absence of zero-test transitions in the corresponding \nrecursive vector addition system AP . To compute state-reachability in P via procedure summarization, \nwe must summarize the recursive transitions of AP by additive transitions (in a non-recursive system) \naccounting for the left-over pending tasks returned by reach procedure. This is not trivial in general, \nsince the space of possibly returned region valuations is in.nite. In increasing dif.culty, we isolate \nthree special cases of single-wait programs, whose analysis problems are simpler than the general case. \nIn the simplest non-aliasing case where the number of tasks stored in each region of a procedure frame \nis limited to one, the execution of ewait statements are deterministic. When the number of tasks stored \nin each region is not limited to one, non-determinism arises from the choice of which completed task \nto pick at each ewait statement (see the .WAIT rule of Figure 5). This added power makes the state-reachability \nproblem at least as hard as state-reachability in vector addition systems i.e., EXPSPACE\u00adhard, though \nthe precise complexity depends on the scope of pending tasks. After examining the PTIME-complete non-aliasing \ncase, we examine two EXPSPACE-complete cases by restricting the scope of task handles, before moving \nto the general case. 5.1 Single-Wait Analysis without Aliasing Many parallel programming languages consume \nonly the compu\u00adtations of precisely-addressed tasks. In futures, for example, the touch x statement applies \nto the return value of a particular procedure the last one whose future result was assigned to x. Similarly, \nin revisions, the join x statement applies to the last revision whose handle was stored in x. Indeed \nin the single-wait program semantics of each case, we are guaranteed that the corre\u00adsponding region, \nrx, contains at most one task handle. Thus the non-determinism arising (from choosing between tasks in \na given region) in the .WAIT rule of Figure 3 disappears. Though both fu\u00adtures and revisions allow task-passing, \nthe following results apply to futures-and revisions-based programs which only pass pending tasks from \nchild to parent. De.nition 3 (Non aliasing). We say a region r . Regs is aliased in a region valuation \nm : Regs . M[Tasks] when |m(r)| > 1. We say r is aliasing in a program P if there exists a reachable \ncon.guration C[(t, m)] of P in which r is aliased in m.A non\u00adaliasing program is a program in which no \nregion is aliasing. Note that the set of non-aliasing region valuations is .nite when the number of program \nvalues is. The non-aliasing restriction thus allow us immediately to reduce the state-reachability problem \nfor single-wait programs to reachability in a recursive .nite-data sequen\u00adtial program. To compute state-reachability \nwe consider a sequence A0A1 ... of .nite-state systems iteratively under-approximating the recursive \nsystem AP given from a single-wait program P . Initially, A0 has only the transitions of AP corresponding \nto intra-procedural and post transitions of P . At each step i> 0, we add to Ai an additive edge summarizing \nan ewait transition rn njr T [ewait r] '-.T [s] , for some t0,tf . Tasks such that j = cn(r, t0), s . \nrvh(tf ), and Tn is reachable at tf from t0 in Ai-1, i.e., Tn . sms(t0,tf , Ai-1). This A0A1 ... sequence \nis guaranteed to reach a .xed-point Ak, since the set of non-aliasing region valuation vectors, and thus \nthe number of possibly added edges, is .nite. Furthermore, as each Ai is .nite-state, only .nite-state \nreachability queries are needed to determine the reachable states of Ak, which are precisely the same \nreachable states of AP . Note that the number of region valuations grows exponentially in the number \nof regions. Theorem 2. The state-reachability problem for non-aliasing single\u00adwait .nite-value programs \nis PTIME-complete for a .xed number of regions, and EXPTIME-complete in the number of regions.  5.2 \nLocal-Scope Single-Wait Analysis De.nition 4 (Local scope). A local-scope program is a program in which \ntasks only return with empty region valuations; i.e., for all reachable con.gurations C[(t[return e] \n,m)] we have m = m\u00d8. To solve state-reachability in local-scope single-wait programs, we compute a sequence \nA0A1 ... of non-recursive vector addition systems iteratively under-approximating the recursive system \nAP arising from a program P . The initial system A0 has only the transitions of AP corresponding to intra-procedural \nand post transitions of P . At each step i> 0, we add to Ai an additive edge summarizing an ewait transition \nr nj 0 T [ewait r] '-.T [s] for some t0,tf . Tasks such that j = cn(r, t0), s . rvh(tf ), and Tn . sms(t0,tf \n, Ai-1); since P is local-scope, every such Tn must equal 0. Since the number of possibly added edges \nis polynomial in P , the A0A1 sequence is guaranteed to reach in a polynomial number of steps a .xed-point \nAk whose reachable states are exactly those of AP . The entire procedure is EXPSPACE-complete, since \neach procedure-summarization reachability query is equivalent to computing state-reachability in vector \naddition systems. Theorem 3. The state-reachability problem for local-scope single\u00adwait .nite-value programs \nis EXPSPACE-complete.  5.3 Global-Scope Single-Wait Analysis Another relatively simple case of interest \nis when pending tasks are allowed to leave the scope in which they are posted, but can only be consumed \nby a particular, statically declared, task in an enclosing scope. This is the case, for example, in asynchronous \nprograms [33], though here we allow for slightly more generality, since tasks can be posted to multiple \nregions, and arbitrary control in the initial procedure frame is allowed. De.nition 5 (Global scope). \nA global-scope programs is a program in which the ewait (and await) statements are used only in the initial \nprocedure frame. Since each non-initial procedure p of a global-scope program can\u00adnot consume tasks, \nthe set of tasks posted by p and recursively-called procedures along any execution from t0 to tf is a \nsemi-linear set, described by the Parikh-image3 of a context-free language. Follow\u00ading Ganty and Majumdar \n[14] s approach, for each t0,tf . Tasks we construct a polynomial-sized vector addition system A(t0,tf \n) characterizing this semi-linear set of tasks (recursively) posted be\u00adtween t0 and tf . Then, we use \neach A(t0,tf ) as a component of a non-recursive vector addition system A ' representing execution of \nP the initial frame. In particular, A ' contains transitions to and from P the component A(t0,tf ) \nfor each t0,tf . Tasks, r nj 0 00 T [ewait r] '-. (q0,T [skip])(qf ,T [skip]) '-.T [s] , for all r . \nRegs such that j = cn(r, t0), s . rvh(tf ), and q0 and qf are the initial and .nal states of A(t0,tf \n). We assume each A(t0,tf ) has unique initial and .nal states, distinct from the states of other components \nA(t0' ,t f ' ). In order to transition to the correct state T [s] upon completion, A(t0,tf ) carries \nan auxiliary state\u00adcomponent T [skip]. In this way, for each task t ' posted to region r ' in an execution \nbetween t0 and tf , the component A(t0,tf ) does the incrementing of the cn(r ' ,t ' )-component of the \nregion-valuation vector. As each of the polynomially-many components A(t0,tf ) are constructed in polynomial \ntime [14], this method constructs A ' P in polynomial time. Thus state-reachability in P is computed \nby state-reachability in the non-recursive vector addition system A ' P , in exponential space. The complexity \nis asymptotically optimal since global-scope single-wait programs are powerful enough to capture state-reachability \nin vector addition systems. Theorem 4. The state-reachability problem for global-scope single\u00adwait .nite-value \nprograms is EXPSPACE-complete. 5.4 The General Case of Single-Wait Analysis In general, the state-reachability \nproblem for .nite-value single-wait programs is as hard as state-reachability in recursive vector addition \nsystems without zero-test edges. Theorem 5. The state-reachability problem for single-wait .nite\u00advalue \nprograms is EXPSPACE-hard, and in 2EXPTIME. Demri et al. [8] s proof of membership in 2EXPTIME relies \n on a non-deterministically chosen reachability witness without materializing a practical algorithm for \nthe search of said witness. Here we give a summarization-based algorithm. To compute state-reachability \nwe consider again a sequence A0A1 ... of non-recursive vector addition systems successively under-approximating \nthe recursive system AP of a single-wait pro\u00ad gram P . Initially A0 has only the transitions of AP corresponding \nto intra-procedural and post transitions of P . At each step i> 0, we add to Ai an additive edge summarizing \nan ewait transition rn njr T [ewait r] '-.T [s] , for some t0,tf . Tasks such that j = cn(r, t0), s \n. rvh(tf ), and Tn . sms(t0,tf , Ai-1). Even though the set of possible added additive edges summarizing \nrecursive transitions is in.nite, with careful analysis we can show that this very simple algorithm terminates, \nprovided we can bound the edge-labels Tn needed to compute state-reachability in AP . It turns out we \ncan bound these 3The Parikh-image of a word w over an alphabet S is the |S|-dimension vector of integers \ncounting the number of occurrences of each symbol of S in w. The image of a language is the set of images \nof its elements. edge labels, by realizing that the minimal vectors required to reach a target state \nfrom any given program location are bounded. We adopt an approach based on iteratively applying backward \nreachability analyses in order to determine for each task t the set of vectors .(t) needed to reach the \ntarget state in AP . Let us .rst recall some useful basic facts. Vector addition systems are monotonic \nw.r.t. the natural ordering on vectors of integers, i.e., if a transition is possible from a vector v, \nit is also possible from any u greater than v. The ordering on vectors of integers is a well quasi-ordering \n(WQO), i.e., in every sequence of vectors v0,v1,..., there are two indices i<j such that vi is less or \nequal than vj . Thus, every in.nite set of vectors has a .nite number of minimals. A set of vectors is \nupward closed if whenever it contains v it also contains all vectors greater than v. Such a set can be \ncharacterized by its minimals. Moreover, the set of all predecessors in a vector addition system of an \nupward closed set of vectors is also upward closed; and therefore backward reachability analysis in these \nsystems always terminates starting from an upward closed set [1, 11]. We observe that for every task \nt, the set .(t) is upward closed (by monotonicity), and therefore we need only determine its minimals. \nHowever, since our model is recursive vector addition systems, we must solve several state-reachability \nqueries on a sequence of vector addition systems with increasingly more transitions, which necessarily \nstabilizes. We elaborate below. First, in order to reason backward about executions to the target state, \nconsider the non-recursive system A ' i obtained by 00 adding return transitions tf '.T [s] from every \nprocedure exit point tf = Tf [return e] and procedure return point T [ewait e] occurring in P such that \ns . rvh(tf ). These extra transitions in A ' i simulate a return from tf to t, transferring all of the \npending tasks from a frame at tf to a frame at T [s], without any contribution from the T [s] s intra-procedural \npredecessor T [ewait e]. Then de.ne a sequence of functions .0,.1,... : Tasks . P(Nk), each .i mapping \neach t . Tasks to the (possibly empty, upward-closed) set of vectors .i(t) such that for any Tn . .i(t),a \ncon.guration (t, Tn) is guaranteed to reach the target reachable state in A ' i and thus (t, Tn) c is \nguaranteed to reach the target reachable state in AP for any c; each .i can be computed in by backward \nreachability in the non-recursive vector addition system as explained above. Since each Ai contains at \nleast the transitions of Ai-1, the .i-sequence is non-decreasing w.r.t. set inclusion; i.e., more and \nmore con.gurations can reach the target state; i.e., for all t . Tasks we have .i-1(t) . .i(t). Since \nthere can be no ever-increasing sequence of upward-closed sets of vectors over natural numbers (by the \nfact that the ordering on vectors of natural numbers if a WQO), the .i sequence must stabilize after \na .nite number of steps. Furthermore, since any Tn . .i(t) is guaranteed to reach the target state, it \nsuf.ces to consider only vectors Tn ' bounded by the minimals of the upward-closed set .i(t). To see \nwhy, notice that if some Tn . .i(t) labels an edge between t0 and t, then every con.guration at t0 is \nguaranteed to reach the target state, since this edge adds the vector guaranteed to reach the target \nfrom t. Additionally, any vector greater than a minimal of .i(t) is already guaranteed to be present \nin .i(t), since .i(t) is upward closed. Thus we need only consider edge-labels bounded by the decreasing \n.0.1 ... sequence, which shows that the A0A1 ... sequence stabilizes after a .nite number of steps. 6. \nMulti-Wait Programs Though single-wait programs capture many parallel programming constructs, they can \nnot express waiting for each and every of an unbounded number of tasks to complete. Some programming \nlanguages require this dual notion, expressed here with await. De.nition 6 (Multi wait). A multi-wait \nprogram is a program which does not contain the ewait statement. Thus, multi-wait programs can wait only \non every pending task (in a given region) at any program point. Many parallel programming constructs \ncan be modeled as multi-wait programs. 6.1 Parallel Programming in Cilk The Cilk parallel programming \nlanguage [30] is an industrial\u00ad strength language with an accompanying runtime system which is used in \na spectrum of environments, from modest multi-core com\u00adputations to massively parallel computations with \nsupercomputers. Similarly to futures (see Section 4.1), Cilk adds a form of procedure call which immediately \nreturns control to the caller. Instead of an operation to synchronize with a particular previously-called \npro\u00adcedure, Cilk only provides an operation to synchronize with every previously-called procedure. At \nsuch a point, the previously-called procedures communicate their results back to the caller one-by-one \nwith atomically-executing procedure in-lined in scope of the caller. Syntactically, Cilk adds two statements \nspawn pep ' sync, where p ranges over procedures, e over expressions, and p ' over procedures declared \nby inlet p ' (var rv: T ) s. Here s ranges over intra-procedural program statements contain\u00ading two \nvariables: rv, corresponding to the value returned from a spawned procedure, and l, corresponding to \nthe local variable of the spawning procedure. Semantically, the spawn statement creates a new process \nin which to execute the given procedure, which pro\u00adceeds to execute in parallel with the caller and all \nother processes created in this way. The sync statement blocks execution of the current procedure until \neach spawned procedure completes, and executes its associated inlet. The inlets of each procedure execute \natomically. Each procedure can spawn an unbounded number of parallel processes, and the order in which \nthe inlets of procedures execute is chosen non-deterministically. Example 3. The Fibonacci function can \nbe implemented as a parallel algorithm using Cilk as follows. proc fib (var n: N) var sum: N if n<2 then \nreturn 1 else spawn fib (n-1) summer; spawn fib (n-2) summer; sync; return sum inlet summer (var i: N) \nsum:=sum+ i As opposed to the usual (na\u00a8ive) sequential implementation operating in time O(n 2), this \nparallel implementation runs in time O(n). The semantics of Cilk is ready expressed with recursively \nparallel programs using the post and await statements. Assuming a region where dpl (v) l [v/rv] executes \nthe top-level statement of the identi.er r0, we encode spawn p e p ' as post r0 . p e dpl sync as await \nr0 def = sp inlet p ' with input parameter v.  6.2 Parallel Programming with Asynchronous Statements \nThe async/.nish pair of constructs in X10 [7] introduces parallelism through asynchronously executing \nstatements and synchronization blocks. Essentially, an asynchronous statement immediately passes control \nto a following statement, executing itself in parallel. A synchronization block executes as any other \nprogram block, but does not pass control to the following statements/block until every asynchronous statement \nwithin has completed. Syntactically, this mechanism is expressed with two statements, async s .nish s \nwhere s ranges over program statements. Semantically, the async statement creates a new process to execute \nthe given statement, which proceeds to execute in parallel with the invoker and all other processes created \nin this way. The .nish statement executes the given statement s, then blocks execution until every process \ncreated within s has completed. Example 4. The Fibonacci function can be implemented as a parallel algorithm \nusing asynchronous statements as follows. proc fib (var n: N) var x, y: N if n<2 then return 1 else .nish \nasync call x := fib (n-1); async call y := fib (n-2); return x+y As opposed to the usual (na\u00a8ive) sequential \nimplementation operating in time O(n 2), this parallel implementation runs in time O(n). Asynchronous \nstatements are readily expressed with (non\u00addeterministic) recursively parallel programs using the post \nand await statements. Let N be the maximum depth of nested .nish statements. Assuming region identi.ers \nr1,..., rN , we encode async s as post ri . ps * d .nish s as await ri where i - 1 is number of enclosing \n.nish statements, and ps is a procedure declared as proc ps (var l: T ) var l0 := l s; return (l0,l) \ndef and d((v0,v1))= assume l= v0; l := v1 models the up\u00addate p performs from an initial (guessed) local \nvaluation v0. Using the same trick we have used to model asynchronous programs in Sec\u00adtion 4.3, we model \nthe sequencing of asynchronous tasks by initially guessing the value v0 which the previously-executed \nasynchronous tasks had written, and validating that value when the return-value handler of a given task \nis .nally run. Note that although X10 allows, in general, asynchronous tasks to interleave their memory \naccesses, our model captures only non-interfering tasks, by assuming either data-parallelism (i.e., disjoint \naccesses to data), or by assuming tasks are properly synchronized to ensure atomicity. 7. Multi-Wait \nAnalysis The presence of await edges implies the presence of zero-test transitions in the recursive vector \naddition system AP corresponding to a multi-wait program P . As we have done for single-wait programs, \nwe .rst examine the easier sub-case of local-scope programs, which in the multi-wait setting corresponds \nconcurrency in the Cilk [30] language (modulo task interleaving), as well as structured parallel programming \nconstructs such as the foreach parallel loop in X10 [7] and in Leijen et al. [25] s task parallel library \n(see our extended online report [3]). The concurrent behavior of the asynchronous statements (Section \n6.2) in X10 [7] does not satisfy the local-scope restriction, since async statements can include recursive \nprocedure calls which are nested without interpolating .nish statements. There computing state-reachability \nis equivalent to determining whether a particular vector is reachable in a non\u00adrecursive vector addition \nsystem a decidable problem which is known to be EXPSPACE-hard, but for which the only known algorithms \nare non-primitive recursive. Since all multi-wait parallel languages we have encountered use only a single-region, \nwe restrict our attention at present to single-region multi-wait programs. 7.1 Local-Scope Single-Region \nMulti-Wait Analysis With the local-scoping restriction, executions of each procedure p . Procs between \nentry point t0 . Tasks and exit point tf . Tasks are completely summarized by a Boolean indicating whether \nor not tf is reachable from t0. However, as executions of p may encounter await statements, modeled by \nzero-test edges in the recursive vector addition system AP , computing this Boolean requires determining \nthe reachable program valuations between each pair of consecutive synchronization points (i.e., occurrences \nof the await statement), which in principle requires deciding whether the vector 0 is reachable in a \nvector addition system describing execution from the program point just after the .rst await statement \nto the point just after the second; i.e., when T1[await r] and T2[await r] are consecutively-occurring \nsynchronization points, we must determine whether (T1[skip] , 0) can reach (T2[skip] , 0). A careful \nanalysis of our reachability problem reveals it does not have the EXPSPACE-hard complexity of determining \nvector\u00adreachability in general, due to the special structure of our reachability query. We notice that \nbetween two synchronization points t1 and t2 of p, execution proceeds in two phases. In the .rst, post \nstatements made by p only increment the vector valuations. In the second phase, starting when the second \nawait statement is encountered, the await statement repeatedly consumes tasks, only decrementing the \nvector valuations the vector valuations can not be re-incremented again because of the local-scope restriction: \neach consumed task is forbidden from returning addition tasks. Due to this special structure, deciding \nreachability between t1 and t2 reduces to deciding if a particular integer linear program I(t1,t2) has \na solution. Since consuming tasks in the await-loop requires using the summaries computed for other procedures, \nwe consider a sequence A0A1 ... of non-recursive vector addition systems iteratively under\u00adapproximating \nthe recursive system AP . Initially A0 has only the transitions of AP corresponding to intra-procedural \nand post transitions of P . At each step i> 0, we add to Ai one of two edges types. One type is an additive \nprocedure-summary edge, used to describe a single task-consumption step of an await transition, r nj \n0 T [await r] '-.T [s; await r] , for some t0,tf . Tasks such that j = cn(r, t0), s . rvh(tf ), and \nsms(t0,tf , Ai-1)= \u00d8. The second possibility is an additive synchronization-point summary edge, summarizing \nan entire of se\u00adquence of program transitions between two synchronization points, 00 T1[skip] '-.T2[skip] \n, where T1[await r] ,T2[await r] . Tasks are consecutive synchro\u00adnization points occurring P , and 0 \n. sms(T1[skip] ,T2[skip] , AP ). The procedure-summary edges are computed using only .nite-state reachability \nbetween program states, using the synchronization\u00adpoint summary edges, while the synchronization-point \nsummary edges are computed by reduction to integer linear programming. As the number of possible edges \nis bounded polynomially in the pro\u00adgram size, the A0A1 sequence is guaranteed to reach a .xed-point Ak \nin a polynomial number of steps, though each step may take nondeterministic-polynomial time, in the worst \ncase, to compute solutions to integer linear programs. The reachable states of Ak are precisely the same \nreachable states of AP . Theorem 6. The state-reachability problem for local-scope multi\u00adwait single-region \n.nite-value programs is NP-complete.  7.2 Single-Region Multi-Wait Analysis Without the local-scoping \nrestriction, each execution of each pro\u00adcedure p . Procs between entry point t0 . Tasks and exit point \ntf . Tasks is summarized by the tasks posted between the last-encountered await statement, at a synchronization \npoint ts . Tasks (note that ts = t0 if no await statements are encoun\u00adtered), and a return statement, \nat the exit point tf . Since p can make recursive procedure calls between ts and tf , and each called \nprocedure can again return pending tasks, the possible sets of pend\u00ading tasks upon p s return at tf is \ndescribed by the Parikh-image3 of a context-free language L(t0,tf ). It turns out we can describe this \nimage as the set of vectors computed by a polynomially-sized vector addition system AL(t0,tf ) without \nrecursion and zero-test edges [14]. We use thus computations of AL(t0,tf ) to summarize the set of possible \nregion-valuations reached in an execution from t0 to tf . However, computing AL(t0,tf ) is not immediate, \nsince between t0 and the last-encountered synchronization point ts, exe\u00adcution of the given procedure \np may encounter await statements (necessarily so when t0 = ts). Since we use zero-test edges to express \nawait statements, we also need to summarize execution between synchronization points (i.e., between the \nprocedure entry point and among await statements) using only additive edges. To further complicate matters, \neach such summarization requires, in turn, the summaries AL(t0' ,t f ' ) computed for other procedures! \nWe break the circular dependence between procedure summaries and synchronization-point summaries by iteratively \ncomputing both. In particular, we compute a sequence AL 0 AL 1 ... of procedure summary vector addition \nsystems along with a sequence A0A1 ... of vector addition systems such that each ALi , for i> 0, is computed \nusing the transitions of Ai-1, and Ai, for i = 0 is computed using the procedure summaries of ALi . Initially \nAL 0 contains only the pending-task sets reachable without taking await transitions, and A0 contains \nonly the transitions of AP corresponding to intra\u00adprocedural and post transitions of P , along with transitions \nto components AL 0 . For i = 0, Ai contains transitions to and from the components ALi (t0,tf ) rnj0 \n00 T [await r] '-. (q0,T [skip]) qf ,T [skip] '-.T [s; await r] for each t0,tf . Tasks such that j = \ncn(r, t0), s . rvh(tf ), and q0 and qf are the unique initial and .nal states of ALi (t0,tf ). (We assume \neach component ALi (t0,tf ) has unique initial and .nal states, distinct from the states of other components. \nAdditionally, we equip each AL(t0,tf ) with auxiliary state to carry the identity T [skip] of the invoking \ntask to ensure the proper return of control when AL(t0,tf ) completes.) At each step i> 0, we add to \nAi an additive edge summarizing the execution between two synchronization points T1[await r] and T2[await \nr] occurring in P : 00 T1[skip] '-.T2[skip] such that T2[skip] is reachable in Ai-1 from T1[skip], i.e., \n0 . sms(T1[skip] ,T2[skip] , Ai-1). Note that when T [await r] is a synchronization point occurring in \nP , T [skip] refers to the program point immediately after the await statement. Since there are only \npolynomially-many such edges that can possibly be added, we are State-Reachability in Recursively Parallel \nPrograms result complexity language/feature Task-Passing general Thm. 1 undecidable futures, revisions \nSingle-Wait non-aliasing Thm. 2 PTIME futures , revisions local scope Thm. 3 EXPSPACE global scope Thm. \n4 EXPSPACE asynchronous programs general Thm. 5 2EXPTIME For programs without task-passing. Multi-Wait \n(single region) local scope Thm. 6 NP Cilk general Thm. 7 decidable async (X10) Figure 8. Summary of \nresults for computing state-reachability for .nite-value recursively parallel programs. guaranteed to \nreach a .xed-point Ak of A0A1 ... in a polynomial number of steps. Furthermore, the reachable states \nof Ak are precisely the same reachable states of AP . However, computing 0 . sms(t1,t2, Ai-1) at each \nstep is dif.cult due to the zero\u00adtest edge in the await statement immediately preceding t2; this is computationally \nequivalent to computing reachability of a particular vector in non-recursive vector addition systems. \nTheorem 7. The state-reachability problem for multi-wait single\u00adregion .nite-value programs is decidable. \nSince practical algorithms to compute vector-reachability is a dif.cult open problem, we remark that \nit is possible to obtain algorithms to approximate our state-reachability problem. Consider, for instance, \nthe over-approximate semantics given by transforming each await r statement into while * do ewait r. \nThough many more behaviors are present in the resulting program, since not every task is necessarily \nconsumed during the while loop, practical algorithmic solutions are more probable (see Section 5.4). \n8. Related Work Formal modeling and veri.cation of multi-threaded programs has been heavily studied, \nincluding but not limited to identifying decid\u00adable sub-classes [20], and effective over-approximate \n[13, 18] and under-approximate [9, 22] analyses. To our knowledge little work has been done in formal \nmodel\u00ading and veri.cation of programs written in explicitly-parallel lan\u00adguages which are free of thread \ninterleaving. Sen and Viswanathan [33] s asynchronous programs, which falls out as a special case of \nour single-wait programs, is perhaps most similar to our work in this regard. Practical veri.cation algorithms \nby combining itera\u00adtive over-and under-approximation [19], and in-depth complexity analysis [14] of asynchronous \nprograms have been studied. Though decidability results of abstract parallel models have been reported \n[5, 10] (Bouajjani and Esparza [4] survey of this line of work), these works target abstract computation \nmodels, and do not identify precise complexities and optimal algorithms for real-world parallel programming \nlanguages, nor do they handle the case where procedures can return unbounded sets of un.nished computations \nto their callers. 9. Conclusion We have proposed a general model of recursively parallel programs which \ncaptures the concurrency constructs in a variety of popular programming languages. By isolating the fragments \ncorresponding to various language features, we are able to associate correspond\u00ading formal models, measure \nthe complexity of state-reachability, and provide precise analysis algorithms. We hope our complexity \nmeasurements may be used to guide the design and choice of con\u00adcurrent programming languages and program \nanalyses. Figure 8 summarizes our results. Acknowledgments We greatly appreciate formative discussions \nwith Arnaud Sangnier and Peter Habermehl, and the feedback of Pierre Ganty, Giorgio Delzanno, Rupak Majumdar, \nTom Ball, Sebastian Burckhardt, and the anonymous POPL reviewers. References [1] P. A. Abdulla, K. Cerans, \nB. Jonsson, and Y.-K. Tsay. General decidability theorems for in.nite-state systems. In LICS 96: Proc. \n11th IEEE Symposium on Logic in Computer Science, pages 313 321. IEEE Computer Society, 1996. [2] E. \nAllen, D. Chase, V. Luchangco, J.-W. Maessen, S. Ryu, G. L. S. Jr., and S. Tobin-Hochstadt. The Fortress \nlanguage speci.cation. Technical report, Sun Microsystems, Inc., 2006. [3] A. Bouajjani and M. Emmi. \nAnalysis of recursively parallel programs. 2011. http://hal.archives-ouvertes.fr/hal-00639351/en. [4] \nA. Bouajjani and J. Esparza. Rewriting models of boolean programs. In RTA 06: Proc. 17th International \nConference on Term Rewriting and Applications, volume 4098 of LNCS, pages 136 150. Springer, 2006. [5] \nA. Bouajjani, M. M\u00a8 uller-Olm, and T. Touili. Regular symbolic analysis of dynamic networks of pushdown \nsystems. In CONCUR 05: Proc. 16th International Conference on Concurrency Theory, volume 3653 of LNCS, \npages 473 487. Springer, 2005. [6] S. Burckhardt, A. Baldassin, and D. Leijen. Concurrent programming \nwith revisions and isolation types. In OOPSLA 10: Proc. 25th Annual ACM SIGPLAN Conference on Object-Oriented \nProgramming, Systems, Languages, and Applications, pages 691 707. ACM, 2010. [7] P. Charles, C. Grothoff, \nV. A. Saraswat, C. Donawa, A. Kielstra, K. Ebcioglu, C. von Praun, and V. Sarkar. X10: an object-oriented \nap\u00adproach to non-uniform cluster computing. In OOPSLA 05: Proc. 20th Annual ACM SIGPLAN Conference on \nObject-Oriented Programming, Systems, Languages, and Applications, pages 519 538. ACM, 2005. [8] S. Demri, \nM. Jurdzinski, O. Lachish, and R. Lazic. The covering and boundedness problems for branching vector addition \nsystems. In FSTTCS 09: IARCS Annual Conference on Foundations of Software Technology and Theoretical \nComputer Science, pages 181 192, 2009. [9] J. Esparza and P. Ganty. Complexity of pattern-based veri.cation \nfor multithreaded programs. In POPL 11: Proc. 38th ACM SIGPLAN-SIGACT Symposium on Principles of Programming \nLanguages, pages 499 510. ACM, 2011. [10] J. Esparza and A. Podelski. Ef.cient algorithms for pre* and \npost* on interprocedural parallel .ow graphs. In POPL 00: Proc. 27th ACM SIGPLAN-SIGACT Symposium on \nPrinciples of Programming Languages, pages 1 11. ACM, 2000. [11] A. Finkel and P. Schnoebelen. Well-structured \ntransition systems everywhere! Theor. Comput. Sci., 256(1-2):63 92, 2001. [12] C. Flanagan and M. Felleisen. \nThe semantics of future and an application. J. Funct. Program., 9(1):1 31, 1999. [13] C. Flanagan and \nS. Qadeer. Thread-modular model checking. In SPIN 03: Proc. 10th International Workshop on Model Checking \nSoftware, volume 2648 of LNCS, pages 213 224. Springer, 2003. [14] P. Ganty and R. Majumdar. Algorithmic \nveri.cation of asynchronous programs. CoRR, abs/1011.0551, 2010. http://arxiv.org/abs/ 1011.0551. [15] \nG. Geeraerts, J.-F. Raskin, and L. V. Begin. Expand, enlarge and check: New algorithms for the coverability \nproblem of wsts. J. Comput. Syst. Sci., 72(1):180 203, 2006. [16] S. Graf and H. Sa\u00a8idi. Construction \nof abstract state graphs with PVS. In CAV 97: Proc. 9th International Conference on Computer Aided Veri.cation, \nvolume 1254 of LNCS, pages 72 83. Springer, 1997. [17] R. H. Halstead Jr. Multilisp: A language for concurrent \nsymbolic computation. ACM Trans. Program. Lang. Syst., 7(4):501 538, 1985. [18] T. A. Henzinger, R. Jhala, \nR. Majumdar, and S. Qadeer. Thread\u00admodular abstraction re.nement. In CAV 03: Proc. 15th International \nConference on Computer Aided Veri.cation, volume 2725 of LNCS, pages 262 274. Springer, 2003. [19] R. \nJhala and R. Majumdar. Interprocedural analysis of asynchronous programs. In POPL 07: Proc. 34th ACM \nSIGPLAN-SIGACT Sympo\u00adsium on Principles of Programming Languages, pages 339 350. ACM, 2007. [20] V. Kahlon. \nBoundedness vs. unboundedness of lock chains: Charac\u00adterizing decidability of pairwise CFL-reachability \nfor threads commu\u00adnicating via locks. In LICS 09: Proc. 24th Annual IEEE Symposium on Logic in Computer \nScience, pages 27 36. IEEE Computer Society, 2009. [21] D. Kozen. Lower bounds for natural proof systems. \nIn FOCS 77: Proc. 18th Annual Symposium on Foundations of Computer Science, pages 254 266. IEEE Computer \nSociety, 1977. [22] A. Lal and T. W. Reps. Reducing concurrent analysis under a context bound to sequential \nanalysis. Formal Methods in System Design, 35(1): 73 97, 2009. [23] J. R. Larus and R. Rajwar. Transactional \nMemory. Morgan &#38; Claypool, 2006. http://www.morganclaypool.com/doi/abs/10. 2200/S00070ED1V01Y200611CAC002. \n[24] E. A. Lee. The problem with threads. IEEE Computer, 39(5):33 42, 2006. [25] D. Leijen, W. Schulte, \nand S. Burckhardt. The design of a task parallel library. In OOPSLA 09: Proc. 24th Annual ACM SIGPLAN \nConference on Object-Oriented Programming, Systems, Languages, and Applications, pages 227 242. ACM, \n2009. [26] R. J. Lipton. The reachability problem requires exponential space. Technical Report 62, Yale \nUniversity, 1976. [27] P. Pratikakis, H. Vandierendonck, S. Lyberis, and D. S. Nikolopoulos. A programming \nmodel for deterministic task parallelism. In MSPC 11: Proc. 2011 ACM SIGPLAN Workshop on Memory Systems \nPerformance and Correctness, pages 7 12. ACM, 2011. [28] C. Rackoff. The covering and boundedness problems \nfor vector addition systems. Theor. Comput. Sci., 6:223 231, 1978. [29] G. Ramalingam. Context-sensitive \nsynchronization-sensitive analysis is undecidable. ACM Trans. Program. Lang. Syst., 22(2):416 430, 2000. \n[30] K. H. Randall. Cilk: Ef.cient Multithreaded Computing. PhD thesis, Department of Electrical Engineering \nand Computer Science, Massachusetts Institute of Technology, May 1998. [31] T. W. Reps, S. Horwitz, and \nS. Sagiv. Precise interprocedural data.ow analysis via graph reachability. In POPL 95: Proc. 22th ACM \nSIGPLAN-SIGACT Symposium on Principles of Programming Lan\u00adguages, pages 49 61. ACM, 1995. [32] C. Segulja \nand T. S. Abdelrahman. Synchronization-free and determin\u00adistic coarse-grain parallelism: Architectural \nsupport and programming model. In FASPP 11: Proc. First International Workshop on Future Architectural \nSupport for Parallel Programming, 2011. [33] K. Sen and M. Viswanathan. Model checking multithreaded \nprograms with asynchronous atomic methods. In CAV 06: Proc. 18th Interna\u00adtional Conference on Computer \nAided Veri.cation, volume 4144 of LNCS, pages 300 314. Springer, 2006.    \n\t\t\t", "proc_id": "2103656", "abstract": "<p>We propose a general formal model of isolated hierarchical parallel computations, and identify several fragments to match the concurrency constructs present in real-world programming languages such as Cilk and X10. By associating fundamental formal models (vector addition systems with recursive transitions) to each fragment, we provide a common platform for exposing the relative difficulties of algorithmic reasoning. For each case we measure the complexity of deciding state-reachability for finite-data recursive programs, and propose algorithms for the decidable cases. The complexities which include PTIME, NP, EXPSPACE, and 2EXPTIME contrast with undecidable state-reachability for recursive multi-threaded programs.</p>", "authors": [{"name": "Ahmed Bouajjani", "author_profile_id": "81100358502", "affiliation": "Universit&#233; Paris Diderot, Paris, France", "person_id": "P2991377", "email_address": "abou@liafa.jussieu.fr", "orcid_id": ""}, {"name": "Michael Emmi", "author_profile_id": "81333488438", "affiliation": "Universit&#233; Paris Diderot, Paris, France", "person_id": "P2991378", "email_address": "mje@liafa.jussieu.fr", "orcid_id": ""}], "doi_number": "10.1145/2103656.2103681", "year": "2012", "article_id": "2103681", "conference": "POPL", "title": "Analysis of recursively parallel programs", "url": "http://dl.acm.org/citation.cfm?id=2103681"}