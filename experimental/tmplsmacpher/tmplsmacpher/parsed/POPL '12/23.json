{"article_publication_date": "01-25-2012", "fulltext": "\n Static and User-Extensible Proof Checking Antonis Stampoulis Zhong Shao Department of Computer Science \nYale University New Haven, CT 06520, USA {antonis.stampoulis,zhong.shao}@yale.edu Abstract Despite recent \nsuccesses, large-scale proof development within proof assistants remains an arcane art that is extremely \ntime\u00adconsuming. We argue that this can be attributed to two profound shortcomings in the architecture \nof modern proof assistants. The .rst is that proofs need to include a large amount of minute detail; \nthis is due to the rigidity of the proof checking process, which can\u00adnot be extended with domain-speci.c \nknowledge. In order to avoid these details, we rely on developing and using tactics, specialized procedures \nthat produce proofs. Unfortunately, tactics are both hard to write and hard to use, revealing the second \nshortcoming of mod\u00adern proof assistants. This is because there is no static knowledge about their expected \nuse and behavior. As has recently been demonstrated, languages that allow type\u00adsafe manipulation of proofs, \nlike Beluga, Delphin and VeriML, can be used to partly mitigate this second issue, by assigning rich \ntypes to tactics. Still, the architectural issues remain. In this paper, we build on this existing work, \nand demonstrate two novel ideas: an extensible conversion rule and support for static proof scripts. \nTogether, these ideas enable us to support both user-extensible proof checking, and sophisticated static \nchecking of tactics, leading to a new point in the design space of future proof assistants. Both ideas \nare based on the interplay between a light-weight staging construct and the rich type information available. \nCategories and Subject Descriptors D.3.1 [Programming Lan\u00adguages]: Formal De.nitions and Theory General \nTerms Languages, Veri.cation 1. Introduction There have been various recent successes in using proof \nassistants to construct foundational proofs of large software, like a C com\u00adpiler [Leroy 2009] and an \nOS microkernel [Klein et al. 2009], as well as complicated mathematical proofs [Gonthier 2008]. Despite \nthis success, the process of large-scale proof development using the foundational approach remains a \ncomplicated endeavor that re\u00adquires signi.cant manual effort and is plagued by various architec\u00adtural \nissues. The big bene.t of using a foundational proof assistant is that the proofs involved can be checked \nfor validity using a very small proof checking procedure. The downside is that these proofs are Permission \nto make digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior speci.c permission and/or a fee. POPL 12, January \n25 27, 2012, Philadelphia, PA, USA. Copyright c &#38;#169; 2012 ACM 978-1-4503-1083-3/12/01. . . $10.00 \nvery large, since proof checking is .xed. There is no way to add domain-speci.c knowledge to the proof \nchecker, which would en\u00adable proofs that spell out less details. There is good reason for this, too: \nif we allowed arbitrary extensions of the proof checker, we could very easily permit it to accept invalid \nproofs. Because of this lack of extensibility in the proof checker, users rely on tactics: procedures \nthat produce proofs. Users are free to write their own tactics, that can create domain-speci.c proofs. \nIn fact, developing domain-speci.c tactics is considered to be good engineering when doing large developments, \nleading to signi.\u00adcantly decreased overall effort as shown, e.g. in Chlipala [2011]. Still, using and \ndeveloping tactics is error-prone. Tactics are essen\u00adtially untyped functions that manipulate logical \nterms, and thus tac\u00adtic programming is untyped. This means that common errors, like passing the wrong \nargument, or expecting the wrong result, are not caught statically. Exacerbating this, proofs contained \nwithin tactics are not checked statically, when the tactic is de.ned. Therefore, even if the tactic is \nused correctly, it could contain serious bugs that manifest only under some conditions. With the recent \nadvent of programming languages that sup\u00adport strongly typed manipulation of logical terms, such as Beluga \n[Pientka and Dun.eld 2008], Delphin [Poswolsky and Sch\u00fcrmann 2008] and VeriML [Stampoulis and Shao 2010], \nthis situation can be somewhat mitigated. It has been shown in Stampoulis and Shao [2010] that we can \nspecify what kinds of arguments a tactic expects and what kind of proof it produces, leading to a type-safe \nprogram\u00adming style. Still, this does not address the fundamental problem of proof checking being .xed \n users still have to rely on using tac\u00adtics. Furthermore, the proofs contained within the type-safe tactics \nare in fact proof-producing programs, which need to be evaluated upon invocation of the tactic. Therefore \nproofs within tactics are not checked statically, and they can still cause the tactics to fail upon invocation. \nIn this paper, we build on the past work on these languages, aiming to solve both of these issues regarding \nthe architecture of modern proof assistants. We introduce two novel ideas: support for an extensible \nconversion rule and static proof scripts inside tactics. The former technique enables proof checking \nto become user-extensible, while maintaining the guarantee that only logically sound proofs are admitted. \nThe latter technique allows for statically checking the proofs contained within tactics, leading to increased \nguarantees about their runtime behavior. Both techniques are based on the same mechanism, which consists \nof a light-weight staging construct. There is also a deep synergy between them, allowing us to use the \none to the bene.t of the other. Our main contributions are the following: First, we present what we believe \nis the .rst technique for hav\u00ading an extensible conversion rule, which combines the follow\u00ading characteristics: \nit is safe, meaning that it preserves logical soundness; it is user-extensible, using a familiar, generic \npro\u00adFigure 1. Checking proof scripts in various proof assistants  gramming model; and, it does not \nrequire metatheoretic addi\u00adtions to the logic, but can be used to simplify the logic instead. Second, \nbuilding on existing work for typed tactic development, we introduce static checking of the proof scripts \ncontained within tactics. This signi.cantly reduces the development effort required, allowing us to write \ntactics that bene.t from existing tactics and from the rich type information available.  Third, we show \nhow typed proof scripts can be seen as an alternative form of proof witness, which falls between a proof \nobject and a proof script. Receivers of the certi.cate are able to decide on the tradeoff between the \nlevel of trust they show and the amount of resources needed to check its validity.  In terms of technical \ncontributions, we present a number of tech\u00adnical advances in the metatheory of the aforementioned program\u00adming \nlanguages. These include a simple staging construct that is crucial to our development and a new technique \nfor variable rep\u00adresentation. We also show a condition under which static checking of proof scripts inside \ntactics is possible. Last, we have extended an existing prototype implementation with a signi.cant number \nof features, enabling it to support our claims, while also rendering its use as a proof assistant more \npractical. 2. Informal presentation Glossary of terms. We will start off by introducing some con\u00adcepts \nthat will be used throughout the paper. The .rst fundamental concept we will consider is the notion of \na proof object: given a derivation of a proposition inside a formal logic, a proof object is a term representation \nof this derivation. A proof checker is a program that can decide whether a given proof object is a valid \nderivation of a speci.c proposition or not. Proof objects are extremely ver\u00adbose and are thus hard to \nwrite by hand. For this reason, we use tactics: functions that produce proof objects. By combining tactics \ntogether, we create proof-producing programs, which we call proof scripts. If a proof script is evaluated, \nand the evaluation completes successfully, the resulting proof object can be checked using the original \nproof checker. In this way, the trusted base of the system is kept at the absolute minimum. The language \nenvironment where proof scripts and tactics are written and evaluated is called a proof assistant; evidently, \nit needs to include a proof checker. Checking proof objects. In order to keep the size of proof objects \nmanageable, many of the logics used for mechanized proof check\u00ading include a conversion rule. This rule \nis used implicitly by the proof checker to decide whether any two propositions are equiv\u00adalent; if it \ndetermines that they are indeed so, the proof of their equivalence can be omitted. We can thus think \nof it as a special tac\u00adtic that is embedded within the proof checker, and used implicitly. The more sophisticated \nthe relation supported by the conversion rule is, the simpler are proof objects to write, since more \ndetails can be omitted. On the other hand, the proof checker becomes more complicated, as does the metatheory \nproof showing the soundness of the associated logic. The choice in Coq [Barras et al. 2010], one of the \nmost widely used proof assistants, with respect to this trade-off, is to have a conversion rule that \nidenti.es propositions up to evaluation. Nevertheless, extended notions of conversion are desirable, \nleading to proposals like CoqMT [Strub 2010], where equivalence up to .rst-order theories is supported. \nIn both cases, the conversion rule is .xed, and extending it requires signi.cant amounts of work. It \nis thus not possible for users to extend it using their own, domain-speci.c tactics, and proof objects \nare thus bound to get large. This is why we have to resort to writing proof scripts. Checking proof scripts. \nAs mentioned earlier, in order to validate a proof script we need to evaluate it (see Fig. 1a); this \nis the modus operandi in proof assistants of the HOL family [Harrison 1996; Slind and Norrish 2008]. \nTherefore, it is easy to extend the checking procedure for proof scripts by writing a new tactic, and \ncalling it as part of a script. The price that this comes to is that there is no way to have any sort \nof static guarantee about the validity of the script, as proof scripts are completely untyped. This can \nbe somewhat mitigated in Coq by utilizing the static checking that it already supports: the proof checker, \nand especially, the conversion rule it contains (see Fig. 1b). We can employ proof objects in our scripts; \nthis is especially useful when the proof objects are trivial to write but trigger complex conversion \nchecks. This is the essential idea behind techniques like proof-by-re.ection [Boutin 1997], which lead \nto more robust proof scripts. In previous work [Stampoulis and Shao 2010] we introduced VeriML, a language \nthat enables programming tactics and proof scripts in a typeful manner using a general-purpose, side-effectful \nprogramming model. Combining typed tactics leads to typed proof scripts. These are still programs producing \nproof objects, but the proposition they prove is carried within their type. Information about the current \nproof state (the set of hypotheses and goals) is also available statically at every intermediate point \nof the proof script. In this way, the static assurances about proof scripts are signi.cantly increased \nand many potential sources of type errors are removed. On the other hand, the proof objects contained \nwithin the scripts are still checked using a .xed proof checker; this ultimately means that the set of \npossible static guarantees is still .xed. Extensible conversion rule. In this paper, we build on our \nearlier work on VeriML. In order to further increase the amount of static checking of proof scripts that \nis possible within this language, we propose the notion of an extensible conversion rule (see Fig. 1c). \nIt enables users to write their own domain-speci.c conversion checks  Figure 2. Staging in VeriML that \nget included in the conversion rule. This leads to simpler proof scripts, as more parts of the proof \ncan be inferred by the conversion rule and can therefore be omitted. Also, it leads to increased static \nguarantees for proof scripts, since the conversion checks happen before the rest of the proof script \nis evaluated. The way we achieve this is by programming the conversion checks as type-safe tactics within \nVeriML, and then evaluating them statically using a simple staging mechanism (see Fig. 2). The type of \nthe conversion tactics requires that they produce a proof ob\u00adject which proves the claimed equivalence \nof the propositions. In this way, type safety of VeriML guarantees that soundness is main\u00adtained. At \nthe same time, users are free to extend the conversion rule with their own conversion tactics written \nin a familiar program\u00adming model, without requiring any metatheoretic additions or ter\u00admination proofs. \nSuch proofs are only necessary if decidability of the extra conversion checks is desired. Furthermore, \nthis approach allows for metatheoretic reductions as the original conversion rule can be programmed within \nthe language. Thus it can be removed from the logic, and replaced by the simpler notion of explicit equal\u00adities, \nleading to both simpler metatheory and a smaller trusted base. Checking tactics. The above approach addresses \nthe issue of being able to extend the amount of static checking possible for proof scripts. But what \nabout tactics? Our existing work on VeriML shows how the increased type information addresses some of \nthe issues of tactic development using current proof assistants, where tactics are programmed in a completely \nuntyped manner. Still, if we consider the case of tactics more closely, we will see that there is a limitation \nto the amount of checking that is done statically, even using this language. When programming a new tactic, \nwe would like to reuse existing tactics to produce the required proofs. Therefore, rather than writing \nproof objects by hand inside the code of a tactic, we would rather use proof scripts. The issue is that \nin order to check whether the contained proof scripts are valid, they need to be evaluated but this \nonly happens when an invocation of the tactic reaches the point where the proof script is used. Therefore, \nthe static guarantees that this approach provides are severely limited by the fact that the proof scripts \ninside the tactics cannot be checked statically, when the tactic is de.ned. Static proof scripts. This \nis the second fundamental issue we ad\u00address in this paper. We show that the same staging construct uti\u00adlized \nfor introducing the extensible conversion rule, can be lever\u00adaged to perform static proof checking for \ntactics. The crucial point of our approach is the proof of existence of a transformation be\u00adtween proof \nobjects, which suggests that under reasonable condi\u00adtions, a proof script contained within a tactic can \nbe transformed into a static proof script. This static script can then be evaluated at tactic de.nition \ntime, to be checked for validity. Last, we will show that this approach lends itself well to writing \nextensions of the conversion rule. We show that we can create a lay\u00adering of conversion rules: using \na basic conversion rule as a starting point, we can utilize it inside static proof scripts to implicitly \nprove the required obligations of a more advanced version, and so on. This minimizes the required user \neffort for writing new conversion rules, and enables truly modular proof checking. t ::= proof object \nconstructors | propositions | natural numbers, lists, etc. | sorts and types | X/s F ::= | F, x : tT \n::=[F]t . ::= | ., X : T s ::= | s, x . t main judgement: .; F f t : t' (type of a logical term) Figure \n3. Assumptions about the logic language 3. Our toolbox In this section, we will present the essential \ningredients that are needed for the rest of our development. The main requirement is a language that \nsupports type-safe manipulation of terms of a par\u00adticular logic, as well as a general-purpose programming \nmodel that includes general recursion and other side-effectful operations. Two recently proposed languages \nfor manipulating LF terms, Beluga [Pientka and Dun.eld 2008] and Delphin [Poswolsky and Sch\u00fcr\u00admann 2008], \n.t this requirement, as does VeriML [Stampoulis and Shao 2010], which is a language used to write type-safe \ntactics. Our discussion is focused on the latter, as it supports a richer ML-style calculus compared \nto the others, something useful for our purposes. Still, our results apply to all three. We will now \nbrie.y describe the constructs that these languages support, as well as some new extensions that we propose. \nThe interested reader can read more about these constructs in Sec. 6 and in our technical report [Stampoulis \nand Shao 2012]. A formal logic. The computational language we are presenting is centered around manipulation \nof terms of a speci.c formal logic. We will see more details about this logic in Sec. 4. For the time \nbeing, it will suf.ce to present a set of assumptions about the syn\u00adtactic classes and typing judgements \nof this logic, shown in Fig. 3. Logical terms are represented by the syntactic class t, and include proof \nobjects, propositions, terms corresponding to the domain of discourse (e.g. natural numbers), and the \nneeded sorts and type con\u00adstructors to classify such terms. Their variables are assigned types through \nan ordered context F. A package of a logical term t to\u00adgether with the variables context it inhabits \nF is called a contex\u00adtual term and denoted as T =[F]t. Our computational language works over contextual \nterms for reasons that will be evident later. The logic incorporates such terms by allowing them to get \nsubsti\u00adtuted for meta-variables X, using the constructor X/s. When a term T =[F']t gets substituted for \nX, we go from the F' context to the current context F using the substitution s. Logical terms are classi.ed \nusing other logical terms, based on the normal variables environment F, and also an environment . that \ntypes meta-variables, thus leading to the .; F f t : t' judge\u00adment. For example, a term t representing \na closed proposition will be typed as ; f t : Prop, while a proof object tpf proving that proposition \nwill satisfy the judgement ; f tpf : t. ML-style functional programming. We move on to the compu\u00adtational \nlanguage. As its main core, we assume an ML-style func\u00adtional language, supporting general recursion, \nalgebraic data types and mutable references (see Fig. 4). Terms of this fragment are typed under a computational \nvariables environment G and a store typing environment S, mapping mutable locations to types. Typing \njudgements are entirely standard, leading to a S; G f e : t judge\u00adment for typing expressions. Dependently-typed \nprogramming over logical terms. As shown in Fig. 5, the .rst important additions to the ML computational \ncore are constructs for dependent functions and products over contextual terms T . Abstraction over contextual \nterms is denoted as .X : T.e. It has the dependent function type (X : T ) . t. The type is dependent \nsince the introduced logical term might be used as the type of  k ::= *| k1 . k2 t ::= unit | int | \nbool | t1 . t2 | t1 + t2 | t1 \u00d7 t2 | \u00b5a : k.t |.a : k.t | a | array t | .a : k.t | t1 t2 | \u00b7\u00b7\u00b7 e ::= \n() | n | e1 + e2 | e1 = e2 | true | false | if e then e1 elsee2 | .x : t.e | e1 e2 | (e1, e2) | projie \n| injie | case(e, x1.e1, x2.e2) | fold e | unfold e | .a : k.e | e t '' | .x x : t.e | mkarray(e,e ' \n) | e[e ' ] | e[e ' ] := e | l | error | \u00b7\u00b7\u00b7 G ::= | G, x : t | G, a : k S ::= | S, l : array t Figure \n4. Syntax for the computational language (ML fragment) t ::= \u00b7\u00b7\u00b7 | (X : T ) . t | (X : T ) \u00d7 t | (f : \nctx) . t e ::= \u00b7\u00b7\u00b7 | .X : T.e | eT | .f : ctx.e | e F |(T, e) ' | let (X, x) = e in e | holcase T return \nt of (T1 . e1) \u00b7\u00b7\u00b7(Tn . en) | ctxcase F return t of (F1 . e1)\u00b7\u00b7\u00b7(Fn . en) Figure 5. Syntax for the computational \nlanguage (logical term constructs) another term. An example would be a function that receives a proposition \nplus a proof object for that proposition, with type: (P : Prop) . (X : P) . t. Dependent products that \npackage a contextual logical term with an expression are introduced through the (T, e) construct and \neliminated using let (X, x) = e in e '; their type is denoted as (X : T ) \u00d7 t. Especially for packages \nof proof objects with the unit type, we introduce the syntax LT(T ). Last, in order to be able to support \nfunctions that work over terms in any context, we introduce context polymorphism, through a similarly \ndependent function type over contexts. With these in mind, we can de.ne a simple tactic that gets a packaged \nproof of a universally quanti.ed formula, and an instantiation term, and returns a proof of the instantiated \nformula as follows: instantiate : (f : ctx, T : [f]Type, P : [f, x : T ]Prop, a : [f]T ) . LT([f] .x \n: T,P) . LT([f]P/[idf, a]) instantiate f T Pa pf = let (H) = pf in (Ha) From here on, we will omit details \nabout contexts and substitutions in the interest of presentation. Pattern matching over terms. The most \nimportant new construct that VeriML supports is a pattern matching construct over logical terms denoted \nas holcase. This construct is used for dependent matching of a logical term against a set of patterns. \nThe return clause speci.es its return type; we omit it when it is easy to infer. Patterns are normal \nterms that include uni.cation variables, which can be present under binders. This is the essential reason \nwhy contextual terms are needed. Pattern matching over environments. For the purposes of our de\u00advelopment, \nit is very useful to support one more pattern matching construct: matching over logical variable contexts. \nWhen trying to construct a certain proof, the logical environment represents what the current proof context \nis: what the current logical hypotheses at hand are, what types of terms have been quanti.ed over, etc. \nBy be\u00ading able to pattern match over the environment, we can look up things in our current set of hypotheses, \nin order to prove further propositions. We can thus view the current environment as repre\u00adsenting a simple \nform of the current proof state; the pattern match\u00ading construct enables us to manipulate it in a type-safe \nmanner. One example is an assumption tactic, that tries to prove a proposition by searching for a matching \nhypotheses in the context: assumption : (f : ctx,P : Prop) . option LT(P) assumption f P = ctxcase f \nof f' , H : P . return (H) | f' , _ . assumption f' P Proof object erasure semantics (new feature). The \nonly con\u00adstruct that can in.uence the evaluation of a program based on the structure of a logical term \nis the pattern matching construct. For our purposes, pattern matching on proof objects is not necessary \n we never look into the structure of a completed proof. Thus we can have the typing rules of the pattern \nmatching construct speci.cally disallow matching on proof objects. In that case, we can de.ne an alternate \noperational semantics for our language where all proof objects are erased before using the original small-step \nreduction rules. Because of type safety, these proof-erasure semantics are guaranteed to yield equivalent \nresults: even if no proof objects are generated, they are still bound to exist. Implicit arguments. Let \nus consider again the instantiate func\u00adtion de.ned earlier. This function expects .ve arguments. From \nits type alone, it is evident that only the last two arguments are strictly necessary. The last argument, \ncorresponding to a proof expression for the proposition .x : T,P, can be used to reconstruct exactly \nthe arguments f, T and P. Furthermore, if we know what the result\u00ading type of a call to the function \nneeds to be, we can choose even the instantiation argument a appropriately. We employ a simple in\u00adferrence \nmechanism so that such arguments are omitted from our programs. This feature is also crucial in our development \nin order to implicitly maintain and utilize the current proof state within our proof scripts. Minimal \nstaging support (new feature). Using the language we have seen so far we are able to write powerful tactics \nusing a general-purpose programming model. But what if, inside our pro\u00adgrams, we have calls to tactics \nwhere all of their arguments are constant? Presumably, those tactic calls could be evaluated to proof \nobjects prior to tactic invocation. We could think of this as a form of generalized constant folding, \nwhich has one intriguing bene.t: we can tell statically whether the tactic calls succeed or not. This \npaper is exactly about exploring this possibility. Towards this effect, we introduce a rudimentary staging \nconstruct in our computational language. This takes the form of a letstatic construct, which binds a \nstatic expression to a variable. The static expression is evaluated during stage one (see Fig. 2), and \ncan only depend on other static expressions. Details of this construct are presented in Fig. 11d and \nalso in Sec. 6. After this addition, expressions in our language have a three-phase lifetime, that are \nalso shown in Fig. 2. - type-checking, where the well-formedness of expressions ac\u00adcording to the rules \nof the language is checked, and inference of implicit arguments is performed - static evaluation, where \nexpressions inside letstatic are reduced to values, yielding a residual expression - run-time, where \nthe residual expression is evaluated 4. Extensible conversion rule With these tools at hand, let us now \nreturn to the .rst issue that motivates us: the fact that proof checking is rigid and cannot be extended \nwith user-de.ned procedures. As we have said in our in\u00adtroduction, many modern proof assistants are based \non logics that include a conversion rule. This rule essentially identi.es proposi\u00ad  CONVERSION d .\u00dfN \nd' d =\u00dfN d' (sorts) s ::= Type | Type ' (kinds) K ::= Prop | Nat | K1 . K2 (props.) P ::= P1 . P2 |.x \n: K.P | x | True | False | P1 . P2 | \u00b7\u00b7\u00b7 (dom.obj.) d ::= Zero | Succ d | P | \u00b7\u00b7\u00b7 (proof objects) p ::= \nx | .x : P.p | p1 p2 | .x : K.p | p d | \u00b7\u00b7\u00b7 (HOL terms) t ::= s | K | P | d | p . ELIM . INTRO .; F f \np : P . P ' .; F, x : P f p : P ' .; F f p' : P Selected rules: .; F f .x : P.p : P . P ' .; F f pp' \n: P ' Figure 6. Syntax and selected rules of the logic language .HOL .; F fc p : PP =\u00dfN P ' .; F fc p \n: P ' (.x : K.d) d'.\u00dfN d[d' /x] natElimK dz ds zero .\u00dfN dz natElimK dz ds (succ d) .\u00dfN ds d (natElimK \ndz ds d) is the compatible, re.exive, symmetric and transitive closure of d .\u00dfN d' Figure 7. Extending \n.HOL with the conversion rule (.HOLc) tions up to some equivalence relation: usually this is equivalence \nup to partial evaluation of the functions contained within propositions. The supported relation is decided \nwhen the logic is designed. Any extension to this relation requires a signi.cant amount of work, both \nin terms of implementation, and in terms of metatheoretic proof required. This is evidenced by projects \nthat extend the con\u00adversion rule in Coq, such as Blanqui et al. [1999] and Strub [2010]. Even if user \nextensions are supported, those only take the form of .rst-order theories. Can we do better than this, \nenabling arbitrarily complex user extensions, written with the full power of ML, yet maintaining soundness? \nIt turns out that we can: this is the subject of this section. The key idea is to recognize that the \nconversion rule is essentially a tactic, embedded within the type checker of the logic. Calls to this \ntactic are made implicitly as part of checking a given proof object for validity. So how can we support \na .exible, extensible alternative? Instead of hardcoding a conversion tactic within the logic type checker, \nwe can program a type-safe version of the same tactic within VeriML, with the requirement that it provides \nproof of the claimed equivalence. Instead of calling the conversion tactic as part of proof checking, \nwe use staging to call the tactic statically after (VeriML) type checking, but before runtime execution. \nThis can be viewed as a second, potentially non-terminating proof checking stage. Users are now free \nto write their own conversion tactics, extending the static checking available for proof objects and \nproof scripts. Still, soundness is maintained, since full proof objects in the original logic can always \nbe constructed. As an example, we have extended the conversion rule that we use by a congruence closure \nprocedure, which makes use of mutable data structures, and by an arithmetic simpli.cation procedure. \n 4.1 Introducing: the conversion rule First, let us present what the conversion rule really is in more \ndetail. We will base our discussion on a simple type-theoretic higher-order .; F fed1: K .; F fcd2: K \n.; F fed : K .; F fed1 = d2: Prop .; F fe re. d : d = d .; F, x : K feP : Prop .; F fed1: K .; F fe p \n: P[d1/x] .; F fe p' : d1 = d2 .; F fe leibniz (.x : K.P) pp' : P[d2/x] .; F, x : K fe p : d1 = d2 .; \nF fe lamEq (.x : K.p) : (.x : K.d1)=(.x : K.d2) .; F, x : K fe p : d1 = d2 .; F fed1: Prop .; F fe forallEq \n(.x : K.p) : (.x : K.d1)=(.x : K.d2) .; F, x : K fed : K ' .; F fed ' : K .; F fe betaEq (.x : K.d) \nd ' : (.x : K.d) d ' = d[d ' /x] Axioms assumed: natElimBaseK natElimStepK : : . fz.. fs.natE. fz.. \nfs..n. limK fz fs zero = natElimK fz fs (sufs n (natElimK fz cc fz n) = fs n) Figure 8. Extending .HOL \nwith explicit equality (.HOLe) logic, based on the .HOL logic as described in Barendregt and Geuvers \n[1999], and used in our original work on VeriML [Stam\u00adpoulis and Shao 2010]. We can think of such a logic \ncomposed by the following broad classes: the objects of the domain of discourse d, which are the objects \nthat the logic reasons about, such as natural numbers and lists; their classi.ers, the kinds K (classi.ed \nin turn by sorts s); the propositions P; and the derivations, which prove that a certain proposition \nis true. We can represent derivations in a linear form as terms p in a typed lambda-calculus; we call \nsuch terms proof objects, and their types represent propositions in the logic. Checking whether a derivation \nis a valid proof of a certain proposition amounts to type-checking its corresponding proof ob\u00adject. Some \ndetails of this logic are presented in Fig. 6; the interested reader can .nd more information about it \nin the above references and in our technical report [Stampoulis and Shao 2012]. In Fig. 6, we show what \nthe conversion rule looks like for this logic: it is a typing judgement that effectively identi.es proposi\u00adtions \nup to an equivalence relation, with respect to checking proof objects. We call this version of the logic \n.HOLc and use fc to denote its entailment relation. The equivalence relation we con\u00adsider in the conversion \nrule is evaluation up to \u00df-reductions and uses of primitive recursion of natural numbers, denoted as \nnatElim. In this way, trivial arguments based on this notion of computa\u00adtion alone need not be witnessed, \nas for example is the fact that (Succ x)+ y = Succ (x + y) when the addition function is de.ned by primitive \nrecursion on the .rst argument. Of course, this is only a very basic use of the conversion rule. It is \npossible to omit larger proofs through much more sophisticated uses. This leads to simpler proofs and \nsmaller proof objects. Still, when using this approach, the choice of what relation is supported by the \nconversion rule needs to be made during the de.\u00adnition of the logic. This choice permeates all aspects \nof the metathe\u00adory of the logic. It is easy to see why, even with the tiny fragment of logic we have \nintroduced. Most typing rules for proof objects in the logic are similar to the rules .INTRO and .ELIM: \nthey are syntax-directed. This means that upon seeing the associated proof object constructor, like .x \n: P.p in the case of .INTRO, we can di\u00adrectly tell that it applies. If all rules were syntax directed, \nit would  \u00dfNequal : (f : ctx,T : Type,t1: T,t2: T ) . option LT(t1 = t2) \u00dfNequal f Tt1 t2 = holcase \nwhnf f Tt1, whnf f Tt2 of ((ta : T '. T ) tb),(tc td ) . do (pf1). \u00dfNequal f (T '. T ) ta tc (pf1). \u00dfNequal \nf T ' tb td return (\u00b7\u00b7\u00b7 proof of ta tb = tc td \u00b7\u00b7\u00b7)| (ta . tb),(tc . td ) . do (pf1). \u00dfNequal f Prop \nta tc (pf1). \u00dfNequal f Prop tb td return (\u00b7\u00b7\u00b7 proof of ta . tb = tc . td \u00b7\u00b7\u00b7)| (.x : T.t1), (.x : T.t2) \n. do (pf). \u00dfNequal [f, x : T ] Prop t1 t2 return (\u00b7\u00b7\u00b7 proof of .x : T.t1 = .x : T.t2 \u00b7\u00b7\u00b7)| t1,t1 . do \nreturn (\u00b7\u00b7\u00b7 proof of t1 = t1 \u00b7\u00b7\u00b7)| t1,t2 . None requireEqual : (f : ctx, T : Type,t1: T,t2: T ).LT(t1 \n= t2) requireEqual f Tt1 t2 = match \u00dfNequal f Tt1 t2 with Some x . x | None . error Figure 9. VeriML \ntactic for checking equality up to \u00df-conversion be entirely simple to prove that the logic is sound by \nan inductive argument: essentially, since no proof constructor for False exists, there is no valid derivation \nfor False. In this logic, the only rule that is not syntax directed is exactly the conversion rule. Therefore, \nin order to prove the soundness of the logic, we have to show that the conversion rule does not some\u00adhow \nintroduce a proof of False. This means that proving the sound\u00adness of the logic passes essentially through \nthe speci.c relation we have chosen for the conversion rule. Therefore, this approach is foundationally \nlimited from supporting user extensions, since any new extension would require a new metatheoretic result \nin order to make sure that it does not violate logical soundness.  4.2 Throwing conversion away Since \nhaving a .xed conversion rule is bound to fail if we want it to be extensible, what choice are we left \nwith, but to throw it away? This radical sounding approach is what we will do here. We can replace the \nconversion rule by an explicit notion of equality, and provide explicit proof witnesses for rewriting \nbased on that equality. Essentially, all the points where the conversion rule was alluded to and proofs \nwere omitted, need now be replaced by proof objects witnessing the equivalence. Some details for the \nadditions required to the base .HOL logic are shown in Fig. 8, yielding the .HOLe logic. There are good \nreasons for choosing this version: .rst, the proof checker is as simple as possible, and does not need \nto include the conversion checking routine. We could view this routine as performing proof search over \nthe replacement rules, so it necessarily is more complicated, especially since it needs to be relatively \nef.cient. Also, the metatheory of the logic itself can be simpli.ed. Even when the conversion rule is \nsupported, the metatheory for the associated logic is proved through the explicit equality approach; \nthis is because model construction for a logic bene.ts from using explicit equality [Siles and Herbelin \n2010]. Still, this approach has a big disadvantage: the proof objects soon become extremely large, since \nthey include painstakingly de\u00adtailed proofs for even the simplest of equivalences. This precludes their \nuse as independently checkable proof certi.cates that can be sent to a third party. It is possible that \nthis is one of the reasons why systems based on logics with explicit equalities, such as HOL4 whnf : \n(f : ctx,T : Type,t : T ) . (t ' : T ) \u00d7 LT(t = t ' ) whnf f Tt = holcase t of (t1: T '. T )(t2: T ' \n) . 't let t1' , pf1 = whnf f (T '. T ) t1 in holcase t1 ' of ' .x : T .tf .([f]tf /[idF,t2],\u00b7\u00b7\u00b7)' | \nt1 .([f]t1 ' t2,\u00b7\u00b7\u00b7) | natElimK fz fsn . ' let (n , pf1) = whnf f Nat n in holcase n ' of zero .([f] \nfz,\u00b7\u00b7\u00b7)' | succ n .([f] fs n ' (natElimK fz fsn ' ),\u00b7\u00b7\u00b7)'' | n .([f]natElimK fz fsn ,\u00b7\u00b7\u00b7) | t .(t, \u00b7\u00b7\u00b7) \n Figure 10. VeriML tactic for rewriting to weak head-normal form [Slind and Norrish 2008] and Isabelle/HOL \n[Nipkow et al. 2002], do not generate proof objects by default.  4.3 Getting conversion back We will \nnow see how it is possible to reconcile the explicit equality based approach with the conversion rule: \nwe will gain the conver\u00adsion rule back, albeit it will remain completely outside the logic. Therefore \nwe will be free to extend it, all the while without risking introducing unsoundness in the logic, since \nthe logic remains .xed (.HOLe as presented above). We do this by revisiting the view of the conversion \nrule as a special trusted tactic, through the tools presented in the previous section. First, instead \nof hardcoding a conversion tactic in the type checker, we program a type-safe conversion tactic, utilizing \nthe features of VeriML. Based on typing alone we require that it returns a valid proof of the claimed \nequivalences: \u00dfNequal : (f : ctx, T : Type, t : T, t ' : T ) . option LT(t = t ' ) Second, we evaluate \nthis tactic under proof erasure semantics. This means that no proof objects are produced, leading to \nthe same space gains as the original conversion rule. Third, we use the staging construct in order to \ncheck conversion statically. Details. We now present our approach in more detail. First, in Fig. 9, we \nshow a sketch of the code behind the type-safe conver\u00adsion check tactic. It works by .rst rewriting its \ninput terms into weak head-normal form, via the whnf function in Fig. 10, and then recursively checking \ntheir subterms for equality. In the equivalence checking function, more cases are needed to deal with \nquanti.\u00adcation; while in the rewriting procedure, a recursive call is miss\u00ading, which would complicate \nour presentation here. We also de\u00ad.ne a version of the tactic that raises an error instead of returning \nan option type if we fail to prove the terms equal, which we call requireEqual. The full details can \nbe found in our implementation. The code of the \u00dfNequal tactic is in fact entirely similar to the code \none would write for the conversion check routine inside a logic type checker, save for the extra types \nand proof objects. It therefore follows trivially that everything that holds for the standard implementation \nof the conversion check also holds for this code: e.g. it corresponds exactly to the =\u00dfN relation as \nde.ned in the logic; it is bound to terminate because of the strong normalization theorem for this relation; \nand its proof-erased version is at least as trustworthy as the standard implementation. Furthermore, \ngiven this code, we can produce a form of typed proof scripts inside VeriML that correspond exactly to \nproof objects in the logic with the conversion rule, both in terms of their actual code, and in terms \nof the steps required to validate them. This is done by constructing a proof script in VeriML by induction \non the derivation of the proof object in .HOLc, replacing each proof object constructor by an equivalent \nVeriML tactic as follows:  constructor to tactic of type .x : P.p Assume e LT([f, H : P]P' ) . LT(P \n. P' ) p1 p2 Apply e1 e2 LT(P . P' ) . LT(P) . LT(P' ) .x : K.p Intro e LT([f, x : T ] P' ) . LT(.x : \nT,P' ) p d Inst e a LT(.x : T,P) . (a : T ) . LT(P/[id, a]) c Lift c (H : P) . LT(P) (conversion) Conversion \nLT(P) . LT(P = P' ) . LT(P' ) Here we have omitted the current logical environment f; it is maintained \nthrough syntactic means as discussed in Sec. 7 and through type inference. The only subtle case is conversion. \nGiven the transformed proof e for the proof object p contained within a use of the conversion rule, we \ncall the conversion tactic as follows: letstatic pf = requireEqual PP ' in Conversion e pf The arguments \nto requireEqual can be easily inferred, making cru\u00adcial use of the rich type information available. Conversion \ncould also be used implicitly in the other tactics. Thus the resulting ex\u00adpression looks entirely identical \nto the original proof object. Correspondence with original proof object. In order to elucidate the correspondence \nbetween the resulting proof script expression and the original proof object, it is fruitful to view the \nproof script as a proof certi.cate, sent to a third party. The steps required to check whether it constitutes \na valid proof are the following. First, the whole expression is checked using the type checker of the \ncom\u00adputational language. Then, the calls to the requireEqual function are evaluated during stage one, \nusing proof erasure semantics. We ex\u00adpect them to be successful, just as we would expect the conversion \nrule to be applicable when it is used. Last, the rest of the tactics are evaluated; by a simple argument, \nbased on the fact that they do not use pattern matching or side-effects, they are guaranteed to ter\u00adminate \nand produce a proof object in .HOLe. This validity check is entirely equivalent to the behavior of type-checking \nthe .HOLc proof object, save for pushing all conversion checks towards the end.  4.4 Extending conversion \nat will In our treatment of the conversion rule we have so far focused on regaining the \u00dfN conversion \nin our framework. Still, there is nothing con.ning us to supporting this conversion check only. As long \nas we can program a conversion tactic in VeriML that has the right type, it can safely be made part of \nour conversion rule. For example, we have written an eufEqual function, which checks terms for equivalence \nbased on the equality with uninter\u00adpreted functions decision procedure. It is adapted from our previous \nwork on VeriML [Stampoulis and Shao 2010]. This equivalence checking tactic isolates hypotheses of the \nform d1 = d2 from the current context, using the newly-introduced context matching sup\u00adport. Then, it \nconstructs a union-.nd data structure in order to form equivalence classes of terms. Based on this structure, \nand using code similar to \u00dfNequal (recursive calls on subterms), we can de\u00adcide whether two terms are \nequal up to simple uses of the equality hypotheses at hand. We have combined this tactic with the original \n\u00dfNequal tactic, making the implicit equivalence supported similar to the one in the Calculus of Congruent \nConstructions [Blanqui et al. 2005]. This demonstrates the .exibility of this approach: equivalence checking \nis extended with a sophisticated decision procedure, which is programmed using its original, imperative \nfor\u00admulation. We have programmed both the rewriting procedure and the equality checking procedure in \nan extensible manner, so that we can globally register further extensions.  4.5 Typed proof scripts \nas certi.cates Earlier we discussed how we can validate the proof scripts resulting from turning the \nconversion rule into explicit tactic calls. This discussion shows an interesting aspect of typed proof \nscripts: they can be viewed as a proof witness that is a .exible compromise between untyped proof scripts \nand proof objects. When a typed proof script consists only of static calls to conversion tactics and \nuses of total tactics, it can be thought of as a proof object in a logic with the corresponding conversion \nrule. When it also contains other tactics, that perform potentially expensive proof search, it corresponds \nmore closely to an untyped proof script, since it needs to be fully evaluated. Still, we are allowed \nto validate parts of it statically. This is especially useful when developing the proof script, because \nwe can avoid the evaluation of expensive tactic calls while we focus on getting the skeleton of the proof \ncorrect. Using proof erasure for evaluating requireEqual is only one of the choices the receiver of such \na proof certi.cate can make. Another choice would be to have the function return an actual proof object, \nwhich we can check using the .HOLe type checker. In that case, the VeriML interpreter does not need to \nbecome part of the trusted base of the system. Last, the safest possible choice would be to avoid doing \nany evaluation of the function, and ask the proof certi.cate provider to do the evaluation of requireEqual \nthemselves. In that case, no evaluation of computational code would need to happen at the proof certi.cate \nreceiver s side. This mitigates any concerns one might have for code execution as part of proof validity \nchecking, and guarantees that the small .HOLe type checker is the trusted base in its entirety. Also, \nthe receiver can decide on the above choices selectively for different conversion tactics e.g. use proof \nerasure for \u00dfNequal but not for eufEqual, leading to a trusted base identical to the .HOLc case. This \nmeans that the choice of the conversion rule rests with the proof certi.cate receiver and not with the \ndesigner of the logic. Thus the proof certi.cate receiver can choose the level of trust they require \nat will. 5. Static proof scripts In the previous section, we have demonstrated how proof checking for \ntyped proof scripts can be made user-extensible, through a new treatment of the conversion rule. It makes \nuse of user-de.ned, type-safe tactics, which are evaluated statically. The question that remains is what \nhappens with respect to proofs within tactics. If a proof script is found within a tactic, must we wait \nuntil that evaluation point is reached to know whether the proof script is correct or not? Or is there \na way to check this statically, as soon as the tactic is de.ned? In this section we show how this is \npossible to do in VeriML using the staging construct we have introduced. Still, in this case matters \nare not as simple as evaluating certain expressions statically rather than dynamically. The reason is \nthat proof scripts contained within tactics mention uninstantiated meta-variables, and thus can\u00adnot be \nevaluated through staging. We resolve this by showing the existence of a transformation, which collapses \nlogical terms from an arbitrary meta-variables context into the empty one. We will focus on the case \nof developing conversion routines, similar to the ones we saw earlier. The ideas we present are gen\u00aderally \napplicable when writing other types of tactics as well; we focus on conversion routines in order to demonstrate \nthat the two main ideas we present in this paper can work in tandem. A rewriter for plus. We will consider \nthe case of writing a rewriter similar to whnf for simplifying expressions of the form x + y, depending \non the second argument. The addition function is de.ned by induction on the .rst argument, as follows: \n(+) = .x..y.natElimNat y (.p..r.Succ r) x  In order for rewriters to be able to use existing as well \nas future rewriters to perform their recursive calls, we write them in the open recursion style they \nreceive a function of the same type that corresponds to the current rewriter. The code looks as follows: \nrewriterType =(f : ctx,T : Type,t : T ) . (t ' : T ) \u00d7 LT(t = t ' ) plusRewriter1 : rewriterType . rewriterType \nplusRewriter1 recursive f Tt = holcase t with x + y . ' let (y , (pfy ')) = recursive f y in let (t \n' , (pft ')) = ' holcase y ' return St ' : [f]Nat.LT([f]x + y = t ' ) of 0 . ( x,\u00b7\u00b7\u00b7 proof of x + 0 = \nx \u00b7\u00b7\u00b7) | Succ y ' . Succ(x + y ' ), ' \u00b7\u00b7\u00b7 proof of x + Succ y = Succ (x + y ' ) \u00b7\u00b7\u00b7'' '' | y .(x + y \n,\u00b7\u00b7\u00b7 proof of x + y = x + y \u00b7\u00b7\u00b7) ' in (t , (\u00b7\u00b7\u00b7 proof of x + y = t ' \u00b7\u00b7\u00b7)) | t .(t,\u00b7\u00b7\u00b7 proof of t = t \n\u00b7\u00b7\u00b7) While developing such a tactic, we can leverage the VeriML type checker to know the types of missing \nproofs. But how do we ' .ll them in? For the interesting cases of x + 0 = x and x +Succ y = Succ (x + \ny ' ), we would certainly need to prove the corresponding lemmas. But for the rest of the cases, the \ncorresponding lemmas would be uninteresting and tedious to state, such as the following for the x + y \n= t ' case: '' lemma1 : .x,y, y ,t ' ,y = y '. (x + y = t ' ) . x + y = t Stating and proving such lemmas \nsoon becomes a hindrance when writing tactics. An alternative is to use the congruence closure conversion \nrule to solve this trivial obligation for us directly at the point where it is required. Our .rst attempt \nwould be: proof of x + y = t '= '' ' let (pf) = requireEqual [f,H1: y = y ,H2: x + y = t ' ](x + y) t \n't in [f]pf/[idf, pfy ' , pft ] The bene.t of this approach is evident when utilizing implicit argu\u00adments, \nsince most of the details can be inferred and therefore omit\u00adted. Here we had to alter the environment \npassed to requireEqual, which includes several extra hypotheses. Once the resulting proof has been computed, \nthe hypotheses are substituted by the actual proofs that we have. The problem with this approach is two-fold: \n.rst, the call to the requireEqual tactic is recomputed every time we reach that point of our function. \nFor such a simple tactic call, this does not impact the runtime signi.cantly; still, if we could avoid \nit, we would be able use more sophisticated and expensive tactics. The second problem is that if for \nsome reason the requireEqual is not able to prove what it is supposed to, we will not know until we actually \nreach that point in the function. Moving to static proofs. This is where using the letstatic construct \nbecomes essential. We can evaluate the call to requireEqual stat\u00adically, during stage one interpretation. \nThus we will know at the time that plusRewriter1 is de.ned whether the call succeeded; also, it will \nbe replaced by a concrete value, so it will not affect the run\u00adtime behavior of each invocation of plusRewriter1 \nanymore. To do that, we need to avoid mentioning any of the metavariables that are bound during runtime, \nlike x, y, and t '. This is done by specifying an appropriate environment in the call to requireEqual, \nsimilarly to the way we incorporated the extra knowledge above and substituted it later. Using this approach, \nwe have: proof of x + y = t '= letstatic (pf) = ' '' let f' =[x,y,y ,t ' : Nat,H1: y = y ,H2: x + y = \nt ' ] in requireEqual f' (x + y) t ' 't in [f]pf/[x/idf,y/idf,y ' /idf,t ' /idf,pfy ' /idf,pft ' /idf] \nWhat we are essentially doing here is replacing the meta\u00advariables by normal logical variables, which \nour tactics can deal with. The meta-variable context is collapsed into a normal con\u00adtext; proofs are \nconstructed using tactics in this environment; last, the resulting proofs are transported back into the \ndesired context by substituting meta-variables for variables. We have explicitly stated the substitutions \nin order to distinguish between normal logical variables and meta-variables. The reason why this transformation \nneeds to be done is that functions in our computational language can only manipulate logi\u00adcal terms that \nare open with respect to a normal variables context; not logical terms that are open with respect to \nthe meta-variables context too. A much more complicated, but also more .exible al\u00adternative to using \nthis collapsing trick would be to support meta\u00adn-variables within our computational language directly. \nOverall, this approach is entirely similar to proving the auxiliary lemma mentioned above, prior to the \ntactic de.nition. The bene.t is that by leveraging the type information together with type in\u00adference, \nwe can avoid stating such lemmas explicitly, while retain\u00ading the same runtime behavior. We thus end \nup with very concise proof expressions that are statically validated. We introduce syn\u00adtactic sugar for \nbinding a static proof script to a variable, and then performing a substitution to bring it into the \ncurrent context, since this is a common operation. (e)static = letstatic (pf) = e in ([f]pf/\u00b7\u00b7\u00b7) Based \non these, the trivial proofs in the above tactic can be .lled in using a simple (requireEqual)static \ncall; for the other two we use (Instantiate (NatInduction requireEqual requireEqual) x)static. After \nwe de.ne plusRewriter1, we can register it with the global equivalence checking procedure. Thus, all \nlater calls to requireEqual will bene.t from this simpli.cation. It is then sim\u00adple to prove commutativity \nfor addition: plusComm : LT(.x,y.x + y = y + x) plusComm = NatInduction requireEqual requireEqual Based \non this proof, we can write a rewriter that takes commu\u00adtativity into account and uses the hash values \nof logical terms to avoid in.nite loops. We have worked on an arithmetic simpli.ca\u00adtion rewriter that \nis built by layering such rewriters together, using previous ones to aid us in constructing the proofs \nrequired in later ones. It works by converting expressions into a list of monomi\u00adals, sorting the list \nbased on the hash values of the variables, and then factoring monomials on the same variable. Also, the \neufEqual procedure mentioned earlier has all of its associated proofs auto\u00admated through static proof \nscripts, using a naive, potentially non\u00adterminating, equality rewriter. Is collapsing always possible? \nA natural question to ask is whether collapsing the metavariables context into a normal context is always \npossible. In order to cast this as a more formal ques\u00adtion, we notice that the essential step is replacing \na proof object p of type [F]t, typed under the meta-variables environment ., by a ' proof object p' of \ntype [F' ]t typed under the empty meta-variables environment. There needs to be a substitution so that \np' gets trans\u00adported back to the F, . environment, and has the appropriate type.  (terms) t ::= s | \nc | fi | bi | .(t1).t2 | t1 t2 | .(t1).t2 | t1 = t2 | re. t | leibniz t1 t2 | lamEq t | forallEq t1 t2 \n| betaEq t1 t2 Syntax of the logic (sorts) s ::= Prop | Type | Type ' (var. context) F ::= | F, t (substitutions) \ns ::= | s, t Example of representation: a : Nat f .x : Nat.(.y : Nat.re. (plus ay))(plus ax) . Nat f \n.(Nat).(.(Nat).re. (plus f0 b0)) (plus f0 b0) [ fil = fi l fm-1 n = bn m [bnln = fm l fi n = fi when \ni < m - 1 m m Freshen: [tln m [biln = bi when i < n lbi = bi+1 Bind: lt n m nn+1 nn+1 [(.(t1).t2)ln \n= .([t1l).[t2ll(.(t1).t2) = .(lt1 ).lt2 [t1 t2l = [t1l[t2llt1 t2 = lt1 lt2 (a) Hybrid deBruijn levels-deBruijn \nindices representation technique t ::= \u00b7\u00b7\u00b7 | fI | Xi/sF ::= | F, t | F, fi s ::= | s, t | s, id(fi) (indices) \nI ::= n | I + |fi| (ctx.terms) T ::=[F]t | [F]F' Syntax (ctx.kinds) K ::=[F]t | [F]ctx (extension context) \n. ::= | ., K (ext. subst.) s. ::= | s., T F.I = t .; F f t1: .(t).t ' .; F f t2: t ..i =[F' ]t ' .; F \nf s : F' .; F f t : t ' (sample) .; F f fI : t .; F f t1 t2:t '\u00b7 (idF,t2) .; F f Xi/s : t '\u00b7 s .; F \nf t : t ' . f F, F' wf . f F wf ..i =[F]ctx . f T : K . f F wf (sample) . f (F, fi) wf . f [F]t : [F]t \n' . f [F]F' : [F] ctx (b) Extension variables: meta-variables and context variables c \u00b7 s = cfI \u00b7 s = \ns.I bi \u00b7 s = bi (.(t1).t2) \u00b7 s = .(t1 \u00b7 s).(t2 \u00b7 s)(t1 t2) \u00b7 s =(t1 \u00b7 s)(t2 \u00b7 s) (I, |fi|) \u00b7 s. =(I \u00b7 \ns.), |F'| when s..i =[_]F' (Xi/s) \u00b7 s. = t \u00b7 (s \u00b7 s.) when s..i =[_]t Subst. application: t \u00b7 s Ext. \nsubst. application (sample) (s, id(fi)) \u00b7 s. = s \u00b7 s., ids..i (F, fi) \u00b7 s. = F \u00b7 s., F' when s..i = \n[_]F' .; F f s : F' .; F f s : F' ..i = [F' ] ctx . f s. : .' .; F f s : F' .; F f : .; F f t : t ' \n\u00b7 s .; F f (s, t) : (F' , t ' ) F' , fi . F .; F f (s, id(fi)) : (F' , fi) . f s. : .' (selected) . f \nT : K \u00b7 s. . f (s., T ) : (.' , K) Subst. lemmas: .; F f t : t ' .; F' f s : F .; F' f t \u00b7 s : t ' \u00b7 \ns .; F' f s : F .; F'' f s' : F' .; F'' f s \u00b7 s' : F . f T : K .' f s. : . .' f T \u00b7 s. : K \u00b7 s. (c) \nSubstitutions over logical variables and extension variables |static = (G, x :st)|static = G|static, \nx : t Syntax: Limit ctx: G ::= | G, x : t | G, x :s t | G, a : ke ::= \u00b7\u00b7\u00b7 | letstatic x = e in e ' (G, \nx : t)|static = G|static (G, a : k)|static = G|static ' : t ; S; G|static f e : t.; S; G,x :s t f ex \n:s t . G .; S; G f e : t (part) .; S; G f letstatic x = e in e ' : t.; S; G f x : t v ::= .(K).ed | \npack T return (.t) with v | () | .x : t.ed | (v, v ' ) | injiv | fold v | l | .a : k.ed S ::= letstatic \nx = in e '| letstatic x = S in e '| .(K).S | .x : t.S | unpack ed (.)x.(S) | case(ed , x.S, x.e2) | \ncase(ed , x.ed , x.S) | .a : k.S | .x x : t.S | unify T return (.t) with (..T ' . S) | Es[S] Evaluation: \n Es ::= EsT | pack T return (.t) with Es | unpack Es (.)x.(e ' ) | Ese '| ed Es | (Es, e) | (ed , Es) \n| proji Es | inji Es | case(Es, x.e1, x.e2) | fold Es | unfold Es | ref Es | Es := e '| ed := Es | !Es \n| Es t ed ::= all of e except letstatic x = e in e ' E ::= exactly as Es with Es . E and e . ed '' ( \n\u00b5 , ed ) -. ( \u00b5 , ed ) ( \u00b5 , S[letstatic x = v in e]) -.s ( \u00b5 , S[e[v/x]] ) Stage 1 op.sem.: '' ( \u00b5 \n, S[ed ]) -.s ( \u00b5 , S[ed ]) ( \u00b5 , letstatic x = v in e ) -.s ( \u00b5 , e[v/x]) (d) Computational language: \nstaging support Figure 11. Main de.nitions in metatheory We have proved that this is possible under \ncertain restrictions: the types of the metavariables in the current context need to depend on the same \nfree variables context Fmax, or pre.xes of that context. Also the substitutions they are used with need \nto be pre.xes of the identity substitution for Fmax. Such terms are characterized as collapsible. We \nhave proved that collapsible terms can be replaced using terms that do not make use of metavariables; \nmore details can be found in Sec. 6 and in the accompanying technical report [Stampoulis and Shao 2012]. \nThis restriction corresponds very well to the treatment of vari\u00adable contexts in the Delphin language. \nThis language assumes an ambient context of logical variables, instead of full, contextual modal terms. \nConstructs to extend this context and substitute a spe\u00adci.c variable exist. If this last feature is not \nused, the ambient con\u00adtext grows monotonically and the mentioned restriction holds triv\u00adially. In our \ntests, this restriction has not turned out to be limiting. 6. Metatheory We have completed an extensive \nreworking of the metatheory of VeriML, in order to incorporate the features that we have presented in \nthis paper. Our new metatheory includes a number of techni\u00adcal advances compared to our earlier work \n[Stampoulis and Shao 2010]. We will present a technical overview of our metatheory in this section; full \ndetails can be found in our technical report [Stam\u00adpoulis and Shao 2012]. Variable representation technique. \nThough our metatheory is done on paper, we have found that using a concrete variable repre\u00adsentation \ntechnique elucidates some aspects of how different kinds of substitutions work in our language, compared \nto having nor\u00admal named variables. For example, instantiating a context variable with a concrete context \ntriggers a set of potentially complicated a-renamings, which a concrete representation makes explicit. \nWe use a hybrid technique representing bound variables as deBruijn in\u00addices, and free variables as deBruijn \nlevels. Our technique is a small departure from the named approach, requiring fewer extra annota\u00adtions \nand lemmas than normal deBruijn indices. Also it identi.es terms not only up to a-equivalence, but also \nup to extension of the context with new variables; this is why it is also used within the Ver\u00adiML implementation.The \ntwo fundamental operations of this tech\u00adnique are freshening and binding, which are shown in Fig. 11a. \nExtension variables. We extend the logic with support for meta\u00advariables and context variables we refer \nto both these sorts of variables as extension variables. A meta-variable Xi stands for a contextual term \nT =[F]t, which packages a term together with the context it inhabits. Context variables fi stand for \na context F, and are used to weaken parametric contexts in speci.c positions. Both kinds of variables \nare needed to support manipulation of open logical terms. Details of their de.nition and typing are shown \nin Fig. 11b. We use the same hybrid approach as above for represent\u00ading these variables. A somewhat subtle \naspect of this extension is that we generalize the deBruijn levels I used to index free variables, in \norder to deal effectively with parametric contexts. Substitutions. The hybrid representation technique \nwe use for variables renders simultaneous substitutions for all variables in scope as the most natural \nchoice. In Fig. 11c, we show some ex\u00adample rules of how to apply a full simultaneous substitution s to \na term t, denoted as t \u00b7 s. Similarly, we de.ne full simultaneous sub\u00adstitutions s. for extension contexts; \nde.ning their application has a very natural description, because of our variable representation technique. \nWe prove a number of substitution lemmas which have simple statements, as shown in Fig. 11c. The proofs \nof these lem\u00admas comprise the main effort required in proving the type-safety of a computational language \nsuch as the one we support, as they represent the point where computation speci.c to logical term ma\u00ad \nnipulation takes place. Computational language. We de.ne an ML-style computational language that supports \ndependent functions and dependent pairs over contextual terms T , as well as pattern matching over them. \nLack of space precludes us from including details here; full details can be found in the accompanying \ntechnical report [Stampoulis and Shao 2012]. A fairly complete ML calculus is supported, with mu\u00adtable \nreferences and recursive types. Type safety is proved using standard techniques; its central point is \nextending the logic sub\u00adstitution lemmas to expressions and using them to prove progress and preservation \nof dependent functions and dependent pairs. This proof is modular with respect to the logic and other \nlogics can eas\u00adily be supported. Pattern matching. Our metatheory includes many extensions in the pattern \nmatching that is supported, as well as a new approach for dealing with typing patterns. We include support \nfor pattern match\u00ading over contexts (e.g. to pick out hypotheses from the context) and for non-linear \npatterns. The allowed patterns are checked through a restriction of the usual typing rules . fpT : K. \nThe essential idea behind our approach to pattern matching is to identify what the relevant variables \nin a typing derivation are. Since contexts are ordered, removing non-relevant variables amounts to replacing \ntheir de.nitions in the context with holes, which leads us to partial contexts i .. The corresponding \nnotion of partial substitutions is denoted as s6.. Our main theorem about pattern matching can then be \nstated as: Theorem 6.1 (Decidability of pattern matching) If . fpT : K, ' fpT : K and relevant(.; F f \nT : K) i = ., then either there exists a unique partial substitution s6. such that f 6. and s. : iT \u00b7 \ns6. = T ', or no such substitution exists. Staging. Our development in this paper critically depends \non the letstatic construct we presented earlier. It can be seen as a dual of the traditional box construct \nof Davies and Pfenning [1996]. De\u00adtails of its typing and semantics are shown in Fig. 11d. We de.ne a \nnotion of static evaluation contexts S, which enclose a hole of the form letstatic x = in e. They include \nnormal evaluation contexts, as well as evaluation contexts under binding structures. We evaluate expressions \ne that include staging constructs using the -.s rela\u00adtion; internally, this uses the normal evaluation \nrules, that are used in the second stage as well, for evaluating expressions which do not include other \nstaging constructs. If stage-one evaluation is suc\u00ad ' cessful, we are left with a residual dynamic con.guration \n(\u00b5 , ed ) which is then evaluated normally. We prove type-safety for stage\u00adone evaluation; its statement \nfollows. Theorem 6.2 (Stage-one Type Safety) If ; S; f e : t then: ei\u00adther e is a dynamic expression \ned; or, for every store \u00b5 such that f \u00b5 : S, we have: either \u00b5,e -.s error, or, there exists an e' , \na new ' store typing S'. S and a new store \u00b5' such that: (\u00b5,e) -. (\u00b5 ,e ' ); f \u00b5 ' : S'; and ; S' ; f \ne ' : t. Collapsing extension variables. Last, we have proved the fact that under the conditions described \nin Sec. 5, it is possible to col\u00adlapse a term t into a term t ' which is typed under the empty exten\u00adsion \nvariables context; a substitution s with which we can regain the original term t exists. This suggests \nthat whenever a proof ob\u00adject t for a speci.c proposition is required, an equivalent proof ob\u00adject that \ndoes not mention uninstantiated extension variables exists. Therefore, we can write an equivalent proof \nscript producing the collapsed proof object instead, and evaluate that script statically. The statement \nof this theorem is the following: Theorem 6.3 If . f [F]t : [F]tT and collapsible(. f [F]t : [F]tT ), \nthen there exist F' ,t ' , t T ' and s such that f F' wf, f [F' ]t ' : ' [F' ]tT ' , .; F f s : F' ,t \n'\u00b7 s = t and t\u00b7 s = tT . T  The main idea behind the proof is to maintain a number of sub\u00adstitutions \nand their inverses: one to go from a general . extension context into an equivalent .' context, which \nincludes only de.ni\u00adtions of the form [F]t, for a constant F context that uses no exten\u00adsion variables. \nThen, another substitution and its inverse are main\u00adtained to go from that extension variables context \ninto the empty one; this is simpler, since terms typed under .' are already essen\u00adtially free of metavariables. \nThe computational content within the proof amounts to a procedure for transforming proof scripts inside \ntactics into static proof scripts. 7. Implementation We have completed a prototype implementation of \nthe VeriML language, as described in this paper, that supports all of our claims. We have built on our \nexisting prototype [Stampoulis and Shao 2010] and have added an extensive set of new fea\u00adtures and improvements. \nThe prototype is written in OCaml and is about 6k lines of code. Using the prototype we have imple\u00admented \na number of examples, that are about 1.5k lines of code. Readers are encouraged to download and try the \nprototype from http://flint.cs.yale.edu/publications/supc.html. New features. We have implemented the \nnew features we have described so far: context matching, non-linear patterns, proof\u00aderasure semantics, \nstaging, and inferencing for logical and com\u00adputational terms. Proof-erasure semantics are utilized only \nif re\u00adquested by a per-function .ag, enabling us to selectively trust tactics. The staging construct \nwe support is more akin to the (\u00b7)static form described as syntactic sugar in Sec. 5, and it is able \nto infer the collapsing substitutions that are needed, following the approach used in our metatheory. \nChanges. We have also changed quite a number of things in the prototype and improved many of its aspects. \nA central change, me\u00addiated by our new treatment of the conversion rule, was to modify the used logic \nin order to use the explicit equality approach; the ex\u00adisting prototype used the .HOLc logic. We also \nswitched the vari\u00adable representation to the hybrid deBruijn levels-deBruijn indices technique we described, \nwhich enabled us to implement subtyping based on context subsumption. Also, we have adapted the typing \nrules of the pattern matching construct in order to support re.ning the environment based on the current \nbranch. Examples implemented. We have implemented a number of ex\u00adamples to support our claims. First, \nwe have written the type-safe conversion check routine for \u00dfN, and extended it to support congru\u00adence \nclosure based on equalities in the context. Proofs of this lat\u00adter tactic are constructed automatically \nthrough static proof scripts, using a naive rewriter that is non-terminating in the general case. We \nhave also completed proofs for theorems of arithmetic for the properties of addition and multiplication, \nand used them to write an arithmetic simpli.cation tactic. All of the theorems are proved by making essential \nuse of existing conversion rules, and are imme\u00addiately added into new conversion rules, leading to a \ncompact and clean development style. The resulting code does not need to make use of translation validation \nor proof by re.ection, which are typi\u00adcally used to implement similar tactics in existing proof assistants. \nTowards a practical proof assistant. In order to facilitate practi\u00adcal proof and program construction \nin VeriML, we introduced some features to support surface syntax, enabling users to omit most de\u00adtails \nabout the environments of contextual terms and the substi\u00adtutions used with meta-variables. This syntax \nfollows the style of Delphin, assuming an ambient logical variable environment which is extended through \na construct denoted as .x : t.e. Still, the full power of contextual modal type theory is available, \nwhich is cru\u00adcial in order to change what the current ambient environment is, used, as we saw earlier, \nfor static calls to tactics. In general the surface syntax leads to much more concise and readable code. \nLast, we introduced syntax support for calls to tactics, enabling users to write proof expressions that \nlook very similar to proof scripts in current proof assistants. We developed a rudimentary ProofGeneral \nmode for VeriML, that enables us to call the VeriML type-checker and interpreter for parts of source \n.les. By adding holes to our sources, we can be informed by the type inference mechanism about their \nexpected types. Those types correspond to what the current proof state is at that point. Therefore, a \npossi\u00adble work.ow for developing tactics or proofs, is writing the known parts, inserting holes in missing \npoints to know what remains to be proved, and calling the typechecker to get the proof state infor\u00admation. \nThis work.ow corresponds closely to the interactive proof development support in proof assistants like \nCoq and Isabelle, but generalizes it to the case of tactics as well. 8. Related work There is a large \nbody of work that is related to the ideas we have presented here. Techniques for robust proof development. \nThere have been multiple proposals for making proof development inside existing proof assistants more \nrobust. A well-known technique is proof-by\u00adre.ection [Boutin 1997]: writing total and certi.ed decision \nproce\u00addures within the functional language contained in a logic like CIC. A recently introduced technique \nis automation through canonical structures [Gonthier et al. 2011]: the resolution mechanism for .nding \ninstances of canonical structures (a generalization of type classes) is cleverly utilized in order to \nprogram automation proce\u00addures for speci.c classes of propositions. We view both approaches as somewhat \nsimilar, as both are based in cleverly exploiting static interpreters that are available in a modern \nproof assistant: the partial evaluator within the conversion rule in the former case; the uni.cation \nalgorithm within instance discovery in the latter case. Our approach can thus be seen as similar, but \nalso as a gen\u00aderalization of these approaches, since a general-purpose program\u00adming model is supported. \nTherefore, users do not have to adapt to a speci.c programming style for writing automation code, but \ncan rather use a familiar functional language. Proof-by-re.ection could perhaps be used to support the \nsame kind of extensions to the con\u00adversion rule; still, this would require re.ecting a large part of \nthe logic in itself, through a prohibitively complicated encoding. Both techniques are applicable to \nour setting as well and could be used to provide bene.ts to large developments within our language. The \nstyle advocated in Chlipala [2011] (and elsewhere) suggests that proper proof engineering entails developing \nsophisticated au\u00adtomation tactics in a modular style, and extending their power by adding proved lemmas \nas hints. We are largely inspired by this ap\u00adproach, and believe that our introduction of the extensible \nconver\u00adsion rule and static checking of tactics can signi.cantly bene.t it. We demonstrate similar ideas \nin layering conversion tactics. Traditional proof assistants. There are many parallels of our work with \nthe LCF family of proof assistants, like HOL4 [Slind and Norrish 2008] and HOL-Light [Harrison 1996], \nwhich have served as inspiration. First, the foundational logic that we use is similar. Also, our use \nof a dedicated ML-like programming language to program tactics and proof scripts is similar to the approach \ntaken by HOL4 and HOL-Light. Last, the fact that no proof objects need to be generated is shared. Still, \nchecking a proof script in HOL requires evaluating it fully. Using our approach, we can selectively evaluate \nparts of proof scripts; we focus on conversion-like tactics, but we are not limited inherrently to those. \nThis is only possible because our proof scripts carry proof state information within their types. Similarly, \nproof scripts contained within LCF tactics cannot be evaluated statically, so it is impossible to establish \ntheir validity upon tactic de.nition. It is possible to do a transformation similar to ours manually \n(lifting proof scripts into auxiliary lemmas that are proved prior to the tactic), but the lack of type \ninformation means that many more details need to be provided.  The Coq proof assistant [Barras et al. \n2010] is another obvious point of reference for our work. We will focus on the conversion rule that CIC, \nits accompanying logic, supports the same prob\u00adlems with respect to proof scripts and tactics that we \ndescribed in the LCF case also apply for Coq. The conversion rule, which identi\u00ad.es computationally equivalent \npropositions, coupled with the rich type universe available, opens up many possibilities for construct\u00ading \nsmall and ef.ciently checkable proof objects. The implementa\u00adtion of the conversion rule needs to be \npart of the trusted base of the proof assistant. Also, the fact that the conversion check is built\u00adin \nto the proof assistant makes the supported equivalence rigid and non-extensible by frequently used decision \nprocedures. There is a large body of work that aims to extend the conver\u00adsion rule to arbitrary con.uent \nrewrite systems (e.g. Blanqui et al. [1999]) and to include decision procedures [Strub 2010]. These approaches \nassume some small or larger addition to the trusted base, and extend the already complex metatheory of \nCoq. Further\u00admore, the NuPRL proof assistant [Constable et al. 1986] is based on extensional type theory \nwhich includes an extensional conver\u00adsion rule. This enables complex decision procedures to be part of \nconversion; but it results in a very large trusted base. We show how, for a subset of these type theories, \nthe conversion check can be re\u00adcovered outside the trusted base. It can be extended with arbitrarily \ncomplex new tactics, written in a familiar programming style, with\u00adout any metatheoretic additions and \nwithout hurting the soundness of the logic. The question of whether these type theories can be supported \nin full remains as future work, but as far as we know, there is no inherrent limitation to our approach. \nDependently-typed programming. The large body of work on dependently-typed languages has close parallels \nto our work. Out of the multitude of proposals, we consider the Russell framework [Sozeau 2006] as the \ncurrent state-of-the-art, because of its high expressivity and automation in discharging proof obligations. \nIn our setting, we can view dependently-typed programming as a spe\u00adci.c case of tactics producing complex \ndata types that include proof objects. Static proof scripts can be leveraged to support ex\u00adpressivity \nsimilar to the Russell framework. Furthermore, our ap\u00adproach opens up a new intriguing possibility: dependently-typed \nprograms whose obligations are discharged statically and automat\u00adically, through code written within \nthe same language. Last, we have been largely inspired by the work on languages like Beluga [Pientka \nand Dun.eld 2008] and Delphin [Poswolsky and Sch\u00fcrmann 2008], and build on our previous work on VeriML \n[Stampoulis and Shao 2010]. We investigate how to leverage type\u00adsafe tactics, as well as a number of \nnew constructs we introduce, so as to offer an extensible notion of proof checking. Also, we address \nthe issue of statically checking the proof scripts contained within tactics written in VeriML. As far \nas we know, our development is the .rst time languages such as these have been demonstrated to provide \na work.ow similar to interactive proof assistants. Acknowledgments We thank anonymous referees for their \nsuggestions and comments on an earlier version of this paper. This research is based on work supported \nin part by DARPA CRASH grant FA8750-10-2-0254 and NSF grants CCF-0811665, CNS-0910670, and CNS 1065451. \nAny opinions, .ndings, and conclusions contained in this document are those of the authors and do not \nre.ect the views of these agencies. References H.P. Barendregt and H. Geuvers. Proof-assistants using \ndependent type sys\u00adtems. In A. Robinson and A. Voronkov, editors, Handbook of Automated Reasoning. Elsevier \nSci. Pub. B.V., 1999. B. Barras, S. Boutin, C. Cornes, J. Courant, Y. Coscoy, D. Delahaye, D. de Rauglaudre, \nJ.C. Filli\u00e2tre, E. Gim\u00e9nez, H. Herbelin, et al. The Coq proof assistant reference manual (version 8.3), \n2010. F. Blanqui, J.P. Jouannaud, and M. Okada. The calculus of algebraic constructions. In Rewriting \nTechniques and Applications, pages 671  671. Springer, 1999. F. Blanqui, J.P. Jouannaud, and P.Y. Strub. \nA calculus of congruent con\u00adstructions. Unpublished draft, 2005. S. Boutin. Using re.ection to build \nef.cient and certi.ed decision proce\u00addures. Lecture Notes in Computer Science, 1281:515 529, 1997. A. \nChlipala. Mostly-automated veri.cation of low-level programs in com\u00adputational separation logic. In Proceedings \nof the 2011 ACM SIG-PLAN conference on Programming Language Design and Implementa\u00adtion. ACM, 2011. R.L. \nConstable, S.F. Allen, H.M. Bromley, W.R. Cleaveland, J.F. Cremer, R.W. Harper, D.J. Howe, T.B. Knoblock, \nN.P. Mendler, P. Panangaden, et al. Implementing Mathematics with the Nuprl Proof Development System. \nPrentice-Hall, NJ, 1986. R. Davies and F. Pfenning. A modal analysis of staged computation. In Proceedings \nof the 23rd ACM SIGPLAN-SIGACT symposium on Principles of programming languages, pages 258 270. ACM, \n1996. G. Gonthier. Formal proof the four-color theorem. Notices of the AMS, 55 (11):1382 1393, 2008. \n G. Gonthier, B. Ziliani, A. Nanevski, and D. Dreyer. How to make ad hoc proof automation less ad hoc. \nIn Proceeding of the 16th ACM SIGPLAN International Conference on Functional Programming, pages 163 175. \nACM, 2011. J. Harrison. HOL Light: A tutorial introduction. Lecture Notes in Computer Science, pages \n265 269, 1996. G. Klein, K. Elphinstone, G. Heiser, J. Andronick, D. Cock, P. Derrin, D. Elkaduwe, \nK. Engelhardt, R. Kolanski, M. Norrish, et al. seL4: Formal veri.cation of an OS kernel. In Proceedings \nof the ACM SIGOPS 22nd Symposium on Operating Systems Principles, pages 207  220. ACM, 2009. X. Leroy. \nFormal veri.cation of a realistic compiler. Communications of the ACM, 52(7):107 115, 2009. T. Nipkow, \nL.C. Paulson, and M. Wenzel. Isabelle/HOL : A Proof Assistant for Higher-Order Logic, volume 2283 of \nLNCS, 2002. B. Pientka and J. Dun.eld. Programming with proofs and explicit contexts. In Proceedings \nof the 10th international ACM SIGPLAN conference on Principles and Practice of Declarative Programming, \npages 163 173. ACM New York, NY, USA, 2008. A. Poswolsky and C. Sch\u00fcrmann. Practical programming with \nhigher-order encodings and dependent types. Lecture Notes in Computer Science, 4960:93, 2008. V. Siles \nand H. Herbelin. Equality is typable in semi-full pure type systems. In 2010 25th Annual IEEE Symposium \non Logic in Computer Science, pages 21 30. IEEE, 2010. K. Slind and M. Norrish. A brief overview of HOL4. \nTheorem Proving in Higher Order Logics, pages 28 32, 2008. M. Sozeau. Subset coercions in coq. In Proceedings \nof the 2006 Interna\u00adtional Conference on Types for Proofs and Programs, pages 237 252. Springer-Verlag, \n2006. A. Stampoulis and Z. Shao. VeriML: Typed computation of logical terms in\u00adside a language with effects. \nIn Proceedings of the 15th ACM SIGPLAN International Conference on Functional Programming, pages 333 \n344. ACM, 2010. A. Stampoulis and Z. Shao. Static and user-extensible proof checking (extended version). \nAvailable in the ACM Digital Library, 2012. P.Y. Strub. Coq modulo theory. In Proceedings of the 24th \nInternational Conference on Computer Science Logic, pages 529 543. Springer-Verlag, 2010.  \n\t\t\t", "proc_id": "2103656", "abstract": "<p>Despite recent successes, large-scale proof development within proof assistants remains an arcane art that is extremely time-consuming. We argue that this can be attributed to two profound shortcomings in the architecture of modern proof assistants. The first is that proofs need to include a large amount of minute detail; this is due to the rigidity of the proof checking process, which cannot be extended with domain-specific knowledge. In order to avoid these details, we rely on developing and using tactics, specialized procedures that produce proofs. Unfortunately, tactics are both hard to write and hard to use, revealing the second shortcoming of modern proof assistants. This is because there is no static knowledge about their expected use and behavior. As has recently been demonstrated, languages that allow type-safe manipulation of proofs, like Beluga, Delphin and VeriML, can be used to partly mitigate this second issue, by assigning rich types to tactics. Still, the architectural issues remain. In this paper, we build on this existing work, and demonstrate two novel ideas: an extensible conversion rule and support for static proof scripts. Together, these ideas enable us to support both user-extensible proof checking, and sophisticated static checking of tactics, leading to a new point in the design space of future proof assistants. Both ideas are based on the interplay between a light-weight staging construct and the rich type information available.</p>", "authors": [{"name": "Antonis Stampoulis", "author_profile_id": "81470653770", "affiliation": "Yale University, New Haven, CT, USA", "person_id": "P2991395", "email_address": "antonis.stampoulis@yale.edu", "orcid_id": ""}, {"name": "Zhong Shao", "author_profile_id": "81351597965", "affiliation": "Yale University, New Haven, CT, USA", "person_id": "P2991396", "email_address": "zhong.shao@yale.edu", "orcid_id": ""}], "doi_number": "10.1145/2103656.2103690", "year": "2012", "article_id": "2103690", "conference": "POPL", "title": "Static and user-extensible proof checking", "url": "http://dl.acm.org/citation.cfm?id=2103690"}