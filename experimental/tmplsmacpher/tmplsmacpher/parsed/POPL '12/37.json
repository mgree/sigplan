{"article_publication_date": "01-25-2012", "fulltext": "\n Randomized Accuracy-Aware Program Transformations For Ef.cient Approximate Computations Zeyuan Allen \nZhu Sasa Misailovic Jonathan A. Kelner Martin Rinard MIT CSAIL zeyuan@csail.mit.edu misailo@mit.edu \n Abstract Despite the fact that approximate computations have come to domi\u00adnate many areas of computer \nscience, the .eld of program transfor\u00admations has focused almost exclusively on traditional semantics\u00adpreserving \ntransformations that do not attempt to exploit the op\u00adportunity, available in many computations, to acceptably \ntrade off accuracy for bene.ts such as increased performance and reduced resource consumption. We present \na model of computation for approximate compu\u00adtations and an algorithm for optimizing these computations. \nThe algorithm works with two classes of transformations: substitution transformations (which select one \nof a number of available imple\u00admentations for a given function, with each implementation offer\u00ading a \ndifferent combination of accuracy and resource consumption) and sampling transformations (which randomly \ndiscard some of the inputs to a given reduction). The algorithm produces a (1 + e) randomized approximation \nto the optimal randomized computation (which minimizes resource consumption subject to a probabilistic \naccuracy speci.cation in the form of a maximum expected error or maximum error variance). Categories \nand Subject Descriptors D.3.4 [Programming Lan\u00adguages]: Processors optimization; G.3 [Probability and \nStatis\u00adtics]: Probabilistic Algorithms; F.2.1 [Analysis of Algorithms and Problem Complexity]: Numerical \nAlgorithms and Problems General Terms Algorithms, Design, Performance, Theory Keywords Optimization, \nError-Time Tradeoff, Discretization, Probabilistic 1. Introduction Computer science was founded on exact \ncomputations with discrete logical correctness requirements (examples include compilers and traditional \nrelational databases). But over the last decade, approx\u00adimate computations have come to dominate many \n.elds. In con\u00adtrast to exact computations, approximate computations aspire only to produce an acceptably \naccurate approximation to an exact (but in many cases inherently unrealizable) output. Examples include \nma\u00adchine learning, unstructured information analysis and retrieval, and lossy video, audio and image \nprocessing. Permission to make digital or hard copies of all or part of this work for personal or classroom \nuse is granted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. POPL \n12, January 25 27, 2012, Philadelphia, PA, USA. Copyright c &#38;#169; 2012 ACM 978-1-4503-1083-3/12/01. \n. . $10.00 kelner@mit.edu rinard@mit.edu Despite the prominence of approximate computations, the .eld \nof program transformations has remained focused on techniques that are guaranteed not to change the output \n(and therefore do not affect the accuracy of the approximation). This situation leaves the developer \nsolely responsible for managing the approximation. The result is in.exible computations with hard-coded \napproximation choices directly embedded in the implementation. 1.1 Accuracy-Aware Transformations We \ninvestigate a new class of transformations, accuracy-aware transformations, for approximate computations. \nGiven a computa\u00adtion and a probabilistic accuracy speci.cation, our transformations change the computation \nso that it operates more ef.ciently while satisfying the speci.cation. Because accuracy-aware transforma\u00adtions \nhave the freedom to change the output (within the bounds of the accuracy speci.cation), they have a much \nbroader scope and are therefore able to deliver a much broader range of bene.ts. The .eld of accuracy-aware \ntransformations is today in its in\u00adfancy. Only very recently have researchers developed general trans\u00adformations \nthat are designed to manipulate the accuracy of the computation. Examples include task skipping [27, \n28], loop per\u00ad foration [14, 23, 24, 31], approximate function memoization [6], and substitution of multiple \nalternate implementations [2, 3, 12, 33]. When successful, these transformations deliver programs that \ncan operate at multiple points in an underlying accuracy-resource consumption tradeoff space. Users may \nselect points that mini\u00admize resource consumption while satisfying the speci.ed accuracy constraints, \nmaximize accuracy while satisfying speci.ed resource consumption constraints, or dynamically change the \ncomputation to adapt to changes (such as load or clock rate) in the underlying computational platform \n[12, 14]. Standard approaches to understanding the structure of the trade\u00adoff spaces that accuracy-aware \ntransformations induce use training executions to derive empirical models [2, 3, 14, 24, 27, 28, 31, \n33]. Potential pitfalls include models that may not accurately capture the characteristics of the transformed \ncomputation, poor correlations between the behaviors of the computation on training and produc\u00adtion inputs, \na resulting inability to .nd optimal points in the tradeoff space for production inputs, and an absence \nof guaranteed bounds on the magnitude of potential accuracy losses.  1.2 Our Result We present a novel \nanalysis and optimization algorithm for a class of approximate computations. These computations are expressed \nas a tree of computation nodes and reduction nodes. Each computation node is a directed acyclic graph \nof nested function nodes, each of which applies an arbitrary function to its inputs. A reduction node \napplies an aggregation function (such as min, max, or mean) to its inputs.  We consider two classes \nof accuracy-aware transformations. Substitution transformations replace one implementation of a func\u00adtion \nnode with another implementation. Each function has a propa\u00adgation speci.cation that characterizes the \nsensitivity of the function to perturbations in its inputs. Each implementation has resource consumption \nand accuracy speci.cations. Resource consumption speci.cations characterize the resources (such as time, \nenergy, or cost) each implementation consumes to compute the function. Ac\u00adcuracy speci.cations characterize \nthe error that the implementation introduces. Sampling transformations cause the transformed reduction \nnode to operate on a randomly selected subset of its inputs, simultane\u00adously eliminating the computations \nthat produce the discarded in\u00adputs. Each sampling transformation has a sampling rate, which is the ratio \nbetween the size of the selected subset of its inputs and the original number of inputs. Together, these \ntransformations induce a space of program con\u00ad.gurations. Each con.guration identi.es an implementation \nfor ev\u00adery function node and a sampling rate for every reduction node. In this paper we work with randomized \ntransformations that specify a probabilistic choice over con.gurations. Our approach focuses on understanding \nthe following technical question: What is the optimal accuracy-resource consumption tradeoff curve available \nvia our randomized transformations? Understanding this question makes it possible to realize a variety \nof optimization goals, for example minimizing resource consumption subject to an accuracy speci.cation \nor maximizing accuracy subject to a resource consumption speci.cation. The primary technical result in \nthis paper is an optimization algorithm that produces a (1 + e)-approximation to the optimal randomized \ncomputation (which minimizes resource consumption subject to a probabilistic accuracy speci.cation in \nthe form of a maximum expected error or maximum error variance). We also discuss how to realize a variety \nof other optimization goals.  1.3 Challenges and Solutions Finding optimal program con.gurations presents \nseveral algorith\u00admic challenges. In particular: Exponential Con.gurations: The number of program con.g\u00adurations \nis exponential in the size of the computation graph, so a brute-force search for the best con.guration \nis computationally intractable.  Randomized Combinations of Con.gurations: A trans\u00adformed program that \nrandomizes over multiple con.gurations may substantially outperform one that chooses any single .xed \ncon.guration. We thus optimize over an even larger space the space of probability distributions over \nthe con.guration space.  Global Error Propagation Effects: Local error allocation de\u00adcisions propagate \nglobally throughout the program. The opti\u00admization algorithm must therefore work with global accuracy \neffects and interactions between errors introduced at the nodes of the computation graph.  Nonlinear, \nNonconvex Optimization Problem: The running time and accuracy of the program depend nonlinearly on the \noptimization variables. The resulting optimization problem is nonlinear and nonconvex.  We show that, \nin the absence of reduction nodes, one can for\u00admulate the optimization problem as a linear program, which \nallows us to obtain an exact optimization over the space of probability dis\u00adtributions of con.gurations \nin polynomial time. The question becomes much more involved when reduction nodes come to the picture. \nIn this case, we approximate the optimal tradeoff curve, but to a (1 + e) precision for an arbitrarily \nsmall constant e> 0. Our algorithm has a running time that is poly\u00adnomially dependent on e 1 . It is \ntherefore a fully polynomial-time approximation scheme (FPTAS). Our algorithm tackles reduction nodes \none by one. For each re\u00adduction node, it discretizes the tradeoff curve achieved by the sub\u00adprogram that \ngenerates the inputs to the reduction node. This dis\u00adcretization uses a special bi-dimensional discretization \ntechnique that is speci.cally designed for such tradeoff problems. We next show how to extend this discretization \nto obtain a corresponding discretized tradeoff curve that includes the reduction node. The .\u00adnal step \nis to recursively combine the discretizations to obtain a dy\u00adnamic programming algorithm that approximates \nthe optimal trade\u00adoff curve for the entire program. We note that the optimization algorithm produces \na weighted combination of program con.gurations. We call such a weighted combination a randomized con.guration. \nEach execution of the .nal randomized program chooses one of these con.gurations with probability proportional \nto its weight. Randomizing the transformed program provides several bene\u00ad.ts. In comparison with a deterministic \nprogram, the randomized program may be able to deliver substantially reduced resource con\u00adsumption for \nthe same accuracy speci.cation. Furthermore, ran\u00addomization also simpli.es the optimization problem by \nreplacing the discrete search space with a continuous search space. We can therefore use linear programs \n(which can be solved ef.ciently) to model regions of the optimization space instead of integer pro\u00adgrams \n(which are, in general, intractable).  1.4 Potential Applications A precise understanding of the consequences \nof accuracy-aware transformations will enable the .eld to mature beyond its current focus on transformations \nthat do not change the output. This in\u00adcreased scope will enable researchers in the .eld to attack a \nmuch broader range of problems. Some potential examples include: Sublinear Computations On Big Data: \nSampling transforma\u00adtions enable the optimization algorithm to automatically .nd sublinear computations \nthat process only a subset of the inputs to provide an acceptably accurate output. Over the past decade, \nresearchers have developed many sublinear algorithms [29]. Accuracy-aware transformations hold out the \npromise of au\u00adtomating the development of many of these algorithms.  Incrementalized and Online Computations: \nMany algo\u00adrithms can be viewed as converging towards an optimal exact solution as they process more inputs. \nBecause our model of computation supports such computations, our techniques make it possible to characterize \nthe accuracy of the current result as the computation incrementally processes inputs. This capability \nopens the door to the automatic development of incrementalized computations (which incrementally sample \navailable inputs un\u00adtil the computation produces an acceptably accurate result) and online computations \n(which characterize the accuracy of the current result as the computation incrementally processes dy\u00adnamically \narriving inputs).  Sensor Selection: Sensor networks require low power, low cost sensors [32]. Accuracy-aware \ntransformations may allow de\u00ad velopers to specify a sensor network computation with ideal\u00adized lossless \nsensors as the initial function nodes in the compu\u00adtation. An optimization algorithm can then select \nsensors that minimize power consumption or cost while still providing ac\u00adceptable accuracy.  Data Representation \nChoices: Data representation choices can have dramatic consequences on the amount of resources (time, \nsilicon area, power) required to manipulate that data [10]. Giving an optimization algorithm the freedom \nto adjust the ac\u00ad   curacy (within speci.ed bounds) may enable an informed auto\u00admatic selection of \nless accurate but more appropriate data repre\u00adsentations. For example, a compiler may automatically replace \nan expensive .oating point representation with a more ef.cient but less accurate .xed point representation. \nWe anticipate the application of this technology in both standard compilers for microprocessors as well \nas hardware synthesis systems. Dynamic Adaptation In Large Data Centers: The amount of computing power \nthat a large data center is able to deliver to individual hosted computations can vary dynamically de\u00adpending \non factors such as load, available power, and the op\u00aderating temperature within the data center (a rise \nin tempera\u00adture may force reductions in power consumption via clock rate drops). By delivering computations \nthat can operate at multiple points in the underlying accuracy-resource consumption trade\u00adoff space, \naccuracy-aware transformations open up new strate\u00adgies for adapting to .uctuations. For example, a data \ncenter may respond to load or temperature spikes by running applications at less accurate but more ef.cient \noperating points [12].  Successful Use of Mostly Correct Components: Many faulty components operate \ncorrectly for almost all inputs. By perturb\u00ading inputs and computations with small amounts of random \nnoise, it is possible to ensure that, with very high probability, no two executions of the computation \noperate on the same val\u00adues. Given a way to check if a fault occurred during the execu\u00adtion, it is possible \nto rerun the computation until all components happen to operate on values that elicit no faults. Understanding \nthe accuracy consequences of these perturbations can make it possible to employ this approach successfully.1 \n The scope of traditional program transformations has been largely con.ned to standard compiler optimizations. \nAs the above examples illustrate, appropriately ambitious accuracy-aware trans\u00adformations that exploit \nthe opportunity to manipulate accuracy within speci.ed bounds can dramatically increase the impact and \nrelevance of the .eld of program analysis and transformation.  1.5 Contributions This paper makes the \nfollowing contributions: Model of Computation: We present a model of computation for approximate computations. \nThis model supports arbitrary compositions of individual function nodes into computation nodes and computation \nnodes and reduction nodes into compu\u00adtation trees. This model exposes enough computational struc\u00adture \nto enable approximate optimization via our two transfor\u00admations.  Accuracy-Aware Transformations: We \nconsider two classes of accuracy-aware transformations: function substitutions and reduction sampling. \nTogether, these transformations induce a space of transformed programs that provide different combina\u00adtions \nof accuracy and resource consumption.  Tradeoff Curves: It shows how to use linear programming, dy\u00adnamic \nprogramming, and a special bi-dimensional discretiza\u00adtion technique to obtain a (1 + E)-approximation \nto the un\u00adderlying optimal accuracy-resource consumption tradeoff curve available via the accuracy-aware \ntransformations. If the pro\u00adgram contains no reduction nodes, the tradeoff curve is exact.  Optimization \nAlgorithm: It presents an optimization algo\u00adrithm that uses the tradeoff curve to produce randomized \npro\u00adgrams that satisfy speci.ed probabilistic accuracy and resource consumption constraints. In comparison \nwith approaches that attempt to deliver a deterministic program, randomization en\u00ad  1 The last author \nwould like to thank Pat Lincoln for an interesting discus\u00adsion on this topic. Figure 1: A numerical \nintegration program. ables our optimization algorithm to 1) deliver programs with better combinations \nof accuracy and resource consumption, and 2) avoid a variety of intractability issues. Accuracy Bounds: \nWe show how to obtain statically guar\u00adanteed probabilistic accuracy bounds for a general class of approximate \ncomputations. The only previous static accuracy bounds for accuracy-aware transformations exploited the \nstruc\u00adture present in a set of computational patterns [6, 22, 23]. 2. Example We next present an example \ncomputation that numerically inte\u00adgrates a univariate function f(x) over a .xed interval [a, b]. The \ncomputation divides [a, b] into n equal-sized subintervals, each of b-a .x length .x = n . Let x =(x1,...xn), \nwhere xi = a + i \u00b7 2 . The value of the numerical integral I is equal to nn 1 I =.x \u00b7 f(xi)= (b - a) \n\u00b7 f(xi) . n i=1 i=1 Say, for instance, f(x)= x \u00b7 sinlog(x)is the function that we want to integrate \nand [a, b] = [1, 11]. Our Model of Computation. As illustrated in Figure 1, in our model of computation, \nwe have n input edges that carry the values of xi s into the computation and an additional edge that \ncarries the value of b-a. For each xi, a computation node calculates the value of (b - a) \u00b7 f(xi). The \noutput edges of these nodes are connected to a reduction node that computes the average of these values \n(we call such a node an averaging node), as the .nal integral I. Program Transformations. The above numerical \nintegration program presents multiple opportunities to trade end-to-end ac\u00adcuracy of the result I in \nreturn for increased performance. Specif\u00adically, we identify the following two transformations that may \nim\u00adprove the performance: Substitution. It is possible to substitute the original implemen\u00adtations of \nthe sin(\u00b7) and log(\u00b7) functions that comprise f(x) with alternate implementations that may compute a \nless accu\u00adrate output in less time.  Sampling. It is possible to discard some of the n inputs of the \naveraging node (and the computations that produce these in\u00adputs) by taking a random sample of s = n inputs \n(here we call s the reduction factor). Roughly speaking, this transformation introduces an error proportional \nto v1s , but decreases the run\u00ad  ning time of the program proportionally to ns . Tradeoff Space. In \nthis numerical integration problem, a program con.guration speci.es which implementation to pick for \neach of the functions sin(\u00b7), log(\u00b7), and (in principle, although we do not  Con.guration. Weight xlog,0 \nxlog,1 xlog,2 xsin,0 xsin,1 xsin,2 s/n Error Speedup C1 0.608 0 0 1 0 0 1 100% 0.024 1.39 C2 0.392 0 \n1 0 0 1 0 47.5% 0.090 2.63 Table 1: The (1 + e)-optimal randomized program con.guration for .=0.05 and \ne =0.01. do so in this example) \u00d7 . The con.guration also speci.es the re\u00adduction factor s for the averaging \nnode. If we assume that we have two alternate implementations of sin(\u00b7) and log(\u00b7), each program con.guration \nprovides the following information: 1) xu,i .{0, 1}indicating whether we choose the i-th implementation \nof the func\u00adtion u .{log, sin}, and i .{0, 1, 2}, and 2) s indicating the reduction factor for the averaging \nnode we choose. A randomized program con.guration is a probabilistic choice over program con\u00ad.gurations. \nFunction Speci.cations. We impose two basic requirements on the implementations of all functions that \ncomprise f(x). The .rst requirement is that we have an error bound and time complexity speci.cation for \neach implementation of each func\u00adtion. In this example we will use the following model: the orig\u00adinal \nimplementation of log(\u00b7) executes in time Tlog,0 with error Elog,0 =0; the original implementation of \nsin(\u00b7) executes in time Tsin,0 with error Esin,0 =0. We have two alternate implementa\u00adtions of log(\u00b7) \nand sin(\u00b7), where the i-th implementation of a given v function u .{log(\u00b7), sin(\u00b7)} runs in time Tu,i \n=1 - i Tu,0 5 with error Elog,i = i \u00b7 0.008, and Esin,i = i \u00b7 0.004 (i .{1, 2}). The second requirement \nis that the error propagation of the entire computation is bounded by a linear function. This require\u00adment \nis satis.ed if the functions that comprise the computation are Lipschitz continuous2. In our example, \nthe function sin(\u00b7) is 1-Lipschitz continuous, since its derivative is bounded by 1. The function log(x) \nis also Lipschitz continuous, when x = 1. Finally, the product function \u00d7 is Lipschitz continuous, when \nthe two in\u00adputs are bounded. We remark here that this second requirement en\u00adsures that an error introduced \nby an approximate implementation propagates to cause at most a linear change in the .nal output. Finding \nthe (1 + e)-Optimal Program Con.guration. Given performance and accuracy speci.cations for each function, \nwe can run our optimization algorithm to (1 + e)-approximately calculate the optimal accuracy-performance \ntradeoff curve. For each point on the curve our algorithm can also produce a randomized program con.guration \nthat achieves this tradeoff. Given a target expected error bound ., we use the tradeoff curve to .nd \na randomized program con.guration that executes in expected time t. The (1 + e)-approximation ensures \nthat this expected running time t is at most (1 + e) times the optimal expected running time for the \nexpected error bound .. In this example we use e =0.01 so that our optimized program will produce a 1.01-approximation. \nIn addition, we de.ne: the number of inputs n = 10000,  the overall expected error tolerance .=0.05, \nand  the running times Tsin,0 =0.08 \u00b5s and Tlog,0 =0.07 \u00b5s.  For this example our optimization algorithm \nidenti.es the point (.,T0/1.71) on the tradeoff curve, where T0 is the running time of the original program. \nThis indicates that the optimized program achieves a speedup of 1.71 over the original program while \nkeep\u00ading the expected value below the bound .. Table 1 presents the randomized program con.guration that \nachieves this tradeoff. This 2 A univariate function is a-Lipschitz continuous if for any d> 0, it follows \nthat |f(x) - f(x + d)| < ad. As a special case, a differentiable function is Lipschitz continuous if \n|f'(x)|= a. This de.nition extends to multivariate functions. randomized program con.guration consists \nof two program con.g\u00adurations C1 and C2. Each con.guration has an associated weight which is the probability \nwith which the randomized program will execute that con.guration. The table also presents the error and \nspeedup that each con.guration produces. The con.guration C1 selects the less accurate approximate ver\u00adsions \nof the functions log(\u00b7) and sin(\u00b7), and uses all inputs to the averaging reduction node. The con.guration \nC2, on the other hand, selects more accurate approximate versions of the functions log(\u00b7) and sin(\u00b7), \nand at the same time samples 4750 of the 10,000 origi\u00adnal inputs. Note that individually neither C1 nor \nC2 can achieve the desired tradeoff. The con.guration C1 produces a more accurate output but also executes \nsigni.cantly slower than the optimal program. The con.guration C2 executes much faster than the optimal \nprogram, but with expected error greater than the desired bound .. The ran\u00addomized program selects con.guration \nC1 with probability 60.8% and C2 with probability 39.2%. The randomized program has ex\u00adpected error . \nand expected running time T0/1.71. We can use the same tradeoff curve to obtain a randomized pro\u00adgram \nthat minimizes the expected error .' subject to the execution time constraint t '. In our example, if \nthe time bound t' = T0/1.71 the optimization algorithm will produce the program con.guration from Table \n1 with expected error .' =.. More generally, our optimization algorithm will produce an ef.cient representation \nof a probability distribution over program con.gurations along with an ef.cient procedure to sample this \ndistribution to obtain a program con.guration for each execution. 3. Model of Approximate Computation \nWe next de.ne the graph model of computation, including the error-propagation constraints for function \nnodes, and present the accuracy-aware substitution and sampling transformations. 3.1 De.nitions Programs. \nIn our model of computation, programs consist of a directed tree of computation nodes and reduction nodes. \nEach edge in the tree transmits a stream of values. The size of each edge indicates the number of transmitted \nvalues. The multiple values transmitted along an edge can often be understood as a stream of numbers \nwith the same purpose for example, a million pixels from an image or a thousand samples from a sensor. \nFigure 2 presents an example of a program under our de.nition. Reduction Nodes. Each reduction node has \na single input edge and a single output edge. It reduces the size of its input by some multiplicative \nfactor, which we call its reduction factor. A node with reduction factor S has an input edge of size \nR \u00b7 S and an output edge of size R. The node divides the R\u00b7S inputs into blocks of size S. It produces \nR outputs by applying an S-to-1 aggregation function (such as min, max, or mean) to each of the R blocks. \nFor clarity of exposition and to avoid a proliferation of notation, we primarily focus on one speci.c \ntype of reduction node, which we call an averaging node. An averaging node with reduction factor S will \noutput the average of the .rst S values as the .rst output, the average of the next S values as the second \noutput, and so on. The techniques that we present are quite general and apply to any reduction operation \nthat can be approximated well by sampling. Section 8 describes how to extend our algorithm to work with \nother reduction operations.  Figure 2: An example program in our model of computation. Computation \nNodes. A computation node has potentially multi\u00adple input edges and a single output edge. A computation \nnode of size R has: a single output edge of size R;  a non-negative number of input edges, each of \nsize either 1 (which we call a control-input edge), or some multiple tR of R (which we call a data-input \nedge).  Each control-input edge carries a single global constant. Data\u00adinput edges carry a stream of \nvalues which the computation node partitions into R chunks. The computation node executes R times to \nproduce R outputs, with each execution processing the value from each control-input edge and a block \nof t values from each data-input edge. The executions are independent. For example, consider a computation \nnode of size 10 with two input edges: one data-input edge of size 1000, denoted by (a1,a2,...,a1000), \nand one control-input edge of size 1, denoted by b. Then, the function that outputs the vector 100 200 \n1000 sin(ai,b), sin(ai,b),..., sin(ai,b)(1) i=1 i=101 i=901 is a computation node. We remark here that \na reduction node is a special kind of com\u00adputation node. We treat computation and reduction nodes sepa\u00adrately \nbecause we optimize computation nodes with substitution transformations and reduction nodes with sampling \ntransforma\u00adtions (see Section 3.2). Inner Structure of Computation Nodes. A computation node can be further \ndecomposed into one or more function nodes, connected via a directed acyclic graph (DAG). Like computation \nnodes, each function node has potentially multiple input edges and a single output edge. The size of \neach input edge is either 1 or a multiple of the size of the output edge. The functions can be of arbitrary \ncomplexity and can contain language constructs such as conditional statements and loops. For example, \nthe computation node in Eq.(1) can be further de\u00ad composed as shown in Figure 3a. Although we require \nthe compu\u00ad tation nodes and edges in each program to form a tree, the function nodes and edges in each \ncomputation node can form a DAG (see, for example, Figure 3b). In principle, any computation node can \nbe represented as a single function node, but its decomposition into multiple function nodes allows for \n.ner granularity and more transformation choices when optimizing entire program. (a) (b) (c) Figure 3: \n(a)(b): A closer look at two computation nodes, and (c): Numerical integration example, revisited. Example. \nSection 2 presented a numerical integration program example. Figure 3c presents this example in our model \nof compu\u00ad tation (compare with Figure 1). Note that the multiplicity of compu\u00ad tation nodes in Figure \n1 corresponds to the edge sizes in Figure 3c. The log function node with input and output edges of size \nn runs n times. Each run consumes a single input and produces a single output. The \u00d7 function node with \ninput edges of size n and 1 runs n times. Each execution produces as output the product of an xi with \nthe common value b - a from the control edge.  3.2 Transformations In a program con.guration, we specify \nthe following two kinds of transformations at function and reduction nodes. Substitution. For each function \nnode fu of size R, we have a polynomial number of implementations fu,1,...,fu,k. The function runs R \ntimes. We require each implementation to have the following properties: each run of fu,i is in expected \ntime Tu,i, giving a total expected running time of R \u00b7 Tu,i, and each run of fu,i produces an expected \nabsolute additive error of at most Eu,i, i.e., .x, E[|fu(x)-fu,i(x)|] = Eu,i. (The expectation is over \nthe the randomness of fu,i and fu.) We assume that all (Tu,i,Eu,i) pairs are known in advance (they are \nconstants or depend only on control inputs). Sampling. For each reduction node r with reduction factor \nSr, we can decrease this factor Sr to a smaller factor sr . {1,...,Sr} at the expense of introducing \nsome additive sam\u00adpling error Er(sr). For example, for an averaging node, instead of averaging all Sr \ninputs, we would randomly select sr in\u00adputs (without replacement) and output the average of the cho\u00adsen \nsamples. For convenience, we denote the sampling rate of node r as .r = Ssrr . If the output edge is \nof size R, the computation selects sr \u00b7 R inputs, instead of all Sr \u00b7 R inputs. The values for the reduction \nnode inputs which are not selected need not be computed. Dis\u00adcarding the computations that would otherwise \nproduce these discarded inputs produces a speed-up factor of .r = Ssrr for all nodes above r in the computation \ntree. The following lemma provides a bound on the sampling error m Sr-sr Er(sr)=(B - A) for an averaging \nnode. The proof sr(Sr-1) is available in the full version of the paper.  Lemma 3.1. Given m numbers \nx1,x2,...,xm . [A, B], ran\u00addomly sampling s of the numbers xi1 ,...,xis (without replace\u00adment) and computing \nthe sample average gives an approximation to x1+\u00b7\u00b7\u00b7+xm m with the following expected error guarantee: \n + \u00b7\u00b7\u00b7 + xis x1 + \u00b7\u00b7\u00b7 + xm xi1 Ei1,...,is- smm - s = (B - A). s(m - 1)  3.3 Error Propagation The errors \nthat the transformations induce in one part of the com\u00adputation propagate through the rest of the computation \nand can be ampli.ed or attenuated in the process. We next provide constraints on the form of functions \nthat characterize this error propagation. These constraints hold for all functions in our model of computa\u00adtion \n(regardless of whether they have alternate implementations or not). We assume that for each function \nnode fu(x1,...,xm) with m inputs, if each input xj is replaced by some approximate input x j such that \nE[|xj - x j |] = dj , the propagation error is bounded by a linear error propagation function Eu: E|fu(x1,...,xm) \n- fu( x1,..., x m)|=Eu d1,...,dm . (2) We assume that all of the error propagation functions Eu for the \nfunctions fu are known a priori: E|fu(x1,...,xm) - fu( x1,..., x m)|= aj dj . (3) j This condition is \nsatis.ed if all functions fu are Lipschitz\u00adcontinuous with parameters a. Furthermore, if fu(x1,...,xm) \nis .fu(x1,...,xm) differentiable, we can let ai = maxx.xi . If fu is itself probabilistic, we can take \nthe expected value of such ai s. Substitute Implementations. For functions with multiple imple\u00admentations, \nthe overall error when we choose the i-th implemen\u00adtation fu,i is bounded by an error propagation function \nEu and the local error induced by the i-th implementation Eu,i (de.ned in the previous subsection): \nE|fu(x1,...,xm)-fu,i( x1,..., x m)|=Eu d1,...,dm +Eu,i. (4) This bound follows immediately from the triangle \ninequality. We remark here that the randomness for the expectation in Eq.(4) comes from 1) the randomness \nof its input x 1,..., x m (caused by errors from previous parts of the computation) and 2) random choices \nin the possibly probabilistic implementation fu,i. These two sources of randomness are mutually independent. \nAveraging Reduction Node. The averaging function is a Lipschitz\u00adcontinuous function with all ai = 1 , \nso in addition to Lemma 3.1 we have: m Corollary 3.2. Consider an averaging node that selects s random \nsamples from its m inputs, where each input x j has bounded error E[|x j - xj |] = dj . Then: xi1 + \n\u00b7\u00b7\u00b7 + xis x1 + \u00b7\u00b7\u00b7 + xm Ei1,...,is,x 1,...,x m - sm m 1 m - s = dj +(B - A). ms(m - 1) j=1 If all input \nvalues have the same error bound E[|x j - xj |] = d, n then 1 m dj = d. mj=1 4. Approximation Questions \nWe focus on the following question: Question 1. Given a program P in our model of computation and using \nrandomized con.gurations, what is the optimal error-time tradeoff curve that our approximate computations \ninduce? Here the time and error refer to the expected running time and error of the program. We say that \nthe expected error of program P ' is ., if for all input x, E[|P (x) - P ' (x)|] = .. The error-time \ntradeoff curve is a pair of functions (E(\u00b7),T (\u00b7)), such that E(t) is the optimal expected error of the \nprogram if the expected running time is no more than t, and T (e) is the optimal expected running time \nof the program if the expected error is no more than e. The substitution and sampling transformations \ngive rise to an exponentially large space of possible program con.gurations. We optimize over arbitrary \nprobability distributions of such con.gura\u00adtions. A naive optimization algorithm would therefore run \nin time at least exponential in the size of the program. We present an al\u00adgorithm that approximately \nsolves Question 1 within a factor of (1+e) in time:3 1) polynomial in the size of the computation graph, \nand 2) polynomial in 1 e . The algorithm uses linear programming and a novel technique called bi-dimensional \ndiscretization, which we present in Section 5. A successful answer to the above question leads directly \nto the following additional consequences: Consequence 1: Optimizing Time Subject to Error Question 2. \nGiven a program P in our model, and an overall error tolerance ., what is the optimal (possibly randomized) \nprogram P ' available within our space of transformations, with expected error no more than .? We can \nanswer this question approximately using the optimiza\u00adtion algorithm for Question 1. This algorithm will \nproduce a ran\u00ad domized program with expected running time no more than (1 + e) times the optimal running \ntime and expected error no more than .. The algorithm can also answer the symmetric question to .nd a \n(1 + e)-approximation of the optimal program that minimizes the expected error given a bound on the expected \nrunning time. Consequence 2: From Error to Variance We say that the overall variance (i.e., expected \nsquared error) of a randomized program P ' is .2, if for all input x, E[|P (x) - P ' (x)|2] = .2. A variant \nof our algorithm for Question 1 (1 + e)\u00adapproximately answers the following questions: Question 3. Given \na program P in our model of computation, what is the optimal error-variance tradeoff curve that our approximate \ncomputations induce? Question 4. Given a program P in our model, and an overall variance tolerance .2, \nwhat is the optimal (possibly randomized) program P ' available within our space of transformations, \nwith variance no more than .2? Section 7 presents the algorithm for these questions. Consequence 3: Probabilities \nof Large Errors A bound on the expected error or variance also provides a bound on the probability of \nobserving large errors. In particular, an execution 3 We say that we approximately obtain the curve within \na factor of (1+e), if for any given running time t, the difference between the optimal error E(t) and \nour E (t) is at most eE(t), and similarly for the time function T (e). Our algorithm is a fully polynomial-time \napproximation scheme (FPTAS). Section 5 presents a more precise de.nition in which the error function \nE (t) is also subject to an additive error of some arbitrarily small constant.  of a program with expected \nerror . will produce an absolute error greater than t. with probability at most 1 t (this bound follows \nimmediately from Markov s inequality). Similarly, an execution of a program with variance .2 will produce \nan absolute error greater than t. with probability at most t1 2 . 5. Optimization Algorithm for Question \n1 We next describe a recursive, dynamic programming optimization algorithm which exploits the tree structure \nof the program. To com\u00adpute the approximate optimal tradeoff curve for the entire program, the algorithm \ncomputes and combines the approximate optimal tradeoff curves for the subprograms. We stage the presentation \nas follows: Computation Nodes Only: In Section 5.1, we show how to compute the optimal tradeoff curve \nexactly when the computa\u00adtion consists only of computation nodes and has no reduction nodes. We reduce \nthe optimization problem to a linear program (which is ef.ciently solvable).  Bi-dimensional Discretization: \nIn Section 5.2, we intro\u00adduce our bi-dimensional discretization technique, which con\u00adstructs a piecewise-linear \ndiscretization of any tradeoff curve (E(\u00b7),T (\u00b7)), such that 1) there are only O( 1 e ) segments on the \ndiscretized curve, and 2) at the same time the discretization approximates (E(\u00b7),T (\u00b7)) to within a multiplicative \nfactor of (1 + e).  A Single Reduction Node: In Section 5.3, we show how to compute the approximate \ntradeoff curve when the given pro\u00adgram consists of computation nodes that produce the input for a single \nreduction node r (see Figure 6). We .rst work with the curve when the reduction factor s at the reduction \nnode r is constrained to be a single integer value. Given an expected error tolerance e for the entire \ncomputa\u00adtion, each randomized con.guration in the optimal randomized program allocates part of the expected \nerror Er(s) to the sam\u00adpling transformation on the reduction node and the remaining expected error esub \n= e - Er(s) to the substitution transfor\u00admations on the subprogram with only computation nodes. One inef.cient \nway to .nd the optimal randomized con.gura\u00adtion for a given expected error e is to simply search all \npossible integer values of s to .nd the optimal allocation that minimizes the running time. This approach \nis inef.cient because the num\u00adber of choices of s may be large. We therefore discretize the tradeoff \ncurve for the input to the reduction node into a small set of linear pieces. It is straightforward to \ncompute the optimal integer value of s within each linear piece. In this way we ob\u00adtain an approximate \noptimal tradeoff curve for the output of the reduction node when the reduction factor s is constrained \nto be a single integer. We next use this curve to derive an approximate optimal trade\u00adoff curve when \nthe reduction factor s can be determined by a probabilistic choice among multiple integer values. Ideally, \nwe would use the convex envelope of the original curve to obtain this new curve. But because the original \ncurve has an in.nite number of points, it is infeasible to work with this convex enve\u00adlope directly. \nWe therefore perform another discretization to ob\u00adtain a piecewise-linear curve that we can represent \nwith a small number of points. We work with the convex envelope of this new discretized curve to obtain \nthe .nal approximation to the optimal tradeoff curve for the output of the reduction node r. This curve \nincorporates the effect of both the substitution trans\u00adformations on the computation nodes and the sampling \ntrans\u00adformation on the reduction node.  Figure 4: Example to illustrate the computation of time and \nerror. The Final Dynamic Programming Algorithm: In Section 5.4, we provide an algorithm that computes \nan approximate error\u00adtime tradeoff curve for an arbitrary program in our model of computation. Each step \nuses the algorithm from Section 5.3 to compute the approximate discretized tradeoff curve for a sub\u00adtree \nrooted at a topmost reduction node (this subtree includes the computation nodes that produce the input \nto the reduction node). It then uses this tradeoff curve to replace this subtree with a single function \nnode. It then recursively applies the al\u00adgorithm to the new program, terminating when it computes the \napproximate discretized tradeoff curve for the output of the .nal node in the program. 5.1 Stage 1: \nComputation Nodes Only We start with a base case in which the program consists only of computation nodes \nwith no reduction nodes. We show how to use linear programming to compute the optimal error-time tradeoff \ncurve for this case. Variables x. For each function node fu, the variable xu,i . [0, 1] indicates the \nprobability of running the i-th implementation fu,i. n We also have the constraint that i xu,i =1. Running \nTime TIME(x). Since there are no reduction nodes in the program, each function node fu will run Ru times \n(recall that Ru is the number of values carried on the output edge of fu). The running time is simply \nthe weighted sum of the running times of the function nodes (where each weight is the probability of \nselecting each corresponding implementation): TIME(x)= (xu,i \u00b7 Tu,i \u00b7 Ru) . (5) ui Here the summation \nu is over all function nodes and i is over all implementations of fu. Total Error ERROR(x). The total \nerror of the program also ad\u00admits a linear form. For each function node fu, the i-th implemen\u00adtation \nfu,i incurs a local error Eu,i on each output value. By the linear error propagation assumption, this \nEu,i is ampli.ed by a constant factor \u00dfu which depends on the program structure. It is possible to compute \nthe \u00dfu with a traversal of the program back\u00adward against the .ow of values. Consider, for example, \u00df1 \nfor function node f1 in the pro\u00adgram in Figure 4. Let a2 be the linear error propagation fac\u00adtor for \nthe univariate function f2(\u00b7). The function f3(\u00b7, \u00b7, \u00b7) is trivariate with 3 propagation factors (a3,1,a3,2,a3,3). \nWe simi\u00adlarly de.ne (a4,1,...,a4,4) for the quadvariate function f4, and (a5,1,a5,2,a5,3) for f5. Any \nerror in an output value of f1 will be ampli.ed by a factor \u00df1: \u00df1 =a2(a4,1+a4,2+a4,3)+(a3,1+a3,2+a3,3)a4,4(a5,1+a5,2). \n The total expected error of the program is: ERROR(x)= (xu,i \u00b7 Eu,i \u00b7 \u00dfu) . (6) ui Optimization Given \na .xed overall error tolerance ., the follow\u00ading linear program de.nes the minimum expected running time: \nVariables: x Constraints: 0 = xu,i = 1,n .u, i i xu,i = 1 .u (7) ERROR(x) = . Minimize: TIME(x) Byswappingtherolesof \nERROR(x) and TIME(x), it is possible to obtain a linear program that de.nes the minimum expected error \ntolerance for a given expected maximum running time.  5.2 Error-Time Tradeoff Curves In the previous \nsection, we use linear programming to obtain the optimal error-time tradeoff curve. Since there are an \nin.nite num\u00adber of points on this curve, we de.ne the curve in terms of func\u00adtions. To avoid unnecessary \ncomplication when doing inversions, we de.ne the curve using two related functions E(\u00b7) and T (\u00b7): De.nition \n5.1. The (error-time) tradeoff curve of a program is a pair of functions (E(\u00b7),T (\u00b7)) such that E(t) \nis the optimal expected error of the program if the expected running time is no more than t and T (e) \nis the optimal expected running time of the program if the expected error is no more than e. We say that \na tradeoff curve is ef.ciently computable if both functions E and T are ef.ciently computable.4 The following \nprop\u00aderty is important to keep in mind: Lemma 5.2. In a tradeoff curve (E, T ), both E and T are non\u00adincreasing \nconvex functions. Proof. T is always non-increasing because when the allowed error increases the minimum \nrunning time does not increase, and simi\u00adlarly for E. We prove convexity by contradiction: assume aE(t1) \n+ (1 - a)E(t2) <E(at1 + (1 - a)t2) for some a . (0, 1). Then choose the optimal program for E(t1) with \nprobability a, and the optimal program for E(t2) with probability 1 - a. The result is a new program \nP ' in our probabilistic transformation space. This new program P ' has an expected running time less \nthan the optimal running time E(at1 + (1 - a)t2), contradicting the optimality of E. A similar proof \nestablishes the convexity of T . We remark here that, given a running time t, one can compute E and be \nsure that (E(t),t) is on the curve; but one cannot write down all of the in.nite number of points on \nthe curve concisely. We therefore introduce a bi-dimensional discretization technique that allows us \nto approximate (E,T ) within a factor of (1 + e). This technique uses a piecewise linear function with \nroughly O( 1 e ) segments to approximate the curve. Our bi-dimensional discretization technique (see \nFigure 5) ap\u00ad proximates E in the bounded range [0,Emax], where Emax is an upper bound on the expected \nerror, and approximates T in the bounded range [T (Emax),T (0)]. We assume that we are given the maximum \nacceptable error Emax (for example, by a user of the program). It is also possible to conservatively \ncompute an Emax by analyzing the least-accurate possible execution of the program. 4 In the remainder \nof the paper we refer to the function E(\u00b7) simply as E and to the function T (\u00b7) as T .  Figure 5: An \nexample of bi-dimensional discretization. De.nition 5.3. Given a tradeoff curve (E, T ) where E and T \nare both non-increasing, along with constants e . (0, 1) and eE > 0, we de.ne the (e, eE )-discretization \ncurve of (E, T ) to be the piecewise-linear curve de.ned by the following set of endpoints (see Figure \n5): the two black points (0,T (0)), (Emax,T (Emax)),  the red points (ei,T (ei)) where ei = eE (1+e)i \nfor some i = 0 and eE (1 + e)i <Emax, and  the blue points (E(ti),ti) where ti = T (Emax)(1 + e)i for \nsome i = 1 and T (Emax)(1 + e)i <T (0).  Note that there is some asymmetry in the discretization of \nthe two axes. For the vertical time axis we know that the minimum run\u00adning time of a program is T (Emax) \n> 0, which is always greater than zero since a program always runs in a positive amount of time. However, \nwe discretize the horizontal error axis proportional to powers of (1 + e)i for values above eE . This \nis because the er\u00adror of a program can indeed reach zero, and we cannot discretize forever.5 The following \nclaim follows immediately from the de.nition: Claim 5.4. If the original curve (E, T ) is non-increasing \nand convex, the discretized curve ( T ) is also non-increasing and E, convex. 5.2.1 Accuracy of bi-dimensional \ndiscretization We next de.ne notation for the bi-dimensional tradeoff curve dis\u00adcretization: De.nition \n5.5. A curve ( T ) is an (e, eE )-approximation to E, (E, T ) if for any error 0 = e = Emax, 0 = T (e) \n- T (e) = eT (e) , and for any running time T (Emax) = t = T (0), 0 = E (t) - E(t) = eE(t)+ eE . We say \nthat such an approximation has a multiplicative error of e and an additive error of eE. Lemma 5.6. If \n( T ) is an (e, eE )-discretization of (E, T ), then E, it is an (e, eE )-approximation of (E, T ). Proof \nSketch. The idea of the proof is that, since we have dis\u00adcretized the vertical time axis in an exponential \nmanner, if we com\u00adpute T (e) for any value e, the result does not differ from T (e) by 5 If instead we \nknow that the minimum expected error is greater than zero (i.e., E(Tmax) > 0) for some maximum possible \nrunning time Tmax, then we can de.ne eE = E(Tmax) just like our horizontal axis.  Figure 6: Algorithm \nfor Stage 2. more than a factor of (1 + e). Similarly, since we have discretized the horizontal axis \nin an exponential manner, if we compute E (t) for any value t, the result does not differ by more than \na factor of (1 + e), except when E(t) is smaller than eE (when we stop the discretization). But even \nin that case the value E (t)-E(t) remains smaller than eE . Because every point on the new piecewise-linear \ncurve ( T ) E, is a linear combination of some points on the original curve (E, T ), 0 = T (e) - T (e) \nand 0 = E (t) - E(t), Because (E, T ) is convex (recall Lemma 5.2), the approximation will always lie \nabove the original curve.  5.2.2 Complexity of bi-dimensional discretization The number of segments \nthat the approximate tradeoff curve has in an (e, eE )-discretization is at most T (0) log Emax def eE \nlog T (Emax) np =2+ + log(1 + e) log(1 + e) 11 1 = O log Emax + log + log Tmax + log , (8)eeE Tmin where \nTmin is a lower bound on the expected execution time and Tmax is an upper bound on the expected execution \ntime. Our dis\u00adcretization algorithm only needs to know Emax in advance, while Tmax and Tmin are values \nthat we will need later in the complexity analysis. 5.2.3 Discretization on an approximate curve The \nabove analysis does not rely on the fact that the original tradeoff curve (E, T ) is exact. In fact, \nif the original curve (E, T ) is only an (e, eE )-approximation to the exact error-time tradeoff curve, \nand if ( T ) is the (e ' E )-discretization of (E,T ), then E, ,e ' one can verify by the triangle inequality \nthat ( T ) is a piecewise E, linear curve that is an (e + e ' + ee ' ,eE + e ' E ) approximation of the \nexact error-time tradeoff curve.  5.3 Stage 2: A Single Reduction Node We now consider a program with \nexactly one reduction node r, with original reduction factor S, at the end of the computation. The example \nin Figure 3c is such a program. We describe our optimization algorithm for this case step by step as \nillustrated in Figure 6. We .rst de.ne the error-time tradeoff curve for the subprogram without the reduction \nnode r to be (Esub,Tsub) (Section 5.1 de\u00ad scribes how to compute this curve; Lemma 5.2 ensures that it \nis non-increasing and convex). In other words, for every input value to the reduction node r, if the \nallowed running time for comput\u00ading this value is t, then the optimal expected error is Esub(t) and similarly \nfor Tsub(e). Note that when computing (Esub,Tsub) as described in Section 5.1, the size of the output \nedge Ri for each node i must be divided by S, as the curve (Esub,Tsub) character\u00adizes each single input \nvalue to the reduction node r. If at reduction node r we choose an actual reduction factor s .{1, 2,...,S}, \nthe total running time and error of this entire program is:6 TIME = Tsub \u00d7 s (9) ERROR = Esub + Er(s) \n. This is because, to obtain s values on the input to r, we need to run the subprogram s times with a \ntotal time Tsub \u00d7 s; and by Corollary 3.2, the total error of the output of an averaging reduction node \nis simply the sum of its input error Esub, and a local error Er(s) incurred by the sampling.7 Let (E1,T1) \nbe the exact tradeoff curve (E1,T1) of the entire program, assuming that we can choose only a single \nvalue of s. We start by describing how to compute this (E1,T1) approximately. 5.3.1 Approximating (E1,T1): \nsingle choice of s By de.nition, we can write (E1,T1) in terms of the following two optimization problems: \n T1(e) = min Tsub(esub) \u00d7 ss.{1,...,S} esub+Er(s)=e and E1(t) = min Esub(tsub)+ Er(s), s.{1,...,S}tsub\u00d7s=t \nwhere the .rst optimization is over variables s and esub, and the second optimization is over variables \ns and tsub. We emphasize here that this curve (E1,T1) is by de.nition non-increasing (be\u00adcause (Esub,Tsub) \nis non-increasing), but may not be convex. Because these optimization problems may not be convex, they \nmay be dif.cult to solve in general. But thanks to the piecewise\u00adlinear discretization de.ned in Section \n5.2, we can approximately solve these optimization problems ef.ciently. Speci.cally, we pro\u00adduce a bi-dimensional \ndiscretization (E sub,T sub) that (e, eE )\u00adapproximates (Esub,Tsub) (as illustrated in Figure 6). We \nthen solve the following two optimization problems: ' T1(e) = min Tsub(esub) \u00d7 s s.{1,...,S}esub+Er(s)=e \n and E1' (t) = min E sub(tsub)+ Er(s). (10) s.{1,...,S}tsub\u00d7s=t We remark here that E1 ' and T1 ' are \nboth non-increasing since E sub and T sub are non-increasing using Claim 5.4. Each of these two problems \ncan be solved by 1) comput\u00ading the optimal value within each linear segment de.ned by (E sub,k,T sub,k) \nand (E sub,k+1,T sub,k+1), and 2) returning the smallest optimal value across all linear segments. Suppose \nthat we are computing T1' (e) given an error e. In the linear piece of T sub = aesub + b (here a and \nb are the slope and intercept of the linear segment), we have esub = e - Er(s). The objective that we \nare minimizing therefore becomes univariate with respect to s: T sub \u00d7 s =(aesub + b) \u00d7 s =(a(e - Er(s)) \n+ b) \u00d7 s. (11) 6 Here we have ignored the running time for the sampling procedure in the reduction node, \nas it is often negligible in comparison to other computations in the program. It is possible to add this \nsampling time to the formula for TIME in a straightforward manner. 7 We extend this analysis to other \ntypes of reduction nodes in Section 8.  The calculation of s is a simple univariate optimization problem \nthat we can solve quickly using our expression for Er(s).8 Com\u00adparing the optimal answers from all of \nthe linear pieces gives us an ef.cient algorithm to determine T1' (e), and similarly for E1' (t). This \nalgorithm runs in time linear in the number of pieces np (re\u00adcall Eq.(8)) in our bi-dimensional discretization. \nThis .nishes the '' ' computation of (E1,T 1) in Figure 6. We next show that (E1' ,T 1) accurately approximates \n(E,T ): Claim 5.7. (E1' ,T 1' ) is an (e, eE )-approximation to (E1,T1). Proof. Because T sub approximates \nTsub, we know that for any esub, 0 = T sub(esub) - Tsub(esub) = eTsub(esub), and this gives: T1' (e) \n= min T sub(esub) \u00d7 s s.{1,...,S} esub+Er (s)=e = min (1+ e)Tsub(esub) \u00d7 s = (1+ e)T1(e). s.{1,...,S} \nesub+Er(s)=e Similarly, this also gives that T1' (e) = T1(e) using 0 = T sub(esub)- Tsub(esub). Using \na similar technique, we can also prove that E1' (t) = E1(t) and E1' (t) = (1 + e)E1(t)+ eE . Therefore, \nthe (E1' ,T 1' ) curve (e, eE)-approximates the exact tradeoff curve (E1,T1). We next further bi-dimensionally \ndiscretize the curve (E1' ,T 1' ) that we obtained into (E 1' ,T 1' ) using the same discretization param\u00adeter \n(e, eE ). A discretization of an approximate curve is still ap\u00adproximate (see Section Section 5.2.3). \nWe therefore conclude that Claim 5.8. (E 1' ,T 1' ) is a (2e+e2 , 2eE)-approximation to (E1,T1).  5.3.2 \nApproximating (E, T ): probabilistic choice of s Now, we de.ne (E,T ) to be the exact tradeoff curve \n(E1,T1) of the entire program, assuming that we can choose s probabilistically. We claim: Claim 5.9. \n(E, T ) is the convex envelope of (E1,T1). Proof. We .rst prove the claim that T is the convex envelope \nof T1. The proof for E is similar. One side of the proof is straightforward: every weighted combination \nof points on T1 should lie on or above T , because this weighted combination is one candidate randomized \ncon.guration that chooses s probabilistically, and T is de.ned to be the optimal curve that takes into \naccount all such randomized con.gurations. For the other side of the proof, we need to show that every \npoint on T is a weighted combination of points on T1. Let us take an arbitrary point (e, T (e)). Suppose \nthat T (e) is achieved when the optimal probabilistic choice of s is {(si,pi)}i=1 at reduction node r, \nwhere we choose si with probability pi and when si is chosen, the overall error-time incurred is (ei,ti). \nTherefore, we nn have e = piei and T (e)= ii piti. Because T (e) is the exact optimal tradeoff curve, \neach ti is also minimized with respect to ei and the .xed choice of si. This is equivalent to saying \nthat (ei,ti) lies on the curve (E1,T1), i.e., T1(ei)= ti. This implies that T is the convex envelope \nof T1. In general, computing the convex envelope of an arbitrary func\u00adtion (E1,T1) may be hard, but thanks \nto our piecewise-linear dis\u00adcretization, we can compute the convex envelope of (E 1' ,T 1' ) easily. \nE ' ' E ' ' Let us denote the convex envelope of ( 1,T1) by ( ,T ). In fact, ( ' ' E ' ' E ,T ) can be \ncomputed in time O(np log np) because ( 1,T1) contains only np endpoints. 8 Note that for each different \ntype of reduction code, the optimization procedure for this univariate optimization can be hardcoded. \nSince (E 1' ,T 1' ) is a (2e + e2,eE )-approximation to (E1,T1) by Claim 5.8, we should expect the same \nproperty to hold for their convex envelopes: Claim 5.10. (E ' ,T ' ) is a (2e+e2 , 2eE )-approximation \nto (E, T ). Proof. By the de.nition of convex envelope, for all time t, there exists some a . [0, 1] \nsuch that E(t)= aE1(t1) + (1 - a)E1(t2) and at1 + (1 - a)t2 = t. Then, '' ' E (t) = aE 1(t1) + (1 - a)E \n1(t2) = 2eE + (1 + 2e + e2)(aE1(t1) + (1 - a)E1(t2)) =2eE + (1 + 2e + e2)E(t) , where the .rst inequality \nuses the fact that E ' is the convex en\u00ad ' E ' velope of E1, and the second uses the fact that 1 approximates \nE1. At the same time, there exists some \u00df . [0, 1] such that E ' (t)= \u00df ' E ' E1(t3) + (1 - \u00df) 1(t4) \nand \u00dft3 + (1 - \u00df)t4 = t. Then, '' ' E (t)= \u00dfE 1(t3) + (1 - \u00df)E 1(t4) = \u00dfE1(t3) + (1 - \u00df)E1(t4) = E(t) \n, where the .rst inequality uses the fact that E 1 ' approximates E1, and the second uses the fact that \nE is the convex envelope of E1. We can derive the two similar inequalities for the time function T ' \nand conclude that (E ' ,T ' ) is a (2e + e2 , 2eE )-approximation to (E, T ). So far, we have .nished \nall steps described in Figure 6. We have ended with a piecewise-linear tradeoff curve (E ' ,T ' ) that \n(2e + e2 , 2eE )-approximates the exact tradeoff curve (E, T ), taking into account the probabilistic \nchoices at this reduction node as well.  5.4 The Final Dynamic Programming Algorithm We next show how \nto compute the approximate error-time trade\u00adoff curve for any program in our model of computation. We \n.rst present the algorithm, then we discuss how to choose the discretiza\u00adtion parameters e and eE and \nhow the discretization errors com\u00adpose. If the program has no reduction nodes, we can simply apply the \nanalysis from Section 5.1. Otherwise, there must exist at least one topmost reduction node r whose input \nis computed from compu\u00adtation nodes only. Assume (E,T ) is the exact error-time tradeoff curve for the \noutput of r. Applying Stage 2 (Section 5.3) to the sub\u00ad program rooted at r, we can ef.ciently .nd some \npiecewise-linear E ' ' curve ( ,T ) that accurately approximates (E, T ). Recall that this piecewise-linear \ncurve (E ' ,T ' ) is convex (since it is a convex envelope). If we pick all of its at most np endpoints \non the curve P = {(Er,i,Tr,i)}np , then 1) every point on the curve i=1 can be spanned by at most two \npoints in P because the function is piecewise linear, and 2) all points that can be spanned by P lie \nabove the curve because the function is convex. The two observations 1) and 2) above indicate that we \ncan re\u00adplace this reduction node r along with all computation nodes above it, by a single function node \nfr such that its i-th substitute im\u00adplementation fr,i gives an error of Er,i and a running time of Tr,i. \nObservation 1) indicates that every error-time tradeoff point E ' ' on ( ,T ) can be implemented by a \nprobabilistic mixture of {fr,i}np , and observation 2) indicates that every error-time trade\u00ad i=1 off \npoint that can be implemented, is no better than the original E ' ' curve ( ,T ). In sum, this function \nnode achieves the same error\u00adtime tradeoff as the piecewise-linear curve (E ' ,T ' ).  This completes \nthe description of the algorithm. We can recur\u00adsively choose a reduction node with only computation nodes \nabove it, replace the subtree rooted at the reduction node with a function node, continue until we have \nno reduction nodes left, and compute the .nal error-time tradeoff curve. We next consider the accuracy \nand running time of this recursive algorithm. Maximum Error Propagation Factor (MEPF). We use a value \nMEPF to bound the cumulative additive error of the optimized program. Speci.cally, we choose MEPF so \nthat a local additive error e at some node in the computation will produce at most a e \u00d7 MEPF additive \nerror at the .nal output. Speci.cally, we set MEPF to be the maximum \u00dfu over all function nodes fu in \nthe program. Accuracy. Our algorithm repeatedly approximates error-time curves with their discretizations. \nThis typically occurs twice during the analysis of each reduction node. If the inputs of a reduction \nnode come from some computation node, the .rst discretization approximates the tradeoff curve for that \ncomputation node (we do not need this step if inputs come from some other reduction node.) The second \ndiscretization approximates the tradeoff curve for the outputs of the reduction node. By bounding the \ntotal error intro\u00adduced by all of these approximations, we show that our algorithm produces a good approximation \nto the exact error-time curve. We remark that there are two very different types of errors be\u00ading discussed \nhere: errors that we trade off against time in our ap\u00adproximate computation and errors that our optimization \nalgorithm incurs in computing the optimal error-time tradeoff curve for the approximate computation. \nFor clarity, we shall refer to the former as computation error and to the latter as optimization error. \nIf we choose parameters (e, eE ) for both discretizations of a single reduction node r, the argument \nin Section 5.2.3 shows that we obtain a (2e + e2 , 2eE )-approximation to its error-time tradeoff curve. \nA computation node may amplify the computation error, which will result in a corresponding increase in \nthe optimization error of our approximation to it. However, we can bound the total effect of all computation \nnodes on our optimization error using our linear error propagation assumption. In particular, we can \nstudy the effect of a single reduction node s (2e + e2 , 2eE)-approximation on the program s overall \nadditive and multiplicative optimization error: The additive error 2eE is multiplied by the computation \nnodes Lipschitz parameters. The result is an overall additive optimiza\u00adtion error of at most 2eE MEPF. \n The multiplicative error is not affected, since both the actual computation error and our approximation \nto it are multiplied by the same factors. The overall multiplicative optimization error introduced is \nthus at most 2e + e2 .  This bounds the optimization errors introduced by the dis\u00adcretization in our \nanalysis of a single reduction node. We can add these up to bound the total optimization error. ef ef \nE We .x parameters e = and eE = for all of O(n) O(n)\u00d7MEPF the discretizations, where n is the number \nof nodes in the original program. Since we have at most n reduction nodes, this sums up to a total multiplicative \nerror of e ' and additive error of e ' (both of E these errors are optimization errors) in the .nal approximation \nof the exact error-time tradeoff curve of the whole program. Our over\u00adall optimization algorithm thus \nproduces a piecewise-linear curve ( E )-approximates the exact error-time tradeoff E, T ), that (e ' \n,e ' curve. 9 If we choose e ' to be the smallest unit of error that we care about, then E this essentially \nbecomes an e ' multiplicative approximation to the actual error-time tradeoff curve. Time Complexity. \nThe most expensive operation in our optimiza\u00adtion algorithm is solving the multiple linear programs that \nthe algo\u00adrithm generates. We next bound the number of linear programs that the algorithm generates. Recall \nby Eq.(8) that np is the number of pieces in our bi\u00addimensional discretizations. By our choices of e \nand eE : 11 1 np = O log Emax + log + log Tmax + log eeE Tmin nnMEPF = O log Emax + log + log Tmax + \nlog . e' e ' E Tmin For each reduction node, we solve a linear program for each of the np points in its \ndiscretization. We therefore run the LP solver O(n \u00d7 np) number of times. The number of variables in \nthese linear program is O(n \u00d7 np), since each node may have np implementations (which occurs when we \nreplace a reduction node with its bi-dimensional discretization). This yields: Theorem 5.11. Our proposed \nrecursive algorithm calls the LP solver O(n \u00d7 np) times, each time with O(n \u00d7 np) variables. The algorithm \nproduces a (1 + e ' )-approximation to the exact error\u00adtime tradeoff curve, and thus approximately solves \nQuestion 1. Note that in practice we can .nd constants that appropriately bound Tmin, Tmax, Emax, MEPF \nand eE .10 In addition, all those numbers stay within log functions. We can therefore assume that in \npractice: n log n np O. e' We also note that 1) the worst-case time complexity of linear programming \nis polynomial, and 2) ef.cient linear programming algorithms exist in practice. 6. Optimization Algorithm \nfor Question 2 At this point, we have approximately solved the error-time tradeoff curve problem in Question \n1. Given an overall error tolerance ., we have an ef.cient algorithm that approximates the optimal running \ntime T (.) up to a multiplicative factor (1 + e). We next show that our algorithm is constructive: it \nuses substitu\u00adtion and sampling transformations to obtain a randomized program P ' with expected error \nbound . that runs in expected time no more than (1 + e)T (.). The algorithm therefore answers Question \n2 ap\u00ad proximately. Our proof .rst considers the two simpler cases that we introduced in Section 5. Stage \n1: Computation Nodes Only. If the program has no reduc\u00adtion nodes, then the solution x to the linear \nprogram gives explicitly the probability of choosing each implementation at each function node, which \nin turn gives us a randomized con.guration for the randomized program P ' . Stage 2: A Single Reduction \nNode. If the program consists of computation nodes that generate the input for a single reduction node \n(this reduction node is therefore the root of the program), recall that we have perform two discretizations \nas illustrated in Figure 6. Speci.cally, let (E, T ) be the exact error-time tradeoff curve of the .nal \noutput, which we have approximated by a piecewise\u00adlinear curve (E ' ,T ' ) in the last step of Figure \n6. We want to obtain ' '' a program P whose error-time pair is (.,T (.)), because T is guaranteed to \nbe a (1 + e)-approximation to T . 10 For example, we may lower bound Tmin with the clock cycle time of \nthe machine running the computation, eE with the smallest unit of error we care about, upper bound Tmax \nwith the estimated lifetime of the machine running the computation, Emax with the largest value representable \non the machine running the computation, and MEPF with the ratio between the largest and smallest value \nrepresentable on the machine running the computation.  By piecewise linearity, the point (.,T ' (.)) \nlies on some linear segment of (E ' ,T ' ): '' ' T (.) = .T (e1) + (1 - .)T (e2) .= .e1 + (1 - .)e2 , \nwhere [e1,e2] are the two endpoints of that segment. To achieve an error-time pair (.,T ' (.)), we can \nlet the .nal program P ' run 1) with probability . some randomized program P1 ' whose error\u00adtime pair \nis (e1,T ' (e1)), and 2) with probability 1 - . some other randomized program P2 ' whose error-time pair \nis (e2,T ' (e2)). We next verify that both P1 ' and P2 ' can be constructed explicitly. We focus on the \nconstruction of P1. The goal is to construct P1 ' with error-time pair p =(e1,T ' (e1)). Note that p \nis an endpoint of (E ' ,T ' ), and thus is also an end\u00ad '' ' point of (E 1' ,T 1). Since (E 1' ,T 1) \nis a discretization of (E1' ,T 1), p is also a point on the curve (E1' ,T 1' ). We can therefore write \np =(e1,T 1' (e1)). Recall that we obtained T1' (e1) by exactly solving the univariate optimization problem \nde.ned in Eq.(11). Because this solution is constructive, it provides us with the optimal reduction factor \ns to use at the reduction node. Substituting this optimal value s into Eq.(10), we obtain the error-time \npair (esub,T sub(esub)) that we should allocate to the subprogram (without the reduction node). We therefore \nonly need to construct a program for the subprogram whose error-time pair is (esub,T sub(esub)). Because \nT sub is a piecewise-linear discretization of Tsub, we can obtain a program whose error-time pair is \n(esub,T sub(esub)) by combining at most two points on the (Esub,Tsub) curve. The linear program described \nabove implements this (Esub,Tsub) curve. This completes the construction of P ' . We have therefore shown \nthat as part of the algorithm for Stage 2 (see Section 5.3), we can indeed construct a program P ' with \nthe desired error and time bounds (.,T ' (.)). Putting It All Together. We prove, by induction, that \nfor a pro\u00adgram with an arbitrary number of reduction nodes, we can obtain a randomized program P ' with \nexpected error bound . and expected running time no more than (1 + e)T (.). We are done if there is no \nreduction node (by Stage 1). Otherwise, suppose that we have nr reduction nodes. We will substitute one \nof the reduction nodes r, along with all computations above it, by a function node fr with piecewise-linear \nE ' ' tradeoff ( ,T ). This function node has a set of implementations {fr,i}np , and in our Stage 2, \nwe have actually shown that each i=1 fr,i, being a point on the (E ' ,T ' ) curve, is achievable by some \nexplicit randomized program Pi. In other words, this new function node fr is a real function node: every \napproximate implementation fr,i has a corresponding randomized program Pi that can be constructed by \nour algorithm. This reduces the problem into a case with nr - 1 reduction nodes. Using our induction \nhypothesis, we conclude that for any ., we can construct a program P ' that runs in expected time no \nmore than (1 + e)T (.). 7. Optimization Algorithm for Question 3 We next describe how to modify our algorithm \nif one instead is interested in the time-variance tradeoff. 7.1 Changes to Function Nodes For each function \nnode fu, the algorithm for Question 1 works with an error-time pair (Tu,i,Eu,i) for each implementation \nfu,i. Now we instead work with a pair (Tu,i,Vu,i), where Vu,i is the variance (i.e., the expected squared \nerror): .x, E[|fu(x) - fu,i(x)|2] = Vu,i , (12) where the expectation is over the the randomness of fu,i \nand fu. We also assume that the variance propagation function is linear with coef.cients a1,...,am. In \nother words, if fu has an arity of m, and each input xi is approximated by some input x i such that E[|xi \n- x i|2] = vi, then the .nal variance: E |fu(x1,...,xm)-fu,i( x1,..., x m)|2 = aj vj +2Vu,i . j (13) \nWe next check that a large class of meaningful functions satis\u00ad.es the above de.nition. Note that if \nfu is a deterministic Lipschitz\u00adcontinuous function (with respect to vector L =(L1,...,Lm)): E |fu(x) \n- fu,i(x )|2 = E |(fu(x) - fu(x )) + (fu(x ) - fu,i(x ))|2 = 2E |fu(x) - fu(x )|2 + |fu(x ) - fu,i(x \n)|2 = 2E |fu(x) - fu(x )|2 +2Vu,i n = 2E ( Li|xi - x i|)2 +2Vu,i i n = 2mE i Li 2(xi - x )2 +2Vu,i n \n= 2mLi 2 vi +2Vu,i . (14) i So let ai =2mL2 i in Eq.(13). If fu is probabilistic and each de\u00adterministic \nfunction in its support is Lipschitz-continuous (perhaps for different L s), then we can similarly set \nthe .nal ai =2mE[L2 i ] using the expectation.11  7.2 Changes to Reduction Nodes Recall that for a reduction \nnode r with reduction factor Sr, we can decrease this factor to a smaller value sr .{1,...,Sr} at the \nexpense of introducing some additive sampling error Er(sr). In our Question 3, we have a different variance \nbound Vr(sr)= Sr-sr (B - A)2 , whose proof can be found in the full version sr(Sr-1) of this paper. Lemma \n7.1. Given m numbers x1,x2,...,xm . [A, B], ran\u00addomly sampling s of the numbers xi1 ,...,xis (without \nreplace\u00adment) and computing the sample average gives an approximation to x1+\u00b7\u00b7\u00b7+xm m with the following \nvariance guarantee: xi1 + \u00b7\u00b7\u00b7 + xis x1 + \u00b7\u00b7\u00b7 + xm 2Ei1,...,is - sm m - s = (B - A)2 . s(m - 1) Similar \nto Corollary 3.2, we also verify that an averaging node satis.es our linear variance propagation assumption, \ndue to its Lipschitz continuity We defer the proof to the full version of this paper. Corollary 7.2. \nConsider an averaging node that will pick s ran\u00addom samples among all m inputs where each input x j has \nbounded variance E[|x j - xj |2] = vj . Then: x i1 + \u00b7\u00b7\u00b7 + xis x1 + \u00b7\u00b7\u00b7 + xm 2 Ei1,...,is,x 1,...,x m \n- sm 2 m - s 2 m = 2(B - A)+ vi . s(m - 1) m i=1 11 Because the randomness within fu is independent of \nthe randomness for input error x i - xi, we have E[L2 xi)2]= E[L2]E[(xi - x i)2]. i (xi - i  7.3 Putting \nThings Together As before, if there is no reduction node, we can write the variance in the following \nlinear form: ERROR2(x)= (xu,i \u00b7 Vu,i \u00b7 \u00dfu) . ui The ampli.cation factors \u00dfu can be pre-computed using \nthe linear variance propagation factors ai. The expressions for the reduction node are slightly changed. \nInstead of Eq.(9), we now have TIME = Tsub \u00d7 s ERROR2 =2Vsub +2Vr(s) . One can use almost the same arguments \nas in Section 5 to show that the optimization problem with respect to one reduction node can be reduced \nto optimizing a univariate function. This yields a fully polynomial-time approximation scheme (FPTAS) \nthat solves Question 3. The solution for Question 4 can be derived similarly if one follows the arguments \nin Section 6. 8. Other Reduction Nodes We have stated our main theorems for programs that contain only \naveraging reduction nodes. More generally, our algorithm supports any reduction node that can be approximated \nusing sampling. This includes, for example, variances, jp norms, and order statistics functions. In this \nsection we discuss the analysis of some additional reduction nodes. Summation Reduction Nodes. For a \nsummation node with re\u00adduction factor Sr (i.e., Sr numbers to sum up), the sampling pro\u00adcedure randomly \nselects sr numbers, then outputs the partial sum multiplied by Sr . The output is therefore an unbiased \nestimator of sr the exact sum. The derivation of the expressions for the expected error and the variance \nof the error closely follows the derivation for averaging nodes. The .nal expressions are the corresponding \naveraging expressions multiplied by Sr. Minimization and Maximization Reduction Nodes. Note that all \nreasonable sampling techniques may, with some non-negligible probability, discard the smallest input \nvalue. We therefore consider percentile minimization nodes, which have two parameters q . (0, 1) and \nB. If the output is any of the smallest LqmJ numbers in the input (here m is the number of inputs), the \nerror is 0. Otherwise, the error is B. We also de.ne m ' = 1(1 - q)ml. Lemma 8.1. For a percentile minimization \n(or maximization) node r with parameters q . (0, 1) and B, sampling sr of m input elements gives an expected \nerror of f m Also, the error propagates linearly (with a constant factor of 1) for such nodes. In other \nwords, if all inputs to a minimization (or maximization) node are subject to an error of E[|xi - x i|] \n= d, the output will have error d + E ' (sr). Now, if we replace Er(sr) r in Eq.(9) with this function \nwe just derived, all of the analysis in Section 5 goes through unchanged. It is also straightforward \nto verify that if one uses the variance of the error, Vr(sr)= Er(sr)B. Argmin and Argmax Reduction Nodes. \nWe can similarly de.ne percentile argmin and argmax nodes, where the output value is a pair (i, v) in \nwhich i is the index for the smallest/largest element and v is the actual minimum or maximum value. The \nerror depends exclusively on the element v. We can output the index i, but it can not be used as a value \nin arithmetic expressions. 9. Related Work Empirical Accuracy-Performance Tradeoffs. Pro.tably trading \naccuracy for performance is a well known practice, often done at an algorithmic or system level. For \nmany computationally expensive problems, researchers have developed approximation, randomized, or iterative \nalgorithms which produce an approximate solution with guaranteed error bounds [25]. Researchers have \nalso proposed a number of system-level techniques that speci.cally aim to trade accuracy for performance, \nenergy, or fault tolerance. The proposed techniques operate at the level of hardware [4, 17, 34], system \nsoftware [9, 13, 17, 20, 21], and user applications [2, 3, 6, 14, 24, 27, 28, 30, 31, 33]. Previous research \nhas explored the tradeoff space by running and comparing the results of the original and transformed \nprograms on either training inputs [12, 27, 28, 31] or online for chosen production inputs [2, 3, 13, \n33]. The result of this exploration is often an empirical approximation of the accuracy-performance tradeoff \ncurve. This approximation may be valid only for inputs similar to those used to construct the empirical \ncurve. In some cases the approximation comes with empirical statistical accuracy bounds [27, 28]. In \nother cases there are no accuracy bounds at all. This paper, in contrast, statically analyzes the computation \nto produce a (1 + E)-approximation to the exact tradeoff curve. This approximation provides guaranteed \nprobabilistic accuracy bounds that are valid for all legal inputs. Probabilistic Accuracy Bounds For \nSingle Loops. Researchers have recently developed static analysis techniques for character\u00adizing the \naccuracy effects of loop perforation (which transforms loops to execute only a subset of their iterations) \n[22, 23] or loops with approximate function memoization (which transforms func\u00adtions to return a previously \ncomputed value) [6]. These techniques analyze single loops and do not characterize how the accuracy ef\u00ad \n . .. .. fects propagate through the remaining computation to affect the ' sr B if sr = m Er(sr)= output. \nThis paper, in contrast, presents a technique that character\u00ad m sr ' izes and exploits the accuracy-performance \ncurve available via sub\u00ad 0 if sr >m , stitution and sampling transformations applied to complete compu\u00ad \nwhich is a convex function. It is also possible to de.ne a simpler approximate bound if m is large or \nsampling is done without replacement: Er(sr) (1 - q)sr B. Proof. There are a total of sm r ways to take \nsamples of size sr. m f of these samples will contain no element in the smallest q sr fraction of the \noriginal Sr inputs. The probability that a sample contains no element that lies in this smallest q fraction \nis therefore (m f ) sr . The expected error is this probability multiplied by B. One (smr) can verify \nthat this is a convex function by taking the second\u00adorder discrete derivative. When m is large, the probability \ncan be approximated by (1 - q)sr , which is obviously convex. tations. Unlike previous research, the \ntechniques presented in this paper capture how global interactions between multiple approxi\u00admate transformations \npropagate through the entire computation (in\u00adstead of just a single loop) and does not require speci.cation \nof probability distributions of the inputs. Analytic Properties of Programs. Researchers have developed \ntechniques to identify continuous or Lipschitz-continuous pro\u00adgrams [5, 6, 18, 19, 26]. Identi.ed applications \ninclude differ\u00ad ential privacy [5, 6, 26] and robust functions for embedded sys\u00ad tems [6, 18, 19]. This \npaper, in contrast, presents techniques that apply accuracy-aware transformations to obtain new computations \nthat occupy more desirable points on the underlying accuracy\u00adperformance tradeoff curve that the transformations \ninduce.  Smooth interpretation [7, 8] uses a gradient descent based method to synthesize control parameters \nfor imperative computer programs. The analysis returns a set of parameters that minimize the difference \nbetween the expected and computed control values for programs that control cyberphysical interactions. \nApproximate Queries in Database Systems. Modern databases often enable users to de.ne queries that operate \non some subset of the records in a given table. Such queries come with no accu\u00adracy or performance guarantees. \nResearchers have explored multi\u00adple directions for supporting approximate queries with probabilistic \nguarantees. Approximate aggregate queries let a user specify a de\u00adsired accuracy bound or execution time \nof a query [1, 15, 16]. The database then generates a sampling strategy that satis.es the spec\u00adi.cation \n[15, 16] or uses a cached sample, when applicable [1]. Online queries compute the exact answer for the \nentire data-set, but provide intermediate results and con.dence bounds [11]. Prob\u00ad abilistic databases \n[35] operate on inherently uncertain data. The accuracy bounds of all queries (including aggregation) \ndepend on the uncertainty of data. These systems work with speci.c classes of queries de.ned in a relational \nmodel. They sample data but do not consider multiple function implementations. They also do not provide \ngeneral mech\u00adanisms to achieve optimal accuracy-performance tradeoffs for sam\u00adpling when processing complex \nnested queries. 10. Conclusion Despite the central role that approximate computations play in many areas \nof computer science, there has been little research into program optimizations that can trade off accuracy \nin return for other bene.ts such as reduced resource consumption. We present a model of computation for \napproximate computations and an algorithm for applying accuracy-aware transformations to opti\u00admize these \napproximate computations. The algorithm produces a randomized program, which randomly selects one of \nmultiple weighted alternative program con.gurations to maximize perfor\u00admance subject to a speci.ed expected \nerror bound. Given the growing importance of approximation in computa\u00adtion, we expect to see more approximate \noptimization algorithms in the future. We anticipate that these algorithms may share many of the key \ncharacteristics of the computations and optimizations that we present in this paper: program transformations \nthat trade off ac\u00adcuracy in return for performance, the ability to optimize programs in the presence \nof errors that propagate globally across multiple composed programming constructs, and randomization \nto improve performance and tractability. Acknowledgements We thank Michael Carbin, Stelios Sidiroglou, \nJean Yang, and the anonymous reviewers for useful feedback on previous versions of this paper. This research \nwas supported in part by the National Science Foundation (Grants CCF-0811397, CCF-0843915, CCF-0905244, \nCCF-1036241 and IIS-0835652), the United States Department of Energy (Grant DE-SC0005288), and a Sloan \nFellowship. References [1] S. Acharya, P. Gibbons, V. Poosala, and S. Ramaswamy. Join synopses for approximate \nquery answering. In SIGMOD, 1999. [2] J. Ansel, C. Chan, Y. Wong, M. Olszewski, Q. Zhao, A. Edelman, \nand S. Amarasinghe. Petabricks: A language and compiler for algorithmic choice. In PLDI, 2009. [3] W. \nBaek and T. Chilimbi. Green: A framework for supporting energy\u00adconscious programming using controlled \napproximation. In PLDI 10. [4] L. Chakrapani, K. Muntimadugu, A. Lingamneni, J. George, and K. Palem. \nHighly energy and performance ef.cient embedded com\u00ad puting through approximately correct arithmetic: \nA mathematical foundation and preliminary experimental validation. In CASES, 2008. [5] S. Chaudhuri, \nS. Gulwani, and R. Lublinerman. Continuity analysis of programs. In POPL, 2010. [6] S. Chaudhuri, S. \nGulwani, R. Lublinerman, and S. Navidpour. Proving Programs Robust. In FSE, 2011. [7] S. Chaudhuri and \nA. Solar-Lezama. Smooth interpretation. In PLDI, 2010. [8] S. Chaudhuri and A. Solar-Lezama. Smoothing \na program soundly and robustly. In CAV, 2011. [9] J. Flinn and M. Satyanarayanan. Energy-aware adaptation \nfor mobile applications. In SOSP, 1999. [10] R. Hameed, W. Qadeer, M. Wachs, O. Azizi, A. Solomatnikov, \nB. Lee, S. Richardson, C. Kozyrakis, and M. Horowitz. Understanding sources of inef.ciency in general-purpose \nchips. In ISCA, 2010. [11] J. Hellerstein, P. Haas, and H. Wang. Online aggregation. In SIG-MOD/PODS, \n1997. [12] H. Hoffman, S. Sidiroglou, M. Carbin, S. Misailovic, A. Agarwal, and M. Rinard. Dynamic knobs \nfor power-aware computing. In ASPLOS, 2011. [13] H. Hoffmann, M. Maggio, M. D. Santambrogio, A. Leva, \nand A. Agar\u00adwal. Seec: A general and extensible framework for self-aware com\u00adputing. Technical Report \nMIT-CSAIL-TR-2011-046, 2011. [14] H. Hoffmann, S. Misailovic, S. Sidiroglou, A. Agarwal, and M. Ri\u00adnard. \nUsing Code Perforation to Improve Performance, Reduce En\u00adergy Consumption, and Respond to Failures . \nTechnical Report MIT\u00adCSAIL-TR-2009-042, 2009. [15] W.-C. Hou, G. Ozsoyoglu, and B. K. Taneja. Processing \naggregate relational queries with hard time constraints. SIGMOD, 1989. [16] Y. Hu, S. Sundara, and J. \nSrinivasan. Supporting time-constrained SQL queries in Oracle. VLDB, 2007. [17] S. Liu, K. Pattabiraman, \nT. Moscibroda, and B. Zorn. Flikker: Saving dram refresh-power through critical data partitioning. ASPLOS, \n2011. [18] R. Majumdar and I. Saha. Symbolic robustness analysis. In RTSS 09. [19] R. Majumdar, I. Saha, \nand Z. Wang. Systematic testing for control applications. In MEMOCODE, 2010. [20] J. Meng, S. Chakradhar, \nand A. Raghunathan. Best-Effort Parallel Execution Framework for Recognition and Mining Applications. \nIn IPDPS, 2009. [21] J. Meng, A. Raghunathan, and S. B. S. Chakradhar. Exploiting the Forgiving Nature \nof Applications for Scalable Parallel Execution. In IPDPS, 2010. [22] S. Misailovic, D. Roy, and M. Rinard. \nProbabilistic and statistical analysis of perforated patterns. Technical Report MIT-CSAIL-TR\u00ad2011-003, \nJanuary 2011. [23] S. Misailovic, D. Roy, and M. Rinard. Probabilistically Accurate Program Transformations. \nIn SAS, 2011. [24] S. Misailovic, S. Sidiroglou, H. Hoffmann, and M. Rinard. Quality of service pro.ling. \nIn ICSE, 2010. [25] R. Motwani and P. Raghavan. Randomized algorithms. Cambridge University Press, 1995. \n[26] J. Reed and B. C. Pierce. Distance makes the types grow stronger: a calculus for differential privacy. \nIn ICFP, 2010. [27] M. Rinard. Probabilistic accuracy bounds for fault-tolerant computa\u00adtions that discard \ntasks. In ICS, 2006. [28] M. Rinard. Using early phase termination to eliminate load imbalances at barrier \nsynchronization points. In OOPSLA, 2007. [29] R. Rubinfeld. Sublinear time algorithms. In Intl. Congress \nof Mathe\u00admaticians, 2006. [30] A. Sampson, W. Dietl, E. Fortuna, D. Gnanapragasam, L. Ceze, and D. Grossman. \nEnerJ: approximate data types for safe and general low\u00adpower computation. In PLDI, 2011. [31] S. Sidiroglou, \nS. Misailovic, H. Hoffmann, and M. Rinard. Manag\u00ading Performance vs. Accuracy Tradeoffs With Loop Perforation. \nIn FSE, 2011. [32] K. Sohraby, D. Minoli, and T. Znati. Wireless sensor networks: technology, protocols, \nand applications. Wiley-Blackwell, 2007. [33] J. Sorber, A. Kostadinov, M. Garber, M. Brennan, M. D. \nCorner, and E. D. Berger. Eon: a language and runtime system for perpetual systems. In SenSys, 2007. \n[34] P. Stanley-Marbell and D. Marculescu. Deviation-Tolerant Computa\u00adtion in Concurrent Failure-Prone \nHardware. Technical Report ESR\u00ad2008-01, Eindhoven University of Technology, 2008. [35] D. Suciu, D. Olteanu, \nC. R\u00b4e, and C. Koch. Probabilistic Databases. Morgan-Claypool, 2011.    \n\t\t\t", "proc_id": "2103656", "abstract": "<p>Despite the fact that approximate computations have come to dominate many areas of computer science, the field of program transformations has focused almost exclusively on traditional semantics-preserving transformations that do not attempt to exploit the opportunity, available in many computations, to acceptably trade off accuracy for benefits such as increased performance and reduced resource consumption.</p> <p>We present a model of computation for approximate computations and an algorithm for optimizing these computations. The algorithm works with two classes of transformations: substitution transformations (which select one of a number of available implementations for a given function, with each implementation offering a different combination of accuracy and resource consumption) and sampling transformations (which randomly discard some of the inputs to a given reduction). The algorithm produces a (1+&#949;) randomized approximation to the optimal randomized computation (which minimizes resource consumption subject to a probabilistic accuracy specification in the form of a maximum expected error or maximum error variance).</p>", "authors": [{"name": "Zeyuan Allen Zhu", "author_profile_id": "81447600723", "affiliation": "MIT, Cambridge, MA, USA", "person_id": "P2991446", "email_address": "zeyuan@csail.mit.edu", "orcid_id": ""}, {"name": "Sasa Misailovic", "author_profile_id": "81330495396", "affiliation": "MIT, Cambridge, MA, USA", "person_id": "P2991447", "email_address": "misailo@mit.edu", "orcid_id": ""}, {"name": "Jonathan A. Kelner", "author_profile_id": "81100275109", "affiliation": "MIT, Cambridge, MA, USA", "person_id": "P2991448", "email_address": "kelner@mit.edu", "orcid_id": ""}, {"name": "Martin Rinard", "author_profile_id": "81100087275", "affiliation": "MIT, Cambridge, MA, USA", "person_id": "P2991449", "email_address": "rinard@mit.edu", "orcid_id": ""}], "doi_number": "10.1145/2103656.2103710", "year": "2012", "article_id": "2103710", "conference": "POPL", "title": "Randomized accuracy-aware program transformations for efficient approximate computations", "url": "http://dl.acm.org/citation.cfm?id=2103710"}