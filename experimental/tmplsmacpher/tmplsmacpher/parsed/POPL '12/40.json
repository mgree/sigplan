{"article_publication_date": "01-25-2012", "fulltext": "\n The Ins and Outs of Gradual Type Inference Aseem Rastogi Stony Brook University arastogi@cs.stonybrook.edu \n Abstract Gradual typing lets programmers evolve their dynamically typed programs by gradually adding \nexplicit type annotations, which con\u00adfer bene.ts like improved performance and fewer run-time failures. \nHowever, we argue that such evolution often requires a giant leap, and that type inference can offer \na crucial missing step. If omitted type annotations are interpreted as unknown types, rather than the \ndynamic type, then static types can often be inferred, thereby removing unnecessary assumptions of the \ndynamic type. The remaining assumptions of the dynamic type may then be re\u00admoved by either reasoning \noutside the static type system, or re\u00adstructuring the code. We present a type inference algorithm that \ncan improve the per\u00adformance of existing gradually typed programs without introducing any new run-time \nfailures. To account for dynamic typing, types that .ow in to an unknown type are treated in a fundamentally \ndif\u00adferent manner than types that .ow out. Furthermore, in the interests of backward-compatibility, an \nescape analysis is conducted to de\u00adcide which types are safe to infer. We have implemented our algo\u00adrithm \nfor ActionScript, and evaluated it on the SunSpider and V8 benchmark suites. We demonstrate that our \nalgorithm can improve the performance of unannotated programs as well as recover most of the type annotations \nin annotated programs. Categories and Subject Descriptors D.3.4 [Programming Lan\u00adguages]: Processors \nOptimization; F.3.2 [Logics and Meaning of Programs]: Semantics of Programming Languages Program analysis; \nF.3.3 [Logics and Meaning of Programs]: Studies of Pro\u00adgram Constructs Type structure General Terms Algorithms, \nLanguages, Performance, Theory Keywords Gradual typing, Type inference, ActionScript 1. Introduction \nGradual Typing and Evolutionary Programming Gradual typ\u00ading [12, 17] aims to combine the bene.ts of static \ntyping and dy\u00adnamic typing in a language. In a gradually typed program, dynam\u00adically typed code can be \nmixed with statically typed code. While the dynamically typed fragments are not constrained to follow \nthe structure enforced by a static type system, the statically typed frag\u00adments enjoy not only some static \nsafety guarantees ( well-typed programs cannot be blamed the blame theorem [24]) but also Permission \nto make digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior speci.c permission and/or a fee. POPL 12, January \n25 27, 2012, Philadelphia, PA, USA. Copyright c &#38;#169; 2012 ACM 978-1-4503-1083-3/12/01. . . $10.00 \nAvik Chaudhuri Basil Hosmer Advanced Technology Labs, Adobe Systems {achaudhu,bhosmer}@adobe.com admit \nperformance optimizations that the dynamically typed frag\u00adments do not. Gradual typing envisions a style \nof programming where dynamically typed programs can be evolved into statically typed programs by gradually \ntrading off liberties in code structure for assurances of safety and performance. Although there has \nbeen much recent progress on mastering the recipe of gradual typing, a key ingredient has been largely \nmissing type inference. The only previous work on type inference for gradually typed languages is based \non uni.cation [18], which is unsuitable for use in object-oriented languages with subtyping. Unfortunately, \nas we argue below, the lack of type inference may be the most signi.cant obstacle towards adopting the \nstyle of evo\u00adlutionary programming envisioned by gradual typing. The Key Missing Ingredient: Type Inference \nIn a gradually typed language, a program may be partially annotated with types. Any missing types are \nuniformly assumed to be the dynamic type. This means that the fragments of the program that have missing \ntype annotations do not enjoy any of the bene.ts of static typing. In particular, their performance is \nhindered by dynamic casts, even if they implicitly satisfy the constraints of static typing (i.e., even \nif the dynamic casts never fail). To improve performance, the missing types have to be declared. However, \nin our experience with a mainstream gradually typed language, ActionScript [13], the task of evolving \ndynamically typed programs to statically typed programs by declaring missing types can be quite onerous. \nThe annotation burden is often intimi\u00addating: in the limit, types must be declared for every variable \nin the evolving fragment, and the programmer may need to juggle several annotations to satisfy the constraints \nof the static type system. Furthermore, the programmer may eventually be forced to de\u00adclare dynamic types \nfor some variables: it may not be possible to satisfy the constraints of the static type system without \nrestructur\u00ading the code. This is because the programmer may be relying on an invariant that cannot be \nexpressed via static types in the language, i.e., the proof of safety may rely on a form of reasoning \n(typically, path-sensitive) that is outside the scope of the static type system. Unfortunately, due to \nthese dif.culties, gradually typed pro\u00adgrams often continue to miss out on important bene.ts of static \ntyping, such as performance optimizations. Consequently, evolu\u00adtionary programming remains a fantasy. \nThe crux of the problem is that a missing type is misinterpreted as the dynamic type, whereas the intended \ninterpretation is that of an unknown type. Often, an unknown type can be inferred to be a static type. \nIn contrast, the dynamic type is a fallback to encode imprecision: a variable must be of the dynamic \ntype when the set of values that the variable may denote cannot be expressed as a static type in the \nlanguage, thereby forcing imprecision in the type abstraction. Therefore, we envision an iterative process \nfor evolution of dy\u00adnamically typed programs to statically typed programs, that alter\u00adnates between two \nstates. In one state, type inference removes un\u00ad  Figure 1. Iterative Evolution of Scripts to Programs \nnecessary assumptions of the dynamic type, so that elimination of any remaining dynamic types in the \ncode requires either reason\u00ading outside the static type system, or restructuring the code. In the other \nstate, the programmer either introduces further type annota\u00adtions or restructures parts of the program \nto conform to the static type discipline. Furthermore, at any state, the programmer may de\u00adcide to pause \nthe evolution process, deploy the program, and take advantage of the improved precision of static types \nthat have al\u00adready replaced dynamic types in the program. At a later point, the programmer should be \nfree to resume the evolution process. This process is depicted in Figure 1. Since the programmer may \nchoose to evolve an existing, de\u00adployed program, type inference must ensure that the program re\u00admains \nbackward-compatible: it should have exactly the same run\u00adtime behavior after type inference as it would \nhave before type in\u00adference. In particular, if the program was safe (did not fail at run time) then it \nshould continue to be safe, and any other programs that ran safely with the program should continue to \nrun safely. Retro.tting Type Inference on Gradually Typed Programs In this paper we study the problem \nof retro.tting type inference in an existing gradually typed language. The practical motivation for our \nwork is to improve the performance of existing ActionScript programs under the hood. Our aim is to let \nexisting programs reap the bene.ts of type-directed performance optimizations as much as possible, while \nremaining backward-compatible. On the other hand, our aim is not to eliminate run-time failures in existing \nprograms: indeed, any run-time failures need to be pre\u00adserved as well. Thus, the static types we infer \nmay not satisfy some static safety guarantees, such as the blame theorem [24]. Neverthe\u00adless, the static \ntypes we infer can improve performance, since they soundly approximate precise sets of run-time values. \nThus, we pre\u00adfer to infer static types where possible rather than falling back on the (imprecise) dynamic \ntype. Of course, it remains possible to re\u00adcover static safety guarantees by forgoing some opportunities \nfor optimization we can fall back on the dynamic type whenever the inferred static type may be cast to \nan incompatible type. Design Challenges The aim of our type inference algorithm is to reduce the imprecision \nassociated with dynamic types, so that pro\u00adgrams with missing type annotations can bene.t from various \nper\u00adformance optimizations. We try to infer, for each variable, a precise type abstraction of the set \nof values that .ow into it. The setting of a gradually typed language and the requirement of backward\u00adcompatibility \npresent several unique challenges, as outlined next. In a purely static type system, the de.nitions of \na variable determine a lower bound on its type, since the type must admit every value .owing into the \nvariable; dually, the uses of a variable determine an upper bound on its type, since the type must admit \nonly those values that are admitted by the type of every context to which the variable .ows out. Thus, \nfor a type that satis.es both sets of constraints, it is possible to prove statically that every use \nof thevariableissafeforeveryde.nition of that variable. In contrast, in a gradual type system, every \nuse of a variable need not be safe for every de.nition of that variable: instead, run-time failures may \nbe avoided if each use is safe for some de.nition. Thus, the type of a variable must be inferred by considering \nonly its de.nitions, deferring type checks on its uses until run time. Furthermore, the type of a variable \nmay only be inferred if it is local, i.e., if all of its de.nitions are visible during compilation. In \nthe absence of this guarantee, we cannot exclude the possibility that values of an unexpected type may \n.ow into the variable at run time: for example, consider a parameter of a global function, which can \nbe de.ned by arbitrary arguments through calls that are not visible during compilation. In such cases, \nreplacing the variable s assumed dynamic type with an inferred static type may break the program in some \nunknown run-time environment. Based on these observations, we infer types of only those vari\u00adables for \nwhich we can infer the types of all values .owing in, i.e., their in.ows; and we derive the solutions \nby computing the least upper bounds of those types, without checking that the solutions satisfy the greatest \nlower bounds of the types of the contexts to which they .ow out, i.e., their out.ows. Higher-order values, \nsuch as functions and objects, present some interesting subtleties. There is an immediate problem if \nwe take the standard least upper bounds of their (higher-order) types, since we will then be taking the \ngreatest lower bounds of the in\u00adput (negative) parts of these types which violates the princi\u00adple we \nmotivated above, giving us prescriptive safe types rather than descriptive precise types for the input \nparts of the higher\u00adorder solution. Another possibility might be to take the least upper bounds of both \ninput and output parts, following na\u00a8ive subtyp\u00ading [24]1. However this also is not quite right, since \nwe are still neglecting to observe the types of the actual inputs, e.g., the types of arguments in calls \nto a function; rather, as with standard subtyp\u00ading, we are still considering the types of values that \nsuch a function considers safe to input, i.e., the function s parameter types. The key insight is that \nif we are to obey the principle of solving based on in.ows alone, we must recursively deconstruct higher-order \ntypes down to their .rst-order parts, solve for those based on in.ows (e.g., the in.ow of arguments to \nthe parameter of a function-typed variable), and then reconstruct the higher-order parts in such a way \nas to preserve the desired asymmetry. We devise an elegant way of inferring such higher-order solu\u00adtions, \nvia a notion of kinds; this requires us to treat higher-order in.ows in a fundamentally different manner \nthan higher-order out\u00ad.ows (Section 2.3). Another complication arises when local functions and objects \nescape. In this case we cannot infer the parts of their types in nega\u00adtive ( input ) positions, since \nvalues may then .ow into them from some code that cannot be analyzed at compile time. Therefore, it seems \nthat we need some kind of escape analysis to .gure out if some local function or object escapes. Perhaps \nsurprisingly, the es\u00adcape analysis we require is carried out for free by our algorithm: it suf.ces to \nassume that in the enclosing program s type, the parts in negative positions are either explicitly annotated, \nor are the dy\u00adnamic type (Section 2.5). Guarantees Our main guarantee is that the type inference algo\u00adrithm \nis sound: if a program fails at run time with our inferred types, it would have failed at exactly the \nsame point with the dynamic type. The soundness guarantee is compositional, in the sense that it holds \neven when the program is placed in an arbitrary run-time en\u00advironment. Furthermore, we show how to recover \nsolutions from our inferred types that satisfy the blame theorem [24]. We also prove that the time complexity \nof our type inference algorithm is quadratic in the size of the input program. Implementation and Evaluation \nWe have implemented our type\u00adinference algorithm for ActionScript, and evaluated it on the Sun-Spider \n[20] and V8 [23] benchmark suites. Overall, we are able to 1 Na\u00a8ive subtyping is covariant in both negative \nand positive positions, unlike standard subtyping (which is contravariant in negative positions). achieve \n1.6x performance improvement on average over partially typed benchmarks (with type annotations only on \nrequired parts of the interfaces), with a maximum performance improvement of 5.6x (Section 5). Contributions \nTo summarize, we make the following main con\u00adtributions in this paper: We argue that the evolution of \ngradually typed programs is essentially an iterative process, with type inference being a key missing \ncomponent.  We design a type inference algorithm for gradually typed lan\u00adguages with the goal of improving \nthe performance of programs as much as possible, while preserving the run-time behaviors of unannotated \nprograms. Our design involves new ideas that are speci.c to the setting of a gradual type system and \nthe re\u00adquirement of backward-compatibility, such as the need to treat de.nitions and uses asymmetrically, \nand the need for escape analysis to decide what types are safe to infer.  We formalize our type inference \nalgorithm in a core calculus of functions and objects, and prove soundness and complexity theorems for \nthe algorithm. Technical novelties in our algorithm include a notion of kinds to compute precise higher-order \nsolu\u00adtions, and the encoding of escape analysis as a syntactic condi\u00adtion on the program s type.  We \nimplement our algorithm for ActionScript, a mainstream gradually typed language, and evaluate its effectiveness, \nwith encouraging results.  Overall, we believe that the techniques and insights developed in this paper \nwill elucidate the intricacies of type inference for gradually typed languages, and spur further research \nin this area.  2. Overview 2.1 Algorithm While existing compilers for gradually typed languages such \nas Ac\u00adtionScript [13] consider missing type annotations to be the dynamic type, our aim is to infer precise \nstatic types where possible, and fall back on the dynamic type only where necessary. As usual, our type \ninference algorithm begins by replacing missing type annotations in the program with fresh type variables. \nThen we compile the program, using type annotations to generate coercions between types. A coercion is \nof the form S C T ,where S and T are types; such a coercion witnesses the .ow of a term of type S into \na context of type T . In particular, coercions involving type variables are interpreted as .ows for those \ntype variables: for example, if X is a type variable, then T CX is an in.ow for X, whereas X CT where \nT is not a type variable is an out.ow for X. Starting from the original set of coercions collected during \ncom\u00adpilation, we now iterate, generating new coercions from existing ones using carefully designed rules \nwhich treat the implications of in.ows and out.ows differently. At each step, new coercions rep\u00adresent \n.ows that are implied by, but are not directly expressed by, extant coercions. Iteration continues until \nour rules can produce no new coer\u00adcions, at which point we have a closure of the .ows implied by the \ntype annotations in the program. For each type variable X,we can now derive a solution, namely the least \nupper bound of all types T that can .ow into X via in.ows expressed by coercions of the form T C X. A \nformal treatment of the algorithm is presented in Section 3. We now demonstrate the .ne points of the \nalgorithm, viz. the computation of closures and the derivation of solutions, using examples in ActionScript \n[13]; ActionScript is an extension of JavaScript with a gradual type system, and is the programming language \nunderlying Flash applications. We show type variables as part of programs below, although they are not \npart of ActionScript. In actual programs, these type variables are just missing annotations. We follow \nthe convention that type variables (in italics) have similar names as corresponding unannotated variables, \nwith the .rst letter capitalized. For example, the type variable for an unannotated variable foo in the \nprogram is written as Foo.If foo is an unannotated function, Foo? represents the type variable for the \nparameter type of foo and Foo! represents the type variable for the return type of foo. The dynamic type \nis denoted by *. Number and Boolean are base types. We also use . to denote the bottom type, although \nit is not part of ActionScript. For the purpose of our examples, operators are typed as follows: + : \n(Number, Number) -> Number < : (Number, Number) -> Boolean  2.2 Local Variables having Base Types We \nstart with a simple example to show how our algorithm infers primitive types for local variables in a \nfunction, and how it im\u00adproves performance. 1 function foo(n:Number):Foo! { 2 var index:Index =0; 3 var \nsum:Sum = index; 4 while(index < n) 5 { index = index + 1; sum = sum + index } 6 return sum 7 } In the \nabsence of type inference, index and sum will be given type *. Since the + operator has type (Number, \nNumber) . Number, evaluation of index + 1 and sum + index will cause in run-time conversions * C Number \nfor index and sum.Tostore the results back to index and sum, further run-time conversions Number C * \nwill happen. As we show later in our experiments, such run-time conversions can hurt the performance \nof programs. We now show how our inference algorithm works. After adding type variables for missing types, \nas shown, we compile the program and collect coercions. The initializations of index and sum on lines \n2 and 3 generate Number C Index and Index C Sum. The operations index < n, index = index + 1,and sum \n= sum + index on lines 4 and 5 generate Index C Number, Number C Index, Sum C Number, Index C Number,and \nNumber C Sum. To solve these .ows, we note that the only type .owing into Index is Number. So, we infer \nIndex = Number.Next,we see that types .owing into Sum are Number and Index. Therefore, the solution for \nSum is the union of Number and Index,which is Number.And .nally, the only type .owing into Foo! is Sum. \nSo, Foo! = Number as well. Annotating index and sum with Number rather than * improves run-time performance \nby eliminating unnecessary run-time conver\u00ad sions between Number and * in the loop body. Note that we \nalso generated Sum C Number and Index C Number but did not use them for inference. A typical type checker \nwill check that the solutions of Sum and Index satisfy these out\u00ad.ows, but we skip this step. We say \nmore about why we do so in Section 2.6. 2.3 Local Variables having Function Types 2.3.1 Unions of Function \nTypes are Problematic When a base type (like Number or Boolean) .ows into a type variable, the solution \nof the type variable is simply that type; and if several base types .ow in, the solution is the union \nof those base types. Unfortunately, the obvious ways of generalizing this approach to higher-order types \n(like function types) do not work. For example, consider the following program: 8 var x:X = function(a:*):Number \n{ ... }; 9 x(1) On line 8, we generate * .Number C X. Thus, we may infer the solution of X to be the \ntype .owing into it, * .Number.However, a more precise solution is Number .Number, based on the fact \nthat the only values .owing into the parameter of the function stored in x are of type Number (line 9). \nFurthermore, when several function types .ow into a type vari\u00ad able, we cannot combine those in.ows by \ntaking the union of all such types. Consider the following example: 10 var x:X; 11 if(b){x= function(y:Number):Number \n{ ... }; x(1) } 12 else { x = function(y:Boolean):Number { ... }; x(true) } We generate Number .Number \nC X from line 11 and Boolean . Number C X from line 12. Since the union of two function types is a type \nS . T where S is the intersection of their parameter types and T is the union of their return types, \nwe would infer X to be ..Number using the standard approach. This would mean that in the running program, \nany application of x to a Number or a Boolean value would result in an error. This is clearly unsound \nas the programmer applies x to a Number atline11, andtoa Boolean at line 12, both of which are safe. \n 2.3.2 Kinds as Higher-Order Solutions To deal with higher-order types, we introduce a notion of kinds.A \nkind can be thought of as a view of a type variable: it describes a structure, and introduces type variables \nfor the types of parts within that structure. Coercions between type variables and kinds are generated \nduring compilation and closure computation, in either direction: a coercion from a type variable T to \na kind K witnesses the .ow of T -typed values into a context whose higher-order type is exempli.ed by \nK; conversely, a coercion from K to T witnesses the .ow of higher-order values of kind K into a T -typed \ncontext. For example, on line 9, the function call x(1) induces a view of X as a function type. To realize \nthis view, we generate the coercion X C X? .X!,where X? .X! is a kind for X corresponding to function \ntypes; this witnesses values of type X .owingtoacontext in which they are used as functions. We also \ngenerate Number C X?, which witnesses the .ow of arguments of type Number into the parameters of those \nfunctions. Combining * .Number C X and X C X? .X!,weget * . Number C X? .X!. Deconstructing this .ow \nusing contravariance for the parameter type and covariance for the return type, we get X? C * and Number \nC X!. Intuitively, these coercions witness the .ows into and out of the concrete function value assigned \nto x on line 8, resulting from the call to x on line 9. Solving these .ows, we obtain X? = Number,and \nX! = Number. But what should X s solution be? We claim it should be X s function kind, i.e. X? .X!, which \nafter substituting for X? and X!, becomes Number . Number. Our closure rules introduce the necessary \nintermediate step, the coercion X? .X! C X, as a consequence of the assignment on line 8. Thus, a kind \nnot only gives a view of a type variable as a higher-order type, but also encodes the corresponding higher-order \nsolution of the type variable. In other programs, there may be no occasion to generate a coercion from \na type variable X to a kind at all, due to an absence of contexts in which X-typed values are used in \na syntactically higher-order way, such as the function application term x(1) in the previous example. \nConsider the following variation: 13 var x:X = function(a:*):Number { ... }; 14 function f(y:Number \n-> Number):Number { y(1) } 15 f(x) We generate * .Number C X and X C Number .Number on lines 13 and 15 \nrespectively. Here we don t need the view of X as a function type since x is not applied directly, but \nwe still need to represent the solution for X. This is accomplished by the coercions introduced by our \nclosure rules as a result of the assignment on line 13, namely * .Number C X? .X!,and X? .X! C X. Combining \nX? .X! C X and X C Number .Number gives us X? .X! C Number .Number. Solving the .ows gives us X? = Number \nand X! = Number. As before, since only function types .ow into X, its solution is its function kind X? \n.X!, which after substituting for X? and X! gives us Number .Number. Finally, decomposition using kinds \nenables us to take unions of kinds, not types, giving us types inferred from in.ows alone. For example, \nthe decomposition of X into X? .X! lets us capture the .ow of argument types to X? in the calls to x \non lines 11 and 12, yielding Number C X? and Boolean C X?, respectively. These .rst-order .ows may then \nbe combined by taking the least upper bound in the usual way, after which reconstruction of X proceeds \nas described above.  2.4 Function Parameters 2.4.1 Functions Callable by Existing Code Are Problematic \nSince we cannot assume that all existing ActionScript programs will be available for our analysis, a \nkey goal of our algorithm is to ensure that our inferred types are backward-compatible i.e., programs \ncompiled using inferred types are able to interoperate with programs compiled under the assumption that \nmissing type annotations are the dynamic type, without introducing any new run\u00adtime failures. We now \nshow an example in which we try to extend our algo\u00ad rithm to infer the parameter type of a function \nthat can be called by other programs that are not available for analysis. 16 function foo(x:Foo?):Foo! \n17 {if(b){ return x+1}else{return 0 }} 18 foo(1) The function call foo(1) on line 18 generates Number \nC Foo?. Furthermore, as before we generate Foo? C Number and Number C Foo! on line 17. Solving for type \nvariables, we get Foo? = Number and Foo! = Number. (As before, we do not use Foo? C Number for inference.) \nSuppose that the function foo is part of the interface of some library, and clients of foo have been \ncompiled under the assump\u00adtion that Foo? and Foo! are *. Then the solution for Foo! is sound, because \nit can be used in any context of type *: in particular, it will not throw an error unless it .ows to \na context whose type is incom\u00adpatible with Number, in which case it would already throw an error without \ntype inference. Unfortunately, the solution for Foo? is not sound. For exam\u00adple, suppose a caller sets \nthe global variable b = false and then calls foo with x = true. In the absence of type inference, the \ntrue argument will be successfully converted at run time from type Boolean to type * after which it will \ngo unused, since b= false, avoiding a run-time error. In contrast, if we add the annota\u00adtion x:Number \nafter type inference, this will have the effect of re\u00adquiring a run-time conversion of the true argument \nfrom Boolean to Number at the point of the call to foo, and this conversion will always fail. Thus inferring \nFoo? = Number can break existing pro\u00adgrams. In general, we conclude that we cannot safely infer the param\u00adeter \ntypes of functions that are part of the interface of the program with existing code, because our analysis \ncannot be guaranteed to include every call to such functions.  2.4.2 Local Functions On the other hand, \nit is safe to infer the parameter types of lo\u00adcal functions i.e., functions de.ned within other functions \nand objects whenever all their calls are available for analysis. Consider the local function foo in the \nfollowing example: 19 function bar(y:Number):Bar! { 20 function foo(x:Foo?):Foo! 21 { if(b){returnx+1}else \n{return0 }} 22 return foo(y) 23 } As before, we get the solutions Foo? = Number and Foo! = Number. (Here, \nwe also get Bar! = Number.) Since foo is a local function that does not escape bar, we know it cannot \nbe called from outside bar, meaning that all calls to foo are available for analysis. In this case it \nis safe to infer Foo? = Number,evenifthe function bar is callable by existing code.  2.4.3 Local Functions \nthat Escape Are Problematic However, if a local function escapes e.g. by being returned as a higher-order \nvalue from a function that is callable by existing code then it becomes available to callers in the wild. \nThis again makes it unsafe to infer the local function s parameter type, since not every call to the \nfunction is available for analysis. 24 function bar(y:Number):Bar! { 25 function foo(x:Foo?):Foo! 26 \n{ if(b){returnx+1}else {return0 }} 27 foo(y); 28 return foo 29 } In this example, inferring Foo? = Number \nbased on function call foo(y) on line 27 would again be unsound, assuming the function bar is callable \nby existing code. Since foo is returned by bar, clients can set b = false and call foo with any x value. \nThus, as in Section 2.4.1, annotating x as Number could introduce run-time errors in existing programs. \n  2.5 What Types are Safe To Infer? To summarize, the examples above illustrate that while it may be \nsafe to infer the types of local variables in a function, the return types of functions, and the parameter \ntypes of local functions that do not escape, it is de.nitely not safe to infer parameter types of functions \nthat are callable by existing code, and those of local func\u00adtions that do escape. Furthermore, if values \nfrom such parameters .ow into other variables, then inferring the types of those variables is clearly \nalso unsafe. So what is the principle that lets us decide which types are safe to infer? 2.5.1 Seeing \nAll In.ows is Necessary We observe that in a gradually typed language, we must see the types of all the \nvalues that may .ow into a variable before we can infer the variable s type. Suppose that at compile \ntime we see only a proper subset of the types of values that may .ow into a variable x i.e., some of \nthe types of values that may .ow into x are unknown at compile time, because they .ow into x from a run-time \nenvironment that cannot be analyzed during compilation. Next, suppose that the compiled program is executed \nin such a run-time environment. In the absence of type inference, x has type *, which means that the \nrun-time environment could write any type of value into x. Furthermore, the programmer could reason \noutside the type system to ensure that once such a value reaches x,itisthenused correctly under appropriate \nconditions. Thus, if the inferred type of x is based only on the subset of types that are seen, then \nwe could end up breaking the program in this run-time environment, by causing a run-time error to be \nthrown upon the write to x: not only would this error not have occurred in the absence of type inference, \nbut as described in the previous paragraph, it is quite possible that no error would have occurred at \nall. 2.5.2 Flows Encode Escape Analysis For a function that is callable by existing code, we require \nthe programmer to explicitly annotate the parts of the function s type that have negative polarity (in \nparticular, the parameter type), if those parts have static types. In the absence of annotations, we \nmust assume they have the dynamic type. For a local function, our ability to infer the parts of its type \nwith negative polarity depends on whether the function escapes into an unknown run-time environment. \nThere are many ways a local function can escape: it can be returned as a value, it can be assigned to \na global variable, it can be assigned to some object property and that object could be returned, and \nso on. At .rst glance, it seems that either we need a sophisticated escape analysis to .gure out if a \nlocal function escapes, or we need the programmer to annotate parameter types for local functions as \nwell. Fortunately, we observe that the .ows already encode an escape analysis. If we require that types \nwith negative polarities in the interface of the program with existing code be explicitly annotated if \nstatic i.e., if we assume that they are the dynamic type when unannotated then our closure computation \nensures that escape information is properly propagated to all type variables. Let us consider the example \nfrom Section 2.4.3 with different possibilities for Bar!. First, suppose that the programmer has annotated \nthe return type of bar as Number . Number. In this case, the programmer has annotated the types with \nnegative polarity in bar explicitly. Now, it s safe to infer Foo? = Number, since the programmer has \ntold us that foo will always be called with x as Number (otherwise there will be a run-time error). On \nthe other hand, if the programmer had not explicitly anno\u00adtated the return type of bar,wewouldhaveassumedittobesome \ntype variable Bar!. As before, we generate Foo? . Foo! C Bar! from return foo on line 28. We introduce \nfunction kind (Sec\u00adtion 2.3.2) for Bar! and add Foo? . Foo! C Bar!? . Bar!! and Bar!? . Bar!! C Bar!. \nNow, we observe that Bar!? has negative polarity. Since the function bar is callable by existing code, \nwe assign Bar!? = *.Thus,wehave Foo? . Foo! C * . Bar!!.De\u00adconstructing the .ow, we get * C Foo?.And \n.nally, we infer Foo? = *, which is always safe. In this way, escape analysis is manifested through our \nclosure computation. Thus, we require explicit type annotations only for types that have negative polarities \nin the interface of the program with existing code; all the remaining types we can infer.  2.6 Out.ows \nSo far we have maintained that we are looking at in.ows, of the form T C X, to solve for type variable \nX. But we have not said anything about out.ows, of the form X C T ,where T is not a type variable. Such \nout.ows correspond to dynamic consistency checks. 2.6.1 Out.ows Need Not Contribute to Solutions We observe \nthat out.ows, of the form X C T , need not contribute to the solution for X although in general, such \nout.ows do play a role in propagating .ows between parts of higher-order types, as we have seen in Section \n2.3.2. If we can analyze all writes to X, we know precisely the types of all values that X will need \nto hold at run time and so, by just looking at in.ows, of the form T C X, we can infer a more precise \nsolution for X.  2.6.2 Out.ows Need Not be Validated Statically We also do not need to validate the \nout.ows at compile time, as the runtime does it anyway. As we show in the following example, if we try \nto validate out.ows statically, we could end up being too conservative and infer less precise types than \nwe could. 30 function foo(f:Foo?):Number { 31 if(b) { return f(true) } else { return 0 } 32 } 33 if(b) \n{ foo(function (x:Boolean):Number { ... }) } 34 else { foo(function (x:Number):Number { ... }) } First \nwe note that this program is safe if Foo? = *, which is the case when there is no type inference. When \nb is false, foo doesn t use its argument and returns 0. In our inference algorithm, we generate the .ows: \nBoolean . Number C Foo?? . Foo?! (line 33), Number . Number C Foo?? . Foo?! (line 34), and Boolean C \nFoo?? (from function call g(true) on line 31). Deconstructing the .rst two .ows gives us Foo?? C Boolean, \nFoo?? C Number,and Number C Foo?!. Therefore, we infer Foo?? = Boolean and Foo?! = Number,and our solution \nis Foo? = Boolean . Number. With these inferred types, the program is still safe. There is no change \nin the run-time semantics of the program. But note that our solution Foo?? = Boolean turns the out.ow \nFoo?? C Number to the dynamic consistency check Boolean C Number, which will fail, if executed at run \ntime. So, if we were to validate out.ows statically, we would have promoted Foo?? to *, which means we \nwould have inferred a less precise type that we could. In this example, the programmer encodes path sensitive \nreason\u00ading about correctness of his program and ensures that the dynamic consistency check Boolean C \nNumber never happens at run time (when the function in line 34 is passed to foo, foo doesn t invoke it). \nSince it is not possible to statically determine whether dynamic consistency checks will .re at run time, \nwe do not validate out\u00ad.ows. But, we maintain that our inference doesn t introduce any new run-time errors. \nOur soundness theorem proves that if a pro\u00adgram fails at run time with our inferred types, it would have \nfailed even if those types were *, and vice versa.  2.6.3 Validating Out.ows for Blame Guarantees Traditionally, \ntype inference algorithms in statically typed lan\u00adguages ensure that the solution of X is a subtype of \nthe greatest lower bound of all the types T that .ow out of X. This implies that all dynamic consistency \nchecks always succeed. In our system we do not validate out.ows because this may un\u00adnecessarily constrain \nour inferred types, as we saw in the previous example. But this comes with a trade-off. Type inference \nfor stati\u00adcally typed languages guarantees that inferred types will not gener\u00adate any run-time type errors. \nWhile we clearly cannot hope for such guarantees, gradually typed languages often provide weaker blame \nguarantees [24] the statically typed parts of a program cannot be blamed for any run-time type errors. \nOn the other hand, by default, we do not give any such guarantee: dynamic consistency checks may throw \nerrors at run time. Instead, our inferred types can be thought of as directives to a runtime to optimize \nmemory, run-time conversions, property lookups, and so on; we only guarantee that they do not introduce \nany new run-time errors in the program. If, on the other hand, the programmer wants some blame guar\u00adantees \nfrom our system, we can run an additional step wherein we validate out.ows. If some out.ow may cause \nan error, we either promote the type variable that may cause the error to the * type, or if there is \nno type variable involved, fail the compilation. In the example of previous section, once we .nd that \ninferring Foo?? = Boolean will fail the consistency check Foo?? C Number, we can promote the type variable \nFoo?? = *, and recover blame guarantees for our system.  2.7 Objects The treatment of objects in our \nsystem follows that of functions, in that we use polarities to model object properties with various access \ncontrols. For example, private properties of objects have no polarity, so we infer types of private properties. \nRead-only properties, like public functions, have positive polarity and so, we infer positive parts of \nthe types of those functions and we require programmers to annotate the negative parts. On the other \nhand, public variables in an object are read-write, so they have both negative and positive polarities, \nand we require programmers to annotate the types of public variables. As before, we do not need escape \nanalysis to .gure out if some private function escapes the scope of an object, or if a private object \nescapes the scope of a function. Escape analysis is manifested in the closure computation. We show an \nexample in ActionScript where object templates are de.ned using classes. 35 class A { 36 private var \nb:B = true; 37 private function foo(var x:Foo?):Foo! 38 { if(b) { return x + 1; } else { return 0; } \n} 39 public function bar():Bar! { return foo; } 40 } In this example, our solution will be: B = Boolean, \nFoo? = *, Foo! = Number, Bar! = * .Number. Note that as before, we infer Foo? = * because it is returned \nfrom a public function bar. Summary Our key observations can be summarized as follows. 1. Unlike type \ninference in statically typed languages, we treat in\u00ad.ows, which represent de.nitions, in a fundamentally \ndifferent manner than out.ows, which represent uses. The in.ows for a type variable determine its solution. \nThe out.ows for a type variable represent dynamic consistency checks that happen at the runtime. 2. \nWhen the in.ows for a type variable involve higher-order types, its solution is encoded by a kind, which \ndeconstructs the the type variable into parts, recursively solves for those parts, and reconstructs the \nsolutions to determine the solution of the type variable. In particular, the negative parts of such a \nsolution are determined by the negative parts of higher-order types in the out.ows for the type variable. \n 3. We need to see all the types of all values .owingintoavariable before we can infer its type. This \nmeans that we can only infer positive parts of the type that serves as the program s interface with existing \ncode. The negative parts of that interface can be de.ned by some unknown run-time environment, so we \neither need the programmer to annotate those parts explicitly, or we assume them to be the dynamic type. \n 4. We do not need a separate escape analysis to .nd which types escape their scope. Once we have put \nsuf.cient type annotations in the interface of the program with existing code, our .ows encode the escape \nanalysis. 5. Our framework extends naturally to objects (and classes). We use polarities to model object \nproperties with various access controls: private, read-only, write-only, and public.   Figure 2. Overall \nArchitecture for Type Inference  3. Formal Development In this section, we formally develop our type \ninference algorithm for a core gradually typed calculus of functions and objects. Unknown types are modeled \nas type variables in the language. We formalize the static semantics of the language by a compila\u00adtion \njudgment that translates a program annotated with types to a program with coercions (Section 3.2). We \nthen describe a type in\u00adference algorithm that analyzes the coercions to compute a .ow re\u00adlation over \ntypes, and derives a solution map for the type variables basedonthis .ow relation (Section 3.3). We also \ndescribe how the solution map may be weakened to recover standard blame guar\u00adantees (Section 3.4). Figure \n2 depicts the overall architecture for type inference. We prove complexity theorems for the type inference \nalgorithm (Section 3.5). Next, we formalize the dynamic semantics of the lan\u00adguage by an evaluation judgment \nthat, under a solution map for the type variables, reduces programs with coercions to values (Section \n3.6). Based on this dynamic semantics, we prove soundness theo\u00adrems for the type inference algorithm, \nby comparing the behaviors of programs under the solution map inferred by our algorithm with those under \nthe default solution map that interprets every type variable as the dynamic type (Section 3.7). 3.1 \nSyntax The syntax of the language is shown below. We model function parameters and object properties \nas variables x. In contrast to stan\u00addard object calculi, we distinguish readability and writability of \nob\u00adject properties: we model access capabilities . for object properties as subsets of {+, -},where + \ndenotes readability and - denotes writability. We express objects and their types as indexed collec\u00adtions \nof object properties and their types; for any non-negative in\u00adteger m the notation [m] means the set \nof indices {1,...,m},and for any syntactic entity f the notation {fi}i.[m] means the collec\u00adtion {f1,...,fm} \nindexed by [m]. term t ::= null | fun (x: T ) t: T' |{x .i : Ti = ti}i.[m] i | x | x = t | t(t') | t.x \n| t.x = t' |(T C T') t | if t then t' else t'' type variable X ::= a | X? | X! | X.x type T ::= . | \nT . T ' |{x .i : Ti}i.[m] i | * | X Terms include functions (of the form fun (x : T ) t : T '), objects \n(of the form {x .i : Ti = ti}i.[m]), and the null value i ' (null), as well as applications of functions \n(of the form t(t)), reads and writes of object properties (of the form t.x and t.x = t'), and null checks \n(of the form if t then t' else t''). They also include reads and writes of variables (of the form x and \nx = t). Furthermore, they include coercions between types (of the form T C T'). Coercions may be interpreted \nsyntactically as .ows, and semantically as representation-conversion operations. We assume that in source \ncode, there are no coercions. Types include function types (of the form T . T '), objects types (of \nthe form {x .i : Ti}i.[m]), the null type (.), and the i dynamic type (*). Furthermore, they include \ntype variables X. Some type variables may be derived by others: X.x denotes the type of the property \nx of objects of type X, while X? and X! denote the type of the input and output of functions of type \nX. We assume that in source code, type variables are distinct and are of the form a. Recursive functions, \nself references, and loops can be read\u00adily encoded in the language; so can blocks with local variables. \nThus, we do not include those constructs in the language. Further\u00admore, we do not model classes: their \ntreatment is similar to that of objects, and we discuss them further in Section 4. 3.2 Compilation We \nnow describe the compilation semantics of our language. The goal of compilation is to compute types and \nembed coercions in source programs, thereby preparing them for type inference (Section 3.3) and evaluation \n(Section 3.6). Compilation proceeds under a type environment. A type envi\u00adronment G is a .nite map from \nvariables to types. Compilation ' judgments are of the form G f ty:: T , meaning that t com\u00ad . tpiles \nto t' of type T under G. Figure 3 lists the rules for deriving compilation judgments. The compilation \nrules are fairly straightforward: the type of a term is computed in a syntax-directed manner, and whenever \na term of type T appears in a context that is annotated with type T', the coercion T CT' is attached \nto the term. We elaborate on the rules for deconstructing functions and objects, namely (C-APP)for function \napplications and (C-PROPR) and (C-PROPW) for object property accesses: they rely on a partial relation \n-between types, de.ned below. Intuitively, this relation captures the condition under which a term of \na certain type can be deconstructed, by viewing the type either as the type of a function that can be \napplied or as the type of an object for which a property can be read or written. De.nition 3.1 (View). \nAtype T canbeviewedasatype T ' if T -T ' can be derived by any of the following rules. X -X? . X! * \n-* . *  T . T ' -T . T '  X -{x . : X.x} for . = {+} or . = {-} * -{x . : *} for . = {+} or . = {-} \n  {x .i : Ti}i.[m] -{x . : Tj } such that j . [m] and . . .j , ij for . = {+} or . = {-} Compilation \nmay fail due to the partiality of the view relation: essentially, whenever a term of some type is deconstructed \nin a context that is annotated with an incompatible type. In particular, a function cannot be viewed \nas an object, an object cannot be viewed as a function, and an object without a particular access capability \nfor a property cannot be viewed as an object with that access capability for that property. Further restrictions \ncan be imposed statically. In particular, a standard gradual type system would detect other unsafe coer\u00adcions, \nby interpreting all type variables as the dynamic type * and ensuring that coercions are between statically \nconsistent types (cf. consistency and consistent-subtyping [16, 17]). Static consis\u00adtency is de.ned in \nSection 3.4, where we discuss how to carry over blame guarantees offered by such systems to our setting. \nHow\u00ad Compilation judgment G fty.t ' :: T (C-NULL) G fnull y.null :: . (C-FUN) ''''' ' G[x .T1] ft2 y.t2 \n:: T2 t2 = (T2 C T2)t2 '' G ffun (x:T1) t2 : T2 y.fun (x: T1)(t2 ): T2 :: T1 .T2 (C-OBJ) G ' =G[x1 .T1,...,xm \n.Tm] ''' ' .i.[m]. G ' fti yi :: T ' ti = (Ti ' C Ti)t .t ii .i i.[m] .i '' i.[m] .i i.[m] G f{x :Ti \n= ti}y.{x : Ti = ti }:: {x : Ti} i ii (C-PROPR) '{+} G fty.t :: TT -{x: Tj } j {+}' G ft.xj y.((T C {x: \nTj })t ).xj :: Tj j (C-PROPW) ' {-} G fty.t :: TT -{x: Tj } j ''' ' G ftj y.tj :: T ' tj = (Tj ' C Tj \n)tj j {-}' '' G ft.xj = tj y.((T C {xj : Tj })t ).xj = tj :: Tj (C-APP) G fty.t ' :: TT -T1 .T2 ''' ' \nG ft1 y.t1 :: T ' t = (T1 ' C T1)t 11 1 ' '' G ft(t1) y.((T C T1 .T2)t )(t1 ):: T2 (C-VARR) G(x)= T G \nfxy.x:: T (C-VARW) G(x)= T G fty.t ' :: T ' ' C T)t ' G fx = ty.x= (T :: T (C-IF) G fty.t ' :: T ''' \n' afresh .i.{1,2}. G fti y.ti :: Ti ti = (Ti C a)ti ' '' '' G fif tthen t1 else t2 y.if t then t1 else \nt2 :: a Figure 3. Compilation ever, we emphasize that we do not require those restrictions for our soundness \ntheorem: we can tolerate existing run-time failures as long as we do not introduce any new run-time failures. \n 3.3 Type Inference Following compilation, the coercions embedded in a program say what types of terms \nappear in what types of contexts. Other co\u00adercions are implicit in the type of the compiled program, \nwhich serves as the interface with existing code: these are coercions be\u00adtween the dynamic type and type \nvariables in the type of the com\u00adpiled program (which continue to be interpreted as the dynamic type \nby existing code). Based on this set of coercions, we conduct type inference as follows: 1. We apply \nsome carefully designed closure rules on this set of coercions, to compute a .ow relation between types \n(Section 3.3.1). The .ow relation overapproximates the set of all possi\u00adble .ows of terms to contexts \nat run time.  2. We then derive a solution map for type variables based on that .ow relation (Section \n3.3.2). In particular, if some context is   Flow judgment T C T ' (F-BASE) T C T ' is a coercion in \nthe compiled program T C T ' (F-COMP) T is the type of the compiled program C .{X C *|X is positive in \nT}.{*C X |X is negative in T} C  (F-PULL) KX C X X C Y KX C Y  (F-FACTOR) T C X T X = KX C .{T C KX \n,KX C X} C (F-TRAN) KX C X X C T KX T KX C T (F-EXPFUNL) *C T1 .T2 *.*C T1 .T2 (F-EXPOBJL) *C {x .i \n: Ti}i.[m] i {x .i : *}i.[m] C {x .i : Ti}i.[m] ii (F-EXPFUNR) T1 .T2 C * T1 .T2 C *.* (F-EXPOBJR) \n.i i.[m] C * {x : Ti} i {x .i : Ti}i.[m] C {x .i : *}i.[m] ii (F-SPLITFUN) '' '' T1 .T2 C T1 .T2 C .{T1 \nC T1,T2 C T2}C (F-SPLITOBJ) {x .i : Ti}i.[m] C {x .i : Ti ' }i.[n] ii n=m .i .[n]: .i ' ..i ''' ' j \n.[n] C .{Tj C Tj |+ ..j }.{Tj C Tj |-..j }C Figure 4. Computation of Closure annotated by a type variable, \nthe solution for that type variable is an overapproximation of the types of terms that may .ow to that \ncontext at run time. 3.3.1 Computation of Closure Given a compiled program and its type, we compute \na .ow relation between types by initializing and growing a set of coercions until .xpoint. Flow judgments \nare of the form T C T ' , meaning that .ows from type T to type T ' are possible, i.e., terms of type \nT may .ow to contexts annotated with type T ' at run time. Figure 4 lists the rules for deriving .ow \njudgments. Those rules are explained below. Unlike usual .ow relations for statically typed languages \nwith subtyping, our .ow relation is not merely a transitive closure of the coercions in a compiled program, \ninterpreted as subtyping con\u00adstraints. Instead, it is designed carefully to account for gradual typ\u00ading, \nand is tuned for ef.ciency. Rules (F-BASE) and (F-COMP) generate the initial facts. In particular, (F-COMP) \nrelies on a standard notion of polarity for type variables, de.ned below, to ensure that any type variable \nthat appears in the type of the compiled program is (or becomes, by other rules) tainted with the dynamic \ntype in the contravariant parts. De.nition 3.2 (Polarity of a type variable in a type). The set of positive \nvariables and the set of negative variables in a type T are P+(T )and P-(T ), respectively, de.ned as \nfollows. Let s range over {+, -};let s be -when s is +, and +when s is -. P+(X)={X}, P-(X)={}  Ps(*)=Ps(.)={} \n Ps(T1 .T2)=Ps(T1).Ps(T2)  .i i.[m] Ps({x : Ti})= {Ps(Ti) | i . [m], -. .i}. i {Ps(Ti)|i .[m], +..i} \nFor example, if the interface of the compiled program with existing code is X1 .X2,then (F-COMP)re.ects \nthe following assumptions: We assume that there is a .ow from * to X1.  We also assume that there is \na .ow from X2 to *,sothatif(say) there is a local function that escapes through X2, then (by other rules) \nwe can derive a .ow from * to the parameter type of the escaping function.  Similar considerations apply \nif the interface is an object type: if a writable property is annotated with a type variable, there is \na .ow from * to that type variable, and if a readable property is annotated with a type variable, there \nis a .ow from that type variable to *. (If a property is both, we assume both .ows; if a property is \nneither, we assume neither.) Other rules, namely (F-PULL), (F-FACTOR), and (F-TRAN), rely on a notion \nof kinding for type variables, de.ned below. Intu\u00aditively, a type variable has, for every type constructor \nthat may form a solution for the type variable, a kind that encodes that solution. De.nition 3.3 (Kinds \nfor a type variable). Kinds for a type variable X, ranged over by KX , are types of the form ., X? . \nX!, .i i.[m] {x :X.xi},or *. i Eventually, the solution of a type variable is derived by the kinds that \n.ow into it, either directly or indirectly through other type variables. Thus, the rule (F-PULL) pulls \nany kind on the left of a type variable to any other type variable on its right. The rule (F-FACTOR) \nfactors a coercion from a type to a type variable into intermediate coercions through a corresponding \nkind for that type variable, computed as shown below. This factoring ensures that .ows from types to \na type variable are appropriately captured in solutions for that type variable. De.nition 3.4 (Kind of \na type for a type variable). The kind T X of a type T ,where T is not a type variable, for a type variable \nX is de.ned as follows: . X =. * X =*  T1 .T2 X =X?.X!  .i i.[m] .i i.[m] {x :Ti}X ={x :X.xi} ii \nThe rule (F-TRAN) limits transitive .ows through a type vari\u00adable in two ways: it only considers a kind \non the left, and it only considers a type on the right such that the kind on the left is dynam\u00adically \nconsistent with the type on its right. Dynamic consistency is a partial relation between types that are \nnot type variables, de\u00ad.ned as follows; it models coercions that never fail at run time. De.nition 3.5 \n(Dynamic Consistency). Atype T is dynamically consistent with another type T ' if T and T ' are not type \nvariables, and TT ' can be derived by any of the following rules. . T T*, *T T1 .T2 T1 ' .T2 ' .i i.[m] \n.i ' {x : Ti}{x : Ti }i.[n] if n = m, and for all ii i .[n], we have . ' i ..i In combination with \n(F-FACTOR), the rule (F-TRAN) ensures that .ows from types on the left of a type variable to types on \nthe right of the type variable are taken into account, without computing a standard transitive closure. \n(In Section 4, we discuss why comput\u00ading a standard transitive closure is undesirable.) The remaining \nrules are fairly straightforward. (F-EXPFUNL), (F-EXPFUNR), (F-EXPOBJL), and (F-EXPOBJR) expand * on \nthe left or right of a higher-order type to the appropriate shape. Finally, (F-SPLITFUN) and (F-SPLITOBJ) \nsplit .ows between higher-order types into .ows between their parts, respecting co\u00advariance and contravariance. \nNote that since we distinguish access capabilities for reading and writing object properties, the types \nof object properties are not necessarily invariant: they may be covari\u00adant (read-only), contravariant \n(write-only), invariant (read-write), or even abstract (no access). 3.3.2 Derivation of Solutions Based \non the .ow judgment, we derive a solution map I that associates each type variable X to a type without \ntype variables. We also extend I to a function lI from types to types without type variables, such that \nlI(T)is the type obtained by substituting each type variable X in type T by I(X). To solve for X,let \nT+(X)be the set of types T such that T is not a type variable, and T C X ; we compute the least upper \nbound of the kinds of types for X in T+(X),asde.ned below. De.nition 3.6 (Least upper bound of kinds). \nThe least upper UKX bound KX 12 of two kinds KX 1 and K2 X for X is de.ned as follows: .UKX =KX , KX \nU.=KX * UKX =*, KX U* =*  (X?.X!)U(X?.X!)=X?.X!  .i .j .k k.[p] {x :X.xi}i.[m] U{y :X.yj }j.[n] ={z \n:X.zk}, ij k k.[p] where {zk}={xi}i.[m] n{yj }j.[n] and for all i .[m], j .[n], and k .[p],if xi =yj \n=zk then .k =.i n.j . .i i.[m] (X?.X!)U{x :X.xi}=*  {x .i :X.xi}i.[m] U(X?.X!)=*  i i The solution \nI(X)for X is then de.ned aslI(UT X ). T .T+(X) Such solutions are always well-founded, since kinds do \nnot have cyclic dependencies.  3.4 Blame Recovery Standard gradual type systems enforce that the coercions \nin a com\u00adpiled program satisfy static consistency, which is a partial relation between types that are \nnot type variables, de.ned as follows: De.nition 3.7 (Static Consistency). Atype T is statically consis\u00adtent \nwith another type T ' if T and T ' are not type variables, and T : T ' can be derived by any of the following \nrules.  .: T  T : *, * : T  T1 .T2 : T1 ' .T2 ' if T1 ' : T1 and T2 : T2 '  .i .i {x : Ti}i.[m] :{x \n: Ti ' }i.[n] if n = m, and for all i i ' ' T ' i . [n], we have .i . .i, and if + . .i then Ti : i ,if \n-..i ' then Ti ' : Ti. This static consistency relation extends similar relations de.ned separately for \na gradually typed language of functions and for a gradually typed language of objects. We conjecture \nthat programs in our language satisfy the blame theorem whenever they compile to programs whose coercions \nsatisfy the static consistency relation, following similar type-safety results for existing languages. \nHowever, the solution map I derived above does not guaran\u00adtee static consistency. This means that blame \nguarantees offered by standard gradual type systems do not carry over with type in\u00adference. Fortunately, \nit is possible to weaken I to recover those blame guarantees, as follows. For any type variable X,let \nT-(X) be the set of types T such that T is not a type variable and XCT . If there is any T . T-(X) such \nthat I(X) T,then I(X) is re\u00adde.ned to be *. We can then prove the following theorem. Theorem 3.8 (Blame \nrecovery). Suppose that for every coercion T CT ' in the compiled program, we have l*(T) : l*(T ' ),where \nl* is the solution map that associates every type variable with *. Then for every .ow judgment T C T \n' , we have lI(T) : lI(T ' ).  3.5 Algorithmic Complexity Although computation of closure may be performed \nby applying the .ow rules in arbitrary order until .xpoint, an effective algorithm would apply them systematically, \nas described below; we can then reason about its ef.ciency. De.nition 3.9 (Flow computation). The computation \nof .ow judg\u00adments proceeds in the following steps until .xpoint: (1) Initially, rules (F-BASE) and (F-COMP) \nare applied until sat\u00aduration. (2) Next, rules (F-PULL) and (F-FACTOR) are applied until satu\u00adration. \n (3) Next, rules (F-TRAN), (F-EXPFUNL), (F-EXPOBJL), (F-EXPFUNR), (F-EXPOBJR) are applied until saturation. \n (4) Next, rule (F-SPLITFUN) and (F-SPLITOBJ) is applied until saturation. (5) Finally, control returns \nto step (2), where if no new .ow judg\u00adments are derived, the algorithm terminates.  We prove that our \nalgorithm terminates and is ef.cient. Theorem 3.10 (Termination). Flow computation terminates. Proof. \nRecall that all type variables in the system are distinct, so they have unique depths. Let d be the maximum \ndepth of types in the system after step (1). Steps (2) and (3) introduce types of depth = d in the system. \nStep (4) introduces types of depth = d - 1. Finally, in step (5), typesofdepth dand type variables of \ndepth 1 can be thrown away. This means that maximum depth of types in the system after step (5) is d-1. \nThus, .ow computation must terminate in =diterations. Theorem 3.11 (Time Complexity). The time complexity \nof .ow computation is quadratic in the size of the program. Proof. Let k be the number of type variables, \nn be the number of types other than type variables, and d be the maximum depth of types after step (1). \n Step (2) takes O(k2) time. Step (3) takes O(kn) time, and increases the number of types by some factor \nw that denotes the maximum width of types. Step (4) takes O(n 2) time, and increases the number of types \nby the same factor w. Thus, before step (5), the total time taken is O((k+ n)2),and after step (5), we \nhave wk variables and wntypes other than type variables. Since the algorithm terminates in = diterations, \nthe total time 2d(k + n)2 complexity is O(w ). Typically, w and d are small constants, so the time complexity \nis almost quadratic in the number of types k + n. In general, if the size of the program is N,then w \nd(k+n)= O(N). Thus the total time complexity is O(N2). 3.6 Evaluation We now describe the evaluation \nsemantics of our language, which enables us to prove that our type inference algorithm is sound. Let \nfrange over locations. A stack S is a sequence of locations. The syntax of values is as follows. value \nv ::= (.C \u00b7\u00b7\u00b7C T)null |(T1 .T2 C \u00b7\u00b7\u00b7C T).S x.t | ({x .i : Ti}i.[m] C \u00b7\u00b7\u00b7C T)f i A value of the form \n(T C T) u is abbreviated as u :: T. Furthermore, we use the notation (T ' C\u00b7\u00b7\u00b7CT '' )(TC\u00b7\u00b7\u00b7CT ' )u to \ndenote (T C \u00b7\u00b7\u00b7 C T ' C \u00b7\u00b7\u00b7 C T '' ) u. Unlike previous work that focuses on space-ef.cient implementations \nof gradually typed languages [19], our dynamic semantics admits unbounded chains of coercions to simplify \nthe speci.cation and proof of soundness: essentially, we keep coercions in symbolic form and check that \nthey normalize along the way. A record is a map from variables to values. A heap H is a map from locations \nto records. We rely on the following operations for querying/updating vari\u00adables on the stack through \nthe heap. H[f .H(f)[x .v]] if x .dom(H(f)) H[x= .v]S,\u00a3 H[xif x/ .v]S .dom(H(f)) H(f)(x) if x.dom(H(f))H[x]S,\u00a3 \n= H[x]S if x/.dom(H(f)) The evaluation judgment is S f(H,t) .s (H ' ,v),where sis a solution map, which \nassociates type variables to types without type variables. Figure 5 lists the rules for deriving evaluation \njudgments. The rules are fairly standard, except that the coercions on val\u00adues are symbolic: the type \nvariables in those coercions are not sub\u00adstituted by their solutions, but instead the solutions are looked \nup when normalizing the coercions. This is convenient for our sound\u00adness proof. Normalization is de.ned \nas follows. De.nition 3.12 (Normalization). The chain of coercions T0 C\u00b7\u00b7\u00b7C Tn+1 normalizes under s if \nfor all j .[n+1], s(T0) s(Tj ). Furthermore, a value v normalizes under s, written v .s,ifit is of the \nform (T0 C\u00b7\u00b7\u00b7CTn+1)uand (T0 C\u00b7\u00b7\u00b7CTn+1)normalizes under s. The deconstruction rules for functions and \nobjects, namely (E-APP), (E-PROPR), and (E-PROPW), rely on the following stan\u00addard splitting rules for \nhigher-order coercions. .i .[n+1].Ti -{xj{+} : Ti ' } {+}{+}' ' (T0 ...Tn+1 C {x: Uj }) - {x: (T0 ...Tn+1)} \njj .i.[n+1].Ti -{x{-}: Ti ' } j {-}{-}' ' (T0 ...Tn+1 C {x: Uj }) - {x: (Tn+1 ...T0)} jj  Evaluation \njudgment S f(H,t) .s (H ' ,v) (E-NULL) S f(H,null) .s (H,null :: .) (E-FUN) S f(H,fun (x:T1) t2 : T2) \n.s (H,.S x.t2 :: T1 .T2) (E-OBJ) fis fresh H1 = H[f .[x1 .null,...,xn .null]] .i.[m].S,ff(Hi,ti) .s (Hi' \n,vi) Hi' [xi .vi]S,\u00a3 = Hi+1 .i .i S f(H,{x : Ti = ti}i.[m]) .s (Hm+1,f :: {x : Ti}i.[m]) ii (E-PROPR) \n'{+} S f(H,t) .s (H,(C)f) C- {x: (Cj )} j H ' (f)= Rvj = R(xj ) (Cj )vj .s S f(H,t.xj ) .s (H ' ,(Cj \n)vj ) (E-PROPW) S f(H,t) .s (H ' ,(C)f) C- {x{-}: (Cj )} j S f(H ' ,tj ) .s (Hj ,vj ) (Cj )vj .s Hj (f)= \nRH '' = Hj [f.R[xj .(Cj )vj ]] S f(H,t.xj = tj ) .s (H '' ,(Cj )vj ) (E-APP) S f(H,t) .s (H ' ,(C).S \nx.t2) C- C1 .C2 S f(H ' ,t1) .s (H1,v1) (C1)v1 .s fis fresh H '' = H1[f.[x .(C1)v1]] S ' ,ff(H '' ,t2) \n.s (H2,v2) (C2)v2 .s S f(H,t(t1)) .s (H2,(C2)v2) (E-VARR) H[x]S = v S f(H,x) .s (H,v) (E-VARW) '' '' \nS f(H,t) .s (H ,v) H [x.v]S = H S f(H,x = t) .s (H '' ,v) (E-CAST) S f(H,t1) .s (H ' ,v1) (T1 C T2)v1 \n.s S f(H,(T1 C T2)t1) .s (H ' ,(T1 C T2)v1) (E-IF) S f(H,t) .s (H ' ,v) v = null .i=1 v = null .i=2 \n ' ''' S f(H ,ti) .s (H ,v ) S f(H,if tthen t1 else t2) .s (H '' ,v ' ) Figure 5. Evaluation .i .[n+1].Ti \n-Ti ' .Ti '' ' ' '''' (T0 ...Tn+1 C U1 .U2)- (Tn+1 ...T0).(T0 ...Tn+1)  3.7 Soundness Since we infer \nmore precise types where there were less precise types (*), the interesting direction for soundness is \nto establish, as above, that we do not introduce any run-time errors. The other di\u00adrection is trivial, \nand can be established by reasoning about positive and negative blames [24] (see Section 6). We prove \nthe following soundness theorem for our type infer\u00adence algorithm, which says that if a compiled program \nevaluates to a value with dynamic types for type variables, then it evaluates to the same value with \ninferred types for type variables. (Recall that the dynamic semantics is symbolic in type variables, \nbut ensures that any coercions in values normalize.) Theorem 3.13 (Soundness). Let \u00d8 f ty. t ' :: T.Let \nI be the inferred solution map for t ' :: T, and let *be the solution map that associates every type \nvariable with *.If \u00d8 f (\u00d8,t ' ) .* (H ' ,v), then \u00d8 f(\u00d8,t ' ) .I (H ' ,v). Soundness follows as a corollary \nof our main lemma, Term Correspondence (Lemma 3.16, see below), which states not only (i) there a correspondence \nbetween reductions in the original program and reductions in the program with inferred types, but also \n(ii) any coercions that may be generated at run time have already been generated at compile time (via \nclosure computation). Of course, (ii) is a crucial invariant to show (i), since it means that the solutions \nwe compute at compile time behave as expected at run time. De.nition 3.14 (Knowledge of coercions). The \nchain of coercions T0 C \u00b7\u00b7\u00b7C Tn+1 is known if for all j .[n+1], Tj-1 C Tj . De.nition 3.15 (Compatibility \nof type environment with stack and heap). The type environment G is compatible with the stack Sunder \nheap H, written G ~H S, if whenever G(x)= Tn+1, we have H[x]S = (T0 C \u00b7\u00b7\u00b7C Tn+1)vand (T0 C \u00b7\u00b7\u00b7C Tn+1)is \nknown. Lemma 3.16 (Term Correspondence). Let G f ty. t ' :: T and G ~H S.If S f(H,t ' ) .* (H ' ,(T0 \nC \u00b7\u00b7\u00b7C Tn+1)v) then: S f(H,t ' ) .I (H ' ,(T0 C \u00b7\u00b7\u00b7C Tn+1)v (T0 C \u00b7\u00b7\u00b7C Tn+1)is known The proof of Term \nCorrespondence requires Value Correspon\u00addence (Lemma 3.17, see below), and two other lemmas on knowl\u00adedge \nof coercions, Function Flow and Object Flow, which are used in the cases of function application and \nobject property access. Lemma 3.17 (Value Correspondence). If T0 C\u00b7\u00b7\u00b7CTn+1 is known and normalizes under \n*, then it normalizes under I. 0 C T '' Lemma 3.18 (Function Flow). If T0 .T ' 1 C \u00b7\u00b7\u00b7C Tn'' +1 is known \nand Ti '' -Ti .Ti ' for all i.[n+1],then Tn ...T1 CT0 is known and T0 ' C T1 ' ...Tn ' is known. Lemma \n3.19 (Object Flow). If {x .i-1 : Ti-1}i.[m] CT '' C\u00b7\u00b7\u00b7C i-11 Tn'' +1 is known and T '' -{x .0 : Ti} \nfor all i . [n+1],then i 0 Tn ...T1 C T0 is known if . ' = {-}and T0 ' C T1 ' ...T ' is known 0 n if \n.0 ' = {+}. The proofs of these lemmas in turn require the closure rules, and the following two basic \nlemmas on dynamic consistency. Lemma 3.20 (Kind Ordering). If UT .T T = T ' then for all T .T, we have \nT . T ' Lemma 3.21 (Monotonicity). If X C Y then I(X) I(Y). Furthermore, we can prove the following \nsoundness theorem that says that our type inference is compositional. Theorem 3.22 (Compositional Soundness). \nSuppose that \u00d8 f t1 y. t ' 1 :: T1.Let I be the inferred solution map for t ' 1 :: T1, and * be the solution \nmap that associates every type variable with *.Let t be a program without type variables, such that \u00d8 \nf ' ''' ty. t :: l*(T1) . T2.Let t1 = (T1 C l*(T1)) t1. Then ''' ''' \u00d8 f(\u00d8,t (t1 )) .* (H,v2) implies \n\u00d8 f(\u00d8,t (t1 )) .I (H,v2). Proof. The new coercion is T1 C l*(T1).However,by (F-COMP) we already know \ncoercions of the form X C * for positive type variables X and *CX ' for negative type variables X ' in \nT1.Soby Term Correspondence, the composition is sound. Finally, in Section 4.4 we discuss the adequacy \nof our inferred types, and conjecture that our algorithm is relatively complete (i.e., the solution maps \nit infers are optimal in a restricted family of possible solution maps). Unfortunately, we cannot compete \nwith manual reasoning: we are working in a setting that admits reasoning outside the static type system \nvia *.  4. Discussion 4.1 Feature Extensions Base Types Base types in our language can be modeled as \nkinds. Kinds correspond to type constructors, and the base types are type constructors of arity 0. By \nDe.nition 3.3, any base type is a kind for any X, and by De.nition 3.4 the kind of a base type for a \ntype variable as that base type itself. Finally, in De.nition 3.6 the least upper bounds over base types \ntakes into account partial subtyping relations between base types in the obvious manner. Classes and \nNominal Subtyping We do not model classes in our formal language, but could do so by closely following \nthe treatment of objects. In particular, instance methods are modeled as read-only properties and instance \n.elds are modeled as read-write properties; private properties are also modeled. Our algorithm and its \nguarantees carry over to a language with nominal subtyping, if the compiler ensures that nominal subtyping \nimplies structural subtyping. For example, in ActionScript, nominal subtyping of objects is available \nvia subclassing. The ActionScript compiler ensures that when class B is a subclass of class A, the structural \ntype of B-objects is a subtype of the structural type of A-objects, by checking that the properties in \nA that are overridden in B are related by subtyping, taking into account read/write polarities. 4.2 \nLimited Transitivity Previous type inference algorithms for object-oriented languages with subtyping \n[14, 15] involve taking full transitive closure of sub\u00adtyping constraints, which makes them O(n 3),where \nn is the size of the program. In contrast, we have designed our closure rules for .ows so that transitivity \nis limited, which not only makes our al\u00adgorithm O(n 2), but also allows for more precise type inference. \nFor example, consider this sequence of .ows: Number C X C Y C Boolean. Our core algorithm infers X = \nNumber and Y = Number. Now, if we want our solutions to satisfy the blame theorem, we fall back to Y \n= *,since Y = Number is inconsistent with Y C Boolean. However, we do not need to fall back to X = *. \nIn contrast, full tran\u00adsitivity would have lost the information that there is no immediate dynamic consistency \ncheck between X and Boolean.  4.3 Performance Optimizations As we will see in our experiments, adding \nmore precise types improves run-time performance in most cases. But more precise types can also degrade \nrun-time performance by introducing new run-time conversions. It does not matter whether these precise \ntypes are added by type inference or manually the effect is the same. For example, consider the following \nprogram: 41 function foo(x:*):Number { return x+1} 42 function bar(max:Number):Number { 43 var sum:Number \n= 0; 44 vary =1; 45 while(sum < max) { sum = sum + foo(y) } 46 return sum 47 } The function foo could \nbe part of some external library which we cannot modify. Without any type annotation, variable y in bar \nwill be assigned * type. So, the function call foo(y) will not involve any run-time conversions for y, \nsince the parameter type for foo is also *. But if we annotate y with Number, either using our type inference \nalgorithm or manually, function call foo(y) will result in a run-time conversion from Number to * for \ny. We observe that the .x for this problem lies in the runtime rather than our inference algorithm. \nOur type inference algorithm tries to infer more precise types for unannotated variables. The runtime \nshould use these precise types to optimize the program. In partic\u00adular, it can do type specialization \n[10] for foo: i.e., it can create a copy of foo which takes a Number argument, and patch the func\u00adtion \ncall foo(y) in bar to this new copy. (Type specialization al\u00adready happens in ActionScript for numeric \noperations.) The result\u00ading program will be more optimized than the unannotated version. 4.4 Adequacy \nof Inferred Types We now analyze how precise our inferred types are. Our closure algorithm ensures that \nif there exists some .ow from a concrete type T to a type variable X via other type variables, we consider \nT when computing the solution for X. We do this to make sure that we do not introduce any new run-time \nerrors by missing out some types that could .ow into X. We promote the solution for X to * when two different \nkinds .ow into X or * .ows into X explicitly. One could give more precise types than our algorithm by \nrea\u00ad soning outside the type system. For example, consider the follow\u00ad ing program: 48 var x:X,y:Y; 49 \nif(b){x=1} else {x =false}; 50 if(b){y=x} else {y=1} We generate Number C X and Boolean C X on line \n49, and X C Y and Number C Y on line 50. When we do closure, we add Boolean C Y also, and we infer X \n= * and Y = *. Whereas, with path-sensitive reasoning (outside the type system), the programmer can argue \nthat Y need not be *, it can be Number. Blame recovery may also reduce the precision of our solutions. \nFor example, if the generated .ows are Number C X, X C Y,and Y C Boolean,weinfer X = Number and Y = Number.Whenwe \ndo blame recovery, we see that the coercion Y C Boolean is not satis.ed, and so we promote Y to *. Whereas, \nthe programmer could get away by annotating Y as Number and making sure that Y C Boolean does not happen \nat run time, again by reasoning outside the type system. However, we expect that if reasoning outside \nthe type system is banned, then our algorithm indeed infers optimal solutions. For\u00admally, we can de.ne \na subset of the static consistency relation that forces the use of purely static reasoning (standard \nsubtyping) for in.ows. We then conjecture that there are no other solutions that are na\u00a8ive subtypes \nof the solutions we infer upon blame recovery, and that satisfy the above relation.  5. Experiments \nWe have implemented our type inference algorithm for Action-Script. Our implementation takes an existing \nActionScript program as input, and returns the same program with inferred types added to the source as \noutput. The output program can then be compiled and run using the compiler and VM, and compared with \nthe input program. 5.1 Methodology We use the SunSpider [20] and V8 [23] benchmarks to evaluate our type \ninference algorithm. These are standard benchmarks used to measure the performance of JavaScript implementations; \nwe use the ActionScript version of these benchmarks, which are part of the test suite of the ActionScript \nVM.   +   +   + ' ! ! ! !  ! !  ! !  ! ! ! % ! ) ! !  ! !  ! ! ! ! ! ! $ &#38;$ \n($ *$ +$ %$$ %&#38;$  Figure 6. Comparison of our algorithm with partially typed code In their original \nform, these benchmarks are fully typed,i.e. their source code has complete type information. We remove \nall type annotations, except in those parts of the interface that are required by our algorithm to be \nexplicitly annotated. We then run our algorithm on these partially typed benchmarks. With performance \nof the fully typed benchmarks as the base\u00adline, we compare the performance of partially typed benchmarks, \nwith and without types added by our inference algorithm. We also compare the results of our algorithm \non partially typed benchmarks with the results of Chang et al. [7], who have recently implemented a much \nsimpler type inference algorithm (that only infers types for a subset of local variables) along with \nseveral other high-level op\u00adtimizations for ActionScript at bytecode level, and report perfor\u00admance increases \ndue to the added type information.  5.2 Results 5.2.1 Comparison with Partially Typed Code Figure 6 \ncompares the performance of partially typed benchmarks, with and without types added by our inference \nalgorithm. Overall, our algorithm gives an average 1.6x performance improvement over partially typed \nbenchmarks, with a maximum improvement of 5.6x. We are able to recover ~100% performance (i.e. the perfor\u00admance \nof fully typed benchmarks) in 13 out of the 17 benchmarks. For v8\\richards, our performance is higher \nthan the fully typed version. There are some private class variables in the benchmark which are typed \nas * in the fully typed version, whereas our algo\u00adrithm is able to infer more precise types for them, \nresulting in an increased performance. In the fully typed version of sunspider\\math-cordic, an ob\u00adject \nis retrieved from an array and implicitly cast to an integer via an explicit int annotation. Since arrays \nare untyped in Action-Script, our inference algorithm infers the type of retrieved object as *. This \none annotation is the reason that we could reach only ~50% performance as compared to the fully typed \nbenchmark. Similar explicit type annotations for properties accessed from the type Object, which are \nalso untyped in ActionScript, hurt our performance in v8\\raytrace.For sunspider\\access-nsieve and sunspider\\access-fankkuch, \nwe infer some variables as Number, whereas in the fully typed benchmarks, they are typed  *   * \n  *      &#38;                                        \n $     ( # %# '# )# *# $## $%# $'#  Figure 7. Comparison of our algorithm with Chang et al. as \nint. In ActionScript, int represents 32 bit integers whereas Number has size 64 bits and can represent \nintegers, unsigned in\u00adtegers, and .oating-point numbers. At run time, operations involv\u00ading int are much \nfaster than those involving Number.Since we do not implement a range analysis, we conservatively infer \nall nu\u00admeric types to be Number. This hurts the performance in the above benchmarks. Our algorithm can \nbe augmented with a simple range analysis to mitigate this problem. 5.2.2 Comparison with Chang et al. \n[7] Figure 7 compares the performance of partially typed benchmarks, with types added by our inference \nalgorithm and with types added by inference algorithm of Chang et al. In 11 out of the 17 bench\u00admarks, \nour numbers are better than them. In 5 out of the 17, the numbers are almost equal. For bitops-bits-in-byte,theyre\u00adport \nthat the higher performance is because of a different inter\u00admediate code representation they implement \nin the ActionScript JIT compiler. This effect is independent of the type inference al\u00adgorithm. 5.2.3 \nPerformance Degradation after Type Inference In Figure 6, we see that for crypto-sha1, the performance \nde\u00adgrades after adding precise type information to the partially typed code. This general issue was also \nfound by [7] (where the effect is even worse), and is already discussed in Section 4.3.  6. Related \nWork Gradual Type Inference The only previous work on gradual type inference is the uni.cation-based \ntechnique of [18], which is not suitable for an object-oriented language with subtyping. Soft Typing \nOverall, our goal is the same as that of soft typing [6]: to improve run-time performance by eliminating \nrun-time type checks where ever possible. However, to our knowledge, all the soft type inference systems \nproposed to date [2, 25] are based on uni.\u00adcation, which is unsuitable for object-oriented languages \nwith sub\u00adtyping. Since we do not aim to infer polymorphic types in Action-Script, we are interested in \nalgorithms with polynomial complexity. Furthermore, treating uses and de.nitions asymmetrically enables \nus to infer more precise types than soft type inference. Finally, ex\u00adisting soft type inference systems \nhave not considered problems of soundness in the presence of partial compilation, whereas our al\u00adgorithm \nretro.ts type inference on existing programs under the as\u00adsumption that complete source code may not \nbe available for anal\u00adysis. This restriction of preserving semantics of code outside the compilation \nunit implies that we must infer types for only those parts of the program that we can see all the writes \nto, i.e., those parts of the program that do not escape the compilation unit. Blame Our algorithm seems \nto share deep connections with the blame calculus [19, 24] and other coercion calculi [11]; exploring \nthese connections should be interesting future work. In particular, the blame calculus de.nes three new \nsubtyping relations: <:+ (positive subtyping), <:- (negative subtyping), and <:n (na\u00a8ive subtyping), \nsuch that S<:n Tdenotes that Sis more precise than T, and holds if and only if S<:+ Tand T<:- S. In particular, \nwe have S<:+ *and *<:- T. The main result in [24] is that if S<:+ Tthen a run-time cast error from Sto \nTcannot be blamed on the term to which the cast is attached, and if S<:- Tthen a run-time cast error \nfrom Sto Tcannot be blamed on the context in which the cast appears. The solutions of our algorithm are \nrelated to the default types by <:n, so we can say that our solutions are more precise than the default \ntypes. In a context that previously expected the default type, we now effectively introduce a cast from \na more precise type; this means that any blame for a run-time cast error must lie with the context, so \nthere is nothing to prove about the program. On the other hand, wherever we infer a more precise type \nfor a context, we effectively introduce a cast from the default type; this means that any blame for a \nrun-time cast error must lie with the pro\u00adgram, i.e., we must prove that a program that executed successfully \nbefore type inference continues to execute successfully after type inference as we do in this paper. \nCombining Static Typing and Dynamic Typing There has been a lot of interest in exploring ways to mix \ntyped and untyped code, e.g., via occurrence typing [22], gradual typing [16, 17], hybrid typing [8], \nand like typing [26]. In these systems, types are supplied by the user. In contrast, our work focuses \non type inference, which is complementary. Static Type Inference Static type inference has been explored \nfor dynamic languages, including Self [1], Ruby [9], Python [4], JavaScript [5, 21], and Scheme [25]. \nThere is also a long history of work on type inference for languages with subtyping [14, 15]. Dynamic \nType Inference Our type system gradually infers some static types, but is still bound by the limitations \nof static analysis on programs that use dynamic types. As such, we believe that dynamic type inference \nwould be useful to improve the precision of those dynamic types which we cannot eliminate in a program. \nRecent work has explored dynamic techniques for type inference and type specialization [3, 10] for dynamic \nlanguages. As future work, we plan to explore such combinations in the just-in-time (JIT) compiler underlying \nthe ActionScript VM.  7. Conclusion In this paper, we design a type inference algorithm that can improve \nthe performance of existing gradually typed programs, without in\u00adtroducing any new run-time failures. \nThe distinguishing features of the algorithm lie in its asymmetric treatment of in.ows and out\u00ad.ows, \nand its encoding of an escape analysis to preserve backward\u00adcompatibility. We prove that our algorithm \nis sound and ef.cient, and demonstrate its applicability on a mainstream gradually typed language, ActionScript. \nAcknowledgments We would like to thank Bernd Mathiske, Dim\u00aditrios Vytiniotis, and several anonymous reviewers \nfor their helpful comments and suggestions. This work was done while the .rst au\u00adthor was an intern at \nAdvanced Technology Labs, Adobe Systems.  References [1] O. Agesen, J. Palsberg, and M.I. Schwartzbach. \nType Inference of SELF. ECOOP, 1993. [2] A. Aiken, E. L. Wimmers, and T. K. Lakshman. Soft Typing with \nConditional Types. In POPL, pages 163 173, 1994. [3] J. D. An, A. Chaudhuri, J. S. Foster, and M. Hicks. \nDynamic Inference of Static Types for Ruby. In POPL, pages 459 472. ACM, 2011. [4] D. Ancona, M. Ancona, \nA. Cuni, and N. Matsakis. RPython: Rec\u00adonciling Dynamically and Statically Typed OO Languages. In DLS. \nACM, 2007. [5] C. Anderson, P. Giannini, and S. Drossopoulou. Towards Type Infer\u00adence for JavaScript. \nIn ECOOP, 2005. [6] R. Cartwright and M. Fagan. Soft typing. In PLDI, pages 278 292, 1991. [7] M. Chang, \nB. Mathiske, E. Smith, A. Chaudhuri, A. Gal, M. Bebenita, C. Wimmer, and M. Franz. The Impact of Optional \nType Information on JIT Compilation of Dynamically Typed Languages. In DLS.ACM, 2011. [8] C. Flanagan. \nHybrid Type Checking. In POPL, pages 245 256. ACM, 2006. [9] M.Furr,J.D.An,J.S.Foster,andM.Hicks. Pro.le-Guided \nStatic Typing for Dynamic Scripting Languages. In OOPSLA, 2009. [10] A. Gal, B. Eich, M. Shaver, D. \nAnderson, D. Mandelin, M. R. Haghighat, B. Kaplan, G. Hoare, B. Zbarsky, J. Orendorff, J. Ru\u00adderman, \nE. W. Smith, R. Reitmaier, M. Bebenita, M. Chang, and M. Franz. Trace-based just-in-time type specialization \nfor dynamic languages. In PLDI, pages 465 478. ACM, 2009. [11] F. Henglein. Dynamic Typing: Syntax and \nProof Theory. Science of Computer Programming, 22(3):197 230, 1994. [12] D. Herman, A. Tomb, and C. \nFlanagan. Space-Ef.cient Gradual Typing. Trends in Functional Programming, 2007. [13] C. Moock. Essential \nActionScript 3.0. O Reilly, 2007. [14] J. Palsberg. Ef.cient Inference of Object Types. In LICS, pages \n186 195. IEEE, 1994. [15] F. Pottier. A Framework for Type Inference with Subtyping. In ICFP, pages \n228 238. ACM, 1998. [16] J. Siek and W. Taha. Gradual Typing for Objects. In ECOOP, pages 2 27. Springer-Verlag, \n2007. [17] J. G. Siek and W. Taha. Gradual Typing for Functional Languages. In Scheme and Functional \nProgramming Workshop, 2006. [18] J. G. Siek and M. Vachharajani. Gradual Typing with Uni.cation-Based \nInference. In DLS, pages 1 12. ACM, 2008. [19] J. G. Siek and P. Wadler. Threesomes, With and Without \nblame. In POPL, pages 365 376. ACM, 2010. [20] SunSpider Benchmarks, 2010. http://www.webkit.org/perf/ \nsunspider/sunspider.html. [21] P. Thiemann. Towards a Type System for Analyzing JavaScript Pro\u00adgrams. \nIn ESOP, 2005. [22] S. Tobin-Hochstadt and M. Felleisen. The Design and Implementation of Typed Scheme. \nIn POPL, 2008. [23] V8 Benchmarks, 2011. http://code.google.com/apis/v8/ benchmarks.html. [24] P. Wadler \nand R. B. Findler. Well-Typed Programs Can t Be Blamed. In ESOP, pages 1 16. Springer-Verlag, 2009. [25] \nA. K. Wright and R. Cartwright. A Practical Soft Type System for Scheme. ACM TOPLAS, 19(1), 1997. \u00a8 Integrating \nTyped and Untyped Code in a Scripting Language. In POPL, pages 377 388. ACM, 2010. [26] T. Wrigstad, \nF. Z. Nardelli, S. Lebresne, J. Ostlund, and J. Vitek.  \n\t\t\t", "proc_id": "2103656", "abstract": "<p>Gradual typing lets programmers evolve their dynamically typed programs by gradually adding explicit type annotations, which confer benefits like improved performance and fewer run-time failures.</p> <p>However, we argue that such evolution often requires a giant leap, and that type inference can offer a crucial missing step. If omitted type annotations are interpreted as unknown types, rather than the dynamic type, then static types can often be inferred, thereby removing unnecessary assumptions of the dynamic type. The remaining assumptions of the dynamic type may then be removed by either reasoning outside the static type system, or restructuring the code.</p> <p>We present a type inference algorithm that can improve the performance of existing gradually typed programs without introducing any new run-time failures. To account for dynamic typing, types that flow in to an unknown type are treated in a fundamentally different manner than types that flow out. Furthermore, in the interests of backward-compatibility, an escape analysis is conducted to decide which types are safe to infer. We have implemented our algorithm for ActionScript, and evaluated it on the SunSpider and V8 benchmark suites. We demonstrate that our algorithm can improve the performance of unannotated programs as well as recover most of the type annotations in annotated programs.</p>", "authors": [{"name": "Aseem Rastogi", "author_profile_id": "81496682403", "affiliation": "Stony Brook University, Stony Brook, NY, USA", "person_id": "P2991287", "email_address": "arastogi@cs.stonybrook.edu", "orcid_id": ""}, {"name": "Avik Chaudhuri", "author_profile_id": "81309507612", "affiliation": "Adobe Systems, San Jose, CA, USA", "person_id": "P2991288", "email_address": "achaudhu@adobe.com", "orcid_id": ""}, {"name": "Basil Hosmer", "author_profile_id": "81496688458", "affiliation": "Adobe Systems, Waltham, MA, USA", "person_id": "P2991289", "email_address": "bhosmer@adobe.com", "orcid_id": ""}], "doi_number": "10.1145/2103656.2103714", "year": "2012", "article_id": "2103714", "conference": "POPL", "title": "The ins and outs of gradual type inference", "url": "http://dl.acm.org/citation.cfm?id=2103714"}