{"article_publication_date": "01-25-2012", "fulltext": "\n Constraints as Control Ali Sinan Koksal\u00a8Viktor Kuncak Philippe Suter * School of Computer and Communication \nSciences (I&#38;C) -Swiss Federal Institute of Technology (EPFL), Switzerland {.rstname.lastname}@ep..ch \nAbstract We present an extension of Scala that supports constraint program\u00adming over bounded and unbounded \ndomains. The resulting lan\u00adguage, Kaplan, provides the bene.ts of constraint programming while preserving \nthe existing features of Scala. Kaplan integrates constraint and imperative programming by using constraints \nas an advanced control structure; the developers use the monadic for construct to iterate over the solutions \nof constraints or branch on the existence of a solution. The constructs we introduce have simple semantics \nthat can be understood as explicit enumeration of values, but are implemented more ef.ciently using symbolic \nreasoning. Kaplan programs can manipulate constraints at run-time, with the combined bene.ts of type-safe \nsyntax trees and .rst-class func\u00adtions. The language of constraints is a functional subset of Scala, \nsupporting arbitrary recursive function de.nitions over algebraic data types, sets, maps, and integers. \nOur implementation runs on a platform combining a constraint solver with a standard virtual machine. \nFor constraint solving we use an algorithm that handles recursive function de.nitions through fair function \nunrolling and builds upon the state-of-the art SMT solver Z3. We evaluate Kaplan on examples ranging \nfrom enumer\u00adation of data structures to execution of declarative speci.cations. We found Kaplan promising \nbecause it is expressive, supporting a range of problem domains, while enabling full-speed execution \nof programs that do not rely on constraint programming. Categories and Subject Descriptors D.3.3 [Programming \nLan\u00adguages]: Language Constructs and Features General Terms Design, Languages Keywords Constraint Programming, \nSatis.ability Modulo Theo\u00adries, Executable Speci.cations, Scala, Embedded Domain-Speci.c Languages, Non-determinism \n1. Introduction Modern mainstream programming languages incorporate advances in memory safety, type systems, \nmeta-programming and modular\u00adity. However, the sequential control in widely used systems remains largely \nunchanged compared to some of the earliest imperative and * Philippe Suter was supported by the Swiss \nNational Science Foundation Grant 200021 120433. Permission to make digital or hard copies of all or \npart of this work for personal or classroom use is granted without fee provided that copies are not made \nor distributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. POPL 12, January 25 27, 2012, Philadelphia, PA, USA. Copyright \nc &#38;#169; 2012 ACM 978-1-4503-1083-3/12/01. . . $10.00 functional language designs. Although this \nsimplicity has advan\u00adtages in terms of predictable compilation, it prevents languages from approaching \nthe abstraction level used in software design. In design we often encounter a conjunction of multiple \northogo\u00adnal requirements that we need to meet simultaneously [29]. When conjoining multiple requirements, \neach of them needs to be a par\u00adtial speci.cation of possible executions, otherwise the conjunc\u00adtion would \nbe contradictory. The desire to use partial constraints leads to non-deterministic speci.cation constructs. \nExamples in\u00adclude guarded commands [18], wide-spectrum languages [6, 43], intermediate veri.cation languages \n[15], Temporal Logic of Ac\u00adtions [36], Alloy [30], as well as models in proof assistants Coq [62] and \nIsabelle [52]. In contrast to speci.cation languages, most mainstream executable languages have dif.culty \nin implementing a conjunction of programs; their sequential commands tightly spec\u00adify the set of possible \nbehaviors. The idea of Logic Programming [14, 34] is to allow programs that are non-deterministic and \nspecify relations (predicates) be\u00adtween values. The original inspiration came from a restricted form \nof resolution as the execution mechanisms, which uses uni.cation of logical variables ranging over .nite \ntrees. Functional logic pro\u00adgramming extends logic programming with the narrowing tech\u00adnique and with \nbene.ts of functional programming [3]. Constraint Logic Programming, CLP [31] and later generations of \nProlog [14] extend logic programming with the ability to perform constraint solving not only over trees \nbut also over numerical domains. These paradigms hold great promise to raise the abstraction level of \nsoft\u00adware. They are related to program synthesis research [26, 35, 39, 53, 58], which can be viewed as \na compilation mechanism for declarative speci.cations. We believe that key obstacles preventing broader \nuse of non-de\u00adterministic declarative programming constructs include: the dif.culty of solving declarative \nconstraints and  the dif.culty of incorporating these constructs into existing languages and platforms. \n We next brie.y outline the approach that our system uses to meet these challenges. Ef.cient interoperability \nwith existing platforms. In this paper we present Kaplan, a system that supports Constraint Programming \non top of the Scala language [50]. We address the dif.culty of in\u00adcorporating into existing platforms \nby choosing not to modify the semantics of the core Scala language, but instead use the .exibil\u00adity of \nfor-comprehensions in Scala. Because for-comprehensions present a syntax for monads in Scala, this aspect \nof our approach is related to using non-determinism monads in Haskell [21]. An im\u00adportant difference \nis that our starting language is not purely func\u00adtional, but can have side effects. Our solution is therefore \nto identify a functional Turing-complete sublanguage for constraints, and use it locally for declarative \npro\u00adgramming within the full Scala language (which has features of object-oriented, imperative and functional \nlanguages). The declar\u00adative control that we explore is based on describing iterations as a search process. \nThe iteration ranges are speci.ed as the set of so\u00adlutions to constraints. Our constraints are declared \nusing .rst-class functions and familiar combinators, such as &#38;&#38;. From a language point of view \nthe integration is appealing because users hardly need to learn any new notation.  One approach to implementation \nof non-deterministic lan\u00adguages is to implement a virtual machine that supports backtrack\u00ading, such as \nWarren s Abstract Machine for Prolog [1] or the Java Path.nder model checker and its extensions [24]. \nUnfortunately, such an approach is costly in terms of both performance and en\u00adgineering effort. We propose \ninstead to encapsulate any need for backtracking into a for-comprehension that iterates over solution \nspaces. In addition, we allow constraint programming in code out\u00adside of loops. In such cases, logical \nvariables and a constraint store help ensure that a program can .nd a solution without backtracking over \nthe host imperative program, keeping backtracking within a specialized constraint solver. Ef.cient solving \nof declarative constraints. The .rst imple\u00admentations of declarative paradigms predate algorithmic advances \nof modern SAT solvers [57, 66]. The advances in SAT led to a paradigm shift in solving combinatorial \nproblems, where it became often pro.table to outsource constraint solving tasks to dedicated implementations, \ninstead of relying on a programmer to hard-code a search strategy. Fueled by the developments of SAT \nsolvers, a more expressive technology emerged as the .eld of Satis.ability Modulo Theories (SMT), with \na number of ef.cient implementations available today [8, 19, 44]. SMT techniques combine SAT solvers \nwith decision procedures and their combination methods [17, 22, 46] that were and remain motivated by \nprogram veri.cation tasks [32, 45]. Our aim is to leverage the remarkable progress in dedicated constraint \nsolvers by deploying an SMT solver as a part of the run-time of a programming language. Whereas the developers \nof Prolog were in.uenced by the state-of-the-art theorem proving technology of their time (resolution \nfor .rst-order logic), we aim to explore the potential of SMT. The fact that SMT solvers were developed \nto model programming language constructs makes them a particularly appropriate choice. However, the original \nmotivation for SMT solvers was program veri.cation, whereas we aim to use them as an execution mechanism \nfor declarative constraints. Many logical theories are natively supported by modern solvers, including \nalgebraic data types, uninterpreted functions, linear arith\u00admetic, and arrays. Nonetheless, we believe \nthat constraint solving can reach its full potential only if users can de.ne their own classes of constraints. \nWe therefore choose to work with a solver for a rich logic in which users can de.ne their own recursive \nfunctions and recursive data types [60]. We thus trade performance and decidabil\u00adity for greater expressive \npower and .exibility. Contributions. This paper makes a concrete and implemented pro\u00adposal for incorporating \nconstraint programming into an underlying stateful language through several individual contributions: \n .rst-class constraints, which can be generated at run-time and which carry type and free-variable information; \nthe constraints can use unrestricted operations on booleans (including nega\u00adtion), as well as built-in \nand user-de.ned recursive operations on integers, sets, maps, and trees;  a programming model for constraint \nprogramming based on creating and iterating over a stream of solutions of a constraint using explicit \ncontrol constructs; our programming model (un\u00adlike most solutions with backtrackable virtual machines) \nhas no penalty for the code that does not use constraint solving;  logical variables of numeric and \nsymbolic types that postpone constraint solving steps, often substantially reducing the size of the search \nspace;  the use of fair function unrolling and SMT solving technology to provide expressive power, predictability, \nand ef.ciency of solving in many domains of interest;  implementation of the system (Kaplan is publicly \navailable from http://lara.epfl.ch);  evaluation of the system on examples from several domains, such \nas executing declaratively speci.ed data structure opera\u00adtions, software testing, and counterexample-driven \nconstruction of functions of a given template and a given speci.cation.  Paper outline. The rest of \nthis paper is organized as follows. The next section presents features of Kaplan through an extensive \nset of examples. Section 3 presents in more detail the Turing-complete language that we use to describe \nconstraints. Section 4 gives the semantics of key Kaplan constructs. Section 5 outlines main im\u00adplementation \naspects of Kaplan. We present further evaluation and illustrate use cases of Kaplan in Section 6. We \n.nally review the remaining related work and conclude. 2. Examples and Features We present some of the \nfeatures of Kaplan through examples. The simpler examples can be tried out directly in a Scala shell, \nprovided that it is launched with the Kaplan plugin. Throughout the paper, we assume basic familiarity \nwith Scala, but explain more advanced concepts and constructs as needed. 2.1 First-class Constraints \nval c1: Constraint2[Int,Int] = ((x: Int, y: Int) . 2*x+3*y == 10 &#38;&#38; x = 0 &#38;&#38; y = 0) This \n.rst command declares a constraint with two free variables. Note that the representation of the constraint \nis a lambda term. The only difference between a declaration of a constraint and an anonymous function \nis the type of the expression. As an alternative to explicitly declaring the value to be of a constraint \ntype, one can also append to the function literal a method call .c, so the following declaration is identical \nto the previous one: val c1 = ((x: Int, y: Int) . 2*x+3*y == 10 &#38;&#38; x = 0 &#38;&#38; y = 0).c \nThe type of c1 is in this case determined by type inference, and this second way is generally shorter. \nIn Kaplan, constraints are in fact extensions (in the object-oriented sense) of functions, and can thus \nbe evaluated, given some values for their argument variables: scala> c1(2,1) result: false scala> c1(5,0) \nresult: true As is common in lambda calculus, the names given to the bound variables play no role; the \nconstraint ((x: Int) . x = 0) is equivalent to the constraint ((y: Int) . y = 0). One should think of \nconstraints being de.ned over (typed) De Bruijn indices rather than named variables. Constraints can \nbe queried for a single solution or for a stream of solutions by calling appropriate methods: scala> \nc1.solve result: (5,0) scala> c1..ndAll result: non-empty iterator  The solve method computes a single \nsolution to the constraint, while .ndAll returns an iterator over all solutions. The iterator com\u00adputes \nand returns solutions on demand. (As a result of displaying the iterator in the console, a search for \nthe .rst solution is trig\u00adgered.) When the set of solutions is .nite, one can compute it for instance \nas follows: scala> c1..ndAll.toList result: List((5,0),(2,2)) The most general way to iterate over solutions \nis using a for\u00adcomprehension: for(s . c1..ndAll) {println(s) } Constraints are .rst-class members of \nKaplan, and can be cre\u00adated and manipulated as such. The following function, for instance, generates \nconstraints describing an integer within bounds; if the second argument is true, the bounds are given \nby [-m; m], else by [0; m]. def bounded(m: Int, neg: Boolean = true) = { val basis: Constraint1[Int] \n= ((x: Int) . x = m) basis &#38;&#38; (if(neg) ((x: Int) . x =-m) else ((x: Int) . x = 0)) } In this \nexample, &#38;&#38; is a call to a method de.ned as part of constraint classes, and which expects as \nargument a constraint of the same arity and with the same types of parameters as the receiver. This ensures \nthat constraints can only be combined when the number and the types of their De Bruijn indices are compatible. \nNote that, thanks to type inference, we need not explicitly state that the two anonymous functions represent \nconstraints. We can now use the function bounded to produce constraints: scala> bounded(3, true)..ndAll.toList \nresult: List(0, -1, -2, -3, 1, 2, 3) scala> bounded(3, false)..ndAll.toList result: List(0, 1, 2, 3) \nAnother convenient constraint combinator is the product method, which combines two constraints into a \nnew one whose solution set is the Cartesian product of the original two: scala> (bounded(1, true) product \nbounded(1,false))..ndAll.toList result: List((-1,0),(0,0),(1,1),(-1,1),(1,0),(0,1)) Constructing constraints \nusing combinators is a very concise way to solve general problems. For instance, consider the problem \nof solving a CNF SAT instance de.ned in a way similar to the standard DIMACS format, where the input \nval p1 = Seq(Seq(1,-2,-3), Seq(2,3,4), Seq(-1,-4)) represents the problem (x1 .\u00acx2 .\u00acx3) . (x2 . x3 . \nx4) . (\u00acx1 .\u00acx4) The following function de.nes a solver for such problems: def satSolve(problem : Seq[Seq[Int]]) \n: Option[Map[Int,Boolean]] = problem.map(l . l.map(i .{ val id = scala.math.abs(i) val isPos = i > 0 \n((m : Map[Int,Boolean]) . m(id) == isPos).c }).reduceLeft( || )).reduceLeft( &#38;&#38; )..nd } Note \nthat in this case we used the .nd method on the .nal constraint rather than solve. They provide the same \nfunctionality, but .nd returns its result in an Option[ ] type, where None corresponds to an unsolvable \nconstraint, while solve throws an exception if the constraint has no solution. Another observation is \nthat we use here a constraint over a single variable of Map type to encode a constraint over an unknown \nnumber of boolean variables. We found this to be a convenient pattern when the more rigid syntax of anonymous \nfunctions with explicit variable naming does not apply. Because all variables are of the same type, the \napproach works in our type-safe framework. We can now solve SAT problems: scala> satSolve(p1) result: \nSome(Map(2 . true, 3 . false, 1 . false, 4 . false))) scala> satSolve(Seq(Seq(1,2), Seq(-1), Seq(-2))) \nresult: None  2.2 Ordering Solutions As illustrated by some of the previous examples, the .ndAll method \ngenerates solutions in no particular order. This corresponds to the intuition that it enumerates the \nunordered set of solutions to a constraint. Similarly, the semantics of calls to solve or .nd are simply \nthat if a value is produced, then it is an element of that solution set. Two invocations of solve on \nthe same constraint may or may not result in the same solution. It is sometimes desirable, though, to \nenumerate solutions in a de.ned order. To this end, Kaplan constraints support two methods; minimizing \nand maximizing. These methods take as argument an objective function. Just like constraints, objective \nfunctions are rep\u00adresented using an anonymous function, in this case one that returns an integer. The \nminimizing and maximizing methods ensure that the De Bruijn indices types match those of the constraint. \nFrom the user s point of view, these functions are typed as IntTerms.1 Consider the knapsack problem: \ngiven a maximum weight and a set of items, each with an associated value and weight, .nd a subset of \nthe items for which the sum of values is maximal while the sum of weights is less or equal to the maximum. \nThe following code produces and solves an instance of the problem, where the solution is represented \nas a map from the item indices to booleans indicating which should be picked. def solveKnapsack(vals \n: List[Int], weights : List[Int], max : Int) = { def conditionalSumTerm(vs : List[Int]) = { vs.zipWithIndex.map(pair \n.{ val (v,i) = pair ((m : Map[Int,Boolean]) . (if(m(i)) v else 0)).i }).reduceLeft( + ) } val valueTerm \n= conditionalSumTerm(vals) val weightTerm = conditionalSumTerm(weights) val answer = ((x : Int) . x = \nmax).compose0(weightTerm) .maximizing(valueTerm) .solve } We brie.y explain the code. A solution to a \nknapsack instance is a map indicating a choice of which objects should be picked. The conditionalSumTerm \nfunction builds, from a list of integers, an integer term parameterized by a choice map and representing \na sum of values. The map de.nes whether each element in the list participates or not to the sum. We use \nthis function twice, to produce the two terms that, given a choice of items, encode the total value and \nthe weight respectively. Observe that we build the .nal constraint using a function composition: we start \nthe construction of the constraint as (x : Int) . x = max and compose it with the weight term to produce \nthe constraint that the weight should not exceed the maximum. Function composition is a general and type\u00adsafe \nway to build constraints from smaller terms, as demonstrated 1 The relationship from an implementation \npoint of view between con\u00adstraints and terms is explained in more details in Section 5.  in this example. \nWe can now .nd optimal choices for instances of the knapsack problem: scala> val vals : List[Int] = List(4, \n2, 2, 1, 10) scala> val weights : List[Int] = List(12, 1, 2, 1, 4) scala> val max : Int = 15 scala> solveKnapsack(vals, \nweights, max) result: Map(0 . false, 1 . true, 2 . true, 3 . true, 4 . true)  2.3 User-de.ned Functions \nand Datatypes An important feature of Kaplan is the ability to perform constraint solving in the presence \nof user-de.ned functions and data types. Consider the following functions which compute, for a 2\u00d72 matrix \n[ ab cd ] represented as a tuple(a, b, c, d), its determinant and whether it is unimodular, respectively: \n@spec def det(a: Int, b: Int, c: Int, d: Int): Int = a*d - b*c @spec def unimodular(a: Int, b: Int, c: \nInt, d: Int): Boolean = {val dt = det(a, b, c, d) dt == 1 || dt == -1 } The @spec annotation indicates \nthat the user wishes to use the func\u00adtions as part of constraints, and the Kaplan compiler enforces that \nsuch functions are written in the subset of Scala supported within constraints (see Section 3). We can \nnow characterize unimodular matrices with small elements as def boundedUnimodular(m: Int) = {val b = \nbounded(m, false) (b product b product b product b) &#38;&#38; (unimodular ) } (The underscore after \nunimodular indicates to the Scala compiler that it should apply an .-conversion to produce an anonymous \nfunction, and ultimately a constraint, from the de.nition.) We can use these new de.nitions to generate \nunimodular matrices: scala> boundedUnimodular(2)..ndAll.take(4).toList result: List((0,1,1,0), (0,1,1,2), \n(0,1,1,1), (1,1,2,1)) Perhaps more interestingly, the @spec functions can be mutu\u00adally recursive. As \nan example, consider the following declarative de.nition of prime numbers: @spec def noneDivides(from \n: Int, j : Int) : Boolean = { from == j || (j % from != 0 &#38;&#38; noneDivides(from+1, j)) }@spec def \nisPrime(i : Int) : Boolean = (i = 2 &#38;&#38; noneDivides(2, i)) val primes = ((isPrime( :Int)) minimizing \n((x:Int) . x))..ndAll which we can subsequently enumerate: scala> primes.take(10).toList result: List(2, \n3, 5, 7, 11, 13, 17, 19, 23, 29) Kaplan users can also de.ne their own recursive data types (also known \nas algebraic data types). The following code declares two such types for red-black trees: @spec sealed \nabstract class Color @spec case class Black() extends Color @spec case class Red() extends Color @spec \nsealed abstract class Tree @spec case class Node(c : Color, l : Tree, v : Int, r : Tree) extends Tree \n@spec case class Leaf() extends Tree Algebraic data types are best manipulated using recursive functions \nand pattern-matching. Kaplan supports such function de.nitions: @spec def size(tree : Tree) : Int = (tree \nmatch {case Leaf() . 0 case Node( ,l, ,r) . 1 + size(l) + size(r) }) ensuring(result . result = 0) This \nfunction also illustrates the use of post-conditions, written in Scala as anonymous functions in an ensuring \nclause [49]. In this case, the post-condition states that the result of size can never be negative. Such \nannotations can help the constraint solver discard parts of the search space. Kaplan uses the Leon veri.cation \nsystem [60] to prove, at compile time, that the annotations are valid, so that it can then rely on them \nat run-time for constraint solving. We can similarly de.ne recursive functions that compute whether a \nred\u00adblack tree contains sorted keys or has the right coloring properties. In the interest of space, we \nomit the complete de.nitions. @spec def orderedKeys(t : Tree) : Boolean = ... @spec def validColoring(t \n: Tree) : Boolean = ... @spec def validTree(t : Tree) = orderedKeys(t) &#38;&#38; validColoring(t) @spec \ndef valsWithin(t : Tree, min : Int, max : Int) : Boolean = ... We expect that what these functions compute \nis clear from their name. With them, we can for instance count the number of red\u00adblack trees with a given \nnumber of elements: scala> (for(i . (0 to 7)) yield ((t : Tree) . validTree(t) &#38;&#38; valsWithin(t, \n0, i) &#38;&#38; size(t) == i )..ndAll.size).toList result: List(1, 1, 2, 2, 4, 8, 16, 33) More advanced \napplications of data structure enumeration are pre\u00adsented in Section 6, including testcase generation \nfor sorted list and tree manipulations.  2.4 Timeouts The language of constraints supported in Kaplan \nis very expressive. For that reason, one cannot expect that any constraint will be solvable. As an example, \ngiven the de.nition (valid in Kaplan) @spec def pow(x : Int, y : Int) : Int = if(y == 0) 1 else x * pow(x, \ny - 1) it would be unreasonable to expect the system to .nd a solution to the constraint val fermat = \n((x : Int, y : Int, z : Int, b : Int) . b > 2 &#38;&#38; pow(x,b) + pow(y,b) == pow(z,b)).c The search \nalgorithm at the core of Kaplan is a semi-decision procedure: if a solution exists, then that solution \nwill be found eventually. If there are no solutions, the procedure can discover this fact and stop [59, \n60], or loop forever. The solve, .nd and .ndAll methods all take a parameter describing the timeout strategy. \nThat parameter is optional, so by default no timeout is used. Because it is also implicit, a timeout \nstrategy can be de.ned for an entire scope by a single de.nition that the Scala compiler then automatically \ninserts at each call site: implicit val timeoutStrategy = Timeout(1.0) Given the above declarations, \nthe following attempt to con.rm Fermat s last theorem results in an exception after 1 second: scala> \nfermat..nd result: TimeoutReachedException: No solution after 1.0 second(s) at .solve(<console>) at .<init>(<console>) \n...  2.5 Logical Variables All the examples so far have illustrated eager solution enumeration, where \nsolving constraints immediately produces concrete values. While this by itself is a convenient facility, \nmuch of the power of constraint programming in general and of Kaplan in our case comes from the ability \nto produce logical variables, which represent the promise of a solution. Logical variables in turn can \nbe used to control the execution .ow of the program in novel ways. We start by describing some of their \nbasic properties. More advanced examples of programming with logical variables follow.  In Kaplan, logical \nvariables are always produced as the result of lazily solving a constraint. This is done by calling lazySolve, \nlazyFind or lazyFindAll instead of solve, .nd or .ndAll respec\u00adtively. Consider the constraint we de.ned \nearlier: val c1 = ((x: Int, y: Int) . 2*x+3*y == 10 &#38;&#38; x = 0 &#38;&#38; y = 0).c We produce logical \nvariables representing a solution as follows: scala> val (x,y) = c1.lazySolve; println((x,y)) result: \n(L(?),L(?)) Notice that the result is not a pair of integers, as in the case of solve, but a pair of \nobjects of type L[Int] representing the promise of integers. The question mark indicates that the value \nhas not yet been .xed. Logical variables in Kaplan have singular semantics [54], meaning that a given \nlogical variable will always represent the same concrete value, even when it is copied or passed as an \nargu\u00adment to a function. The identity of a logical variable is determined by the identity of the instance \nof the L[ ] class. The value of a logi\u00adcal variable is .xed as soon as it is queried, which can be done \nwith the .value method: scala> x.value result: 5 scala> println((x,y)) result: (L(5),L(?)) (Note that \neven though the y can at this point only hold the value 0, the solver has not necessarily detected that \nthe solution is unique and thus still considers the value not to be .xed.) Fixing a value needs not always \nbe done explicitly. The Kaplan library de.nes two implicit conversions between concrete and logical values: \nimplicit def concretize[T](l : L[T]) : T = l.value implicit def lift[T](v : T) : L[T] = new FixedL[T](v) \nThe .rst one simply .xes the value whenever a function expects a concrete value and receives a logical \nvariable. The second one con\u00adverts a concrete value into a specialized representation of a logical variable. \nIt is used to provide a common representation for concrete and logical values: because Kaplan is built \nas a thin layer over Scala and most code must execute as usual, we cannot afford to treat ev\u00adery value \nas logical. Instead, users decide which function can han\u00addle logical values by using L[ ] types in their \nsignature. Because concrete values can be lifted to logical status, such functions work indifferently \nwith standard and logical variables. Logical variables created through this lifting mechanism are treated \nspecially and do not add any complexity to the solving process.  2.6 Imperative Constraint Programming \nScala being a multi-paradigm language, users can alternate be\u00adtween different programming styles depending \non their preferences and on the task at hand. The same is true for Kaplan; while ea\u00adger solution enumeration \nis best used within functional style for\u00adcomprehensions, logical variables can be used rather naturally \nin an imperative style, thanks to novel control constructs. The Kaplan library de.nes the assuming-otherwise \nbranching construct. It is similar in nature to if-then-else, except for an important difference: rather \nthan strictly evaluating the branching condition, assuming\u00adotherwise blocks check whether the condition \nis feasible and, if so, constrain the logical variables in the environment so that their val\u00adues satisfy \nthe branching constraint. If the condition contains no logical variable (or only logical variables that \nhave already been .xed), then assuming-otherwise behaves exactly like if-then-else. As an example, consider \nthe classical puzzle that consists in .nding distinct values for the letters representing digits in the \nfollowing addition such that the sum is valid: s end + mor e = m o ney We now present a solution in Kaplan \nwritten in an imperative style. val anyInt = ((x : Int) . true).c val letters @ Seq(s,e,n,d,m,o,r,y) \n= Seq..ll(8)(anyInt.lazySolve) This second line uses pattern-matching syntax to achieve two things at \nonce: 1) the eight letter variables are bound to eight independent (lazy) representations of a solution \nto the trivial anyInt constraint and 2) the variable letters is bound to the sequence of all letter variables. \nAt this point, we have 8 unconstrained integer logical variables. We can imperatively add constraints: \nfor(l . letters) asserting(l = 0 &#38;&#38; l = 9) If this loop terminates without any error, then at \nthe end of it, the 8 variables each represent a number between 0 and 9. (Calling as\u00adserting(cond) is \nequivalent to assuming(cond) {} otherwise {error }, just like assert(cond) is (conceptually at least) \nequiv\u00adalent to if(cond) {} else { error }. More details are given in Section 5.1.) We further constrain \nthe letters representing most\u00adsigni.cant digits: asserting(s > 0 &#38;&#38; m > 0) We can now perform \nsymbolic arithmetic using the existing and new logical variables. We de.ne a new variable for the sum \nof each line and constrain it to the expected value: val fstLine = anyInt.lazySolve asserting(fstLine \n== 1000*s + 100*e + 10*n + d) val sndLine = anyInt.lazySolve asserting(sndLine == 1000*m + 100*o + 10*r \n+ e) val total = anyInt.lazySolve asserting(total == 10000*m + 1000*o + 100*n + 10*e + y) At this point, \nthe value of the letters is still not .xed, however Kaplan ensures that all variables admit solutions \nsatisfying the asserted constraints. Finally, we check for a solution to the puzzle using one last assuming-otherwise \nblock: scala> assuming( distinct(s,e,n,d,m,o,r,y) &#38;&#38; fstLine + sndLine == total) {println( Solution: \n + letters.map( .value)) } otherwise { println( The puzzle has no solution. ) } result: Solution: List(9, \n5, 6, 7, 1, 0, 8, 2) We mentioned that assuming-otherwise is conceptually close to if\u00adthen-else, and \nin fact equivalent in the absence of logical variables. One important difference is that the construct \nis asymmetrical; while if(cond) thenExpr else elseExpr is equivalent to if(!cond) elseExpr else thenExpr \nthe same transformation cannot be applied to assuming-otherwise blocks; indeed, the semantics are that \nthe control will attempt to satisfy the branching condition, so whether the positive or negative condition \nis tested has an impact on the rest of the execution. Section 5.1 discusses the implementation of assuming-otherwise \nblocks in terms of lazyFind.  Int . T Boolean . T T . T Set[T ] . T T1 . T T2 . T T1 . T T2 . T Map[T1,T2] \n.T T1 . T2 .T sealed abstract class C C .T case class C(n : D) extends ED .T E .T C .T Figure 1. Inductive \nde.nition of PureScala types, where Set and Map are the types de.ned in the package scala.collection.immutable. \n3. Constraint Sublanguage In this section, we describe the subset of Scala in which we can specify constraints \nin Kaplan, and which we call PureScala. PureScala is an extension of the language supported by the pub\u00adlicly \navailable Leon veri.cation system [60]. Since it is a subset of Scala, the language is executable and \ndeterministic. 3.1 Language Data types. Figure 1 presents an inductive de.nition of the data types supported \nin PureScala. An important feature is the ability to de.ne (recursive) algebraic data types. In Scala, \nthese types are de.ned using a hierarchy of special case classes. Algebraic data types are typically \nmanipulated with pattern-matching, as shown in some of the examples in Section 2. A current limitation \nis that these user-de.ned types cannot take type parameters. The Map type represents .nite, immutable \nmaps, which can thus be viewed as partial functions. Because it supports unbounded data types and arbitrary \nrecursive functions, the constraint language is itself Turing-complete. This can be viewed both as an \nadvantage and an inconvenience: on one hand, this expressive power guarantees that just about any constraint \nis expressible, but on the other hand, standard incompleteness theorems predict that some constraints \ncannot be shown to have no solutions. In practice, we have found that constraints that come up in programming \ntasks such as the ones presented in this paper and in the context of functional software veri.cation \n[60] (as opposed to theoretically hard ones) are handled well. Expressions and function de.nitions. PureScala \nexpressions can contain all standard arithmetic operators, map applications and up\u00addates, set operators \nand membership tests, function applications (of user-de.ned or anonymous functions), and constructor \nand selec\u00adtors from user-de.ned data types. Expression can also contain vals to factor out common subexpressions. \nIndeed, a Scala block { val x1 = e2 ... val xn = en e } is to be understood as let x1 = e2 in ... in \nlet xn = en in e. Expressions can contain pattern-matching on user-de.ned data types. Any sub-pattern \ncan be bound to a variable and used accord\u00adingly on the right-hand side of patterns. Furthermore, patterns \ncan contain arbitrary guards. At compile-time, Kaplan relies on Leon to prove that pattern-matching expressions \nare exhaustive, and thus rules out any possibility of a run-time match error in @spec func\u00adtions. This \nexhaustiveness check goes beyond the capabilities of the standard Scala compiler, in that it takes into \naccount the path conditions leading to the match expression. A PureScala function body is de.ned by a \nsingle expression whose free variables are the arguments of the function. Func\u00adtions can optionally be \nannotated with a post-condition, which can in some cases help the run-time constraint solver. These post-conditions \nare proved at compile-time, just like the pattern\u00admatching expressions. Kaplan can thus be used purely \nas a veri.\u00adcation system, and therefore strictly subsumes Leon.  3.2 Solver Kaplan invokes Leon s core \nsolving procedure both at compile\u00adtime, to validate post-conditions and prove that pattern-matching expressions \nare exhaustive, and at run-time, to .nd solutions to con\u00adstraints. The procedure is based on a re.nement \nloop that expands function de.nitions in a fair way, to guarantee that no valid solution is ever ignored. \nOne can think of this search procedure as a form of bounded model-checking for functional programs. We \nhave found that in practice it is fast in .nding counter-examples. The details of the procedure are presented \nin [60]. Conceptually, Leon is the only solver used in the implementa\u00adtion of Kaplan. Because it is built \nas a layer on top of Z3, though, we will sometimes refer to Z3 directly when we discuss implemen\u00adtation \ndetails in Section 5. In particular, in the absence of user\u00adde.ned recursive functions, Leon behaves \nexactly as Z3. 4. Semantics In this section, we present an overview of the semantic aspects of Kaplan \nthrough operational semantic rules, shown in Figure 2. All features of Kaplan can be implemented in terms \nof the two con\u00adstructs .nd and lazyFind, as well as conversions between concrete values and logical variables \n(denoted by l). Section 5 describes how to use the host language (Scala) to reduce other constructs (includ\u00ading \nsolve, .ndAll, lazyFindAll) to this core. A state consists of a triple expr|(\u00b5, .), where expr is the \nex\u00adpression under evaluation, \u00b5 encodes the part of the state that is proper to Scala, and . is a constraint \nstore. A constraint store is conceptually a formula whose free variables correspond to all log\u00adical variables \nused since the beginning of the computation. The HOST rule captures the intuition that in the absence \nof invo\u00adcations to the constraint solver, Kaplan behaves exactly like Scala: we assume the existence \nof a transition relation .H describing the execution of normal Scala code, and which is lifted to Kaplan \nthrough the HOST rule. As one would expect, applying such transi\u00adtions leaves the constraint store unchanged. \nThe rules S-SAT and S-UNSAT describe the possible results of an invocation of .nd, the simplest form \nof constraint solving. We use (.x.f(x)) to denote a constraint that ranges over the variables x and that \ndoes not refer to logical variables. By M we denote a map from variables to constants. The condition \nM|= f denotes that M is a valid model of f, i.e., a mapping of variables x of f to constants such that \nf[x .M(x)] holds. Examining the rules for .nd, S-SAT and S-UNSAT, we note that .nd has no impact and \nno dependency on the constraint store, which is consistent with its eager semantics. The rules L-SAT \nand L-UNSAT for lazyFind are analogous, but with crucial differences: 1) logical variables can be present \nin the constraints to solve, which we therefore denote (.x.f(x, l)), and 2) the L-SAT rule does not produce \nconcrete values, but rather fresh logical variables. The LIFT rule applies whenever a constant value \nneeds to be used in place of a logical variable. Conceptually, it adds to the constraint store a new \nlogical variable whose value is immediately constrained to be the constant. Finally, the VALUE rule speci.es \nhow concrete values can be extracted from a satisfying assignment  t1 | \u00b51 .H t2 | \u00b52 HOST t1 |(\u00b51,.). \nt2 |(\u00b52,.) M|= f \u00ac.M . M|= f S-SAT S-UNSAT (.x.f(x))..nd |(\u00b5, .). Some(M(x)) |(\u00b5, .) (.x.f(x))..nd |(\u00b5, \n.). None |(\u00b5, .) M|= . . f[x . lf ] lf fresh in . L-SAT (.x.f(x, l)).lazyFind |(\u00b5, .). Some(lf ) |(\u00b5, \n. . f[x . lf ]) \u00ac.M . M|= . . f[x . lf ] lf fresh in . L-UNSAT (.x.f(x, l)).lazyFind |(\u00b5, .). None |(\u00b5, \n.) M|= .c is a constant lf fresh in . VALUE LIFT l.value |(\u00b5, .).M(l) |(\u00b5, . . (l = M(l))) c.lift |(\u00b5, \n.). lf |(\u00b5, . . (lf = c)) Figure 2. Small-step semantics of Kaplan-speci.c constructs. (model) for the \nconstraint store. Observe that when the rule ap\u00adplies, the value of the logical variable is then .xed \nfor the rest of the program execution by the added equality to .. This ensures sin\u00adgular semantics [54]: \nin any execution trace, all applications of the VALUE rule for a given logical variable l will produce \nthe same value; a different value would contradict the premise that M is a valid model for the store. \nWe now show that execution never gets stuck in one of the Kaplan-speci.c rules. Because we are examining \nthe behavior of our constructs taking a constraint solver as a parameter, we delib\u00aderately ignore termination \nproperties of the constraint solver and assume that the outcome of the constraint solver call, denoted \n|=, becomes immediately available to test the applicability of a rule. Theorem. Suppose the constraint \nstore is satis.able in the initial state and that the .H relation is total. Then the transition relation \ngiven by Figure 2 is also total. Proof. The case of HOST is clear from the hypothesis that .H is total. \nFor evaluations of .nd, we observe that the rules S-SAT and S-UNSAT have complementary premises, and \ntherefore one of them necessarily applies. Similarly for lazyFind, L-SAT and L-UNSAT are complementary, \nregardless of the value of .. LIFT has no precondition, so it remains to show that VALUE cannot get stuck, \ni.e. that it cannot be the case that there is no model M for the constraint store .. Using the assumption \nthat in the initial state the constraint store is satis.able, it is suf.cient to show that no transition \nwill make it unsatis.able. Observe that only three rules affect the con\u00adstraintstore; VALUE, LIFT,and \nL-SAT.Theadditionoftheequality (lf = c) in LIFT clearly does not affect satis.ability, because lf is \nfresh. Similarly, the addition of (l = M(l)) in VALUE preserves the model M by de.nition. Finally, L-SAT \nis guarded by a satis.\u00adability check on precisely the next state of the store, so the model obtained \nin the premise is a valid model for the next state. The proof suggests an implementation strategy: preserve \nat all times, alongside the constraint store, a satisfying assignment. When L-SAT is applied, cache the \nmodel M obtained from the satis.a\u00adbility check. When LIFT applies, augment the cached model with the \nappropriate value for lf . Finally, to apply VALUE, use M from the cache. It is not hard to see that \nthis strategy is a valid re.nement of the presented rules, and it is, in fact, the strategy we have implemented \nin Kaplan, as explained in the next section. 5. Implementation We implemented our extension to Scala \nas a combination of a run\u00adtime library and a compiler plugin, both implemented in Scala. In this section, \nwe present the implementation aspect of these parts, along with the interaction with the underlying SMT \nsolver Z3. 5.1 Run-Time Library First-class constraints. In Kaplan, .rst-class constraints are imple\u00admented \nas a hierarchy of Term classes, as shown in Figure 3. The base Term class represents a lambda expression \nand is parameter\u00adized by its argument types and return type. A constraint is simply a Term instance where \nthe return type is instantiated as boolean. We de.ne subclasses of Term for each arity, generating them \nautomat\u00adically, as it is the case for the tuple and function de.nitions in the Scala library. The base \nclass de.nes methods common to terms of all arities, such as the solve, .nd and .ndAll methods for query\u00ading \nconstraints. We ensure that these methods are only applicable when the return type is boolean by constraining \nit using an implicit parameter asBool. We use the same technique to guarantee that term instances are \ncombined in a fully type safe way. We use the c method to trigger an implicit conversion from lambda \nexpressions to Term instances. Each term subclass extends the corresponding function class in Scala, \nand uses the original Scala code for the lambda expression to de.ne function application. These classes \nde.ne optimization methods (minimizing and maximizing) to obtain optimization con\u00adstraints. The implementation \nof the optimization procedures is dis\u00adcussed later in this section. The creation of terms from Scala \nlambda expressions relies on compile-time transformations. The transformations are imple\u00admented in a \nplugin for the of.cial Scala compiler. We describe how the compile-time transformations work in Section \n5.2. Logical variables. In Kaplan, logical variables are instances of the L class, which is parameterized \nby the type of the symbolic value that it encapsulates. We de.ne LIterator classes that extend the Iterator \ntrait of Scala and that enumerate tuples of L values. We discuss the implementation of logical variables \nin Section 5.3. The assuming-otherwise construct. We can de.ne the assuming\u00adotherwise construct naturally \nat the library level in Kaplan: it boils down to checking the satis.ability of a constraint of arity \n0, and can therefore be implemented in terms of lazyFind. Figure 4 shows the code for this construct \nin Kaplan. The assuming block creates an Assuming instance. We rely on the implicit conversion mechanism \n abstract class Term[T,R] { self . def .nd(implicit isBool: R =:= Boolean): Option[T] = ... def solve(implicit \nisBool: R =:= Boolean): T = this..nd.getOrElse(throw new UnsatException) def .ndAll(implicit isBool: \nR =:= Boolean) : Iterator[T] = ... def c(implicit isBool: R =:= Boolean): self.type = this ... } class \nTerm0[R] extends Term[Unit,R] with Function0[R] {... }class Term1[T1,R] extends Term[T1,R] with Function1[T1,R] \n{... } class Term2[T1,T2,R] extends Term[(T1,T2),R] with Function2[T1,T2,R] {def ||(other: Term2[T1,T2,Boolean]) \n(implicit isBool: R =:= Boolean): Term2[T1,T2,Boolean] = ... def &#38;&#38;(other: Term2[T1,T2,Boolean]) \n(implicit isBool: R =:= Boolean): Term2[T1,T2,Boolean] = ... def unary !(implicit isBool: R =:= Boolean): \nTerm2[T1,T2,Boolean] = ... def compose0[A1](other: Term1[A1,T1]): Term2[A1,T2,R] = ... def compose1[A1](other: \nTerm1[A1,T2]): Term2[T1,A1,R] = ... def compose0[A1,A2](other: Term2[A1,A2,T1]): Term3[A1,A2,T2,R] = \n... def compose1[A1,A2](other: Term2[A1,A2,T2]): Term3[T1,A1,A2,R] = ... ... def product1[A1](other: \nTerm1[A1,Boolean]) (implicit isBool: R =:= Boolean): Term3[T1,T2,A1,Boolean] = ... def product2[A1,A2](other: \nTerm2[A1,A2,Boolean]) (implicit isBool: R =:= Boolean): Term4[T1,T2,A1,A2,Boolean] = ... def minimizing(objective: \nTerm2[T1,T2,Int]) (implicit asBool : R =:= Boolean): MinConstraint2[T1,T2] = ... ... } ... type Constraint[T] \n= Term[T,Boolean] type Constraint0 = Term0[Boolean] type Constraint1[T1] = Term1[T1,Boolean] ... Figure \n3. Term class hierarchy. of Scala for implementing the construct: if the optional otherwise block is \nnot de.ned, the type checking phase will insert a call to as\u00adsuming2value, which will trigger a conversion \nfrom the Assuming instance to the value it encapsulates. There exists a difference in the treatment of \nthe optional second part of the built-in if-then-else construct and our library extension for assuming-otherwise: \nwithout an optional else block, an if(...) {... } block is always type-checked as Unit. This is built \nin in the Scala compiler. We cannot achieve the same effect with assuming\u00adotherwise without making deep \nchanges to the compiler, so in our case, an assuming(...) { ... } block will throw an exception at run\u00adtime \nif the following three conditions occur: 1) the assuming test fails, 2) no otherwise block is de.ned, \nand 3) an expression of a type different from Unit is required.  5.2 Scala Compiler Plugin The compile-time \ntransformations of Kaplan programs include the following: def assuming[A](cond: Constraint0)(block: . \nA): Assuming[A] = { val v: Option[A] = cond.lazyFind match { case Some( ) . Some(block) case None . None \n } new Assuming(v) } .nal class Assuming[A](val thenResult: Option[A]) { def otherwise(elseBlock: . A): \nA = thenResult match { case None . elseBlock case Some(tr) . tr }} implicit def assuming2value[A](a: \nAssuming[A]): A = { a.thenResult match { case Some(tr) . tr case None . throw new Exception( otherwise \nblock not de.ned ) }} Figure 4. Implementation of assuming-otherwise in terms of lazyFind. 1. extracting \nuser-de.ned speci.cation functions and algebraic data types; 2. generating methods to allow conversion \nbetween values of these data types and their representation in our solver Leon; 3. transforming implicit \ncalls to conversion methods in order to instantiate Term instances.  These transformations are implemented \nas a compiler plugin that constitutes a compiler phase that follows the type checking phase. Functions \nand classes that carry the @spec annotation are expected to be in the PureScala constraint sublanguage, \npresented in Section 3. Alternatively, developers can group these speci.ca\u00adtion functions and data types \nin annotated Scala objects, instead of annotating each of them. These speci.cations are extracted during \ncompilation to obtain a representation that we use in solving. In ad\u00addition, method de.nitions are generated \nand inserted into the code in order to convert between these types and their representation. We rely \non the Scala compiler to signal to us the locations where a function literal needs to be lifted to a \nconstraint literal. The type checking phase, which runs before ours, does so by surrounding such function \nliterals by a call to an implicit function2term con\u00adversion function. These functions are de.ned in the \nKaplan library but have no implementation: they simply serve as a guide to in\u00addicate to the type checker \nthat the conversion is legal. (The effect of compiling code written for Kaplan without the Kaplan plugin \nis thus that all constraint manipulation operations result in a run-time NotImplemented exception.) \n 5.3 Implementation of the Core Solving Algorithms Our implementation leverages the SMT solver Z3 through \nits ex\u00adtension, Leon [60]. In the following we refer simply to Z3, as the algorithms we cover in this \nsection would remain the same if we used Z3 alone; what we gain by using Leon is the additional ex\u00adpressive \npower of recursive functions within constraints. We now describe the interactions with the solver that \nallow us to put into practice features such as enumeration, minimization, and logical variables. Solution \nenumeration. .nd and lazyFind are used to implement .ndAll and lazyFindAll, respectively, through the \nuse of an iterator.  def solveMinimizing(f, tm) {solve(f) match { case ( SAT , m) . model = m v = modelValue(m, \ntm) pivot = v - 1 lo = null hi = v +1 while (lo == null . hi - lo > 2) { solve(f . tm = pivot) match \n{ case ( SAT , m) . model = m if (lo == null) { pivot = pivot = 0 ? -1 : pivot \u00d7 2 hi = pivot +1 } else \n{ lv = modelValue(m, tm) pivot = lv +(pivot +1 - lv)/2 hi = pivot +1 } case ( UNSAT , ) . pivot = pivot \n+(hi - pivot)/2 lo = pivot } } return ( SAT , model) case ( UNSAT , ) . return ( UNSAT , null) }} Figure \n5. Pseudo-code of the solving algorithm with minimiza\u00adtion. We invoke our base satis.ability procedure \nvia calls to solve. The iterator maintains a constraint at all times, starting with the original one. \nEach time a new solution is required, the iterator destructively updates the constraint by adding to \nit the negation of the previous solution, thus ensuring that all following solutions will be different. \nTo make this process ef.cient, Kaplan relies on the incremental reasoning capabilities of Z3 (and thus \nLeon) to avoid solving the entire constraint each time. The implementation of lazyFindAll is conceptually \nidentical, with the difference that it returns logical variables instead of concrete values. These logical \nvariables are constrained in the store to be all-different. For an enumeration using lazyFindAll to terminate, \nLeon must therefore prove that the constraint has .nitely many solutions. Optimization constraints. Our \nprocedure for optimizing a con\u00adstraint with respect to an objective function can be seen as a gen\u00aderalization \nof binary search over the range of values that the objec\u00adtive can take. Let us consider the case of minimization \n(the max\u00adimization procedure is analogous). The pseudo-code for the algo\u00adrithm can be seen in Figure \n5.2 It starts by attempting to .nd a satisfying assignment for the constraint. It then repeatedly looks \nfor a model in which the objective is smaller than the last satisfy\u00ading value, by exponentially increasing \nthe difference until a lower bound is found. It further reduces the interval until the optimal value \nfor the objective is found. The procedure maintains the in\u00advariants that: 1) lo is always less than any \nsatisfying assignment to tm; and 2) there always exists a satisfying assignment to tm which is less than \nhi. Ordered enumeration. Having de.ned the solving procedure that minimizes a given term, we can now \ncompose it with solution enumeration to obtain ordered enumeration. We present in Figure 6, 2 We use \npseudo-code for clarity, but it is not hard to see that it can be implemented using only standard Scala \nalong with .nd or lazyFind. def orderedEnum(f, tm) {solveMinimizing(f) match { case ( SAT , m) . vm = \nmodelValue(m, tm) .ndAll(f . tm = vm) ++ orderedEnum(f . tm >vm, tm) case ( UNSAT , ) . return Iterator.empty \n}} Figure 6. Pseudo-code of the ordered enumeration algorithm. a recursive algorithm that will enumerate \nsolutions to f, ordered by the value of tm, which should be minimized. In this pseudo-code, we use .ndAll \nto get an iterator of all val\u00adues satisfying the given predicate, and ++ to concatenate iterators. Handling \nlogical variables. We use a global context to keep track of the constraints associated with logical variables. \nGiven a logical variable l and the constraint cl associated with it, we create a guard (a boolean literal) \ngl, denoting the liveness of the logical variable, i.e. whether its value has not been .xed yet. Throughout \nthe execution, we maintain in the global context the set of guards that are still alive. Upon creation \nof the variable l, we add gl to the set of alive variables and we assert cl, guarded by the disjunction \nof all the guards in the set G, de.ned as the set containing gl and the guards associated with all the \nlogical variables that appear in cl: g . cl g.G The reason for considering the guards associated with \nthe other logical variables is to ensure the single value semantics of these. Consider the following \nsimple example that illustrates the situation: for (x . ((x: Int) . x = 0 &#38;&#38; x < 4).lazyFindAll) \n{ val y = ((y : Int) . y == x).lazyFind assuming (x = 2) { ... // .rst block } assuming (y < 2) { ... \n// second block }} In each iteration of the outer for-comprehension, if the .rst block is entered, we \ndo not want to enter the second one, as this would violate the single value semantics for the variable \nx. Since the constraint y == x will be enforced as long as either x or y has not been .xed, the con.icting \nsituation is correctly avoided. In the terminology of Section 4, gl denotes whether the VALUE rule has \nbeen used for l (true means it has not), cl is the constraint on which L-SAT was applied to introduce \nl into . (as part of lf ) and the logical variables represented by the set of guards G are those that \ncontributed to the constraint (the l variables in L-SAT). Note that using individual guards for logical \nvariables is not required by the semantics, but rather is an optimization that allows us to reduce the \nsize of the constraint store when some values become known. When the value vl of the variable l is set, \nwe remove gl from the set of alive guards, and we assert the following: \u00acgl . l = vl When all of the \nliterals guarding a constraint cl are removed d from the alive set, the formula g.G g . cl that was asserted \nbecomes trivially true, and the constraint can be discarded by Z3. Another source of optimization is \nthat we override the .nalize method of the L instances such that, even if their value is never .xed, \ntheir guard is removed from the set of alive variables when they are being considered for garbage collection \nby the JVM. This example  time (s) .rst examples 0.16 unimodular matrices 0.76 sat solving 0.05 knapsack \n0.18 prime numbers 0.80 all red-black trees up to 7 nodes 27.45 send+more=money 1.17 Figure 7. Evaluation \nresults for the examples presented in Sec\u00adtion 2. is again not strictly speaking required by the semantics, \nbut helps reducing the overhead of tracking all logical variables. Invocations of the solver. We summarize \nthis section by presenting a list of all the places where an invocation of the solver occurs: calls \nto .nd, as they search for a solution eagerly,  calls to lazyFind, as they check whether a satisfying \nassignment exists for logical variables,  calls to the hasNext method of the iterators returned by calls \nto .ndAll and lazyFindAll, which are then translated into calls to .nd and lazyFind respectively, and \n evaluation of the constraint of assuming blocks, as they indi\u00adrectly invoke the solver by invoking \nlazyFind on the constraint.  Note that, as the proof in Section 4 suggested, calls to the value method \nof logical variables do not trigger a solver invocation. 6. Advanced Usage Scenarios and Evaluation In \nthis section, we present an experimental evaluation of our con\u00adstraint programming system by considering \na number of examples. As a .rst overview of the performance of our implementation, we present the running \ntimes for the examples that were introduced in Section 2. The results of our evaluation can be seen in \nFigure 7. We observe that most problems are solved almost instantly, with the exception of the red-black \ntree enumeration. We discuss the dif.culty of enumerating data structures satisfying an invariant in \nthe subsequent examples. 6.1 Enumerating Data Structures for Testing One use of the .ndAll construct \nconsists in enumerating data structures that satisfy given invariants. This is a problem that has been \nstudied previously, and was motivated by [11]. Subsequent work [24] presents a Java-based language with \nnon-deterministic choice operators that can be used for enumerating linked data struc\u00adtures. We describe \nour experience in using Kaplan to enumerate func\u00adtional data structures to .nd input values that violate \nfunction con\u00adtracts. We consider a functional speci.cation of red-black trees. We enumerate solutions \nto function preconditions and check whether the postconditions hold. As in [11, 24], we enumerate the \ndata structures while bounding the range of values than can be stored in nodes. A red-black tree is a \nbinary search tree characterized by the following additional properties: 1) each node is either red or \nblack; 2) all leaves are black; 3) both children of every red node are black; and 4) every simple path \nfrom the root to any leaf contains the same number of black nodes. The .rst method we consider is balance, \nwhich de.nes one of the cases erroneously to duplicate one of the subtrees and forgetting another while \nrebalancing the tree. This results in violating the post-condition of the add method as the result tree \ndoes not have the expected content. In this case, an example violating the post\u00adcondition is found after \nenumerating twelve trees. Our test harness list size 20 40 60 80100 time (s) 0.24 0.45 0.56 0.72 0.98 \nFigure 8. Evaluation results for declarative last method. consists of enumerating trees that satisfy \nthe precondition of add and calling the method in the body of the loop. We then consider the case where \nthe add method has a missing precondition, namely that the tree must be black-balanced. In this case, \nthe precondition to a method that is called within add fails, and we .nd a bug-producing value using \na similar harness after enumerating four trees, in 0.187 seconds. We argue that a random test-case generation \napproach would be insuf.cient in enumerating such data structures that satisfy complex invariants. While \nthe re\u00adsults using constraint solving is not as fast as the specialized solving in UDITA [24], we should \nkeep in mind that this is an experiment in using a general-purpose constraint solving engine. The gener\u00adality \nof Kaplan is in contrast to previously proposed solutions for data structure enumeration, which rely \non specialized techniques for linked heap data structures, or even techniques specialized to a particular \ndata structure [7]. In the light of the generality of our technique and the overall dif.culty of the \nproblem, we consider these to be good results.  6.2 Executable Speci.cations As another application \nof implicit computation, we now explore ex\u00adamples that consist of the execution of declarative speci.cations \ninstead of explicit computation. Execution of speci.cations is the approach taken in Plan B [55], in \nwhich speci.cations are used as a fallback mechanism upon contract violations. Consider, for instance, \na function that computes the last element of a given list. We can de.ne this function declaratively, \nby stating that the input list is equal to some list concatenated with the list that has only the element \nthat we are looking for: def last(list : List) : Int = {val (elem, ) = ((e: Int, zs: List) . concat(zs, \nCons(e, Nil())) == list).solve elem } As a more elaborate example, consider adding an element to (or \nremoving an element from) a red-black tree. The explicit insertion and removal have to consider multiple \ncases in order to keep the invariants that red-black trees should satisfy, and are known to be tricky \nto implement. On the other hand, these methods can be stated succinctly in a declarative manner, using \nfunctions that check if a given tree is indeed a red-black tree, along with functions computing the content \nof the tree as a set: def addDeclarative(x: Int, tree: Tree) : Tree = ((t: Tree) . isRedBlackTree(t) \n&#38;&#38; content(t) == content(tree) ++ Set(x)).solve def removeDeclarative(x: Int, tree: Tree) : Tree \n= ((t: Tree) . isRedBlackTree(t) &#38;&#38; content(t) == content(tree) -- Set(x)).solve The performance \nof the above methods is presented in Figure 9. We also show the results for the similar case of inserting \ninto and removing from a sorted list. Note that we encounter essentially the same running times for the \nproblem of declaratively sorting a list, by asking for a sorted list of the same content as a given list. \nThe above examples show that it is possible to replace the ex\u00adplicit computation for a method by its \npurely declarative speci.ca\u00adtion, even for sophisticated contracts such as the ones on red-black trees. \nThe declarative variants are notably slower than the imper\u00adative implementations, but it is likely preferable \nto rely on these executable speci.cations instead of simply crashing when the im\u00adperative version violates \nthe contract.  size list add list remove RBT add RBT remove 0 1 2 3 4 5 6 7 8 9 10 0.07 0.08 0.12 0.16 \n0.24 0.39 0.55 0.97 1.48 2.27 3.32 0.02 0.02 0.05 0.10 0.18 0.38 0.45 0.67 1.09 1.80 2.22 0.03 0.10 0.14 \n0.55 0.66 1.07 1.51 9.32 11.13 24.49 11.51 0.02 0.05 0.09 0.41 0.76 0.91 1.56 13.09 18.80 25.79 20.55 \n Figure 9. Evaluation results for declarative add and remove, for red-black trees and for sorted lists. \nsize is the size of the structure without the element. All times are in seconds. As a .nal example, let \nus consider the implicit computation that v gives the integer square root L iJ of a positive integer \ni. This is concisely stated as following: def sqrt(i : Int) : Int = ((res: Int) . res > 0 &#38;&#38; \nres * res = i &#38;&#38; (res + 1) * (res + 1) > i).solve Our implementation can handle numbers as large \nas hundreds of thousands, performing under 0.3 seconds in all cases.  6.3 Counter-example Guided Inductive \nSynthesis Counter-example guided inductive synthesis (CEGIS) is a method for effectively solving .. constraints \nusing SMT solvers. Such con\u00adstraints are particularly important for program synthesis, identi.ed as the \nclass of .. synthesis problems in [53]. The counter-example guided approach for this problem was successfully \napplied to soft\u00adware synthesis in the algorithm for combinatorial sketching using SAT solvers [58]. This \ntechnique was later applied with an SMT solver as the satis.ability engine to synthesize loop-free bit-vector \ncode fragments [26]. The technique starts by choosing some initial set of values for the universally \nquanti.ed variables, then solving the constraint for the existentially quanti.ed variables. If the values \nfor the existen\u00adtially quanti.ed variables work for all values of the universally quanti.ed variables, \na solution has been found. Otherwise, there is a counterexample which is some valuation of the universally \nquan\u00adti.ed variables. This counterexample is added to the set of values for universally quanti.ed variables \nand the procedure is repeated until a solution is found. This synthesis loop can be expressed in our \nsystem succinctly, using .rst-class constraints. As an example we consider the fol\u00adlowing: Are there \nintegers a and b such that, for every integer x, a \u00b7 (x - 1) <b \u00b7 x ? In Kaplan we can describe the CEGIS \nap\u00adproach as in Figure 10. When executed, the program .nds correct values for a and b in two iterations \nof the loop. The output of the program is: Initial x: 0 candidate parameters a = 1, b = 0 counterexample \nfor x: 1 candidate parameters a = 1, b = 1 proved! The example generalizes to arbitrary constraints (which, \nin this example, are cnstrGivenX and cnstrGivenParams), keeping the same simple structure. Note that \nwe here explicitly constructed increasingly stronger .rst-class constraints; we can instead use logical \nvariables and incrementally augment the constraint store. var continue = true val initialX = ((x: Int) \n. true).solve def cnstrGivenX(x0: Int): Constraint2[Int,Int] = ((a: Int, b: Int) . a * (x0 - 1) < b * \nx0) def cnstrGivenParams(a0: Int, b0: Int): Constraint1[Int] = ((x: Int) . a0 * (x - 1) < b0 * x) var \ncurrentCnstr = cnstrGivenX(initialX) while (continue) { currentCnstr..nd match { case Some((a, b)) .{println( \ncandidate parameters a = + a + , b = + b) (! cnstrGivenParams(a, b))..nd match { case None . println( \nproved! ) continue = false case Some(ce) . println( counterexample for x: + ce) currentCnstr = currentCnstr \n&#38;&#38; cnstrGivenX(ce) } } case None . println( cannot prove property! ) continue = false }} Figure \n10. Counter-example guided inductive synthesis in Kaplan list size time in Kaplan (s) time in Curry (s) \n10000 < 0.01 0.21 100000 < 0.01 2.41 1000000 < 0.01 23.88 Figure 11. Evaluation results for functional \nlast method.  6.4 Comparison to Other Systems Providing a fair comparison of running times of Kaplan \nagainst competing systems is dif.cult because Kaplan covers many areas for which specialized tools have \nbeen developed. We do not expect to match performance of each of these specialized systems. At the same \ntime, there is no single system that subsumes Kaplan. This section illustrates how other tools can solve \nsome of the problems we presented in this paper and solved using Kaplan. The running times should not \nbe taken as a de.nitive statement of the relative merits of the systems, but rather as a guide to understanding \nkey differences of the underlying constraint solving techniques. Last element of the list. We .rst compare \nthe performance of Kaplan and Curry on computing the last element of a list, both in a declarative way \nas presented in Section 6.2, and as a tail\u00adrecursive functional method. In Curry, the declarative version \ncan be expressed as: last xs | concat ys [x] =:= xs = x where x,ys free The performance of the Curry \nimplementation surpasses the performance of Kaplan that we reported in Figure 8, running under 0.01 seconds \nin lists of size up to 100. However, in Kaplan we also have the freedom of writing the constraint directly \nas a functional program. In such style, Curry performs at about the same speed as for a constraint-based \nde.nition, so Kaplan outperforms Curry on longer lists. Figure 11 shows the evaluation results for this \ncase. It is of course the Scala compiler and the Java Virtual Machine that take credit for the good performance \nof the functional implementation; to the credit of Kaplan is simply that it does not lose any of this \nunderlying ef.ciency. This basic consequence of Kaplan design is very important from a practical point \nof view. In principle, a static analysis could be used to recognize among logical constraints special \nclasses that do not require constraint solving (consider, for example, mode analysis of Mercury [51]). \nHowever, this approach clearly requires signi.cant compilation effort to merely recover the baseline \nperformance of a functional or imperative code, whereas in Kaplan it follows by construction.  Generating \ndata structures with complex invariants. We now re\u00adport on our experience in comparing Kaplan with ScalaCheck \n[48] for generating red-black trees (results were similar for sorted lists). ScalaCheck is a tool for \nproducing test cases using random test gen\u00aderation, similar to QuickCheck [13] for Haskell. We implemented \nbasic generators for lists and trees using generator combinators in ScalaCheck. The problem with random \ngeneration is that, for many classes of properties, the probability of a random structure satisfy\u00ading \nit may tend to zero as the structure size grows [10], making test cases vacuous for larger structures. \nTo see this in practice, consider .rst our own performance: Figure 7 shows that Kaplan takes 27.45 seconds \nin total to generate all red-black trees of size n containing elements 1 to n, for all n from 0 to 7. \nWe therefore de.ned a ba\u00adsic ScalaCheck generator that, with equal probability, generates an empty or \na non-empty tree. We ran it for 29.1 seconds, generating 200 000 trees. Although 62% of all of them were \nred-black, that is because 104 544 were, in fact, empty. Of the rest, 18 191 had size one, 934 size two, \nand there were no red-black trees of larger size. In this experiment, we gave both Kaplan and ScalaCheck \nonly the constraints, without any additional insight, which makes the com\u00adparison fair. Experienced users \ncould write better ScalaCheck gen\u00aderators, but they could similarly write better Kaplan checkers. Like \nUDITA [24], Kaplan supports a full spectrum of intermediate ap\u00adproaches, where one puts more attention \ninto either writing better generators, to increase the ratio of valid testcases, or into manually decomposing \nthe implicit computation to reduce the search space. Constraint satisfaction problems. Tools for constraint \nsolving and optimization over .nite domains have developed somewhat independently of SMT and the related \ntechnologies [4, 23, 47, 61]. Our anecdotal experience suggests that, for problems of moderate size over \n.nite domains, the two technologies yield comparable results. A full experimental comparison between \nthe state of the art of these two communities is beyond the scope of this paper. Note that we could incorporate \ninto Kaplan solvers from either class, as long as they support our domains of interest (including, for \nexample, algebraic data types). 7. Related Work Existing languages. Functional logic programming [3] \namal\u00adgamates the functional programming and logic programming paradigms into a single language. Functional \nlogic languages, such as Curry [41], bene.t from ef.cient demand-driven term reduc\u00adtion strategies proper \nto functional languages, as well as non\u00addeterministic operations of logic languages, by using a technique \ncalled narrowing, a combination of term reduction and variable instantiation. Instantiation of unbound \nlogic variables occur in con\u00adstructive guessing steps, only to sustain computation when a re\u00adduction \nneeds their values. The performance of non-deterministic computations depends on the evaluation strategy, \nwhich are for\u00admalized using de.nitional trees [2]. Applications using functional logic languages include \nprogramming of graphical and web user interfaces [27, 28] as well as providing high-level APIs for access\u00ading \nand manipulating databases [12]. Our work can be seen as the practical experiment of building from existing \ncomponents a sys\u00adtem close in functionality to Curry: we proceeded by extending an existing language \nwhile preserving its execution model, rather than starting from scratch. As such, we lose the luxury \nof a to\u00adtal integration of paradigms; for instance, users of Kaplan need to specify which functions can \nhandle logical variables. On the other hand, such a separation of features comes with bene.ts: we can \nreadily use the full power of Z3 and the Leon veri.cation system for constraint solving, and the execution \nef.ciency of the Java vir\u00adtual machine for regular code. Implementations of functional logic languages, \non the other hand, must typically focus on ef.ciently executing either the logical or the functional \ncomponents of the code. The Oz language and the associated Mozart Programming Sys\u00adtem is another admirable \ncombination of multiple paradigms [64], with applications in functional, concurrent, and logic program\u00adming. \nIn particular, Oz supports a form of logical variables, and logic programming is enabled through uni.cation. \nOne limitation is that one cannot perform arithmetic operations with logical vari\u00adables (which we have \ndemonstrated in several of our examples), because uni.cation only applies to constructor terms. Some \nof the earlier efforts to integrate constraint and imper\u00adative programming resulted in the languages \nKaleidoscope [37], Alma-0 [5] and Turtle [25]. This line of research put emphasis on designing novel \nways to de.ne constraints, ideally to resem\u00adble imperative-style programming. Kaleidoscope, for instance, \npro\u00admotes the integration of constraints with objects; programmers can de.ne instance constraints that \nrelate various members of an object, and constraints can be reassigned, just like mutable variables. \nCon\u00adstraints are also associated to a duration (once or always), specify\u00ading intuitively the scope in \nwhich they can affect variables. Kaplan currently does not support constraints over mutable data types \nand could bene.t from the integration of such features. The integration of constraints as .rst-class \nmembers of the lan\u00adguage is tighter in Kaplan than in any of the languages discussed above. For example, \nin Curry programmers can de.ne constraints as functions that return the special type Success (which is \nnot iden\u00adtical to Boolean). However, such functions (or rather predicates) in Curry can only be built \nfrom equality predicates and cannot be combined freely: while conjunction and disjunction are valid com\u00adbinators, \nnegation, for instance, is not. In Kaplan, on the other hand, any predicate expressed in PureScala can \nbe used as a constraint, and terms of other types can also be manipulated and composed freely. Another \ndistinguishing feature of Kaplan is the native support for many theories (integers, sets, maps, data \ntypes) that comes from using Leon [60] and Z3 as the underlying constraint solver. Language design. Monadic \nconstraint programming [56] inte\u00adgrates constraint programming into purely functional languages by using \nmonads to de.ne solvers. The authors de.ne monadic search trees, corresponding to a base search, that \ncan be transformed by the use of search transformers in a composable fashion to increase performance. \nOur system differs from this work in its use of SMT solvers for search, and a more .exible way of mixing \nconstraints with imperative programming. Localized code synthesis techniques have been proposed to turn \nimplicit declarations into explicit code [35]. The main limitation of that approach is the requirement \nthat the theory should be de\u00adcidable. We have shown that using a powerful, undecidable, logic for constraints \ncan be bene.cial. Ideally, compile-time techniques such as [35] should be combined with the run-time \napproach of constraint solving. The work on uniform reduction to bit-vector arithmetic [40] (URBIVA) \nproposes a C-like language for specifying constraints. It uses symbolic execution to encode problems \ninto bit-vector arith\u00admetic and invokes one of the underlying bit-vector and SAT solvers. The system \nallows for the comparison of different solvers on ex\u00adamples involving solution enumeration and functional \nequivalence checking. The use of symbolic values in conditional statements and array indexing is not \npermitted. Because it uses native enumera\u00adtion capability of solvers such as CLASP, it can be faster \nthan our system on some of the benchmarks. URBIVA does not support un\u00adbounded domains; we are aware of \nno techniques to enumerate so\u00adlutions for unbounded domains more ef.ciently than in Kaplan.  SMT as \na programming platform. The Dminor language [9] introduces the idea of using an SMT solver to check subtyping \nre\u00adlations between re.nement types; in Dminor, all types are de.ned as logical predicates, and subtyping \nthus consists of proving an im\u00adplication between two such predicates. The authors show that an impressive \nnumber of common types (including for instance alge\u00adbraic data types) can be encoded using this formalism. \nIn this con\u00adtext, generating values satisfying a predicate is framed as the type inhabitation problem, \nand the authors introduce the expression ele\u00admentof T to that end. It is evaluated by invoking Z3 at \nrun-time and is thus conceptually comparable to our .nd construct but without support for recursive function \nunfolding. We have previously found that recursive function unfolding works better as a mechanism for \nsatis.ability checking than using quanti.ed axiomatization of re\u00adcursive functions [60]. In general, \nwe believe that our examples are substantially more complex than the experiences with elementof in the \ncontext of Dminor. The ScalaZ3 library [33] is our earlier effort to integrate invo\u00adcations to Z3 into \na programming language. Because it is imple\u00admented purely as a library, we were then not able to integrate \nuser\u00adde.ned recursive functions and data types into constraints, so the main application is to provide \nan embedded domain-speci.c lan\u00adguage to access the constraint language of Z3 (but not to extend it). \nA similar approach has been taken by others to invoke the Yices SMT solver [19] from Haskell.3 Applications \nof declarative programming. One approach in using speci.cations for software reliability is data structure \nrepair [16, 20], where the goal is to recover from corrupted data structures by transforming states that \nare erroneous with respect to integrity constraints into valid ones, performing local heuristic search. \n[65] uses method contracts instead of data structure integrity constraints to be able to support rich \nbehavioral speci.cations. While the pri\u00admary goal is to perform run-time recovery of data structures, \nre\u00adcent work [38] extends the technique for debugging purposes, by abstracting concrete repair actions \nto program statements perform\u00ading the same actions. Data structure repair differs from our system in \nthat it can perform local search to modify existing states, while we do not currently do so. We do not \nexpect that a general-purpose constraint solving such as ours can immediately compare with these dedicated \ntechniques. The idea to use speci.cations as a fall-back mechanism, as in one of our application examples, \nwas adopted in [55]. Similarly to our setting, dynamic contract checking is applied and, upon vio\u00adlations, \nspeci.cations can be executed. The technique ignores the erroneous state and computes output values for \nmethods given con\u00adcrete input values and the method contract. The implementation uses a relational logic \nsimilar to Alloy [30] for speci.cations, and deploys the Kodkod model .nder [63]. A related tight integration \nbetween Java and the Kodkod engine is presented in [42]. In both cases, due to the .nite bound on the \nsearch space, a satisfying an\u00adswer may not always be found, which makes the techniques in\u00adcomplete. We \nhave shown that executing declarative speci.cations is possible in our setting. We expect that a constraint \nsolver such as the one deployed in Leon will ultimately be better suited than an approach based on KodKod, \ndue to the presence of unbounded or large data types such as integers and recursive structures. 3 http://hackage.haskell.org/package/yices-easy \n8. Conclusion We presented Kaplan, an extension of the multi-paradigm Scala language that integrates \nconstraint programming while preserving the existing functional and object-oriented features of Scala. \nThe behavior and performance of Kaplan is identical to Scala in the absence of declarative constraint \nsolving. Kaplan integrates con\u00adstraints as .rst-class objects in Scala, and logical variables for solv\u00ading \nconstraints lazily. It allows for solving optimization problems, as well as enumerating solutions in \na user-speci.ed order. Kaplan is implemented as a combination of a compiler plugin and a run\u00adtime library, \nand allows users to express constraints in a powerful and expressive logic. Kaplan relies on a procedure \nfor satis.ability modulo recursive functions [59, 60] to solve these constraints. We evaluated our system \nby considering applications such as solution enumeration, execution of declarative speci.cations, test\u00adcase \ngeneration for bug-.nding and counterexample guided induc\u00adtive synthesis, as well as numerous smaller, \nyet expressive, snip\u00adpets. We observed that our model for introducing non-determinism using for-comprehensions \nintegrates well with Scala. Based on our experience with the Z3 SMT solver and the Leon veri.cation system \nin constraint programming, we found that a number of features, if natively supported by solvers, could \ndirectly bring bene.ts to constraint programming. These include 1) support for enumeration of theory \nmodels and 2) solving constraints while minimizing/maximizing a given term. Overall, we believe there \nis great potential in extending standard programming languages with constraint solving capabilities. \nMany interesting problems are left open, both in language design and in constraint solving; a system \nsuch as Kaplan that integrates state-of-the-art tools from both domains is likely to bene.t from progress \nmade in each of them. Acknowledgments We thank Sergio Antoy for an interesting discussion on the status \nof the Curry programming language. We thank Nikolaj Bj\u00f8rner and Leonardo de Moura for their help in using \nZ3, and Adriaan Moors for his help with the Scala compiler. We thank Swen Jacobs, Eva Darulov\u00b4 a, and \nthe anonymous reviewers for their feedback. References [1] H. A\u00a8it-Kaci. Warren s Abstract Machine: A \nTutorial Reconstruction. MIT Press, 1991. [2] S. Antoy. De.nitional trees. In ALP, pages 143 157, 1992. \n[3] S. Antoy and M. Hanus. Functional logic programming. CACM, 53 (4):74 85, 2010. [4] K. R. Apt and \nM. Wallace. Constraint logic programming using Eclipse. Cambridge University Press, 2007. [5] K. R. Apt, \nJ. Brunekreef, V. Partington, and A. Schaerf. Alma-O: An imperative language that supports declarative \nprogramming. TOPLAS, 20(5):1014 1066, 1998. [6] R.-J. Back and J. von Wright. Re.nement Calculus. Springer-Verlag, \n1998. [7] T. Ball, D. Hoffman, F. Ruskey, R. Webber, and L. J. White. State generation and automated \nclass testing. Softw. Test., Verif. Reliab., 10 (3), 2000. [8] C. Barrett and C. Tinelli. CVC3. In CAV, \nvolume 4590 of LNCS, 2007. [9] G. M. Bierman, A. D. Gordon, C. Hritcu, and D. E. Langworthy. Semantic \nsubtyping with an SMT solver. In ICFP, pages 105 116, 2010. [10] A. Blass, Y. Gurevich, and D. Kozen. \nA zero-one law for logic with a .xed-point operator. Inf. Control, 67, October 1986.  [11] C. Boyapati, \nS. Khurshid, and D. Marinov. Korat: Automated test\u00ading based on Java predicates. In Proc. International \nSymposium on Software Testing and Analysis, pages 123 133, July 2002. [12] B. Bra\u00dfel, M. Hanus, and M. \nM\u00a8uller. High-level database program\u00adming in Curry. In PADL, pages 316 332, 2008. [13] K. Claessen and \nJ. Hughes. Quickcheck: a lightweight tool for random testing of haskell programs. In ICFP, pages 268 \n279, 2000. [14] A. Colmerauer, H. Kanoui, and M. V. Caneghem. Last steps towards an ultimate PROLOG. \nIn IJCAI, pages 947 948, 1981. [15] R. DeLine and K. R. M. Leino. BoogiePL: A typed procedural language \nfor checking object-oriented programs. Technical Report MSR-TR-2005-70, Microsoft Research, March 2005. \n[16] B. Demsky and M. C. Rinard. Automatic detection and repair of errors in data structures. In OOPSLA, \npages 78 95, 2003. [17] D. Detlefs, G. Nelson, and J. B. Saxe. Simplify: a theorem prover for program \nchecking. J. ACM, 52(3):365 473, 2005. [18] E. W. Dijkstra. Guarded commands, nondeterminacy and formal \nderivation of programs. Commun. ACM, 18(8):453 457, 1975. [19] B. Dutertre and L. de Moura. The Yices \nSMT solver, 2006. http: //yices.csl.sri.com/tool-paper.pdf. [20] B. Elkarablieh and S. Khurshid. Juzi: \na tool for repairing complex data structures. In ICSE, pages 855 858, 2008. [21] S. Fischer, O. Kiselyov, \nand C. Shan. Purely functional lazy non\u00addeterministic programming. In ICFP, volume 44, pages 11 22, 2009. \n[22] H. Ganzinger, G. Hagen, R. Nieuwenhuis, A. Oliveras, and C. Tinelli. DPLL(T): Fast decision procedures. \nIn CAV, pages 175 188, 2004. [23] I. P. Gent, C. Jefferson, and I. Miguel. MINION: A fast, scalable, \nconstraint solver. In European Conference on Arti.cial Intelligence, pages 98 102. IOS Press, 2006. [24] \nM. Gligoric, T. Gvero, V. Jagannath, S. Khurshid, V. Kuncak, and D. Marinov. Test generation through \nprogramming in udita. In ICSE (1), pages 225 234, 2010. [25] M. Grabm\u00a8uller and P. Hofstedt. Turtle: \nA constraint imperative pro\u00adgramming language. In Innovative Techniques and Applications of Arti.cial \nIntelligence, 2003. [26] S. Gulwani, S. Jha, A. Tiwari, and R. Venkatesan. Synthesis of loop\u00adfree programs. \nIn PLDI, pages 62 73, 2011. [27] M. Hanus. Type-oriented construction of web user interfaces. In PPDP, \npages 27 38, 2006. [28] M. Hanus and C. Klu\u00df. Declarative programming of user interfaces. In PADL, pages \n16 30, 2009. [29] D. Jackson. Structuring Z speci.cations with views. ACM Transac\u00adtions on Software Engineering \nand Methodology, 4(4), October 1995. [30] D. Jackson. Alloy: a lightweight object modelling notation. \nACM Trans. Softw. Eng. Methodol., 11(2):256 290, 2002. [31] J. Jaffar and J.-L. Lassez. Constraint logic \nprogramming. In POPL, 1987. [32] J. C. King. A Program Veri.er. PhD thesis, CMU, 1970. [33] A. S. K\u00a8oksal, \nV. Kuncak, and P. Suter. Scala to the power of Z3: Integrating SMT and programming (system description). \nIn CADE, pages 400 406, 2011. [34] R. A. Kowalski and D. Kuehner. Linear resolution with selection function. \nArtif. Intell., 2(3/4):227 260, 1971. [35] V. Kuncak, M. Mayer, R. Piskac, and P. Suter. Complete functional \nsynthesis. In PLDI, pages 316 329, 2010. [36] L. Lamport. Specifying Systems: The TLA+ Language and Tools \nfor Hardware and Software Engineers. Addison-Wessley, 2002. [37] G. Lopez, B. Freeman-Benson, and A. \nBorning. Kaleidoscope: A constraint imperative programming language. In Constraint Program\u00adming, pages \n313 329. Springer-Verlag, 1994. [38] M. Z. Malik, J. H. Siddiqui, and S. Khurshid. Constraint-based program \ndebugging using data structure repair. In ICST, pages 190 199, 2011. [39] Z. Manna and R. Waldinger. \nA deductive approach to program syn\u00adthesis. ACM Trans. Program. Lang. Syst., 2(1):90 121, 1980. ISSN \n0164-0925. doi: http://doi.acm.org/10.1145/357084.357090. [40] F. Maric and P. Janicic. Urbiva: Uniform \nreduction to bit-vector arithmetic. In IJCAR, pages 346 352, 2010. [41] E. Michael Hanus. Curry: An integrated \nfunctional logic language. http://www.curry-language.org, 2006. vers. 0.8.2. [42] A. Milicevic, D. Rayside, \nK. Yessenov, and D. Jackson. Unifying execution of imperative and declarative code. In ICSE, pages 511 \n520, 2011. [43] C. Morgan. Programming from Speci.cations (2nd ed.). Prentice-Hall, Inc., 1994. [44] \nL. de Moura and N. Bj\u00f8rner. Z3: An ef.cient SMT solver. In TACAS, 2008. [45] G. Nelson. Techniques for \nprogram veri.cation. Technical report, XEROX Palo Alto Research Center, 1981. [46] G. Nelson and D. C. \nOppen. Fast decision procedures based on congruence closure. Journal of the ACM (JACM), 27(2):356 364, \n1980. ISSN 0004-5411. doi: http://doi.acm.org/10.1145/322186. 322198. [47] N. Nethercote, P. Stuckey, \nR. Becket, S. Brand, G. Duck, and G. Tack. MiniZinc: Towards a standard CP modelling language. Principles \nand Practice of Constraint Programming, pages 529 543, 2007. [48] R. Nilsson. Scalacheck user guide. \nhttp://code.google.com/p/ scalacheck/wiki/UserGuide, 2011. [49] M. Odersky. Contracts in Scala. In International \nConference on Runtime Veri.cation. Springer LNCS, 2010. [50] M. Odersky, L. Spoon, and B. Venners. Programming \nin Scala: a comprehensive step-by-step guide. Artima Press, 2008. [51] D. Overton, Z. Somogyi, and P. \nJ. Stuckey. Constraint-based mode analysis of Mercury. In ACM SIGPLAN Workshop on Principles and practice \nof declarative programming (PPDI), 2002. [52] L. C. Paulson, T. Nipkow, et al. Isabelle theorem prover \n-of.cial website. http://www.cl.cam.ac.uk/Research/HVG/Isabelle. [53] A. Pnueli and R. Rosner. On the \nsynthesis of a reactive module. In POPL, 1989. [54] A. Riesco and J. Rodr\u00b4iguez-Hortal\u00b4 a. Programming \nwith singular and plural non-deterministic functions. In PEPM, pages 83 92, 2010. [55] H. Samimi, E. \nD. Aung, and T. D. Millstein. Falling back on exe\u00adcutable speci.cations. In ECOOP, pages 552 576, 2010. \n[56] T. Schrijvers, P. J. Stuckey, and P. Wadler. Monadic constraint pro\u00adgramming. J. Funct. Program., \n19(6):663 697, 2009. [57] J. P. M. Silva and K. A. Sakallah. GRASP -a new search algorithm for satis.ability. \nIn ICCAD, pages 220 227, 1996. [58] A. Solar-Lezama, L. Tancau, R. Bod\u00b4ik, S. A. Seshia, and V. A. Saraswat. \nCombinatorial sketching for .nite programs. In ASPLOS, 2006. [59] P. Suter, M. Dotta, and V. Kuncak. \nDecision procedures for algebraic data types with abstractions. In ACM SIGPLAN POPL, 2010. [60] P. Suter, \nA. S. K\u00a8oksal, and V. Kuncak. Satis.ability modulo recursive programs. In Static Analysis Symposium (SAS), \n2011. [61] G. Tack. Constraint Propagation -Models, Techniques, Implementa\u00adtion. PhD thesis, Saarland \nUniversity, 2009. [62] The Coq Development Team; INRIA LogiCal Project. The Coq proof assistant -of.cial \nwebsite. http://coq.inria.fr. [63] E. Torlak and D. Jackson. Kodkod: A relational model .nder. In Tools \nand Algorithms for Construction and Analysis of Systems (TACAS), 2007. [64] P. Van Roy. Logic programming \nin Oz with Mozart. In ICLP, 1999. [65] R. N. Zaeem and S. Khurshid. Contract-based data structure repair \nusing Alloy. In ECOOP, pages 577 598, 2010. [66] L. Zhang, C. F. Madigan, M. W. Moskewicz, and S. Malik. \nEf.cient con.ict driven learning in boolean satis.ability solver. In ICCAD, pages 279 285, 2001.   \n \n\t\t\t", "proc_id": "2103656", "abstract": "<p>We present an extension of Scala that supports constraint programming over bounded and unbounded domains. The resulting language, Kaplan, provides the benefits of constraint programming while preserving the existing features of Scala. Kaplan integrates constraint and imperative programming by using constraints as an advanced control structure; the developers use the monadic 'for' construct to iterate over the solutions of constraints or branch on the existence of a solution. The constructs we introduce have simple semantics that can be understood as explicit enumeration of values, but are implemented more efficiently using symbolic reasoning. Kaplan programs can manipulate constraints at run-time, with the combined benefits of type-safe syntax trees and first-class functions. The language of constraints is a functional subset of Scala, supporting arbitrary recursive function definitions over algebraic data types, sets, maps, and integers.</p> <p>Our implementation runs on a platform combining a constraint solver with a standard virtual machine. For constraint solving we use an algorithm that handles recursive function definitions through fair function unrolling and builds upon the state-of-the art SMT solver Z3. We evaluate Kaplan on examples ranging from enumeration of data structures to execution of declarative specifications. We found Kaplan promising because it is expressive, supporting a range of problem domains, while enabling full-speed execution of programs that do not rely on constraint programming.</p>", "authors": [{"name": "Ali Sinan K&#246;ksal", "author_profile_id": "81470653567", "affiliation": "Swiss Federal Institute of Technology (EPFL), Lausanne, Switzerland", "person_id": "P2991365", "email_address": "alisinan.koksal@epfl.ch", "orcid_id": ""}, {"name": "Viktor Kuncak", "author_profile_id": "81100277693", "affiliation": "Swiss Federal Institute of Technology (EPFL), Lausanne, Switzerland", "person_id": "P2991366", "email_address": "viktor.kuncak@epfl.ch", "orcid_id": ""}, {"name": "Philippe Suter", "author_profile_id": "81453631112", "affiliation": "Swiss Federal Institute of Technology (EPFL), Lausanne, Switzerland", "person_id": "P2991367", "email_address": "philippe.suter@epfl.ch", "orcid_id": ""}], "doi_number": "10.1145/2103656.2103675", "year": "2012", "article_id": "2103675", "conference": "POPL", "title": "Constraints as control", "url": "http://dl.acm.org/citation.cfm?id=2103675"}