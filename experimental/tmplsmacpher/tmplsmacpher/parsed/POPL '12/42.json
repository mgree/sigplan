{"article_publication_date": "01-25-2012", "fulltext": "\n Clarifying and Compiling C/C++ Concurrency: fromC++11toPOWER Mark Batty1 Kayvan Memarian1,2 Scott Owens1 \nSusmit Sarkar1 Peter Sewell1 1University of Cambridge 2INRIA {.rst.last}@cl.cam.ac.uk Abstract The upcoming \nC and C++ revised standards add concurrency to the languages, for the .rst time, in the form of a subtle \nrelaxed memory model (the C++11 model). This aims to permit compiler optimisation and to accommodate \nthe differing relaxed-memory behaviours of mainstream multiprocessors, combining simple se\u00admantics for \nmost code with high-performance low-level atomics for concurrency libraries. In this paper, we .rst establish \ntwo simpler but provably equiv\u00adalent models for C++11, one for the full language and another for the \nsubset without consume operations. Subsetting further to the fragment without low-level atomics, we identify \na subtlety arising from atomic initialisation and prove that, under an additional condi\u00adtion, the model \nis equivalent to sequential consistency for race-free programs. We then prove our main result, the correctness \nof two proposed compilation schemes for the C++11 load and store concurrency primitives to Power assembly, \nhaving noted that an earlier proposal was .awed. (The main ideas apply also to ARM, which has a similar \nrelaxed memory architecture.) This should inform the ongoing development of production compilers for \nC++11 and C1x, clari.es what properties of the machine architecture are required, and builds con.dence \nin the C++11 and Power semantics. Categories and Subject Descriptors C.1.2 [Multiple Data Stream Architectures \n(Multiprocessors)]: Parallel processors; D.1.3 [Con\u00adcurrent Programming]: Parallel programming; F.3.1 \n[Specifying and Verifying and Reasoning about Programs] General Terms Languages, Reliability, Standardization, \nTheory, Veri.cation Keywords Relaxed Memory Models, Semantics 1. Introduction Most work on semantics \nand reasoning for shared-memory concurrency has assumed a sequentially consistent (SC) mem\u00adory [Lam79], \nin which a single shared memory is acted upon by interleaved threads. In practice, however, mainstream \nmultiproces\u00adsors (x86, Sparc, Power, ARM, Itanium) provide weaker and more Permission to make digital \nor hard copies of all or part of this work for personal or classroom use is granted without fee provided \nthat copies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. POPL 12, January 25 27, 2012, Philadelphia, \nPA, USA Copyright c &#38;#169; 2012 ACM 978-1-4503-1083-3/12/01. . . $10.00 subtle relaxed memory models, \nto permit various hardware optimi\u00adsations. Mainstream concurrent programming languages (e.g. Java, C, \nC++) also provide relaxed memory models, although of rather different kinds, both to permit compiler \noptimisations and so that they can be compiled to those processors without excessive use of hardware \nmemory barriers. Moreover, for several of those multi\u00adprocessors and languages, the actual memory model \nprovided has long been poorly speci.ed and not well understood. In recent years that situation has improved. \nOf par\u00adticular relevance here, the ISO C++ Standards Committee (JTC1/SC22/WG21) has introduced a memory \nmodel, as out\u00adlined by Boehm and Adve [BA08], into their revised standard (C++11) [Bec11], and it is \nexpected that the upcoming revision of the C standard (informally, C1x) will adopt essentially the same \nmodel. This standard is written in prose, subject to the usual prob\u00adlems of ambiguity and not directly \ntestable, but in previous work we produced a formal semantics for C++11 concurrency [BOS+11]. In discussion \nwith members of the WG21 concurrency subgroup, we identi.ed various issues with earlier drafts [BOS+10], \npropos\u00ading solutions that are now incorporated into the new ISO C++11 standard; the result has a close \ncorrespondence between the formal semantics and the standard prose. The C++11 memory model is an axiomatic \none: presuming a threadwise operational semantics that de.nes candidate execu\u00adtions consisting of the \nsets of memory read and write events for threads in isolation, it de.nes when such a candidate is consistent \nand whether it has a race of some kind; consistency is de.ned in terms of a happens-before relation. \nIt is a data-race-free (DRF) model [AH90]: for a program that has no race in any consistent execution, \nthe semantics is the set of all such, while other pro\u00adgrams are unde.ned (and implementations are unconstrained). \nThe design is strati.ed: there is a sublanguage with sequentially con\u00adsistent atomic operations which \nis intended to have simple seman\u00adtics for common use, then additional release/acquire and relaxed atomic \noperations (needing less hardware synchronisation to im\u00adplement) for expert use in high-performance concurrency \nlibraries and system code, and .nally release/consume atomics, for maxi\u00admum performance on architectures \nsuch as Power and ARM where dependencies provide useful ordering guarantees. The whole is rel\u00adatively \ncomplex for example, to accommodate release/consume atomics, it involves a happens-before relation that \nis, by design, not necessarily transitive. The .rst contribution of this paper is to simplify the C++11 \nmodel, proving that it is equivalent to a simpler model M1, without the technical concept of visible \nsequences of side effects . Subset\u00adting the language to remove release/consume atomics gives a fur\u00adther \nsimpli.cation, M2, with a transitive happens-before relation. Subsetting still further, to the fragment \nwithout low-level atomics, one would like to know that that model is equivalent to a sequen\u00adtially consistent \nmodel M3. In Section 4 we show that this holds for programs that are race-free in M2, but that the M2 \nand M3 notions of race can differ, with an example involving atomic initialisation; we give additional \nconditions under which the two do coincide.  Meanwhile, on the hardware side, Sarkar et al. [SSA+11] \nhave established a rigorous memory model for Power multiprocessors, based on extensive testing and discussion \nwith IBM staff. Power has a more relaxed and more subtle model than Sparc TSO or x86-TSO, but it and \nARM are similar in this respect, and prelimi\u00adnary testing and discussion with ARM staff suggests that \nthe same model, with minor differences, applies there (Itanium is also quite relaxed but rather different). \nThis model is of a very different kind to that for C++11: it deals with the Power dependencies and barriers \n(sync, lwsync, isync) rather than the C++11 atomic primitives; it gives semantics to all programs, not \njust race-free programs; and the presence of speculative execution is observable to the program\u00admer on \nthese machines. Accordingly, this model is in an abstract\u00admachine style rather than an axiomatic style: \nit de.nes labelled transition systems for a thread semantics (with explicit speculation) and for a storage \nsubsystem (abstracting from the cache hierarchy), with interactions between the two. Between programming \nlanguage and processor, how can one compile a programming language with the C++11 memory model to Power \nor ARM? There are three main parts to this question: one has to consider how each C++11 atomic operation \nis compiled to assembly code, perhaps with barriers and/or preserved dependen\u00adcies; when conventional \ncompiler optimisations are legal in this setting; and what new optimisations are needed, e.g. to remove \nre\u00addundant barriers. In the relaxed-memory-concurrency setting, all of these are suf.ciently delicate \nthat it is hard to have any con.dence in correctness without proof. We focus here on the .rst. A naive \ncompilation scheme would be to insert the heavyweight Power sync barrier between every C++11 memory access. \nEven here, it is not at all obvious that this would be a correct implemen\u00adtation (indeed, on Itanium, \ninserting the mf barrier between every access is not suf.cient to regain SC), and for a realistic implemen\u00adtation, \nwith good performance, it is essential not to introduce un\u00adnecessary synchronisation. McKenney and Silvera \nhave proposed a mapping of the core C++ atomic primitives to Power assem\u00adbly [MS11]. For the different \n.avours of (nonatomic and atomic) loads and stores, this is: C++11 Operation Power Implementation Non-atomic \nLoad Load Relaxed Load Consume Load Acquire Load Seq Cst Non-atomic Store Store Relaxed Store Release \nStore Seq Cst ld ld ld (and preserve dependency) ld; cmp; bc; isync sync; ld; cmp; bc; isync st st lwsync; \nst sync; st This compiles some C++11 operations to normal Power loads and stores, some with a Power \nlightweight synchronisation barrier lwsync or heavier sync, and in some cases uses just a combination \nof an arti.cial control dependency (of a compare cmp and condi\u00adtional branch bc)and an isync. Our second \ncontribution is to prove the above mapping correct. While the mapping involves only a few assembly instructions, \nits correctness involves much of the behavioural subtlety of the C++11 and Power relaxed memory models, \nand is far from obvious. In\u00addeed, the mapping above has been updated in the light of our com\u00adments following \nSarkar et al. [SSA+11]: an earlier proposal in\u00adcluded an optimisation that we observed is not sound w.r.t. \nthe Power architectural intent (we give two counterexamples here). Our proof relies on the simpli.ed \ngeneral C++11 model M1 from the .rst part. Along the way we obtain results about the Power archi\u00adtecture \nof independent interest, e.g. showing that (and why) insert\u00ading a sync between all accesses does restore \nSC. We also prove soundness of an alternative mapping, with the SC atomics imple\u00admented as below the \ntrailing sync convention, suggested (in the ARM context, with DMB in place of sync and lwsync)by Boehm \n[Boe11]: C++11 Operation Power Implementation Load Seq Cst ld; sync Store Seq Cst lwsync; st; sync Depending \non the relative frequencies of SC loads and store, and on the costs of the different hardware barriers \n(which may vary signi.cantly from one architecture to another, and also between implementations), this \nmay have better performance. Note that for code produced by different compilers to correctly interoperate, \nthey must all make the same choice of mapping, which should be part of the ABI. We also show that both \nthese mapping are local optima: weak\u00adening any of the clauses (e.g. replacing a sync by a lwsync) yields \nan unsound scheme. In the .rst contribution we covered the full C++11 primitives as in [BOS+11], but \nfor these correctness results we focus just on the load and store operations in the tables above, omitting \ndynamic thread creation, C++11 fences, read-modify-writes, and locks. The .rst two are minor simpli.cations, \nbut the last two are substantial questions for future work, as their Power implementations involve the \nload-reserve/write-conditional instructions, for which we do not yet have a well-established relaxed-memory \nsemantics. To make this paper as self-contained as possible, we give a brief introduction to our source \nand target starting points, the C++11 [BOS+11] and Power [SSA+11] models, in Sections 2 and 6 respectively. \nSection 3 describes our simpli.cations of C++11 and Section 4 the atomic initialisation issue; Section \n5 discusses the correctness of the above C++11-to-Power compilation schemes informally, looking at speci.c \nexamples; and Section 7 discusses our correctness statement and proof that they are correct in general. \nWe discuss related work and conclude in Sections 8 and 9. We focus throughout on the key ideas, as it \nis not possible to include full details here (the model de.nitions alone for C++11 and Power are around \n15 and 50 pages of typeset mathematics), but our de.nitions and proofs are available on-line [BMO+]. \nOur de.nitions are expressed in Lem [OBZNS11], a lightweight lan\u00adguage of machine-typechecked type, function, \nand relation de.\u00adnitions, from which one can generate LaTeX, prover de.nitions in HOL4, Isabelle/HOL, \nand (in progress) Coq, and executable OCaml code. Our proofs are a combination of machine-checked interactive \nHOL4 proof and rigorous hand proof (with auxiliary de.nitions and lemma statements in Lem). This is a \ndomain in which rigorously proved results are of di\u00adrect interest to mainstream industrial software development, \nwhich typically relies almost entirely on testing for assurance. The non\u00addeterminism of relaxed memory \nmultiprocessors makes effective random testing challenging, and the fact that the programming lan\u00adguage \nand processor architecture speci.cations are looser than cur\u00adrent implementations makes it insuf.cient \neven in principle. Com\u00adpiler groups are now implementing C++11 concurrency, and we are in ongoing discussion \nwith members of the Red Hat GCC and ARM groups on how this should be done correctly, based in part on \nthe work we present here (e.g., one early-draft implementation was found to put the SC store sync on \nthe wrong side of the store).  2. Background: C++11 concurrency This section gives a general introduction \nto the C++11 concurrency primitives and their semantics. We refer the reader to [BOS+11, Bec11, BA08] \nfor a full discussion of many subtle points that we cannot cover here.  2.1 The Language: C++11 concurrency \nprimitives C++11 is a shared-memory-concurrency language with explicit thread creation and implicit sharing: \nany location might be read or written by any thread for which it is reachable from variables in scope. \nIt is the programmer s responsibility to ensure that such accesses are race-free. For example, in the \nfollowing program the spawned thread, executing thread body(&#38;x), writes x while the main thread reads \nx, without any synchronisation between the two. These are non-atomic reads and writes, and this constitutes \na data race; the fact that the program exhibits an exe\u00ad #include <thread> using namespace std; void thread_body(int* \nint main() { int x = 1; p) {*p = 2;} cution containing a race thread t1(thread_body, &#38;x); makes the \nprogram un\u00ad int r = x; de.ned according to the t1.join(); standard: an implemen\u00ad return 0; } tation \nhas freedom to behave arbitrarily (in particular, compilers can perform optimisations that are not sound \nfor racy programs, which means that common thread-local optimisations can still be done without the compiler \nneeding to determine which accesses might be shared). The language provides several mechanisms for concurrent \npro\u00adgramming without races. First, accesses to shared state can be pro\u00adtected using mutex locks of various \nkinds, e.g. using m.lock() and m.unlock(). We then have various sequentially consistent atomic operations \non objects of integral type (bytes, integer types, etc.): atomic loads, stores, and various read-modify-write \nopera\u00adtions. Atomic operations do not race with each other, by de.nition, so changing the above to use \nSC atomic operations for x gives a well-de.ned program, and the allowed behaviours are intended to be \nthose one would naively expect in an SC semantics. The inten\u00adtion is that locks and SC atomics will suf.ce \nfor most program\u00admers. However, making all racy accesses sequentially consistent is not always required \nand can incur a signi.cant performance cost, so C++11 also provides several low-level atomic operations \n(with different memory order parameters) for expert use, e.g. in concur\u00adrency libraries and other systems \ncode. For message-passing id\u00adioms, in which one thread writes some (perhaps multi-word) data x and then \na .ag y, while another waits to see that .ag write before reading the data, it suf.ces to make the .ag \nwrite a release and the .ag read an acquire atomic: int x; atomic<int> y(0); // sender thread T0 x=1; \ny.store(1,memory_order_release); // receiver thread T1 while (0==y.load(memory_order_acquire)) {}; int \nr=x; The synchronisation between the release and acquire ensures that the sender s write of x will be \nseen by the receiver, i.e., that the read of x must be from the write x=1 rather than from some uninitialised \nvalue. On multiprocessors with weak memory orders, notably Power and ARM, release/acquire pairs are cheaper \nto implement than SC atomics but still signi.cantly more expensive than plain stores and loads (c.f. \nthe preceding lwsync for the release and following cmp; bc; isync for the acquire in Section 1; we return \nlater to how these act). However, these architectures do guarantee that certain dependencies in an assembly \nprogram are respected, and in many cases those suf.ce on the reader side. This motivates a release/consume \nvariant of release/acquire atomics. For example, suppose one thread writes some data (again perhaps multi-word) \nthen writes the address of that data to a shared atomic pointer, while the other thread reads the shared \npointer, dereferences it and reads the data. int x; atomic<int *> p(0); // sender thread x=1; p.store(&#38;x,memory_order_release); \n // receiver thread int* xp = p.load(memory_order_consume); int r=*xp; Here the combination of the release/consume \npair and the de\u00adpendency at the receiver from the read of p to the read of x suf.ces to ensure the intended \nbehaviour. Finally, in some cases one needs only very weak guarantees, and here memory_order_relaxed \natomics can be used. The language also includes explicit fences, in acquire, release, acquire-release, \nand SC forms, to ease porting of older fence-based code.  2.2 Semantics: the C++11 axiomatic memory \nmodel Traditional shared-memory operational semantics involves an ex\u00adplicit memory, with interleaved \ntransitions by threads reading and writing that memory. That is intrinsically SC, and for a relaxed\u00admemory \nlanguage such as C++11 a quite different semantic style is needed. We say a candidate execution ex of \na C++ program comprises a set of memory actions together with various relations over them. The actions \nare of the form: action ::= a:Rna x=v non-atomic read | a:Wna x=v non-atomic write | a:Rmo x=v atomic \nread | a:Wmo x=v atomic write | a:RMWmo x=v1/v2 atomic read-modify-write | a:L x lock | a:U x unlock \n| a:Fmo fence Here a is a unique id (including a thread id), x a location, and v a value. Memory orders \nmo range over those mentioned earlier, abbreviated thus: SC, RLX, REL, ACQ, CON,and A/R. The relations \ninclude four binary relations determined by a threadwise operational semantics: sequenced-before (sb), \naddi\u00adtional synchronises with (asw), data dependency (dd), and control dependency (cd); together with \nthree that determine an interrela\u00adtionship between memory actions of different threads: reads-from (rf) \nrelates a write to all the reads that read from it; the sequential consistent order (sc) is a total order \nover all SC actions; and mod\u00adi.cation order (mo) is the union of a per-location total order over writes \nto each atomic location. Given a candidate execution, the semantics de.nes various derived relations: \nrelease-sequence, hypothetical-release\u00adsequence (a variant of release-sequence used in the fence semantics), \nsynchronizes-with (sw), carries-a-dependency-to (cad), dependency-ordered-before (dob), inter-thread-happens\u00adbefore \n(ithb), happens-before (hb), visible-side-effect,and visible-sequences-of-side-effects. These are used \nto de.ne when a candidate execution is consistent and whether it contains a race of some kind: a data \nrace, unsequenced race,or indeterminate read, which we collect into a predicate race ex . A data race \nis a pair of accesses to the same address, at least one of which is a non-atomic write, which are not \nhappens-before related. The consistent reads from mapping check has subclauses as on the right below. \n consistent execution = consistent reads from mapping = well formed threads. consistent non atomic read \nvalues. consistent locks. consistent atomic read values. consistent inter thread hb. coherent memory \nuse . consistent sc order. rmw atomicity . consistent modi.cation order. sc reads restricted . well formed \nreads from mapping. sc fences heeded consistent reads from mapping For a program c prog for which all \nits consistent executions are race-free with respect to all three kinds of race (which we write as drf \nc prog), the semantics is the set of all those con\u00adsistent executions; other programs are unde.ned. For \nexample, the release-acquire program excerpted above is race-free, and one of its consistent executions \nis shown below (suppressing the ini\u00adtialisation write i:Wna y=0). To give some .avour of the seman\u00adtics, \nwe describe why this execution is indeed consistent, refer\u00adring the reader to [BOS+11] for the detailed \nde.nitions. The well formed threads and well formed reads from mapping predi\u00adcates are straightforward \nsanity conditions. For this simple ex\u00adample the inter-thread-happens-before (ithb) and happens-before \nrelation (hb) coincide, and are just the transitive closure of the union of sequenced-before (sb) and \nthe synchronised-with (sw) edge created by the release/acquire pair (b, c).The con\u00adsistent inter thread \nhappens before predicate checks that this is acyclic. The consistent locks and con\u00adsistent sc order checks \nare vacuous, as a:Wna x=1 there are no lock or SC atomic opera-sb,hb tions in this example. The modi.cation \norder (mo) is empty, as there is only one write to the atomic location, so consis\u00adtent modi.cation order \nis vacuous. For consistent reads from mapping, the main subclause is consis-d:Rna x=1 tent non atomic \nread values, which checks that the non-atomic read of x takes the value of a visible side effect, which \nin a race-free program is unique and is the most re\u00adcent write in happens-before. Then consistent atomic \nread values permits the atomic read of y to read from any element of a visible sequence of side effects, \nwhich is a set of writes of y, ordered by modi.cation order, headed by a visible side effect and terminated \nbefore any write that happens-after the read. The coherent memory use does not contribute because the \nhappens\u00adbefore condition already enforces coherence here. The remaining three (rmw atomicity, sc reads \nrestricted, sc fences heeded)are vacuous, as there are no RMW operations or SC atomics.   3. Simplifying \nthe C++11 model In this section, we establish two simpli.cations of the C++ model of [BOS+11]. The standard \ntries to give an intuitive de.nition of which values an atomic read might take using visible sequences \nof side effects. As Batty et al. [BOS+11, BOS+10] note, taken alone they are too weak, and additional \ncoherence axioms (now incor\u00adporated into the ISO C++ Standard) are necessary to capture the intended \nsemantics. Given those, we can prove that the concept of visible sequences of side effects is unnecessary, \npermitting a much simpler de.nition of consistent atomic read values in a model M1: consistent atomic \nread values1 = .b.actions. (is read b . is at atomic location lk b) . (if (.a vse.actions. (a vse, b) \n. vse) then (.a.actions. ((a, b) . rf ) .\u00ac ((b, a) . hb)) else \u00ac (.a.actions. (a, b) . rf )) THEOREM \n1. A C++11 candidate execution is consistent in C++ iff it is consistent in M1, and the races are identical. \n[Proof: the happens-before relations coincide; mechanised in HOL4] This has already been useful in automatic \nanalysis [BWB+11], as it removes the only use of an existentially quanti.ed set of sets of actions in \nthe semantics. Our second simpli.ed model, M2, removes the complications introduced by atomic consume \noperations (included only for expert programmers on Power and ARM) for the subset of the language without \nthem. The original C++ model and M1 have a complex, non-transitive de.nition of happens-before, from \nthe standard: ithb1 = let r = sw . dob . (sw; sb) in (r . (sb; r))+ hb1 = sb . ithb If we subset the \nlanguage to remove atomic consume operations, we can give an equivalent model M2 with a much simpler \nhb2 =(sb . sw)+ THEOREM 2. A C++11 candidate execution without consume op\u00aderations is consistent in M1 \niff it is consistent in M2, and the races are identical. [Proof: the two hb relations coincide; HOL4] \n 4. Atomic initialisation and SC semantics Subsetting further to remove release/acquire and relaxed \nopera\u00adtions, leaving just nonatomics and SC atomics, one would like to prove a result along the lines \nof Boehm and Adve [BA08], show\u00ading that for such programs the M2 semantics (and hence M1 and Standard \nC++) and an SC-for-DRF-programs semantics are equiv\u00adalent. Our third simpli.ed model, M3, has the obvious \nSC notion of consistent execution, with a total order over all operations, and requiring reads to read \nthe most recent writes in that. For de.ning data races, it uses the same style of de.nition as C++11, \nusing a happens-before relation, but calculates that from the total order. To prove the result, we would \nneed to show that a program is race-free in M2 iff it is race-free in M3, and, for any race-free program, \nthe sets of consistent executions are the same. Unfortunately, this is not true in general. Given a program \nwhich is race-free in M2, we can show that it is race-free in M3 and that it has the same consistent \nexecutions in both: THEOREM 3. drf2 c prog . drf3 c prog . .ex. opsem c prog ex . (consistent ex2 ex \n.. consistent ex3 ex) (Herewewrite opsem c prog ex to mean that candidate execution ex is admitted by \nthe threadwise operational semantics opsem for C++11 program c prog.) This is an easy corollary of the \nfollowing lemma: LEMMA 4. 1. consistent ex2 ex . consistent ex3 ex . race2 ex 2. consistent ex3 ex . \nconsistent ex2 ex . race3 ex 3. consistent ex3 ex .opsem c prog ex . consistent ex2 ex .  ''' ' .ex. \nconsistent ex2 ex . race2 ex. opsem c prog ex [Proof: the .rst two are straightforward; for the third, \nwe can identify a minimal race in the M3 total order and transfer that to a consistent execution in M2.] \nHowever, the other direction fails, as the program below shows. In the SC M3 semantics, it is race-free, \nbut in M2 it has a consistent execution that has races. The example relies on the fact that in C++11 \nthere can be non-atomic writes to atomic locations: the initialisation of atomic locations are non-atomic \n(so that they can be implemented without fences).  atomic<int> x(0); atomic<int> y(0); if (1==x.load(memory_order_seq_cst)) \natomic_init(&#38;y,1); if (1==y.load(memory_order_seq_cst)) atomic_init(&#38;x,1); In the SC M3 semantics, \nboth loads must read 0 and so nei\u00adther of the (non-atomic) atomic init writes can take place; the program \nhas no races and so has de.ned behaviour. But in M2, there is a consistent execution in which both loads \nread 1.This execution has races, and so in M2 the program s behaviour is un\u00adde.ned. (This execution also \nhas dynamic re-initialisation; the pro\u00adgram might also be considered unde.ned in M2 for that reason.) \nThis issue can be eliminated by requiring atomics to be ini\u00adtialised when constructed, removing atomic \ninit, and requiring that atomics are accessed via a path through sequenced-before and reads-from corresponding \nto data-.ow from their creation (i.e., not by forging pointers). Under these additional restrictions, \nwe have proved the desired equivalence of M2 and M3, and hence that for this fragment C++11 has SC-for-DRF-programs \nsemantics. THEOREM 5. A C++11 candidate execution that uses only nonatomic operations, SC-atomic operations, \nand locks, and that satis.es the conditions above, is consistent in C++11 iff it is con\u00adsistent in M3, \nand the races are identical.  5. Correctness of the mapping, informally We now instantiate the .rst \nmapping of \u00a71 for some key examples, giving an introduction to the behaviour that Power does and does \nnot guarantee for them, and hence explaining informally why the mapping is correct in these speci.c cases. \nWe also show that both mappings are locally optimal. In C++11, synchronisation effects are associated \nto atomic loads and stores with particular memory order parameters, but Power assembly language has just \nplain loads and stores; it provides the sync, lwsync,and isync instructions, together with certain dependencies, \nto constrain the otherwise highly relaxed memory model. We introduce these constraints and the model \nby example here, the key properties we depend on are summarised in \u00a76. Ap\u00adplying the \u00a71 mapping to the \nC++11 release-acquire example of \u00a72.1 gives a Power assembly program as follows: y=0 T0 T1 r1=1; r2=&#38;x; \nr3=&#38;y r2=&#38;x; r3=&#38;y a: stw r1,0(r2) b: lwsync c: stw r1,0(r3) write x=1 from write-rel write \ny=1 loop: d: lwz r4,0(r3) e: cmpwi r4,0 f: beq loop g: isync h: lwz r5,0(r2) read y from read-acq read \nx Here stw is a store, lwz a load, cmpwi a compare-immediate, and beq a conditional branch. This is \nessentially the same as the MP+lwsync+ctrlisync example of [SSA+11]: it has an lwsync between the two \nThread 0 writes and an isync before the sec\u00adond Thread 1 read that follows a conditional branch that \nis data\u00addependent (via the compare) on the .rst Thread 1 read; we call the latter a control-isync relationship. \nFor this to be a correct implementation, that Power synchronisa\u00adtion must be strong enough to exclude \nany outcomes that the C++11 semantics forbids. In particular, it must prevent the Thread 1 read of x \nfrom reading the initial state instead of from the Thread 0 write of x=1.The lwsync and control-isync \nare both necessary and suf.cient. The lwsync keeps the two Thread 0 writes in or\u00adder as observed by any \nother thread (otherwise, as they are to dif\u00adferent locations, they might be re-ordered in the storage \nhierar\u00adchy, or even committed out-of-order). Replacing the lwsync by an isync would still permit the \nformer reordering, as is observable on Power 6 and 7 processors (MP+isync+ctrlisync), and just by typ\u00ading \nit cannot be replaced by a control-isync, as one cannot have a dependency from a write. Meanwhile, the \ncontrol-isync relation\u00adship ensures that the read of x cannot be satis.ed until the isync is committed, \nwhich requires the program-order-previous branch to be committed, which requires the read of y=1 to be \ncommitted. Without the isync, the Power architecture permits a processor to speculatively satisfy the \nread of x out-of-order, before satisfy\u00ading the program-order-previous read(s) of y, despite the intervening \nconditional branch. It can thereby read from the initial state, and this is observable in practice, for \na litmus test MP+lwsync+ctrl. (That test is also C++11-expressible: it is essentially the result of applying \nthe mapping to the release-acquire example with the read-acquire replaced by a read-relaxed.) The isync \nalone, with\u00adout the control dependency from the preceding read to a condi\u00adtional branch, also would not \nsuf.ce: in this case the isync could be committed .rst, enabling read h to be satis.ed and commit\u00adted \nbefore anything else occurs. We have not observed this out\u00adcome in practice on current Power 6 or Power \n7 processors (for test MP+lwsync+isync), but it is architecturally permitted. For the analogous ARM test \nMP+dmb+isb, the outcome is observable. Now consider the release-consume example of \u00a72.1, mapped to: p=0 \nT0 T1 r1=1; r2=&#38;x; r3=&#38;p r3=&#38;p a: stw r1,0(r2) write x=1 b: lwsync from write-rel c: stw \nr2,0(r3) write p=&#38;x d: lwz e: lwz r4,0(r3) r5,0(r4) read p read *xp The writing side has an lwsync \nas before, but now the reading side has no control-isync. It is correct nonetheless, because the Power \narchitecture respects address dependencies between loads: here the read e reads from an address in register \nr4 which takes its value from the program-order-previous read d. In such a case the reads must be satis.ed \n(and indeed also committed) in program order. The above examples involve only two threads, which turns \nout to be a very special case. With three or more threads, one can ob\u00adserve that on Power a write does \nnot necessarily become visible to the other threads atomically, and indeed can be propagated to them \nin different orders, or even never propagated to some threads. Any transitive reasoning across multiple \nthreads relies on the cumula\u00adtivity of the Power sync and lwsync barriers. Consider a three\u00adthread version \nof the release-acquire example, with release-acquire synchronisation between the .rst pair of threads \nand between the second pair: int x; atomic<int> y(0); atomic<int> z(0); T0 x=1; y.store(1,memory_order_release); \nT1 while (0==y.load(memory_order_acquire)) {}; z.store(1,memory_order_release); T2 while (0==z.load(memory_order_acquire)) \n{}; r=x; The mapping introduces control-isyncs (from the load-acquires) and lwsyncs (from the store-releases) \nas in the candidate Power execution illustrated below: Thread 0 Thread 1 Thread 2 a: W[x]=1 c: R[y]=1 \ne: R[z]=1 ctrlisynclwsync rf b: W[y]=1 d: W[z]=1 f: R[x]=0  Here the effect of the Thread 1 control-isync \nis subsumed by the following lwsync (eliminating the former is likely to be an impor\u00adtant compiler optimisation). \nThat leaves the Thread 2 control-isync, which acts just as in the .rst release-acquire example, and the \ntwo lwsyncs. To rule out the C++11-forbidden execution shown, we need to know that the writes a and d, \nby different threads and to different locations, are propagated to Thread 2 in that order. By the thread-local \nproperty of lwsync used above, the Thread 1 lwsync ensures that a and b are propagated to Thread 1 in \nthat order, so a must have been propagated to Thread 1 before c reads from b and hence before the Thread \n1 lwsync is committed. This means that a is in the Group A of that lwsync, the set of writes that have \nbeen propagated to its thread already. In turn, the cumulativity of Power lwsyncs means that a must be \npropagated to any other thread, e.g. Thread 2, before the lwsync is, and hence before write d is. Now \nconsider the classic four-thread IRIW example (Indepen\u00addent Reads of Independent Writes). Here Threads \n0 and 2 write to two different locations; the question is whether Threads 1 and 3 can see those writes \nin opposite orders, one reading x=1 then y=0 while the other reads y=1 then x=0. atomic<int> x(0); atomic<int> \ny(0); T0 x.store(1,memory_order_release); T1 r1=x.load(memory_order_acquire); r2=y.load(memory_order_acquire); \nT2 y.store(1,memory_order_release); T3 r3=y.load(memory_order_acquire); r4=x.load(memory_order_acquire); \n With release-acquire atomics, this is permitted in C++11 (this is clearly not a sequentially consistent \noutcome, and so release\u00adacquire atomics do not guarantee SC), and indeed applying the mapping gives essentially \nthe Power test IRIW+lwsyncs, which is architecturally permitted and observable in practice. With SC atomics, \non the other hand, this non-SC outcome is forbidden in C++11. To forbid it in the implementation requires \na Power sync between both pairs of SC-atomic reads, not just an lwsync.The sync is stronger in that its \nGroup A writes (or some coherence-successors thereof), including any program-order\u00adprevious writes, must \nbe propagated to all threads before the sync completes and subsequent memory access instructions can \nbegin. A sync is also needed between the other three combinations of an SC atomic read or write, as shown \nby our SB+lwsyncs and R01 examples [SSA+11] for write-to-read and write-to-write, and by the WRW+WR+lwsync+sync \nexample below for the read-to\u00adwrite case, all of which are architecturally permitted. SB+lwsyncs is also \nobservable on current implementations, whereas we have not observed the last two. Both of the latter \ncan be explained by the fact that a Power coherence edge does not add writes into the Group A of a subsequent \nbarrier (WRW+WR is similar to R01 but with the initial write pulled back along an rf edge). They are \ncounterexamples to the correctness of a previous version of the mapping, which proposed an lwsync in \nplace of the sync for SC stores in normal cacheable memory. Thread 0 Thread 1 Thread 2 a: W[x]=1 b: \nR[x]=1 d: W[y]=2 rf lwsync c: W[y]=1 rf e: R[x]=0 Test WRW+WR+lwsync+sync : Allowed The .rst \u00a71 mapping \ncreates these intervening syncs by putting a sync before the load or store of an SC atomic. One could \nequally well put them after, as in the second mapping, though in C++11 SC atomics also have release/acquire \nsemantics, so one then also needs a preceding lwsync for the SC atomic store (in the .rst mapping, the \npreceding sync does duty for both purposes). Using the above examples, and some others, we can show: \nTHEOREM 6. Both the \u00a71 mappings are pointwise locally optimal. PROOF For each possible weakening there \nis a C++11 example with some forbidden behaviour which would map onto a Power program for which that \nbehaviour is allowed, as checked by our cppmem [BOS+11] and ppcmem [SSA+11] tools using automati\u00adcally \ngenerated executable versions of the models. All these exam\u00adples are collected in the supplementary material \n[BMO+]. The load-consume dependency cannot be removed (C++ message-passing with release/consume . Power \nMP+lwsync+po).  The load-acquire control-isync cannot be weakened to just a control dependency (MP+release/acquire \n. MP+lwsync+ctrl), or to just an isync ( . MP+lwsync+isync).  The load-seq-cst sync, whether before \nor after the load, cannot be weakened to an lwsync (IRIW+seq-csts . IRIW+lwsyncs), or to an isync ( . \nIRIW+ctrlisyncs).  The load-seq-cst control-isync in the original mapping cannot be weakened to just \ncontrol (MP+seq-csts . MP+sync+ctrl), or to just isync ( . MP+sync+isync).  The store-release lwsync \ncannot be weakened to isync (MP+release/acquire . MP+isync+ctrlisync).  The store-seq-cst sync, whether \nbefore or after the store, cannot be weakened to lwsync (R01+seq-csts . R01), or to an isync ( . R+isync+sync). \n D  6. Background: the Power memory model In preparation for the correctness proof in the next section, \nwe recall the overall structure of the Power memory model [SSA+11], sketching how the concepts we used \ninformally in \u00a75, e.g. of a write being propagated to a thread, re.ect the actual semantics. As we saw \nthere, the fact that reads can be satis.ed specula\u00adtively, even program-order-after a conditional branch, \nis observ\u00adable. This pushes us towards an operational abstract-machine se\u00admantics, expressed as a parallel \ncomposition of a thread model and a storage subsystem. At any one time, a thread may have many in-.ight \n(uncommitted) instructions; reads can be satis.ed and instructions committed out\u00adof-order, subject to \nconstraints from barriers and dependencies, and uncommitted instructions are subject to restart or abort. \nThe storage subsystem abstracts from the cache and store buffering hierarchy (and does not involve speculation). \nIt does not have an explicit memory, but instead maintains the global accumulated constraints on the \ncoherence order between writes to the same address that have been built up, together with a list for \neach thread of the writes and barriers that have been propagated to that thread. The thread and storage \nsubsystem models are each labelled transition systems (LTSs) synchronising on labels as in the diagram \nabove; their composition gives another LTS with labels: label ::= | FETCH tid ii | COMMIT WRITE INSTRUCTION \ntid ii w  | COMMIT BARRIER INSTRUCTION tid ii barrier | COMMIT READ INSTRUCTION tid ii rr | COMMIT \nREG OR BRANCH INSTRUCTION tid ii | WRITE PROPAGATE TO THREAD wtid | BARRIER PROPAGATE TO THREAD barrier \ntid | SATISFY READ FROM STORAGE SUBSYSTEM tid ii w | SATISFY READ BY WRITE FORWARDING tid ii1 wii2 | \nACKNOWLEDGE SYNC barrier | PARTIAL COHERENCE COMMIT w1 w2 | REGISTER READ PREV tid ii1 reg ii2 | REGISTER \nREAD INITIAL tid ii reg | PARTIAL EVALUATE tid ii Here tid ranges over thread ids, ii over instruction \ninstances, w over write events, rr over read-response events, barrier over sync or lwsync events, and \nreg over Power register names. A Power execution t is simply a trace of abstract-machine states and these \nlabels; we write machine executions power prog for the set of all such for a Power program power prog \n(Power programs, of type POWER PROGRAM, are essentially maps from addresses to assembly instructions, \ntogether with the initial register state, including PC, for each thread.) Key properties required for \nthe proofs of \u00a77 are derived from the preconditions and postconditions of the transitions. As an example, \nbefore a barrier (sync or lwsync) is propagated to a new thread by the BARRIER PROPAGATE TO THREAD transi\u00adtion, \nany writes propagated before it to its original thread (the Group A writes) must .rst be propagated to \nthat thread (by a WRITE PROPAGATE TO THREAD transition). For a sync,the cor\u00adresponding ACKNOWLEDGE SYNC \ntransition cannot .re until that sync is propagated to all threads (and this implies that all its Group \nA writes, or coherence successors thereof, must have been propa\u00adgated to all threads before that in the \ntrace). The combination of a conditional branch and an isync instruction ( ctrlisync ) stops program-order-later \nreads being able to do their SATISFY READ transitions before the branch is resolved and committed. To \nisolate particular events of interest, we let the following metavariables range over transition labels \nfrom a trace as follows: Metavariable Transition labels ranged over wc COMMIT WRITE INSTRUCTION rc COMMIT \nREAD INSTRUCTION bc COMMIT BARRIER INSTRUCTION mc (memory) COMMIT WRITE INSTRUCTION . COMMIT READ INSTRUCTION \nCOMMIT WRITE INSTRUCTION . COMMIT BARRIER INSTRUCTION  7. Correct compilation from C++11 to Power We \nnow prove that the mappings of Section 1 are correct in general, focusing on the .rst mapping and then \ndiscussing the changes required for the second. 7.1 Correctness Statement The usual form of whole-program \ncorrectness statement one might expect for a compiler is an inclusion between the observable be\u00adhaviours \nof the compiled program and those of the source: C++11 semantics c prog C++11 execution observations \ncompilation . Power semantics power prog Power execution observations Here, however, we are proving \ncorrectness of compilation schemes for particular constructs (the C++11 atomic and nonatomic loads and \nstores), not correctness of a speci.c compiler. We therefore want to factor out as much as possible of \nthe de.nition of the source language and of the rest of the compiler, both to remove extraneous complexity \nthat is irrelevant to the relaxed-memory behaviour and to let us state a general result that should be \nre-usable in multiple later compiler correctness proofs. We do so by axiomatising the required threadwise \nproperties of a C++11 operational semantics and a good compiler. For the source language, we take a C++11 \nprogram, of type C PROGRAM, to be a parallel composition of single-threaded programs from an abstract \ntype C PROGRAM THREAD,withan arbitrary threadwise operational semantics for them that satis\u00ad.es certain \nrather mild axioms. An operational semantics opsem is a relation between C PROGRAMs and sets of C++11 \nactions actionsc equipped with the three relevant operational-semantics relations of \u00a72.2: sequenced-before \nsbc, data-dependency ddc,and control-dependency cdc, packaged into a record opc (additional\u00adsynchronises-with \nis empty in the absence of dynamic thread cre\u00adation). Recall that this threadwise operational semantics \nshould leave the values of memory reads unconstrained, as that is handled by the C++11 axiomatic memory \nmodel of \u00a72.2. We return below to the axioms we need. We take a strong intensional notion of observation: \ngiven a Power machine trace t we build read and write events eventst from the COMMIT READ INSTRUCTION \nand COMMIT WRITE INSTRUCTION commit labels, together with a reads-from relation rft . The observation \npredicate obs eq tactionsc rfc requires that there is an exact correspon\u00addence between these eventst \nand the actionsc of a C++11 execution, with isomorphic reads-from relations rft and rfc. Given a Power \ntrace t, it is also straightforward to construct analogues of the threadwise C++11 relations over the \nconstructed events, for program order pot , data/address dependency ddt ,and control dependency cdt ; \nby analysing the trace, we can also identify the events from the same thread that are separated by control-isync, \nsync, or lwsync instructions in the trace, constructing relations ctrlisynct , synct ,and lwsynct . A \ncompiler comp is a function from C PROGRAM to POWER PROGRAM;itsatis.esthreadwise good compilerifforany \nC++11 program c prog and any Power trace of comp c prog,there is a set of C++11 actionsc with associated \nsbc, ddc, cdc,and rfc relations such that 1. the C++11 data has the same observations as the trace: obs \neq tactionsc rfc; 2. the C++11 data is allowed by the threadwise operational\u00adsemantics: opsem c prog \nactionsc opc; 3. the C++11 relations sbc, ddc,and cdc are subsets of the cor\u00adresponding relations pot \n, ddt ,and cdt built from the trace (note that the C++11 sequenced-before relation may not be total within \na thread, as evaluation order is sometimes unconstrained, but the Power program order is total within \na thread); and 4. if one applies the mapping to the C++11 actions actionsc, the result is included in \nthe ctrlisynct , synct ,and lwsynct relations constructed from the Power trace.  For example, a C++11 \naction a:WREL x=1 can be pre\u00adsumed to arise from a source program instruction x.store(e,memory order \nrelease), and so a threadwise good compiler following the mapping should have compiled it into a Power \nlwsync preceding a store, and any trace should contain a corresponding COMMIT BARRIER INSTRUCTION label \nfollowed (perhaps not immediately) by a COMMIT WRITE INSTRUCTION. Finally, we express correctness on \na per-execution basis and can assume that the source program is race-free, otherwise C++11 regards it \nas unde.ned and compilation is unconstrained. That leads us to the following statement of our main theorem: \n THEOREM 7 (Correctness). .comp c prog. threadwise good compiler comp . drf c prog . (let power prog \n= comp c prog in (.t.machine executions power prog. (.actionsc opc rfc moc scc. obs eq tactionsc rfc \n. opsem c prog actionsc opc . consistent ex actionsc opc rfc moc scc))) Our proof follows this structure. \nConsider a Power trace t of a Power program power prog produced by a threadwise-good com\u00adpiler from a \nC++ program c prog. The threadwise good compiler assumption tells us that there are some corresponding \n(up to obs eq) C++11 actionsc, opc,and rfc for which the memory ac\u00adtions of each thread satisfy the threadwise \noperational semantics. We now construct a C++11 modi.cation order moc and sequential consistent order \nscc, and then have to either prove that the C++11 execution comprising all this data is consistent, or \nto construct an\u00adother consistent execution which contains a race, thereby contra\u00addicting the drf assumption. \nThe construction of a C++11 modi.cation order moc is straight\u00adforward, as (like sbc, ddc, cdc,and rfc) \nthere is a direct analogue in the Power trace: the coherence constraints in the storage subsystem state \nincrease monotonically along a trace, so we can just take their union cot , choose an arbitrary per-location \nlinearisation thereof, and restrict to the atomic locations. Construction of a C++11 SC order scc is \nmore involved, as we see below in \u00a77.3. Note that, in contrast to many compiler correctness proofs, this \nis not a simulation argument: the C++11 de.nition of consistent execution is a predicate on complete \ncandidate executions; it is not generated by an operational semantics and does not involve any notions \nof a C++11 whole-program state or transition.  7.2 Analysis of a Power trace w.r.t. C++11 inter-thead-happens-before \nThe C++11 de.nition of consistent execution comprises seven con\u00additions, as listed in \u00a72.2, almost all \nof which rely heavily on the happens-before relation, which is the union of sequenced-before and a complex \ninter-thread-happens-before relation. To establish those conditions, we need to analyse the Power trace \nwith the shape of the inter-thread-happens-before relation in mind, making use of various ordering properties \nof the Power semantics and the fact that the Power program is obtained using the given mapping. In broad \nterms, if there is a C++11 inter-thread-happens-before relation from a write to a read, we need to show \nthat the write has propagated to the reading thread before the read is satis.ed. For read-to-read pairs, \nwe need to show that any write propagated to the thread of the .rst read, before it is satis.ed, has \npropagated similarly. For write-to-write and read-to-write pairs, we need to ensure propagation before \nthe commit of the .nal write. In more detail, we must also allow coherence successors of writes to be \npropagated in their place, must consider propagation of barriers, must refer to the last satisfy label \nof any read that reads from a different thread, and must split the different-thread/same\u00adthread cases. \nTo capture all this, we de.ne a propBefore relation over memory and barrier commit labels. For labels \nfrom different threads, we say: propBefore bc ------. rc iff the barrier has propagated to the reading \nthread before the read is .nally satis.ed, i.e., if there is a BARRIER PROPAGATE label for bc trace-order-before \nthe last SATISFY READ for rc; propBefore wc ------. rc iff the write, or some coherence-successor write, \nhas propagated to the reading thread before the read is .nally satis.ed; and propBefore wc1 ------. wc2 \niff the write of wc1, or some coherence\u00adsuccessor write, has propagated to the thread of wc2 before the \nwc2 commit. For labels from the same thread, we say: propBefore xc ------. rc iff xc is either a write \ncommit label and is read by rc or it is trace-order-before rc is .nally satis.ed; and propBefore xc ------. \nwc iff xc is trace-order-before wc. Consider now the C++11 de.nition of happens-before (hb). Without \nrelaxed or consume operations, it only crosses threads with a reads-from edge of a release/acquire pair \nthat synchronise with each other. Under the mapping, this rft edge must be preceded by synct or lwsynct \nand followed by a ctrlisynct . With relaxed atomics, the acquire might read from something in the release \nse\u00adquence of the release it synchronises with. In Power terms, a re\u00adlease sequence is a series of same-thread \ncot edges (if we cov\u00adered read-modify-write operations, these would not necessarily be on the same thread). \nAdding consume atomics, release/consume synchronisation involves the dependency-ordered-before (dob) \nre\u00adlation, which is similar to ithb except that on the right hand side it only reaches to dependent operations, \nnot to all program-order successors of the read. In Power terms, in this case the ctrlisynct is replaced \nby ddt *. All this motivates the de.nition of a machine analogue of ithb, machine-ithbt : ( )+ re. * \n* re. (synct . lwsynct ); coit ; rfet ;(ctrlisynct . ddt ) where coit is the machine coherence order \nrestricted to pairs of writes by the same thread, and rfet is the machine reads-from relation restricted \nto write/read pairs on distinct threads. LEMMA 8. Given two C++11 memory actions m1 and m2 and the ithbc \ncorresponding Power commit events mc1 and mc2,if m1 ---. machine-ithbt m2 then mc1 --------. mc2. PROOF \nBy a series of lemmas about the C++11 auxiliary relations, unfolding the de.nitions, with reasoning as \nabove. D To prove the required propagation property (Corollary 10 be\u00adlow), we .rst consider the left \npart of this relation: LEMMA 9 (Propagation). * ((synct .lwsynct )re. ;coit ;rfet )+ propBefore 1. If \nwc -------------------. rc,then wc ------. rc. * ((synct .lwsynct )re. ;coit ;rfet )+ 2. If rc1 -------------------. \nrc2, then for any write propBeforepropBefore commit wc ------. rc1, we have wc ------. rc2. PROOF By \ninduction on the transitive closure of the relation. In the base case, by decomposing the large relation, \ntwo intermediate write commit events appear (wc1 and wc2). In the case where synct .lwsynct mc ---------. \nwc1,if mc is a write, then it falls into the Group A of the intervening barrier; if it is a read, then \nany write propagated to its thread before it is satis.ed falls into the Group A of the barrier. The coit \n* relation is a subset of pot , hence we synct .lwsynct also have mc ---------. wc2. Hence, using the \nA-cumulativity property of the Power machine and since we know that wc2 does propagate to the thread \nof rc before rc is last satis.ed (because it is read from), the writes in the Group A of the barrier \n(or some coherence successors) must have previously been propagated to propBefore that thread. This establishes \nthe desired ------. relationship. In the other case, mc = wc1,wejustsaw that wc2 does propagate to the \nthread of rc in time and it is here a coherence-successor of mc. For the inductive case we use the same \nreasoning as for the .rst step to establish that the writes under consideration are .rst propagated to \nthe thread of the intermediate read, and then the induction hypothesis. D  COROLLARY 10 (Propagation \nw.r.t. ithb). machine-ithbt 1. If rc --------. mc then for any write commit wc with propBeforepropBefore \nwc ------. rc, we have wc ------. mc. machine-ithbt propBefore 2. If wc --------. mc then wc ------. \nmc. PROOF ctrlisynct and ddt are subsets of po so, without loss of generality, we can neglect all those \nedges except the last one: * ((synct .lwsynct )re. ;coit ;rfet )+ (ctrlisynct .ddt * ) wc -------------------. \nrc ----------. mc Lemma 9 establishes propagation to the thread of rc, then, using either the ctrlisynct \nor ddt ,if mc is a read then it is satis.ed after rc is committed, and if mc is a write then it is committed \nafter rc is committed. D Several of the C++11 conditions are acyclicity properties (ei\u00adther explicitly \nso, e.g. the consistency of ithb, or requiring consis\u00adtency between relations, e.g. rf c and hbc). To \nestablish these, it is convenient to reduce them to the acyclicity of trace order, by the following lemma. \nLEMMA 11 (Trace order respects machine-ithbt ). machine-ithbt If mc1 --------. mc2 then mc1 is before \nmc2 in the trace. PROOF Along the same lines as the reasoning above, by induction on the transitive closure \nof machine-ithbt . D  7.3 Analysis of a Power trace to construct a C++11 SC order C++11 demands a single \ntotal order sc over all the SC atomic ac\u00adtions, subject to some consistency conditions. Broadly, this \norder must be consistent with the sequenced-before relation, with modi\u00ad.cation order, and for each SC \nread rc, if it reads from a SC write wc,then wc must be the last SC write to that location before rc \nin the SC order, while if it reads from a non-SC write wc ' ,then wc ' cannot be after (in happens-before) \nany SC write to the same location which is itself after rc in the SC order. In Power terms, the .rst \ntwo conditions are straightforward to express: we need to construct a total order on SC actions which \nsc sc includes the subparts of pot and cot (pot and cot respectively) that are restricted to SC actions. \nHowever, there is no obvious cor\u00adresponding total order visible in the trace. In particular, recall from \n\u00a76 that for write commit labels trace order is not necessarily con\u00adsistent with the coherence order cot \n. It is also not suf.cient to use sc sc an arbitrary linearisation of (pot . cot ) * (though pot and \ncot are individually acyclic), since that may introduce bad intervening writes to the same location in \nbetween a pair of a write and a read that reads-from it. Consider then an SC read rc, and the write wc \n(not necessarily SC) that it reads-from, and recall that we have a coherence order cot for writes to \nthat location. We de.ne two new derived relations on SC reads and writes: from-reads (frtsc)de.ned cot \nas rc -fr-tsc . wc1 if wc - . wc1,i.e. rc reads-from a coherence pre\u00addecessoroftheSC wc1; and extended-reads-from \n(erftsc)de.ned erftsc as wc1 ---. rc if wc1 is the last SC write in coherence order equal to or before \nthe wc that rft reads from. From-reads relations have been used by various researchers [ABJ+93, AMSS10, \nAlg10], and the latter reference proves that if the union of program-order, coherence, reads-from, and \nfrom-reads is acyclic, then the execu\u00adtion is an SC execution by the traditional de.nition [Lam79] of \na single global order over all events. We use this characterisation in Theorem 13 below. We will use \nas the SC order an arbitrary linearisation of (potsc . cotsc . frtsc . erftsc) * . We now show that this \ncombination of relations is acyclic, and hence can be consistently extended to a total order. First note \nthat if the above relation has a cycle, sc po t there must be at least one mc1 --. mc2 edge in the cycle, \nas otherwise it is straightforward to deduce that the coherence relation implied by the machine trace \nis itself cyclic. Under the mapping, this means that mc1 and mc2 must be separated in the trace by a \nsync and its ACKNOWLEDGE SYNC transition. Our main lemma for this (Lemma 12 below) says that for any \nmc with sc sc (po .co .frsc .erf sc ) * ttt t mc ---------------. mc1, it must be in trace-order before \nthat ACKNOWLEDGE SYNC transition (call this the S transition), and this contradicts mc2 being after that \ntransition. We will prove Lemma 12 by induction on the length of the relation, case analysing the relation \nin the .rst step. We have to strengthen the inductive hypothesis in two ways to make the proof go through. \nFor example, consider the case where the .rst step is a rc -fr-tsc . wc edge. We know inductively that \nwc is before S in the trace, but this does not immediately help us know that the read rc is also before \nS. What we need is the strong property of the sync (recall \u00a76), that before the acknowledgement for a \nsync in the trace, preceding writes are not merely committed but also they (or some coherence successors) \nare propagated to all threads. This suf.ces, since if wc is propagated to the reading thread, and it \nis not read\u00adfrom, the read must already have been done. For the second strengthening, consider the case \nwhere the .rst rft step is a wc -. rc edge (one kind of an erftsc edge is a rft edge). As above, we will \nhave to inductively establish that the write wc have been propagated to all threads before the sync acknowledge\u00adment \nS. The Power machine guarantees this for any sync on the same thread as rc, but not necessarily for syncs \non other threads. To make the induction work, we will .nd a new intermediate sync acknowledgement Sn, \nwhich is on the same thread as an event re\u00adlated to rc in the potsc-free part of the relation. Since \nthis involves only co sc , frsc and erf sc edges, these will be on the same location, tt t and wc will \nthen be correctly related by coherence to a write that is known to be propagated everywhere before Sn \nin the trace. The .nal statement is then: pot LEMMA 12 (Main Lemma for SC). Suppose mc1 - . mc2, and \nthere is a sync ack S between mc1 and mc2 in the trace. Suppose sc sc (pot .cot .frsc .erf sc ) * tt \nmcn ---------------. mc1. Then there is an mc ' and a n sync ack Sn such that: 1. mcn is before Sn in \nthe trace, and Sn is before or equal to S in the trace; 2. If mcn is a write, then that write or a coherence \nsuccessor has propagated to all threads before Sn in the trace; 3. mcn ' is before Sn in the trace; \n 4. mcn ' and Sn are from the same thread; and  sc (co .frtsc .erf sc ) * tt ' 5. mcn ------------. \nmcn (the subpart of the relation be\u00adtween mcn and mcn ' is pot -free). PROOF By induction on the length \nof the chain sc sc (po .co .frsc .erf sc ) * ttt t mcn ---------------. mc1. In the base case, mcn = \nmc1, and the required conditions are easily established by taking mc ' = mcn and Sn = S. n In the inductive \ncase, we case analyse on the kind of relation in the .rst step. For cotsc , frtsc and erftsc , we use \nthe same intermedi\u00adate event and sync acknowledgement as the inductive hypothesis, making mc ' n = mc \n' n-1 and Sn = Sn-1. Then, for example, if the  sc co t edge is wc1 --. wc2, we have inductively that \nwc2 is propagated to all threads as required before Sn, and thus a coherence successor frtsc erf sc t \nof wc1 is as well. We outlined the rc --. wc and wc ---. rc cases in the description above. sc po t In \nthe remaining case, mcn --. mcn-1,wehavetwo SC events separated in program order. By the mapping, there \nmust be a sync between them in program order. Let Sn be the acknowledge\u00adment transition for that sync, \nand mcn ' = mcn. Then the required conditions are easy to check. D Notice in the proof that we use the \nsync program-order between any two SC actions from the same thread. This is crucial for the induction \nto go through. Indeed, as shown in \u00a75, for any pair of SC actions, if we weaken the sync to anything \nelse (lwsync or weaker), we have a counterexample to show a program with only SC actions behaving in \na non-sequentially consistent manner. The proof is applicable in a wider setting than just for C++11. \nThe key fact we used to create the SC order is that every pair of SC actions on the same thread is separated \n(in program order) by a sync. Looking at the proof for the case that every memory access is a SC atomic \naction (i.e. no other memory types, or non\u00adatomic accesses) shows that such a program has only sequentially \nconsistent behaviour, an interesting fact in its own right about Power assembly programs. THEOREM 13 \n(Syncs between every pair of accesses restore SC). Suppose we have a Power (assembly) program with every \npair of memory accesses on the same thread separated in program order by a sync. Then the program has \nonly sequential consistent behaviour. PROOF Given the premises, the proof of Lemma 12 shows that (pot \n. cot . frt . rft ) * is acyclic, then by [Alg10, \u00a74.2.1.3] the execution is Lamport-SC. D  7.4 Veri.cation \nof consistent execution We now have the tools to verify the satisfaction of all the conjuncts of the \nconsistent execution predicate, as listed in \u00a72.2 and sim\u00adpli.ed by Theorem 1 of \u00a73. The two well formed \nconjuncts hold by the obs eq and assumptions on opsem, e.g. that the sequenced\u00adbefore relation only relates \nactions of the same thread. The consis\u00adtent locks conjunct is vacuous in the sublanguage we consider \nin this section. For the others, we use the correspondences between C++11 actions and relations and Power \ntrace events and relations we established in \u00a77.1 and in \u00a77.2, mostly using Corollary 10 and Lemma 11. \nFirst, consistent inter thread happens before states the acyclicity of ithb. Using Lemma 11, we prove \nby contradiction that such a cycle implies a cycle in the trace order. Next we have four coherence conditions \nfor pairs of a read and a write (CoRR, CoRW, CoWR and CoWW), which are part of consistent modi.cation \norder and coherent memory use (itself part of consistent reads from mapping). These diagrams involve \nrfc and hbc edges relating two writes and some reads, all at the same atomic location; they require the \nwrites to be correctly or\u00addered in moc. For example: LEMMA 14 (CoRW). Given C++11 actions w1, r, and \nw2, all to moc the same location, and an rfc and hbc edge as below, the w1 --. w2 edge exists. rfc w1 \n r hbc moc w2 PROOF To establish this we .rst show that the Power write wc1 corresponding to w1 is \npropagated to the thread of the Power write wc2 corresponding to w2 before wc2 is committed. In more \ndetail: rft For the rfc edge, the inclusion from \u00a77.1 gives us a Power wc1 -. rc relation between the \ncorresponding commit events from the trace. For the hbc edge, by the de.nition of C++11 happens-before, \neither (case 1) there is an sbc edge, in which case we use the pot inclusion from \u00a77.1 to show rc - . \nwc2 (and they are on the same thread), or (case 2) an ithbc relationship, in which case there is an machine-ithbt \nrc --------. wc2 edge. We know (because of the Power rules involved in a reads-from relation, and with \na case split on whether wc1 and rc are on the same thread or not) that wc1 propagates to the thread of \nrc before rc is .nally satis.ed. Hence wc1 is propagated to the thread of wc2 before the latter is committed, \neither (in case (1)) because the semantics of the commit transition guarantees that pot is respected \nby the trace order for events at a same location, and using transitivity, or (in case (2)), by Corollary \n10. Finally, by the storage-subsystem semantics for COMMIT WRITE INSTRUCTION, when a write is committed, \nit automatically becomes coherence-after all the writes that have cot already been propagated to its \nthread, so wc1 - . wc2. And by moc the construction of moc,thatgives w1 --. w2. D The other three coherence \nproperties are similar, albeit administra\u00adtively more complex. Two additionally use the fact that a Power \nread can only be satis.ed from the last write (to the relevant ad\u00address) that has been propagated to \nits thread. The consistent sc order predicate checks that SC is a strict total order over the SC actions. \nThe totality is by construction; strictness is by acyclicity. It also requires that hbc and moc restricted \nto SC atomics are included in scc. The .rst is immediate from the construction of hbc from subsets of \nthe relations involved in the construction of scc. The second makes use of a lemma stating the inclusion \nin cot of the relation from which we build the SC\u00adorder when restricted to pairs of writes at a same \nlocation, and the construction of moc. The consistent modi.cation order predicate consists of CoWW, dealt \nwith above, a totality condition that is immediate from the construction of moc as a linearisation, and \na check that it only relates writes at atomic locations, which is also by construction. Now we have the \nsubclauses of consistent reads from mapping. Two, rmw atomicity and sc fences heeded, are vacuous for \nthe sublanguage we consider. We dealt with the coherence conditions of coherent memory use above. The \npredicate sc reads restricted forces any SC read to read from the last scc-preceding write (wc) at the \nsame location, or some non-SC write that is not hbc-before wc. The proof is by construct\u00ading a SC-cycle \nfor each forbidden situation, which contradicts the results of \u00a77.3. An interesting observation here \nis that, through the application of the acyclicity result of \u00a77.3, we make use for the .rst time of the \nbarriers placed by the mapping before the compilation of SC reads. And in fact, we do not rely anywhere \nelse on these barriers as they play no part in the propagation property stated by Lemma 9. Finally, we \nget to the constraints on read values. The consis\u00adtent non atomic read values predicate says that a read \nr at a non\u00adatomic location must read from a visible side-effect, i.e., a write w that happens-before \nr such that there is no write w ' that happens\u00adbetween w and r. Assuming the contrary, there are three \npossible situations: 1. r reads from a write that happens-after it; 2. r reads from an hb-hidden write: \nthere exists a w ' that happens\u00adbetween w and r;or 3. r reads from an hb-unrelated write.   Cases \n1 and 2 are lengthy but in the same style as the reasoning for CoRW and CoWR, showing that writes propagate \nappropriately. Case 3 is quite different. Here we have a Power read rc that hbchbc reads from a write \nwc for which neither w - . r nor r - . w hold. Intuitively, this is a race, but this is not a consistent \nexecution: hbc r cannot read from w in C++ unless w - . r. In the next section we show that in this case \nthe original program has some other candidate execution that is consistent and that has a race (a data \nrace or indeterminate read), contradicting the top-level assumption that the program was drf. A similar \nsituation arises for the last subclause, consistent atomic read values. Formally, we case split at the \ntop level on the following rf inhb predicate, capturing additional assumptions that ensure consistency: \nrf inhb = .wr. rfchbc (w - . r . r is a non-atomic read . w --. r) . rfc (w - . r . r is an atomic read \n. hbc ' '' (.w . is write w ' . same location ww . w --. r)) If rf inhb holds, case (3) is excluded and \nconsis\u00adtent non atomic read values and consistent atomic read values can be established directly.  7.5 \nConstruction of a racy consistent execution In the situation where the rf inhb predicate of the previous \nsec\u00adtion does not hold, our strategy is to 1) .nd a pre.x of the trace for which rf inhb holds and build \na consistent execution for it by applying the reasoning of Section 7.4 to that pre.x; 2) add an in\u00addeterminate \nor data-race read to the consistent execution; 3) return any missing sequenced-before predecessor actions \nto the consistent execution; 4) extend the consistent execution until it is a complete execution of the \noriginal program. For Step 1, we .nd the .rst read commit rc on the trace that causes a violation of \nrf inhb, and build a consistent execution for part of the trace that precedes it. Our ability to use \nSection 7.4 for this relies on the fact that the commit labels on the trace respect rft , ddt ,and cdt \n, which are in turn consistent with rfc, ddc,and cdc. Steps 2 4 rely on the following receptiveness axiom \nabout the C++ threadwise operational semantics, which we believe any rea\u00adsonable operational semantics \nshould satisfy. ' .c prog actions actionsc opc anew a. c opsem c prog actionsc opc . ' a . (actionsc \n\\ actions ) . new a . all values a . c ' is read a . actions . actionsc . c ' (actions .{a}) is downward \nclosed under (ddc . cdc)+ c . (.new actc new opc. ' let stay actc = actions . c (ddc . cdc )+ ' {b|b.actionsc \n\\ actions . b =.a . \u00ac a ---------. b} c in stay actc n new actc = \u00d8. ((opc|(stay actc .{a}))[new a/a] \n= new opc|(stay actc .{new a})) . (new ddc . new cdc )+ (.xy. x ---------------. y . x . new actc . \ny .. stay actc .{new a}) . opsem c prog (stay actc .{new a}. new actc) new opc) It states that if the \noperational semantics can perform a read at a certain point, it can instead perform any read that is \nof the same kind and from the same location, but that reads a (potentially) different value. This re.ects \nthe fact that the threadwise operational semantics relies on the memory model to determine how memory \nreads are satis.ed. Furthermore, later actions that do not depend on the read via (ddc . cdc)+ are unaffected \nby the change of value read. This re.ects the fact that the threadwise operational semantics is required \nto correctly calculate the notions of control and data dependence. We now return to the construction \nof the racy execution. For Step 2, if there is a visible side effect w ' of r (recall that r is the read \nthat violated hbin rf ), we create a new ' '' read r according to the opsem axiom and have r read from \nw . This r ' then races with the write w that r read from originally. If there is no visible side effect, \nwe have r read from nothing, and the execution has an indeterminate read. We prove that this new execution \nis consistent by showing that the happens before relation is unchanged, except for switching r to r ' \n. Step 3 is necessary because the Power can speculatively exe\u00adcute reads out of program order, and so \nthe pre.x chosen in Step 1 might be missing actions that the C++ threadwise operational semantics requires. \nIn other words, the trace ordering is not nec\u00adessarily consistent with pot , and hence the trace pre.x \nmight not be downward closed in sbc. The addition of such a missing read r in Step 3 follows the same \nreasoning as in Step 2 with the added observation that r cannot be (atomic) sequentially consistent or \nac\u00adquire because the Power cannot speculate past the control+isync dependency that would follow such \na read. The addition of missing writes is straightforward. Step 4 is straightforward because there are \nno constraints on how the execution extended to completion; it already contains the race, and further \napplication of opsem axiom will not disturb it. This completes the proof of Theorem 7. 7.6 Alternate \ntrailing-sync mapping As mentioned in \u00a71, a modi.ed mapping has been proposed that has a sync barrier \nas the last instruction in the mapping for all SC actions. This keeps a sync barrier program-order between \nany pair of SC actions from the same thread, which ensures the results of \u00a77.3 hold. The rest of the \nproof interacts with SC actions through the SC order, so it can be carried over unchanged. One additional \nsubtlety is that SC stores are also release stores, and SC loads in C++ are also acquire loads. The .rst \nfact requires the lwsync be\u00adfore the store for SC stores, as for release stores. The second re\u00adquires \nprogram-order-later loads to be stopped from being satis.ed before the SC load is. The trailing sync \nsuf.ces here.  8. Related Work The most closely related work is our correctness proof of a compi\u00adlation \nscheme from C++11 to x86 [BOS+11]. That covered RMWs in addition to loads and stores, but was nonetheless \ncomparatively simple, as x86 has a strong TSO-based semantics [SSO+10]. For Section 3, Boehm and Adve \ngave a de.nition [BA08] of a mem\u00adory model based on an earlier C++11 working paper, broadly in the same \nstyle as the eventual draft standard and [BOS+11], but dif\u00adfering in many details and with the semantics \nfor low-level atomics only sketched. They give a hand proof that in that model DRF pro\u00adgrams without \nlow-level atomics have SC semantics. Some previous work proves correctness of compilers for con\u00adcurrent \nlanguages, rather than compilation schemes for particular primitives, in a relaxed-memory context. .c\u00b4ik \net al. prove cor\u00ad Sev.rectness of CompCert-TSO [SVZN+11], a compiler from a con\u00adcurrent C-like language \nwith TSO semantics to the x86 TSO model (building on Leroy s single-threaded CompCert [Ler09]). Here \nthe source and target share the same relatively simple memory model, and the correctness proof is simulation-based, \ntaking advantage of the fact that x86-TSO has a simple operational characterisation. Lochbihler proves \ncorrectness of compilation from a formalisation of multi-threaded Java to a JVM [Loc10], but this assumes \nSC and does no optimisation. Sev..c\u00b4ik [. Sev11] proves the correctness (or otherwise) for various optimisations \nfor DRF languages, though not speci.cally for C++11. Burckhardt et al. [BMS10] consider the cor\u00adrectness \nof transformations in relaxed models. Vafeiadis and Zappa Nardelli verify fence elimination optimisations \nin the context of the CompCert-TSO compiler [VZN11].  There is an extensive line of work inserting fences \nto restore SC, starting with Shasha and Snir [SS88]. Fang et al. [FLM03] do this in practice for sync \non Power 3 machines, but they appear to implicitly assume that writes are atomic (discussing only thread-local \nreorder\u00ading), which they are not on more recent Power implementations. Alglave et al. [AMSS10, Alg10, \nAM11] consider Power fence in\u00adsertion with respect to an axiomatic memory model, the CAV2010 model. That \nmodel was a precursor to the abstract-machine model of Sarkar et al. [SSA+11] that we use in this paper. \nAs described there, the CAV2010 model is stronger than the architectural intent for the R01 test (in \nthis sense it is unsound w.r.t. the architecture, al\u00adthough not observed to be unsound w.r.t. current \nimplementations), and it is weaker than the architecture (and current implementations) for cases such \nas MP+lwsync+addr. The latter might be generated by the mapping for a C++ release/consume example, so \nthis model is too weak to prove the mapping correct. Lea produced a guide to implementing the JSR-133 \nJava Memory Model on various multiprocessors [Lea]. Written when the Power architectural intent was less \nclear (considerably be\u00adfore [SSA+11]), its suggested uses of isync are not correct w.r.t. the architecture. \nIt also focuses on the synchronisation re\u00adquired between pairs of operations on the same thread, without \ndiscussing the Power cumulativity properties that are essential for C++11 release/acquire, release/consume, \nand SC atomics.  9. Conclusion We have proved the correctness of a realistic relaxed-memory com\u00adpilation \nscheme, from the programming language memory model proposed for the mainstream C and C++ standards, to \na realistic memory model for Power multiprocessors. The C++11 model was designed with implementation \nabove the Power (and ARM) model in mind, among others, and this establishes that it is in fact im\u00adplementable \nwith what appears to be a reasonable mapping (us\u00ading hardware synchronisation mechanisms commensurate \nwith the C++11 semantics required). Moreover, our development explains why the mapping is correct and \nalso what properties of the Power are actually required, increasing understanding and con.dence in both \nmodels, and providing a good basis for compiler developers compiling C++11 and C1x to Power and ARM; \nwe are discussing this with GCC and ARM compiler groups. There are many interesting directions for future \nwork. First, the development should be extended to cover other C++11 features: read-modify-write and \nlocks operations require .rst a semantics for the Power load-reserve/store-conditional instructions, \nand one would also like to cover fences and dynamic thread creation. Second, while a full compiler correctness \nproof for C++ (or even C) is still a long way off, we would like to instantiate our compilation-scheme \nproof to a concrete operational semantics and compiler for a small fragment. Third, the proof suggests \nthe construction of a more abstract axiomatic Power model, based on the derived properties we use in \nthe proof. We can see there exactly which machine trace events are relevant, e.g. the last satisfy label \nfor each committed read, and certain write propagation events; the more abstract model could deal just \nwith those. This would give a simpler foundation for developing analysis and reasoning techniques for \nPower and ARM concurrent software. Acknowledgements We thank Hans Boehm, Paul McKenney, Jaroslav Sev.c\u00b4ik, \nand Francesco Zappa Nardelli for discussions .on this work, and acknowledge funding from EPSRC grants \nEP/F036345, EP/H005633, and EP/H027351.  References [ABJ+93] Mustaque Ahamad, Rida A. Bazzi, Ranjit \nJohn, Prince Kohli, and Gil Neiger. The power of processor consistency. In Proc. SPAA, 1993. [AH90] S. \nV. Adve and M. D. Hill. Weak ordering a new de.nition. In Proc. ISCA, 1990. [Alg10] J. Alglave. A Shared \nMemory Poetics. PhD thesis, Universite\u00b4Paris 7 and INRIA, 2010. [AM11] J. Alglave and L. Maranget. Stability \nin weak memory mod\u00adels. In Proc. CAV, 2011. [AMSS10] J. Alglave, L. Maranget, S. Sarkar, and P. Sewell. \nFences in weak memory models. In Proc. CAV, 2010. [BA08] H.-J. Boehm and S.V. Adve. Foundations of the \nC++ concur\u00adrency memory model. In Proc. PLDI, 2008. [Bec11] P. Becker, editor. Programming Languages \n C++. 2011. ISO/IEC 14882:2011. A non-.nal but recent version is avail\u00adable at http://www.open-std.org/jtc1/sc22/wg21/ \ndocs/papers/2011/n3242.pdf. [BMO+] M. Batty, K. Memarian, S. Owens, S. Sarkar, and P. Sewell. http://www.cl.cam.ac.uk/users/pes20/cppppc. \n[BMS10] S. Burckhardt, M. Musuvathi, and V. Singh. Verifying local transformations on relaxed memory \nmodels. In CC, 2010. [Boe11] Hans Boehm. Atomic synchronization sequences, 2011. Mailing list communication, \nJuly 18th. [BOS+10] M. Batty, S. Owens, S. Sarkar, P. Sewell, and T. Weber. Math\u00adematizing C++ concurrency: \nThe post-Rapperswil model. Technical Report N3132, ISO IEC JTC1/SC22/WG21, Au\u00adgust 2010. http://www.open-std.org/jtc1/sc22/ \nwg21/docs/papers/2010/n3132.pdf. [BOS+11] M. Batty, S. Owens, S. Sarkar, P. Sewell, and T. Weber. Math\u00adematizing \nC++ concurrency. In Proc. POPL, 2011. [BWB+11] J. C. Blanchette, T. Weber, M. Batty, S. Owens, and S. \nSarkar. Nitpicking C++ concurrency. In Proc. PPDP, 2011. [FLM03] X. Fang, J. Lee, and S. P. Midkiff. \nAutomatic fence insertion for shared memory multiprocessing. In Proc. ICS, 2003. [Lam79] L. Lamport. \nHow to make a multiprocessor computer that cor\u00adrectly executes multiprocess programs. IEEE Trans. Comput., \nC-28(9):690 691, 1979. [Lea] D. Lea. The JSR-133 cookbook for compiler writers. http: //gee.cs.oswego.edu/dl/jmm/cookbook.html. \n[Ler09] X. Leroy. A formally veri.ed compiler back-end. Journal of Automated Reasoning, 43(4):363 446, \n2009. [Loc10] A. Lochbihler. Verifying a compiler for Java threads. In Proc. ESOP 10, 2010. [MS11] P. \nE. McKenney and R. Silvera. Example POWER implementation for C/C++ memory model. http: //www.rdrop.com/users/paulmck/scalability/ \npaper/N2745r.2011.03.04a.html, 2011. [OBZNS11] S. Owens, P. B\u00a8ohm, F. Zappa Nardelli, and P. Sewell. \nLem: A lightweight tool for heavyweight semantics. In Proc. ITP, LNCS 6898, 2011. Rough Diamond section. \n[.Sev. Sev11] J. .c\u00b4ik. Safe optimisations for shared-memory concurrent programs. In Proc. PLDI, 2011. \n[SS88] D. Shasha and M. Snir. Ef.cient and correct execution of parallel programs that share memory. \nTOPLAS, 10:282 312, 1988. [SSA+11] S. Sarkar, P. Sewell, J. Alglave, L. Maranget, and D. Williams. Understanding \nPOWER multiprocessors. In PLDI, 2011. [SSO+10] P. Sewell, S. Sarkar, S. Owens, F. Zappa Nardelli, and \nM. O. Myreen. x86-TSO: A rigorous and usable programmer s model for x86 multiprocessors. C. ACM, 53(7):89 \n97, 2010. [SVZN+11] J. Sev.c.\u00b4ik, V. Vafeiadis, F. Zappa Nardelli, S. Jagannathan, and P. Sewell. Relaxed-memory \nconcurrency and veri.ed compilation. In Proc. POPL, 2011. [VZN11] V. Vafeiadis and F. Zappa Nardelli. \nVerifying fence elimina\u00adtion optimisations. In Proc. SAS, 2011.  \n\t\t\t", "proc_id": "2103656", "abstract": "<p>The upcoming C and C++ revised standards add concurrency to the languages, for the first time, in the form of a subtle *relaxed memory model* (the *C++11 model*). This aims to permit compiler optimisation and to accommodate the differing relaxed-memory behaviours of mainstream multiprocessors, combining simple semantics for most code with high-performance *low-level atomics* for concurrency libraries. In this paper, we first establish two simpler but provably equivalent models for C++11, one for the full language and another for the subset without consume operations. Subsetting further to the fragment without low-level atomics, we identify a subtlety arising from atomic initialisation and prove that, under an additional condition, the model is equivalent to sequential consistency for race-free programs.</p> <p>We then prove our main result, the correctness of two proposed compilation schemes for the C++11 load and store concurrency primitives to Power assembly, having noted that an earlier proposal was flawed. (The main ideas apply also to ARM, which has a similar relaxed memory architecture.)</p> <p>This should inform the ongoing development of production compilers for C++11 and C1x, clarifies what properties of the machine architecture are required, and builds confidence in the C++11 and Power semantics.</p>", "authors": [{"name": "Mark Batty", "author_profile_id": "81479651209", "affiliation": "University of Cambridge, Cambridge, United Kingdom", "person_id": "P2991294", "email_address": "Mark.Batty@cl.cam.ac.uk", "orcid_id": ""}, {"name": "Kayvan Memarian", "author_profile_id": "81496689961", "affiliation": "University of Cambridge &#38; INRIA, Cambridge, United Kingdom", "person_id": "P2991295", "email_address": "Kayvan.Memarian@cl.cam.ac.uk", "orcid_id": ""}, {"name": "Scott Owens", "author_profile_id": "81337492133", "affiliation": "University of Cambridge, Cambridge, United Kingdom", "person_id": "P2991296", "email_address": "Scott.Owens@cl.cam.ac.uk", "orcid_id": ""}, {"name": "Susmit Sarkar", "author_profile_id": "81392603911", "affiliation": "University of Cambridge, Cambridge, United Kingdom", "person_id": "P2991297", "email_address": "Susmit.Sarkar@cl.cam.ac.uk", "orcid_id": ""}, {"name": "Peter Sewell", "author_profile_id": "81100511814", "affiliation": "University of Cambridge, Cambridge, United Kingdom", "person_id": "P2991298", "email_address": "Peter.Sewell@cl.cam.ac.uk", "orcid_id": ""}], "doi_number": "10.1145/2103656.2103717", "year": "2012", "article_id": "2103717", "conference": "POPL", "title": "Clarifying and compiling C/C++ concurrency: from C++11 to POWER", "url": "http://dl.acm.org/citation.cfm?id=2103717"}