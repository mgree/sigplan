{"article_publication_date": "01-25-2012", "fulltext": "\n A Uni.ed Approach to Fully Lazy Sharing Thibaut Balabonski Univ Paris Diderot, Sorbonne Paris Cit\u00b4e, \nPPS, UMR 7126, CNRS, F-75205 Paris, France thibaut.balabonski@pps.jussieu.fr Abstract We give an axiomatic \npresentation of sharing-via-labelling for weak .-calculi, that makes it possible to formally compare \nmany different approaches to fully lazy sharing, and obtain two impor\u00adtant results. We prove that the \nknown implementations of full lazi\u00adness are all equivalent in terms of the number of \u00df-reductions performed, \nalthough they behave differently regarding the duplica\u00adtion of terms. We establish a link between the \noptimality theories of weak .-calculi and .rst-order rewriting systems by expressing fully lazy .-lifting \nin our framework, thus emphasizing the .rst\u00adorder essence of weak reduction. Categories and Subject Descriptors \nI.1.3 [Languages and Sys\u00adtems]: Evaluation strategies General Terms Theory, Languages Keywords Sharing, \nFull laziness, Lambda-lifting, Rewriting, Lambda-calculus, Weak reduction, Optimality, Labelling. 1. \nIntroduction In the implementation of functional programming languages, a fundamental problem is the \nef.cient evaluation of \u00df-reduction. This problem has been studied for a long time. Its dif.culty comes \nfrom the fact that one has to minimize the number of \u00df-steps as well as control the actual (amortized) \ncost of single \u00df-reduction steps. The minimization of the number of \u00df-steps requires, in turn, to handle \ntwo different issues: avoiding non-needed computations, and minimizing duplications of un.nished work. \nIn .-calculus, some reduction strategies [BKKS87] can com\u00adpletely avoid non-needed computations. However, \nit is also known that no reduction strategy can completely avoid duplications [Lam90]. Hence, in any \ncase, one has to cope with duplications that still oc\u00adcur, and .nd some appropriate ways to deal with \nthem. This is exactly the point of sharing: building implementations in which the duplicated occurrences \nof a given original subterm keep a unique shared representation. This allows one to evaluate all the \ncopies simultaneously, as if they were only one. The idea is to make sure that some parts of a program \nwhich are logically duplicated (in the term representation of the program) remain physically single pieces \n(in the memory of the evaluator). Permission to make digital or hard copies of all or part of this work \nfor personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior \nspeci.c permission and/or a fee. POPL 12, January 25 27, 2012, Philadelphia, PA, USA. Copyright c &#38;#169; \n2012 ACM 978-1-4503-1083-3/12/01. . . $10.00 Sharing cannot be achieved using only .-terms: it requires \nthe use of other technical tools, for instance graphs, closures, or pro\u00adgram transformations. The various \nresulting formalisms may be hardly comparable. This paper focuses on one particularly rich .avor of sharing \ncalled fully lazy sharing (described in section 1.2) and aims at for\u00admally relating its various implementations. \nThis is done by de.ning a framework called sharing-via-labelling systems in which they can all be expressed \nand compared. This uni.ed approach provides compiler writers with increased knowledge on the wide panorama \nof full laziness. In particular, it replaces a series of sometimes informal justi.cations of equiv\u00adalence \nby a central theorem ensuring that all the considered ap\u00adproaches are equivalent with respect to the \nnumber of shared \u00df\u00adsteps. As a consequence, one can safely restrict any subsequent comparison of two \nfully lazy models to other parameters of interest such as their space consumption or the actual cost \nof maintaining sharing. Having a uni.ed framework will also simplify the task of comparing full laziness \nto the other ef.cient implementation tech\u00adniques, such as other degrees of laziness (up to optimality), \nor par\u00adtial evaluation. The rest of the introduction is organized as follows: Section 1.1 presents the \nvarious technical tools commonly used to specify shar\u00ading, Section 1.2 describes how these tools have \nbeen used and com\u00adbined over the past 40 years to propose different de.nitions of fully lazy sharing, \nand Section 1.3 details our approach and the contri\u00adbutions of the paper. 1.1 Many tools for sharing \nGraphs. The most intuitive way of expressing sharing might be by using graphs. In the pictures, the binary \nnode @ represents application, and redexes1 are marked with bold lines. For instance in the center of \nthe following picture, an abstraction .x.t is applied to an argument a. The function body t contains \ntwo occurrences of the variable x: the argument a is thus logically duplicated (on the left). @ .x xx \nThe simplest notion of sharing, which may be referred to as lazy sharing, or just laziness, prevents \nthe previous duplication by keeping a physically unique, with two pointers to its location (right hand \nside part of the previous picture). Here the term laziness is to be taken in literal sense: the duplica\u00adtions \nare postponed as long as it is possible, but some of them will 1a reducible expression, or redex, designs \na place where an evaluation step can take place  eventually happen. For instance, a shared function \nhas to be copied prior to any instantiation, as shown in the picture below. @ .x xx  Extraction of \nfree parameters. The previous pictures feature graphs built with .-calculus constructs, and in particular \nwith binders. This requires either to de.ne variable renaming (a\u00adconversion) or to add some special structure \nto represent binding. In any case the resulting graph formalism is quite complex. Graph reduction can \nbe made easier by turning .-terms (that are higher\u00adorder terms) to applicative expressions (that are \n.rst-order terms). This is the point when compiling the .-calculus into combina\u00adtors [Tur79] or supercombinators \n[Hug82], techniques that .nally led to the .-lifting program transformation [Joh85, Jon87]. This transformation \nextracts all the free variables from a func\u00adtion and replaces what remains of the function by a symbol \ncalled supercombinator. New reduction rules are added to deal with the new symbols. Closures and memory \nheaps. While graphs are a simple and old way to describe lazy sharing, the reference system for the semantics \nof lazy evaluation is J. Launchbury s natural semantics [Lau93]. It introduces let ... in ... constructs \nto name the arguments of the applications, and then puts these arguments in a heap. Sharing then appears \nas memoization: when one needs to access the content of a variable, the corresponding expression found \nin the heap is evaluated, and the heap is updated with the obtained result. In contrast to the previous \nbig-step approach, a small-step de\u00adscription of lazy evaluation based on terms is the call-by-need .\u00adcalculus \nof Z. Ariola et al. [AFM+95]. Here again, sharing is ex\u00adpressed thanks to additional let ... in ... constructs \nused as closures. Similar effects can also be achieved by expressing sharing using explicit substitutions \n[Yos94]. The big-step and small-step styles can be related by well-known transformations [DMMZ10]. Labels \nand weak reduction. Finally, laziness is seen in [Yos94, Mar91] as the optimal way of sharing in weak \n.-calculi: variants of the .-calculus where reduction under .-abstractions is restricted. First, J.-J. \nL\u00b4evy [L\u00b4e80] described optimal sharing for the plain .-calculus (unrestricted, untyped, usual .-calculus) \nby means of labelled .-terms. Then L. Maranget [Mar91] adapted these ideas for a weak .-calculus and \nfor .rst-order rewriting and got an additional result that is not valid for the plain .-calculus: labelled \nterms represent graphs implementing optimal sharing. In [Mar91] the link between labelled terms and graphs \nis made by interpreting the label of a term as its location in memory, or graphically by its coordinate: \n @a @ .x\u00df .x @. @ . xx We call this principle sharing-via-labelling. The idea is also ex\u00adplored in [DLLL05]. \nIn this setting the equality of labels corre\u00adsponds to the physical equality of two terms, which should \nin turn imply their syntactic equality: two terms stored/drawn at the same place ought to be equal. The \nreduction of a graph-redex is simu\u00adlated by the reduction of all the labelled term-redexes with a given \nlabel. One then needs to ensure that the sharing property (terms with equal labels are syntactically \nequal) is preserved by reduction.  1.2 Full laziness: State of the art The main idea. Full laziness \nis based upon the following remark: the constant parts of a function body are not affected by the instan\u00adtiation \nof the function, hence they need not be duplicated. This can be formalized by means of the notion of \nfree expres\u00adsion. We recall the de.nition given in [Jon87]. Say a subterm s of t is free in .x.t if all \nthe free variables of s are free in .x.t.A max\u00adimal free expression of .x.t is a free expression of .x.t \nwhich is not contained into any other free expression of .x.t. Fully lazy sharings. The various de.nitions \nof fully lazy sharing come from a combination of the previous idea with one or more of the technical \ntools described in section 1.1. The .rst description of fully lazy sharing is in the graph evalu\u00adation \ntechnique presented by C.P. Wadsworth [Wad71]. This graph reduction performs only a partial copy of a \nduplicated function body, by avoiding the copy of its maximal free expressions (see Example 1a). O. Shivers \nand M. Wand [SW05] enrich the graph structure of [Wad71] to allow a simple and ef.cient implementa\u00adtion. \nFor this they also use a different characterization of what has to be copied, which we detail in Section \n3.1. Two other approaches combine graphs with other tools. First S. Peyton-Jones [Jon87] reaches a simple \ngraph formalism thanks to a fully-lazy version of .-lifting. Following [Hug82], fully-lazy .-lifting \nreplaces the extraction of the free variables of a function by the extraction of its maximal free expressions. \nSecond, T. Blanc, J.-J. L\u00b4evy and L. Maranget [BLM07] derive a graph implementa\u00adtion of fully lazy sharing \nthrough the sharing-via-labelling princi\u00adple, using labels that characterize optimal sharing for a weak \n.\u00adcalculus studied in [C\u00b8 H98]. This approach can copy fewer graph nodes of the duplicated abstractions \n(see Example 1b). Finally, Z. Ariola and M. Felleisen [AF97] and P. Sestoft [Ses97] use the extraction \nof maximal free expressions to build fully lazy versions of (respectively) the call-by-need .-calculus \n[AFM+95] and Launchbury s natural semantics for laziness [Lau93]. Both so\u00adlutions are based on closures \nrepresented by let ... in ... constructs. The former solution [AF97] uses a more restrictive de.nition \nof free expressions and hence may in some cases copy more nodes than the others (see Example 1c). Example \n1. Bold lines identify the parts of the function that are duplicated by the different models. See Section \n3.1 for a formal statement. .x .x .x yy yy yy (a) [Wad71] (b) [BLM07] (c) [AF97] Summary. The following \ntable sums up how each of the previous works gives its own view on fully lazy sharing, with different \ninter\u00adpretations of the same main idea and using various combinations of technical tools that are sometimes \nhardly comparable. We use the symbol = to mean as many copied nodes as [Wad71] . Dupl. Tools Graphs Extraction \nClosures Labels [Wad71] = . [Jon87] = . . [AF97] More . . [Ses97] = . . [SW05] = . [BLM07] Fewer . . \n  This paper proposes a formal setting in which all the approaches mentioned above can be expressed. \nThis allows us to formally compare them and leads us to the two following conclusions: The previous \napproaches correspond to at least three different graph implementations. This means that, strictly speaking, \nthey do not all induce the same amount of sharing. Hence, despite the fact that all these approaches \nintend to implement the same idea their equivalence is not obvious.  However, all these approaches have \nthe same reduction space. This means that the different implementations of fully lazy sharing perform \nthe same number of \u00df-reductions. In other words, any further comparison of these approaches need not \nanymore take this parameter into account.   1.3 Content of the paper An axiomatic framework for sharing-via-labelling. \nWe build in Section 2 an axiomatic framework which generalizes the work of T. Blanc, J.-J. L\u00b4 evy and \nL. Maranget [BLM07] and allows us to express all the previous approaches. We use labelled terms to describe \nthe graphs realizing optimal sharing for a given notion of weak reduction. Various weak reduction notions \nare de.ned thanks to an axiomatic description of the parts of the program where reduction is forbidden. \nIn any case the restrictions concern only evaluation in the body of a non-instantiated function, called \npartial evaluation. This implies that the call-by-value and call-by-name strategies are always valid. \nHowever, the different weak calculi may or may not be con.uent (see [C\u00b8 H98] and Example 3). This approach \nof sharing-via-labelling allows us in Section 3 to relate all the de.nitions of fully lazy sharing that \ndo not rely on supercombinators and .-lifting. In other words this axiomatic framework, which is designed \nin higher-order rewriting, cov\u00aders the de.nitions of full laziness which directly operate in the higher-order \nworld [Wad71, AF97, Ses97, SW05, BLM07]. The remaining approaches using a translation to .rst-order rewrit\u00ading \nby .-lifting [Hug82, Jon87] are studied separately in Sec\u00adtion 4. The translation to .rst-order by means \nof combinators of D. Turner [Tur79] is out of the scope of the present paper, since these combinators \nsimulate explicit substitutions and then intro\u00adduce additional reduction steps. Notably due to its axiomatic \nnature, our framework is not suit\u00adable for an immediate implementation. On the other hand, this ap\u00adproach \nteaches us something about full laziness in general and on its various concrete implementations. The \nnovelty of our frame\u00adwork lies in the fact that it cannot be seen as a straightforward generalization \nof any of the aforementioned embodiments of full laziness taken in isolation: the axiomatization rather \ncomes from an analysis of the similarities and the differences of all the concrete systems. This yields \na new system whose speci.c properties may be understood as the intersection of the particular properties \nof the various concrete systems. In other words, our axiomatization tries to grasp the essence of full \nlaziness. A formal coding of higher order into .rst order by .-lifting. The .-lifting program transformation \nturns a .-term into a .rst-order term. The main feature of .-lifting is the transformation of .\u00adabstractions \ninto function symbols, also called supercombinators, over which .rst-order reduction rules are de.ned. \nAs emphasized in [LM09], this transformation unveils a tight relation between weak .-calculus and .rst-order \nrewriting. Usual de.nitions of .-liftings [Joh85] proceed by .rst de.ning the transformation of .-abstractions, \nand then iteratively applying the process to a .-term until it contains no more .-abstractions. De.nitions \ndiffer in particular in the way in which a single .\u00adabstraction is transformed and on the order in which \nthe iteration is applied. For instance, [Jon87] describes a bottom-up transforma\u00adtion, while [Ses97] \niterates in an unspeci.ed order. We ensure the consistence of these two views by giving a de.nition of \nfully-lazy .-lifting in which the order of the iterative process is irrelevant. Since .-lifting is an \niterative process that turns progressively a .-term into a .rst-order term, none of the intermediate \nsteps is in either of these worlds. Nevertheless, we would like to embody the source, the target, and \nall the intermediate steps of the transfor\u00admation into a single formalism. To this aim we use Combinatory \nReduction Systems (CRS), a higher-order rewriting framework in\u00adtroduced by J.W. Klop and reviewed in \n[KvOvR93] that mixes ab\u00adstractions and symbols. The \u00df-reduction as well as the target .rst\u00adorder reduction \nhave a straightforward encoding into CRS rules. Moreover, fully lazy .-lifting itself can then be seen \nas a rewrit\u00ading process: it is expressed in Section 4 as a con.uent and strongly normalizing CRS reduction \nrelation. We provide a new proof of correctness of fully lazy .-lifting by showing that the transformation \npreserves reduction sequences: each single reduction step in the source (resp. target) system is simulated \nby exactly one single step in the target (resp. source) system. The proof is small-step: the reduction \nsequences are proved to be preserved in every intermediate step of the transformation. Moreover, we prove \nthat the notion of optimal sharing is also preserved, which has two consequences: The direct [Wad71] \nand the .-lifting based [Jon87] approaches of full laziness are reduction-wise equivalent.  Fully-lazy \n.-lifting establishes a link between optimal sharing in the weak .-calculus [BLM07] and the better known \noptimal\u00adity theory of .rst-order rewriting [Mar91, Ter03]. This empha\u00adsizes in a new way the .rst-order \nnature of weak reduction, without any de Bruijn indices or explicit substitutions (contrary to [Mar91]). \n A .nal bonus remark is an incidental point which happens to have some theoretical signi.cance: while \n\u00df-reduction and .-lifting considered separately can be seen as orthogonal systems2, their combination \ncannot. As far as the author is aware, the system de\u00adrived in this paper is the .rst successful optimality-oriented \nla\u00adbelling of a non-orthogonal system. Outline. The paper comprises three main parts: Section 2 presents \nthe abstract notions of pre.x, weak reduction, and sharing-via\u00adlabelling and gives a proof of the sharing \nproperty for the axiomatic framework. Section 3 restricts the axiomatics to enforce full lazi\u00adness and \nproves a generic equivalence between several notions of fully lazy sharing. Section 4 focuses on the \nparticular fully lazy system of [Wad71, SW05] whose properties allow a clean de.ni\u00adtion of fully lazy \n.-lifting which establishes a strong link between weak \u00df-reduction and .rst-order rewriting. For lack \nof space most proofs are only sketched here. The full versions are in a companion technical report [Bal11]. \n2. Sharing and \u00df-reduction We de.ne in this section an axiomatic framework in which the higher-order \napproaches [Wad71, AF97, Ses97, SW05, BLM07] to fully lazy sharing can be expressed. We propose an axiomatic \nno\u00adtion of weak \u00df-reduction in Subsection 2.1, whose optimal sharing is characterized by the sharing-via-labelling \nsystems introduced in Section 2.2. Section 2.3 then shows that reduction of labelled terms in sharing-via-labelling \nsystems represents reduction of graphs. All this is expressed in Combinatory Reduction Systems (CRS). \nFor lack of space, we only recall the basic syntax and mechanisms. 2 in brief, a system is orthogonal \nwhen no two rules are applicable to overlapping sets of positions of a term, see for instance [Ter03, \nBru03]  We refer the reader to [KvOvR93] for a comprehensive presenta\u00adtion. The grammar of metaterms \nin aCRSis: t ::= x | [x]t | f(t1, ..., tn) | Z(t1, ..., tn) where x is a variable, [x] denotes the binding \nof a variable, f is an n-ary function symbol taken in a signature S, and Z is an n\u00adary meta-variable.A \nterm is a metaterm without meta-variable, and a reduction rule is a pair L . R of closed metaterms satisfying \nthe following conditions: the meta-variables in L appear as Z(x1, ..., xn) with x1, ..., xn distinct \nbound variables, and all the meta-variables of R also appear in L. A rule matches a term by application \nof a valuation s that maps n-ary meta-variables to n\u00adary contexts avoiding variable capture. Reduction \nby a rule L . R with valuation s in a context c is c[Ls] . c[Rs]. 2.1 Weak \u00df-reduction systems This \nsection gives an abstract de.nition of weak reduction in the .\u00adcalculus and states one of its crucial \nproperties: disjoint redexes remain disjoint along any reduction sequence (Lemma 1). This lemma serves \nin particular in the de.nition of graph reduction in Section 2.3. Weak reduction forbids the reduction \nof so-called frozen re\u00addexes, which are identi.ed by their belonging to the pre.x of some .-abstraction. \nPre.xes are parts of .-abstractions de.ned by a pre\u00ad.x function satisfying the axioms of a weak \u00df-reduction \nsystem. Weak \u00df-reduction systems are CRS over the signature S com\u00adprising: a binary symbol @ for application, \n a unary symbol . for .-abstraction,  a unary dummy symbol E,  for all n . N, a countable set Fn of \nn-ary symbols.  From now on, by term we mean a CRS term over the signature S (notation t, u, v, w, a). \nWe use the usual notion of positions of terms (notation q), contexts and free variables [KvOvR93]. We \nwrite t{x:=u} the substitution by u of all the free occurrences of the variable x in t. Application and \n.-abstraction symbols are used to embody .\u00adterms in this signature, which is made in the usual way: the \n.-term (.x.x)y for instance is encoded in the CRS term @(.([x]x),y). We write .x.t as a shorthand for \n.([x]t). Hence the encoding of (.x.x)y is simply written @(.x.x, y), and the usual \u00df-reduction is represented \nby the CRS rule @(.x.Z(x),Z') . Z(Z'). The symbols in the sets Fn are used in Section 4 to represent \nsupercombinators. Until then they play no role and may be ignored. The dummy symbol E has no meaning \nin itself. It is needed for labelling (Subsection 2.2), and serves in particular as a container for dynamically \ncreated labels. In the graphical interpretation of labelled terms, the occurrences of E will represent \nindirections (see Subsection 3.1). As a consequence, occurrences of E should not interfere with \u00df-reduction. \nThis leads to the following countable set of rules to simulate \u00df-reduction by allowing any number of \nE s between the application and the .-abstraction: \u00df0: @(.x.Z(x), Z') .\u00df E(Z(E(Z'))) \u00df1: @(E(.x.Z(x)), \nZ') .\u00df E(Z(E(Z'))) \u00df2: @(E(E(.x.Z(x))), Z') .\u00df E(Z(E(Z'))) ... The two E s in the right hand sides are \nused for the correct la\u00adbelling of collapsing reductions (see Subsection 2.2). The use of the dummy symbol \nE is inspired by the notion of expansion in term rewriting systems [Ter03, Chap. 8]. Write . : t . t' \na reduction . of a term t to a term t'. The usual notions of ancestors and descendants, which track subterms \nalong reduction in the .-calculus are straightforwardly adapted, as illustrated in Example 2. A residual \nof a redex r is a descendant of r which is still a redex. Example 2. ' The term t = @(E(.x.@(x, x)),y) \nreduces by rule \u00df1 to t= E(@(E(y),E(y))). The two occurrences of y in t' are the descen\u00addants of the \ny in t, and the latter is the ancestor of the formers. The E in t has no descendant and the E s in t' \nhave no ancestor. We call plain .-calculus the usual reduction relation where the previous rules can \nbe applied in any context. Weak reduction consists in restricting this reduction relation. Particularly, \nit affects the reduction under .-abstractions. Before introducing the formal de.nition, let us present \ntwo different well-known examples: Example 3. 1. The naive weak reduction simply forbids any reduction \nunder .-abstractions. 2. A more re.ned version, studied in particular in [C\u00b8 H98], allows no reduction \nbetween an occurrence of a bound variable and its binder. Formally, if r is a redex of contractum r', \nthen the reduction C[r] . C[r'] is allowed if and only if the context C binds no variable that appears \nfree in r. We call this version CH-weak reduction.  It is known that CH-weak reduction yields a con.uent \nweak calcu\u00adlus while naive weak reduction does not [C\u00b8 H98]. To specify the previous notions, we introduce \na notion of pre.x: call a n-ary closed pre.x of a term t a n-ary context p which does not contain any \nfree variable and such that there are terms t1, ..., tn satisfying t = p[t1, ..., tn]. Example 4 gives \ntwo closed pre.xes of the same term. Call a pre.x function a function that takes a term t as input and \nreturns a closed pre.x of t. Example 4. Let t = .x.@(@(z1,z2), .y.@(@(y, z3),x)). The two contexts .x.@([], \n.y.@([],x)) and .x.@([], .y.@(@(y, []),x)) are two closed pre.xes of t, called respectively spine and \nskeleton (see Section 3.1). These two pre.xes are marked with bold lines in the two following pictures. \n.x .x @@ @ .y @ .y z1 z2 z1 z2 @@ @ x @ x yz3 yz3 Spine Skeleton A weak \u00df-reduction system is de.ned \nbelow by a pre.x func\u00adtion P satisfying some conditions. The .rst condition is a simple restriction linked \nto bound variables. The second condition con\u00adtrols the evolution of P(t) when free variables of t are \nsubstituted {x:=u} by terms. In particular, P(t) is required to contain P(t), and {x:=u} the extension \nfrom P(t) to P(t) has to be uniform. This uni\u00adformity is enforced by the use of an auxiliairy function \nPD. Call a weak \u00df-reduction system a pre.x function P such that: For any term .x.t such that P(.x.t)= \np and .x.t = p[t1, ..., tn], the variable x does not appear free in any of the ti s (which are called \nthe parameters of .x.t). In other words, P(.x.t) contains all the occurrences of x that are free in t. \n There is an auxiliary pre.x function PD such that for any p in the codomain of P and for any terms \nt1, ..., tn where no free   variable of a ti is bound in p, the equation P(p[t1, ..., tn]) = p[PD(t1), \n..., PD(tn)] holds. A weak \u00df-reduction system de.nes a notion of weak reduc\u00adtion as follows: \u00df-reduction \nis forbidden in the pre.x of any .\u00adabstraction. Call a frozen position of a term t a position that is \nin the pre.x of some .-abstraction of t. Call a frozen \u00df-redex a redex whose main @ symbol occurs at \na frozen position. Example 5. The two weak reductions of Example 3 can be captured by our axiomatic de.nition: \n1. Naive weak reduction is given by Pn such that Pn(t)= p where t = p[x1, ..., xn] and x1, ..., xn are \nall the free variable occurrences of t. The auxiliary function is Pn D = Pn (the whole substituted term \nis included into the pre.x). 2. CH-weak reduction can be given by Pch such that Pch(t)= p where t = \np[t1, ..., tn] and t1, ..., tn are all the maximal free expressions of t. The auxiliary function PD is \nthe constant  ch mapping returning the empty unary context [] (the pre.x is stable by substitution). \nWe will see in Section 3.2 that Pch is not the unique representation of CH-weak reduction. An important \nfeature of weak reduction is that it cannot nest the residuals of disjoint redexes. This fact is formalized \nin Lemma 1, and will be useful in Section 2.3 to ground the notion of parallel reduction. Lemma 1 (Disjoint \nresiduals). Let . : t . t ' be a reduction and r1, ..., rn (non frozen) redexes of t occurring at disjoint \npositions. Then the descendants of r1, ..., rn also occur at disjoint positions. Example 6 shows why \nLemma 1 is a feature of weak reduction which is not valid in the plain .-calculus. Example 6. Suppose \nr1 and r2 are two redexes. In the left term r1 is frozen for any weak \u00df-reduction system. To prepare \nthe implementation of optimal sharing for weak \u00df\u00adreduction systems in next section (2.2), we give a characterization \nof redex creation in these systems. Suppose . : t .\u00df t ' in a weak \u00df-reduction system. A redex of t ' \nis created by . if it is not the descendant of a (non frozen) redex of t. The reduction can create redexes \nin t ' at exactly three places: 1. At the root of the contractum, the body of the main .-abstraction \nis connected to the context. This can create a new contact be\u00adtween an application and a .-abstraction. \n 2. At the places where a substitution occurs, the argument is con\u00adnected to the body of the main .-abstraction, \nor to the context if the body is degenerated. 3. In the pre.x of the main .-abstraction, a previously \nfrozen redex can be unfrozen by ., as r1 in Example 6. In other words, a reduction forbidden by the weak \nrestriction in t can be authorized in t ' .   2.2 Sharing-via-labelling systems We de.ne in this section \na labelling for weak \u00df-reduction systems which characterizes optimal sharing and yields a graph implemen\u00adtation. \nAs in [L\u00b4 e80], the labels record the past history of a term. This is done in a distributed way since \neach label remembers only what is relevant to its position. The important point for optimality is that \nthe labels of a redex r are characterized by the past reduc\u00adtions that contributed to the creation of \nr, which is ensured by the concluding lemmas 2 and 3. These two lemmas also play a key role in the proof \nof the preservation of the graph structure (so-called sharing property) in Section 2.3. To embody this \ncontribution relation into the labels, we build compound labels of the form [O,a] where a pre-existing \nlabel a is modi.ed by the name O of a contributing redex. The labelled \u00df\u00adreduction then modi.es the labels \nof the positions where the reduc\u00adtion can contribute to something, following the characterization of \nredex creation given at the end of the previous subsection. Names and contribution are required to satisfy \nthree axioms which ensure that the name of a redex correctly re.ects its contributors. The labelled terms \nare formalized as usual CRS terms over a labelled signature. Since the labels should not interfere with \nthe normal reduction behaviour, the labelled \u00df-reduction is de.ned for any possible labelling of the \nsource. For any countable set L whose elements are called labels (and written a, \u00df, ....), a labelled \nsignature SL is de.ned as the set {f a|f . S,a . L}. From now on, a L-labelled term denotes a CRS term \nover SL. In other words, the labels are associated to the symbols, and never directly to the variables \nor the bindings. Remark that the labels are arbitrary objects: in a concrete de.nition they can be simple \nletters as well as richly structured objects. We write .ax.t (resp. x a) as a shorthand for .a([x]t) \n(resp. Ea(x)). Write t(t) for the label of the root symbol of the labelled a1...ak term t. Write Ek (t) \nas a shorthand for Ea1 (...Eak (t)), where the case k =0 represents t. Write |.| the (trivial) map from \nterms over SL to terms over S that removes the labels of all symbols. By requiring the condition |P(t)| \n= P(|t|), we get a straightforward extension to SL of any weak \u00df-reduction system P over S. Since neither \nE s nor labels shall interfere with \u00df-reduction, labelled \u00df-redexes allow any number of E occurrences \nand can be decorated with any labels: for any L,a L-labelled \u00df-redex is a \u00df1...\u00dfk L-labelled term of \nthe form @a(Ek (.. x.t),a). Labelled \u00df\u00adreduction is de.ned later, since it requires an additional notion \nof sharing-via-labelling systems given below. We consider sets of labels of the form L = VN , generated \nby the following grammar for any two countable sets V and N : VN ::= V| [N , VN ] The label [O,a] denotes \nthe label a modi.ed by O . Write [O1...On,a] as a shorthand for [O1, ...[On,a]]. Any Oi is called a modi.er \nof [O1...On,a]. The virginal labels (the labels in V) denote positions that are free from any past history, \nthey will be modi.ed into labels of the form [O1...On,a] along the reductions. Let S be a tuple (P, N \n, V,., Y .) where: P is a weak \u00df-reduction system.  N is a countable set whose elements are called \n(redex) names (notation O).  V is a countable set whose elements are called virginal labels, with two \ndistinguished elements T and ..  . is a function from VN -labelled redexes to names.  Y . is a transitive \nand irre.exive relation on N called contribu\u00adtion relation.  The terms considered in S are the VN -labelled \nterms. Call virginal term a term whose labels are all virginal and different. Write . O= {O ' | O ' Y. \nO} the set of all the contributors of the name O. Contribution is extended to labels: de.ne . a = \u00d8 if \na is a virginal label, and . [O,a]= {O}. . O .. a otherwise. Write O Y. a when O ..a. S is a sharing-via-labelling \nsystem if the following axioms are satis.ed: (Redex contributors) \u00df1...\u00dfk ..(@a(Ek (.. x.t),a)) = .a \n. ( i .\u00dfi) ... (Name scope) If .. x.p is in the codomain of P, and at,au are any terms and t1, ..., tn,u1, \n..., un are terms whose free variables are not bound in .. x.p, then \u00df1...\u00dfk .(@a(Ek (.. x.p[t1, ..., \ntn]),at)) = \u00df1...\u00dfk .(@a(Ek (.. x.p[u1, ..., un]),au)) (Name equality) If .(r)= .(r ' ) then t (r)= t \n(r ' ). The axiom Redex contributors states that the name of a redex collects the contributors of the \nessential parts of the redex, that means its main application, its main .-abstraction, and all that lays \nin between these two positions. The axiom Name scope states that the name of a redex does not depend \nof what is deeper than the smallest pre.x of the main abstraction, while axiom Name equality states that \nthe equality of the names of two redexes implies the equality of their respective root labels. Example \n7. Let V be any countable set. 1. The names in Nseq are sequences of labels. Since the names also take \npart into the de.nition of the labels, a mutually recur\u00adsive de.nition of names and labels is required: \nNseq ::= L; ...; L L ::= VNseq De.ne the name of a redex as \u00df1...\u00dfk .seq(@a(E(.. x.t),a)) = a; \u00df1; ...; \n\u00dfk; . k The contribution relation is de.ned by: O Y.seq a1; ...; an if and only if there is at least \none i such that O Y.seq ai. For P .{Pn, Pch}, the system (P, Nseq, V,.seq,Y.seq) is a sharing-via-labelling \nsystem. The de.nitions of Nseq, .seq, and Y.seq correspond to the system presented in [BLM07]. 2. The \nnames in Nctx are L-labelled contexts, with once again a mutually recursive de.nition using L ::= VNctx \n. De.ne a\u00df1...\u00dfk . a\u00df1...\u00dfk . .ctx(@(Ek (.x.t),a))=@(Ek (.x.p), []) where P(.. x.t)= .. x.p. Write O \nY.ctx c if there is a label a in c such that O Y.ctx a. For (Pch, Nctx, V,.ctx,Y.ctx) to be a sharing-via-labelling \nsystem, the axiom Redex contributors requires that the pre.xes satisfy the following property: all the \ncontributors of the labels of a pre.x .. x.p contribute to .. Fortunately, in this system this property \nis an invariant of labelled \u00df-reduction (de.ned below). The system (Pn, Nctx, V,.ctx,Y.ctx) is not a \nsharing-via\u00adlabelling system since it breaks the axiom Name scope. Labelled \u00df-reduction is de.ned by \na rule scheme which propa\u00adgates the name of the reduced redex in the reduced term for record\u00ading of the \ncontributions. For this, the constructor [., .] extends to a function on labelled terms: [O,t] is de.ned \nas the labelled term t in which all the labels are modi.ed by the function a . [O,a]. Labelled \u00df-reduction \nin a sharing-via-labelling system is de\u00ad.ned by the rule scheme: \u00df1...\u00dfk @a(Ek (.. x.p[t1, ..., tn]),a) \n.\u00df E[O,T] ([O,p]){x:=e[O,.](a)}[t1, ..., tn] where .. x.p = P(.. x.p[t1, ..., tn]) and \u00df1...\u00dfk where \nO= .(@a(Ek (.. x.p[t1, ..., tn]),a)). The name O is added in three areas of the reduced term: 1. At the \nroot of the contractum, with the new label [O, T]. 2. At the places where a substitution occurs, with \nthe new label [O, .]. Remark in Example 8 how T and . work as parentheses in the syntactic tree of the \nterm. 3. In the pre.x of the main abstraction of the redex, as an addi\u00adtional modi.er to pre-existing \nlabels.  Remark that these three places follow the three cases of redex creation given in Section 2.1 \nand that removing the labels in this rule yields exactly the unlabelled \u00df-reduction of Section 2.1. Example \n8. @a(.. \u00b5.s Let r = x.@d(@.(x,x ),.. y.y ),a) be a \u00df-redex. We reduce r in two sharing-via-labelling \nsystems of Example 7. 1. In (Pn, Nseq, V,.seq,Y.seq) the name of r is the sequence O= a; ., and the pre.x \nof the .-abstraction is its whole body. Then all the labels are modi.ed in the contractum: @a .. E[O,T] \nx @d @[O,d]  @. .. @[O,.] .[O,.] y y x\u00b5 x . E[O ,\u00b5] E [O,.] s y [O,s] yE[O,.] E[O,.] 2. In (Pch, \nNctx, V,.ctx,Y.ctx), the name of r is the labelled @a\u00b5. context O= (.. x.@d(@.(x,x ), []), []). The only \nmod\u00adi.ed labels are those of the pre.x of the .-abstraction, marked with bold lines below. @a .. E[O,T] \nx  @[O,d] @d @.  .. .. @[O,.] y y x \u00b5 x . E[O ,\u00b5] E [O,.] ss y y E[O,.] E[O,.] This section ends \nwith the two lemmas 2 and 3, which show how the labels re.ect the contribution relation between redexes. \nIn particular, the name of a redex r characterizes the past reductions that led to the creation of r. \n Lemma 2 (Redex stability). Let r be a (non frozen) redex of a term t. If rd is a descendant of r after \na reduction . : t .\u00df t ', then rd is still a (possibly frozen) redex and .(rd)= .(r). Lemma 3 (Direct \ncontribution). If a redex of name Oc is created by the reduction of a redex of name O, then O Y. Oc. \n 2.3 Sharing This section proves the main property of sharing-via-labelling sys\u00adtems: parallel labelled \nreduction simulates graph reduction (The\u00adorem 1). As in [BLM07], the labelled terms are linked to graphs \nwith the sharing-via-labelling principle seen in the introduction: la\u00adbels are interpreted as memory \nlocations. The proof of the simula\u00adtion is then done by ensuring that the two following invariants are \npreserved by parallel labelled reduction: 1. A term t has the sharing property, written S(t), when any \ntwo subterms of t with same label are syntactically equal. This is the main property we want to preserve. \n 2. A term t has the maximality property, written M(t), when for any (non frozen) redex of name O and \nany subterm of label a in t it is not true that O Y. a. This property is widely used in the subsequent \nproofs, it ensures in particular that all the occurrences of a given label are created at the same time \n(see Example 9). Lemmas 2 and 3 are the cornerstone of the preservation of the maximality property. \n Parallel labelled reduction is de.ned for any term t satisfying the sharing property S(t): let a be \nthe label of a (non frozen) redex of t. Since S(t) holds, all the redexes labelled by a are equal and \nhave disjoint positions (however, some may be frozen). The parallel labelled reduction of a, written \nt =a ', is then de.ned as .\u00df t the simultaneous replacement of all the (non frozen) redexes with label \na by their contractum. By lemmas 1 and 2, parallel labelled reduction is well de.ned as a sequence of \nsingle steps, for instance any iterated reduction of the non frozen redexes with label a. Theorem 1 (Preservation \nof sharing). If M(t), S(t) and t =a0 .\u00df t ', then M(t ' ), and S(t ' ). Proof. (Sketch) Veri.cation of \nM(t ' ). Suppose there is a redex r ' with name O ' and a subterm u ' with label a ' in t ' such that \nO ' Y. a '. If r ' is created by the reduction, then Lemma 3 contradicts M(t). Else, Lemma 2 and axiom \nRedex contributors contradict either M(t) or axiom Name equality. ''' ' Veri.cation of S(t ). Let u and \nv be two subterms of t with same label a '. By case on the origin of both labels a ': if one is created \nand the other is a descendant of a label a ' of t, then by M(t) and Lemma 3 we reach a contradiction, \nelse both have the same origin and evolution (using S(t)). Example 9 shows why maximality Mis necessary \nto the preser\u00advation of sharing S. Example 9. Consider the system (Pch, Nseq, V,.seq,Y.seq) de.ned in \nExam\u00ad [.;d,.] ..ple 7 and the labelled term t =@a(x, @. (.d z.z ,y )). The property S(t) holds since \nall the labels of t are different. Remark (.d .. that t contains a redex @. z.z ,y ) of name .; d. Hence \nM(t) does not hold since t contains a label [.; d, .]. Then t =..)) = t ' where the two subterms .\u00df @a(x[.;d,.],E[.;d,.](y \nof t ' with label [.; d, .] are different: S(t ' ) is falsi.ed. Finally, labelled terms represent graphs \nand parallel labelled reduction represents graph reduction. 3. Full laziness This section shows how the \nvarious known implementations of full laziness correspond to several (at least three) different sharing-via\u00adlabelling \nsystems (Section 3.1). Thus, they correspond to different graph reductions featuring different amounts \nof sharing. However, we are going to prove in Section 3.2 that these implementations are reduction-wise \nequivalent. Along this section, the signature S is restricted to {@, ., E}. 3.1 Encodings into sharing-via-labelling \nsystems The motto of sharing-via-labelling is labels denote memory loca\u00adtions . What happens to the labels \nduring reduction describes di\u00adrectly what happens to the nodes of the corresponding graph: A new label \ncorresponds to a new node. There are two cases: A label of the form [O, T] or [O, .] appears only on \nE. It represents a new indirection node that contains a pointer leading to the term. Any other [O,a] \ndenotes a new copy of a node labelled a. An unchanged label is a node unaffected by the reduction. The \nkey of the encoding of graph reduction systems or closure\u00adbased systems into sharing-via-labelling systems \nis to modify ex\u00adactly the labels of what is needed to be copied. Since the rules of a sharing-via-labelling \nsystem modify exactly the labels of the pre\u00ad.x of the main .-abstraction, this amounts to take as pre.x \nof a .-abstraction exactly what has to be duplicated of its body. Remark 1. Our \u00df-rule falsely suggests \nthat an arbitrary number of indirections can be contracted in unit time. The techniques pre\u00adsented in \n[Jon87] can be used to avoid chains of indirections. To describe the encodings of the higher-order approaches \nto full laziness into sharing-via-labelling systems, we formally de.ne two useful pre.xes mentioned in \nExample 4. Call spine of a term .x.t the pre.x .x.p where p is the pre.x of t which contains exactly \nthe positions that are above a free occurrence of x, including the free occurrences of x. Remark that \nin any weak \u00df-reduction system P, any pre.x P(.x.t) contains the spine of .x.t.  Call skeleton of a \nterm .x.t the pre.x .x.p where p is the pre.x of t containing exactly the positions that are not in a \nfree expression of .x.t. As done in [SW05] the skeleton can also be seen as an iterated spine: to get \nthe skeleton of .x.t, start with the spine of .x.t and iteratively add to the obtained pre.x the spines \nof all the .-abstractions that are in the pre.x built so far.  The two approaches by C.P. Wadsworth \n[Wad71] and O. Shivers and M. Wand [SW05] reach fully lazy sharing by two graph imple\u00admentations in which \nthe duplicated part of an instantiated function is its skeleton. The former uses the de.nition based \non the maximal free expressions while the latter follows the characterization by it\u00aderated spine. They \nare both represented by the weak \u00df-reduction system Pch such that Pch(.x.t) is the skeleton of .x.t. \nIn [Ses97], P. Sestoft revises Launchbury s lazy semantics [Lau93] and proposes a fully lazy variant \nusing additional let-bindings: if .x.p is the skeleton of .x.t and .x.t = .x.p[t1, ..., tn], then .x.t \nis replaced by let x1 = t1, ..., xn = tn in .x.p[x1, ..., xn] with x1, ..., xn fresh variables. After \nthis extraction of the maximal free expressions t1, ..., tn of .x.t, a duplication of the .-abstraction \nduplicates the subterm .x.p[x1, ..., xn] but does not duplicate the let-parameters t1, ..., tn. This \nis again represented by the weak reduction system Pch. The work by T. Blanc, J.-J. L\u00b4 evy and L. Maranget \n[BLM07] already uses a system isomorphic to a sharing-via-labelling system.  Their labelled \u00df-reduction \nmodi.es only the labels of the spine of the main .-abstraction. Thus it corresponds to a weak \u00df-reduction \nsystem Pblm where Pblm(.x.t) is the spine of .x.t. Moreover their frozen redexes are the redexes containing \na free occurrence of a variable bound above: they coincide with those given by Pch. In their call-by-need \n.-calculus [AF97], Z. Ariola and M. Felleisen allow the substitution, and thus the duplication, of values. \nTheir fully lazy extension consists in restricting these allowed duplica\u00adtions to a set of fully lazy \nvalues: values that do not contain any free expression . The difference with the previous cases lies \nin their non-standard de.nition of free expressions: they use the usual criterion given in Section 1.2 \nbut they exclude the variables and the .-abstractions. Hence their fully lazy values correspond to the \npre.xes of the weak \u00df-reduction system Paf such that: If .x.p is the skeleton of .x.t and .x.t = .x.p[t1, \n..., tn] then Paf (.x.t)= .x.p[PaD af (tn)]. f (t1), ..., PD  PaD f (.x.t)= Paf (.x.t).  PaD f (@(t1,t2)) \n= [].  If PD f (t) = [] then PD af (E(t)) = E(PD  aaf (E(t)) = [] else PD af (t)). Finally, the higher-order \napproaches [Wad71, AF97, Ses97, SW05, BLM07] to full laziness correspond to three different weak \u00df-reduction \nsystems Pch, Pblm and Paf , which we are going to relate in the next section. Example 10 illustrates \nhow the systems Pblm and Paf can yield the same parallel labelled reduction in spite of their differences. \nExample 10. Let t = .x..y.@(@(x, r1), .z.r2) be a term such that r1, r2 are two redexes that do not contain \nany free occurrence of x. Hence a duplication of the spine Pblm(t) of t (marked with bold lines in the \npicture) do not duplicate these redexes. .x Proof. (Sketch) We prove two invariants on the pair (t1, \nt2): .y 1. The equivalence classes of non frozen applications are equal. @ 2. In each spine, the equivalence \nclasses of applications are equal. x r1 y @ r2 .z z By de.nition of t1 ~ t2 there is a parallel labelled \nmirror reduction sequence (Q1, Q2) leading from a virginal term t0 to (t1, t2). The proof of both invariants \nis by induction on the number of parallel  If r1 (resp. r2) contains a free occurrence of y (resp. z), \nthen both redexes are at least partially contained in Paf (t), and thus duplicated in this system. However, \nin this case r1 and r2 are and remain frozen in both systems, and their label will change before they \nare unfrozen: the additional duplications are not harmful.  3.2 Equivalence of the parallel labelled \nreductions In this section we de.ne a family W of weak \u00df-reduction systems that contains Pch, Pblm and \nPaf and we show that all the sharing\u00advia-labelling systems based on the weak \u00df-reduction systems of W \nare equivalent, in the sense that they can simulate one another using exactly the same number of shared \nreduction steps. For this we de.ne a relation ~ on labelled terms which is stable by reduction, and prove \nthat any two terms in relation share the same non-frozen redexes. The relation ~ relates two terms t1 \nand t2 of two different systems S1 and S2 whenever t1 and t2 can be reached from a common source by two \nequivalent reduction sequences in S1 and S2 (the whole being called a mirror reduction sequence). The \nde.nition of W follows two ideas: the weak \u00df-reduction systems in W shall enforce CH-weak reduction (for \nthis the third point in particular prevents some applications to be included into a pre.x), and all the \npre.xes are built from spines and skeletons (the .rst two points, which make the proofs tractable). A \nweak \u00df-reduction system P is in W if the following addi\u00adtional conditions are satis.ed: For each abstraction \nt = .x.u, either P(t) is the spine of t, or P(t)= p[PD(t1), ..., PD(tn)] where p is the skeleton of t \nand t = p[t1, ..., tn]. For any .x.t, PD(.x.t) = [] or PD(.x.t)= P(.x.t).  PD(@(t1,t2)) = [].  If \nPD(t) = [] then PD(E(t)) = [] else PD(E(t)) = E(PD(t)).  Let P1 and P2 be two systems of W.A mirror \nreduction sequence in the systems P1 and P2 is a pair (Q1,Q2) such that Qi = .i 1....ni is a reduction \nsequence in Pi and for any j .{1...n}the redexes reduced by the single steps .1 j and .2 j have the same \nposition qj in the respective source terms. The notion of mirror reduction sequence extends to parallel \nlabelled reduction in two sharing-via-labelling systems based on two (possibly equal) weak \u00df-reduction \nsystems of W. Let t1 (resp. t2) be a term in a sharing-via-labelling system S1 (resp. S2). Write t1 ~ \nt2 when there is a virginal term t0 in the intersection of S1 and S2 and a mirror reduction sequence \n(Q1,Q2) in the systems S1 and S2 such that Qi is a parallel labelled reduction sequence from t0 to ti. \nRemark 2. If t1 ~ t2 then the two terms have the same set of posi\u00adtions. In particular they contain the \nsame number of E occurrences. For any labelled term t, the labels in t induce an equivalence relation \non the subterms of t. Say that two subterms u1 and u2 of t are label-equivalent if and only if t (u1)= \nt (u2). Theorem 2 (Sharing equivalence). If t1 ~ t2 then the labellings of t1 and t2 induce the same \nlabel-equivalence on their non frozen applications. steps in (Q1,Q2). Remark by Theorem 1 that any intermediate \nterm t in the mirror reduction sequence satis.es M(t) and S(t). Theorem 2 proves in particular that ~ \nis a bisimulation: any ~~ ~ tutu tu diagram \u00df or \u00df can be closed as \u00df \u00df '' '' ~ This means that in \nW, two sharing-via-labelling systems generate the same notion of parallel labelled reduction. In other \nwords, their (possibly different) sharings have the same impact on the reduction. This applies in particular \nto Pch, Pblm and Paf , and thus it shows that all the notions of full laziness in [Wad71, AF97, Ses97, \nSW05, BLM07] de.ne the same reduction spaces. tu tu 4. Fully lazy .-lifting The primary goal of the present \nsection is to prove that the notion of full laziness de.ned by fully lazy .-lifting in [Hug82, Jon87] \nis equivalent to the uni.ed notion of the previous section. This study of .-lifting also reveals a strong \nrelationship between optimal sharing in weak .-calculi [BLM07] and the optimality theory of .rst-order \nrewriting [Mar91, Ter03]. In this section fully lazy .-lifting is ultimately seen as a mor\u00adphism between \ntwo systems: the source is the set of the .-terms equipped with weak \u00df-reduction, whereas the target \nis the set of .rst-order terms built with supercombinators equipped with their associated .rst-order \nreduction rules. However, the source and the target system are mixed in the intermediate steps. Fully \nlazy .\u00adlifting is then de.ned as an endomorphism of an object system combining the source and the target. \n Our object system is a CRS over a labelling of the signature S de.ned in Section 2.1. The weak \u00df-reduction \nis as de.ned in Sec\u00adtion 2. The supercombinators forming the target subsystem are rep\u00ad resented by the \nsymbols in F = n Fn and their reduction rules are de.ned in Section 4.1. Fully lazy .-lifting itself \nis represented by a set of CRS rules which is proved to be con.uent (Lemma 4), to be strongly normalizing \n(Lemma 5), to preserve one-step reduction (Theorem 3) and to preserve shared reduction (Theorem 4). Since \nwe aim for this last result on shared reduction and since shared re\u00adduction is formalized in this paper \nby the labels, our .-lifting is de.ned on labelled terms. Section 4.1 introduces an extension of sharing-via-labelling \nsys\u00adtems and de.nes fully lazy .-lifting as a rewriting process. Sec\u00adtion 4.2 proves that fully lazy \n.-lifting is a bisimulation between the source system and the target system, and Section 4.3 proves that \nparallel labelled fully lazy .-lifting preserves optimal sharing. 4.1 Fully lazy .-lifting systems This \nsection introduces the fully lazy .-lifting systems as an exten\u00adsion of the sharing-via-labelling systems. \nThen, .-lifting is de.ned as a CRS reduction in a fully-lazy .-lifting system. This reduction is con.uent \nand strongly normalizing. The basic mechanism of .-lifting is the replacement of a whole pre.x by a supercombinator, \nthe pre.x and the supercombinator being related by an abstract invertible function called contractor. \nIn order to preserve sharing, this replacement has to preserve all the information contained in the labels. \nOur solution consists in la\u00adbelling the supercombinator with a structured label containing all the labels \nof the contracted pre.x. Hence the basic operations of .-lifting are reversible, and we de.ne two inverse \ntransformations (contraction and expansion) which reversibly relate labelled pre\u00ad.xes and labelled supercombinators. \nFully lazy .-lifting is related in this paper to the weak \u00df\u00adreduction system Pch of Example 5, whose \nde.nition is reminded below. The ch subscript is omitted along this section. P(.x.t) is the skeleton \nof .x.t.  PD(t) = [] for any term t.  The structured labels used for labelled .-lifting are tree-shaped, \nsimilarly to terms (see Example 11). For any countable set V (whose elements are called atomic labels), \nthe set V of tree labels over V is de.ned by the following grammar: V ::= D |T|.|V|V (V , ..., V ) The \nlabel D is an empty label which is used to denote the lack of label of the empty context [] (see Example \n11). The labels in V are used as virginal labels along this section. A tree label is well-formed when \nno atomic label appears twice in it. For any O1, ..., On, a label [O1...On,a] with a .V is well-formed \nwhen a is well-formed. The clash relation is de.ned on tree labels as: a / \u00df when a and \u00df have an atomic \nlabel in common. The notion is used in Section 4.3 to express invariants on labels. For any V and any \nN , the clash extends to V N as follows: a / \u00df if and only if a = [O1...On,a * ] and \u00df = [O1...On,\u00df * \n] with a * ,\u00df * .V and a * / \u00df * . A contractor is an injective partial function . mapping unla\u00adbelled \nn-ary skeletons to unlabelled n-ary function symbols. For any set of atomic labels V and any set of names \nN , a contractor . is extended to V N -labelled skeletons and V N -labelled symbols by the following \nrules. If p is labelled with virginal labels, then .(p)= fa such that f = .(|p|) and a = lpl, where l[]l \n= D lx al = a lEa(t)l = a(ltl) l.ax.tl = a(ltl) l@a(t1,t2)l = a(lt1l, lt2l) lfa(t1, ..., tn)l = a(lt1l, \n..., ltnl) If p can be decomposed as p = [O,p ' ], then .(p) = [O,.(p ' )].  Else, .(p) is unde.ned. \n The extension of . to labelled skeletons is still injective, hence the labelled . admits an inverse. \nExample 11. 1. Consider the skeleton p = .a x.@\u00df ([],.. y.@d(@.(y . , []),x .)). The collected label \nis lpl = a(\u00df(D,.(d(.(., D),.)))). It is interesting to compare the graphical representations of the pre\u00ad.x \nand the label: .x a  \u00df @ [] .y D . @ d  x . . @ y [] . D 2. Suppose .(.x.@([],x)) = g. Then a(\u00df(D,.)). \n.-1(g )= .a x.@\u00df([],x ) [O,.]) .-1(g[O,a(\u00df(D,.))])= .[O,a]x.@[O,\u00df]([],x .-1(g a(\u00df(d(.),.))) is unde.ned \nbecause the label has not the same structure as .-1(g). .(.[O1,a] x.@[O2,\u00df]([],x[O2,.])) is unde.ned \nbecause the labels have different modi.ers O1 and O2. For any set of atomic labels V, for any set of \nnames N and for any contractor ., .-contraction and .-expansion are two CRS rule schemes on V N -labelled \nterms. For any skeleton .a x.p and symbol f\u00df such that .(.a x.p)= f \u00df we have the two rules: (.-contraction) \n.a x.p[Z1, ..., Zn] .c f\u00df (Z1, ..., Zn) (.-expansion) f\u00df (Z1, ..., Zn) .e .a x.p[Z1, ..., Zn] Call an \nobject redex a labelled term having one of the two following forms: \u00df1...\u00dfk (source redex) @a(Ek (.. \nx.t),a) \u00df1...\u00dfk (target redex) @a(Ek (f. (t1, ..., tn)),a) Remark that the set of object redexes is stable \nby .c and .e. A fully lazy .-lifting system is a tuple (., N , V, ., Y.) such that: (P, N , V , ., Y.) \nis a sharing-via-labelling system.  . is a contractor.  P commutes with .-contraction and .-expansion. \n . is a function from object redexes to names that is stable by .-contraction and .-expansion.  In \na fully lazy .-lifting system (., N , V, ., Y.) we consider L\u00adlabelled terms with L = V N .  Example \n12 shows a straightforward extension of the function .seq of Example 7 which is not stable by contraction. \nExample 13 then gives stable variants of the functions .seq and .ctx. Example 12. @a(... Consider the \nredex r = x.@d(x ,y .),a), which can be .\u00ad' @a(f.(d(.,D))(y contracted to r = .),a) for some unary symbol \nf. Then .seq(r)= a; . and .seq(r ' )= a; .(d(., D)): the name is not stable. Example 13. \u00df1...\u00dfk seq \n 1. De.ne . ' (@a(Ek (.. x.t),a)) = a; \u00df1; ...; \u00dfk; d with . = [O1...On,. * ] and d = [O1...On,d * ] \nwhere . * is virginal and d * is the leftmost atomic label of . *. This name function is stable since \nthe leftmost atomic label is stable by contraction and expansion. \u00df1...\u00dfk 2. If P(.x.t)= p, then de.ne \n. ' (@a(Ek (.. x.t),a)) = ctx \u00df1...\u00dfk @a(Ek (fd([], ..., [])), []) where fd is the unique normal form \nof .. x.p by .-lifting (see de.nition of .-lifting and lem\u00admas 4 and 5 below). This name function is \nstable by normaliza\u00adtion. We call source reduction the \u00df-reduction, whose rule scheme can be simpli.ed: \n\u00df1...\u00dfk @a(Ek (.. x.p[Z1, ..., Zn]),Z) .\u00df E[O,T]([O,p]){x:=e[O,.](Z)}[Z1, ..., Zn] where .. x.p is a \nskeleton and \u00df1...\u00dfk where O= .(@a(Ek (.. x.p[z1, ..., zn]),z)). We call target reduction the .rst-order \nreduction de.ned by the scheme: \u00df1...\u00dfk @a(Ek (f . (Z1, ..., Zn)),Z) .t E[O,T][O,.](Z)} ([O,p]){x:=e[Z1, \n..., Zn] where .-1(f. )= .d x.p and \u00df1...\u00dfk where O= .(@a(Ek (f. (z1, ..., zn)),z)). Call object reduction \nthe union of source reduction and target reduction: .o = .\u00df ..t. Remark 3. Target reduction can be decomposed \nby .-expanding the function symbol and then applying \u00df-reduction. Remark 4. Consider the system S = (Pch, \nNctx, V,. ' ,Y.ctx) ctxwhere Nctx and Y.ctx are de.ned in Example 7, . ' is de.ned in ctx Example 13, \nand V is any countable set. The target reduction of S is isomorphic to what is called L\u00b4 evy-labelling \nin [Ter03, Chap. 8]. For a clean de.nition of .-lifting and for simple proofs of its basic properties \nwe use an extended notion of positions of a term. The so-called .-positions contain the usual syntactic \npositions but also the positions that are hidden in supercombinators: through the contractor . each symbol \nrepresents a pre.x, and the positions of these pre.xes are taken into account in .-positions. The set \nQ(t) of .-positions of t is de.ned by: Q(x)= {e} Q([]) = {e} Q(E(t)) = {e}. 1.Q(t) Q(.a x.t)= {e}. 1.Q(t) \nQ(@a(t1,t2)) = {e}. 1.Q(t1) . 2.Q(t2)  Q(fa(t1, ..., tn)) = {e}. 0.Q(.-1(fa)) . ( i.Q(ti)) i The following \ncases de.ne .-lifting as a CRS reduction: (Reify) If .a x.p is a skeleton such that .(.a x.p)= f \u00df , \nthen e .a x.p[Z1, ..., Zn] .lft \u00df(Z1, ..., Zn) -f .-1q.-1\u00df (Inside) If (f a)[x1, ..., xn] -(g )[x1, ..., \nxn] .lft where x1, ..., xn are fresh variables then a0.q\u00df f(Z1, ..., Zn) - .lft g (Z1, ..., Zn) q1 (Context) \nIf t -.lft t ' and c is a unary context with a hole at position q2, then q2.q1 ' c[t] ---.lft c[t ] While \nReify is the main rule of .-lifting, Inside allows us to close con.uence diagrams (see Example 14) and \nto consider .-lifting as an orthogonal rewriting system (in the sense of [GKK05]). An Inside reduction \ncan be seen as a reduction Reify inside a symbol. An equivalent of Inside naturally appears in [AF97] \nor [Ses97] as a reduction in the context of a let ... in ... construct. Please note that in the rule \nContext, c is an arbitrary context. This means that there is no particular weak restriction here. Example \n14. Let t = .a x.@\u00df (@(z1,z2),.. y.@d(@.(y . ,z3),x .)) be a la\u00adbelled term, where some labels are omitted. \nWrite .-1 (f )= .x.@([], .y.@(@(y, []),x)) \u00b5 = a(\u00df(D,.(d(.(., D),.)))) .-1 (g)= .y.@(@(y, []), []) s \n= .(d(.(., D), D)) .-1(f ' )= .x.@([],g([],x)) \u00b5 ' = a(\u00df(D,s(D,.))) The dotted lines denote the skeleton \nof the abstraction .. y, while the bold lines (solid or dotted) denote the skeleton of the abstrac\u00adtion \n.a x. .a .a xx  @\u00df @\u00df 1.2  .. @   @ s y g z1 z2 z1 z2 z3 . x @d @ . . x z3 y . e e   '\u00b5 \nf\u00b5 f 0.1.2  z3 z3 @ @ Inside z1 z2 z1 z2 Remark 5. The reduction Reify is a straightforward use \nof con\u00adtraction .c, and the reduction Inside can be factorized using con\u00adtraction .c and expansion .e: \nif t .lft t ' by Inside, then there ' '' are u and u such that t .e u .c u .c t . Lemma 4 (Diamond property). \nIf t .lft u and t .lft v with u = v then there is w such that u .lft w and v .lft w. Lemma 5 (Termination). \nThe system .lft is strongly normalizing. Lemma 4 implies that .lft is con.uent, and that all the .lft\u00adsequences \nbetween two given terms have the same length [Ter03]. An immediate corollary of lemmas 4 and 5 is that \nany term has a unique .lft-normal form.  4.2 Fully-lazy .-lifting as a bisimulation This section proves \nthat .lft does not only transform terms but also reduction sequences, and that the transformation operates \nforward as well as backward. Formally, the re.exive-transitive closure of the relation .lft (written \n-lft) is a bisimulation between .o and itself. Thanks to the CRS formalism which allows reasoning on \nthe intermediate steps of the transformation, the proof can be reduced to a one-step simulation property \n(Lemma 6). Lemma 6 (One-step simulation). ' ''' If t o. t .lft u then there is u such that t -lft u lft. \nu. '' '' It t .lft u .o u then there is t such that t .o t -lft u . Proof. (Sketch) By case on the relative \npositions of the two redexes. Most cases use the stability of . and the commutation of P with .lft. The \ndiagram is closed by t ' -lft u ' because a .o-reduction can destroy or duplicate the considered .lft-redex. \nLemma 6 yields as immediate corollary: Theorem 3 (Bisimulation). The relation -lft is a bisimulation \nbetween the reduction .o and itself, which means that any diagram lft lft lft t ut ut u o or o can \nbe closed as o o '' '' tu t u lft This bisimulation is a strong property for .-lifting: it associates \na progressive transformation of reduction sequences to the progres\u00adsive transformation of terms. Moreover, \nthere is a bijection be\u00adtween the single steps of the image and the antecedent reduction sequences. In \nthe next section, we show that this holds also for par\u00adallel labelled reduction.  4.3 Fully-lazy .-lifting \nas a graph bisimulation For r .{\u00df, t, o, c, e, lft}, write .r =a the simultaneous reduction of all the \n.r-redexes of label a in a term satisfying the sharing property S. This last section shows that =preserves \nsharing: .lft the same subterms are shared in the source and in the target of this reduction. This allows \nus to conclude that the full lazinesses of [Wad71] and [Jon87] are bisimilar. In this section we use \nagain the invariants S and M introduced in Section 2.3. In our new setting we extend Mto any object redex. \nWe moreover use the following three invariants: 3. Say a term t has the independent labelling property, \nwritten I(t), when all the labels of t are well-formed and there is no pair of labels (a, \u00df) of t such \nthat a = \u00df and a / \u00df. This property is useful to ensure that contraction and expansion do not break the \nsharing property.  4. Say a term t has the tuned skeletons property, written T(t), when every skeleton \np in t is of the form [O1...On,p * ] where p * has only virginal labels. This property allows applying \na con\u00adtractor . to any pre.x, and rules out the last case of Example 11.  5. Say a term t has the harmonious \nbinding property, written H(t), when any two variables with the same label are either both free or both \nbound by abstractions bearing the same label. This property strengthens the sharing property: when both \nare present, two whole skeletons are shared whenever one of their nodes is shared.  Finally, say a \nterm t has the SMITHproperty (written SMITH(t)) when the .ve properties are satis.ed by t as well as \nby .-1(fa) for any fa appearing in t (or recursively in the antecedent by . of a symbol). Lemma 7. For \nany r .{\u00df, t, o, c, e, lft} and any two terms t, t ' , a0 if SMITH(t) and t =.r t ' then SMITH(t ' ). \nProof. (Sketch) By remarks 3 and 5 we need only consider the cases where r .{\u00df, c, e}. As in Theorem \n1 the proof is by case analysis on the origin of the considered labels. Theorem 4 (Preservation of optimal \nsharing). Let t be a term a0 such that SMITH(t). Suppose t =.lft t '. Let u ' (resp. v ') be a subterm \nof t ', with ancestor u (resp. v) in t. Then t(u ' )= t (v ' ) if and only if t(u)= t(v). Proof. Suppose \nt (u ' )= t (v ' )= a '. Case on the origin of a ' : If t (u)= t(v)= a ', it s over.  If t(u)= a ' \nand t (v)= a ', then t (v)= a0 such that a0 = [O1...On,a * 0] and a ' = [O1...On,a * ] with a0 * appearing \ninto a *, which implies a ' / a0 and contradicts I(t).  If t (u)= a ' and t(v)= a ' then t(u)= t (v)= \na0. Suppose t (u)= t(v)= a. If a = a0, then t (u ' )= t(u)= ' a0 ' a0 ' '' t(v)= t (v ). Else u = v, \nu =.lft u , v =.lft v and u = v . In particular t(u ' )= t(v ' ). Finally, write t t ' is t and t ' \n.lft-convertible. We deduce that are = t ut ut u any o or o can be closed as o o '' '' tu t u This \nimplies that the implementations of full laziness in [Wad71] and [Jon87] de.ne the same reduction space, \nwhich also corre\u00adspond to optimal sharing along [Mar91, Ter03] for the .rst-order system de.ned by the \ntarget reduction .t, and to optimal sharing along [BLM07] for the CH-weak reduction of the .-calculus. \n5. Conclusion Sharing, and in particular fully lazy sharing, is described and imple\u00admented by different \ntechnical tools including graphs, closures, and program transformations. As a consequence, the many de.nitions \nof fully lazy sharing [Wad71, Jon87, Ses97, AF97, SW05, BLM07] are sometimes hardly comparable. Yet they \nall intend to implement the same basic ideas. This paper uni.es all these views of full laziness. To \nachieve this we de.ne an axiomatic framework of sharing-via-labelling systems, in which the various approaches \ncan be expressed. Then we prove that all the resulting systems are bisimilar, in the sense that they \nhave isomorphic reduction spaces. In particular, by linking [BLM07] to other de.nitions of full laziness, \nwe con.rm the intuition of its authors that fully lazy shar\u00ading gives an optimal sharing for the weak \n.-calculus of [C\u00b8 H98]. Last but not least, we show that weak reduction in .-calculus can be expressed \nin orthogonal .rst-order rewriting by means of fully lazy .-lifting, with a one-to-one correspondence \nbetween their reduction steps. This remarkable last property makes our .rst\u00adorder formulation really \ndifferent from the formulations that use de Bruijn indices or explicit substitutions [Tur79, Mar91]. \nMoreover, our transformation preserves optimal sharing and expresses fully lazy sharing as optimal sharing \nfor the target .rst-order system. 5.1 Related work Related approaches to the ef.cient implementation \nof functional programming languages include in particular the study of optimal reduction and the attempts \nto implement it. The study of optimal reduction [L\u00b4 e80, Mar91, GK96, vO96, BLM07] traditionally uses \nthree equivalent characterizations called labelling, extraction, and zig-zag. This paper extends the \nlabelling-based characterization of optimality to weak \u00df-reduction systems.  The possibility of a straightforward \nimplementation in graphs of the label-based characterization of optimality has for long been known to \nbe a feature of .rst-order rewriting [Mar91] that did not hold in the .-calculus. However, more recently \nthis feature has been observed in a weak restriction of the .-calculus [BLM07]. The present paper con.rms \nthis observation by showing that it holds in any weak \u00df-reduction system, and explains this .rst-order \nbehaviour of weak reduction by expliciting a link between weak \u00df-reduction systems and .rst-order rewriting. \nTwo kinds of implementations are known to perform less shared \u00df-reduction steps than some fully lazy \nimplementations. In par\u00adticular partial evaluation [HG91] has been compared to [Jon87], and optimality \n[L\u00b4 e80] is born as an idealization of [Wad71]. Hence this paper shows that partial evaluation [HG91] \nas well as any im\u00adplementation of optimality [AG98, PQ07] is able to perform less shared \u00df-reduction \nsteps than any implementation of full laziness. On the other hand, a global comparison between all these \nap\u00adproaches is still missing. In particular, the following facts keep the question open for now: the \nnumber of shared \u00df-reduction steps in a fully lazy system is polynomially related to the actual cost \nof performing the reduction on a Turing machine (simple extension of [LM09]), but this does not hold \nwith optimal sharing [AM98].  5.2 Future work Fully lazy .-lifting is shown to be a powerful tool to \ngive a faith\u00adful .rst-order account of higher-order systems. This phenomenon prompts us to carry on investigations \nin at least two directions. Generalization of .-lifting to any weak \u00df-reduction system sat\u00adisfying our \naxioms. Two interesting and challenging examples would be the plain .-lifting of [Joh85], in which the \npre.xes are not stable by reduction, and a new notion of .-lifting based on Pblm, which would turn the \nspines into supercombinators. Since the spines can bind variables in their holes, some function symbols \nwould also bind some variables in their arguments.  Generalization of sharing and .-lifting to higher-order \nrewrit\u00ading, which includes richer systems that rewrite functions, such as proof assistants and compilers. \nAs far as the author is aware, general higher-order rewriting knows no notion of weak reduc\u00adtion, sharing-via-labelling, \nor .-lifting. However, the present work seems to be abstract enough to be generalized to higher\u00adorder \nframeworks such as combinatory reduction systems.  Acknowledgments The author would like to thank Delia \nKesner for her continuous support, Vincent van Oostrom for several discussions and for a mysterious but \ninsightful remark that triggered this investigation, and Roberto di Cosmo, Olivier Danvy, John Field \nand the anony\u00admous reviewers for numerous helpful comments and suggestions. References [AF97] Z.M. Ariola \nand M. Felleisen. The call-by-need lambda cal\u00adculus. J. Funct. Program., 7(3):265 301, 1997. [AFM+95] \nZ.M. Ariola, M. Felleisen, J. Maraist, M. Odersky, and P. Wadler. The call-by-need lambda calculus. In \nPOPL, pages 233 246, 1995. [AG98] A. Asperti and S. Guerrini. The optimal implementation of functional \nprogramming languages. Cambridge University Press, 1998. [AM98] A. Asperti and H.G. Mairson. Parallel \nBeta Reduction is not Elementary Recursive. In POPL, pages 303-315, 1998. [Bal11] T. Balabonski. A Uni.ed \nApproach to Fully Lazy Sharing. http://hal.archives-ouvertes.fr/hal-00637048/. Rapport technique PPS, \n2011. [BKKS87] H.P. Barendregt, R. Kennaway, J-W. Klop, and M. Ronan Sleep. Needed reduction and spine \nstrategies for the lambda calculus. Inf. Comput., 75(3):191 231, 1987. [BLM07] T. Blanc, J.-J. L\u00b4evy, \nand L. Maranget. Sharing in the Weak Lambda-Calculus Revisited. In Re.ections on Type Theory, Lambda \nCalculus and the Mind, 2007. [Bru03] H.J.S. Bruggink. Residuals in higher-order rewriting. In RTA, pages \n123 137, 2003. [C\u00b8H98] N.C\u00b8a.gmanandJ.R.Hindley.Combinatoryweakreductionin lambda calculus. TCS, 198(1-2):239 \n247, 1998. [DMMZ10] O. Danvy, K. Millikin, J. Munk, and I. Zerny. Defunctionalized Interpreters for Call-by-Need \nEvaluation. In FLOPS, pages 240 256, 2010. [DLLL05] D. Dougherty, P. Lescanne, L. Liquori, and F. Lang. \nAddressed Term Rewriting Systems: Syntax, Semantics, and Pragmatics: Extended Abstract. ENTCS, 127(5):57 \n82, 2005. [GK96] J. Glauert and Z. Khasidashvili. Relative normalization in deterministic residual structures. \nIn CAAP, pages 180 195, 1996. [GKK05] J.R.W. Glauert, D. Kesner, and Z. Khasidashvili. Expression reduction \nsystems and extensions: An overview. In Processes, Terms and Cycles, pages 496 553, 2005. [HG91] C.K. \nHolst and D.K. Gomard. Partial evaluation is fuller laziness. In PEPM, pages 223 233, 1991. [Hug82] R.J.M. \nHughes. Super combinators: A new implementation method for applicative languages. In LFP, pages 1 10, \n1982. [Joh85] T. Johnsson. Lambda lifting: Transforming programs to recur\u00adsive equations. In FPCA, pages \n190 203, 1985. [Jon87] S. Peyton Jones. The Implementation of Functional Program\u00adming Languages. Prentice-Hall, \nInc., 1987. [KvOvR93] J.W. Klop, V. van Oostrom, and F. van Raamsdonk. Com\u00adbinatory reduction systems: \nIntroduction and survey. Theor. Comput. Sci., 121(1&#38;2):279 308, 1993. [Lam90] J. Lamping. An algorithm \nfor optimal lambda calculus reduc\u00adtion. In POPL, pages 16 30, 1990. [Lau93] J. Launchbury. A natural \nsemantics for lazy evaluation. In POPL, pages 144 154, 1993. [LM09] U. Dal Lago and S. Martini. On constructor \nrewrite systems and the lambda-calculus. In ICALP (2), pages 163 174, 2009. [L\u00b4evy. Optimal reductions \nin the lambda-calculus. In To e80] J.-J. L\u00b4 H.B. Curry: Essays on Combinatory Logic, Lambda Calculus \nand Formalisms, pages 159 191, 1980. [Mar91] L. Maranget. Optimal Derivations in Weak Lambda-calculi \nand in Orthogonal Terms Rewriting Systems. In POPL, pages 255 269, 1991. [PQ07] M. Pedicini and F. Quaglia. \nPelcr: Parallel environment for op\u00adtimal lambda-calculus reduction. ACM Trans. Comput. Logic, 8, July \n2007. [Ses97] P. Sestoft. Deriving a lazy abstract machine. J. Funct. Pro\u00adgram., 7(3):231 264, 1997. \n[SW05] O. Shivers and M. Wand. Bottom-up \u00df-reduction: Uplinks and .-DAGs. In ESOP, pages 217 232, 2005. \n[Ter03] Terese. Term Rewriting Systems. Cambridge Univ.Press, 2003. [Tur79] D.A. Turner. A new implementation \ntechnique for applicative languages. In Softw., Pract. Exper., 9(1):31 49, 1979. [vO96] V. van Oostrom. \nHigher-order families. In RTA, pages 392 407, 1996. [Wad71] C. P. Wadsworth. Semantics and Pragmatics \nof the Lambda Calculus. Ph.D. thesis, 1971. [Yos94] N. Yoshida. Optimal reduction in weak-.-calculus \nwith shared environments. J. of Computer Software, 11(5):2 20, 1994.    \n\t\t\t", "proc_id": "2103656", "abstract": "<p>We give an axiomatic presentation of sharing-via-labelling for weak lambda-calculi, that makes it possible to formally compare many different approaches to fully lazy sharing, and obtain two important results. We prove that the known implementations of full laziness are all equivalent in terms of the number of beta-reductions performed, although they behave differently regarding the duplication of terms. We establish a link between the optimality theories of weak lambda-calculi and first-order rewriting systems by expressing fully lazy lambda-lifting in our framework, thus emphasizing the first-order essence of weak reduction.</p>", "authors": [{"name": "Thibaut Balabonski", "author_profile_id": "81466647879", "affiliation": "Univ Paris Diderot, Sorbonne Paris Cit&#233;, PPS, UMR 7126, CNRS, Paris, France", "person_id": "P2991286", "email_address": "Thibaut.Balabonski@pps.jussieu.fr", "orcid_id": ""}], "doi_number": "10.1145/2103656.2103713", "year": "2012", "article_id": "2103713", "conference": "POPL", "title": "A unified approach to fully lazy sharing", "url": "http://dl.acm.org/citation.cfm?id=2103713"}