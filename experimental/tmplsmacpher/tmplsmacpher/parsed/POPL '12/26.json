{"article_publication_date": "01-25-2012", "fulltext": "\n Resource-Sensitive Synchronization Inference by Abduction Matko Botin.can Mike Dodds Suresh Jagannathan \nUniversity of Cambridge University of Cambridge Purdue University matko.botincan@cl.cam.ac.uk mike.dodds@cl.cam.ac.uk \nsuresh@cs.purdue.edu Abstract We present an analysis which takes as its input a sequential pro\u00adgram, \naugmented with annotations indicating potential paralleliza\u00adtion opportunities, and a sequential proof, \nwritten in separation logic, and produces a correctly-synchronized parallelized program and proof of \nthat program. Unlike previous work, ours is not an in\u00addependence analysis; we insert synchronization \nconstructs to pre\u00adserve relevant dependencies found in the sequential program that may otherwise be violated \nby a na\u00a8ive translation. Separation logic allows us to parallelize .ne-grained patterns of resource-usage, \nmoving beyond straightforward points-to analysis. Our analysis works by using the sequential proof to \ndiscover dependencies between different parts of the program. It leverages these discovered dependencies \nto guide the insertion of synchro\u00adnization primitives into the parallelized program, and to ensure that \nthe resulting parallelized program satis.es the same speci.cation as the original sequential program, \nand exhibits the same sequential behaviour. Our analysis is built using frame inference and abduc\u00adtion, \ntwo techniques supported by an increasing number of separa\u00adtion logic tools. Categories and Subject Descriptors \nD.2.4 [Software Engineer\u00ading]: Program Veri.cation Correctness proofs; D.3.3 [Program\u00adming Languages]: \nLanguage Constructs and Features Concurrent programming structures General Terms Languages, Theory, Veri.cation \nKeywords Separation Logic, Abduction, Frame Inference, Deter\u00administic Parallelism 1. Introduction Automatically \nverifying the safety properties of shared-memory concurrent programs remains a challenging problem. To \nbe useful in practice, proof tools must (a) explore a potentially large number of interleavings, and \n(b) construct precise .ow-and path-sensitive abstractions of a shared heap. Just as signi.cantly, veri.cation \ncomplexity is often at odds with the straightforward intentions of the programmer. Low-level concurrency \ncontrol abstractions such as locks obfuscate higher\u00adlevel notions such as atomicity and linearizability, \nlikely to be exploited by the programmer when writing programs. While at\u00adtempts to incorporate these \nnotions directly into programs have met Permission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior \nspeci.c permission and/or a fee. POPL 12, January 25 27, 2012, Philadelphia, PA, USA. Copyright c &#38;#169; \n2012 ACM 978-1-4503-1083-3/12/01. . . $10.00 with some success for example, in software transactional \nmem\u00adory [21], there remains a substantial burden on the programmer to ensure programs can be partitioned \ninto easily-understood atomic and thread-local sections. Automated proof engines used to verify program \ncorrectness must also assimilate this knowledge to ensure proof-search focusses on relevant behaviour \n(e.g., serializability), and eschews irrelevant details (e.g., thread-local state) [18]; the alternative, \nexpressing such distinctions in programmer-supplied speci.cations, is too often onerous to be acceptable \nin practice. In any case, the inherent complexity of dealing with concurrency, both for the programmer \nand for the veri.er, unfortunately remains. In many cases concurrency is an optimization, rather than \nin\u00adtrinsic to the behaviour of the program. That is, a concurrent pro\u00adgrams is often intended to achieve \nthe same effect of a simpler se\u00adquential counterpart. Consequently, an attractive alternative to con\u00adstructing \na concurrent programming is to automatically synthesise one. In this approach, the programmer writes \na sequential program; the program is automatically transformed into a concurrent one ex\u00adhibiting the \nsame behaviour [3 5, 17]1. In this context, the veri.ca\u00adtion problem becomes signi.cantly more tractable. \nUnderstanding and verifying the concurrent program reduces to .rst verifying the sequential program, \nand second, verifying the parallelising trans\u00adformation. We propose a program analysis and transformation \nthat yields a parallelized program, guided by a safety proof of the original sequential program. To guide \nthe transformation, we assume that the programmer annotates points in the sequential program where concurrency \ncan be pro.tably exploited, without supplying any additional concurrency control or synchronization. \nThe result of the transformation is a concurrent program with corresponding behaviour, and a safety proof \nfor the concurrent program; in this sense, parallelized programs are veri.ed by construction. While our \nanalysis is driven by a safety proof, parallelization protects all behaviours, not just those speci.ed \nin the proof. Thus we can apply our approach to complex algorithms without needing to verify total functional \ncorrectness. Our transformation ensures that behaviour is preserved by requiring that the concurrent \nprogram respects sequential data\u00addependencies. In other words, the way threads access and modify shared \nresources never results in behaviour that would not be possi\u00adble under sequential evaluation. To enforce \nthese dependencies, the transformation injects barriers, signalling operations that regulate when threads \nare allowed to read or write shared state. These bar\u00adriers can be viewed as resource transfer operations \nwhich acquire and relinquish access to shared resources such as shared-memory data structures and regions \nwhen necessary. Our analysis is built on separation logic [32]. By tracking how resources are demanded \nand consumed within a separation logic 1 This approach is commonly known as deterministic parallelism, \nalthough our approach does not, in fact, require that the input sequential program is deterministic. \n proof, we can synthesize barriers to precisely control access to these resources. Our approach critically \nrelies on frame inference and abduction [11], two techniques that generate .ne-grained infor\u00admation on \nwhen resources are necessary and when they are redun\u00addant. This information enables optimizations that \ndepend on deep structural properties of the resource for example, we can split a linked list into dynamically-sized \nsegments and transmit portions between threads piece-by-piece. Our analysis thus enforces sequential \norder over visible be\u00adhaviours, while allowing parallelization where it has no effect on behaviour. Insofar \nas our technique safely transforms a sequential program into a concurrent one, it can also be viewed \nas a kind of proof-directed compiler optimization. Notably, our transformation is not based on an independence \nanalysis our focus is not to iden\u00adtify when two potentially concurrent code regions do not interfere; \ninstead, our analysis injects suf.cient synchronization to ensure rel\u00adevant sequential dependencies are \npreserved, even when regions share state and thus potentially interfere. Contributions. 1. We present \nan automated technique to synthesize a parallel pro\u00adgram given a partially-speci.ed sequential program \naugmented with annotations indicating computations that are candidates for parallelization. The provided \nspeci.cations are used to de.ne relevant loop invariants. 2. We leverage abduction and frame inference \nto de.ne a path\u00adand context-sensitive program analysis capable of identifying per program-point resources \nthat are both redundant these are resources that would no longer be used by the thread executing this \ncomputation; and, needed these are resources that would be required by any thread that executes this \ncomputation, but which is not known to have been released at this point. 3. We use information from \nthe above analysis to inject grant and allowed barriers into the original program; their semantics enable \nresource transfer from redundant to needed resource points. The analysis also constructs a safety proof \nin separation logic which validates the correctness of the barrier injections. 4. We prove that our \ntransformation is speci.cation-preserving: the parallelized program is guaranteed to satisfy the same \nspec\u00adi.cation as the sequential program. Moreover, for terminating programs that do not dispose memory, \nwe also show that the transformed program preserves the behaviour of the original. Complete proofs and \nother supporting material can be found in an associated technical report [8].  Paper structure. An overview \nof the approach and motivating examples are given in \u00a72. Issues that arise in dealing with loops and \nrecursive datatypes (such as lists) are discussed in \u00a73. The analysis and transformation are formalized \nin \u00a74 and \u00a75. Behaviour preservation results are discussed in \u00a76. Observations about the interplay between \nanalysis precision and the predicates available in de.ning speci.cations are given in \u00a77. \u00a78 discusses \nrelated work. 2. Overview The objective of our analysis is to take a sequential program (with annotations \nindicating where concurrency may be exploited), and to produce a parallelized program that conforms to \nthe same spec\u00adi.cation. To do this, we insert synchronization barriers enforcing those sequential dependencies \nthat can affect the observable be\u00adhaviour of the program. Our analysis uses separation logic to capture \nand manipulate these dependencies. The programmer must provide a proof of safety for the sequential program. \nThis proof is used to drive the parallelization process, allowing our analysis to calculate precisely \nFigure 1. A simple parallel program that operates over shared locations x and y. The number of concurrent \niterations performed is chosen nondeterministically to represent the act of receiving values from some \nunknown process or user interaction. global *x, *y; void main() { void f(i){ void g(*p, v){ local i, \nn; local v=*x; *p = v; n = nondet(); if (v>=i) { } x = alloc(); g(y, v); y = alloc(); } pfor(i=1;i++;i<n){ \nelse { f(i); g(x, 0); } } } } which resources can be accessed in parallel, and which must be accessed \nsequentially, in order to preserve salient sequential de\u00adpendencies. 2.1 Parallelization and Parallel-for \nWe assume that the portions of a program that can be parallelized will be written as parallel-for loops, \npfor. The intended semantics of a parallel-for loop is that every iteration will run in parallel, but \nthat the behaviour will be identical to running them in sequence; this semantics provides a simple but \nuseful form of data parallelism in which concurrently executing computations may nonetheless have access \nto shared state. For example, consider the program shown in Fig. 1. How can we parallelize this program \nwithout introducing any unwanted new behaviours, i.e., behaviours not possible under sequential execu\u00adtion? \nNa\u00a8ively, we might simply run every iteration of the pfor in parallel, without synchronization. That \nis: f(1) || f(2) || ... In some cases, this parallelization is good, and introduces no new observable \nbehaviours (e.g., if v is greater than n). But, under others, the second call to f() may write to a memory \nlocation that is subsequently read by the .rst call to f(), violating intended sequential ordering. For \nexample, consider the case when v is initially 1 and n is 2; sequential execution would produce a .nal \nresult in which the locations pointed to by x and y resp. are 0 and 1, while executing the second iteration \nbefore the .rst would yield a result in which the location pointed to by y remains unde.ned. To be sure \nthat no new observable behaviour is introduced, we must ensure that: no iteration can read from a location \nthat was already written to by a later iteration.  no iteration can write to a location that was already \nread from or written to by a later iteration.  In order to enforce this behaviour, we must introduce \na mechanism to force later iterations to wait for earlier ones. Note, crucially, that later iterations \nneed not always wait for ear\u00adlier ones synchronization is only needed when reads and writes to a particular \nmemory location could result in out-of-order behaviour. We want to enforce salient dependencies, while \nallowing bene.cial race-free concurrency.  2.2 Dependency-Enforcing Barriers Our analysis inserts barriers \ninto the parallelized program requir\u00ading logically later iterations to wait for earlier ones. We introduce \ngrant(), a barrier that signals to subsequent iterations that a re\u00adsource can safely be accessed, and \nwait(), a barrier that blocks until the associated resource becomes available from preceding it\u00aderations \n[17].  local v=*x; *p=v; if (v>=i) { grant(wy); grant(wx); } ga1(y, v); ga2(y, v); } } else { gb1(*p, \nv){ else { gb2(*p, v){ grant(wy); *p=v; gb2(x, 0); *p=v; gb1(x, 0) grant(wx); wait(wy); } }} } } } Figure \n2. Parallelization of f(). Only synchronization barriers between the .rst and second iterations are shown. \nHow can we use these barriers to enforce sequential depen\u00addencies in the example program? The exact pattern \nof paralleliza\u00adtion depends on the granularity of our parallelization analysis. In the best case, there \nare no dependencies between iterations (e.g., each iteration operates on a different portion of a data \nstructure). In this case, we need no barriers, and all iterations run indepen\u00addently. However, our example \nprogram shares the locations x and y, meaning barriers are required. In the worst case, each iteration \nof the pfor begins with a call to wait() and ends with a call to grant() this would enforce sequential \nordering on the iterations. A better (although still relatively simple) parallelization is shown in Fig. \n2. To simplify the presentation, we only show the synchronization barriers between the .rst and second \niterations of pfor. We write f1 for the .rst iteration, and f2 for the second. (Our full analysis generates \na single version of f() with suf.cient barri\u00aders to run an arbitrary number of iterations in parallel. \nSee \u00a72.6 for a description.) Two channels, wx and wy, are used to signal between f1 and f2 wx (resp. \nwy) is used to signal that the later thread can read and write to the heap location referred to by x \n(resp. y). The function g() has different resource requirements depending on its calling context. Consequently, \nwe have specialized it to embed appropriate barriers depending on its context. Our analysis inserts barriers \nso that an iteration of the parallel\u00adfor only calls grant() on a signal when the associated resource \nwill no longer be accessed. Similarly, it ensures that an iteration calls wait() to acquire the resource \nbefore the resource is ac\u00adcessed. How does our analysis generate these synchronization barriers? The \nexample we have given here is simple, but in general our anal\u00adysis must cope with complex dynamically-allocated \nresources such as linked lists. It must deal with the partial ownership (for example, read-access), and \nwith manipulating portions of a dynamic struc\u00adture (for example, just the head of a linked list). We \ntherefore need a means to express complex (dynamic) patterns of resource man\u00adagement and transfer. To \nachieve this, our analysis assumes a sequential proof, writ\u00adten in separation logic, rather than just \nan undecorated program. This proof need not capture full functional correctness it is suf.\u00adcient just \nto prove memory-safety. We exploit the dependencies ex\u00adpressed within this proof to determine the resources \nthat are needed at a program point, and when they can be released. Our analysis in\u00adserts barriers to \nenforce the sequential dependancies represented in this proof. As our proof system is sound, these dependencies \nfaith\u00adfully represent those in the original sequential program.  2.3 Resources, Separation and Dependency \nAt the heart of our analysis is automated reasoning using concur\u00adrent separation logic [28, 32]. Separation \nlogic is a Hoare-style pro\u00adgram logic for reasoning about mutable, allocatable resources. A separation \nlogic proof of a program C establishes a speci.cation {P } C {Q} This can be be read as saying: the program \nC, if run in a re\u00adsource satisfying P , will not fault, and will give a resource satisfy\u00ading Q if it \nterminates . In general, a resource can be anything for which ownership can be partitioned ( separated \n) between different threads [10]. In practice, resources are most often heap-allocated structures such \nas lists, trees, locks, etc, where separation corre\u00adsponds to disjointness between underlying heap addresses. \nAn essential feature of separation logic is that speci.cations are tight. This means that all of the \nresources accessed by a program C must be described in its precondition P , or acquired through explicit \nresource transfer. No other resources will be accessed or affected by C when executed from a resource \nsatisfying P . The tight interpretation is essential for our analysis. Suppose we prove a speci.cation \nfor a program (or portion thereof); resources outside of the precondition cannot affect the program s \nbehaviour, and consequently can be safely transferred to other threads. One result of this tight interpretation \nis the frame rule, which allows a small speci.cation to be embedded into a larger context. {P } C {Q} \nFRAME {P * F } C {Q * F } Here * is the separating conjunction. An assertion P *F is satis.ed if P and \nF hold, and the portions of the state they denote do not overlap. The concurrent counterpart of the frame \nrule is the parallel rule. This allows two threads that access non-overlapping resources to run in parallel, \nwithout affecting each other s behaviour. {P1} C1 {Q1}{P2} C2 {Q2} PARALLEL {P1 * P2} C1dC2 {Q1 * Q2} \nThe parallel rule enforces the absence of data-races between C1 and C2. The two threads can consequently \nonly affect each other s behaviour by communicating through race-free synchronization mechanisms, such \nas locks or barriers. Automatic inference. Automated reasoning in separation logic revolves around two \nkinds of inference questions. The .rst, frame inference, calculates the portion of a formula S which \nis left over , once another formula P has been satis.ed. We call this remainder F? the frame, and write \nthe inference question as follows: S f P * [F?] (Throughout the paper, we use square brackets to indicate \nthe portion of the entailment that is to be computed.) The second kind of inference question, abduction, \ncalculates the missing formula that must be combined with a formula S in order to satisfy some other \nformula P . We call this formula A? the antiframe, and write the inference question as follows: S * [A?] \nf P Frame inference and abduction form the basis of symbolic execu\u00adtion in separation logic [2, 11]. \nIntuitively, frame inference lets us reason forwards, while abduction lets us reason backwards. Frame \ninference can work out which portions of the state will (and will not) be affected by the command, while \nabduction can work out what extra resources are necessary to execute the command safely. Suppose we have \na symbolic state represented by an assertion S, and a command c with speci.cation {P } c {Q}. If we calculate \nthe frame in S f P * [F?], the symbolic state after executing c must be Q * F?. The tight interpretation \nof triples is necessary for this kind of reasoning. Because a speci.cation must describe all the resources \naffected by a thread, any resources in the frame must be unaffected. Conversely, if we calculate the \nantiframe S * [A?] f P , Redundant and needed resources. The tight interpretation means that a proof \nin separation logic expresses all resources modi.ed by each individual command. Because separation logic \nensures race\u00adfree behaviour, a proof also expresses all the resources that can affect the observable \nbehaviour of each individual command in the program. Our parallelization analysis uses this to calculate \nre\u00adsources that are redundant and those that are needed during the execution of the program.  We use \nframe inference to determine redundant resources resources that will not be accessed in a particular \nportion of the program, and which can thus be safely transferred to other threads. Conversely, we use \nabduction to determine needed resources resources that must be held for a particular portion of the program \nto complete, and which must thus be acquired before the program can proceed safely. In our analysis, \nwe generally calculate the redundant resource from the current program point to the start of the subsequent \nitera\u00adtion. This resource can be transferred to the subsequent iteration us\u00ading a grant() barrier. We \ngenerally calculate the needed resource from the end of the previous iteration to the current program \npoint. This resource must be acquired from the previous iteration using a wait() barrier before execution \ncan proceed further. Note that these two resources need not be disjoint. A resource may be used early \nin an iteration, in which case it will be both needed from the previous iteration to the current point, \nand redun\u00addant from the current point up to the next iteration. Note also that redundant and needed resources \nneed not cover the entire state some resource described in the proof may never be accessed or modi.ed \nby the program.  2.4 Algorithm Overview The user supplies a sequential program which makes use of the \npfor parallel-for construct, as well as a sequential proof written in separation logic establishing the \nmemory-safety of the program. The high-level structure of our algorithm is then as follows: 1. The resource \nusage analysis phase uses abduction and frame inference to discover redundant and needed resources for \ndif\u00adferent portions of the program. 2. The parallelising transformation phase consists of two parts: \n (a) The resource matching phase matches redundant resources in one iteration of the parallel-for with \nneeded resources in the subsequent iteration. (b) The barrier insertion phase converts the sequential \nprogram into a concurrent program and inserts grant() and wait() barriers consistent with the discovered \nresource-transfer.   The result is a parallelized program, and its separation logic proof. 2.5 Resource \nUsage Analysis The resource usage analysis takes as its input the program, and a separation logic proof \nfor the program for some speci.cation. Con\u00adsider once again the functions f() and g(), which we introduced \nat the start of this section. Let us assume the programmer proves the following speci.cation: {x . x \n* y . y}f(i) {(x = i . x . x * y . x) . (x . 0 * y . y)} Fig. 3 shows an outline proof of this speci.cation. \nFor each program point, the resource usage analysis computes a pair of assertions: the needed resource \nand the redundant resource x . x * y . yp .. v = v void f(i) {void g(*p, v) { x . x * y . yp .. v = \nv localv=*x;*p=v; } v = x . x . x * y . y p . v if(v>= i) { v = x . v = i . x . x * y . y g(y, v); \n} else {  v = x . v < i . x . x * y . y g(x, 0); } }  (x = i . x . x * y . x) . (x . 0 * y . y) Figure \n3. Proofs of functions f() and g(). (see Fig. 4). The needed resource is the portion of the sequential \nprecondition required to reach the program point without faulting; the redundant resource is the portion \nof the resource held in the sequential proof that is unnecessary for reaching the end of the function. \nIn other words, needed assertions at program point p denote resources that are needed from the beginning \nof the function up to (but not including) p; redundant assertions denote resources that are not required \nfrom p to the end of the function. For example, the redundant resource established within the false branch \nof the conditional in f asserts that a thread executing this function will no longer require access to \ny at this point whenever v <i. The needed resource here asserts that the thread reaching this point still \nrequires access to x, provided v<i.  2.6 Generalizing to n Threads Our analysis can handle an unbounded \nnumber of loop iterations running in parallel. Fig. 5 shows the .nal parallelization of our example, \ngeneralized to deal with an arbitrary number of iterations. A pfor is translated to a sequential for \nloop, in which each iteration forks a new copy of the parallelized loop body. Resources are transferred \nbetween the threads in order of thread-creation. That is, the nth iteration of the pfor acquires resources \nfrom the logically-preceding (n - 1)th iteration, and releases resources to the logically (n + 1)th iteration. \nThis ordering is implemented through shared channels a thread shares with its predecessor a set of channels \nfor receiving resources, and with its successor a set of channels for sending resources. The number of \nthreads and so, the required number of channels is potentially decided at run-time. Consequently, chan\u00adnels \nare dynamically allocated in the main for-loop using the newchan() operation [17]. Each iteration creates \na set of new channels, and passes the prior and new set to the forked thread. The parallelized version \nof f (Fig. 5) now takes four channel arguments, a pair of each for x and y. The prior channels are used \nfor resource transfer with the logically-preceding thread (here wx, wy), and the new channels are used \nto communicate resource transfer with the logically-following thread (here, wx , wy ). Our analysis generates \ntwo versions of function g, specialized to the two contexts in which it is invoked. Function ga executes \nwhen v >= i. Here, the executing thread needs write access to y, which it acquires by calling wait(wyp). \nFunction gb needs write access to x, but it runs in a context where the resource x has already been acquired. \nConsequently, it need not call wait. Both versions release a resource to the following thread using grant \nbefore returning.  void f(i) { n: emp r : x< i . y . y local v = *x; n: x . x r : x = i . x . x) . (x< \ni . y . y) if (v>=i){ n: x = i . x . x r : x = i . x . x g(y, v); -. void g(*p, v) { n: x = i . x . x \nr : x = i . x . x *p = v; n: x = i . x . x * y . y r : x . x * y . y } } else { n: x< i . x . x r : \nx< i . y . y g(x, 0); -. void g(*p, v) { n: x< i . x . x r : x< i . y . y *p = v; n: x< i . x . x r : \nx . x * y . y } } n:(x = i . x . x * y . y) . (x< i . x . x) r : x . x * y . y } Figure 4. Redundant \nand needed resources in the function f(). Function g() is inlined, as its resource usage depends on the \ncalling context. 3. Loops and Recursive Data Structures Up to this point, we have dealt with channels \ntransferring single heap locations injected into straight-line code. Our analysis can in fact handle \nmore complicated heap-allocated data-structures such as linked lists, and control-.ow constructs such \nas loops. To illus\u00adtrate, consider the example program sum_head(n) shown (and ver\u00adi.ed) in Fig. 6. sum_head(n) \nsums the values stored in the .rst n elements of the list, then zeros the rest of the list. An important \nfeature of our approach is that the input safety proof need not specify all the relevant sequential properties; \nall sequential dependencies are enforced. Thus we can see sum_head as representative of the class of \nalgorithms that traverse the head of a list, then mutate the tail. With minor modi.cations, the same \nproof pattern would cover insertion of a node into a list, or sorting the tail of the list. Consider \ntwo calls to this function within a parallel-for: void main(){ pfor(i=1;i++;i<3){ n=nondet(); sum_head(n) \n} As above, for the sake of exposition we specialize the .rst and second iterations of the pfor loop, \ncalling them sum_head1 and sum_head2 . Our analysis generalizes to n iterations exactly as de\u00adscribed \nin \u00a72.6. The barriers injected into sum_head must enforce the following properties: sum_head2 must not \nwrite to a list node until sum_head1 has .nished both writing to and reading from it. Consequently, if \nn2 void main() { local i, n = nondet(); x = alloc(); y = alloc(); chan wx , wy ; chan wx = newchan(); \nchan wy = newchan(); grant(wx); grant(wy); for (i =0; i++;i<n){ wx = newchan(); wy = newchan(); fork(f(i, \nwx, wy, wx , wy )); wx=wx ;wy =wy ; } wait(wx); wait(wy); } f(i, wxp, wyp, wx, wy){ ga(*p, v, wyp, wy){ \nwait(wxp); wait(wyp); local v = *x; *p = v; if (v >= i) { grant (wy); grant(wx); } ga(y, v, wy); } else \n{ gb(*p, v, wx){ wait(wyp); *p = v; grant(wy); grant(wx); gb(x, 0, wx); } } } Figure 5. Parallelization \nof our running example generalized to deal with n threads. Channels are used to signal availability of \nresources from one thread to another. < n1, sum_head2 must wait for sum_head1 to .nish summing the values \nstored in the .rst n1 nodes before writing to them. sum_head2 must not read from a list node until sum_head1 \nhas .nished writing to them. Consequently, sum_head2 must wait for sum_head1 to .nish zeroing the value \nstored in a node before reading from the node. This example is substantially more subtle than our earlier \none, requiring more than a simple points-to analysis, because the list is not divided into statically-apparent \nreachable segments. In the worst case, a wait() at the start of sum_head2 () and a grant() at the end \nof sum_head1 () enforces sequential order. However, by reasoning about the structure of the manipulated \nlist using the safety proof given in Fig. 6, our approach can do considerably better. The parallelization \nsynthesized by our analysis is shown in Fig. 7 (the general n-thread version is given in the technical \nreport [8]). This parallelization divides the list into two segments, consisting of the portions read \nand written to by sum head1(). A shared heap\u00adlocation xpr stores the starting address of the portion \nwritten by sum_head1 (). The thread sum_head2 uses xpr to control when to access the second segment of \nthe list2. We discuss how the analysis materializes xpr below. Handling dynamic structures means dealing \nwith allocation and disposal. Fortunately, separation logic handles both straightfor\u00adwardly. Updates \nto the data-structure and object allocation are by assumption re.ected in the invariants of the original \nsequential proof. Thus updates and allocations are also re.ected in the in\u00advariants which our analysis \nconstructs to represent the contents of channels. However, introducing allocation and disposal affects \nthe behaviour-preservation result discussed in \u00a76; this result ensures 2 For simplicity, here we write \nsum head2 with wait controlled by condi\u00adtionals. The actual transformation performs syntactic loop-splitting \nto avoid the need to modify the loop invariant. Details are given in \u00a75.2.  lnode *hd; hd . h * lseg(h, \nnil) sum_head(n) { local i, sum, x; inti= 1; intsum =0; x = *hd; hd . x * lseg(x, nil) // entailment \non lseg predicate hd . h * lseg(h, x) * lseg(x, nil) while(x!=nil &#38;&#38; i!=n) { hd . h * lseg(h, \nx) *.v, y. x.val . v * x.nxt . y * lseg(y, nil) sum += x.val; i++; x = x.nxt; } hd . h * lseg(h, x) \n* lseg(x, nil) while(x!=nil) { hd . h * lseg(h, x) *.v, y. x.val . v * x.nxt . y * lseg(y, nil) x.val \n= 0; x = x.nxt; } } hd . h * lseg(h, nil) Figure 6. Separation logic proof of sum_head, a list-manipulating \nprogram whose automated parallelization requires reasoning over complex assertions and predicates. sum_head1(n){ \nsum_head2(n){ i=1; i=1; sum=0; sum=0; x = *hd; wait(i1); while(x!=nil &#38;&#38; i!=n){ x = *hd; sum+=x.val; \nwhile(x!=nil &#38;&#38; i!=n){ i++; if(x==*xpr) wait(i2); x = x.nxt; sum +=x.val; } i++; *xpr = x; x \n= x.nxt; grant(i1); } while(x!=nil){ while(x!=nil){ x.val = 0; if(x==*xpr) wait(i2); x = x.nxt; x.val \n= 0; } x = x.nxt; grant(i2); } }} Figure 7. Parallelization of sum_head for two threads. the program \nbehaviour is unaffected by the translation (i.e. the translation enforces deterministic parallelism). \nReasoning about list segments. We assume that our separation logic domain includes the predicate lseg(x, \nt), which asserts that a segment of a linked list exists with head x and tail-pointer t. We de.ne lseg \nas the least separation logic predicate satisfying the following recursive equation3: lseg(x, t) x = \nt . emp . .v, y. x.val . v * x.nxt . y * lseg(y, t) We assume that the programmer proves the following \nspeci.ca\u00adtion for the sequential version of sum_head: {hd . h*lseg(h, nil)} sum_head(n) {hd . h*lseg(h, \nnil)} 3 Our analysis depends strongly on the choice of these basic predicates. See \u00a77 for a discussion \nof alternatives to lseg. This speci.cation is trivial: all it says is that executing sum_head beginning \nwith a list segment, results in a list segment. Fig. 6 shows a proof of this speci.cation. We run our \nresource-usage analysis over the program to deter\u00admine redundant and needed resources. Consider the following \nloop from the end of sum_head1 : ... while(x!=nil) { x.val = 0; x = x.nxt; } Our analysis reveals that \nonly the resource de.ned by lseg(x, nil) is needed from the start of this loop to the end of the iteration. \nCom\u00adparing this resource to the corresponding invariant in the sequential proof reveals that the resource \n.h. hd . h*lseg(h, x) is redundant at this point. This assertion represents the segment of the list that \nhas already been traversed, from the head of the list to x. Materialization and barrier injection. Notice \nthat the assertions generated by our analysis are partly expressed in terms of the local variable x, \nwhich may change during execution. In order to safely transfer these assertions to subsequent iterations \nof the pfor, we need them to be globally accessible and invariant. To satisfy this goal, we could simply \nexistentially quantify the offending variable, x, giving a redundant invariant .h, y. hd . h * lseg(h, \ny) However, such a weakening loses important information, in partic\u00adular the relationship between the \nnecessary resource, the list seg\u00adment from x and the tail of the list. To retain such dependency re\u00adlationships, \nour analysis materializes the current value of x into a location xpr shared between sum_head1 and sum_head2 \n. An as\u00adsignment is injected into sum_head1 at the start of the second loop: ... *xpr = x; while(x!=nil) \n{ ... After the assignment to xpr, the redundant state can now be de\u00adscribed as follows: 1/2 .h, y. \nhd . h * lseg(h, y) * xpr - . y Here we use fractional permissions in the style of [6] to allow a 1/2 \nlocation to be shared between threads. The assertion xpr - . y represents fractional, read-only permission \non the shared location xpr. This helps in binding together the head of the list and the remainder of \nthe list when they are recombined. When traversing the list, sum_head2 compares its current posi\u00adtion \nwith xpr. If it reaches the pointer stored in xpr, it must wait to receive the second, remainder segment \nof the list from sum_head1 . 4. Technical Background 4.1 Programming Language and Representation We \nassume the following heap-manipulating language4: e ::= x | nil | t(\u00afe) | ... (expressions) b ::= true \n| false | e = e | e (booleans) = e | ... a ::= ... (atomic commands) C ::= C; C | e : skip | e : a | \ne : x := f(\u00afe) | e : return e | e : if(b) { C } else { C }| e: while(b) { C }P ::= global r\u00af; (f(\u00afx) \n{local \u00afy; C })+ 4 To avoid introducing an extra construct, we de.ne for(C1; C2; b){C3}as C1; while(b){C2; \nC3}.  void f(int i){ fs : x . x * y . y \u00a31 : \u00a32 : \u00a33 : int v = *x; if (v >= i){ g(y, v); e1 : e2 : x \n. x * y . y v = x . x . x * y . y } e3 : v = x . v = i . x . x * y . y else{ \u00a34 : g(x, 0); } } e4 : fe \n: v = x . v < i . x . x * y . y (x = i . x . x * y . x) . (x < i . x . 0 * y . y) gs : p . . v = v void \ng(int* p, int v){ \u00a35 : *p = v; e5 : ge : p . . v = v p . v } Figure 8. Left: labels for commands in \nf and g. Right: associated assertions in the sequential proof. To simplify the exposition slightly, we \nassume a .xed input pro\u00adgram Pof the following form: mainP() { pfor(C1; C2; b) { work(); }} Our parallelization \napproach generates a new program Ppar that allows safe execution of (a transformed version of) work in \nseparate threads. We denote by Func 3{main, work} the set of functions declared in a program P. Labelling \nof commands. Every command C . Cmd in the program is indexed by a unique label e . Label; function identi.ers \nare also treated as labels. We identify a particular command by its label, and when needed denote by \ncmd(e) the command at the label e. The set of all labels occurring in C is given by Lbs(C). The left\u00adhand \nside of Fig. 8 shows a labelling of the functions f and g. Commands within a block (function, if-else \nor while) form a sequence identi.ed by the sequence of the corresponding labels. Functions pred, succ \n: Label -Label return the label of the previ\u00adous (the next, resp. ) command in the sequence. For e corresponding \nto a function name, a function call or a while loop, the predeces\u00adsor of the .rst command and successor \nof the last command in the block are denoted by es and ee, respectively. For e corresponding to tt ff \nif-else, es (ee) and es (ee) are labels of the predecessor of the .rst (the successor of the last) commands \nin the if and else branches, respectively. Given labels e and e' within the same block, we say that e<e' \nif ' n .n = 1 such that e= succ (e). Let e. and e. denote the smallest (resp. largest) label in the block \ncontaining e. Let [e, e') denote the sequence of labels between e inclusively and e' exclusively, and \nlet P[\u00a3,\u00a3!) stand for the corresponding program fragment. Analogously, we de.ne (e, e') and P(\u00a3,\u00a3!). \nProgram representation. A program is represented via a control\u00ad.ow graph with Label as the set of nodes \nand as the set of edges all (e, e') such that e corresponds to a label of a function call and e' . tt \nff (es,ee), or e corresponds to if-else and e' .(es,e e).(es,e e). A path delimiter is a .nite sequence \nof nodes in the control\u00ad.ow graph. Intuitively, a path delimiter represents the sequence of function \ncalls and conditionals that need to be traversed in order to reach a particular assertion. Due to the \nstructure of our analysis, we are only interested in the set of path delimiters in the function work, \ndenoted by AP. We often want to manipulate and compare path delimiters. For . = e1 ...en . AP, we write \n.[i] to denote ei, .[i..j] to denote ei ...ej and |.| to denote the length n. For path delimiters . and \n.' , .A.' denotes their longest common pre.x, i.e., for k = |.A.'|we have .j = k, . A .' = .[j]= .'[j] \nand if |.|, |.'| >k then .[k +1] = .'[k +1]. We de.ne a partial order -on AP as follows. We say that \n. -.' iff .A.' = . and |.| < |.'|, or for k = |.A.'| we have |.|, |.'| >k and .[k +1] <.'[k +1]. We say \nthat . : .' iff . -.' or . = .'. Lemma 1. (AP, :) is a lattice with the least element works and the greatest \nelement worke. For G ..n AP we de.ne max(G) as max(G) := {. . G | \u00ac..' . G .. -.'}. We de.ne min(G) analogously. \n 4.2 Assertion Language and Theorem Prover Assertions in our approach are expressed using a class of \nseparation logic formulae called symbolic heaps. A symbolic heap . is a formula of the form .\u00af x. . . \nS where . (the pure part) and S (the spatial part) are de.ned by: . ::= true | e = e | e = e | p(e) | \n. . . S ::= emp | x . e | lseg(e, e) | s(e) | S * S Here x\u00afare logical variables, e ranges over expressions, \np(e) is a family of pure (.rst-order) predicates (such as e.g., arithmetic inequalities, etc), and s(e) \na family of other spatial predicates (such as e.g., doubly-linked lists, trees, etc) besides points-to \nand linked\u00adlist predicates previously discussed. We refer to the pure and the spatial part of . as .. \nand .S. We denote set of all quanti.er-free .rst-order formulae built in the same way as . but also allowing \nthe . connective by ... During our analysis, we often need to substitute variables, for example when \nrecasting an assertion into a different calling con\u00adtext. If e = x\u00af . e\u00afis a mapping from variables in \nx\u00afto e\u00afthen by .[e] we denote the formula obtained by simultaneously substi\u00adtuting every occurrence of \nxi in . with the corresponding ei. We denote by d-1 the inverse variable mapping, if d is injective5. \nThe set of all symbolic heaps is denoted by SH. We represent a disjunction of symbolic heaps as a set \nand interchangeably use the . and . operators. The set of all disjunctive symbolic heaps is P(SH). We \noverload the . and * operators in a natural way: for .i =.i . Si, i =1, 2, we de.ne .1 * .2 = (.1 . .2) \n. (S1 * S2), . . .i = (. . .i) . Si and S * .i =.i . (S * Si). Operators . and * distribute over ., thus \nwe allow these operations on disjunctive heaps just as if they were on symbolic heaps and furthermore \nuse the same notation . to refer to both symbolic and disjunctive symbolic heaps. We assume a suf.ciently \npowerful (automated) prover for sepa\u00adration logic that can deal with three types of inference queries: \n .1 f .2 * [.F ] (frame inference): given .1 and .2 .nd the frame .F such that .1 f .2 * .F holds;  \n.1 * [.A] f .2 (abduction): given .1 and .2 .nd the missing assumption .A such that .1 * .A f .2 holds; \n .1 * [.A] f .2 * [.F ] (bi-abduction): given .1 and .2 .nd .A and .F such that .1 * .A f .2 * .F holds. \n As before, square brackets denote the portion of the entailment that should be computed. We sometimes \nwrite [] for a computed assertion that is existentially quanti.ed and will not be reused. None of these \nqueries has a unique answer in general. However, for our analysis any answer is acceptable (though some \nwill give rise to a better parallelization than the others). Existing separation logic tools generally \nprovide only one answer. 5 In our framework, substitutions are always guaranteed to be injective be\u00adcause \nthe variables being substituted correspond to heap locations and chan\u00adnel resources whose denotations \nare guaranteed to be distinct; if the substi\u00adtution involves values, then they must be implicitly existentially \nquanti.ed, and can therefore be assumed to be distinct.  4.3 Sequential Proof We assume a separation \nlogic proof of the program P, represented as a map P : Label .P(SH). Intuitively, assertions in the proof \nhave the property that for any label e executing the program from a state satisfying P(e) up to some \nsubsequent label e ' will result in a state satisfying P(e ' ), and will not fault. More formally, we \nassume functions Pre, Post : Label -P(SH) associating labels of atomic commands and function calls with \ntheir pre-and post-condition, respectively, and a function Inv : Label -P(SH) associating while labels \nwith loop in\u00advariants. We also assume O, a mapping from labels to variable substitutions such that O(l)= \nx\u00af . x maps formal variables x\u00af \u00af' \u00af' sertion. Finally, we assume a function F : Label -P(SH) giving \nthe framed portion of the state. We write .[O(l)] to represent the heap constructed by applying the substitutions \nde.ned by O(l) to the assertion .. Then at each label e we have P(e) f Pre(e)[O(e)] * F(e) and Post(e)[O(e)] \n* F(e) f P(succ(e)). If e is a while label then Pre(e) and Post(e) are replaced by Inv(e). This structure \nmeans that the proof is modular, i.e., each atomic command, function and loop is given a speci.cation \nin isolation , without a reference to the particular context in which the speci.\u00adcation is being used. \nThe right-hand side of Fig. 8 shows a proof for the functions f and g which is in this form. Our approach \nis agnostic to the method by which the proof P is created: it can be discovered automatically (e.g., \nby a tool such as Abductor [11]), prepared by a proof assistant, or written manually. in the speci.cation \nassertion to actual variables x in the proof as- Proof assertions inlining. The proof P assumes use of \na modular speci.cation for each function. However, our analysis needs to refer to the assertions in the \nlocal function with respect to the variables of the caller, all the way up to the top-most work function. \nWe therefore de.ne a process for lifting local assertions with respect to their global contexts. For \n. . AP such that |.| = m and e1,...,en are the labels corresponding to the function calls (in the order \nof occurrence as in .) we de.ne the lifted ( inlined ) proof assertion P(.) as F(e1)*(F(e2)*... (F(en)*P(.[m])[O(en)])[O(en-1)]...)[O(e1)] \nIntuitively, the assertion P(.) represents the global proof state at . (including the framed portions) \nin terms of work s variables. Finally, let pc: AP -.. represent the path constraint asso\u00adciated with \neach path delimiter. The path constraint at . comprises the pure part of P(.) corresponding to the assumptions \nfrom the conditionals encountered on the way from works to .. The path constraint may be extracted directly \nfrom the proof or computed by some other means. 5. Parallelization Algorithm We now formally de.ne our \nparallelization algorithm. The goal of our approach is to construct a parallelized version of the input \nprogram P. In particular, our approach generates a new function work ' in Ppar (invoking possibly transformed \ncallees) such that mainPpar () { // initial channel creation. for(C1; C2; b) { // channel creation. fork(work \n' ,...); } // channel .nalization. } has the same behaviour as the original mainP. Algorithm 1 Computing \nlocally needed resources using backward symbolic execution. function NEEDED-LOC((e ' ,e '' ): Label \u00d7 \nLabel, .: P(SH)) e := e '' while e = e ' do e := pred(e) if cmd(e) matches iff(b) { C } else { C ' } \nthen tt NEEDED-LOC((e ,e ), .) . se . := P(e). . ff NEEDED-LOC((e ,e ), .) se if cmd(e) matches while(b) \n{ C } then Inv(e)[O(e)] * [.A] f . * [] . := P(e). . (Inv(e)[O(e)] * .A) else Post(e)[O(e)] * [.A] f \n. * [] . := P(e). . (Pre(e)[O(e)] * .A) return .  5.1 Resource Usage Analysis Our approach to parallelization \ntraverses the program by referring to a .nite pre.x-and <-closed subset of path delimiters P ..n AP. \nThis subset re.ects the portions of the program that are path-and context-sensitive targets of parallelization. \nThe set P provides pre\u00adcise control over which functions the analysis should address, but how this set \nis chosen is not considered here. Function invocations whose successors are not in P are treated as a \nsingle operation with effects de.ned by their speci.cation. The goal of resource usage analysis is to \ncompute the maps: redundant : P \u00d7 P -P(SH) needed : P \u00d7 P -P(SH) For p, q . P such that p -q, redundant(p, \nq) gives the resources that are guaranteed to not be accessed by the program between p and q. In parallelization, \nthese are resources that can safely be transferred to other parallel threads. For p -q, needed(p, q) \ngives the resources that might be accessed during execution from p to q. In parallelization, these are \nthe resources that must be acquired before execution of the current thread can proceed. The function \nNEEDED-LOC (Alg. 1) uses backward symbolic execution to compute needed resources between pairs of path \nde\u00adlimiters (e ' ,e '') in the same function block. It uses the pure parts of the sequential proof to \nguide the abductive inference. Since we al\u00adready have function summaries and loop invariants in the sequential \nproof, NEEDED-LOC gives a symbolic heap suf.cient to execute the fragment P[\u00a3!,\u00a3!!). Lemma 2. NEEDED-LOC(e \n' ,e '' ) is a suf.cient precondition for P[\u00a3!,\u00a3!!). Proof. Follows from the disjunctive version of the \nframe rule: .A .F {P } C {Qi} . *j f P * i.I j.J k.K k {(. * .jA)} C {(Qi * .Fk )} j.J i.I,k.K and \nHoare s rule of composition. The function NEEDED(.e,.s) (Alg. 2) lifts NEEDED-LOC to the context-sensitive \ninterprocedural level. Given two assertion points represented as path delimiters .s and .e, NEEDED(.e,.s) \nworks by successively pushing backwards an assertion . from .e to .s. The algorithm operates in two phases. \nIn phase A, it steps backwards from .e towards the outermost calling context in the function-invocation \nhierarchy. This context, represented as the longest common pre.x of .s and .e, is the dominator of the \ntwo  function NEEDED(.s : P,.e : P) k := |.e|. := P(.e) while k> |.s A .e| +1 do e := O(.e[1..k]) . \n:= NEEDED-LOC((.e[k].,.e[k]), .[e -1])[e] A k := k - 1 if cmd(.e[k]) is function call then . := .[O(.e[k])-1] \ne := O(.s[1..k]) . := NEEDED-LOC((.e[k],.s[k]), .[e -1])[e] while k< |.s| do if cmd(.s[k]) is function \ncall then . := .[O(.s[k])] B k := k +1 e := O(.s[1..k]) . := NEEDED-LOC((.s[k],.s[k].), .[e -1])[e] \n return . . . . . .. . . . . . . .. . . Algorithm 3 Computing redundant resources function REDUNDANT(.s \n: P,.e : P) P(.s) f NEEDED(.s,.e) * [.R] return .R functions in which .s and .e are found in the function \ncall graph. Phase B of the algorithm keeps stepping backwards, but proceeds inwards into the function-invocation \nhierarchy towards .s. Both phases of the algorithm use Alg. 1 to compute the needed resources in-between \nfunction call boundaries: in phase A we establish the needed assertions from the dominating point to \n.e, and in phase B from .s to the dominating point. Since the invariants of the input proof are written \nin terms of the outermost calling context, comparing locally-computed speci.\u00adcations with these invariants \nrequires the local speci.cations to be recast in terms of the outer context. In the .rst line of phase \nA we construct a variable substitution e that recasts the assertion in terms of the calling context at \nthe start of .e. The second line constructs .[e -1] the running state recast in terms of .e s starting \ncontext; this is typically the context de.ned by the work() function used in a pfor command. NEEDED-LOC \nconstructs a new needed state up to the start of the current block. Finally, e recasts the resulting \nstate back into the current context. When a function call is reached, we unwind the variable substitution \nby one call since we now have moved from the callee s context to a caller s. Operations in phase B are \nsimilar. The results computed by NEEDED are tabulated as follows. If P(.s) f NEEDED(.s,.e) * [], then \nneeded(.s,.e) := P(.s). . NEEDED(.s,.e); otherwise, needed(.s,.e) := P(.s). Lemma 3. needed(.s,.e) is \na suf.cient precondition to execute P from .s to .e. In order to be able to use the needed map further \nin the al\u00adgorithm we must ensure that it grows monotonically, i.e., that -. '' ..,. ' ,. '' . P such \nthat . -. ' we have needed(., . '' ) f needed(., . ' ) * []. If the underlying theorem prover behaves \ncon\u00adsistently with respect to failing and precision this property always holds. However, we can also \nmake an additional check and insert assertions from the sequential proof as needed. The redundant resource \nbetween two path delimiters is the por\u00adtion of the inlined proof-state in P that is not required by the \nneeded map. In Alg. 3 we calculate this by frame inference. fe e1 x < i . y . y e2 (v = x . v = i . \nx . x) . (v = x . v < i . y . y) e2e3 v = x . v = i . x . x e2e3ge v = x . v = i . x . x * y . y e2e4 \nv = x . v < i . y . y e2e4ge v = x . v < i . x . x * y . y Figure 10. redundant map with respect to fe. \n Lemma 4. If P(.s) f redundant(.s,.e) * [.] then . is a suf.cient precondition to execute Pfrom .s to \n.e. Consider the functions f and g from \u00a72. Fig. 8 shows the labels and sequential proof for these functions. \nThe map obtained by NEEDED is shown in Fig. 9. Fig. 10 shows the map computed by REDUNDANT with respect \nto path delimiter fe.  5.2 Parallelising Transformation We now describe a parallelising transformation \nbased on our resource-usage analysis. The construction proceeds in two phases. First, we compute an idealized \nresource transfer between threads. Then we inject grant and wait barriers to realise this resource transfer. \nThe resource transfer mechanism transfers a resource from one invocation of the work function to another. \nThis parallelising transformation can be viewed as just one application of the resource-usage analysis; \nmore optimized proof\u00adpreserving parallelising transformations are certainly possible. Our overall goal \nis to describe a framework for a resource-sensitive dependency-preserving analysis. Conditions on released \nand acquired resources. In the .rst phase we determine resources that should be released and acquired \nat particular points in the parallelized program. Released resources cannot be revoked, i.e., each released \nresource should be included in the redundant map from the point of the release to the end of the work \nfunction this way we know the resource will not be needed further. Acquired resources are held by the \nexecuting thread un\u00adtil released. Resources that are acquired along a sequence of path delimiters should \ncontain what is prescribed by the needed map between each of the path delimiters. We represent the result \nof this phase of the algorithm via the following maps: resource : ResId .P(SH), denoting resource identi.ers \nthat identify released and acquired resources;  released: P -ResId \u00d7 Subst, representing resources that \nare going to be released at a path delimiter together with the variable substitution applied at that \npoint;  acquired : P -ResId, representing resources that are going to be acquired at a path delimiter. \n We require the following well-formedness properties: 1. .. . dom(released) . .. ' > .. released(.)=(r, \n.) . (redundant(.,. ' ) f resource(r)[.] * [ ]); 2. .. . P . @r.dom(resource) {resource(r) |.. ' -. \n. acquired(. ' )= r}f needed(., worke) * []; 3. .. . dom(released) . @r.dom(resource) {resource(r) |.. \n' \u00ad. . acquired(. ' )= r}f released(.) * [].  The .rst property states we can release only resources \nthat are not needed between the given path delimiter and any subsequent one. The second property states \nthat the resources needed at a  \u00a32 \u00a32\u00a33 \u00a32\u00a33ge \u00a32\u00a34 \u00a32\u00a34ge fe \u00a31 x . x x = i . x . x x = i . x . x * \ny . y x < i . x . x x < i . x . x (x = i . x . x * y . y) . (x < i . x . x) \u00a32 v = x . v = i v = x . \nv = i * y . y v = x . v < i v = x . v < i . x . x (v = x . v = i * y . y) . (v = x . v < i . x . x) \u00a32\u00a33 \nv = x . v = i * y . y v = x . v = i * y . y \u00a32\u00a33ge v = x . v = i \u00a32\u00a34 v = x . v < i * x . x v = x . v \n< i * x . x \u00a32\u00a34ge v = x . v < i Figure 9. needed map (some entries omitted). Algorithm 4 Computing \nreleased and acquired resources. 1: N := needed; R := redundant 2: C := max{. . P |N (works,.)= emp} \n3: while C = {worke} do 4: SR := N (choose({(., . ' ) | . .C,. ' . P}))S 5: e := \u00afx . \u00afx ', where x\u00af' \nfresh 6: SR ' := SR[e] [ R(., worke)S f S ' ] R * [ 7: Cr := min . . P ... ' .C.. ' :. 8: if ..Cr pc(.) \n. true then 9: r := fresh resource id 10: resource(r) := S ' R 11: for all . .Cr do 12: released(.) := \n(r, e) 13: for all . ' s.t. . : . ' do 14: R(. ' , worke)S f SR * [.] 15: R(. ' , worke) := . 16: for \nall . .C do 17: acquired(.) := r 18: for all . ' ,. '' s.t. . : . ' : . '' do 19: N (. ' ,. '' ) * [] \nf SR ' * [.] N (. ' '' 20: ,. ) := . 21: C := max{. ' |.. .C . N (., . ' )= emp} path delimiter must \nhave already been acquired. The third property states that only the resources that have been previously \nacquired can be released.6 In general, there are many solutions satisfying properties 1 3. For instance, \nthere is always a trivial solution that acquires needed(works, worke) before the .rst command and releases \nit after the last, causing each invocation of work to be blocked until the preceding invocation .nishes \nthe last command. Of course, some solutions are better than others. Computing released and acquired maps. \nAlgorithm 4 constructs released, acquired and resource maps satisfying properties 1 3. Each iteration \nof the algorithm heuristically picks a needed re\u00adsource, and then iteratively searches for matching redundant \nre\u00adsource along all paths. The algorithm maintains a set C of all path delimiters up to which no more \nresources are needed. It terminates once no unsatis.ed needed resources remain (line 3). At the start \nof the main loop (line 4) the algorithm picks a still\u00adneeded resource between a path delimiter in C and \nsome further path delimiter. The picking of the needed resource is governed by a 6 We could relax the \nthird requirement if we extended our barriers to support renunciation [17], the ability to release a \nresource without .rst acquiring it. Renunciation allows a resource to skip iterations, giving limited \nout\u00adof-order signalling. We believe it would be straightforward to fold such techniques into the analysis, \nalthough such extensions are outside the focus of this paper. heuristic function choose, for which we \nmake no assumption. The choose function serves as a proxy for external knowledge about likely points \nfor parallelization. The key step of the algorithm is performed on line 7:  )S f S ' '' Cr := min. . \nP R(., workeR * [] ... .C.. :. Here R(., worke) is the redundant resource from . to the end of the work \nfunction and S ' is the candidate resource that we R want to acquire. The constructed set Cr is a set \nof path delimiters along which we can satisfy the candidate needed resource. In line 8, the algorithm \nchecks that Cr covers all paths by checking the conjunction of path constraints is tautologous. Resources \nstored in needed contain path constraints (and other conditions on local variables) embedded in the pure \npart of the symbolic heap. Since we can transfer resources between different path delimiters, we only \ntake the spatial part of the resource into consideration when asking entailment questions; this is denoted \nby a superscript S. Moreover, since the acquired resource is being sent to a different function invocation, \nwe substitute a fresh set of variables (line 5). The remainder of the algorithm is devoted to constructing \nthe new resource (line 10), and with updating released (lines 11 15), acquired (lines 16 20), and C (line \n21). Lemma 5. Maps resource, released and acquired computed by Algorithm 4 satisfy properties 1 3. Consider \nour running example. If choose picks (e1,e2) in the .rst iteration and (e2e3,e2e3ge) in the second iteration, \nthen the end result of Alg. 4 is resource = {r1 . (x . x),r2 . (y . y)}, released = {e2e3 . (r1, \u00d8),e2e4ge \n. (r1, \u00d8),e2e3ge . (r2, \u00d8),e2e4 . (r2, \u00d8)} and acquired = {e1 . r1,e2e3e5 . r2,e2e4 . r2}. Inserting \ngrant and wait barriers. In this phase we transform the sequential program P into a parallel program \nPpar by inserting grant and wait barriers. The inserted barriers realise resource transfer de.ned by \nthe maps released and acquired. (p)(p) We generate the parallel function work ' (i\u00afr , i\u00afr, env, env) \nin Ppar as follows: 1. To each r . ResId we assign a unique channel name ir. Denote by ir (p) the corresponding \nchannel of the previous thread in the sequence. 2. Let env be an associative array that for each channel \nmaps (escaped) local variable names to values. Let env(p) be such map from the previous thread in the \nsequence. env and env(p) are used for materialization. 3. For each . = e1 ...en . dom(released) . dom(acquired) \nlet ek1 ,...ekm be the labels in . corresponding to function calls. Then for each .j := e1 ...ekj we \ncreate in Ppar an identical copy f ' of the function f called at ekj and replace the call to f with the \ncall to f '. Let us denote by tr(. ' ) the path delimiter   4. For each . . dom(acquired) such that \nacquired(.)= r we insert a wait barrier wait(ir (p)) between path delimiters tr(pred(.)) and tr(.). \n 5. For each . . dom(released) such that released(.)=(r, ), between path delimiters tr(pred(.)) and tr(.) \nwe insert a se\u00adquence of assignments of the form env(ir)[ y ] := y for every local variable y, followed \nby a grant barrier grant(ir).  Each invocation of work creates a fresh set of local variables that are \nbound to the scope of the function. However, some resources must be expressed in terms of function-local \nvariables. Paralleliza\u00adtion must take account of this. If the structure of a resource depends on local \nvariables from a previous invocation, this must be encoded explicitly by materialising the variables \nof the previous invocation. The main function mainPpar in Ppar .rst creates the set of dummy channels; \nthen in the while loop repeatedly creates a set of new channels for the current iteration, forks a new \nthread with work ' taking the channels from the previous iteration as i\u00afr (p) and from the current iteration \nas i\u00afr, and at the end of the loop body assigns the new channels to the previous channels; and, after \nthe while loop completes waits on the channels in the last set. We generate the parallel proof Ppar from \nthe sequential proof P using the following speci.cations from [17]: {emp} i := newchan() {req(i,R) * \nfut(i,R)}{req(i,R) * R} grant(i) {emp}{fut(i,R)} wait(i) {R} Each variable R in Ppar associated with \nchannel ir is instantiated with the corresponding resource resource(r). The predicates req and fut track \nthe ownership of the input and output ends of the channel. To reason about threads, we use the standard \nseparation logic rules for fork-join disjoint concurrency (e.g., as in [19]). Theorem 6. Ppar is a proof \nof the parallel program Ppar, and de.nes the same speci.cation for mainPpar as P does for mainP. Loop-splitting. \nThe approach presented so far treats a loop as a single command with a speci.cation derived from its \ninvariant. Ac\u00adquiring or releasing resources within a loop is subtle, as it changes the sequential loop \ninvariant. It is not clear how to handle this in full generality, so we take a pragmatic approach that \nperforms heuristic loop-splitting. The example in \u00a73 uses two channels to transfer the segment of the \nlist traversed after the .rst and the second while loop, respectively. The resource released via channel \ni1 in Fig. 7 is hd . h * lseg(h, xpr). In the following iteration, the needed resource for the whole \nloop is hd . h * lseg(h, x). If we try to match released against needed, the entailment R(., worke)S \nf S ' ] R * [ in Algorithm 4 will fail. This is because the value of x is unknown at the start of the \nloop, meaning we cannot establish whether the released resource will cover the needed resource. One way \nto resolve this would be to acquire the entire list before the .rst loop, but this would result in a \nvery poor parallelization. Instead, we modify the structure of the loop to expose the point at which \nthe second list segment becomes necessary: 1. We split the spatial portion of the resource needed by \nthe whole loop into a dead (already traversed) and a live (still to be traversed) part. In our example, \nhd . h * lseg(h, x) would be the dead and lseg(x, nil) the live part. This kind of splitting is speci.c \nto some classes of programs, e.g., linked list programs that do not traverse a node twice. 2. We match \nthe resource against the dead part of the loop invariant and infer the condition under which the two \nresources  ... xpr:=envp(i1)[\"x\"]; x:=x.nxt; while(x!=nil&#38;&#38;i!=n&#38;&#38;x!=xpr){ while(x!=nil&#38;&#38;i!=n){ \nsum+=x.val; sum+=x.val; i++; i++; x:=x.nxt; x:=x.nxt; } } if(x!=nil&#38;&#38;i!=n&#38;&#38;x==xpr){ // \nremainder skipped. sum+=x.val; ... i++; else{ ... Figure 11. Loop splitting for the sum_head example. \n are the same. In our example, the entailment between the two resources holds if x = xpr. This condition \ncan be inferred by asking a bi-abduction question R(., worke)S . [c] f S ' ] R * [ with the pure abducted \nfact c. Now we can syntactically split the loop against the inferred condi\u00adtion c =(x = xpr) and obtain \na transformed version that ensures that after entering the true branch of the if statement the condition \nc holds. The transformation of our example is shown in Fig 11. Formally, we de.ne splitting of a command \n(comprising possi\u00adbly multiple loops) against a condition c as follows (to simplify, we assume here that \nevery command ends with skip): function SPLIT(C : Cmd,c:..) match C with | while(b) { C ' }; C '' . \nwhile(b .\u00acc ' ) { C ' }; if(b . c ' ) { C ' ; while(b) { C ' }; C '' }else { SPLIT(C '' ,c) } '''' '' \n| C ; C . C ; SPLIT(C ,c) | skip . skip The condition c ' is obtained from c by replacing a reference \nto (p)(p) every primed variable y ' by a reference to env(ir )[ y ], where ir is the channel name associated \nto the resource. It is not dif.cult to see that the accompanying proof of C can be split in a proof preserving \nway against c. This transformation can either be applied to P between the resource-usage analysis and \nparallelization, or embedded within Alg. 4.  5.3 Implementation We have validated our parallelization \nalgorithm by crafting a pro\u00adtotype implementation on top of the existing separation logic tool, coreStar \n[7]. While our implementation is not intended to provide full end-to-end automated translation, it is \ncapable of validating the algorithms on the examples given in the paper, and automatically answering \nthe underlying theorem proving queries. Our parallelization algorithm does not assume a shape invariant \ngenerator, except possibly to help construct the sequential proof. Soundness is independent of the cleanliness \nof the invariants (the analysis will always give a correct result, in the worst case default\u00ading to sequential \nbehaviour). Our examples in coreStar [7] have been validated using automatically-generated invariants. \nOther ef\u00adforts [11] indicate that bi-abduction works well with automatically\u00adgenerated invariants produced \nby shape analysis, even over very large code bases. 6. Behaviour Preservation A distinctive property \nof our parallelization analysis is that it en\u00adforces sequential data-dependencies in the parallelized \nprogram even if the safety proof does not explicitly reason about such de\u00adpendencies. The result is that \nour analysis preserves the sequential behaviour of the program: any behaviour exhibited by the paral\u00adlelized \nprogram is also a behaviour that could have occurred in the original sequential program. However, there \nare important caveats relating to termination and allocation.  Termination. If the original sequential \nprogram does not termi\u00adnate, our analysis may introduce new behaviours simply by virtue of running segments \nof the program that would be unreachable un\u00adder a sequential schedule. To see this, suppose we have a \npfor such that the .rst iteration of the loop will never terminate. Sequen\u00adtially, the second iteration \nof the loop will never execute. However, our parallelization analysis will execute all iterations of \nthe loop in parallel. This permits witnessing behaviours from the second (and subsequent) iterations. \nThese behaviours were latent in the original program, and become visible only as a result of parallelization. \nAllocation and disposal. If the program both allocates and dis\u00adposes memory, the parallelized program \nmay exhibit aliasing that could not occur in the original program. To see this, consider the following \nsequential program: x=alloc(); y=alloc(); dispose(x). For simplicity, we have avoided re-structuring \nthe program to use data parallelism via pfor this example could easily be encoded as such, however. Parallelization \nmight give us the following program: (x=alloc(); grant(wx); y=alloc()) || (wait(wx); dispose(x)) This \nparallelized version of the program is race-free and obeys the required sequential ordering on data. \nDepending upon the imple\u00admentation of the underlying memory allocator, however, x and y may be aliased \nif the dispose operation was interleaved between the two allocations. Such aliasing could not happen \nin the original non-parallelized version. Either kind of new behaviour might result in further new behaviours \nfor example, we might have an if-statement condi\u00adtional on x==y in the second example above. These caveats \nare common to our analysis and others based on separation logic for example, see the similar discussion \nin [13]. Proving behaviour preservation. We now sketch a behaviour preservation result (a detailed proof \nis given in [8]). The theorem de.nes preservation in terms of an interleaved operational seman\u00adtics for \na core language, similar to the one described in \u00a74.1, ad\u00additionally equipped with threads, and operations \non channels (such as grant and wait). Data-races are interpreted as faults in this se\u00admantics. We prove \nour result for a sequential thread t possibly exe\u00adcuting concurrently with other sequential threads this \ndegenerates into the purely sequential case when there are no other runnable threads. Because t is sequential, \nit does not call fork; we also as\u00adsume that it is guaranteed to terminate, and that it never disposes \nmemory based on the caveats discussed above. Theorem 7. Let t be a sequential thread, and let tpar a \ncorrespond\u00ading parallelized version, equipped with thread creation operations, and synchronization actions \nthat enforce sequential dependencies among the child threads it creates. Let Kpar be a terminating non\u00adfaulting \n(i.e., data-race free) trace of an execution of tpar. There exists a corresponding trace K derived by \nsubstituting correspond\u00ading operations in t for tpar such that: (1) K and Kpar begin in the same state; \n(2) for every thread t ' = t in K, K and Kpar ex\u00adhibit identical thread-local behaviour; and (3) the \nterminal state of thread t in Kpar is a substate of the corresponding state in K. Proof sketch. We show \nthat, under the assumption that forked child threads never wait for channels granted by their parent \nor later\u00adforked child threads, any terminating non-faulting trace Kpar can be reordered into a sequentialized \ntrace Kseq with the same thread\u00adlocal behaviour. By sequentialized, we mean that forked children must \nexecute to completion before their parents can be scheduled. We establish a simulation invariant between \nexecutions of tpar decorated with arbitrary calls to newch, wait, grant and fork, and executions of tpar \n. identical to tpar except with all such con\u00adstructs erased. The invariant establishes that every non-faulting \nse\u00adquentialized trace Kseq of tpar simulates some trace K of tpar. . We tie this result to our analysis \nby observing that paralleliza\u00adtion only inserts the four constructs newch, wait, grant and fork (leaving \naside issues of loop-splitting and materialization). As a re\u00adsult, tpar . = t. We also show that the \nway we insert signals ensures that they are ordered with respect to thread creation, as assumed in the \nprevious step. This establishes a behaviour-preservation result for non-faulting traces, but parallelization \nmight introduce faults. However, paral\u00adlelized programs are veri.ed using separation logic, meaning we \ncan assume that they do not fault when executed in a state satisfy\u00ading the precondition. This completes \nthe proof. One attractive property is that this proof does not place any explicit requirements on the \npositioning of barriers it is suf.cient that we can provide a separation-logic proof for tpar. The caveat \non mem\u00adory disposition manifests in the way we establish our simulation in\u00advariant, which necessarily \nassumes newly allocated data is always fresh and thus does not alias with any accessible heap-allocated \nstructure. The caveat on termination is necessary to ensure we can suitably reorder forked threads to \nyield a sequentialized trace. In addition to inserting barriers, our analysis mutates the pro\u00adgram by \nmaterialising thread-local variables and splitting loops. Both of these can be performed as mutations \non the initial sequen\u00adtial program, and neither affect visible behaviour. Loop-splitting is straightforwardly \nsemantics-preserving, while materialized vari\u00adables are only used to control barrier calls. Theorem 7 \nguarantees that parallelisation does not introduce deadlocks; otherwise the simulation relation would \nnot exist. The ordering on barriers ensures that termination in the sequential pro\u00adgram is preserved \nin the resulting parallel program. 7. Re.ning the Predicate Domain The structure of the sequential proof \naffects the success of paral\u00adlelization in two ways. First, the loop invariants may allow the anal\u00adysis \nto verify a parallelized program when it would otherwise have failed. Second, the choice of predicate \ndomain controls how much information about resource-usage is available to the analysis. Intu\u00aditively, \nenriching the domain may permit a .ner-grained splitting of resources, allowing redundant resources to \nbe identi.ed earlier or missing resources later. To see how the choice of abstract domain in.uences precision \nand effectiveness of the analysis, consider the example discussed in \u00a73; there, we parallelized sum_head \nusing the list segment pred\u00adicate lseg. An alternative predicate is lsegni(h, t, n, i), which ex\u00adtends \nlseg with a parameter n . Z recording the length of the list segment, and with a parameter i . (0, 1] \nrecording the permission on the list segment. If i =1 the thread has read-write access; other\u00adwise it \nhas read access only. We de.ne lsegni as the least predicate satisfying the following equation: lsegni(x, \nt, n, i)(x = t . n =0 . emp) . ( .v, y. x.val -i -i . v * x.nxt . y * lsegni(y, t, n-1,i) The equivalence \nlseg(h, t) .. .n. lsegni(h, t, n, 1) would allow the analysis to exploit this .ner-grained predicate \neven if the sequential proof is written using the coarse-grained lseg predicate.  sum_head1(n){ sum_head2(n){ \n*npr=n; i=1; grant(i1); sum = 0; i = 1; wait(i1); sum=0; x=*hd; x = *hd; while(x!=nil &#38;&#38; i!=n){ \nwhile(x!=nil &#38;&#38; i!=n){ if(n==*npr) wait(i2); sum += x.val; sum += x.val; i++; i++; x = x.nxt; \nx = x.nxt; }} *xpr = x; wait(i2); grant(i2); while(x!=nil){ while(x!=nil){ if(x==*xpr) wait(i3); x.val \n= 0; x.val = 0; x = x.nxt; x = x.nxt; }} grant(i3); } } Figure 12. Improved parallelization of sum_head. \nFunction sum_head only writes to the list after traversing n nodes. Consequently, it needs only non-exclusive, \nread-only access to the .rst n nodes. (Or the entire list, if the list is shorter than n nodes long.) \nWe can extend our resource usage analysis to deal with this richer predicate such that it identi.es the \nfollowing as a suf.cient precondition for the entire sum_head function: f j(.t. lsegni(h, t, n,i) * lseg(t, \nnil)) . hd - . h * ' '' (.n. lsegni(h, nil,n ,i) . n< n) Without the lsegni predicate, this precondition \ncould not be ex\u00adpressed. The availability of lsegni would also allow the analysis to discover that this \nresource is redundant at the start of sum_head: .h, i, j. hd -j . h * ' '' lsegni(h, t, n,i) . (.n. lsegni(h, \nnil,n ,i) . n<n) When sum_head(n) is called in a parallel-for, the subsequent call to sum_head can immediately \nread from the .rst n nodes of the list. Applying our analysis using the lsegni predicate yields the the \ntwo-thread parallelization shown in Fig. 12. i1 signals that the .rst n nodes can be read by the subsequent \niteration of sum_head. Similar to the transformation discussed in \u00a73, grant calls can be inserted on \ni2 to signal that the subsequent iteration can write to the .rst n nodes; and on i3 to signal that the \nentire list can be written to. Because this parallelization calls grant earlier than the parallelization \ndiscussed earlier, it can consequently extract more parallelism from the original sequential program. \nAs this discussion reveals, our analysis is generic in the choice of abstract domain; any separation \nlogic predicate could be used in place of lseg, for example. However, the success of automated parallelization \nis highly dependent on the power of the entailment prover in the chosen domain. The lseg domain is one \nof the best\u00addeveloped in separation logic, and consequently automated paral\u00adlelization is feasible using \ntools such as coreStar [7]. Other do\u00admains (such as trees) are far less developed. 8. Related Work Resource-usage \ninference by abduction. We have de.ned an in\u00adterprocedural, control-.ow-sensitive analysis capable of \ndetermin\u00ading the resource that will (and will not) be accessed between partic\u00adular points in the program. \nAt its core, our analysis uses abductive reasoning [11] to discover redundancies that is, state used \nearlier in the program that will not be accessed subsequent to the current program point. Using abduction \nin this way was .rst proposed in [15], where it is used to discover memory leaks, albeit without con\u00additionals, \nprocedures, loops, or code specialization. In [12], abduction is used to infer resource invariants for \nsyn\u00adchronization, using a process of counterexample-driven re.nement. Our approach similarly infers resource \ninvariants, but using a very different technique: invariants are derived from a sequential proof, and \nwe also infer synchronization points and specialise the program to reveal synchronization opportunities. \nBehaviour-preserving parallelization. We expect our resource\u00adusage analysis can be used in other synchronization-related \nopti\u00admizations, but in this paper, we have used it as the basis for a par\u00adallelising transformation. \nThis transformation is in the style of de\u00adterministic parallelism [3 5, 9] although our approach does \nnot, in fact, require determinacy of the source program. In this vein, our transformation ensures that \nevery behaviour of the parallelized pro\u00adgram is a behaviour of the source sequential program (modulo \nthe caveats about allocation and termination discussed in \u00a76). Previous approaches to deterministic parallelism \noperate with\u00adout the bene.t of a high-level speci.cation. This places a substan\u00adtial burden on the analysis \nand runtime to safely extract informa\u00adtion on resource usage and transfer information that is readily \navailable in a proof. As a result, these analyses tend to be much more conservative in their treatment \nof mutable data. Our proof\u00adbased technique gives us a general approach to splitting mutable resources; \nfor example, by allowing the analysis to perform ad-hoc list splitting, as we do with sum_head. Our approach \ntransforms a sequential for-loop by running all the iterations in parallel and signalling between them. \nThis idea has been proposed previously, for example in numerical computa\u00adtion [33]. The novelty in our \napproach lies in inferring synchro\u00adnization over (portions of) complex mutable data-structures. Alter\u00adnatively, \nwe could have used a more irregular concurrency annota\u00adtion, for example safe futures [27], or an unordered \nparallel-for, as in the Galois system [30]. In the former case, our resource-usage analysis would be \nmostly unchanged, but our parallelized program would construct a set of syntactically-distinct threads, \nrather than a pipeline of identical threads. Removing ordering between itera\u00adtions, as in the latter \ncase, would mean replacing ordered grant\u00adwait pairs with conventional locks, and would introduce an obli\u00adgation \nto show that locks were always acquired together, as a set. Proof-driven parallelization. A insight central \nto our approach is that a separation logic proof expresses data dependencies for parts of a program, \nas well as for the whole program. These internal de\u00adpendencies can be used to inject safe parallelism. \nThis insight is due to [13, 31] and [24], both of which propose parallelization analy\u00adses based on separation \nlogic. The analyses proposed in these pa\u00adpers are much more conservative than ours, in that they discover \nindependence which already exists between commands of the pro\u00adgram. They do not insert synchronization \nconstructs, and conse\u00adquently cannot enforce sequential dependencies among concurrent computations that \nshare and modify state. Indeed, [31] does not consider any program transformations, since the goal of \nthat work is to identify memory separation of different commands, while [24] expresses optimizations \nas reordering rewrites on proof trees. Bell et. al [1] construct a proof of an already-transformed mul\u00adtithreaded \nprogram parallelized by the DSWP transformation [29]. This approach assumes a speci.c pattern of (linear) \ndependencies in the while-loop consistent with DSWP, a speci.c pattern of se\u00adquential proof, and a .xed \nnumber of threads. In our sum_head example, the outermost (parallelising) loop contains two succes\u00adsive \ninner loops, while the example in Fig. 2 illustrates how the technique can deal with interprocedural \nand control-.ow sensitive dependencies. In both cases, the resulting parallelization is special\u00adized \nto inject synchronization primitives to enforce sequential de\u00adpendencies. We believe examples like these \ndo not fall within the scope of either DSWP or the proof techniques supported by [1].  Outside separation \nlogic, Deshmukh et. al [14] propose an anal\u00adysis which augments a sequential library with synchronization. \nThis approach takes as input a sequential proof expressing the cor\u00adrectness criteria for the library, \nand generates synchronization en\u00adsuring this proof is preserved if methods run concurrently. A basic \nassumption is that the sequential proof represents all the properties that must be preserved. In contrast, \nwe also preserve sequential or\u00adder on access to resources. Consequently, Deshmukh et al. permit parallelizations \nthat we would prohibit, and can introduce new be\u00adhaviours into the parallelized program. Another difference \nis that [14] derives a linearizable implementation given a sequential spec\u00adi.cation in the form of input-output \nassertions; because they do not consider specialization of multiple instances of the library run\u00adning \nconcurrently, it is unclear how their approach would deal with transformations of the kind we use for \nsum_head. Separation logic and concurrency. Separation logic is essential to our approach. It allows \nus to localise the behaviour of a program fragment to precisely the resources it accesses. Our proofs \nare writ\u00adten using concurrent separation logic [28]. CSL has been extended to deal with dynamically-allocated \nlocks [19, 23, 25], re-entrant locks [20], and primitive channels [1, 22, 26, 34]. Sequential tools for \nseparation logic have achieved impressive scalability for ex\u00adample [11] has veri.ed a large proportion \nof the Linux kernel. Our work can be seen as an attempt to leverage the success of such sequential tools. \nOur experiments are built on coreStar [7, 16], a language-independent proof tool for separation logic. \nThe parallelization phase of our analysis makes use of the spec\u00adi.cations for parallelization barriers \nproposed in [17]. That pa\u00adper de.ned high-level speci.cations representing the abstract be\u00adhaviour of \nbarriers, and veri.ed those speci.cations against the barriers low-level implementations. However, it \nassumed that bar\u00adriers were already placed in the program, and made no attempt to infer barrier positions. \nIn contrast, we assume the high-level speci\u00ad.cation, and de.ne an analysis to insert barriers. The semantics \nof barriers used in that paper and here was initially proposed in [27]. Acknowledgments This work was \nsupported by the Gates trust, by EPSRC grant EP/H010815/1, and by NSF grant CCF-0811631. Thanks to Dino \nDistefano, Matthew Parkinson, Mohammad Raza, John Wickerson and the anonymous reviewers for comments \nand suggestions. References [1] C. J. Bell, A. Appel, and D. Walker. Concurrent Separation Logic for \nPipelined Parallelization. In SAS, pages 151 166, 2009. [2] J. Berdine, C. Calcagno, and P. W. O Hearn. \nSmallfoot: Modular automatic assertion checking with separation logic. In FMCO, pages 115 137, 2005. \n[3] T. Bergan, O. Anderson, J. Devietti, L. Ceze, and D. Grossman. Core-Det: A Compiler and Runtime System \nfor Deterministic Multithreaded Execution. SIGPLAN Not., 45(3):53 64, 2010. [4] E. D. Berger, T. Yang, \nT. Liu, and G. Novark. Grace: Safe multi\u00adthreaded programming for C/C++. In OOPSLA, pages 81 96, 2010. \n[5] R. L. Bocchino, Jr., V. S. Adve, D. Dig, S. V. Adve, S. Heumann, R. Komuravelli, J. Overbey, P. Simmons, \nH. Sung, and M. Vakilian. A Type and Effect System for Deterministic Parallel Java. In OOPSLA, pages \n91 116, 2009. [6] R. Bornat, C. Calcagno, P. O Hearn, and M. Parkinson. Permission Accounting in Separation \nLogic. In POPL, pages 259 270, 2005. [7] M. Botin.can, D. Distefano, M. Dodds, R. Griore, Naud.zi\u00afunien.e, \nand M. Parkinson. coreStar: The Core of jStar. In Boogie, pages 65 77, 2011. [8] M. Botin.can, M. Dodds, \nand S. Jagannathan. Resource-Sensitive Syn\u00adchronization Inference by Abduction. Technical Report 808, \nUniver\u00adsity of Cambridge Computer Laboratory, 2011. [9] J. Burnim and K. Sen. Asserting and Checking \nDeterminism for Multithreaded Programs. Commun. ACM, 53:97 105, June 2010. [10] C. Calcagno, P. W. O \nHearn, and H. Yang. Local Action and Abstract Separation Logic. In LICS, pages 366 378, 2007. [11] C. \nCalcagno, D. Distefano, P. O Hearn, and H. Yang. Compositional Shape Analysis by Means of Bi-Abduction. \nIn POPL, pages 289 300, 2009. [12] C. Calcagno, D. Distefano, and V. Vafeiadis. Bi-abductive Resource \nInvariant Synthesis. In APLAS, pages 259 274, 2009. [13] B. Cook, S. Magill, M. Raza, J. Simsa, and S. \nSingh. Making Fast Hardware with Separation Logic, 2010. Unpublished, http://cs. cmu.edu/~smagill/papers/fast-hardware.pdf. \n[14] J. V. Deshmukh, G. Ramalingam, V. P. Ranganath, and K. Vaswani. Logical Concurrency Control from \nSequential Proofs. In ESOP, pages 226 245, 2010. [15] D. Distefano and I. Filipovi\u00b4c. Memory Leaks Detection \nin Java by Bi-abductive Inference. In FASE, pages 278 292, 2010. [16] D. Distefano and M. J. Parkinson \nJ. jStar: Towards Practical Veri.ca\u00adtion for Java. In OOPSLA, pages 213 226, 2008. [17] M. Dodds, S. \nJagannathan, and M. J. Parkinson. Modular Reasoning for Deterministic Parallelism. In POPL, pages 259 \n270, 2011. [18] T. Elmas, S. Qadeer, and S. Tasiran. A Calculus of Atomic Actions. In POPL, pages 2 15, \n2009. [19] A. Gotsman, J. Berdine, B. Cook, N. Rinetzky, and M. Sagiv. Local Reasoning for Storable Locks \nand Threads. In APLAS, pages 19 37, 2007. [20] C. Haack, M. Huisman, and C. Hurlin. Reasoning about Java \ns Reen\u00adtrant Locks. In APLAS, pages 171 187, 2008. [21] T. Harris, J. Larus, and R. Rajwar. Transactional \nMemory, 2nd edition. Morgan-Claypool, 2010. [22] C. A. R. Hoare and P. W. O Hearn. Separation Logic Semantics \nfor Communicating Processes. ENTCS, 212:3 25, 2008. [23] A. Hobor, A. W. Appel, and F. Zappa Nardelli. \nOracle semantics for concurrent separation logic. In ESOP, 2008. [24] C. Hurlin. Automatic Parallelization \nand Optimization of Programs by Proof Rewriting. In SAS, pages 52 68, 2009. [25] B. Jacobs and F. Piessens. \nModular full functional speci.cation and veri.cation of lock-free data structures. Technical Report CW \n551, Katholieke Universiteit Leuven, Dept. of Computer Science, 2009. [26] K. R. M. Leino, P. M\u00a8uller, \nand J. Smans. Deadlock-free Channels and Locks. In ESOP, pages 407 426, 2010. [27] A. Navabi, X. Zhang, \nand S. Jagannathan. Quasi-static Scheduling for Safe Futures. In PPoPP, pages 23 32. ACM, 2008. [28] \nP. W. O Hearn. Resources, Concurrency and Local Reasoning. TCS, 375:271 307, 2007. [29] G. Ottoni, R. \nRangan, A. Stoler, and D. I. August. Automatic Thread Extraction with Decoupled Software Pipelining. \nIn MICRO, pages 105 118, 2005. [30] K. Pingali, D. Nguyen, M. Kulkarni, M. Burtscher, M. A. Hassaan, \nR. Kaleem, T.-H. Lee, A. Lenharth, R. Manevich, M. M\u00b4endez-Lojo, D. Prountzos, and X. Sui. The Tao of \nParallelism in Algorithms. In PLDI, pages 12 25, 2011. [31] M. Raza, C. Calcagno, and P. Gardner. Automatic \nParallelization with Separation Logic. In ESOP, pages 348 362, 2009. [32] J. C. Reynolds. Separation \nLogic: A Logic for Shared Mutable Data Structures. In LICS, pages 55 74, 2002. [33] P. Tang, P. Tang, \nJ. N. Zigman, and J. N. Zigman. Reducing Data Com\u00admunication Overhead for DOACROSS Loop Nests. In International \nConference on Supercomputing, pages 44 53, 1993. \u00b4 Heap-Hop. In TACAS, pages 275 279, 2010. [34] J. \nVillard, E. Lozes, and C. Calcagno. Tracking Heaps That Hop with   \n\t\t\t", "proc_id": "2103656", "abstract": "<p>We present an analysis which takes as its input a sequential program, augmented with annotations indicating potential parallelization opportunities, and a sequential proof, written in separation logic, and produces a correctly-synchronized parallelized program and proof of that program. Unlike previous work, ours is not an independence analysis; we insert synchronization constructs to preserve relevant dependencies found in the sequential program that may otherwise be violated by a naive translation. Separation logic allows us to parallelize fine-grained patterns of resource-usage, moving beyond straightforward points-to analysis. Our analysis works by using the sequential proof to discover dependencies between different parts of the program. It leverages these discovered dependencies to guide the insertion of synchronization primitives into the parallelized program, and to ensure that the resulting parallelized program satisfies the same specification as the original sequential program, and exhibits the same sequential behaviour. Our analysis is built using frame inference and abduction, two techniques supported by an increasing number of separation logic tools.</p>", "authors": [{"name": "Matko Botincan", "author_profile_id": "81444607580", "affiliation": "University of Cambridge, Cambridge, United Kingdom", "person_id": "P2991410", "email_address": "matko.botincan@cl.cam.ac.uk", "orcid_id": ""}, {"name": "Mike Dodds", "author_profile_id": "81418593776", "affiliation": "University of Cambridge, Cambridge, United Kingdom", "person_id": "P2991411", "email_address": "mike.dodds@cl.cam.ac.uk", "orcid_id": ""}, {"name": "Suresh Jagannathan", "author_profile_id": "81100208907", "affiliation": "Purdue University, West Lafayette, IN, USA", "person_id": "P2991412", "email_address": "suresh@cs.purdue.edu", "orcid_id": ""}], "doi_number": "10.1145/2103656.2103694", "year": "2012", "article_id": "2103694", "conference": "POPL", "title": "Resource-sensitive synchronization inference by abduction", "url": "http://dl.acm.org/citation.cfm?id=2103694"}