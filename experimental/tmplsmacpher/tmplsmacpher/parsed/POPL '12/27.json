{"article_publication_date": "01-25-2012", "fulltext": "\n Syntactic Control of Interference for Separation Logic Uday S. Reddy John C. Reynolds University of \nBirmingham Carnegie-Mellon University u.s.reddy@cs.bham.ac.uk jcr@cs.cmu.edu Abstract Separation Logic \nhas witnessed tremendous success in recent years in reasoning about programs that deal with heap storage. \nIts success owes to the fundamental principle that one should keep separate ar\u00adeas of the heap storage \nseparate in program reasoning. However, the way Separation Logic deals with program variables continues \nto be based on traditional Hoare Logic without taking any bene\u00ad.t of the separation principle. This has \nled to unwieldy proof rules suffering from lack of clarity as well as questions surrounding their soundness. \nIn this paper, we extend the separation idea to the treat\u00adment of variables in Separation Logic, especially \nConcurrent Sepa\u00adration Logic, using the system of Syntactic Control of Interference proposed by Reynolds \nin 1978. We extend the original system with permission algebras, making it more powerful and able to \ndeal with the issues of concurrent programs. The result is a streamined pre\u00adsentation of Concurrent Separation \nLogic, whose rules are memo\u00adrable and soundness obvious. We also include a discussion of how the new \nrules impact the semantics and devise static analysis tech\u00adniques to infer the required permissions automatically. \nCategories and Subject Descriptors D.3.1 [Programming Lan\u00adguages]: Formal De.nitions and Theory Syntax; \nF.1.2 [The\u00adory of Computation]: Computation by Abstract Devices Models of Computation Parallelism and \nconcurrency; F.3.1 [Logics and Meanings of Programs]: Specifying and Verifying and Rea\u00adsoning about Programs \nLogics of programs; F.3.2 [Logics and Meanings of Programs]: Semantics of Programming Languages Denotational \nsemantics General Terms Program Logic, Concurrency, Denotational Se\u00admantics, Type Systems Keywords Separation \nLogic, Syntactic Control of Interference, Conditional Critical Regions, Fractional Permissions, Static \nAnal\u00adysis 1. Introduction In reasoning about programs that alter the state, one often encoun\u00adters stylized \nside conditions that have to do with how variable sym\u00adbols are used. For example, the invariance rule \nof Hoare Logic [3] Permission to make digital or hard copies of all or part of this work for personal \nor classroom use is granted without fee provided that copies are not made or distributed for pro.t or \ncommercial advantage and that copies bear this notice and the full citation on the .rst page. To copy \notherwise, to republish, to post on servers or to redistribute to lists, requires prior speci.c permission \nand/or a fee. POPL 12 January 25 27, 2012, Philadelphia, PA, USA. Copyright c &#38;#169; 2012 ACM 978-1-4503-1083-3/12/01. \n. . $10.00 (or the constancy rule in Speci.cation Logic [37]), written as {P } C {Q} {P . R} C {Q . \nR} has a side condition that states that C should not modify any vari\u00adables occurring free in R. This \nrule becomes the all-important frame rule in Separation Logic [30] but the same side condition is retained. \nSimilar conditions also occur in the rules for proce\u00addures [3, 37]. In fact, such conditions are not \nonly employed in the proof rules for procedures, but it has also been argued that intel\u00adligible programming \nrequires adherence to them even if no efforts are made at formal reasoning. A procedure call P (A) is \nregarded as intelligible only if the procedure P does not modify any of the variables occurring free \nin the argument A and, likewise, the ar\u00adgument A does not cause state changes via the variables occurring \nfree in P . (This is more commonly called aliasing control. Con\u00adsider call-by-name or call-by-reference \nparameter passing methods or higher-order arguments to see the full effect of this observation.) These \nobservations are also made with additional force in con\u00adcurrent programming. Hoare [17, 18] and Brinch \nHansen [10] have argued convincingly that parallel processes should not interfere with each other. A \nprocess should not modify variables that are concurrently used by other processes unless the variables \nare un\u00adder the control of resources enforcing mutual exclusion. Program logic proof rules similarly employ \nthese conditions in their proof rules [17, 25, 31]. Noticing that essentially the same side condition \narises in all such contexts, Reynolds [36] formulated it as a uniform princi\u00adple of non-interference. \nTwo program phrases P1 and P2 are con\u00adsidered non-interfering if the variables used in one of them for \nstate-modi.cation do not occur free in the other phrase. This work presents a system of rules called \nsyntactic control of interference (SCI) which bring structure to the conditions employed in intelligi\u00adble \nprogramming as well as the formal rules of programming log\u00adics. These rules incorporate, at syntactic \nlevel, what we now regard as the separation principle, the same principle that is responsible for the \nsuccess of Separation Logic in reasoning about heap stor\u00adage. The SCI system has been studied quite extensively \nsince this early work. O Hearn [26] reformulated Reynolds s rules in the no\u00adtation of type systems (or \nproof theory) and noted its overriding similarity to Girard s Linear Logic [15]. Reddy [33] formulated \na novel semantic model for programs in the SCI framework, ex\u00adploiting the non-interference property of \nthe programs in a crucial way, which turns out to be the .rst fully abstract model ever dis\u00adcovered for \na higher-order imperative programming language [22]. (The games models of Abramsky et al. [1, 2] generalize \nReddy s model to deal with interference and represent fully abstract mod\u00adels as well.) Generalizing the \nSCI framework, O Hearn and Pym formulated bunched type systems and the logic of Bunched Impli\u00adcations \n[27], the latter of which forms the foundation for Separation Logic [30, 34]. In retrospect, it is fair \nto say that SCI has proved to be a deep foundational principle of imperative programs leading to numerous \ndevelopments in our understanding of their structure. Curiously, despite all the historical background, \nSCI has not been used in formulating Separation Logic itself. We believe that this has led to unwieldy \nproof rules fraught with side condi\u00adtions. The problems become critical in the formulation of Con\u00adcurrent \nSeparation Logic [25]. Brookes s attempt to formalize such rules [11] turned out to be faulty, with known \ncounterexamples to their soundness [4]. In this paper, we reformulate the rules of sequential as well \nas concurrent Separation Logic using the principles of SCI to bring structure to their side conditions. \nIt turns out that the traditional SCI is not quite adequate to the task. It incorporates a limited treat\u00adment \nof passive or read-only uses of variables which is unable to deal with the more advanced usage of variables \nin concurrent programs. We enrich the traditional SCI with the idea of fractional permissions, borrowed \nfrom Boyland [8] and Bornat et al [6], to devise a more powerful variant. This system is then used to \ncreate a streamlined formulation of Separation Logic. While fractional permissions for variables allow \na streamlined presentation of the proof rules and clarify their semantics, they would be an overkill \nto use in practice. A programmer should not have to explicitly annotate all the variable uses in processes \nand shared resources with fractional annotations. To address the issue, we devise a permission inference \nsystem, which can take a Separation Logic proof outline without any permissions speci.ed with shared \nresources and .lls them in if at all possible according to the rules of the logic. The effect is similar \nto that of Hindley-Milner type inference in programming languages like ML [14, 23]. Related Work Hoare \n[17] and Brinch Hansen [9] have formulated conventions for controlling variable aliasing in concurrent \nprograms. Hoare also proposed proof rules for reasoning with conditional critical regions. Owicki and \nGries [31] generalized Hoare s conventions as well as the proof rules. O Hearn [25] extended the Owicki-Gries \nsystem to deal with heap storage, formulating a Concurrent Separation Logic, which is currently a subject \nof active study [11, 13, 16, 20, 32, 38]. We refer to this logic as the Owicki-Gries-O Hearn system. \nThe main issue of our concern is how the variable usage is con\u00adtrolled across parallel processes and \nthe interplay between such control and the proof rules of the programming logic. The original logic formulated \nby Owicki and Gries employed informal state\u00adments of the form variable not modi.ed by any other process \n. Such a statement is ambiguous (e.g., does it include modi.cation inside critical regions?). It is also \nnon-compositional. Checking if a proof is correctly constructed involves examining the entire program. \n(For instance, the Smallfoot veri.cation tool implements such global analysis [19].) It is also problematic \nin de.ning seman\u00adtics of the programming logic and verifying its soundness. Two previous attempts have \nbeen made to formalise the variable usage rules of Concurrent Separation Logic. Brookes [11] formu\u00adlated \na compositional set of rules in his effort to prove the sound\u00adness of the logic. However, the rules generalize \nthe original Owicki-Gries rules in new ways and their soundness is not obvious. In fact, subsequently, \nIan Wehrman has found counterexamples to their soundness in one particular case [4]. The second attempt \nwas that of Bornat et al. [7, 32], where they treat variables as resources sim\u00adilar to heap locations, \nwhose access is controlled via programming logic proof rules. Their rules do address the non-compositionality \nissue mentioned above and the soundness of their rules is more im\u00admediate. However the rules are clumsy \nto use in practice because the normal pun of variable symbols as mutable variable and logical variables \nis not retained. For instance, a formula such as x = x is true if x is a logical variable, but not necessarily \nso when x is a program variable. Although we arrived at our formulation via a different route, using \nthe ideas of syntactic control of interference to formalize the original Owicki-Gries-O Hearn system, \nour system can also be seen as a syntactic variant of the Parkinson-Bornat-Calcagno logic described in \n[32]. The bene.ts of using the syntactic approach are: The normal conventions of variable usage in Hoare-style \nlogics are respected. So pitfalls in reasoning from improper treatment of variable symbols can be avoided. \n We are able to devise a compositional (or modular ) static analysis system to automatically infer permissions \nrequired for variable usage.  The system should be extensible to higher-order languages with procedures \nand objects. For instance, the methods of objects can be categorized as active or passive. (Even though \nwe do not discuss the higher-order aspects in this paper, the traditional principles of syntactic control \nof interference for higher-order procedures apply. These principles however do not generalize to storable \nprocedures as in ML. So, further work is needed to address such issues.)  In recent work, Brookes [12] \nhas independently devised a re\u00advised set of proof rules for Concurrent Separation Logic using ideas resembling \nours. His rules do not employ fractional permissions as ours do and the relationship to SCI is less clear \ncut. We do not know at present the precise relationship between his formulation and ours, but we anticipate \nthat the two are very close. Inference of fractional permissions has been studied by Yasuoka and Terauchi \n[40] and Bierhoff [5]. Both these pieces of work ad\u00address the permissions needed for heap cells (which \nis a harder prob\u00adlem than that for variables). However, they do not deal with concur\u00adrency, which is \nour main concern. Yasuoka et al. use a region-based analysis to make the heap permission problem tractable, \nwhich may be seen as a reduction to the corresponding problem for variable names. The techniques employed \nin their work rely on sophisti\u00adcated constraint-solving methods. In contrast, our permission in\u00adference \nalgorithm a two-pass algorithm on the syntax tree, similar to regular compiler analysis methods, with \nonly rudimentary con\u00adstraint solving issues. The rest of this paper is organized as follows. In Section \n2, we informally motivate the issues addressed by our formulation of Sep\u00adaration Logic. The logical system \nitself is described in Sections 3 and 4. We also include a detailed comparison with the proof sys\u00adtems \nof O Hearn [25] Parkinson et al. [32]. A comparison with Brookes s original system [11] is include in \nAppendix B. In Sec\u00adtion 5, we describe the denotational semantic framework of the proof system and indicate \nhow the soundness is proved. Finally, in Section 6, we describe an algorithm to automatically infer variable \npermissions needed in the proof system. 2. Motivation As mentioned in the Introduction, Hoare and Brinch \nHansen advo\u00adcated the avoidance of interference between concurrent processes as a good practice of intelligible \nprogramming. That requires that, in forming a parallel composition of commands C1 I C2, one must ensure \nthat C1 does not modify any variable that occurs free in C2 and vice versa. We use the terminology actively \nused forvari\u00adables that are used for state-modi.cation. Any variables that are used only for reading \nthe state are said to be passively used. We .rst consider how to treat active free variables of phrases \nusing Syntactic Control of Interference using the notations of [26, 29]. If a command C is formed using \na set of active free variables S, it is described using a judgement of the form S f C Comm. x:=0;a:=0;b:=0; \n{x = a + b*a =0 *b =0}resource r(x, a, b) {x = a + b} in begin {a =0} {b =0} with r do with r do {a =0 \n*x = a + b}{b =0 *x = a + b}x:=x+1; || x:=x+1; a:=1 b:=1 {a =1 *x = a + b}{b =1 *x = a + b}  od od {a \n=1} {b =1} end {x = a + b*a =1 *b =1}{x =2} Table 1. Example proof outline in Concurrent Separation Logic \nNow, the well-formedness of an intelligible parallel composition in the sense of Hoare and Brinch Hansen \ncan be described by the rule: S1 f C1 Comm S2 f C2 Comm S1, S2 f (C1 I C2) Comm Notice that the active \nfree variables of C1 and C2 are combined multiplicatively, requiring them to be separate or disjoint. \nThus, the non-interference conditions of Hoare and Brinch Hansen can be described in a pleasingly symmetric \nfashion without employing side conditions. The judgements used above describe the well-formedness of \ncommands. Rules of program logic can be stated in essentially the same way. The Separation Logic proof \nrule for parallel composition becomes: S1 f{P1} C1 {Q1} S2 f{P2} C2 {Q2} S1, S2 f{P1 *P2} C1 I C2 {Q1 \n*Q2} Each judgement in this rule asserts the well-formedness of a Hoare triple speci.cation as well as \nthe truth of the speci.cation. Once again, no side conditions are required to describe a sound infer\u00adence. \nWhile Reynolds [36] only considered independent parallel com\u00adposition, it is possible to add shared resources, \ne.g., Hoare-style resources and conditional critical regions, in the same way. A re\u00adsource declaration \ncommand To solve the problem, Owicki and Gries recommend adding auxiliary variables a and b and using \nthem to record control infor\u00admation about the increment actions performed in the two processes. The auxiliary \nvariables are also included in the shared resource. So, a and b can only be modi.ed inside critical regions.1 \nThe resource invariant x = a + b captures the control information recorded by a and b. However, notice \nthat a and b need to be mentioned in as\u00adsertions outside the critical regions. Owicki and Gries tailor \ntheir proof rules to allow such usage. Evidently, we are entering tricky territory here. The variable \nx cannot be used outside critical regions whereas the variables a and b are allowed to be used. The difference \nis that x is modi.ed in both the processes. Making assertions about it in one of the processes would \nnot be sound because the other pro\u00adcess can invalidate the assertions. On the other hand, the variable \na is only modi.ed in the left process. So, assertions mentioning a remain true independent of the progress \nof the other process. Thus, Owicki-Gries as well as O Hearn s proof systems use a critical re\u00adgion proof \nrule which allows the variables owned by a resource to appear in local assertions of a process, as long \nas they are not modi.ed in other processes. Note that the notion of a variable being modi.ed in other \npro\u00adcesses is quite subtle. One might expect that neither x nor a should be regarded as being modi.ed \nin the other process because the other process does not have direct access to them. Any modi.cation happens \nonly inside critical regions. So, the modi.cation actions cannot be attributed to the process. Rather \nthey should be charged to the resource, with the understanding that entering critical sec\u00adtions adds \nthe access rights of the resource to the process. The pu\u00adtative Syntactic Control of Interference framework \nwe alluded to above would treat the variables in that way. To handle these issues, we generalize the \nactive versus passive free variable distinction inherited from [29, 36] to total versus par\u00adtial ownership \nof the free variables [6, 8]. (It has become conven\u00adtional to call such ownership constraints permissions. \nWe con\u00adtinue to use that terminology even though we regard it as mislead\u00ading.) A total permission for \na variable allows writing to the variable (in other words, an active use) and a partial permission allows \nonly reading (a passive use). In the algebra of fractional permissions,a total permission is denoted \nby 1 and a partial permission by some non-zero fraction. The use of permissions gives us more powerful \ncontrol over variable usage because fractional permissions can be combined, possibly leading to a total \npermission, which then allows writing. Returning to our example in Table 1, we can de.ne the the re\u00ad \n11 source r to contain the permissions x 1 ,a . The entire pro\u00ad 2 ,b 2 resource r(S0) in C gram is \nspeci.ed in the context x 1 ,a 1,b1. The remaining permis\u00ad should split the available active variable \ncontext into two separate parts, S0 for the variables encapsulated in the resource and the re\u00adsions a \n 12 and b 12 are distributed to the two processes: a 12 to the left process and b 1 2 to the right process. \nThis allows the two processes mainder of the context for the body C. A critical region command: to use \na and b in their local assertions because such usage is passive. When the left process enters its critical \nregion, its local permissions with r when B do C od are combined with those owned by the resource, leading \nto the set should add the encapsulated context of the resource r to the current of permissions x 1 ,a \n1 ,b 1 2 . This allows the critical region to mod\u00ad context for the scope C. All this seems essentially \nstraightforward. However, it turns out to be inadequate in practice. To see the problem, consider the \nexample proof outline shown in Table 1, discussed by Owicki and Gries [31]. Even though we use separating \nconjunction * in assertions, * has the same force as the ordinary conjunction . here because the formulas \ninvolved are pure. The purpose of the proof outline is to argue that running x := x +1 in parallel with \nitself increments x by 2. The variable x is placed in a resource r, allowing it to be safely shared across \nthe parallel branches. Notice that placing it in the resource precludes it from being mentioned in the \nparallel branches outside any critical regions. So, it is not possible to write assertions that show \nthat each critical region increments x. ify x and a, while only reading is permitted for b. The right \nprocess is similar. This provides a compositional description of the variable usage in the example, eliding \nthe references to other processes. In the following sections, we formalize the system of Syntactic Control \nof Interference with permissions and use it to formulate the rules of sequential as well as concurrent \nSeparation Logic. 1 In Brookes s variant of the Owicki-Gries-O Hearn system [11], a and b need not be \nincluded among the owned variables of the resource. Thus, Brookes s logic is subtly more general than \nthe original Concurrent Separa\u00adtion Logic.  3. Sequential Separation Logic Our form of syntactic control \nis a modi.ed version of Reynolds SCI, using the ideas of permissions for read-only access [6, 8]. We \nassume a permission algebra (P, ., T), i.e., a partial com\u00admutative semigroup that is cancellative, has \na distinguished element T denoting full permission and satis.es the following axioms [32]: ' (non-zero) \n.p, p' .P.p . p= p (top) .p .P. T. p is unde.ned (divisibility) .p .P. .p1,p2 .P.p = p1 . p2 A signi.cant \ncase of permission algebras is that of fractional per\u00admissions: the real interval (0, 1] with . being \nthe partial operation of addition and T =1. The idea is that a full permission (1 in the fractional permission \nalgebra) allows an active usage, i.e., both reading and writing, whereas a partial permission (represented \nby fractional values in the fractional algebra) allows a read-only or passive usage. A variable context \nS is an unordered list of the form p1 pn x,...,x 1 n where x1,...,xn are variable symbols and p1,...,pn \nare permis\u00adsions, subject to the following condition:2 if the same variable symbol x occurs in S multiple \ntimes with permissions pi1 ,...,pik respectively then pi1 .\u00b7\u00b7\u00b7. pik is de.ned. We call a putative variable \ncontext well-de.ned when it satis.es this condition. If the variables x1,...,xn are pairwise distinct, \nthen we say that the variable context is in normal form. A non\u00adnormal form variable context can be normalized \nby replacing the multiple copies of each variable by a single copy and associating with it the permission \npi1 .\u00b7\u00b7\u00b7. pik as above. We denote the normalized version of variable context S by norm(S). Whenever two \nvariable contexts are combined, as in S1, S2 , one needs to ensure that the combination is well-de.ned. \nWe say that S1 and S2 are compatible, and denote this fact by S1 r S2. We assume that all the variable \ncontexts appearing in legal inferences are well-de.ned, i.e., any inference that leads to an ill\u00adde.ned \nvariable context is illegal. (Formally, our system of rules is a natural deduction system, where the \nvariable contexts are used as assumptions of the deductions. Even though we use the notation of sequents \nfor presenting the deduction rules, it is not a sequent calculus.) The syntactic well-formedness of program \nphrases is expressed using a variety of judgements: S f x Var S f E Exp S f P Assert S f C Comm These \nsay, respectively, that the displayed phrase is a well-formed variable, expression, assertion or command \nin the variable context S. All these forms of judgements have a structural rule: S,xp,xq fS Contraction \np.q S,xfS This allows two copies of a variable x to be combined into a single copy or to split a single \ncopy into two, while keeping account of the permissions. 2 It is more conventional to require that all \nthe variable symbols listed in a context are distinct. It would be possible to formulate variants of \nour rules using such a convention. But we feel that our approach is more intuitive. The following rules \nwill be admissible rules in our proof sys\u00adtems (if the premises are derivable then so is the conclusion): \nS fS Weakening ' S, SfS S f E Exp S,x T f P Assert SubstA S f P [E/x] Assert The substitution rules \nallow a variable with a full permission to be substituted by an expression. To use a variable symbol \nx as a variable phrase in a program (thereby allowing assignments to it), one needs the full permission \nfor the variable. On the other hand, to use a variable as an expres\u00adsion, any permission will do. T p \nS,xf x Var S,xf x Exp More generally, for all expressions and assertions, the requirement is that all \ntheir free variables must have some permission in S.We omit the formal rules for brevity. We can write \ndown well-formedness rules for commands as well, but we will save a bit of work by combining the well\u00adformedness \nof commands with program logic, which we look at next. (For completeness, we include the well-formedness \nrules in Appendix A.) A judgement of sequential Separation Logic is of the form S f{P } C {Q} (1) which \nmeans that: 1. P , C and Q are well-formed phrases in the context S,and 2. the failure-avoiding speci.cation \n{P } C {Q} holds assuming a variable context S.  The variables that are modi.ed in the command C would \nbe re\u00adquired to have T permission in S. Other variables, which might be employed in C in a read-only \nfashion or employed only in asser\u00adtions, can have non-T permissions. The rules for commands are shown \nin Table 2. Since we incor\u00adporate the well-formedness of assertions and commands in speci\u00ad.cations, most \nrules have premises to do with well-formedness of assertions, commands or components of commands. In \nthe rule for assignment, we depend on the admissible rule SubstA which al\u00adlows us to substitute for a \nvariable symbol with the T permission. The rule for heap cell lookup illustrates the use of side conditions \nfor specifying genuine logical conditions about the occurrence of free variables (as opposed to the conditions \nthat are purely to do with well-formedness issues). Contrast this with the rule for local variable declaration, \nwhere we require that E, P and Q should be well-formed in the outer variable context. So, they cannot \nhave x occurring free. This seems to be a reasonable choice, because most programmers understand the \nscope of x to be command C. So, its free occurrence in other places would be considered odd. The frame \nrule of Separation Logic gives us the .rst application of the syntactic control of interference: S f{P \n} C {Q} S' f R Assert FRAME S, S' f{P*R} C {Q*R} (Note that there is an implicit side condition for \nthe rule that says that S, S' is a well-formed variable context.) Since the variable contexts of {P } \nC {Q} and R are required to be separate, it is not possible for C to modify any free variables of R.If \nC modi.es a variable x then S needs to include x T.Butthen xp cannot occur in S', for any permission \np, because T. p is unde.ned. Thus the splitting of the variable context into S and S' has exactly the \n S f P ' ,Q ' Assert S f{P } C {Q} '' if P ' . P and Q . Q ' S f{P } C {Q } S f P Assert S f x Var S \nf E Exp S f P Assert S f{P } skip {P } S f{P [E/x]} x := E {P } S f x Var S f E Exp S f E ' Exp `\u00b4 S \nf E Exp S f E ' Expif x . FV (E, E ' ) '' ' S f{P [E /x] . E . E } x := [E] {P . E . E } S f{E {E . \nE ' .-} [E]:= E ' } S f{P . B} C1 {Q} S f{P .\u00acB} C2 {Q} S f P, Q Assert S,x T f{P } C {Q} S f{P } if \nB then C1 else C2 {Q} S f{P } local x in C {Q} Table 2. Proof rules of sequential Separation Logic S \nf x Var S f E Exp S f P Assert S | G f{P . B} C1 {Q} S | G f{P .\u00acB} C2 {Q} ASSIGN COND S | G f{P [E/x]} \nx := E {P } S | G f{P } if B then C1 else C2 {Q} S1 | G f{P1} C1 {Q1} S2 | G f{P2} C2 {Q2} PAR S1, S2 \n| G f{P1 *P2} C1 I C2 {Q1 *Q2} S f P, Q Assert S, S0 | G f{P*R . B} C {Q*R} CRIT S | G,r(S0): R f{P \n} with r when B do C od {Q} S0 f R Assert S | G,r(S0): R f{P } C {Q} RESOURCE (R precise) S, S0 | G f{P*R} \nresource r(S0) in C {Q*R} S f P, Q Assert S,XT | G f{P } C {Q}AUXILIARY if X is auxiliary for C S | \nG f{P } C \\ X {Q} Table 3. Proof rules of Concurrent Separation Logic same force as the usual side condition \nC does not modify any appropriate permission contexts S to be used with them, avoiding free variables \nof R in the conventional formulation of Separation the annotation burden for the programmer. Logic. The \nwell-formedness of commands is de.ned using judgements As an example, using the fractional permission \nalgebra, we can of the form 12 derive the inference using FRAME: S | G f C Comm Here, S is a variable \ncontext and G is a resource context of the f{y =0} x := y {x =0} 12 1212 1 f y = z Assert x ,y y ,z \nform r1(S1),...,rn(Sn) where ri are resource names, Si are variable contexts owned by the resources, \nsubject to the following 11 f{y =0 * y = z} x := y {x =0 * y = z} x ,y ,z conditions: The resource names \nri are distinct from each other. 4. Concurrent Separation Logic In this section, we formalize the rules \nof O Hearn s Concurrent Separation Logic, treating Hoare-style resources and conditional critical regions. \nThe context-free syntax of the commands is: C ::= x := E | x := [E] | [E]:= E ' | skip | C1; C2 | if \nB then C1 else C2 | C1 I C2 | with r when B do C od | resource r(S) in C Note that the resource declarations \ninclude permission contexts S for the variables associated with them. The notation enhances that of Owicki \nand Gries [31] and O Hearn [25], who list only variable names with resource declarations. In Section \n6, we present an inference algorithm that allows the resource declarations to be written simply in the \nform resource r in C and .nds the The variable context S, S1,..., Sn is well-de.ned. A putative syntactic \ncontext satisfying these conditions is said to be well-de.ned. Note that only commands require resource \ncon\u00adtexts (which get used in checking the well-formedness of critical regions). Variables, expressions, \nand assertions only need variable contexts. Just as in the sequential case, our rules of the programming \nlogic incorporate the well-formedness of commands. So, no special attention needs to be paid to their \nwell-formedness. The programming logic is formulated using judgements of the form S | G f{P } C {Q}Here, \nS is a variable context and G is an annotated resource context where each resource ri(Si) is annotated \nwith a resource invariant formula Ri which is a precise assertion [25] and satis.es Si f{57 .-}Ri Assert. \nThis means that a resource invariant for a resource can resource r1(p ) in only employ the variables \navailable in its variable context. All the rules of the sequential Separation Logic can be lifted resource \nr2(p 2 ) in begin  with r1 do (with r2 do p:=0 od); [57]:= 3 od to Concurrent Separation Logic by simply \nadding | G to all the speci.cation judgements. For example, see the rules for assignment and conditional \ncommands in Table 3. The resource contexts do not play any rule in the sequential fragment of the programming \nlanguage. The proof rule for parallel composition is the rule PAR.As one would expect, the variable context \nof the composite command, S1, S2, needs to be split into separate portions S1 and S2,for the two processes. \nThe resource context, on the other hand, is shared. The rule allows C1 and C2 to share read-only variables, \nvia separate copies with partial permissions. However, it is not possible for one process to modify a \nvariable employed in the other process or its proof. A resource s variables can be imported when a critical \nregion is entered (the CRIT rule). The body of the critical region, C, can use the combined variable \ncontexts of the process and the re\u00adsource, S and S0 respectively. However, the pre-condition and the \npost-condition can only employ the variables available in the pro\u00adcess s context. This captures the Owicki-Gries \nrequirement that they should only employ variables not modi.ed by other pro\u00adcesses . The rule for the \nresource declaration is RESOURCE.The vari\u00adable context S0 is sliced out of the current context, and transferred \nto the resource r. The resource invariant is based on these variables. The body of the resource declaration, \nC, can only use the remaining context S outside any critical regions. Finally, the rule AUXILIARY, which \nis similar to the rule for local variable declaration in its form, allows a set of variables X = {x1,...,xn} \nto be deleted from a command C along with all assignments to them, provided they are auxiliary , i.e., \neach free occurrence in C of a variable from X is in an assignment whose left hand side also belongs \nto X. The notation C \\ X denotes the command obtained by deleting all the assignments to variables in \nX. Note that all the variables in X are assigned the T permission in the second premise. This guarantees \nthat the variables do not occur in S or the permission contexts in G. 4.1 Comparison with Owicki-Gries-O \nHearn system O Hearn s version of Concurrent Separation Logic [25] is based on the Owicki-Gries system \n[31] as its underlying framework for variable usage. In this system, the free variables of the resource \ninvariant must be listed in the resource, similar to our RESOURCE rule. The rules governing the variables \nof a resources are as follows: 1. If a variable x belongs to a resource r, it cannot appear in a parallel \nprocess except in a critical region for r. 2. If a variable x is changed in process Si, it cannot appear \nin Sj (i = j) unless it belongs to a resource.  The rule 1 is relaxed in our proof rules. Recall that \nour resources en\u00adcapsulate not merely variables but variables with permissions. So, if x belongs to a \nresource with permission T then the restrictions on its usage in our rules are exactly the same as in \nthe Owicki-Gries system. However, if x belongs to the resource with a partial permis\u00adsion, then one or \nmore processes can possibly use x in a read-only fashion using the remaining partial permission. The \nrule 2 is represented exactly the same way in our proof rules. The rule 1 is somewhat misleading. While \nit requires that a variable x belonging to a resource cannot appear in the code of a parallel process \nexcept in a critical region, it nevertheless permits it to appear in the assertions of the process outside \ncritical regions. I with r2 do (with r1 do p:=1 od); [57]:= 4 od end {57 .-} Table 4. Problematic program \ndue to Berdine and Reynolds Thus, the proof outline of Table 1 is legal in the Owicki-Gries\u00adO Hearn \nsystem. However, there is a rider to this allowance in the Owicki-Gries proof rule for critical regions. \nA variable occurring free in the assertions surrounding a critical region should not be changed in another \nprocess . The allowance as well as its rider are already covered in our relaxation of the rule 1 above. \nWe treat the free occurrences of variables in assertions as well as read-only occurrences in code in \nexactly the same way. A variable that is not modi.ed in another process, is available to the current \nprocess with a partial permission. So, it can use it in a read-only fashion in both code and assertions. \nOur relaxation of the Owicki-Gries rule 1 leads to a simpler formulation. Thus all valid proof outlines \nof the Owicki-Gries-O Hearn sys\u00adtem remain valid proof outlines in our logic with syntactic control of \ninterference. It is quite straightforward to come up with an as\u00adsignment of permissions to the variables \nlisted in a resource. If a variable appears in multiple processes, either in code or as\u00adsertions, and \nmodi.ed in at least one of them, then the resource should contain the T permission for the variable. \n If a variable has read-only occurrences in one or more pro\u00adcesses, then then resource may contain any \npermission p for the variable and the complement of p should be distributed to all processes that use \nit outside critical sections.  If a variable is used in only one process (but possibly in as\u00adsertions \noutside critical regions), then the resource may contain any permission p for the variable and the complement \nof p is given to the process.  For the example in Table 1, the variable x appears in multiple processes. \nSo, it gets the permission 1 in the resource. The variable a (respectively, b) is used only in the left \nprocess (respectively, the right process). So, the resource is given 12 permission and the process is \ngiven the remaining 12 . However, our version of the Concurrent Separation Logic is more expressive. \nBy associating permission contexts with re\u00adsources, we make it possible for the permission to be combined \nin nested critical regions. For example, consider the program frag\u00adment shown in Table 4 due to Berdine \nand Reynolds [35]. The purpose of the two resources r1 and r2 is to achieve mutual exclu\u00adsion to a shared \ndata structure, in this case just the location 57.If the speci.cation has a proof in Concurrent Separation \nLogic, the race-freedom property of the logic guaruantees that only one pro\u00adcess can potentially access \nthe memory location 57 at any given time. A proof can be given in our version of the logic using the \nfollowing resource invariants: R1 =(p =0 . 57 .-) . (p =0 . emp) R2 =(p =0 . emp) . (p =0 . 57 .-)  \nNote that R1 *R2 is equivalent to 57 .-. So, both the pre\u00adcondition and the post-condition can be rewritten \nto R1 *R2. What makes the proof work is the idea that the permissions for the variable p are split across \nthe two resources. So, a process can modify it only by entering critical regions for both the resources. \nThis form of split-permissions for variables is not available in the Owicki-Gries-O Hearn system. Brookes \n[11], in his effort to prove the soundness of Concurrent Separation Logic, de.ned a variant of the original \nsystem which is subtly more general. Unfortunately, the generalization proved to be unsound. However, \nall the valid proofs that can be carried out in Brookes s system can be represented in our system. A \ndetailed comparison with Brookes s system, along with soundness issues, appears in Appendix B.  4.2 \nComparison with Variables as Resource systems Parkinson et al. [32] and Brookes [13] de.ne a general \nscheme of treating variables as resources with permissions. In contrast to our approach of syntactic \ncontrol, the variable resources are included in program assertions, through ownership formulas of the \nform ownp(x) and used with all the normal logical connectives. So, this approach can be termed logical \ncontrol of interference for variables. It is easy to see that the syntactic control system can be trans\u00adlated \nto the logical control system. For every variable context S= p1 pn (x1 ,...,xn ), there is an ownership \nformula OS = ownp1 (x1)* \u00b7\u00b7\u00b7 * ownpn (xn). A judgement S | G f{P } C {Q} of our system can be translated \nto a judgement G f{OS .P }C {OS .Q} in the Variables as resource system. In fact, Parkinson et al [32] \ngive translations of this form for Hoare logics. It is not possible to go in the reverse direction. The \nVariables as resource system uses logical formulas to express ownership of variables. So, it can express \na much richer set of ownership con\u00adstraints than possible in the syntactic control system. For example, \nthe formula (x =0 . ownT(y)) . (x =0 . ownT(z)) does not correspond to any syntactic variable context. \nThus, the Variables as Resource logic is more expressive than the syntactic control system. However, \nwe argue that the syntactic control system offers considerable simplicity and convenience. In particular, \n There are no issues of unde.nedness in expressions and formu\u00adlas. So, one does not need to write formulas \nof the form E = E just to ensure that E is de.ned in the current context.  Substitution is a valid operation \nin expressions and assertions.  The system has no logical anomalies, e.g., the equivalence \u00ac(E1 = E2) \n.. E1 = E2 holds in our system, whereas the two formulas have different interpretations in the Variables \nas Resource logic.  We need no special treatment of logical variables. The pun of program variables \nas logical variables, characteristic of Hoare logics, continues to work in our system.   5. Semantics \nand soundness The standard proof of soundness for sequential Separation Logic is due to Yang and O Hearn \n[39]. Bornat et al. [6] have extended it to deal with permissions. The soundness proof of the original \nConcurrent Separation Logic was provided by Brookes [11] us\u00ading novel denotational methods. Brookes [13] \nhas also used these methods to prove the soundness of the variables as resource sys\u00adtem. Since then, \nother proofs of soundness have appeared. See [38] for an overview. We regard Brookes s semantics as the \ncanonical one since it is denotationally based and allows easy extensions and adaptations. In this section, \nwe discuss how the presentation of Separation Logic using the SCI principles impacts the semantics. We \nregard the SCI judgements for phrases and speci.cations as a form of type system, and use the approach \nof Church typing to de.ne the semantics, i.e., we regard well-formedness judgement S f C Comm and S | \nG f C Comm as a form of typing for C and interpret C using denotations that are appropriate for the speci.ed \ncontext S | G. It is also possible to conceive of a Curry typing semantics where the commands are interpreted \nwithout regard to their contexts of well-formedness, and the well-formed judgements are given a logical \nmeaning as properties of the untyped denotations. However, we follow the Church typing approach here \nbecause it seems more natural. 5.1 Sequential Separation Logic A state is modelled as a pair (s, h) \nof a store and a heap, which are .nite partial functions from, respectively, variables and addresses. \nTo keep track of permissions, we de.ne them to map their arguments to pairs of values and permissions: \nStore = Vars -Val \u00d7P Heap = Addr -Val \u00d7P  We refer to such maps as permissive store and permissive heap \nrespectively, and both kinds of maps generically as permissive maps. Two permissive maps f1 and f2 are \nsaid to be compatible, denoted f1 rf2, if, for all arguments common to both of their domains, they agree \non values and provide compatible permissions. More formally, f1 rf2 iff: '' '' f1(x)=(v, p) . f2(x)=(v \n,p )=. v = v . p . p is de.ned If f1 and f2 are compatible, their joining operation is denoted f1 \u00b7 f2 \n(which combines permissions whenever both f1 and f2 are de.ned). It is extended to states by de.ning \n(s1,h1) \u00b7 (s2,h2)= (s1 \u00b7 s2,h1 \u00b7 h2). p1 pn Given a variable context S with norm(S) = (x1 ,...,xn ), \nastore s is said to be of type S if dom s = {x1,...,xn} and the permission component of s(xi) is pi for \nevery i. It is easy to see that, whenever S1 r S2,anystores s1 of type S1 and s2 of type S2 are compatible, \nand s1 \u00b7 s2 is of type S1, S2. A state s =(s, h) is said to be of type S just if s is of type S. The \nheap component of the state is unconstrained. If (s1,h1) and (s2,h2) are states of of type S1 and S2 \nrespectively, S1 r S2 and h1 rh2,then (s1,h1) \u00b7 (s2,h2) is of type S1, S2. The meaning of a command in \nthe sequential programming language is de.ned in [39] as a local state transformer, i.e., a binary relation \n[[C]] . State \u00d7 State l{fault} satisfying safety monotonicity, termination monotonicity and the frame \nproperty. It was extended to permissive states in [6]. While it is not stated there, it is also easy \nto see that [[C]] always preserves the domain and permission structure of the store. This allows us to \nde.ne a typed semantics for commands. If S f C Comm is a well-formedness judgement then its meaning is \na relation [[C]]S consisting of just the pairs (s, s ' ) where both s and s ' are of type S. DEFINITION \n1. A judgement of the sequential Separation Logic S f{P } C {Q} is valid iff, for all states s of type \nS satisfying P : (s, fault) . [[C]]S, and  if (s, s ' ) . [[C]]S then s ' is of type S and satis.es \nQ.  THEOREM 2 (Soundness). Every derivable judgement of sequen\u00adtial Separation Logic is valid. The proof \nis by induction on the derivation of the judgement. Consider the FRAME rule as a signi.cant example. \nLet s be a state of type S, S ' satisfying P*R.Then s can be written as s1 \u00b7 s0 where s1 is of type S \nand satis.es P and s0 is of type S ' and satis.es R. Then by inductive hypothesis, S f{P }C {Q} is valid. \nHence [[C]]S is safe for s1 and, whenever (s1,s1' ) . [[C]]S, s1 ' is of type S and satis.es Q. So, by \nthe safety monotonicity and frame properties, [[C]]S,S' is safe for s,and (s, s ' ) . [[C]]S,S' implies \ns ' is of type S, S ' and s ' satis.es Q*R.  5.2 Concurrent Separation Logic The denotational semantics \nof commands in the concurrent pro\u00adgramming language is given in two stages. First, commands are in\u00adterpreted \nas traces, i.e., stylized sequences of actions. Second, these traces are described by their effect on \nstates as state transitions. It is not possible to interpret the commands directly as state transitions, \nbecause such transitions only relate initial and .nal states whereas parallel composition makes intermediate \nstates visible.  Trace semantics A pre-action (or an untyped action) is a syntactic token given by the \nsyntax: . ::= d | x = v | x := v | [l]= v | [l]:= v | try(r) | acq(r) | rel(r) | abort As in [11], d \nis a do-nothing or idle action, x = v denotes the action of reading the variable x, x := v denotes the \naction of writing to the variable x. The actions [l]= v and [l]:= v denote similar actions for heap locations. \nThe tokens try(r), acq(r) and rel(r) denote the actions of attempting to acquire a resource, acquiring \na resource and releasing a resource respectively. The token abort denotes the action of aborting a computation \nin case of an error. A pretrace is a possibly in.nite sequence of actions subject to the identi.cations \na \u00b7 d \u00b7 \u00df = a \u00b7 \u00df,and a \u00b7 abort \u00b7 \u00df = a \u00b7 abort. We model the actions and action traces appropriate for \na syntactic context S | G as a form of typing. First of all, the contexts enable certain actions and \nprohibit others. A variable action x = v or x := v would only be possible in a context that contains \nx with requisite permissions. The resource actions try(r) and acq(r) would only be possible in a context \nthat contains a resource named r. Secondly, as a result of an action, the context available for the rest \nof a trace might change. For instance, acq(r) has the effect of removing the resource r(S0) from the \nresource context and adding its variables S0 to the variable context. A rel(r) action has the opposite \neffect. We represent these effects by a transition relation . -. on contexts. Finally, when a resource \nis acquired by a process, it is not available for another acquisition until it is released. At the same \ntime, the type information of the resource should continue to be retained in the context. Therefore, \nwe work with a form of extended contexts where the resources acquired by a trace are marked busy, by \nenclosing them in square brackets as [r(S)]. An extended context is a context of the form '' '' S | r1(S1),...,rn(Sn), \n[r1(S1)],..., [rm(Sm)] such that the resource names r1,...,rn,r1' ,...,r ' are all distinct, and m the \nvariable context S, S1,..., Sn is well-de.ned. A putative extended context satisfying these conditions \nis said e to be well-de.ned. We use the letter G to range over extended resource contexts where some \nof the resources are marked busy. The notation (G)e. denotes the underlying resource context of Gewhere \nall the busy markers are erased. An action is a triple (S|Ge, S ' |e,.) consisting of the initial G ' \nand .nal contexts and a pre-action that leads from the former to . the latter. We write it using the \nnotation S|Ge-. S ' |Ge' .The list of actions used in the semantics of the programming language are shown \nin Table 6. There are no constraints on the actions for reading and writing heap locations because the \naccess to heap d S | Ge-. S | Ge x=v S | Ge-. S | Gewhere xp . S for some p x:=v S | Ge-. S | Gewhere \nx T . norm(S) [l]=v S | Ge-. S | Ge [l]:=v S | Ge-. S | Ge try(r) S | Ge,r(S0) -. S | Ge,r(S0) acq(r) \nS | Ge,r(S0) -. S, S0 | Ge, [r(S0)] rel(r) S, S0 | Ge, [r(S0)] -. S | Ge,r(S0) Table 6. Actions used \nin traces locations is controlled in the programming logic rather than the syntax. A trace is a .nite \nor in.nite sequence of the form .1.2.3 S0|Ge0 -. S1|Ge1 -. S2|Ge2 -. \u00b7\u00b7\u00b7 If the sequence a = .1.2 \u00b7\u00b7\u00b7 \nis .nite, we use the notation a S0|Ge0 -. Sn|Gen to denote the corresponding trace. If it is in.\u00ad a \n nite, we use the notation S0|Ge0 -. 8. We also use the notation a S0|Ge0 -. \u00b7 for both .nite and in.nite \ntraces, and say that the pretrace a is enabled in the context S0|Ge0. For de.ning the meaning of parallel \ncomposition, we de.ne an operation of interleaving two traces. Suppose a1 and a2 are two traces with \na1 enabled in a context S1|Ge1 and a2 enabled in a con\u00adtext S2|Ge2.Then Ge1 and Ge2 should have the same \nunderlying re\u00adsource contexts, i.e., (Ge1). =(Ge2)., and they should mark disjoint sets of resources \nas busy. Then the resource context obtained by marking the busy resources of both Ge1 and Ge2 is denoted \nGe1 . Ge2. Interleaving is only possible for traces a1 and a2 such that Ge1 and e G2 are in this form \nand S1, S2 | Ge1 . Ge2 is well-de.ned. Two actions .1 and .2 are said to interfere, written .1 . .2, \nif .1 writes to a heap location l and .2 readsorwritesthe same a1 location l,or vice versa.The setof \nmutex fairmerges of S1|Ge1 -. a2 \u00b7 and S2|Ge2 -. \u00b7 is a set of traces of type S1, S2|Ge1 . Ge2 -. \u00b7 \ngiven by induction on the lengths of a1 and a2: a1 I E = {a1} E I a2 = {a2} (.1a1) I(.2a2)= { abort | \n.1 . .2}. .1G ' \u00df { S1, S2 | Ge1 . Ge2 -. S1' , S2 | e1 . Ge2 -. \u00b7 such that \u00df . a1 I(.2a2) }. .2\u00df { \nS1, S2 | Ge1 . e-. S1, S2 ' | Ge1 . Ge' -. \u00b7 G22 such that \u00df . (.1a1) I a2 } This de.nition is a typed \nversion of the notion of mutex fairmerges in Brookes [11]. Note that the typing information of traces \nobviates the need to consider possible interference via variable usage. The de.nition is extended to \nsets of traces in the natural way. If T1 and T2 are trace sets enabled in contexts S1|Ge1 and S2|Ge2 \nthen the trace set T1 I T2, obtained as the union of all a1 I a2 for all a1 . T1 and a2 . T2 is enabled \nin the context S1, S2 | Ge1 . Ge2. A (well-bracketed) trace for an extended context S | Geis either a \n abort, a .nite trace a such that S | Ge-. S | Ge, or an in.nite trace whose every .nite pre.x can be \nextended to a well-bracketed .nite trace. The terminology is motivated by thinking of the acq(r) and \nrel(r) actions as brackets. A trace set T is a (well-bracketed) [[x]]S = { (x = v, v) | v Value } [[E1 \n+ E2]]S = { (.1.2,v1 + v2) | (.1,v1) . [[E1]]S . (.2,v2) . [[E2]]S } [[skip]]S|G = {d} [[x := E]]S|G \n= { .(x := v) | (., v) . [[E]]S } [[x := [E]]]S|G = { .([v]= v ' )(x := v ' ) | (., v) . [[E]]S } [[[E]:= \nE ' ]]S|G = { .. ' ([v]:= v ' ) | (., v) . [[E]]S . (. ' ,v ' ) . [[E ' ]]S } [[if B then C1 else C2]]S|G \n=([ B]]S I true)[[C1]]S|G . ([[B]]S I false)[[C2]]S|G [[local x in C]]S|G = { (. \\ x) | . . [[C]]S,xT|G \n} [[C1 I C2]]S1,S2|G =[[C1]]S1|G I[[C2]]S2|G [[with r when B do C od]]S|G,r(S0) = wait * enter . wait. \n where wait = acq(r)([ B]]S,S0 I false) rel(r) .{try(r)}enter = acq(r)([ B]]S,S0 I true)[[C]]S,S0|G rel(r) \n[[resource r(S0) in C]]S,S0|G = { . \\ r | . . [[C]]S|G,r(S0) } Table 5. Trace semantics of phrases trace \nset for context S | Geif each trace in T is a well-bracketed trace for the context. LEMMA 3 (Weakening \nof contexts). If a is a trace for an extended Ge' context S|Ge, and S, S ' |Ge, is a longer well-de.ned \nextended Ge' context, then a is a trace for S, S ' |Ge, . LEMMA 4 (Parallel composition preserves contexts). \nIf a1 and a2 are traces for extended contexts S1|Geand S2|Gerespectively, and S1, S2|Geis a well-de.ned \nextended context then a1 I a2 is a trace set for the context S1, S2|Ge. All expressions and commands \ncan be given a compositional semantics in terms of trace sets. The meaning of an expression S f E Exp \nis a set of pairs . (., v) where . is an action trace of type S| -. S| (i.e., a context with no resources, \nbecause expressions do not access resources), and v is a value (obtained as the result of evaluating \nE). We denote it by [[E]]S. The meaning of a command S | G f C Comm is a set of traces . for the context \nS|G.Wedenoteitby [[C]]S|G. The semantics is de.ned in the standard fashion [11]. However, it is de.ned \nby induction on the derivations of well-formedness judgements S f E Exp and S | G f C Comm, instead of \ninduction on the structure of terms. We show the meanings of sample phrases in Table 5. The notation \n[[E]]S I v denotes the set of traces { . | (., v) . [[E]]S }. The notations .\\x and .\\r remove the actions \nmentioning x and r respectively from .. THEOREM 5 (Type soundness of trace semantics). The meaning of \ncommand S | G f C Comm is a (well-bracketed) trace set for the context S | G. Likewise, for every (., \nv) in the meaning of an expression S f E Exp, . is a (well-bracketed) trace for the context S | .  Local \nstate semantics A state for a concurrent program is a triple (s, h, A) where s is a permissive store, \nh is a permissive heap and A is a set of resource names (deemed to have been acquired by the process). \nWe also use an error state abort. The types for states will be annotated extended contexts of the form \nS | Gewhere the resources are annotated with resource invariants as in r(S0): R.Itisa characteristic \nof Brookes s semantics for Concurrent Separation Logic that the resource invariants play a central role \nin the state transition semantics. A state of type S | Geis either abort or a normal state (s, h, A) \nwhere s is a store of type S, h is a heap, and A is a subset of the resources marked busy in Ge. We can \ninterpret actions (and action traces) of type S | Ge-. S ' | Ge' as state transformations that transform \nstates of type S | Ge eee to states of type S ' | G ' . For actions of type S | G -. S | G (where the \nstate type is unchanged), the state transformations are as follows: d (s, h, A) -. (s, h, A) abort \n(s, h, A) -. abort x=v (s, h, A) -. (s, h, A) iff .p. s(x)=(v, p) x:=v (s, h, A) -. (s[x . (v, T)],h,A) \niff .v0.s(x)=(v0, T) [l]=v (s, h, A) -. (s, h, A) iff .p. h(l)=(v, p) [l]=v (s, h, A) -. abort iff \nl . dom(h) [l]:=v (s, h, A) -. (s, h[l . (v, T)],A) iff .v0.h(l)=(v0, T) [l]:=v (s, h, A) -. abort \niff l . dom(h) try(r) (s, h, A) -. (s, h, A) For an acq action of type acq(r) S | Ge,r(S0): R -. S, \nS0 | Ge, [r(S0): R] the transformations are given by: acq(r) (s, h, A) -. (s \u00b7 s0,h \u00b7 h0,A .{r}) iff \n(s0,h0) |= R, srs0,hrh0 For a rel action of type rel(r) S, S0 | Ge, [r(S0): R] -. S | Ge,r(S0): R the \ntransformations are: rel(r) (s \u00b7 s0,h \u00b7 h0,A l{r}) -. (s, h, A) iff (s0,h0) |= R rel(r) (s, h, A) -. \nabort iff .h0 . h. \u00ac(s, h0) |= R The key property of these transformations, inherited from Brookes [11], \nis that the transitions for acq(r) extend the current state with an ar\u00adbitrary state of the resource \nsatisfying the resource invariant R. The condition srs0 ensures that the values of any common vari\u00adables \nagree. The transitions for rel(r) do the opposite: they remove the state of the resource from the current \nstate. If and when the resource is reacquired in a future action, the state of the resource obtained \nmay bear no relationship to the state previously released. In fact, since other processes can intervene \nin the interim, nothing more can be assumed about the reacquired state of the resource. LEMMA 6 (Type \nsoundness of traces). Given a trace a of type S | a a Ge-. S ' | Ge' and a state (s, h, A) of type S \n| Ge,if (s, h, A) -. '' G ' (s ,h ' ,A ' ) then (s ,h ' ,A ' ) is of type S ' | e.  Soundness DEFINITION \n7 (Validity). A judgement S | G f{P } C {Q} is valid iff, for all well-bracketed traces a for the context \nS|G in [[C]]S|G, all local states (s, h, \u00d8) and s ' of type S|G, a (s, h) |= P . (s, h, \u00d8) -. s ' =. \n.s ' ,h ' .s ' =(s ' ,h ' , \u00d8) . s ' |= Q THEOREM 8 (Soundness). Every provable judgement of concur\u00adrent \nSCI Separation Logic is valid. Standard semantics In addition to the semantics de.ned above, which is \nwith respect to a program proof, traces can be interpreted as actions on global states. The relation \nis denoted (s, h, A)=a. (s ' ,h ' ,A ' ) and is similar to an untyped version of the local state transition \nsemantics, except that the rules for acq and rel actions are modi.ed as follows: acq(r) (s, h, A)=. (s, \nh, A .{r}) if r . A rel(r) (s, h, A l{r})=. (s, h, A \\{r}) if r . A This relation corresponds to running \na process on the global state without any interference from any other processes. The following result \nsays that the standard semantics obtained by executing traces on the global state corresponds to the \nlocal state semantics de.ned above. The notation inv(G) stands for the conjunction of all the resource \ninvariants in G. THEOREM 9. Let (s, h, \u00d8) be a global state and S | G a context. Suppose the state (s, \nh) can be split as (s1,h1) \u00b7 (s2,h2) where (s1,h1, \u00d8) is of type S | G and (s2,h2) |= inv(G). a a If \n(s, h, \u00d8)=. abort then (s1,h1, \u00d8) -. abort. ' If (s, h, \u00d8)=.. (s ,h ' , \u00d8) then either . (s1,h1, \u00d8) -. \nabort,or ' ''' (s ,h ' ) can be split as (s1,h1' )\u00b7(s2,h2' ) such that (s1,h1' , \u00d8) . is of some type \nS ' | G ' , (s1,h1, \u00d8) -. (s1' ,h ' 1, \u00d8) and (s ' 2,h2' ) |= inv(G ' ).  6. Permission inference In \nthis section, we investigate the problem of permission inference. We construct an algorithm which, given \na program and a proof outline with no variable contexts listed with resources, .lls them in if at all \npossible in accordance with the rules of SCI Separation Logic. We restrict our attention to the permission \nalgebra of fractional permissions, the real interval (0, 1] with addition as the partial binary operation. \nFor theoretical simplicity, we extend the algebra to include 0 as an abnormal permission value, indicating \nthat the resource or the process possesses no permission for the variable, and extend addition to 0 in \nthe standard way. An element of [0, 1] is referred to as an extended permission. A normal form context \nwith n variables and m resources is of the form p01 p0n x,...,x| 1 n p11 p1n pm1 pmn r1(x1 ,...,xn ): \nR1, ..., rm(x1 ,...,xn ): Rm f where each pij is an extended permission, with the index i cor\u00adresponding \nto the owner of the permission (0 for the process or self, 1,...,n for the shared resources), and the \nindex j corre\u00adsponding to the variable. We represent all the data in the context by two .nite functions: \n .:Vars . Owners . [0, 1] .:Resources . Invariant where Owners = {self}l Resources,and . satis.es So.Owners \n(. vo) = 1 The sets Vars and Resources include all the variable and resource names appearing in the \nprogram fragment being analyzed. Using these notations, the proof system of SCI Separation Logic can \nbe rewritten using judgements of the form: .|. f E Exp .|. f P Assert (Passive) .|. f x Var .|. f{P \n} C {Q} (Active) (where the .rst three forms have the resource context . added for uniformity in discussion). \nFor example, the parallel composition rule is rewritten as: .1 | . f{P1} C1 {Q1} .2 | . f{P2} C2 {Q2} \n. | . f{P1 *P2} C1 I C2 {Q1 *Q2} if .1 vo =.2 vo =. vo for all o = self . v self =.1 v self +.2 v self \n= 1 We also use abbreviated rules for the passive judgements: if .v . FV (E), . v self > 0 . | . f E \nExp if .v . FV (P ), . v self > 0 . | . f P Assert De.ne a write-proof as a proof where the side conditions \nof pas\u00adsive judgements are ignored. Since the passive judgements involve variable reading, this means \nthat the permissions needed for vari\u00adable reading are not checked. However, the permissions needed for \nvariable writing are still checked, hence the name. De.ne a pre-judgement as a judgement with variable \ncontexts . erased, i.e., a judgement of one of the forms: . f E Exp . f P Assert (Passive) . f x Var \n. f{P } C {Q} (Active) A pre-rule is an SCI Separation Logic rule with variable con\u00adtexts erased. A \npre-inference is an instance of a pre-rule and a pre-proof is a derivation made up of pre-inferences. \nThe era\u00adsure of a judgement, rule, inference or proof X is a pre-judgement, pre-rule, pre-inference or \npre-proof (respectively) denoted X0, ob\u00adtained by erasing all the variable contexts. In that case, we \nsay that X erases to X0 or X extends X0 . The problem of permission inference is now stated formally \nas follows: Given a pre-proof P 0, is there a proof P whose erasure is P 0? The algorithm described \nbelow answers the question. Moreover, if the answer is yes, it produces a maximally permissive proof \nP max that extends P 0 . We regard proof trees as formal trees, i.e., graphs satisfying the tree conditions, \nlabelled by judgements. P 0 and P are different labellings of the same formal tree. We use the notation \n(P 0)N and (P )N , respectively, to refer to the judgements labeling a node N of the formal tree. We \nuse a few auxiliary concepts: A permission restriction F is an assignment [v1 : O1, ..., vk : Ok] where \nvi . Vars and Oi . Owners. We also feel free to treat F as a partial function of type Vars -P(Owners).Sucha \n {57 .-} resource r1 in resource r2 in begin with r1 do (with r2 do p:=0 od);[57] := 3 od I with r2 \ndo (with r1 do p:=1 od);[57] := 4 od end {57 .-} Table 7. Problematic program due to Berdine and Reynolds \nF represents the condition that, for each of the variables vi,the owners in Oi share the full permission \nfor vi.Avariable vi will occur in a permission restriction exactly when the program phrase being described \ncontains an assignment to vi. The corre\u00adsponding Oi lists all the owners that can contribute permissions \nrequired for that assignment to be legal. Formally, the satisfac\u00adtion of a permission restriction by \na variable context is de.ned as: . |=F .. .(vi : Oi) . F. So.Oi (. vi o)=1 Note that . vi o must be 0 \nfor all owners outside Oi.Thereare no constraints on . for the other variables not mentioned in F. We \nde.ne a permission ordering on variable contexts . . . ' by the rule: . vo> 0=. . ' vo> 0 We say that \n. ' is more permissive than .. The intuition is that . ' has non-zero permissions for at least as many \ncombina\u00adtions as .. A permission restriction F=[v1 : O1,...,vk : Ok] is satis.able only if every Oi is \nnonempty. In that case, a maximally permissive variable context satisfying F can be de.ned as follows: \n8 <1/#O, if (v : O) . F . o . O .max vo =0, if (v : O) . F . o . O : 1/(#Owners + 1), if v . dom F where \n#S denotes the size of the set S. In other words, a full permission is apportioned among all the owners \npermitted by F or, if F imposes no restriction, then a partial permission is apportioned among all owners. \nOur algorithm for permission inference is a two-phase algo\u00adrithm. The .rst phase traverses a pre-proof \nleaves to root ( bottom\u00adup in the syntax tree), and computes, at each inference step, the permission \nrestriction that must be satis.ed by any write-proof. If any permission restriction computed in this \nphase is unsatis.able then there is no proof corresponding to the pre-proof. The second phase traverses \nthe pre-proof from the root to leaves ( top-down in the syntax tree), computing variable contexts that \nextend the pre\u00adproof to a maximally permissive write-proof in the sense of the pre\u00adorder .. The maximally \npermissive write-proof is then checked to verify that it contains non-zero permissions for all the passive \nuses of variables. We illustrate the algorithm using the problematic program of Berdine and Reynolds \n[35], reproduced in Table 7 for ease of reference: Let . stand for the resource context r1 : R1,r2 : \nR2. The .rst phase of the algorithm traverses the pre-proof leaves to root and computes, at each inference \nstep, the permission restric\u00adtions needed to extend the pre-proof to a write proof. Since the in\u00adference \nsteps correspond to program terms, we just show the terms involved in each case. 1. For the variable \np, i.e., the inference step concluding . f p Var, the permission restriction is F1 =[p : {self}].The \ntotal permission must be owned by self at this point in order to allow assignments to p. 2. For the \ncommand p := 0, the permission restriction is the same, F2 =[p : {self}]. 3. For the critical section \nwith r2 do p := 0 od, the permission restriction is F3 =[p : {self,r2}]. This means that both the process \nand the resource r2 could have non-zero permission for p. Since the critical section combines the permissions \nfrom self and r2 to execute the body, this is well-justi.ed.  4. The command [57] := 3 does not write \nto any variables. So, its permission restriction is empty: F4 =[]. 5. For the outer critical section \n P1 = with r1 do (with r2 do p := 0 od);[57] := 3 od r1 is added to the restriction: F5 =[p : {self,r1,r2}]. \n 6. The second process P2 similarly has the permission restriction F6 =[p : {self,r1,r2}]. 7. For the \nparallel composition P1 I P2, the permission restriction is F7 =[p : {r1,r2}], i.e., self is removed \nfrom the permis\u00adsion restrictions obtained from the component processes. Why? In this phase of the algorithm, \nwe are only considering what permissions are needed for writing variables. Since both the processes have \npermission restrictions for p, that means that they are both writing to p, which is only possible if \neach of them has 0 as the self permission for p. (If the .rst process has non-zero permission for p then, \nsince the second process has the sum of all its permissions for p summing to 1, the total sum of the \npermissions for p in the parallel composition would exceed 1, which is forbidden.) All the permissions \nfor writing to p in both the processes must be obtained by entering critical regions for the resources. \n 8. resource r2 in P1 I P2 has the permission context F8 =[p : {r1, self}], which is obtained by replacing \nr2 in F7 by self. This is justi.ed by noting that the resource declaration allows the process to shift \nsome portion of the permission for p from self to r2.Since F7 potentially requires a non-zero permission \nfor p in r2, F8 must require it in self. 9. resource r1 in resource r2 in P1 I P2 has the permission \nrestriction F9 =[p : {self}], using the same reasoning as in the previous step.  The key observation \nis the fact that permission restriction F7 for P1 I P2 does not contain self. This requires us to divide \nthe full permission for p among only the two resources r1 and r2. Since all the permission restrictions \ncomputed in phase 1 are satis.able, we proceed to phase 2 of the algorithm. This phase moves top-down, \nfrom the root to the leaves, using the permission restrictions computed in the previous phase. 1. For \nthe overall program, the permission restriction is F9 =[p : {self}]. The maximally permissive variable \ncontext satisfying F9 is given by . p =[self :1]. 2. The last inference step is of the form: .1 |f R1 \nAssert .2 | r1 : R1 f{R2} resource r2 in P1 I P2 {R2}0 1 [] [F8] resource r1 in . |f{R1 *R2} @ resource \nr2 in A {R1 *R2} [F9] P1 I P2 (where the . s need to satisfy various side conditions detailed in the \nformal rules given below). Note that the permission re\u00adstriction for the .rst premise is empty because \nit is a passive judgement. We calculate maximally permissive variable con\u00adtexts .1 and .2 using . (obtained \nin the previous step) and the permission restrictions for the premises [] and F8 calculated in the .rst \nphase. Recall that F8 =[p : {r1, self}]. This implies that .2 p should be of the form [r1 : p1, self \n: ps] for some non-zero fractions p1 and ps such that p1 + ps =1. The pre\u00adcise fractions do not matter, \njust that they should be non-zero. For instance, we can pick p1 == 1 . .1 should be of the  ps 2 form \n[p :[self : p1]] because the permission allocated to self in the resource invariant should be the permission \nallocated to r1 in .2. 3. Moving top-down in the pre-proof, we need to construct the inference: . ' 1 \n|f R2 Assert [] . ' 2 | r1 : R1,r2 : R2 f{emp} P1 I P2 {emp} [F7] .2 | r1 : R1 f{R2} resource r2 in P1 \nI P2 {R2} [F8] where .2 =[p :[r1 : p1, self : ps]] is the variable context from the previous step. Proceeding \nsimilarly to the previous step, we can calculate that the variable context . ' 2 in the judge\u00adment should \nbe of the form [r1 : p1' ,r2 : p2' , self : ps' ] such that p1 ' = p1 and p2 ' + ps ' = ps. However, \nthe permission re\u00adstriction F7 only lists r1 and r2 for p. Hence, ps should be 0, and p2 ' = ps.If p1 \n= ps = 21 was chosen in the previous step, = p ' 1 then we obtain p1 ' 2 = 2 . We omit the remaining \nsteps, which are straightforward. Note that the main task of the algorithm is now accomplished. The permis\u00adsions \nfor p in the two resources r1 and r2 have been inferred. They are 21 each. The algorithm for permission \ninference takes as input a pre\u00adproof P 0, regarded as a labeling function (P 0)N of a formal tree of \nnodes. In phase 1, it traverses the tree leaf-to-root and constructs a permission restriction FN for \neach node N. If the pre-inference for (P 0)N is of the form .N1 f SN1 \u00b7\u00b7\u00b7 .Nk f SNk X0 : (2) .N f SN \nthen the algorithm computes the permission restriction FN for node N as a partial function FR(FN1 ,..., \nFNk ) of the permission re\u00adstrictions of its children (antecedents of the pre-inference), satisfy\u00ading: \nProperty L0: If each domFNi contains exactly the modi.ed free variables of SNi (i.e., varaibles that \noccur on the left hand sides of assignments) then dom FN likewise contains exactly the modi.ed free variables \nof SN . Property L1: For every inference X that extends X0: .1 | .N1 f SN1 \u00b7\u00b7\u00b7 .k | .Nk f SNk X : . | \n.N f SN we have ( Vk .i |=FNi )=. . |=FN where FN = i=1 FR(FN1 ,..., FNk ). If, on the other hand, FR(FN1 \n,..., FNk ) is unde.ned then there exists no inference X extending X0. This case arises only for the \nvariable declaration rule. LEMMA 10. Given a pre-proof P 0,if FN is a family of of permis\u00adsion restrictions \nfor the nodes of P 0 produced in phase 1, then, for every write-proof P w that extends P 0, the variable \ncontext .N of (P w)N satis.es FN . The proof is by induction on the structure of the underlying tree \nof P 0. Thus, the result holds for all sub-proofs of P 0 as well. In phase 2, we construct a maximally \npermissive write-proof that extends P 0 by calculating .max for every node N.Forthe N root node, we \nchoose a maximally permissive . satisfying Froot. Then phase 2 proceeds from the root to leaves, constructing \n.max N for each node using the .max of the the parent (consequent of the pre-inference) and the permission \nrestrictions computed in phase 1. Speci.cally, given a pre-inference X0 .N1 f SN1 \u00b7\u00b7\u00b7 .Nk f SNk 0 X : \n.N f SN and .max satisfying FN , it computes .max ,..., .max for the N N1 Nk child nodes of N (antecedents \nof the pre-inference) as a function , .max GR(FN1 ,..., FNk ) of the given .max and the permission NN \n restrictions FN1 ,..., FNk of the child nodes, satisfying: Property L2: .max |=FN1 ,...,.max |=FNk , \nand the follow- N1 Nk ing is a legal inference that extends X0: .max .max | .N1 f SN1 \u00b7\u00b7\u00b7 | .Nk f SNk \nN1 Nk Xmax : .max N | .N f SN Moreover, for any other legal inference X that extends X0: .1 | .N1 f \nSN1 \u00b7\u00b7\u00b7 .k | .Nk f SNk X : . | .N f SN  such that .i. .i |=FNi and . |=FN ,wehave ..max =. N .max .i..i \n. Ni LEMMA 11. Given a pre-proof P 0, a family of permission restric\u00adtions FN produced in phase 1, and \na variable context .max sat\u00ad root isfying Froot,let P max be the write-proof on the same underlying tree \nof nodes obtained by using the given .max and contexts .max root N satisfying the Property L2. Then \n1. P max is a legal write-proof extending P 0 . 2. if P is any other write-proof extending P 0 using \nvariable con\u00ad  .max .max texts .N , and .root root,then .NN for all nodes N. The proof is by induction \non the depth of the nodes in the underly\u00ading tree of P 0 . We describe all these aspects compactly by \nwriting down the rules of SCI Separation Logic using the notations of this section, and displaying the \ncomputations of both the phases of the algo\u00adrithm. We decorate the judgements with schematic permission \nre\u00adstrictions F: . | . f S [F] in order to refer to the permission restrictions computed in phase 1 \nand used in phase 2. The side conditions of passive rules are ignored in phase 1, but used in phase 2. \nSome of rules are as follows (the others are similar): Expressions: where .v . FV (E). . v self > 0 \n. | . f E Exp [] Phase 1 is trivial: F=[]. Phase 2 checks to verify that .max satis.es the side condition. \nIf and only if the side condition is satis.ed, the write-proof that extends P 0 with .max for this node \nwill be a proof. The rule for Assertions is similar.  Assignable Variable: j 1, if o = self where . \nxo = . | . f x Var [F] 0, otherwise  Phase 1: F=[x : {self}] Resource declaration: Phase 2 computation \nis trivial because there are no premises. .1 | . f R Assert [] .2 | .,r : R f{P } C {Q} [F2] Sequencing: \n. | . f{P*R} resource r in C {Q*R} [F] .1 | . f{P } C1 {Q} [F1].2 | . f{Q} C2 {R} [F2] where R is precise \n.2 vo =. vo for all o .{self,r} . | . f{P } C1; C2 {R} [F] . v self =.2 v self +.2 vr = 1 where .1 =.2 \n=. j .2 vr, if o = self .1 vo = 0, otherwise Phase 1: Phase 1: domF = domF2 dom F = dom F1 . dom F2 \n8< F v = { o . F2 v | o = self,o = r }. F1 v if v . dom F1 \\ dom F2 { self | self . F2 v . r . F2 v } \nF2 v if v . dom F2 \\ dom F1 Phase 2: The context .max2 is de.ned as follows: F v = : (F1v n F2v) if \nv . dom F1 n dom F2 =.max =.max Phase 2: .max1 2 . All other rules such as conditionals, assignment, \nlookup and mutation are similar to Sequencing in that . remains unchanged in the premises. Their Phase \n1 and Phase 2 computations are exactly the same as for Sequencing. Parallel composition: .1 | . f{P1} \nC1 {Q1} [F1].2 | . f{P2} C2 {Q2} [F2] . | . f{P1 *P2} C1 I C2 {Q1 *Q2} [F] where .1 vo =.2 vo =. vo for \nall o = self . v self =.1 v self +.2 v self = 1 Phase 1: vo is .max  For o .{self,r}, .max2 vo.  The \npair (.max2 v self, .max2 vr) is as follows: If v . domF2 .max 1 .max  then it is ( 12 v self, 2 v self).If \nv . dom F2: If self . F2 v and r . F2 v,itis ( 12 .max v self, 1 .max 2 v self). If self . F2 v and r \n. F2 v,itis (.max v self, 0). If self . F2 v and r . F2 v,itis (0, .max v self). If self . F2 v and r \n. F2 v,itis (0, 0).  .max 12 1 v self is the same as .max vr.For all o = self, .max vo is 0. Variable \ndeclaration: .1 | . f P, Q Assert [] .2 | . f{P } C {Q} [F2] dom F = dom F1 . dom F2 F1 v if v . dom \nF1 \\ dom F2 . f . f{P } local x in C {Q} [F] F2 v if v . dom F2 \\ dom F1 8< F v = : (F1v n F2v) \\{self} \nif v . dom F1 n dom F2 j where .2 vo =. vo for all v = x 1, if o = self vo, .max .2 xo = Phase 2: The \npair (.max1 2 vo) is as follows: 0, otherwise If o = self,itis (.max vo, .max vo).  If o = self and \nv . dom F1 \\ dom F2,itis (.max v self, 0)  If o = self and v . dom F2 \\ dom F1,itis (0, .max v self). \n If o = self and v . dom F1 n dom F2,itis ( 12 .max v self, 1 .max  2 v self). If o = self and v . \ndom F1 . dom F2,itis ( 12 .max v self, 1 .max 2 v self). The Frame rule is similar to parallel composition. \n Critical regions: .1 | .,r : R f P, Q Assert [] .2 | . f{(P*R) . B} C {Q*R} [F2] . | .,r : R f{P } \nwith r when B do C od {Q} [F] where .2 vo =. vo for all o .{self,r}.2 v self =. v self +. vr = 1 .2 vr \n=0 .1 =. Phase 1: domF = domF2 F v = { o . F2 v | o = self,o = r }.{ self | self . F2 v }.{ r | self \n. F2 v } Phase 2: vo =.max For o .{self,r}, .max vo. 2 .max 2 v self =.max v self +.max vr. .max =.max \nand .max vr =0. 12 .1 vo =. vo for all v = x .1 xo =0 Phase 1: If self . F2 x or x . dom F2 then the \ncomputation is: domF = (domF2) \\{x}F v =F2 v for all v = x If x . dom F2 and self . F2 x then Phase \n1 fails, i.e., there is no write-proof extending P 0 . Phase 2: .max 2 xo is 1 when o is self, 0 otherwise. \nFor all other v, .max vo =.max vo. 2 .max =.max 1 . It may be veri.ed that all the phase 1 and phase \n2 computations listed above satisfy the properties L1 and L2 respectively, complet\u00ading the proof of correctness. \n 7. Conclusion We have provided a streamlined formulation of Sequential and Concurrent Separation Logic \nrules without awkward side condi\u00adtions for variable usage. The rules are more expressive than the original \nOwicki-Gries-O Hearn system. Yet, they retain the syn\u00adtactic character of the variable conditions without \nadding proof burden in the programming logic itself. This syntactic character is exploited in devising \nan algorithm to automatically infer the anno\u00adtations required in resource declarations. This should prove \nuseful for Separation Logic-based veri.cation tools like Smallfoot. Our work is also a modest contribution \nto the theory of Syn\u00adtactic Control of Interference, which dates back to 1978. While the system has been \nstudied from a semantics point of view, it has not been previously applied to the formulation of programming \nlogics, which is somewhat paradoxical given its natural .t with reason\u00ading principles. We have extended \nthe traditional framework with permission algebras, which should prove useful for further devel\u00adopment. \n Further work along this line would include the extension of Concurrent Separation Logic with higher-order \nfeatures such as procedures and objects, for which Syntactic Control of Interference is well-suited. \n  Acknowledgments We are grateful to Philippa Gardner and her group for the yak session at Imperial \nCollege where this work was .rst presented. Peter O Hearn has provided much appreciated encouragement \nand inspiration throughout the development of this work. Spe\u00adcial thanks go to Josh Berdine and Ian Wehrman \nfor making us aware of the technical problems with the earlier proof systems. John Reynolds s work was \npartially supported by National Science Foundation Grant CCF-0916808.  References [1] S. Abramsky and \nG. McCusker. Linearity, sharing and state. In Algol\u00adlike Languages O Hearn and Tennent [28], chapter \n20. [2] S. Abramsky, K. Honda, and G. McCusker. A fully abstract game semantics for general references. \nIn LICS 1998, pages 334 344, 1998. [3] K. R. Apt. Ten years of Hoare s logic: A survey. ACM Trans. Program. \nLang. Syst., 3(4):431 483, Oct. 1981. [4] J. Berdine and I. Wehrman. Variable conditions and CSL. Private \ncommunication, 4th April, 2011. [5] K. Bierhoff. API protocol compliance in object-oriented software. \nTechnical Report CMU-ISR-09-108, Carnegie-Mellon University, Apr 2009. [6] R. Bornat, C. Calcagno, P. \nO Hearn, and M. Parkinson. Permission accounting in Separation Logic. In ACM Symp. on Princ. of Program. \nLang., pages 59 70. ACM Press, 2005. [7] R. Bornat, C. Calcagno, and H. Yang. Variables as resource in \nSepa\u00adration Logic. In Proc. 22nd Ann. Conf. on Math. Found. of Program. Semantics (MFPS XXII) Main et \nal. [21], pages 247 276. [8] J. Boyland. Checking interference with fractional permissions. In R. Cousot, \neditor, Static Analysis: 10th Intern. Symp., volume 2694 of LNCS, pages 55 72. Springer, 2003. [9] P. \nBrinch Hansen. Operating System Principles. Prentice-Hall, Engle\u00adwood Cliffs, 1973. [10] P. Brinch Hansen. \nStructured multiprogramming. Comm. ACM, 15: 574 577, July 1972. [11] S. D. Brookes. A semantics for Concurrent \nSeparation Logic. Theo\u00adretical Comput. Sci., 375(1-3):227 270, Apr 2007. [12] S. D. Brookes. A revisionist \nhistory of Concurrent Separation Logic. In Mislove and Ouaknine [24], pages 5 28. [13] S. D. Brookes. \nVariables as resource for shared-memory programs: Semantics and soundness. In Proc. 22nd Ann. Conf. on \nMath. Found. of Program. Semantics (MFPS XXII) Main et al. [21], pages 123 150. doi: DOI: 10.1016/j.entcs.2006.04.008. \n[14] L. Damas and R. Milner. Principal type-schemes for functional pro\u00adgrams. In ACM Symp. on Princ. \nof Program. Lang., pages 207 212, 1982. [15] J.-Y. Girard. Linear logic. Theoretical Comput. Sci., 50:1 \n102, 1987. [16] A. Gotsman, J. Berdine, and B. Cook. Precision and the conjunction rule in Concurrent \nSeparation Logic. In Mislove and Ouaknine [24]. [17] C. A. R. Hoare. Towards a theory of parallel programming. \nIn C. A. R. Hoare and R. H. Perrott, editors, Operating Systems Techniques, pages 61 71. Academic Press, \n1972. [18] C. A. R. Hoare. Monitors: An operating system structuring concept. Comm. ACM, 17(10):549 \n558, Oct. 1974. [19] B. J., C. Calcagno, and P. W. O Hearn. Smallfoot: Modular automatic assertion checking \nwith Separation Logic. In F. S. de Boer, editor, For\u00admal Methods for Components and Objects, 4th Intern. \nSymp., volume 4111 of LNCS, pages 115 137. Springer-Verlag, 2005. [20] K. Kapoor, K. Lodaya, and U. S. \nReddy. Fine grained concurrency with Separation Logic. J. Philosophical Logic, 40(5):583 632, Oct 2011. \ndoi: 10.1007/s10992-011-9195-1. [21] M. Main, A. Melton, and M. Mislove. Proc. 22nd Ann. Conf. on Math. \nFound. of Program. Semantics (MFPS XXII), volume 158 of Elect. Notes in Theor. Comput. Sci. Elsevier, \n2006. [22] G. McCusker. A graph model for imperative computation. Logical Methods in Comp. Sci., 6(1-2), \nJan 2010. [23] R. Milner. A theory of type polymorphism in programming. J. Com\u00adput. Syst. Sci., 17:348 \n375, 1978. [24] M. Mislove and J. Ouaknine, editors. Proc. 27nd Ann. Conf. on Math. Found. of Program. \nSemantics (MFPS XXVII), volume 276 of Elect. Notes in Theor. Comput. Sci. Elsevier, 2011. [25] P. W. \nO Hearn. Resources, concurrency and local reasoning. Theoret\u00adical Comput. Sci., 375(1-3):271 307, May \n2007. [26] P. W. O Hearn. Linear logic and interference control. In Category Theory and Computer Science, \nvolume 350 of LNCS, pages 74 93. Springer-Verlag, 1991. [27] P. W. O Hearn and D. J. Pym. The logic of \nbunched implications. Bulletin Symbolic Logic, 5(2):215 244, June 1999. [28] P. W. O Hearn and R. D. \nTennent. Algol-like Languages (Two vol\u00adumes).Birkh\u00a8 auser, Boston, 1997. [29] P. W. O Hearn, A. J. Power, \nM. Takeyama, and R. D. Tennent. Syn\u00adtactic control of interference revisited. In S. D. Brookes, M. Main, \nA. Melton, and M. Mislove, editors, Math. Found. of Program. Se\u00admantics: Eleventh Ann. Conference, volume \n1 of Elect. Notes in Theor. Comput. Sci. Elsevier, 1995. (Reprinted as Chapter 18 of [28]). [30] P. \nW. O Hearn, J. C. Reynolds, and H. Yang. Local reasoning about programs that alter data structures. In \nL. Fribourg, editor, CSL 2001, volume 2142 of LNCS, pages 1 19, Berlin, 2001. Springer-Verlag. [31] S. \nOwicki and D. Gries. Verifying properties of parallel programs: An axiomatic approach. Comm. ACM, 19(5):279 \n285, May 1976. [32] M. Parkinson, R. Bornat, and Calcagno. Variables as resource in Hoare Logics. In \nSymp. on Logic in Comput. Sci., pages 137 146. IEEE, 2006. [33] U. S. Reddy. Global state considered \nunnecessary: An introduction to object-based semantics. J. Lisp and Symbolic Computation, 9:7 76, 1996. \n(Reprinted as Chapter 19 of [28]). [34] J. Reynolds. Separation Logic: A logic for shared mutable data \nstructures. In LICS, pages 55 74, 2002. [35] J. C. Reynolds. A problematic program (joint work with Josh \nBerdine). Presentation at the Dagstuhl workshop on Types, Logics and Semantics for State, 2008. [36] \nJ. C. Reynolds. Syntactic control of interference. In ACM Symp. on Princ. of Program. Lang., pages 39 \n46. ACM, 1978. (Reprinted as Chapter 10 of [28]). [37] J. C. Reynolds. Idealized Algol and its speci.cation \nlogic. In D. Neel, editor, Tools and Notions for Program Construction, pages 121 161. Cambridge Univ. \nPress, 1982. (Reprinted as Chapter 6 of [28]). [38] V. Vafeiadis. Concurrent Separation Logic and operational \nsemantics. In Mislove and Ouaknine [24]. [39] H. Yang and P. W. O Hearn. A semantics basis for local \nreasoning. In FOSSACS, pages 402 416, Berlin, 2002. Springer-Verlag. [40] H. Yasuoka and T. Terauchi. \nPolymorphic fractional capabilities. In Static Analysis Symposium/Workshop on Static Analysis, pages \n36 51, 2009. doi: 10.1007/978-3-642-03237-0 5.  \n\t\t\t", "proc_id": "2103656", "abstract": "<p>Separation Logic has witnessed tremendous success in recent years in reasoning about programs that deal with heap storage. Its success owes to the fundamental principle that one should keep separate areas of the heap storage separate in program reasoning. However, the way Separation Logic deals with program variables continues to be based on traditional Hoare Logic without taking any benefit of the separation principle. This has led to unwieldy proof rules suffering from lack of clarity as well as questions surrounding their soundness. In this paper, we extend the separation idea to the treatment of variables in Separation Logic, especially Concurrent Separation Logic, using the system of Syntactic Control of Interference proposed by Reynolds in 1978. We extend the original system with permission algebras, making it more powerful and able to deal with the issues of concurrent programs. The result is a streamined presentation of Concurrent Separation Logic, whose rules are memorable and soundness obvious. We also include a discussion of how the new rules impact the semantics and devise static analysis techniques to infer the required permissions automatically.</p>", "authors": [{"name": "Uday S. Reddy", "author_profile_id": "81100154268", "affiliation": "University of Birmingham, Birmingham, United Kingdom", "person_id": "P2991413", "email_address": "u.s.reddy@cs.bham.ac.uk", "orcid_id": ""}, {"name": "John C. Reynolds", "author_profile_id": "81100470240", "affiliation": "Carnegie-Mellon University, Pittsburgh, PA, USA", "person_id": "P2991414", "email_address": "John.Reynolds@cs.cmu.edu", "orcid_id": ""}], "doi_number": "10.1145/2103656.2103695", "year": "2012", "article_id": "2103695", "conference": "POPL", "title": "Syntactic control of interference for separation logic", "url": "http://dl.acm.org/citation.cfm?id=2103695"}