{"article_publication_date": "01-25-2012", "fulltext": "\n A Type Theory for Probability Density Functions Sooraj Bhat Ashish Agarwal Richard Vuduc, Alexander \nGray Georgia Institute of Technology New York University Georgia Institute of Technology sooraj@gatech.edu \nashish.agarwal@nyu.edu {richie,agray}@cc.gatech.edu Abstract There has been great interest in creating \nprobabilistic programming languages to simplify the coding of statistical tasks; however, there still \ndoes not exist a formal language that simultaneously provides (1) continuous probability distributions, \n(2) the ability to naturally express custom probabilistic models, and (3) probability density functions \n(PDFs). This collection of features is necessary for mech\u00adanizing fundamental statistical techniques. \nWe formalize the .rst probabilistic language that exhibits these features, and it serves as a foundational \nframework for extending the ideas to more general languages. Particularly novel are our type system for \nabsolutely continuous (AC) distributions (those which permit PDFs) and our PDF calculation procedure, \nwhich calculates PDFs for a large class of AC distributions. Our formalization paves the way toward the \nrigorous encoding of powerful statistical reformulations. Categories and Subject Descriptors F.3.2 [Logics \nand Meanings of Programs]: Semantics of Programming Languages Program analysis; F.3.1 [Logics and Meanings \nof Programs]: Specifying and Verifying and Reasoning about Programs Mechanical veri.\u00adcation; G.3 [Probability \nand Statistics]: Statistical computing General Terms Theory, Languages Keywords Continuous Probability, \nProbability Density Functions 1. Introduction In the face of more complex data analysis needs, both \nthe ma\u00adchine learning and programming languages communities have rec\u00adognized the need to express probabilistic \nand statistical computa\u00adtions declaratively. This has led to a proliferation of probabilistic programming \nlanguages [7, 9, 12 14, 16, 18 22]. Program trans\u00adformations on probabilistic programs are crucial: many \ntechniques for converting statistical problems into ef.cient, executable algo\u00adrithms are syntactic in \nnature. A rigorous language de.nition aids reasoning about the correctness of these program transformations. \nHowever, several fundamental statistical techniques cannot cur\u00adrently be encoded as program transformations \nbecause current lan\u00adguages have weak support for probability distributions on contin\u00aduous or hybrid discrete-continuous \nspaces. In particular, no exist\u00ading language rigorously supports expressing the probability density function \n(PDF) of custom probability distributions. This is an im\u00adpediment to mechanizing statistics; continuous \ndistributions and Permission to make digital or hard copies of all or part of this work for personal \nor classroom use is granted without fee provided that copies are not made or distributed for pro.t or \ncommercial advantage and that copies bear this notice and the full citation on the .rst page. To copy \notherwise, to republish, to post on servers or to redistribute to lists, requires prior speci.c permission \nand/or a fee. POPL 12, January 25 27, 2012, Philadelphia, PA, USA. Copyright c &#38;#169; 2012 ACM 978-1-4503-1083-3/12/01. \n. . $10.00 their PDFs are ubiquitous in statistical theory and applications. Techniquessuch as maximumlikelihoodestimation \n(MLE), L2 esti\u00admation (L2E), and nonparametric kernel methods are all formulated in terms of the PDF \n[3, 23, 24]. Speci.cally, we want the ability to naturally express a probabilistic model over a discrete, \ncontinuous or hybrid space and then mechanically obtain a usable form of its PDF. Usage of the PDF may \nentail direct numerical evaluation of the PDF or symbolic manipulation of the PDF and its derivatives. \nCon\u00adtinuous spaces pose some unique obstacles, however. First, the exis\u00adtence of the PDF is not guaranteed, \nunlike the discrete case. Second, stating the conditions for existence involves the language of mea\u00adsure \ntheory, an area of mathematics renowned for nonconstructive results, suggesting that mechanization may \nnot be straightforward. Notably, obtaining a PDF from its distribution is a non-computable operation \nin the general case [11]. In light of these issues, we make the following new contributions: We present \na formal probability language with classical measure\u00adtheoretic semantics which allows naturally expressing \na variety of useful probability distributions on discrete, continuous and hybrid discrete-continuous \nspaces, as well as their PDFswhen they exist (Section 3). The language is a core calculus which omits \nfunctions and mutation.  We de.ne a type system for absolutely continuous probabil\u00adity distributions, \ni.e. those which permit a PDF. The type sys\u00adtem does not require mechanizing s-algebras, null sets, the \nLebesgue measure, or other complex constructions from mea\u00adsure theory. The key insight is to analyze \na distribution by how it transforms other distributions instead of using the obvious in\u00adduction on the \nmonadic structure of the distribution (Section 4).  We de.ne a procedure that calculates PDFs for a \nlarge class of distributions accepted by our type system. The design per\u00admits modularly adding knowledge \nabout individual distribu\u00adtions with known PDFs (but which cannot be calculated from scratch), enabling \nthe procedure to proceed with programs that use these distributions as subcomponents (Section 5).  \nWe believe this is the .rst general treatment of PDFs in a language. We deliberately omit features that \nare not essential to the current investigation (e.g. expectation, sampling). Finally, we discuss the \nrelation to existing and future work (Sections 6 and 7). In particu\u00adlar, we save a treatment of PDFs \nin the context of conditional prob\u00adability for future work. 2. Background and motivation We .rst introduce \nprobability in the context of countable spaces to emphasize the complications that arise when moving \nto continuous spaces. Wefocusonlyonissuessurrounding PDFs.Weoccasionally deviate from standard probability \nnotation to circumvent impreci\u00adsion in the standard notation and to create a harmonious notation throughout \nthe paper. In this section we present a specialized ac\u00ad  -3-2-10 1 2 3 -3-2-10 1 2 3 -3-2-10 1 2 Figure \n1. The CDF and PDF of a standard normal distribution and the CDF of a distribution that does not have \na PDF. count of probability for ease of exposition. We discuss the rigorous and generalized de.nitions \nin Section 3.3. We use the term discrete distribution for distributions on dis\u00adcrete spaces (countable \nsets); continuous distribution for distribu\u00adtions on the continuous spaces R and Rn;and hybrid distribution \nfor distributions on products of discrete and continuous spaces that are themselves neither discrete \nnor continuous, such as R \u00d7 Z. 2.1 Probability on countable spaces Consider a set of outcomes A. For \nnow, let A be countable. It is meant to represent the possible states of the world we are modeling, such \nas the set of possible outcomes of an experiment or measure\u00adments of a quantity. An event is a subset \nof A, also understood as a predicate on elements of A. Events denote some occurrence of in\u00adterest and \npartition the outcomes into those that exhibit the property and those that do not. A probability distribution \nP (or simply, distri\u00adbution)on A is a function from events to [0, 1] such that P(X) = 0 8 8 for all events \nX, P(A)=1 and P( Xi)=P(Xi) for i=0 i=0 countable sequences of mutually disjoint events Xi. Distributions \ntell us the probability that different events will occur. It is gener\u00adally more convenient to work with \na distribution s probability mass function (PMF) instead, de.ned f (x)= P({x}), which tells us how likely \nan individual outcome is. It satis.es P(X)=f(x) x.X for all events X on A. For example, if P is the \ndistribution char\u00adacterizing the outcome of rolling a fair die, its PMF is given by f(x)= 61 ,where x \n. A and A = {1, 2, 3, 4, 5, 6}. The probabil\u00ad 11 1 ity an even number is rolled is P({2, 4, 6})= 1 ++= \n. 666 2  2.2 Moving to continuous spaces A probability density function (PDF) is the continuous analog \nof the PMF. Unfortunately, although every distribution on a countable set has a PMF, not every distribution \non a continuous space has a PDF. Consider distributions on the real line. We say that a function f is \na PDF of a distribution P on R if for all events X, P(X)=f(x) dx, (1) X which states that the probability \nof X is the integral of f on X (in the simplest case, X is an interval). This idea can be extended to \nmore general spaces. This equation does not determine f uniquely, but any two solutions f1 and f2 are \nequal almost everywhere (see Section 3.3) and give identical results under integration. Thus, we often \nrefer to a PDF as the PDF. For the spaces we consider in this paper, the property of having a PDF is \nequivalent to being absolutely continuous (AC). Roughly speaking, a distribution is AC if it never assigns \npositive probabil\u00adity to events that have size zero in the underlying space. For in\u00adstance, the standard \nnormal distribution is absolutely continuous v and has the PDF f(x)=exp(-x 2/2)/ 2p. On the other hand, \nthe distribution of y in the model z ~ CoinFlip(1/2) 0 if z = heads y = x ~ Normal(0, 1) x if z = tails. \n does not have a PDF.Wehaveused random variables to write the model; this is a commonly used informal \nnotation that is shorthand for a more rigorous expression that de.nes the model. The model represents \nthe following process: .ip a fair coin; return 0 if it is heads, and sample from the standard normal \ndistribution otherwise. We can see that it is not AC:the event {0} occurs with probability 1/2 (whenever \nthe coin comes up heads), but has an interval length of zero. We use the cumulative distribution function \n(CDF)tovi\u00adsualize each distribution (Figure 1); the CDF F of a distribution P on R is F (x)= P((-8,x]) \nand gives the probability that a sample from the distribution takes a value less than or equal to x. \nFrom Equation 1 we know that P has a PDF if and only if there ex\u00ad fx ists a function f such that F (x)= \n-8 f (t) dt.Clearly,nosuch function exists for the CDF of y due to the jump discontinuity. Mixing discrete \nand continuous types is not the only culprit. Consider the following process: sample a number u uniformly \nrandomly from [0, 1] and return the vector x =(u, u) . R2 . The distribution of x (a distribution on \nR2) is not AC:the event X = {(u, u) | u . [0, 1]} has probability 1, but X is a line segment and thus \nhas zero area. Likewise, there is no PDF on R2 we could integrate to give positive mass on this zero \narea line segment. 2.3 Applications of the PDF The PDF is often used to compute expectations (and probabilities, \nwhich are a special case of expectation). Expectation is a funda\u00admental operation in probability and \nis used in de.ning quantities such as mean and variance. The expectation operation E of a distri\u00adbution \nP on R is a higher-order function that satis.es 8 E(g)=g(x) \u00b7 f(x) dx -8 when P has a PDF, f, and the \nintegral exists. Another application is maximum likelihood estimation (MLE), which addresses the prob\u00adlem \nof choosing the member of a family of distributions that best explains observed data. Let P( \u00b7 ; \u00b7 ) \nbe a parameterized family of distributions, where P( \u00b7 ; .) is the distribution for a given parame\u00adter \n..The MLE estimate . * of P for observed data x is given by . * =arg max f(x; .) . where f( \u00b7 ; .) is \nthe PDF of P( \u00b7 ; .). For example, x could be a set of points in Rn we wish to cluster, and . * could \nbe the estimate of the locations of the cluster centroids. P would be the family of distributions we \nbelieve generated the clusters (a family parameterized by the positions of the cluster centroids), such \nas a mixture-of-Gaussians model. More details are available in [3].  2.4 Challenges for language design \nCategorically, probability distributions form a monad [8, 20]. This structure forms the basis of many \nprobabilistic languages because it is minimal, elegant, and presents many attractive features. First, \nit provides the look and feel of informal random variable notation, al\u00adlowing us to express models as \nwe normally would, while remain\u00ading mathematically rigorous. The monad structure affords formu\u00adlating \nprobability as an embedded domain speci.c language [7, 13, 18, 20] or as a mathematical theory in a proof \nassistant [2]. Addi\u00adtionally, many proofs about distributions expressed in the probabil\u00adity monad are \ngreatly simpli.ed by the monadic structure. We feel it is desirable to structure a language around the \nprobability monad, and we investigate supporting PDFs speci.cally in such languages. The probability \nmonad consists of monadic return and monadic bind, as usual. Monadic return corresponds to the point \nmass distri\u00adbution. We also provide the Uniform(0,1) distribution as a monadic value. These three combinators \ncan be used to express a variety of distributions. The main issue when designing a type system for ab\u00adsolute \ncontinuity is that return creates non-AC distributions (on con\u00adtinuous types), yet as a core member of \nthe monadic calculus it appears in the speci.cation of nearly every distribution, even those that are \nAC. The obvious induction along the monadic structure is dif.cult to use to prove absolute continuity \nin the cases of inter\u00adest. Consider for instance the joint distribution of two independent Uniform(0,1) \nrandom variables, written in our language as var x ~ random in var y ~ random in return (x, y). (2) It \nis AC even though the subexpressions return (x, y) and var y ~ random in return (x, y) are both not AC, \nwhere we treat x and y as real numbers, as dictated by the typing rule for bind. Also, as implementors \nwe have found it dif.cult to eyeball the rules for absolute continuity. For example, only the .rst of \nthese distribu\u00adtions is AC even though they are all nearly identical to Equation 2: var x ~ random in \nvar y ~ random in return (x, x + y) var x ~ random in var y ~ random in return (x, y - y) var x ~ random \nin var y ~ random in return (x, y, x + y) Clearly, what is needed is a principled analysis. We provide \nthis in Section 4. A natural urge is wanting to remove return to create a language in which only AC distributions \nare expressible. We feel this is undesirable. Without return, we would not be able to express something \nas simple as adding two random variables (consider (x + y) instead of (x, y) in Equation 2). Essentially, \nreturn allows us to express random variables as transformations of other random variables a fundamental \nmodeling tool we feel should be supported, allowing users to write down models that most naturally capture \ntheir domain. Without return we must extend the core calculus for each transformation we wish to use \non random variables, and we must do so carefully if we want to ensure that non-AC distributions remain \ninexpressible. This extension of the core detracts from minimality and elegance, and it complicates developing \nthe theory in a veri.cation environment such as COQ, one of our eventual goals. Finally, in addition \nto checking for existence, we would like to also calculate a usable form for the PDF. Many current probabilis\u00adtic \nlanguages focus on distributions with only .nitely many alter\u00adnatives, which allows for implementing \ndistributions as weighted lists of outcomes. The probability monad in this case is similar to the list \nmonad, with some added logic describing how bind should propagate the weights. The weighted lists correspond \ndirectly to the PMF, but no such straightforward computational strategy exists for the PDF. We explore \nthis further in Section 5. Variables x, y, z, u, v Literals l Base types t ::= bool | Z | R | t1 \u00d7 t2 \nTypes t ::= t | dist t Expressions e ::= x | l | op e1...en | if e1 then e2 else e3 Primops op ::= + \n|*| neg | inv | = | < | (\u00b7, \u00b7) | fst | snd | exp | log | sin | cos | tan | Rof Z Distributions e ::= \nrandom | return e | var x ~ e1 in e2 Programs p ::= pdf e Contexts G ::= \u00d8| G,x : t . ::= \u00d8| .,x : t \n~ e Substitution e[x := e] Free variables FV ( \u00b7 ) Figure 2. The abstract syntax.  3. The language \nIn this section we present the abstract syntax, type system and semantics for our probabilistic language, \nexcept for the parts related to PDFs, which we cover in Section 4. 3.1 Abstract syntax Figure 2 contains \nthe syntax de.nitions. In addition to the stan\u00addard letters for variables, we also use u and v when we \nwant to emphasize that a random variable is distributed according to the Uniform(0,1) distribution. The \nsyntactic category for literals in\u00adcludes Boolean (bool), integer (Z), and real number (R) literals. \nTypes are strati.ed to ensure that distributions (dist t) are only over base types. Integers are a distinct \ntype from the reals; there is no subtyping in the language. We also stratify terms to simplify analysis. \nExpressions and primitive operations (primops) take their standard mathematical meaning, unless noted \notherwise. For sim\u00adplicity, we overload addition, multiplication, negation, and integer literals on the \nintegers and reals, but fundamentally there is a + for integers and a separate + for reals, etc.Inversion \ninv denotes the reciprocal operation, and log is the natural logarithm. We give our semantics in terms \nof classical mathematics, so we do not con\u00adcern ourselves with the issue of computation on the reals. \nEquality is de.ned on all base types in the usual way, and less-than is de\u00ad.ned only on the numeric types. \nWe write (e1,e2, ..., en-1,en) as shorthand for (e1, (e2, ...(en-1,en)...)). The function Rof Z injects \nan integer into the reals. The distribution random corre\u00adsponds to the Uniform(0,1) distribution. The \nnext two constructs correspond to monadic return and bind for the probability monad. The distribution \nreturn e is the point mass distribution, which as\u00adsigns probability 1 to the event {e}. A random variable \ndistributed according to return e is in fact deterministic: there is no variation in the value it can \ntake. The bind construct, var x ~ e1 in e2, is used to build complex distributions from simpler ones. \nIt can be read: introduce random variable x, distributed according to the distribution e1, with scope \nin the distribution e2 . It is the only binding construct in the language. For simplicity, we have chosen \nto omit let-bindings and functions from our language, but we use both in our examples. We can use standard \nsubstitution rules to reduce such examples to the syntax of Figure 2. Exam\u00adples include (e) := if e then \n1 else 0 (to inject Booleans into the reals), e1 - e2 := e1 + neg e2, e1/e2 := e1 * inv e2,and e1 <e2 \n<e3 := if e1 <e2 then e2 <e3 else false. Finally, free variables, capture-avoiding substitution, and \ntyping contexts (G) are de.ned in the usual way. The probability context . is used to additionally keep \ntrack of the distributions that random variables are bound to. When we use . in places G is expected, \nthe under\u00adstanding is that the extra information carried by . is ignored.  3.2 Examples of expressible \ndistributions With just random, return, and bind, we can already construct a wide variety of distributions \nwe might care to use in practice. Though we do not have a formal proof of this expressivity, existing \nwork on sampling suggests that this is the case. Non-uniform ran\u00addom variate generation is concerned \nwith generating samples from arbitrary distributions using only samples from Uniform(0,1) [6]. We can \nsee the connection with our language if we view the con\u00adstructs by a sampling analogy, which emphasizes \nunderstanding a distribution by its generating process: the phrase var x ~ e1 in e2 samples a value x \nfrom the sampling function e1, which is used to create a new sampling function e2; random samples from \nthe Uniform(0,1) distribution; return e always returns e as its sample. For instance, the standard normal \ndistribution can be de.ned in our language using the Box-Muller sampling method: var u ~ random in var \nv ~ random in std normal := return (sqrt(-2 * log u) * cos(2 * p * v)) where sqrt e := exp ((1/2) * log \ne). In particular, our language is amenable to inverse transform sampling. Likewise, we can express other \ncommon continuous distributions: uniform e1 e2 := std logistic := var u ~ random in var u ~ random in \nreturn ((e2 - e1) * u + e1) return (log (1/u - 1)) normal e1 e2 := std exponential := var x ~ std normal \nin var u ~ random in return (e2 * x + e1) return (-log u) These are the Uniform(a,b), standard logistic, \nstandard exponential, and Normal(\u00b5, s) distributions. We intentionally parameterize the normal distribution \nby its standard deviation instead of its variance, for simplicity. We de.ne it as a transformation of \na standard normal random variable and require e2 > 0. We can also express discrete distributions, such \nas the coin .ip distribution, .ip e := var u ~ random in return (u<e), which takes the value true with \nprobability e. This is equivalent to the Bernoulli distribution. In fact, we can express any distribution \nwith .nitely many outcomes: var u ~ random in return (if u< 1/3 then 10 else if u< 2/3 then 20 else 30) \nAdmittedly, a more satisfying de.nition would be possible if we had lists in the language. The reason \nwe do not is that, although oth\u00aders have addressed recursion and iteration in the context of de.ning \nprobability distributions [14], we have not yet fully reconciled re\u00adcursion with PDFs. The absence of \nrecursion also means that we do not support distributions in the style of rejection sampling meth\u00adods, \nwhich resample values until a stopping criterion is met. Fur\u00adthermore, we do not elegantly support in.nite \ndiscrete distributions in the core language, many of which are naturally described using recursion. However, \nin Section 5 we describe how to add special support for any distribution with a known PDF. We also de.ne \nsome higher-order concepts. The following func\u00adtions are used to create joint distributions and mixture \nmodels: mix ee1 e2 := join e1 e2 := var z ~ .ip e in var x1 ~ e1 in var x1 ~ e1 in var x2 ~ e2 in var \nx2 ~ e2 in return (x1,x2) return (if z then x1 else x2). The mixture model is created by .ipping a coin \nwith the speci.ed probability to determine which component distribution to sam\u00adple from. For instance, \na simple mixture-of-Gaussians is given by mix (1/2) std normal std normal. We have de.ned discrete and \ncontinuous distributions, and now we can use join to de.ne non\u00adtrivial hybrid distributions as well, \nsuch as join (.ip (1/2)) random, which has type dist (bool \u00d7 R). Essentially, if-expressions enable mixture \nmodels and tuples enable joint models. These two concepts are special cases of hierarchical models, which \nare models that are de.ned in stages. Distributions de.ned using nested instances of bind correspond \nto hierarchical models. These examples are all AC, but we can also express non-AC dis\u00adtributions, such \nas the example from Section 2, written jumpy := mix (1/2) (return 0) std normal. In our language, jumpy \nwill suc\u00adcessfully type check as a distribution, but the program pdf jumpy will be rejected as it should \nbe because jumpy is not abso\u00adlutely continuous. The ability to represent non-AC distributions, even though \nthey cannot be used in programs, is in anticipation of future language features such as expectation and \nsampling, which can be used with non-AC distributions. 3.3 Measure theory preliminaries Measure theory \nis the basis of modern probability theory and uni\u00ad.es the concepts of discrete and continuous probability \ndistribu\u00adtions. It is a precise way of de.ning the notion of volume. We de\u00advelop our formalization within \nthis framework. We give only a brief overview of the necessary concepts; details are available in [17]. \nBasics Let A be a set we wish to measure. A s-algebra M on A is a subset of the powerset P(A) that contains \nA and is closed under complement and countable union. The pair (A, M) is a measurable space. A subset \nX of A is M-measurable if X .M. In the context of probability, A is the set of outcomes and M is the \nset of events. For a function f : A . B,the f-image of a subset X of A, written f[X], denotes the set \n{f (x) | x . A}, and the f-preimage of a subset Y of B, written f-1[Y ], denotes the set {x . A | f (x) \n. Y }.When f is on measurable spaces, we say f is (MA, MB )-measurable when the f-preimage of any MB-measurable \nset is MA-measurable. Measurable functions are closed under composition. We drop the pre.x and say measurable \n(for functions or sets) when it is clear what the s-algebras are. The s-algebra machinery is needed to \nensure a consistent theory; there are spaces which contain pathological sets that violate intuition about \nvolume, e.g. the Banach-Tarski doubling ball paradox. Measure theory sidesteps these issues by preferring \nmeasurable sets and functions as much as possible. When A is countable, no such problems arise, and we \ncan always take P(A) for M. Measures A nonnegative function \u00b5 : M. R+.{8} is a mea\u00ad 8 sure if \u00b5(\u00d8)=0, \n\u00b5(X) = 0 for all X in M,and \u00b5( i=1 Xi)= 8 i=1 \u00b5(Xi) for all sequences of mutually disjoint Xi (countable \nadditivity). The triple (A, M,\u00b5) is a measure space. If addition\u00adally \u00b5(A)=1 then \u00b5 is a probability \nmeasure (conventionally written P), and the triple is a probability space.Weuse theterms probability \nmeasure, probability distribution and distribution inter\u00adchangeably. We use C to denote the counting \nmeasure, which uses the number of elements of a set as the set s measure. We use L to denote the Lebesgue \nmeasure on R, which assigns the length |b - a| to an open interval (a, b); the sizes of other sets can \nbe un\u00adderstood by complements and countable unions of intervals. The product measure \u00b5A . \u00b5B of two measures \n\u00b5A and \u00b5B on measur\u00adable spaces (A, MA) and (B, MB) is the measure \u00b5,on A \u00d7 B and the product s-algebra \nMA .MB , such that \u00b5(X \u00d7 Y )= \u00b5A(X) \u00b7 \u00b5B (Y ) for X .MA and Y .MB. The measure is unique when \u00b5A and \n\u00b5B are s-.nite. The s-.niteness condition is a technical condition that is satis.ed by all measures we \nwill consider in this paper and requires that the space can be covered by a countable number of pieces \nof .nite measure. Null sets A measurable set X is \u00b5-null if \u00b5(X)=0; X is said to have \u00b5-measure zero. \nThe empty set is always null, the only C-null set is the empty set, and all countable subsets of R are \nL\u00adnull. A propositional function holds \u00b5-almost everywhere (\u00b5-a.e.) if the set of elements for which \nthe proposition does not hold is \u00b5-null. For instance, two functions of type R . R are equal L\u00adalmost \neverywhere if they differ at only a countable number of points. A measure space (A, M,\u00b5) is complete \nif all subsets of any \u00b5-null set are M-measurable. Completion is an operation that takes any measure \nspace (A, M,\u00b5) and produces an equivalent complete measure space (A, M',\u00b5') such that \u00b5'(X)= \u00b5(X) for \nX .M. Null sets are ubiquitous in measure theory, so it will be handy to work in spaces that support \nnull sets as much as possible. Thus, completion makes measure spaces nicer to work with. The n\u00addimensional \nLebesgue measure Ln is the n-fold completed product of L. For measures \u00b5 and . on a measurable space, \n. is absolutely continuous with respect to \u00b5 if each \u00b5-null set is also .-null. Integration A fundamental \noperation involving measures is the abstract integral, a generalization of the Riemann integral that \navoids some of its de.ciencies. The abstract integral of a measur\u00ad f able function f : A . R w.r.t. a \nmeasure \u00b5 on A is written fd\u00b5. The integral is always de.ned for nonnegative f. The integral for arbitrary \nf is de.ned in terms of the positive and negative parts of f and may not exist; if it does we say f is \n\u00b5-integrable. We write ff X fd\u00b5 as shorthand for .x . 1X (x) \u00b7 f(x) d\u00b5, which restricts the integral \nto the subset X. We write 1X for the indicator function on X. Expectation refers to abstract integration \nw.r.t. a distribution. f The abstract integral satis.es \u00b5(X)= 1X d\u00b5 for all measur\u00adable X. In terms of \nprobability, it says that the probability of X is the expectation of 1X . Another consequence is that \nnull sets can\u00adnot affect integration: two functions that are equal \u00b5-a.e. give the same results under \nintegration w.r.t. \u00b5. Abstract integration w.r.t. C and L is ordinary (possibly in.nite) summation and \nthe ordinary Lebesgue integral, respectively. The Lebesgue integral agrees with the Riemann integral \non Riemann-integrable functions. Measurability Ordinarily, to conclude that a distribution such as var \nx ~ e in return (fx) is well-formed, we are obligated to verify that f is a measurable function. However, \nnon-measurable sets and functions are actually quite pathological and constructing them requires the \nAxiom of Choice [25]. None of the constructs in our language are as powerful as the Axiom of Choice (though \nwe do not have a formal proof of this), thus all constructible expressions represent measurable functions. \nThis discharges the obligation, and we do not make any further mention of checking for measurability. \nStocked spaces For most applications, we often have a standard idea of how spaces are measured. We now \nformalize this practice. A space A is a stocked space if it comes equipped with a complete measure space \n(A, MA,\u00b5A), which is the stock measure space of A. We call MA the stock s-algebra of A and \u00b5A the stock \nmeasure of A. The abstract integral w.r.t. \u00b5A is the stock integral of A.We de.ne stock measure spaces \nfor the spaces B = {true, false}, Z, R, and product spaces between stocked spaces as follows: (MB ,\u00b5B)=(P(B), \nC) (MZ ,\u00b5Z)=(P(Z), C) )=(the L-measurable sets, L) (MR,\u00b5R(MA\u00d7B,\u00b5A\u00d7B)= completion(MA .MB,\u00b5A . \u00b5B). This \nde.nition matches what is used in practice: e.g. C becomes the measure for countable spaces, and Ln becomes \nthe measure for Rn. For the rest of the paper, we assume spaces are stocked, unless G f e : t T-RAND \nT-RET G f random : dist R G f return e : dist t G f e1 : dist t1 G,x : t1 f e2 : dist t2 T-BIND G f var \nx ~ e1 in e2 : dist t2 Figure 3. Standard monadic typing rules for distributions. explicitly noted otherwise. \nWe say that a distribution on A is ACif it is AC with respect to \u00b5A. Densities A function f is a PDF \nof a distribution P on A if f P(X)= X fd\u00b5A for all measurable X. Expectation can be written using the \nPDF: gdP = .x . g(x) \u00b7 f(x) d\u00b5A. A joint PDFisthe PDFofa joint distribution, which is simply a distribution \non a product space. We later use the fact that the joint PDF f of a model such as x1 ~ P1,x2 ~ P2( \u00b7 \n; x1) can be written as the product of the individual (parameterized) PDFs: f(x1,x2)= f1(x1) \u00b7 f2(x2; \nx1). 3.4 Type system and semantics for distributions We now discuss the type system and semantics for \nsyntactic cate\u00adgories besides programs. The type system for expressions is ordi\u00adnary. We assume an external \nmechanism for enforcing the precon\u00additions necessary to ensure totality of functions, such as an auto\u00admated \ntheorem prover or the programmer themself. For instance, log must be applied to only positive real numbers. \nDistributions obey standard monadic typing rules (Figure 3). The random vari\u00adables introduced by bind \nare really just normal variables and are typed as such; calling them random variables is a reminder about \nthe role they play. The typing rules ensure that random variables are never used outside a probabilistic \ncontext. We give our language a semantics based in classical mathemat\u00adics with total functions. Base \ntypes have the usual meaning. The denotation of dist t is the set of distributions possible on t: T [[dist \nt ]] = {P | (T [[t ]], Mt , P) is a probability space}. We overload stock measure space notation for \ntypes; thus, Mt and \u00b5t are shorthand for MT [[t]] and \u00b5T [[t]].Let E[[e]]. be the denota\u00adtion of a distribution \ne under the environment ., also overloaded for expressions e. Expressions have the semantics of their \ncorrespond\u00ading forms from classical mathematics. As stated before, random is the Uniform(0,1) distribution \nE[[random]]. = .X . L(X n [0, 1]), which says that the probability of an event X is its interval size \non [0,1]. Return is the point mass distribution E[[return e]]. = .X . 1X (E[[e]].), which gives an event \nX probability 1 as long as it includes the outcome e. Bind expresses the Law of Total Probability, ' \nE[[var x ~ e1 in e2]]. = .Y . .x' . f(x)(Y ) dP, where f(x')= E[[e2]](.{x . x'}) and P = E[[e1]].. The \nfamily of distributions e2 is parameterized by the variable x, in essence. The probability of an event \nY is the average opinion (the P\u00adexpectation) of what each member of the family thinks is the probability \nof Y . The integral exists because it is the expectation of a bounded function.  4. Type system and \nsemantics for programs The program pdf e is well-formed if the distribution e permits a PDF. The following \ntheorem gives us a suf.cient condition. Theorem 4.1 (Radon-Nikodym). For any two s-.nite measures \u00b5 and \n. on the same measurable space such that . is absolutely con\u00ad f tinuous w.r.t. \u00b5, there is a function \nf such that .(X)= X fd\u00b5. We call f a Radon-Nikodym derivative of . with respect to \u00b5, denoted d./d\u00b5; \npdf corresponds to the Radon-Nikodym operator. The condition is also necessary: given a satisfying f, \n. is (trivially) AC. All stock measures we de.ne and all distributions are s-.nite, so for our purposes \nabsolute continuity is equivalent to possessing a PDF. Though not necessarily unique, Radon-Nikodym derivatives \nare equal \u00b5-almost everywhere. When \u00b5 is the counting measure, the Radon-Nikodym derivative is a PMF. \nFor hybrid spaces, it is a function which must be summed along one dimension and inte\u00adgrated along the \nother to obtain quantities interpretable as proba\u00adbilities. Radon-Nikodym derivatives unify PMFs, PDFs \nand hybrids of the two. For this reason, we refer to all of these as PDFs. We use PMF when we want to \nemphasize its discrete nature. De.ning a type system for absolute continuity in terms of the straightforward \ninduction on distribution terms proves unwieldy. Suppose we want to check if the distribution in Equation \n2 is AC; we must verify that the probability of any L2-null set Z is zero. A straightforward induction \nleads us to trying to show () var y ~ random in P({x | (Z)=0 })=0 return (x, y) where P is the Uniform(0,1) \ndistribution, and we have abused no\u00adtation slightly by mingling object language syntax with ordinary \nmathematics. This states that the body of the outermost bind assigns Z probability zero, P-almost always. \nIt is unclear how to proceed from here or how to remove concepts like null sets from the mech\u00adanization. \nWe take an alternate approach based on the insight that we can reason about a distribution by examining \nhow it transforms other distributions. Our approach, and outline of the following sub\u00adsections, is as \nfollows: We introduce the new notion of a non-nullifying function and prove a transformation theorem \nstating that when a random variable is transformed, the output distribution is AC if the input distribution \nis AC and the transformation is non-nullifying. We also prove some results about non-nullifying functions. \n We de.ne the random variables transform of any distribution written in our language and show that for \na large class of distributions the transformation theorem is applicable.  We present a type system which \nde.nes absolute continuity of a distribution in terms of whether its RV transform is non\u00adnullifying. \nAs implementors, we have found it easier to come up with the rules for non-nullifying functions.  Measure-theoretic \nconcepts like s-algebras, null sets, and the Lebesgue measure, while present in the metatheory, do not \nneed to be operationalized for implementing the type checker. Also, due to the measure-theoretic foundation, \nwe correctly handle cases that are not typically explained, such as PDFs on hybrid spaces. We conclude \nthe section with the semantics of programs. 4.1 Absolute continuity and non-nullifying functions A function \nh : A . B is non-nullifying if the h-preimage of each \u00b5B -null set is \u00b5A-null; preimages of null sets \nare always null, and forward images of non-null sets are always non-null. A function that fails to be \nnon-nullifying is called nullifying. The next theorem establishes the link between absolute continuity \nand non-nullity. Theorem 4.2 (Transformation). For a function h : A . B and an AC distribution P on \nA, the distribution Q(Y )= P(h-1[Y ]) (3) on B is AC if h is non-nullifying. Proof. Let Y be a \u00b5B -null \nset. By the non-nullity of h,the set h-1 [Y ] is \u00b5A-null. By the absolute continuity of P,wehave P(h-1[Y \n]) = 0, implying that Y is also Q-null. This style of de.ning Q may seem odd, but it actually underlies \nthe use of random variables as a modeling language. For instance, the model x ~ P,y = h(x) exhibits the \nrelationship Q(Y )= P(h-1[Y ]),where Q is the distribution of y. In general, the reverse direction does \nnot hold; h can be nullifying even if Q is AC.This happens when h has nullifying behavior only in regions \nof the space where P is assigning zero probability. This will be a source of incompleteness in the type \nsystem. Lemma 4.3 (Discrete domain). A function h : A . R is nullifying if A is non-empty and countable. \nProof. Let x be an element of A.Theset {x} has positive counting measure while its h-image, which is \na singleton set, is L-null. This implies Rof Z is nullifying, meaning that when we view an integer random \nvariable as a real random variable, it loses its ability to have a PDF. This is desirable behavior; different \nspaces have different ideas of what it means to be a PDF. We would not want to mark an integer random \nvariable as AC and later attempt to integrate its PMF in a context expecting a real random variable. \nLemma 4.4 (Discrete codomain). A function h : A . B is non\u00adnullifying if B is countable. Proof. The h-preimage \nof the empty set (the only C-null set) is the empty set, which is always null. This reasoning corroborates \nthe fact that distributions on count\u00adable spaces always have a PMF. Lemma 4.5 (Interval). A function \nh : R . R is nullifying if it is constant on any interval. Proof. Let h be constant on (a, b); (a, b) \nis not L-null, but its h\u00adimage (a singleton set) is L-null. One way to visualize how this leads to a \nnon-AC distribution is to observe that the transformation h takes all the probability mass along (a, \nb) and non-smoothly concentrates it onto a single point in the target space. Lemma 4.6 (Inverse). An \ninvertible function h : R . R is non\u00adnullifying if its inverse h-1 is an absolutely continuous function. \nProof. We have discussed absolute continuity of measures; the ab\u00adsolute continuity of functions is a \nrelated idea. It is a stronger no\u00adtion than continuity and uniform continuity. Absolutely continuous \nfunctions are well behaved in many ways; in particular, the im\u00adages of null sets are also null sets. \nCoupled with the fact that an h-preimage is an h-1-image, this proves the claim. More details on absolutely \ncontinuous functions can be found in [17]. This result shows that log, exp, and non-constant linear func\u00adtions \nare non-nullifying. We believe the idea can be extended with\u00adout much dif.culty to show that functions \nwith a countable number of invertible pieces, such as the trigonometric functions and non\u00adconstant polynomials, \nare also non-nullifying. Lemma 4.7 (Piecewise). For functions c : A . B and f, g, h : A . B,where h(x)= \nif c(x) then f(x) else g(x), h is non\u00adnullifying if f and g are non-nullifying. Proof. Let Y be a \u00b5B \n-null set. The set h-1[Y ] is a subset of f-1[Y ] . g -1[Y ] and is thus \u00b5A-null, by non-nullity of f \nand g, and the countable additivity and completeness of \u00b5A. Lemma 4.8 (Composition). The set of non-nullifying \nfunctions is closed under function composition. Proof. Let f : A . B and g : B . C be non-nullifying \nfunctions and let h = g . f .The h-preimage of a \u00b5C -null set Z is given by h-1-1 [Z]= f-1[g [Z]], and \nis thus \u00b5A-null, by the non-nullity of f and g. Lemma 4.9 (Projection). The function h(x, y)= x of type \nA \u00d7 B . A is non-nullifying. Proof. Let X be a \u00b5A-null set. Its h-preimage is X \u00d7 B.Bythe properties \nof product measure, we have that \u00b5A\u00d7B(X \u00d7 B)= \u00b5A(X) \u00b7 \u00b5B (B)=0 \u00b7 \u00b5B (B)=0. Even if \u00b5B (B)= 8, the measure-theoretic \nde.nition of multipli\u00adcation on extended nonnegative reals de.nes 0 \u00b78 =0. Along these lines, we can \nshow that returning a permutation of a subset of tuple components is also a non-nullifying function. \nThe last two results permit us to ignore uninvolved arguments when reasoning about the non-nullity of \nthe body of a function.  4.2 Distributions and RV transforms A large class of distributions in our language \ncan be understood by Equation 3. From the syntax we know that a distribution e must take the form of \nzero or more nested binds terminating in a body that is either random or return e. We focus on the latter, \nnon-trivial case. The expression e represents a transformation of the random variables xi introduced \nby the binds. The function .(x1, ..., xn) . e is the random variables transform (RV transform) of the \ndistribu\u00adtion e, where we use tuple pattern matching shorthand to name the components of a tuple argument. \nThe correspondence between dis\u00adtributions in our language and Theorem 4.2 is as follows: let Q be the \ndenotation of e,let h be the RV transform of e, and let P be the joint distribution of the random variables \nintroduced on the spine of e. The class of distributions for which the theorem is applicable is given \nby the set of distributions for which each ei is parametrically AC w.r.t. the random variables preceding \nit, where ei is the distri\u00adbution corresponding to xi. In other words, the distribution for ei must be \nAC while treating free occurrences of x1, ..., xi-1 as .xed, unknown constants. This ensures that the \njoint distribution is also AC; the joint PDF can be written as the product of the individual pa\u00adrameterized \nPDFs. This is a commonly used (implicit) assumption in practice. For example, the distribution var u \n~ random in var z ~ .ip u in return (u + (z)) has the RV transform .(u, z) . u + (z), which has type \nR \u00d7B . R and is transforming the joint distribution of random and e2 := .ip u.The variable u appears \nfree in e2, making e2 parametric in u; the restriction requires that e2 is AC for all possible values \nof u, which is the case here. Two extensionally equivalent dis\u00adtributions may have different RV transforms \nand spines because of intensionally different representations. To show that this choice of P, Q,and h \nsatis.es Equation 3, we appeal to the semantics of distributions (de.ned in Section 3.4). Consider the \ngeneral case .; . f e NN AC-RAND AC-RET .; . f random AC .; . f return e AC .; \u00d8f e1 AC . f e1 : dist \nt .,x : t ~ e1;.,x f e2 AC AC-BIND .; . f var x ~ e1 in e2 AC Figure 4. The absolute continuity judgment, \n.; . f e AC. e := var xi ~ ei in return e, where we have used the bar as short\u00adhand for nested binds. \nThe denotation Q of e under an environment . is given by Q(Y )= dPi .x ' i. 1Y (E[[e]].{xi . x ' }) i \n where Pi is the denotation of ei (extending . as necessary) and we have again used the bar notation, \nto denote iterated expecta\u00adtion and the repeated extension of the environment . with variable mappings. \nWe can now rewrite the expectations to use their cor\u00adresponding PDFs fi and then replace the iterated \nintegrals with a single product integral using their joint PDF f: ''' ' Q(Y )= d\u00b5ti .x ' i .fi(xi; x1, \n..., x i-1) \u00b7 1Y (E[[e]].{xi . xi}) = d\u00b5t .x . f(x) \u00b7 1Y (h(x)) = dP .x . 1Y (h(x)) -1 = P({x | h(x) \n. Y })= P(h[Y ]) where x =(x1' , ..., x n' ), h(x)= E[[e]].{xi . xi}, ti is the type of each xi,and \nt is their product. We have also used the fact that the expectation of the indicator function on a set \nis the probability of that set (the set here is {x | h(x) . Y }, not Y ). Replacing an iterated integral \nwith a product integral is not always legal but is possible here because the integral is of a nonnegative \nfunction w.r.t. independent measures (see Tonelli s theorem, [17]). 4.3 Type system for programs All \njudgments are de.ned modulo a-conversion. A program pdf e is well-formed if e is an AC distribution (\u00d8f \ne : dist t holds for some t and \u00d8; \u00d8f e AC holds). If the judgment .; . f e AC (Figure 4) holds then \ne is an AC distribution under the probability context . and the active variable context .,where . is \ngiven by the grammar . ::= \u00d8| .,x. Variables in . are currently active and should be understood in a \nprobabilistic sense, while those not in . are inactive and should be treated as .xed parameters. The \ncontexts obey the following invariant: . is always the pre.x of ., i.e. the variables in . correspond \ndirectly to the n most recent entries added to .,where n is the length of ..Rule AC-RAND asserts that \nthe Uniform(0,1) distribution is AC. The main action of rules AC-BIND and AC-RETURN is to prepare a call \nto the non-nullity judgment. For Theorem 4.2 to be applicable, a distribution along the spine must be \nparametrically AC w.r.t. the random variables preceding it; thus, in AC-BIND we check that e1 is AC without \nmarking any current random variables as active. We reach the body of the RV transform in AC-RETURN. Roughly \nspeaking, . (pointing into .)and e correspond to P and h in Theorem 4.2. Next is the non-nullity judgment \n(Figure 5). If .; . f e NN holds, then e represents the body of a non-nullifying function under . and \n.. The variables in . are the arguments to the RV transform. Throughout this discussion, we implicitly \nuse the composition and projection lemmas (Lemmas 4.8 and 4.9) to ignore uninvolved arguments during \nanalysis. For example, in rule NN-VAR, we could x . .. f e : tt countable NN-VAR NN-COUNT .; . f x NN \n.; . f e NN .; . f e NN op .{neg, inv, log, exp, sin, cos, tan} NN-OP .; . f op e NN .; . f e1 NN .; \n. f e2 NN NN-IF .; . f if e then e1 else e2 NN .; . f e NN op .{fst, snd} NN-PROJ .; . f op e NN .; . \nf e1 . e2 .; . f e1 NN .; . f e2 NN NN-PAIR .; . f (e1,e2) NN xi . . x1, ..., xn are distinct NN-VARS \n.; . f (x1, ..., xn) NN .; . f (e1,e2) NN NN-PLUS .; . f e1 + e2 NN FV (e2) n .= \u00d8 .; . f e1 NN NN-LINEAR \n.; . f e1 + e2 NN .; . f (e1,e2) NN l = 0 .;. f e NN NN-MULT NN-SCALE .; . f e1 * e2 NN .; . f l * e \nNN Figure 5. The non-nullity judgment, .; . f e NN. be analyzing a function with multiple inputs, but \nwe can drop all of them but x, leaving us to analyze the function .x . x,which is trivially non-nullifying. \nUnder the hood, what we are actually doing is representing the original transform as the composition \nof a function that selects a single components of a tuple with the identity function .x . x. The composition \nlemma is also the justi.cation for being able to recurse into subexpressions. Rule NN-COUNT is merely \nan application of Lemma 4.4; the types bool, Z and products thereof de.ne the countable types. Note that \nthis covers the cases of =, <,integer neg, + and *, and Boolean and integer literals. Rules NN-OP, NN-IF \nand NN-PROJ are direct translations of Lemmas 4.6, 4.7 and 4.9. The injection from integers into the \nreals is nullifying (Lemma 4.3), so there is no rule for Rof Z.Rule NN-PAIR expresses the idea that the \njoint distribution of independent AC distributions is AC.If .; . f e1 . e2 holds then e1 and e2 represent \nindependent distributions under . and .. Its de.nition is . n Anc(.,FV (e1)) n Anc(.,FV (e2)) = \u00d8 INDEP \n.; . f e1 . e2 where Anc(.,X)= x.X anc(.,x). It states that e1 and e2 must not have any ancestors in \ncommon. The function anc(.,x) computes the ancestors of a random variable x. A random variable y is the \nparent of a random variable x if y appears free in the distribution that x is bound to. Rule NN-VARS \ncorresponds to the corollary of Lemma 4.9 that states that you can drop and permute tuple components. \nThe requirement that the variables are distinct is important; the distribution var u ~ random in return \n(u, u) is not AC, as we saw in Section 2. We have multiple rules for addition because they each capture \na different usage of plus. Rule NN-PLUS states that if the formation of the pair (e1,e2) is non-nullifying, \nthen e1 + e2 is also non-nullifying because it is the composition of tuple formation with (+) : R \u00d7 R \n. R, where the latter is non\u00adnullifying by corollary to Lemma 4.6. Rule NN-LINEAR represents the idea \nof composing with the non-nullifying function .x . x + c, where c is a constant w.r.t. the arguments \nof the RV transform. There is an analogous rule for when the constant appears as the left operand. Rules \nNN-MULT and NN-SCALE are analogous. Note that NN-SCALEisslightlyweakerthanitscounterpart NN-LINEAR,only \nbecause it needs to prove that the scaling coef.cient is nonzero. Discussion We believe our type system \nis sound; the only remain\u00ading case to rigorously prove is NN-PAIR. The soundness of the re\u00adduction to \nnon-nullity is given by Theorem 4.2, and the soundness of the other cases in the non-nullity judgment \nare covered by the lemmas in Section 4.1. Stating the needed lemma for NN-PAIR es\u00adsentially requires \nformalizing the idea that the conditional distribu\u00adtion of the second component conditioned on the .rst \ncomponent should be AC. In non-nullity terms, the second component should still have a degree of freedom \neven after .xing the .rst. Rigorously stating this involves conditional probability, putting it outside \nthe scope of the current work. There are few sources of incompleteness in our type system. For instance, \nNN-PAIR conservatively requires e1 and e2 to be independent. The distribution var x ~ random in var y \n~ random in return (exp x, x + y) is AC despite the fact that the tuple components are not indepen\u00addent: \neven if we know the value of exp x, the residual stochas\u00adticity in the quantity x + y is still AC. The \njoint PDF is given by multiplying the marginal PDF of the .rst component by the condi\u00adtional PDF of the \nsecond component conditioned on the .rst. This is a similar issue as the parametric AC requirement on \nspine distri\u00adbutions. Formulating this generalization of NN-PAIR is interesting futurework.Likewise, \nNN-IFconservativelyrequiresbothbranches of an if-expression to be non-nullifying. The distribution var \nx ~ std normal in return(if x< 0 then min x 0 else max x 0) is not accepted as AC because both branches \n(.x . min x 0 and .x . max x 0) are nullifying, even though the distribution is ex\u00adtensionally equivalent \nto var x ~ std normal in return x,which is AC.Wede.ne min and max in the usual way, using if. Finally, \nnon-nullity is suf.cient but not necessary for absolute continuity to hold. For instance, the RV transform \nof var x ~ random in return (if x< 100 then x else 100) is .x . if x< 100 then x else 100, which is \nnullifying due to the constant portion, thus our type system does not accept this distribu\u00adtion as AC.However, \nx only takes values on (0, 1), so the second branch is never entered, and thus the distribution is extensionally \nequivalent to the AC distribution var x ~ random in return x. 4.4 Semantics of programs The denotation \nof a program pdf e is that it is a member of the set of Radon-Nikodym derivatives of the distribution \ne: [[pdf e]] .{f |.X, P(X)= fd\u00b5t } X where P = E[[e]]{} is the denotation of e under the empty envi\u00adronment \nand e has type dist t . The procedure discussed in the next section calculates a member of this set. \n 5. Calculating density functions The previous sections have de.ned a language in which it is pos\u00adsible \nto express PDFs. Our goal now is to mechanically obtain a usable form of the PDF for a given distribution. \nBut what consti\u00adtutes a usable form? We are motivated by applications of the PDF and the need to interface \nwith existing software. For instance, we may want to use numerical optimization software to perform MLE, \nwhere the PDF appears in the objective function; we may also want to symbolically derive gradient information \nto improve the search. Or, we may want to use the PDF to calculate an expectation using a numerical integrator. \nRoughly speaking, we call a term usable if we can map it onto the capabilities of existing software in \naccor\u00addance with common practice. For example, the term .x . x +5 is Target types Target terms Typing \ns ::= t | s1 . s2 d ::= e | .x : t . d | d1 d2 | G f d : t . R G f f d : R T-INT d Semantics E[[ d ]]. \n= E[[d]].d\u00b5t Figure 6. The target language. usable; in practice, real addition is mapped to .oating \npoint addi\u00ad f 5tion. Likewise, 0 x 2 dx is usable; the integral is Riemannian and in a form accepted \nby computer algebra systems (CAS) and numer\u00ad f ical integrators. On the other hand, terms like gdP and \ndP/dL make use of measure-theoretic operations such as abstract integra\u00adtion and the Radon-Nikodym derivative. \nCurrent software do not handle these operations (though, progress on mechanizing measure theory has been \nmade [15]). Thus, the basic plan is to eliminate measure-theoretic concepts during PDF calculation. This \nmeans the constructs random, return, bind, and pdf should not appear in a PDF term because they involve \nmeasure theory, metatheoretically. It will take some ingenuity to remove the Radon-Nikodym derivative \n(pdf). It has been shown that the Radon-Nikodym deriva\u00adtive is a non-computable operator: given a distribution, \nthere is no general computable procedure for computing its PDF [11]. The dis\u00adcrete case at least enjoys \nthe fact that the PMF has a straightforward de.nition in terms of its distribution; if P is an executable \nimple\u00admentation of a discrete distribution, an executable implementation of its PMF dP/dC is given by \n.x . P({x}). In general, however, we will need to tackle the calculation of PDFs with a collection of \ntech\u00adniques. Our basic approach is as follows. First, we de.ne a target language that de.nes what constitutes \na usable form. Second, we provide a procedure that converts many distributions accepted as AC by our \ntype system into PDFs expressed in the target language. Some RV transforms are mathematically inconvenient, \nso we will not be able to calculate certain PDFs from scratch; in particular, dependence between random \nvariables makes the general case dif\u00ad.cult. However, the design permits modularly adding knowledge about \nindividual distributions with known PDFs, enabling the pro\u00adcedure to calculate PDFs for programs that \nuse these distributions as subcomponents. This allows us to handle many useful cases. 5.1 The target \nlanguage The target language extends expressions with .-abstraction, appli\u00adcation, and the stock integral \n(Figure 6). We treat functions in a standard way. Notationally, we skip specifying t in abstractions \nwhen the choice of t is clear. Computing closed-form solutions for integrals is not always feasible or \npossible, so integrals can\u00adnot be completely eliminated from the target language. The inte\u00adgral is well-formed \nif its integrand is real-valued and summable (a f function f is \u00b5-summable if fd\u00b5 is .nite). We require \nusers of the target language (compiler writers) to manually ensure summa\u00adbility; this is reasonable for \na back-end language. We have veri\u00ad.ed summability for each use of stock integration in the compilers \npresented in this section. Although a measure-theoretic concept, stock integration is close enough to \nthe notion of integration used by numerical and symbolic solvers to be useful as a compilation target. \nRecall, stock integration over C and L is ordinary summa\u00adtion and Lebesgue integration, respectively. \nFor most applications, Lebesgue integration will coincide with Riemann integration. random $ d . .x : \nR . (0 <x< 1)* dx return e $ d . de . d '' e2 $ d . d ' e1 $ .x . d ' var x ~ e1 in e2 $ d . d'' Figure \n7. The probability compiler, e $ d . d ' . P-RAND .; . f random r .x : R . (0 <x< 1). f e1 : dist t .,x \n: t ~ e1;.,x f e2 r d P-BIND .; . f var x ~ e1 in e2 r d .; . f e . d P-RET .; . f return e r d Figure \n8. The distribution-to-PDF converter, .; . f e r d.  5.2 The probability compiler We need to calculate \nprobabilities as a subroutine of PDF calcula\u00adtion. We achieve this by translating distributions into \nKozen-style terms [14]. The probability compiler e $ d . d ' performs this translation (Figure 7). It \ntakes a distribution e of type dist t and a function d from t to [0, 1] and returns the expectation of \nd w.r.t. e. When d is the indicator function on a set X, d ' is the e-probability of X. For instance, \nsuppose we want to know the probability that a sample from .ip (3/4) is true. We invoke the probability \ncompiler with e := .ip (3/4) and d := .z : bool . (z), producing .x : R . (0 <x< 1)* (.u . (.z . (z))(u< \n3/4)) x f 1 for d ' , which is equivalent to (x< 3/4) dx =3/4, as expected. 0 Likewise, to derive the \nprobability that a standard normal random variable stays within a standard deviation of its mean, we \nwould invoke the probability compiler with e := std normal and d := .x . (-1 <x< 1). Details on how this \ncomputes probabilities are given by Kozen and can also be understood by the expectation monad [20]. We \nalso need the judgment . f e $ d . d ' ,which invokes the probability compiler on the distribution corresponding \nto the RV transform body e in the context .. 5.3 The PDF calculation procedure We structure the PDF \ncalculation procedure as we did the type sys\u00adtem: the judgment on distributions prepares a call to the \njudgment on RV transforms. The PDF of a well-formed program pdf e is givenbythe d satisfying \u00d8; \u00d8f e \nr d. The judgment .; . f e r d calculates the PDF d of the distribution e under . and . (Figure 8). Rule \nP-RAND gives the PDF of Uniform(0,1): the indi\u00adcator function on (0, 1).Rules P-RETand P-BINDbuildthecontexts \nand invoke the next compiler. The real work begins in the judgment .; . f e . d, which computes the PDF \nd corresponding to the RV transform body e under . and .. We present this judgment in two parts, one \neach for univariate and multivariate transforms. The multivariate transforms must deal with the issue \nof dependence be\u00adtween inputs or between outputs of the transform. Univariate transforms We use univariate \nfor RV transforms be\u00adtween spaces that are not product spaces. The correctness of rules P-LOG, P-EXP, \nP-LINEAR,and P-SCALE is given by the following lemma. .; . f ed P-LOG .; . f log e .x : R . d (exp x) \n* exp x .; . f ed P-EXP .; . f exp e .x : R . d (log x) * (1/x) FV (e2) n .= \u00d8 .; . f e1 d P-LINEAR .; \n. f e1 + e2 .x : R . d (x - e2) .; . f ed l> 0 P-SCALE .; . f l * e .x : R . d (x/l) * (1/l) .; . f ed \nP-NEG .; . f neg e .x : R . d (-x) .; . f ed P-INV .; . f inv e .x : R . d (1/x) * (1/(x * x)) Figure \n9. The transform-to-PDF converter, .; . f ed, univari\u00adate cases. Lemma 5.1. For absolutely continuous \ndistributions P and Q on R and a function h : R . R such that Q(Y )= P(h-1[Y ]),if h is strictly increasing, \ndifferentiable and invertible, then the function d g(y)= f(h-1(y)) \u00b7 h-1(y). dy is a PDF of Q,where f \nis the derivative of the CDF F of P. Proof. The derivative of a CDF is a PDF.The CDF G of Q is G(y)= \nQ((-8,y]) = P(h-1[(-8,y]]) = P((-8,h-1(y)]) = F (h-1(y)), where we have used the fact that the h-preimage \nof (-8,y] is (-8,h-1(y)] because h is strictly increasing and invertible. The claim follows from the \nfact that g is the derivative of G. The lemma is easily modi.ed for P-NEG and also P-INV;an extra minus \nsign appears because they consist of strictly decreas\u00ading components. It is possible to de.ne a version \nof P-SCALE for negative literals, as well as integer versions of P-NEG, P-LINEAR, and P-SCALE. With these \nrules (and P-VAR, discussed below) we can already compute some continuous PDFs. Consider the stan\u00addard \nexponential from Section 3.2; we derive its PDF with \u00d8; \u00d8f std exponential r d, which builds the contexts \n.:= u and .:= u : R ~ random and invokes the chain '''' '' .; . f-log u dd := .x . (0 <x < 1) '''' ' \n.; . f log ud d := .x . (0 < exp x< 1)* exp x .; . f ud '' d := .x . (0 < exp (-x) <1)* exp (-x). We \n\u00df-reduce for clarity. The chain ends with P-VAR, which gives the PDF of Uniform(0,1) for d '' ; then, \nP-LOG and P-NEG produce d ' and d. The latter is equivalent to .x . (0 <x)* exp (-x),which is easily \nseen to be the PDF of the standard exponential. Likewise, the PDF of uniform e1 e2 is correctly calculated \nto be d := .x . (0 < (x - e1)/(e2 - e1) < 1)* (1/(e2 - e1)), which is equivalent to .x . (e1 <x<e2)* \n(1/(e2 - e1)).Wedo not provide rules for sin, cos,and tan because we are unaware of any simple closed-form \nexpression for the corresponding PDFs. Multivariate transforms We use multivariate for RV transforms \nto or from a product space. The presence of multiple dimensions introduces the issue of dependence between \nthe inputs or between the outputs of the transform, making it dif.cult to provide rules that work in \nthe general case. As a result, some of the following rules introduce speci.c independence requirements. \n\u00d8f l : tt countable P-LIT .; . f l .x : t . (x = l) . f e : bool .; . f e $ .x : bool . (x) . d P-BOOL \n.; . f e .x : bool . if x then d else 1 - d {.; . f e1 . ei}i=2,3 {.; . f ei di}i=1,2,3 P-IF .; . f if \ne1 then e2 else e3 .x . d1 true * d2 x + d1 false * d3 x .= {x}{y1, ..., ym}J (.; .) . d f P-VAR .; \n. f x .x . .(y1, ..., ym) . d .= {x1, ..., xn}{y1, ..., ym}J (.; .) . d f P-VARS .; . f (x1, ..., xn) \n.(x1, ..., xn) . .(y1, ..., ym) . d .; . f ed f P-FST .; . f fst e .x . .y . d (x, y) .; . f e1 . e2 \n{.; . f ei di}i=1,2 P-PAIR .; . f (e1,e2) .(x1,x2) . d1 x1 * d2 x2 .; . f e1 . e2 {.; . f ei di}i=1,2 \nf P-PLUS .; . f e1 + e2 .x : R . .t : R . d1 x * d2 (t - x) Figure 10. The transform-to-PDF converter, \nmultivariate cases. .; \u00d8f e r d J (.; .) . d ' J-NIL J-CONS J (.; \u00d8) . 1 J (.,x : t ~ e;.,x) . dx * d \n' Figure 11. The joint PDF body constructor, J (.; .) . d. Rule P-LITstatesthatthe PMFofapointmassdistributionon \nl is simply the indicator function on {l}. The transforms corresponding to the rules in this section \ntend to be less obvious; the transform in question for P-LIT is the constant function on l, whose argument \nmay be a tuple. Rule P-BOOL calculates the PMF of a Boolean random variable, which is a simple expression \nof the probability that the random variable is true. We thus invoke the probability compiler in the current \ncontext to compute this probability d.This rule covers the cases for < and =. The ability to represent \nthe PMF of a Boolean random variable allows us to encode arbitrary probability queries. Rule P-IF computes \nthe PDF of a mixture, which is a weighted combination of the component PDFs, where the mixing probability \nis the probability the if-condition is true. For this to be valid, the if-condition must be independent \nof its branches, as required. For instance, the PDF of var x ~ random in var y ~ uniform 23 in return \n(if x< 1/2 then x else y). is not equivalent to .x . (1/2) *(0 <x< 1) +(1/2) *(2 <x< 3), as would be \ncalculated without the restriction (there should be no probability mass on [1/2, 1]). Rule P-VAR is a \nspecial case of P-VARS. The transform corre\u00adsponding to P-VARS is a function that returns a permutation \nof a subset of components of its tuple argument. We assume x1, ..., xn and y1, ..., ym are distinct, \nand we use to denote disjoint union. The resulting PDF is a marginal PDF.The marginal PDF of a joint \nf PDF f on A \u00d7 B is given by g(x)= .y . f(x, y) d\u00b5B ; g is a PDF on A whose density at x is given by \nadding up the contri\u00adbution of the joint PDF along the other dimension, B. The corre\u00adsponding process \nis one which generates tuples but then discards the second component, returning the .rst. We generalize \nto higher dimensions by integrating out random variables not appearing in the result tuple. When this \nset is empty (m =0), the integral re\u00adduces to d. The resulting PDF may be computationally inef.cient \ndue to a large number of nested integrals. More ef.cient schemes that take advantage of the graphical \nstructure of the probabilistic model, such as variable elimination, are possible [26]. The judg\u00adment \nJ (.; .) . d constructs the body of the joint PDF of the active random variables (Figure 11). Rule J-CONS \n.rst computes the PDF of e, parametric in all of the preceding random variables (thus, invoking the distribution-to-PDF \nconverter withno active ran\u00addom variables). It then constructs the product with the PDFsofthe remaining \nactive variables; the product of these parametric PDFsis the joint PDF. The terms d and d ' in J-CONS \nhave type t . R and R, respectively. The judgment returns an open term and relies on the fact that the \nfree variables will be bound appropriately by the invoking judgment. Rule P-FST is analogous to P-VARS;weask \nfor a PDF and compute the marginal PDF of the .rst component. We de.ne an analogous rule for snd.Rules \nP-PAIR and P-PLUS state the well known results that the joint PDF and the PDF of the sum of independent \nrandom variables is the product of and convolution of their individual PDFs, respectively. On the face \nof it, these rules handle mixture models and joint models, but where they really shine is on general \nhierarchical mod\u00adels. For example, the PDF of hier := var x ~ random in var y ~ uniform 0 x in return \ny is not immediately obvious. The process is generated by sampling avalue x uniformly from (0,1), and \nthen sampling uniformly from (0,x), discarding x. We calculate the PDF with \u00d8; \u00d8f hier r d, which builds \n.:= y : R ~ uniform 0 x, x : R ~ random and .:= y, x for .; . f yd.Rule P-VAR then produces .y . .x .((0 \n<(y-0)/(x-0) < 1)*1/(x-0))*(0<x< 1)*1 for d, where we have \u00df-reduced for clarity. The body of the inner \n.\u00adabstraction is generated by the joint PDF body constructor; the two non-trivial multiplicands are the \nparametric PDF of uniform 0 x and the PDF of random, respectively. With some manipulation we f 1 can \nshow d corresponds to f(y)= y 1/x dx = - log(y) for y . (0, 1) and zero otherwise. The rules do not perform \nalgebraic simpli.cations, but the bene.t of automation can still be felt clearly. Modularity Some RV \ntransforms are inconvenient to work with, preventing us from calculating certain PDFs. For example, we \ncan\u00adnot calculate the PDF of std normal from scratch because its speci\u00ad.cation uses cos, which we do \nnot handle. However, the design al\u00adlows us to modularly address cases like this, where we want to spe\u00adcially \nhandle the PDF for a speci.c distribution. We can add the rule .; . f std normal r f,where f:= .x.exp(-x*x/2)/sqrt(2*p) \nis the PDF of the standard normal. This new rule is used by the joint body constructor whenever std normal \nappears on the spine of a distribution, enabling the calculation of PDFs for hierarchical models using \nstd normal that were previously not compilable. For example, the PDF of normal \u00b5s can now be calculated \nas '' '''' .; . f s * x + \u00b5 dd := .x . fx ' ''' .; . f s * xd d := .x . f (x /s) * (1/s) .; . f xd '' \nd := .x . f ((x - \u00b5)/s) * (1/s) using the rules P-VAR, P-SCALE,and P-LINEAR,where .:= x and .:= x : R \n~ std normal. We can see d is equivalent to the classic formula for the normal PDF, f(x)= v1 exp(- 2s1 \n2 (x - \u00b5)2). s 2p Likewise, we can now handle distributions like the log-normal and mixture-of-Gaussians. \nTo support an in.nite discrete distribution with a known PDF, such as the Poisson distribution, we can \nadd a new primitive to the core calculus (poisson e) and handle it specially in the distribution-to-PDF \nconverter.  6. Related Work Our work builds on a long tradition of probabilistic functional lan\u00adguages, \nmost connected to the probability monad in some way. They work by incorporating distributional semantics \ninto a func\u00adtional language, so that one can express values which represent a distribution over possible \noutcomes. The distribution can either be manifest (available to the programmer) or implicit (existing \nonly in the metatheory). An early incarnation of the latter was given by Kozen in [14], in which he provides \nthe semantics for an imperative language endowed with a random number primitive supplying sam\u00adples from \nUniform(0,1). Values of type A in the object language are given semantics in functions of type (A . [0, \n1]) . [0, 1] in the metatheory. These functions represent distributions over A and sat\u00adisfy the expected \nlaws for measures. Kozen s work is far-reaching and will continue to inspire future languages: it can \naccommodate continuous and hybrid distributions; it handles unbounded itera\u00adtion (general recursion), \na traditionally thorny issue for probabilis\u00adtic languages; and it even provides a treatment of distributions \non function types. However, PDFs are not addressed at all. Though not explicitly cast as functional or \nmonadic, Kozen s approach forms the basis for Audebaud and Paulin-Mohring s monadic development for reasoning \nabout randomized algorithms in COQ [2]. Their focus is on veri.cation, and they de.ne the prob\u00adability \nmonad from .rst principles (modulo an axiomatization of arithmetic on the [0,1] interval), whereas we \nprovide it axiomati\u00adcally. We hope to inspire a cross-fertilization of ideas between the efforts as we \nbring our theory of PDFsinto COQ. While suitable for semantics and veri.cation, Kozen s represen\u00adtation \nis not ideal for direct use in computing certain operations. For instance, it is unclear how to sample \nor compute general expecta\u00adtions ef.ciently given a term of type (A . [0, 1]) . [0, 1].More recent works \nexplore alternate concrete embodiments of the prob\u00adability monad; Ramsey and Pfeffer discuss some of \nthe possibili\u00adties [20]. A popular choice is to represent distributions as weighted lists or trees. This \nhas the drawback that only distributions with .nitely many outcomes are expressible (ruling out essentially \nall commonly used continuous distributions), and PMFs are the only supported form of PDFs. On the other \nhand, distributions can occur on arbitrary types, expectation and computing the PMF is straight\u00adforward, \nand the approach works well as an embedded domain\u00adspeci.c language (PFP [7], HANSEI [13], probability \nmonads in Haskell [20]). Dedicated languages like IBAL [19] or Church [9] offer more scope for program \nanalysis, which is crucial for escap\u00ading the limitations of an embedded approach and mitigates some of \nthe fundamental drawbacks of the representation. Ultimately, how\u00adever, these languages do not support \ncontinuous or hybrid distribu\u00adtions (nor their PDFs) in a general sense. Sampling functions are a fun \nalternative representation. They are used by .O [18] to support continuous and hybrid distributions in \na true sense and also allow distributions on arbitrary types. Distributions are represented by sampling \nfunctions that return a sample from the distribution when requested. Sampling and sampling-based routines \nare the only sup\u00adported operations, thus PDFs are not accommodated. Another recent work also rigorously \nsupports continuous and hybrid distributions by providing a measure transformer semantics for a core \nfunctional calculus [4]. The work does not provide PDFs but is novel for its ability to support conditional \nprobability in the presence of zero probability events in continuous spaces, a feature necessary in many \nmachine learning applications. Their formaliza\u00adtion is similar to ours, as both are based in standard \nmeasure theory. They have independently recognized the importance of analyzing distributions by their \ntransformations, doing so in the context of conditional probability, whereas we have developed the idea \nfor PDFs. This hints that reasoning via transforms may be a technique that is more broadly applicable \nto other program analyses for prob\u00adabilistic languages. The Hierarchical Bayes Compiler (HBC) is a toolkit \nfor im\u00adplementing hierarchical Bayesian models [5]. Its speci.cation lan\u00adguage represents a different \npoint in the design space. Essentially, it removes return while adding a set of standard distributions \n(with PDFs) to the core calculus. This guarantees that all constructible models are AC. Many powerful \nmodels used in machine learning are expressible in HBC. However, something as basic as adding two random \nvariables is not. Furthermore, if a distribution outside of the provided set is required, it must be \nadded to the core. This is the fundamental tension surrounding return: with it, the core is mini\u00admal, \nexpressivity is high, and PDFs are non-trivial; without it, PDFs are easily supported, but the core becomes \nlarge, and expressivity is crippled. HBC is not formally de.ned. An entirely different tradition incorporates \nprobabilistic se\u00admantics into logic programming languages (Markov Logic [21], BLOG [16], BLP [12], PRISM \n[22]). These languages are well suited for probabilistic knowledge engineering and statistical relational \nlearning. In Markov Logic, for instance, programmers associate higher weights with logical clauses that \nare more strongly believed to hold. The semantics of a set of clauses is given by undirected graphical \nmodels, with the weights determining the potential func\u00adtions (e.g. by Boltzmann weighting). Certain \ncontinuous distri\u00adbutions can be supported by manipulating the potential function calculation. Supporting \nPDFs in this context should not be prob\u00adlematic; the potential functions (essentially, unnormalized PDFs) \nalways exist, by design. However, like HBC, it appears these lan\u00adguages are not quite as expressive as \nis possible in a probabilistic functional language. The AutoBayes system [10] shares a key feature with \nour lan\u00adguage in that PDFs are manifest in the object language. AutoBayes automates the derivation of \nmaximum likelihood and Bayesian es\u00adtimators for a signi.cant class of statistical models, with a focus \non code generation, and can express continuous distributions and PDFs. However, despite their focus on \ncorrectness-by-construction, the language is not formally de.ned. Furthermore, it is unclear how general \nthe language actually is, i.e. how custom the models can be. Our work could serve as a formal basis for \ntheir system.  7. Conclusion We have presented a formal language capable of expressing dis\u00adcrete,continuousandhybriddistributionsandtheir \nPDFs.Ournovel contributions include a type system for absolutely continuous dis\u00adtributions and a modular \nPDF calculation procedure. The type sys\u00adtem uses the new ideas of RV transforms and non-nullifying func\u00adtions. \nThere are several interesting avenues for future work. The .rst is to address PDFs in the context of \nconditional probability, perhaps by incorporating our formalization of PDFs with the ideas presented \nin [4]. Secondly, to provide a complete account of con\u00adtinuous probability, one must support expectation. \nGenerically sup\u00adporting expectation requires a treatment of integrability or summa\u00adbility; reasoning \nvia the RV transform may be a productive route. Finally, combining this work with a formal language for \noptimiza\u00adtion such as [1] would create a true formal language for statistics, which would be able to \nexpress statistical problems in the object language itself. Current languages express probability; any \nnotion of statistics is outside the language.  Acknowledgments We thank Prof. Christopher Heil for valuable \ninput on the idea of non-nullifying functions. We also thank the anonymous reviewers, whose thoughtful \nsuggestions have greatly improved the paper.  References [1] A. Agarwal, S. Bhat, A. Gray, and I. E. \nGrossmann. Automating Math\u00adematical Program Transformations. In Practical Aspects of Declara\u00adtive Languages, \n2010. [2] P. Audebaud and C. Paulin-Mohring. Proofs of Randomized Algo\u00adrithms in Coq. In Mathematics \nof Program Construction, pages 49 68. Springer, 2006. [3] C. Bishop. Pattern Recognition and Machine \nLearning. Springer, 2006. [4] J. Borgstram, A. D. Gordon, M. Greenberg, J. Margetson, and J. V. Gael. \nMeasure Transformer Semantics for Bayesian Machine Learn\u00ading. In European Symposium on Programming, pages \n77 96, 2011. [5] H. Daume III. \u00b4HBC: Hierarchical Bayes Compiler, 2007. URL http://hal3.name/HBC. [6] \nL. Devroye. Non-Uniform Random Variate Generation, 1986. [7] M. Erwig and S. Kollmansberger. Functional \nPearls: Probabilistic Functional Programming in Haskell. Journal of Functional Program\u00adming, 16(01):21 \n34, 2005. [8] M. Giry. A Categorical Approach to Probability Theory. Categorical Aspects of Topology \nand Analysis, 915:68 85, 1981. [9] N. Goodman, V. Mansinghka, D. Roy, K. Bonawitz, and J. Tenen\u00adbaum. \nChurch: A Language for Generative Models. In Uncertainty in Arti.cial Intelligence, 2008. [10] A. G. \nGray, B. Fischer, J. Schumann, and W. Buntine. Automatic Derivation of Statistical Algorithms: The EM \nFamily and Beyond. In Advances in Neural Information Processing Systems, 2003. [11] M. Hoyrup, C. Rojas, \nand K. Weihrauch. The Radon-Nikodym oper\u00adator is not computable. In Computability &#38; Complexity in \nAnalysis, 2011. [12] K. Kersting and L. De Raedt. Bayesian Logic Programming: Theory and Tool. In Introduction \nto Statistical Relational Learning. 2007. [13] O. Kiselyov and C. Shan. Embedded Probabilistic Programming. \nIn Working Conference on Domain Speci.c Languages. Springer, 2009. [14] D. Kozen. Semantics of Probabilistic \nPrograms. Journal of Computer and System Sciences, 22(3):328 350, 1981. [15] T. Mhamdi, O. Hasan, and \nS. Tahar. On the Formalization of the Lebesgue Integration Theory in HOL. Interactive Theorem Proving, \npages 387 402, 2010. [16] B. Milch, B. Marthi, S. Russell, D. Sontag, D. Ong, and A. Kolobov. BLOG: Probabilistic \nModels with Unknown Objects. In International Joint Conference on Arti.cial Intelligence, volume 19, \n2005. [17] O. Nielsen. An Introduction to Integration and Measure Theory. Wiley-Interscience, 1997. [18] \nS. Park, F. Pfenning, and S. Thrun. A Probabilistic Language based upon Sampling Functions. In Principles \nof Programming Languages, pages 171 182. ACM New York, NY, USA, 2005. [19] A. Pfeffer. IBAL: A Probabilistic \nRational Programming Language. In International Joint Conference on Arti.cial Intelligence, 2001. [20] \nN. Ramsey and A. Pfeffer. Stochastic Lambda Calculus and Monads of Probability Distributions. volume \n37, pages 154 165. ACM, 2002. [21] M. Richardson and P. Domingos. Markov Logic Networks. Machine Learning, \n62(1):107 136, 2006. [22] T. Sato and Y. Kameya. PRISM: A Symbolic-Statistical Modeling Language. In \nInternational Joint Conference on Arti.cial Intelligence, pages 1330 1339, 1997. [23] D. Scott. Parametric \nStatistical Modeling by Minimum Integrated Square Error. Technometrics, 43(3):274 285, 2001. [24] B. \nSilverman. Density Estimation for Statistics and Data Analysis. Chapman &#38; Hall/CRC, 1986. [25] R. \nSolovay. A Model of Set-Theory in Which Every Set of Reals is Lebesgue Measurable. Annals of Mathematics, \npages 1 56, 1970. [26] L. Wasserman. All of Statistics: A Concise Course in Statistical Inference. Springer, \n2004.  \n\t\t\t", "proc_id": "2103656", "abstract": "<p>There has been great interest in creating probabilistic programming languages to simplify the coding of statistical tasks; however, there still does not exist a formal language that simultaneously provides (1) continuous probability distributions, (2) the ability to naturally express custom probabilistic models, and (3) probability density functions (PDFs). This collection of features is necessary for mechanizing fundamental statistical techniques. We formalize the first probabilistic language that exhibits these features, and it serves as a foundational framework for extending the ideas to more general languages. Particularly novel are our type system for absolutely continuous (AC) distributions (those which permit PDFs) and our PDF calculation procedure, which calculates PDF s for a large class of AC distributions. Our formalization paves the way toward the rigorous encoding of powerful statistical reformulations.</p>", "authors": [{"name": "Sooraj Bhat", "author_profile_id": "81330488113", "affiliation": "Georgia Institute of Technology, Atlanta, USA", "person_id": "P2991305", "email_address": "sooraj@gatech.edu", "orcid_id": ""}, {"name": "Ashish Agarwal", "author_profile_id": "81323487529", "affiliation": "New York University, New York, USA", "person_id": "P2991306", "email_address": "ashish.agarwal@nyu.edu", "orcid_id": ""}, {"name": "Richard Vuduc", "author_profile_id": "81100139377", "affiliation": "Georgia Institute of Technology, Atlanta, USA", "person_id": "P2991307", "email_address": "richie@cc.gatech.edu", "orcid_id": ""}, {"name": "Alexander Gray", "author_profile_id": "81100401442", "affiliation": "Georgia Institute of Technology, Atlanta, USA", "person_id": "P2991308", "email_address": "agray@cc.gatech.edu", "orcid_id": ""}], "doi_number": "10.1145/2103656.2103721", "year": "2012", "article_id": "2103721", "conference": "POPL", "title": "A type theory for probability density functions", "url": "http://dl.acm.org/citation.cfm?id=2103721"}