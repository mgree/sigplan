{"article_publication_date": "10-01-2004", "fulltext": "\n Measuring the Dynamic Behaviour of AspectJ Programs* Bruno Dufour School of Computer Science McGill \nUniversity bdufou1@cs.mcgill.ca Oege de Moor Oxford University Computing Laboratory oege@comlab.ox.ac.uk \n Christopher Goard School of Computer Science McGill University cgoard@cs.mcgill.ca Ganesh Sittampalam \nOxford University Computing Laboratory ganesh@comlab.ox.ac.uk Laurie Hendren School of Computer Science \nMcGill University hendren@cs.mcgill.ca Clark Verbrugge School of Computer Science McGill University \n clump@cs.mcgill.ca  ABSTRACT This paper proposes and implements a rigorous method for study\u00ading the \ndynamic behaviour of AspectJ programs. As part of this methodology several new metrics speci.c to AspectJ \nprograms are proposed and tools for collecting the relevant metrics are presented. The major tools consist \nof: (1) a modi.ed version of the AspectJ compiler that tags bytecode instructions with an indication \nof the cause of their generation, such as a particular feature of AspectJ; and (2) a modi.ed version \nof the *J dynamic metrics collection tool which is composed of a JVMPI-based trace generator and an analyzer \nwhich propagates tags and computes the proposed met\u00adrics. This dynamic propagation is essential, and \nthus this paper contributes not only new metrics, but also non-trivial ways of com\u00adputing them. We furthermore \npresent a set of benchmarks that exercise a wide range of AspectJ s features, and the metrics that we \nmeasured on these benchmarks. The results provide guidance to AspectJ users on how to avoid ef.ciency \npitfalls, to AspectJ implementors on promising areas for future optimization, and to tool builders on \nways to understand the runtime behaviour of AspectJ. Categories and Subject Descriptors D.3.3 [Programming \nLanguages]: Language Constructs and Fea\u00adtures; D.2.8 [Software Engineering]: Metrics General Terms Experimentation, \nLanguages, Measurement, Performance.  Keywords Aspect-oriented Programming, AspectJ, Dynamic Metrics, \nProgram Analysis, Java, Optimization, Performance. *This work was supported, in part, by NSERC, FQRNT \nand EP-SRC. Permission to make digital or hard copies of all or part of this work for personal or classroom \nuse is granted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. OOPSLA \n04, Oct. 24-28, 2004, Vancouver, British Columbia, Canada. Copyright 2004 ACM 1-58113-831-8/04/0010 ...$5.00. \n1. INTRODUCTION Aspect-oriented programming [17] is a new technique for mod\u00adularizing a program. An aspect \nis a feature that cross-cuts the traditional abstraction boundaries of classes and methods; the most \ncommon examples of aspects are ones used for tracing or logging the execution of an existing program, \nbut aspect-oriented design techniques have also been used successfully for more closely cou\u00adpled functionality \nimprovements, such as connection pooling. The most popular implementation of these ideas is AspectJ [16], \nan extension of Java. The textbook by Laddad [19] provides a nice introduction, both to the language \nand its potential applications. AspectJ started out as a pioneering research effort, but has quickly \nreached a level of maturity where it is on the verge of being used for production programming, and we \ntherefore believe that the time is right for the research community to pay more attention to the performance \nof AspectJ programs. The conceptual model behind AspectJ execution is one in which aspects dynamically \nobserve the execution of a base Java pro\u00adgram. At certain points during this execution, known as join \npoints and speci.ed (in aspects) by pointcuts, an aspect inserts or substi\u00adtutes its own code, known \nas advice. Of course, this conceptual model would be extremely expensive to implement literally; in\u00adstead, \nAspectJ is implemented as a compiler which statically weaves advice code into the base code. In many \ncases, whether or not ad\u00advice would apply at runtime (in the conceptual model) is statically determinable, \nand so this can be done without introducing runtime overhead. However, it is not always possible to decide \nthis at com\u00adpile time, and so a runtime test has to be inserted, particularly when the more complex features \nof pointcuts are being used. However, it is a stated goal of the AspectJ compiler to minimize these over\u00adheads; \nindeed, the AspectJ FAQ [35] states: We aim for the performance of our implementation of AspectJ to be \non par with the same functionality hand\u00adcoded in Java. Anything signi.cantly less should be considered \na bug. It appears to be generally believed in the AspectJ community that the compiler does not introduce \noverheads, and indeed we have con.rmed that in many situations it is the case that equivalent Java and \nAspectJ programs have essentially the same performance. How\u00adever, we have also identi.ed a number of \nexamples in which the AspectJ compiler does impose a signi.cant overhead, contradict\u00ading this belief. \nThe FAQ goes on to say: There is currently no benchmark suite for AOP lan\u00ad guages in general or for AspectJ \nin particular. It is probably too early to develop such a suite because As\u00adpectJ needs more maturation \nof the language and the coding styles .rst. Coding styles really drive the de\u00advelopment of the benchmark \nsuites since they suggest what is important to measure. We contend that the development of a benchmark \nset which shows good as well as bad uses of AspectJ language features will help to inform the development \nboth of the AspectJ language and compiler, and of coding styles; and that it is better to view the situation \nas a two-way process where benchmarking both drives and is driven by such development. The overheads \nthat we have found using our benchmark set con.rm this. In detail, the contributions of this paper are \nas follows: We provide a new set of dynamic metrics and tools for mea\u00adsuring the performance of AspectJ \nprograms and attributing elements of this performance between the original Java code, the introduced \naspect code and the compilation overhead of individual AspectJ language features.  We have collected \nthe .rst benchmark set of AspectJ pro\u00adgrams, from a variety of public sources. Despite the growing popularity \nof AspectJ, it has proved rather dif.cult to .nd publicly available programs. We hope that it will form \nthe basis of a generally accepted suite of benchmarks and we welcome further contributions from the AspectJ \ncommunity.  We explain the conventional wisdom that the AspectJ com\u00adpiler introduces no runtime overhead, \nby showing a series of benchmarks in which this overhead is indeed negligible.  We show that in other \nbenchmarks, there is a signi.cant over\u00adhead. We identify the language features and patterns of usage \nthat lead to this overhead.  Using the Dava decompiler [25] from the Soot toolkit [32], we investigate \nthe output of the AspectJ compiler where our tools pinpointed a signi.cant performance impact, and demon\u00adstrate \nvarious ways in which improvements could be made. This measure-identify-decompile-.x cycle is very economic \nin the AspectJ situation, where a new language paradigm calls for novel analyses and optimizations: it \nwould be im\u00admensely labour-intensive to obtain the same results through direct experiments with different \nversions of the compiler.  These contributions will be of bene.t to three groups of people: AspectJ \nusers: Our results provide guidance on which As\u00adpectJ idioms are cheap to use and which impose a perfor\u00admance \npenalty. For example, we found that directions for advice placement (before/after/around) can have a \nsigni.cant impact on performance, and our experiments explain why.  AspectJ compiler implementors: We \nidentify areas in which compilers could be improved, for example by using more sophisticated static analyses \nto eliminate runtime checks for pointcut matching. Some of these suggestions are very easy to implement, \nand indeed we report one such optimization which we applied in a modi.ed version of the compiler.  AspectJ \ntool developers: The power of AspectJ makes it very easy to write a seemingly innocuous piece of advice \nthat turns out to have dramatic consequences for performance. Our results point the way towards interactive \ntools that warn the programmer of such situations, and help to remedy the problem when it arises.  The \nremainder of this paper is structured as follows. In Section 2 we provide a brief overview of the AspectJ \nlanguage, and in Sec\u00adtion 3 we provide an overview of the statistics we collect for our set of benchmarks. \nIn Section 4 we give the full details of our toolset, which con\u00adsists of a modi.ed version of the AspectJ \ncompiler [3] that tags bytecode instructions according to their provenance (the base Java program, aspect \ncode, or compiler overhead from particular lan\u00adguage features), along with a modi.ed version of the *J \nmetric tool [8] which collects statistics for each of these tags. The tag\u00adging is performed both statically \nand dynamically to allow some tags to be context-dependent; this is vital since in some cases code that \nis compiler overhead may make calls that should also be at\u00adtributed to this overhead, but in other cases \nit may call aspect code that should also be attributed correctly. Developing this tag prop\u00adagation scheme \nhas been a major part of our work. The benchmarks themselves are presented in Section 5. We split them \ninto two categories: those that do not demonstrate signi.cant compiler overhead and those that do. In \nthe case of the latter cat\u00adegory, we investigate the reasons for this overhead in detail and suggest \npossible improvements. While we believe this is the .rst systematic study of the dynamic behaviour of \nAspectJ, there is naturally a wealth of related work on collecting dynamic metrics. We discuss these, \nand also existing efforts to improve the runtime behaviour of AspectJ programs, in Section 6. Finally \nwe discuss our conclusions in Section 7. 2. A BRIEF INTRODUCTION TO ASPECTJ AspectJ is an extension \nof Java; it provides novel features for modularization, in particular when adding new functionality to \nan existing base program . The novel features can be classi.ed into two groups. The .rst group allows \none to in.uence the dynamic behaviour of the base program by injecting new code when certain events occur \nin its execution. We discuss these dynamic features in Section 2.1. The second group of features allows \none to stati\u00adcally add new members to classes. These features are reviewed in Section 2.2. This introductory \nsection only covers the very basics, and readers who are new to AspectJ may wish to consult one of the \ntextbooks [10, 18, 19] for a more comprehensive introduction. 2.1 Join points, pointcut and advice When \nadding tracing functionality to an existing program, it is often undesirable to modify the program itself: \nthe implementa\u00adtion of tracing is scattered over the design, and hence it obscures the existing code, \nand it is dif.cult to maintain itself. It would be preferable if we could describe the execution events \nthat we wish to trace, and the action to take upon each such event. AspectJ al\u00adlows us to do this by \nspecifying such execution events. The events are called join points, the pattern that speci.es a set \nof join points is called a pointcut and the additional code that gets run is called advice. The join \npoints that can be be selected via pointcuts can be thought of as nodes in the dynamic call tree of the \nprogram. Be\u00adsides nodes for method calls, this call tree also includes nodes for the execution of a method \nbody, exception handlers, and so on. To illustrate these abstract de.nitions, let us examine a tiny ex\u00adample, \nshown in Figure 1. It consists of a base program (the class Example) and an aspect (named ExampleAspect). \nThe base pro\u00adgram consists of two methods called foo and bar. The purpose of the aspect is to signal \nany calls that are made to bar within the dy\u00adnamic scope of foo. In terms of the call tree, this means \nthat we are interested in bar nodes that occur below a call to foo. In the aspect, this is expressed \nas follows. It says that before entering any join point selected by the pointcut public class Example \n{ public static void main(String[] args) { Example e = new Example(); e.bar(); e.foo(); } public void \nfoo() { System.out.println( foo ); bar(); } public void bar() { System.out.println( bar ); }} aspect \nExampleAspect {before(): call(void Example.bar()) &#38;&#38; c.ow(call(void Example.foo())) {System.out.println( \nfoo->bar ); }} Figure 1: Example AspectJ program call(void Example.bar()) &#38;&#38; c.ow(call(void \nExample.foo())) the message foo -> bar should be displayed on the standard out\u00adput. The pointcut itself \nconsists of two parts: it says we want a call to bar, and furthermore we must be dynamically within the \ncontrol .ow (c.ow)of foo. While the matching of join points to pointcuts is conceptually a dynamic process \nthat happens entirely at runtime, the AspectJ compiler shifts a lot of the work to compile-time. In the \nabove example, it will identify the calls to bar in the program text, effec\u00adtively matching the .rst \npart of the pointcut. The second part of the pointcut (involving c.ow) is however matched dynamically, \nand to this end some extra code is inserted, which checks whether we are in the dynamic scope of foo. \nTo make that check, it is in turn nec\u00adessary to do a little administration at each call to foo. As we \nshall discuss in more detail later, the AspectJ compiler mimics the call stack by recording each entry \nto foo, and each exit. There is some terminology to ease discussion of these issues. The place in a program \ntext that gives rise to a particular join point at runtime is called a shadow. As we have just explained, \nthe com\u00adpiler matches pointcuts against such shadows, possibly leaving a dynamic residue for the tests \nthat could not be resolved completely. The process of producing combined code for the base program and \nits aspects is called weaving. Our own understanding of join points and advice was mostly shaped by [33], \nwhich gives a de.nitional interpreter for join points and advice. Our discussion of the weaving process \nhas been greatly in.uenced by [23], which explains it in terms of partial evaluation of the interpreter \nin [33]. The de.nitive account of the way the AspectJ compiler works can be found in [14]. We shall introduce \nfurther features relating to join points and advice as we discuss speci.c benchmarks later on in this \npaper. In particular, we shall examine different placements of advice (after and around) in addition \nto before. Finally, we should remark that the example in this section does not require the use of c.ow. \nAspectJ has another kind of pointcut, namely withincode that would be preferable to use for such a sim\u00adple \napplication, because it is more ef.cient. One aim of the present paper is to elucidate such issues. \n 2.2 Intertype declarations While advice is a powerful mechanism to modularize designs where the traditional \nabstractions of Java fail, it is not always enough on its own. Sometimes it is necessary to make a static \nchange to an existing class, for example to add a new method. AspectJ allows such intertype declarations. \nFor example, the aspect in Figure 1 could enhance the Example class with a new method called goo by including \nthe line public void Example.goo() { System.out.println( goo ); foo(); } Client code of Example (introduced \nby the aspect) can now refer to goo in the same way as it references foo or bar. Similar ideas can be \nfound in other extensions to Java, in partic\u00adular MultiJava [5] and RMJ [26]. These designs are in fact \nmore disciplined than AspectJ, and they allow for modular type checking, which AspectJ does not; furthermore \nthey include multimethods, a feature that AspectJ lacks at present.  3. MEASUREMENTS AND DYNAMIC METRICS \nIn order to study the dynamic behaviour of AspectJ, it was nec\u00adessary to develop a methodology to collect \nmeasurements and dy\u00adnamic metrics for AspectJ programs. Our approach uses the fol\u00adlowing three kinds \nof measurements. 3.1 Execution Time The most coarse-grained measurement is the execution time of a program, \nwhich we use as a .rst-order measurement of the over\u00adheads incurred by using aspects. In particular, \nwe compare the ex\u00adecution time of an AspectJ version of a program and an equivalent Java program. All \nexecution times in this paper were collected on an Athlon MP 1.66 GHz machine with 2 Gbyte of memory. \nAll pro\u00ad.ling data used to compute the dynamic metrics was collected on an Intel Pentium 4 1.80 GHz machine \nwith 512 Mbyte of memory. Both machines use Sun s Java 2 Runtime Environment, Standard Edition (build \n1.4.0-b921) on Debian Linux. 3.2 Java-based dynamic metrics As well as execution time, one would also \nlike more speci.c measurements of the dynamic behaviour of both the Java and As\u00adpectJ versions of benchmarks. \nSince both Java and AspectJ pro\u00adgrams are compiled to Java bytecode, it was possible, using *J [8], an \nexisting tool, to measure relevant dynamic metrics. The *J tool collects a wide variety of metrics, and \nwe have found several met\u00adrics to be useful in our evaluation of AspectJ benchmarks. For example, the \nbase metrics can be used to measure: (1) the total number of bytecode instructions executed, a VM-neutral \nmea\u00adsure of execution time; (2) the total number of distinct bytecodes loaded and executed, which gives \na measurement of total and live program size; and (3) the total number of bytes allocated, which measures \nhow memory hungry the benchmarks are. We can also use more detailed metrics to measure speci.c behaviours. \nFor ex\u00adample, we can look at the density of important (expensive) opera\u00adtions such as virtual method \ninvocations, .eld read/writes and ob\u00adject allocations. Speci.c examples of these metrics are given in \nthe discussion of our benchmarks in Section 5. It is often useful to differentiate between application \ncode and library code, especially in the case of AspectJ programs. We de.ne application code as the set \nof class .les that are directly generated 1Flaws in the implementation of the JVMPI interface in more \nre\u00adcent versions of Sun s JRE caused the collected data to be incom\u00adplete, and precluded their use in \nthis study. Using a more recent release of Sun s JRE (build 1.4.2-b28) does result in signi.cantly faster \nrunning times for some benchmarks, but such improvements do not contradict the observations made in this \npaper. by the AspectJ compiler. In addition to generating WHOLE PRO-GRAM versions of the metrics, *J \nis also able to generate APPLI-CATION ONLY versions of them by only taking into consideration the contributions \nmade by the application code. 3.2.1 Dead Code and Code Coverage In our study of AspectJ benchmarks we \nfound that the AspectJ compiler sometimes includes code that is never executed; in par\u00adticular, methods \nthat are never called. Since the entire class must be loaded, this causes unnecessary time to be spent \nin class loading and veri.cation. Thus, we found that it was useful to add two new metrics to our standard \nJava metrics. The dead code metric measures the number of bytecode instructions that are loaded, but \nnever executed. The code coverage metric is computed as the ratio of live code over loaded code. Thus, \na program that loads 10,000 bytecodes and has 2,000 dead bytecodes, has a code coverage of 0.80, that \nis (10,000 -2,000)/10,000. It should be noted that the dead code metric is also dynamic and is reporting \nthe code dead for a particular execution of the program. It may be the case that a different execution \nwould touch different parts of the code. Also, in some cases, the dead code may never execute in any \ngiven run, but is a necessary consequence of support for incremental compilation and weaving, since a \nchange to the base program might cause the code to become required and we would not want to have to recompile \nthe aspect in that case.  3.3 AspectJ-speci.c dynamic metrics Although the previous two kinds (execution \ntime and Java-based dynamic metrics) of measurements give a good overall idea of over\u00adheads incurred \nby the use of AspectJ, they do not help identify the cause of such overheads, nor do they expose any \nbehaviours that are speci.c to AspectJ programs. In order to study these it was necessary to de.ne new \nmetrics and extend existing tools in non\u00adtrivial ways to compute them. These extensions are described \nin more detail in Section 4, but mainly consist of associating a tag to every executed bytecode instruction \nindicating its purpose. In the following subsections we describe the new metrics that were designed speci.cally \nfor analyzing AspectJ programs. 3.3.1 Tag Mix The tag mix metric partitions all executed bytecode instructions \ninto 29 different bins, where each bin corresponds to a speci.c pur\u00adpose. Bins are reported as a percentage \nof total executed instruc\u00adtions. This breakdown of executed bytecodes is useful in determin\u00ading which \nparticular features of AspectJ are used in a benchmark. Individual tags can be grouped into categories \naccording to the AspectJ language feature that they relate to. We de.ne 10 cate\u00adgories of tags, 9 of \nwhich correspond to overhead code introduced by the AspectJ compiler. A detailed list of tags and categories \nis given in Appendix I, and example measurements of these tags are given in Section 5. A short description \nof all categories, along with their most important tags, is presented next. Readers who are unfamiliar \nwith AspectJ may wish to skim this section .rst time through, and then return to it after seeing some \nexample programs in Section 5. General tags. This category contains tags which are associated with user-de.ned \ncode. For analysis purposes, we distinguish be\u00adtween regular code and advice code. The BASE CODE bin \nrep\u00adresents all executed instructions that correspond to the base pro\u00adgram (regular Java), whereas the \nASPECT CODE bin corresponds to code that was executed as part of the aspect. This includes all non-overhead \ninstructions corresponding to the body of an advice and all non-overhead instructions in code called \nfrom the body. It also includes all non-overhead instructions in methods introduced by intertype declarations. \n In making the distinction between base program and aspect, we err on the side of underestimating the \neffect of aspects, for example by making all instructions due to callbacks from native methods contribute \nto the BASE CODE bin. Advice-related tags. This category contains tags that are com\u00admon to before, after \nand around advice. The ADVICE EXECUTE tag identi.es overhead associated with executing an advice body. \nThe ADVICE ARG SETUP tag identi.es overhead associated with exposing parameters to the advice body. The \nADVICE TEST tag is associated with dynamic guards inserted by the compiler in cases where it could not \ndetermine whether a particular advice body should always be executed for a given join point. Tags speci.c \nto around advice. Unlike before and after, around advice replaces existing code with the advice body. \nThe original code can still be invoked through the special proceed() statement, though implementation \nof this feature implies additional overhead.TheAROUND PROCEED tagidenti.esinstructionswhich are inserted \nto make a call to proceed() from within an advice body. Under some circumstances, it is possible that \nthe call to proceed() is not implemented using the inlining strategy, but im\u00adplemented using a more general \ntechnique, a closure. We there\u00adfore de.ne the tag AROUND CALLBACK which serves the same purpose as AROUND \nPROCEED, but which additionally identi.es the tagged instructions as part of the closure implementation. \nThe CLOSURE INIT tag is used to identify instructions which initialize the closure objects that are created. \nTags speci.c to after advice. There are two distinct kinds of overhead that are associated with the use \nof after advice. As with around advice, exposing the return value of a method to the advice body requires \nsupport from the compiler, leading to the ad\u00addition of some overhead code. Also, because after advice \nmust execute regardless of whether the method terminated normally or not, the compiler adds exception \nhandlers to the original code in order to address this issue. The AFTER RETURNING EXPOSURE and AFTER \nTHROWING HANDLER tags are associated with these two situations, respectively. Intertype declaration tags. \nIntertype declarations in AspectJ can lead to several forms of overhead being introduced: additional \nmethod invocations, accessor methods for introduced .elds, vari\u00adable initialization, etc. Several tags \nare de.ned to identify each kind of overhead. perthis and pertarget-speci.c tags. Normally aspects are \nsin\u00adgletons; however, they can also be de.ned on a per-object basis. This category contains instructions \nwhich are used to manipulate aspect instances when there are multiple instantiations rather than a single \none. C.ow-speci.c tags. Because c.ow pointcuts and perc.ow as\u00adpects (as with perthis and pertarget, perc.ow \nis de.ned on a per\u00adobject basis) require some knowledge of the dynamic control .ow of the application, \nthe compiler inserts overhead code in order to create and maintain a representation of this information. \nThere are two tags, CFLOW ENTRY and CFLOW EXIT, to identify instruc\u00adtions which keep this data structure \nupdated.        Exception softening tags. This category contains a single tag, EXCEPTION SOFTENER, \nwhich identi.es instructions which are used to wrap instances of checked exceptions into an unchecked \norg.aspectj.SoftException instance. Tags speci.c to privileged aspects. Privileged aspects have access \nto private methods and .elds of classes. The compiler makes it possible by adding public wrappers to \nthe appropriate classes. This category contains tags that identify instructions which are part of the \ninserted wrapper methods. Miscellaneous aspect tags. This category contains two kinds oftags. The.rstkindoftag, \nCLINIT,isassociatedwithinstructions which are found in static initializers of aspect classes. The second \nkind, INLINE ACCESS METHOD, identi.es the overhead involved in calling a method de.ned on an aspect when \nthere is a static dis\u00adpatch method. An important bene.t of our tool set is that it is easy to extend \nthe set of bins, thus giving .ne-grained information to language designers and compiler writers about \nthe code emanating from new language features. 3.3.2 Aspect Overhead Once every executed bytecode has \nbeen tagged appropriately, it is possible to compute the percentage of executed instructions which fall \ninto the overhead category. We de.ne overhead as all instructions executions which do not fall within \nthe general tag category (BASE CODE or ASPECT CODE). This closely corre\u00adsponds to the instruction executions \nthat would not be found in a hand-woven implementation of the same application. The aspect overhead metric \ncan also be expressed as the prod\u00aduct of two other ratios. The overhead to advice ratio indicates the \nrelative amount of overhead per introduced advice. It is measured as the number of executed overhead \nbytecode instructions divided by the number of executed advice instructions. The advice to total ratio \nmeasures the proportion of the executed code that belongs to advice bodies, and is computed as the number \nof executed advice instructions divided by the total number of executed instructions. 3.3.3 AspectJ \nRuntime In order to truly measure the proportion of the code that can be attributed to the use of AspectJ, \nit is necessary to keep track of the calling context. The AspectJ runtime library metric measures the \npercentage of the code that is executed as part of the AspectJ library, or on its behalf. 3.3.4 Advice \nExecution In many cases, the AspectJ compiler can statically determine if a piece of advice should be \nexecuted at all join points correspond\u00ading to a given join point shadow. In these cases, no dynamic test \nis required to determine if the advice code should be executed or not. There are cases for which static \nanalysis cannot determine the applicability of the advice. For example, the if pointcut contains a boolean \nexpression which is evaluated to determine join point membership; this expression may contain references \nto dynamic values, and so it may not be statically determinable whether it eval\u00aduates to true or false. \nThe c.ow pointcut also generally results in a dynamic test. The advice execution metric reports on the \noutcome of those checks, categorizing them into three bins, those that always suc\u00adceed, those that always \nfail, and those that sometimes succeed and sometimes fail. Clearly those checks that sometimes succeed \nand sometimes fail are needed. However, those checks that always suc\u00adceed or always fail (in one particular \nrun) are potential places where a stronger static analysis might be able to eliminate the check, thus \neliminating unnecessary overhead and improving performance. Of course it may be the case that some checks \nthat are measured as al\u00adways going one way actually could go the other way in a different run of the \nprogram, so it is not necessarily the case that all of those which are identi.ed could really be removed. \n   3.3.5 Hot Shadows Recall that a shadow is the static location in a program text that gives rise \nto a particular join point at runtime. The hot shadows metric measures the percentage of all shadows \nthat account for 90% of the total advice body invocations. This gives an indication of whether runtime \nadvice execution is mostly concentrated on a few shadows, or whether it is thinly spread; this metric \nthus helps us to understand whether we might obtain a performance gain by con\u00adcentrating on just a few \nlocations (and for example inlining advice bodies at those locations). Note that if there are overlapping \npoint\u00adcuts, it is possible for one shadow to invoke multiple advice bodies.  4. TOOLS FOR COLLECTING \nDYNAMIC METRICS An overview of the tools that we use for collecting the dynamic metrics is given in Figure \n2. The darker shaded boxes correspond to new tools, and the more lightly shaded boxes correspond to com\u00adponents \nof existing tools that we modi.ed. Our main tools implement the two important components of our approach: \n(1) a static tagger, which tags bytecode instructions with tags corresponding to their associated purpose; \nand (2) a dynamic analyzer, which propagates the bytecode tags across method calls, according to the \ncontext of the call, and computes the dynamic met\u00adrics. In addition to these main tools we have also \ndeveloped two utili\u00adties. The Retagger utility allows us to modify the tags by hand in\u00adteractively, so \nthat we can experiment with new tagging approaches. The TagReader utility allows us to print a textual \nrepresentation of the tagged bytecode so that we can check its correctness and view the details of which \nbytecode instructions are tagged. In the following subsections we .rst provide an illustrative ex\u00adample, \nshowing examples of static tagging and tag propagation (Section 4.1). We then provide more speci.c details \non the imple\u00admentation of the two main components of our approach, the static tagger, based on the AspectJ \n1.1.12 compiler (Section 4.2), and the dynamic metric analyzer, based on *J (Section 4.3). 4.1 An example \nTo demonstrate our approach to static tagging and dynamic prop\u00adagation, consider the small AspectJ program \nin Figure 1. The ad\u00advice declared in ExampleAspect should execute before every call to bar() (selected \nby the .rst call pointcut) for which there is a call to foo() somewhere in the call stack (selected by \nthe c.ow pointcut). The listing in Figure 3 shows the bytecode instructions for each of the methods in \nExample.class, with the added instruction tags that were produced by our static tagger. Each line of \nbytecode cor\u00adresponding to instructions introduced by the AspectJ compiler is annotated with the tag \nassociated with it. Many bytecodes do not have a tag and these bytecodes will be assigned a tag during \nthe 2 At the time of this writing, version 1.2 of the AspectJ compiler has been recently released. Preliminary \nexperiments show that it does not signi.cantly affect the present discussion. Modified AspectJ Compiler \n  TagReader .class files with tags in attributes Standard JVM  textual representation of tagged class \nfile JVMPI Events *J Dynamic Metric Tool  standard dynamic metrics AspectJ-specific dynamic metrics \nFigure 2: Overview of Metric Collection Tools Tag Shadow public Example() 0: aload 0 1: invokespecialObject() \n4: return public static void main(String[] args) 0: newExample 3: dup 4: invokespecialExample() \n7: astore 1 8: aload 1 ADVICE TEST 12 9: getstaticCFlowStack ExampleAspect.ajc$c.owStack$0 ADVICE TEST \n12 12: invokevirtualboolean CFlowStack.isValid() ADVICE TEST 12 15: ifeq. 24 ADVICE ARG SETUP 12 18: \ninvokestaticExampleAspect ExampleAspect.aspectOf() ADVICE EXECUTE 12 21: invokevirtualvoid ExampleAspect.ajc$before$ExampleAspect$148() \n 24: invokevirtualvoid Example.bar() 27: aload 1 CFLOW ENTER 13 28: bipush0 CFLOW ENTER 13 30: anewarrayObject[] \nCFLOW ENTER 13 33: astore 3 CFLOW ENTER 13 34: getstaticCFlowStack ExampleAspect.ajc$c.owStack$0 CFLOW \nENTER 13 37: aload 3 CFLOW ENTER 13 38: invokevirtualvoid CFlowStack.push(Object[]) 41: invokevirtualvoid \nExample.foo() CFLOW EXIT 13 44: goto. 24 CFLOW EXIT 13 47: astore4 CFLOW EXIT 13 49: getstaticCFlowStack \nExampleAspect.ajc$c.owStack$0 CFLOW EXIT 13 52: invokevirtualvoid CFlowStack.pop() CFLOW EXIT 13 55: \naload4 CFLOW EXIT 13 57: athrow CFLOW EXIT 13 58: nop CFLOW EXIT 13 59: getstaticCFlowStack ExampleAspect.ajc$c.owStack$0 \nCFLOW EXIT 13 62: invokevirtualvoid CFlowStack.pop() 65: nop  66: return public void foo() 0: getstaticPrintStream \nSystem.out 3: ldc foo 5: invokevirtualvoid PrintStream.println(String) 8: aload 0 ADVICE TEST 17 9: \ngetstaticCFlowStack ExampleAspect.ajc$c.owStack$0 ADVICE TEST 17 12: invokevirtualboolean CFlowStack.isValid() \nADVICE TEST 17 15: ifeq. 24 ADVICE ARG SETUP 17 18: invokestaticExampleAspect ExampleAspect.aspectOf() \nADVICE EXECUTE 17 21: invokevirtualvoid ExampleAspect.ajc$before$ExampleAspect$148() 24: invokevirtualvoid \nExample.bar()  27: return public void bar() 0: getstaticPrintStream System.out 3: ldc bar 5: invokevirtualvoid \nPrintStream.println(String)  8: return Figure 3: Tagged class .le for example AspectJ program subsequent \ndynamic analysis. Let us now examine the static tag\u00adging and dynamic tag propagation for our example. \nInstructions 9 15 in both main(String[]) and foo() are tagged ADVICE TEST;theseinstructionsperformthematchingofthe \nc.ow pointcut, and test for the presence of a call to foo() in the call stack. If this test succeeds, \nthe advice is executed. Instructions 18 21 in both methods are advice execution over\u00adhead, tagged ADVICE \nARG SETUP and ADVICE EXECUTE. The distinction is made between these two tags because they propagate differently. \nInstruction 18 is a call to the aspectOf() method, which acquires the aspect instance. All of the untagged \ninstructions in as\u00adpectOf() will inherit the tag of instruction 18 (ADVICE ARG SET-UP), as they also \nrepresent the same kind of overhead. Instruction 21, however, calls the advice body, which is not overhead, \nand so its tag is not propagated by the analyzer. Instead, the ASPECT CODE tag is propagated to the advice \nbody method. Instructions 28 38 (CFLOW ENTRY) and 44 62 (CFLOW EXIT) manage the representation of the \ncall stack, required by the c.ow pointcut. This call stack representation is described in more detail \nin Section 5.3.2. Before each call to foo(), a value is pushed onto the CFlowStack corresponding to the \nrelevant c.ow pointcut. On returning from that call, either normally or by thrown exception, the CFlowStack \nis popped. Both of these tags, CFLOW ENTRY and CFLOW EXIT,propagatetothecalledmethodssincethe push() \nand pop() methods represent the same kinds of overhead. 4.2 Static Tagging: annotating class .les using \na modi.ed AspectJ compiler The AspectJ compiler, since version 1.1, operates in two stages. The .rst \nis a compilation stage, using the Java compiler from the Eclipse project, which produces class .les with \nspecial attributes. These attributes contain information for the second stage, where aspects are woven \ninto the bytecode of a base program. We have modi.ed the bytecode weaver of version 1.1.1 of the AspectJ \ncompiler to annotate the classes it produces. A .rst set of annotations assigns tags to certain bytecode \ninstructions. These tags aim at identifying the role of the instruction in the generated code, such as \ndynamically guarding a given piece of advice, invok\u00ading an advice body, etc. The tag annotations are \nfocused on study\u00ading the use of the different language features that AspectJ supports; 27 out of the \n29 possible tags represent overhead instructions (the other two are for base and aspect code respectively). \nA second set of annotations identify the join point shadows into which instructions have been inserted \nduring weaving. Each added instruction is tagged with a shadow ID corresponding to a sin\u00adgle join point \nshadow. For example, the single advice declaration listed in Figure 1 results in instructions being added \nto multiple join point shadows in the base program. These added instructions have shadow ID tags as shown \nin Figure 3. The three join point shad\u00adows, each corresponding to a method call, have IDs 12, 13, and \n17. The weaver additionally stores a table mapping each shadow ID to its shadow kind (e.g., method-call) \nand its signature (e.g., void Example.bar() for shadow 12 in the example.) 4.2.1 Tagging during weaving \nIn the AspectJ compiler, the major changes made to the classes being woven into are performed by two \nkinds of munger. The .rst is the type munger, which is responsible for changing the type structure of \nthe program and implements intertype declarations. The second is the shadow munger, which is responsible \nfor manipu\u00adlating join point shadows, implementing, for example, the weaving in of advice. Consider the \nsimple case of the before advice declared in the example in Figure 1. During the weaving stage this advice \nis represented by a shadow munger which operates on shadows for which a subset of associated join points \nare selected by the advice s pointcut. The body of the advice is compiled as a method on the as\u00adpect \nclass during the compilation stage; the shadow munger inserts into the shadow the instructions necessary \nfor calling this advice body method, and, if necessary, test instructions to determine at runtime if \na join point matches the pointcut.      Our modi.ed AspectJ weaver tags all the instructions according \nto their purpose. The .rst set of new instructions created by the weaver expose arguments to the advice \nand acquire the aspect in\u00adstance. We add as attributes to each generated instruction object the ADVICE \nARG SETUP tag. Then the advice execution instruc\u00adtion is created, which is an invoke to the advice body \nmethod. We tag this ADVICE EXECUTE in the same way. Finally, if it hasn t been statically determined \nthat this advice should always execute at this shadow, test instructions are generated, which we tag \nas AD-VICE TEST. This newly generated instruction list is then inserted into the shadow, which is a range \nof instructions in a method in the base program. Our examples so far have demonstrated some of the most \ncom\u00admon tags. However, the AspectJ weaver introduces new instruc\u00adtions into the base program to implement \nmany other features, both as a result of static cross-cutting and dynamic cross-cutting. 4.2.2 Pretagging \nNot all of the instructions we wish to annotate during the weav\u00ading stage are generated during the weaving \nstage. Existing in\u00adstructions in aspect classes, generated during the front-end AspectJ compilation, \nmay also represent overhead. The front-end compiler could be modi.ed to tag these instructions as they \nare generated, in the same manner that instructions are tagged during weaving, however, since AspectJ \nsupports the weaving of binary aspects for which the source may be unavailable, it is desirable to instead \nper\u00adform all tagging during the weaving stage. Therefore, at the be\u00adginning of this stage, we search \nfor existing overhead instructions within aspect classes and tag them. Since the AspectJ compiler au\u00adtomatically \ngenerates names for advice bodies and other methods on the aspect class, this is accomplished by searching \nfor bytecode patterns in methods whose names match the naming conventions. An example case is that of \nan around advice body. The body of this around advice is implemented as a method on the aspect class. \nFor this method, we isolate the instructions implementing the pro\u00adceed() call, and tag them appropriately. \n 4.2.3 Generating attributed class .les After all tagging and weaving has been performed on all classes, \nand as classes are being written, our modi.ed AspectJ compiler converts the tag attributes on the instruction \nobjects into a code at\u00adtribute for each method which is stored in the generated class .les. For those \ninstructions with explicit tags we use that tag value, and for instructions without tags a placeholder \ntag is assigned, namely NO TAG. This will be replaced by a proper tag during the dynamic analysis phase. \n 4.3 Dynamic metric analysis with tag propa\u00adgation using *J *J is a framework designed to perform dynamic \nanalyses of Java programs. While it was primarily designed for computing dynamic metrics, it can be easily \nextended to include various other kinds of analyses. The *J framework uses a trace collection agent which \nis based on the Java Virtual Machine Pro.ler Interface (JVMPI). This agent receives execution events \nfrom a regular Java Virtual Ma\u00adchine (JVM) and encodes the information in the form of an event trace. \nThis trace can then be processed by the analyzer, which inter\u00adnally consists of a sequence of operations \norganized as a pipeline structure. Each analysis in the pipeline receives events from the trace sequentially. \n*J provides a number of default analyses in its library, many of which provide services to subsequent \nanalyses in the pipeline. It also includes a full set of general-purpose dynamic metric computation modules. \n   4.3.1 Modi.cations to the *J analyzer: Static tagging identi.es bytecode that is added to support \nAs\u00adpectJ constructs. Because only the application classes are compiled with the modi.ed AspectJ compiler, \nusing only the static instruc\u00adtion tags in an analysis results in a signi.cant underestimate of the overhead \ncode. For example, it is possible for parts of the Java standard library to be called in AspectJ overhead \ncode as well as from the original application. It is thus necessary to propagate the statically-assigned \ntags dynamically based on the control .ow of the application in order to obtain a correct measurement \nof over\u00adhead. Several additions were made to *J in order to make it recognize and use the bytecode tags. \nThe *J class .le reader was extended to enable reading of the encoded information, and association of \ntags with each loaded bytecode instruction. For untagged bytecodes, a default tag value, NO TAG, serves \nas a placeholder. The most signi.cant addition to *J consists of the tag propaga\u00adtion analysis. This \nanalysis is responsible for dynamically assign\u00ading tags to executed bytecodes by pushing tags along invocation \nedges in the dynamic call graph of the application. For example, in Figure 1, the invokestatic bytecode \nat offset 18 in main(String[]) has a static ADVICE ARG SETUP tag. This instruction invokes the aspectOf() \nmethod on the ExampleAspect aspect class. At runtime, the ADVICE ARG SETUP tag will be propagated to \nall bytecodes in the aspectOf() method, and all bytecodes in methods that it calls, etc. If an instruction \nhas no static tag, and no tag has been propa\u00adgated to it, it is assigned the default tag, BASE CODE. \nThis guar\u00adantees that all bytecodes executed during normal control .ow re\u00adceive a dynamic tag every time \nthey are executed. The exception to this is when program code is entered from places the *J agent cannot \nobserve. This can happen in the case of callbacks from JNI code, or the execution of the class loader, \nfor example. In the cases described, where this task is especially dif.cult, we always opt for the conservative \nsolution, ensuring that our analysis will never overestimate the overhead. The meaning associated with \nsome tags precludes their propaga\u00adtion. For example, the ADVICE EXECUTE tag is used for calls to methods \ncorresponding to advice code; the call (and subsequent re\u00adturn statement) are overhead, but the body \nof the advice is not and should be tagged ASPECT CODE. In this and similar cases, partic\u00adular tags trigger \npropagation of different tags. Therefore, we de.ne a propagation table. This table provides a mapping \nfrom each tag to another tag which is to be used in its stead when propagating. Most tags are propagated \nas themselves; the exceptions are listed in Table 1.     Current Propagated ADVICE EXECUTE ASPECT \nCODE INTERMETHOD ASPECT CODE INLINE ACCESS METHOD ASPECT CODE AROUND CALLBACK BASE CODE or ASPECT CODE \nAROUND PROCEED BASE CODE or ASPECT CODE Table 1: Dynamic Propagation Table The INTERMETHODand INLINE \nACCESS METHODtags,like AD-VICE EXECUTE, both identify call sites which invoke user-de.ned aspect code, \nand thus have the same propagation behaviour. The AROUND CALLBACK and AROUND PROCEED tags identify call \nsites which implement the proceed() construct, and can propagate either the BASE CODE or the ASPECT CODE \ntag depending on the calling context at the advised join point. Keeping track of the depth of nested \naspect code is thus necessary to determine the context of proceed() calls.    The propagation algorithm \nis further complicated by tags which are to be propagated to bytecode instructions which already pos\u00adsess \na tag. In such cases, it is sometimes necessary to allow the new tag to temporarily override the previous \none. While tags iden\u00adtifying overhead code should not be overridden, it must be possible to override \nthe tags which correspond to base or aspect code. This is best illustrated by a simple example. An instance \nof an aspect can be accessed via the static method aspectOf() on the as\u00adpect class. This call can originate \nfrom within user-de.ned code, as well as from within the code inserted by the weaver to implement advice \nexecution. In order to support the .rst case, the method is statically tagged ASPECT CODE. In the second \ncase, the invoke is tagged ADVICE ARG SETUP (as illustrated in Figure 3), which we wish to propagate \nto the method. To correctly handle all sim\u00adilar situations, it is necessary that instances of the ASPECT \nCODE and BASE CODE tags can temporarily be overridden by other tags during analysis. Note that in order \nto support the .rst case, an instruction tagged BASE CODE must not be allowed to override a statically \nassigned ASPECT CODE tag. In cases where it would, an ASPECT CODE tag is propagated instead. 4.3.2 Collecting \nthe AspectJ-speci.c metrics The entire tag propagation scheme is implemented as a separate *J analysis, \nso that subsequent AspectJ-speci.c analyses can be implemented independently and easily. Since each bytecode \nexecution now has an associated dynami\u00adcally computed tag, it is a simple addition to the *J analyzer \nto collect the tag mix metric, which counts the number of bytecodes executed for each tag. We can also \napportion other existing metrics, such as allocation counts, between the different tags. In addition, \nthe analyzer also tracks all dynamic guards on ad\u00advice, and for each such guard computes whether the \nguard always succeeds, always fails, or sometimes succeeds. A count of the num\u00adber of times each guard \nis executed is also maintained.   5. BENCHMARKS In this section we provide the results and analysis \nfor eight bench\u00admarks which span a wide variety of uses of AspectJ. Although As\u00adpectJ is becoming quite \npopular there is no existing AspectJ bench\u00admark set, thus our .rst challenge was to collect benchmarks \nthat were representative of many different applications of AspectJ. All of our benchmarks were collected \nfrom public sources on the web, and can be obtained from http://www.sable.mcgill.ca/ benchmarks/. We \nbelieve that providing an interesting and di\u00adverse benchmark set is an important contribution in itself. \nFour of our benchmarks have equivalent Java versions, while the other four are too large and/or complex \nto easily produce Java equivalents. The benchmarks with Java versions are particularly valuable because \nwe can compare runtime overheads shown by direct timing comparisons with overheads shown by our dynamic \nmetric analysis; the timings tell us where there is observable over\u00adhead, and the metric analysis helps \nus understand the reasons for that overhead. When analyzing the benchmarks we did not know what to ex\u00adpect \na priori. The general belief in the AspectJ community seems to be that overheads are low. Thus, an important \npart of our study was to .nd out if and why this is true. The .rst four benchmarks, presented in Section \n5.2, are examples where we found low over\u00adall overheads. However, somewhat to our surprise we found three \nbenchmarks which had extremely high overheads, and for those benchmarks we have made a detailed examination \nof the source of the overheads, as presented in Section 5.3. Finally, we investigated one benchmark for \nwhich the aspect is intended to improve perfor\u00admance. We discuss it in Section 5.4.     5.1 Overall \nData Table 2 gives an overview of the key data for all eight bench\u00admarks. Each heading of related rows \ncontains references to those sections of the paper that discuss the relevant metrics in detail. At the \ntop of the table we give the metrics that measure program size. Note that six of the benchmarks are quite \nlarge, and are com\u00adposed of between 24 and 252 application classes (classes that are not part of the \nstandard Java library). Two benchmarks, Bean and Figure are smaller, but have been selected to illustrate \nsome stan\u00addard uses of AspectJ. Also, note that as with all Java programs, the size of the programs, \nwhen the Java libraries are included, are very large, even for the small applications. The region of \nthe table labelled EXECUTION TIME MEA-SUREMENTS gives measurements for execution time, including both \nreal execution times and metrics. For real execution times we consider three different con.gurations \nof the Java VM (Java HotSpot Client VM (build 1.4.0-b92, mixed mode)). In the .rst con.guration we use \nthe default mode which enables the client VM. For this con.guration we also provide the amount of time \nspent in the JIT compiler, and total GC time. Since the ajc com\u00adpiler s code generation strategy assumes \na VM with a JIT that in\u00adlines, we also provide the performance for the client VM when in\u00adlining is disabled. \nFinally, in order to see performance of the byte\u00adcode directly, without the effect of JIT compilation, \nwe provide the time for the interpreter con.guration. Another important aspect of performance is space \nusage. In the section of the chart labelled EXECUTION SPACE MEASURE-MENTS , a key metric is the Object \nAllocation Density which mea\u00adsures the number of objects allocated per 1000 bytecode instruc\u00adtions executed \n(kilobytecode or kbc). If the allocation density is high, then it is important to examine the ASPECTJTAG \nMIX FOR ALLOCATIONS section at the bottom of the table to deter\u00admine if signi.cant space is used for \nAspectJ overhead. In the section labelled ASPECTJMETRICS SUMMARIZING OVERHEAD we provide those measurements \nthat summarize overheads. Benchmarks with high AspectJ Overhead are those most likely to have performance \nproblems. The sections for ASPECTJTAG MIX provide a more detailed breakdown of the overheads, .rst considering \nall instructions, and then the tag mix for allocations only. Finally, in the section labelled ASPECTJMETRICS \nFOR SHAD-OWS , we give two metrics. The .rst one refers to the hot shadow metric as de.ned in Section \n3.3.5. The second one, called Shadow Guards Runtime Const. , is computed using the advice execution metrics \nde.ned in Section 3.3.4, and is simply the percentage of all shadow guards that always evaluate to true \nor always evaluate to false (i.e., those guards that are runtime constants and perhaps could be optimized \naway using a compiler analysis). A detailed individual analysis of all benchmarks is given in the next \nthree subsections. For each benchmark we give the source of the benchmark, a brief description of the \naspects involved, and a discussion of our performance measurements.  5.2 Benchmarks with low runtime \noverhead In this section we present four benchmarks which seem to con\u00ad.rm the general opinion that AspectJ \nprograms have low overhead when compared to equivalent hand-woven Java programs. As shown in the bold \nentries in Table 2, the .rst four benchmarks   Classes Loaded 55 28 60 5 252 12 60221 Instructions \nLoaded 3198 5543 529 13954 616 27845 48227 1231 154 6901 233 11830 28491 62 71 51 62 58 41    \n     Classes Loaded 390 322 1007 372 573 295 385 905 Instructions Loaded 112588 83275 313425 99477 \n107486 74922 122689 184517 EXECUTION TIME MEASUREMENTS (WHOLE PROGRAM) (\u00a75.1, \u00a73.1) # instr. (million \nbytecodes) 3646 2213 68 158 5034 1623 2984     Total time -client (sec) 8.46 1.78 125.83 1.74 33.13 \n8.90 96.91 24.52 JIT time -client (sec) 0.29 0.14 0.37 0.08 0.29 0.05 0.55 0.66 GC time -client (sec) \n0.64 0.03 0.05 0.04 9.52 0.14 72.42 7.41 Slowdown vs. handcoded(\u00d7) 1.00 18.56 32.53 1.24 Time -client \nnoinline (sec) 8.79 1.86 125.83 1.80 34.58 10.07 97.18 26.44 Slowdown vs. handcoded (\u00d7) 1.07 18.82 23.01 \n1.26 Time -interpreter (sec) 56.05 15.99 125.97 5.61 192.62 44.76 158.78 85.08 Slowdown vs. handcoded \n(\u00d7) 1.15 13.57 21.11 1.08 EXECUTION SPACE MEASUREMENTS (WHOLE PROGRAM) (\u00a75.1, \u00a73.2)     Mem. Alloc. \n(million bytes) 367 30 12 110 5705 374 998 177 Obj. Allocation Density (per kbc) 2.14 0.35 1.90 21.59 \n32.29 9.86 11.78 0.49 #Garbage Collections 374 38 13 144 5818 488 1104 46 ASPECTJMETRICS SUMMARIZING \nOVERHEAD (\u00a73.3.2, \u00a73.3.3) AspectJ Overhead % (whole) 4.92 0.73 0.37 14.24 69.56 92.54 96.33 2.80 #overhead/#advice \n(whole) 0.05 0.01 0.19 0.18 20.03 125.17 30.68 1.77 #advice/#total (whole) 0.94 0.99 0.02 0.79 0.03 \n0.01 0.03 0.02    AspectJ Runtime Lib % (whole) 2.74 0.01 0.05 0.00 21.39 85.27 90.33 0.00 Aspect \nOverhead % (app) 16.98 11.25 7.84 39.48 73.28 82.86 97.94 3.56 #overhead/#advice (app) 0.22 0.13 1.09 \n0.92 19.49 48.33 142.30 1.77 ASPECTJTAG MIX FOR ALL INSTRUCTIONS (WHOLE PROG.) (%) (\u00a73.3.1, appendix) \n         BASE CODE 1.40 0.06 97.62 7.20 26.97 6.72 0.53 95.61 ASPECT CODE 93.68 99.21 2.00 \n78.55 3.47 0.74 3.14 1.59 AspectJ Overhead (total) 4.92 0.73 0.37 14.24 69.56 92.54 96.33 2.80  INTERMETHOD \n0.23 0.51 INTERFIELDINIT 0.09 1.27 INTERCONSTRUCTOR PRE 0.07 INTERCONSTRUCTOR POST 0.23 INTERCONSTRUCTOR \nCONVERSION 0.03  ADVICE EXECUTE 0.29 0.002 0.04 0.76 1.74 0.25 0.008 0.11 ADVICE ARG SETUP 3.17 0.02 \n 0.24 5.19 26.66 0.62 0.21 0.95 ADVICE TEST 10.84 0.22 0.64   AROUND CONVERSION 1.15 0.004 0.002 8.31 \nAROUND CALLBACK 0.01 0.002 16.35 AROUND PROCEED 0.30 0.01 0.08 2.53 7.81 CLOSURE INIT 0.007 0.003 \n8.68   AFTER RETURNING EXPOSURE 0.002 AFTER THROWING HANDLER 0.002   CFLOW ENTRY 36.97 44.06 CFLOW \nEXIT 43.87 51.83  PEROBJECT ENTRY CLINIT 0.001 0.001 INLINE ACCESS METHOD 2.66 ASPECTJTAG MIX FOR \nALLOCATIONS ONLY (WHOLE PROG.) (%) (\u00a73.3.1, appendix)  BASE CODE 0.38 0.57 95.52 3.74 19.25 0.02 0.03 \n100.00 ASPECT CODE 26.25 53.54 3.70 92.69 0.27 AspectJ Overhead (total) 73.38 45.88 0.78 3.56 80.75 \n99.98 99.70 0.003      INTERFIELDINIT 3.56 INTERCONSTRUCTOR PRE 18.72 INTERCONSTRUCTOR POST 21.06 \nINTERCONSTRUCTOR CONVERSION  ADVICE ARG SETUP 54.39 4.07 0.59 53.85 0.27  AROUND CONVERSION 18.98 \n 0.09 AROUND PROCEED 2.03 0.09 26.90  CFLOW ENTRY 99.98 99.42 PEROBJECT ENTRY 0.009 CLINIT 0.005 \n0.001 0.02 0.002 ASPECTJMETRICS FOR SHADOWS (WHOLE PROGRAM) (%) (\u00a73.3.5, \u00a73.3.4) Hot Shadows (for 90%) \n3.12 33.33 4.17 100.00 2.94 100.00 27.43 66.67 Shadow Guards Runtime Const.(%) 100.00 100.00 100.00 \n     Table 2: Benchmark Measurements either show a low amount of total overhead as computed by our \nmetrics (DCM, ProdLine and Tetris), or show little or no slowdown when compared to an equivalent Java \nprogram (Bean). The overall conclusion is that total overhead is not a problem when: (1) each advice \nbody represents a large amount of work, so the overhead per advice application is low; (2) the application \nspends most of its time in the Java library, which usually does not have advice applied to it; (3) the \napplication spends very little time in the part of the code which has advice applied to it (so even if \nthe overhead per advice instruction is high, the overall overhead is low); or (4) there is some noticeable \noverhead in the code produced by ajc, but a good inlining JIT compiler removes the overhead. In the following \nsections we expand upon these conclusions and examine each of the low-overhead benchmarks in more detail. \n  5.2.1 DCM One important use of AspectJ is to provide a convenient way of instrumenting a base Java \nprogram. In this case the base pro\u00adgram doesn t change, but aspects are used to inject instrumentation \ncode to measure some sort of dynamic behaviour. Hassoun, John\u00adson and Counsell have suggested a new dynamic \ncoupling metric (DCM) [12] and a validation of that metric using AspectJ [13]. We have implemented a \nmore ef.cient version of their aspects (using a hash table with one entry per class, instead of one entry \nper ob\u00adject) which computes their proposed dynamic coupling metric. The aspects use around and after \nadvice. The basic idea is that each constructor call and each method call is instrumented so as to in\u00adcrement \na time step counter and to compute a dynamic coupling metric as a function of the value of the metric \nat the previous time step, the number of currently live objects, and the static coupling metric values. \nComputing this function is quite expensive as it re\u00adquires iterating through the entries in a hash table, \nwhere there is one entry for each class in the application. Since this aspect can be applied to any program, \nwe applied it to a reasonably large Java benchmark, Certrevsim, which is a dis\u00adcrete event simulator \nused to simulate the performance of various certi.cate revocation schemes [1]. This seemed to be a suitable \nbenchmark because it has non-trivial uses of objects and it com\u00adputes something useful. The performance \nmeasurements for the DCM aspects applied to the Certrevsim program are given in the column labelled DCM \nin Table 2. As shown by the bold entry, the AspectJ Overhead is only 4.92%. Furthermore, as expected, \nthe ASPECTJTAG MIX metrics shows that over 93% of the instructions executed are in the aspect code. This \nis completely reasonable, since the advice bodies are very expensive, and they involve calls to relatively \nexpensive hash table routines in the Java library. A more detailed analysis does show that the overhead \nwhen look\u00ading at just the application code (Aspect Overhead (app)) is higher, at 16.98%. Furthermore, \nin the TAG MIX metrics for allocations, 19% of all allocations are due to AROUND CONVERSION. These overheads \ndo not matter for this particular benchmark, but for a benchmark with smaller advice bodies, it could \nbe a problem, and may be worth further investigation and possible improvements to the compiler. 5.2.2 \nProdLine Intertype declarations in AspectJ allow one to de.ne new .elds, constructors and methods for \nexisting Java classes. Lopez-Herrejon and Batory use this idea to experiment with using AspectJ to im\u00adplement \nproduct lines, where a product line is a family of related software applications [22]. Their application \nexperiments with a product line for related graph algorithms. This application is in\u00adteresting because \nit heavily uses intertype declarations. The base program is effectively just a collection of empty classes \n(for exam\u00adple Edge, Vertex and Graph) and various aspects that use intertype declarations to insert .elds, \nconstructors and methods into those classes (for example, Directed, Undirected, DFS), plus some uses \nof advice to splice in some method calls. The underlying imple\u00admentations of the graph data structures \nand algorithms make heavy use of the LinkedList implementation in the standard Java library. We used \nthe original benchmark as provided by the authors, but added our own module to generate random graphs, \nand run larger tests suitable for timing. The performance numbers are given in the column labelled Prod-Line \nin Table 2. The overall AspectJ overhead is very low at 0.73% and almost all of the overhead comes from \nthe intertype tags. However, note that the AspectJ overhead for the application only is much higher at \n11.25%. This indicates the benchmark spends a ma\u00adjority of its time in the Java library. Also, a potentially \nimportant overhead is found in the ASPECTJTAG MIX for allocations. It ap\u00adpears that the heavy use of \nintertype constructors in this benchmark leads to considerable space overhead, with about 40% of the \ntotal space used due to objects allocated in the pre and post processing of constructors that have been \nintroduced using intertype declara\u00adtions. This may be another area where a better compilation strategy \ncan avoid some of that overhead. 5.2.3 Tetris Graphical, interactive applications pose dif.culties for \nanalysis in that they both require human intervention and may have large variations in execution time \nthereby. However, they certainly form a large class of applications, and the performance and overhead \nof aspects in such a context is quite relevant in terms of program response times, or the cost of background \ncomputations. We have analyzed an AspectJ version of the arcade game Tetris, available on the web [9]. \nIn order to get reproducible results, we have modi.ed the program to use a seeded random number gener\u00adator, \nand to (non-interactively) replay a previously-recorded inter\u00adactive session. The code to accomplish \nthis naturally changes the program; however, the core program logic is unaltered, and the use of aspects \nremains the same as the original program. Aspects in this situation were used to augment the base game \nwith new functionality. A number of aspects were applied, though most of them apply to situations that \ndid not happen or which hap\u00adpened only a few times during our sample game play. The remain\u00ading aspects \n(NEW BLOCKS and NEXT BLOCK in [9]) are applied to code that is exercised every few game moves, roughly \nin (a re\u00adduced) proportion to the number of game events, or sequences of active code execution. Overall \naspect overhead in Tetris is low, accounting for less than 1% of executed bytecodes (see the Tetris column \nin Table 2). This is further demonstrated by the limited use of aspects with re\u00adspect to the overall \nprogram advice constitutes only 2% of the program. In fact, the WHOLE PROGRAM metrics are dominated by \ncosts external to the application (startup, GUI library code). This can be seen in the relative size \n(Instructions Loaded) of the application ver\u00adsus the whole program, but is also apparent in the APPLICATION \nONLY version of the aspect metrics. Overhead rises to 7.84%, and is now greater than the cost of the \naspect code itself (overhead to advice ratio is 1.09). Program design in this case limits any apparent \noverhead. Varia\u00adtions in Java library/startup design may change the relative weight of application code, \nand thus the visibility of this overhead.  5.2.4 Bean This is example is taken from the AspectJ primer \non the website aspectj.org.3 Once again, we modi.ed it slightly to increase the running time. It starts \nwith a class named Point for representing pairs of x and y coordinates, and it adds the functionality \nof Java beans with bound properties to this class. In order to do so, it injects a new private .eld into \nthe Point class; this new .eld has type PropertyChangeSupport; all the asso\u00adciated methods are added \nas well, and the Point class is declared to be an implementation of Serializable. All these additions \nare ac\u00adcomplished via the static features of AspectJ. Furthermore, it also .res a property changer whenever \neither the x or y coordinate is changed. This additional functionality is achieved with a pointcut and \naround advice, for each of x and y separately. For comparison, we wove the AspectJ version by hand to \nobtain a pure Java program. Both the AspectJ and the pure Java program were compiled with the JIT inliner \nturned on and off. The results for these versions are shown in the Bean column of Table 2. From the tag \nmix, it is apparent that this benchmark spends most of its time in aspect code, which consists of library \ncalls introduced via intertype declarations, but there is also some around advice. The overhead in terms \nof bytecodes executed is quite signi.cant (14.24%). This is re.ected in the execution time when run through \nthe interpreter. However, it appears that the JIT compiler is able to eliminate most of the overhead. \nWithout inlining turned on, there is still a discernible price in execution time of about 7%. With inlin\u00ading, \nthe JIT compiler completely eliminates the cost of the overhead instructions inserted by the AspectJ \ncompiler. In the context of this small benchmark, these numbers appear to justify an assumption of the \nAspectJ implementors, stated in [14], that the inliner eliminates most overhead of intertype declarations, \nand also of advice declarations where there is no dynamic residue of pointcut matching. It is however \nnotoriously dif.cult to predict the effect of inlining strategies, so further benchmarking is neces\u00adsary \nto justify the assumption in general. 5.3 Benchmarks with high overheads Contrary to the belief that \nthere are no signi.cant overheads for AspectJ we did .nd extremely large overheads in three bench\u00admarks. \nIn this section we present these benchmarks, examine where the overheads come from and suggest some solutions \nfor both the programmer (what to avoid using in AspectJ) and for compiler writ\u00aders (what can be improved \nand some ideas on how to make those improvements). 5.3.1 NullCheck Users of AspectJ have found many different \nkinds of applications for aspects. One potential use, as outlined in a short online article by Asberry, \nis to use aspects to enforce coding standards [2]. He suggests several applications, one of them being \nan aspect to detect when methods return null. According to Asberry, the justi.cation for this aspect \nis that sometimes programmers use the on error condition, return null from method anti-pattern. This \nis consid\u00adered to be bad coding style, since throwing a meaningful exception would be much preferable. \nHe suggests the following pointcut and around advice to detect all occurrences of returning null from \na method. // First primitive pointcut matches all calls, // second avoids those with void return type. \n3An earlier version on that webpage was .awed; we are using the revision suggested in an early draft \nof this paper and also on the aspectj-devlist by Gregor Kiczales on January 14, 2004. pointcut methodsThatReturnObjects(): \ncall(* *.*(..)) &#38;&#38; !call(void *.*(..)); Object around(): methodsThatReturnObjects() { Object \nlRetVal = proceed(); if (lRetVal == null) { System.err.println( Detected null return value after calling \n + thisJoinPoint.getSignature().toShortString() + in .le + thisJoinPoint.getSourceLocation().getFileName() \n+ at line + thisJoinPoint.getSourceLocation().getLine()); } return lRetVal; } Since this is another \ncase of an aspect that can be applied to any Java program, we applied it to the same Java benchmark, \nCertrevsim, that we used for the DCM example in Section 5.2.1. Our .rst experiment was to analyze the \ndynamic behaviour of the original Certrevsim benchmark and compare it with the same benchmark, but with \nthe suggested null check aspect applied to it. Results given in Table 3 in the column labelled Orig. \nAspectJ . The results were very surprising, as the original Java benchmark runs in 1.49 sec\u00adonds, but \nthe AspectJ benchmark runs in 33.13 seconds, a 19-fold slowdown. This was completely unexpected, because \naccording to the description of the aspect, the only new useful code being in\u00adserted is a check of the \nreturn value of all non-void methods.4 To verify that such checks should not account for such a slowdown \nwe hand-wove the checks into the original program, and the dy\u00adnamic measurements for this version are \ngiven in the last column of Table 3, labelled Hand-woven Java . The runtime for this hand\u00adwoven version \nis 1.78 seconds, which is only 19% slower than the benchmark without the checks. Thus, there is a huge \ngap between the performance of the AspectJ program (1856% slower) and the hand-woven program (19% slower). \nThe hand-woven version does of course not admit the collection of the AspectJ metrics, and there\u00adfore \nthat part of the table has been omitted in the relevant column. Our metrics indicate the source of the \nproblem. First, there is a lot of around overhead this is to be expected. However, AROUND CONVERSIONshouldbesigni.cantonlywhennon-object \ntypes have to be boxed (and unboxed) upon invocation of the around advice body. Here we did not expect \nthat to happen, as we only wish to process method results that are objects in the .rst place. However, \nthe around advice was being applied to all method calls returning values (including methods returning \nscalar types such as integers) instead of just those that returned values with some Ob\u00adject type (i.e., \nany type that is Object or a subclass of Object).5 Of course, looking back to the pointcut methodsThatReturnObjects, \nwe can see that it does apply to all methods with non-void return type. Thus, we .xed the pointcut designator \nto be the following. pointcut methodsThatReturnObjects(): call(Object+ *.*(..)); This .xed pointcut matches \nonly those method calls which re\u00adturn Object types, as intended, and the dynamic measurements of applying \nthis .xed pointcut to the simulator benchmark are given in Table 3, in the column labelled Fixed AspectJ \n. Note that the runtime is still much larger than expected, 10.69 seconds, or about 6 times slower than \nthe handwoven Java program. The WHOLE PROGRAM dynamic metrics give us some insight into this large performance \ndifference. The .xed AspectJ version executes 1938 million instructions, whereas the hand-woven Java \n4It turns out that the Certrevsim benchmark is well written and does not return null from methods, so \nthe check against null never suc\u00adceeds. Thus, the runtime overhead is simply the check against null and \na branch. 5This could also be observed using the Eclipse plugin for AspectJ.  Orig. Fixed Pruned Best \nHand-woven AspectJ AspectJ AspectJ AspectJ Java (all non-void (only Object+ (not within (after (with \nnull methods) methods) aspect code) returning) checks)   PROGRAM SIZE (APPLICATION ONLY)     Classes \nLoaded 252 138 48 48 22 Instructions Loaded 13954 8510 7660 3859 2421 Instructions Dead 6901 4967 4872 \n2028 1042 Code Coverage (%) 51 42 36 47 57 PROGRAM SIZE WITH JAVA LIBRARIES (WHOLE PROGRAM) Classes \nLoaded 573 459 369 369 342 Instructions Loaded 107486 102042 101192 97391 95946   EXECUTION TIME \nMEASUREMENTS (WHOLE PROGRAM)    # instr. (million bytecodes) 5034 1938 1313 1088 963 Total time -client \n(sec) 33.13 10.69 1.89 1.82 1.78 JIT time -client (sec) 0.29 0.13 0.09 0.08 0.07 GC time -client (sec) \n9.52 2.61 0.02 0.01 0.01 Slowdown vs. handcoded(\u00d7) 18.56 5.99 1.06 1.02 1.00 Time -client noinline (sec) \n34.58 11.06 2.18 2.02 1.84 Slowdown vs. handcoded (\u00d7) 18.82 6.02 1.19 1.10 1.00 Time -interpreter (sec) \n192.62 61.18 19.49 16.67 14.20 Slowdown vs. handcoded (\u00d7) 13.57 4.31 1.37 1.17 1.00 EXECUTION SPACE \nMEASUREMENTS (WHOLE PROGRAM)    Mem. Alloc. (million bytes) 5705 1529 2 2 2 Obj. Allocation Density \n(per kbc) 32.29 19.34 0.03 0.04 0.04 #Garbage Collections 5818 1526 3 2 2 ASPECTJMETRICS SUMMARIZING \nOVERHEAD  AspectJ Overhead % (whole) 69.56 50.15 25.63 13.75 #overhead/#advice (whole) 20.03 19.49 5.40 \n 6.00 #advice/#total (whole) 0.03 0.03 0.05 0.02    AspectJ Runtime Lib % (whole) 21.39 3.88 0.00 \n0.00 ASPECTJTAG MIX FOR ALL INSTRUCTIONS (WHOLE PROG.) (%) BASE CODE 26.97 47.28 69.62 83.96 ASPECT \nCODE 3.47 2.57 4.75 2.29 AspectJ Overhead (total) 69.56 50.15 25.63 13.75    ADVICE EXECUTE 1.74 \n1.29 1.90 3.44 ADVICE ARG SETUP 22.51 16.13 8.02    AROUND CONVERSION  8.31 0.64 0.95 26.66 AROUND \nCALLBACK 16.35 13.49 AROUND PROCEED 7.81 5.79 6.64 CLOSURE INIT 8.68 6.43    AFTER RETURNING EXPOSURE \n2.29 ASPECTJTAG MIX FOR ALLOCATIONS ONLY (WHOLE PROG.) (%) BASE CODE 19.25 0.10 99.26 99.75 AspectJ \nOverhead (total) 80.75 99.90 0.74 0.25  Table 3: Nullcheck metrics version executes only 963 million \ninstructions. However, most sur\u00adprising is that even the .xed AspectJ benchmark allocates 1529 million \nbytes, whereas the original Java version only allocated 2 million bytes. This is a huge increase in memory \nconsumption, considering the aspect body itself is very simple, the check against null never succeeds \nin this benchmark, and thus the aspect body does not explicitly allocate any objects at all. When we \nlook at the APPLICATION ONLY dynamic metrics we see that the hand-woven Java benchmark loaded only 22 \napplica\u00adtion classes (2421 instructions), whereas the .xed AspectJ version loaded 138 classes (8510 instructions), \nanother source of overhead for the class loader and JIT compiler. By looking at the ASPECTJTAG MIX metrics \nwe can see there is a large amount of overhead, mostly attributed to the tags AD-VICE ARG SETUP,AROUND \nCALLBACK,AROUND PROCEED and CLOSURE INIT. Furthermore, by concentrating on the ASPECTJ TAG MIX FOR ALLOCATIONS \nONLY metrics, it is clear that the around advice tags ADVICE ARG SETUP and AROUND PROCEED account for \nalmost 100% of the allocations in the program. Given that all overhead was coming from around advice, \nwe decom\u00adpiled the class .les and studied the code generated by the AspectJ compiler to implement the \naround advice. We found that, in this case, closures are created to handle the around advice. By study\u00ading \nthe code produced we estimated that each method call with around advice has an overhead of 2 invokespecial \ncalls, 5 invokestaticcalls, 2 invokevirtualcalls, 2 array alloca\u00adtions, 3 object allocations, 3 .eld \nread/write instructions, 4 cast/in\u00adstanceof instructions, plus numerous simple load and store in\u00adstructions. \nClearly this use of closures is a very heavy-weight solu\u00adtion, using many expensive bytecode instructions \nand considerable memory allocation, and it certainly accounts for the increase in run\u00adtime. In order \nto understand why closures were being used to imple\u00adment the around advice for such a simple case, we \nstudied the AspectJ compiler and found that there are two strategies for im\u00adplementing around advice, \none uses closures and the other uses an inlining strategy. By default the compiler will try to inline; \nhowever there are two situations in which closures will be used: (1) the com\u00adpiler .ag -XnoInlinehas \nbeen set; or (2) the around body has around advice which applies to it. For our benchmark, the body of \nthe around advice contains several method calls returning Object types (namely the string operations \nin the argument of println), so situation (2) applies and thus the AspectJ compiler selects the clo\u00adsure \nstrategy for all method calls which have this kind of around advice applied. To study the performance \nof the inlining strategy, we changed the pointcut designator to eliminate those method calls that were \nin our aspect code as follows. pointcut methodsThatReturnObjects(): call(Object+ *.*(..)) &#38;&#38; \n!within(lib.aspects.codingstandards.*); The dynamic measurements of this version are given in Table 3 \nin the column labelled Pruned AspectJ . Clearly the inlining strat\u00adegy for around advice is much more \nef.cient than the closure strat\u00adegy. However, it is somewhat alarming that such a minor change to the \npointcut speci.cation has such a large impact on the perfor\u00admance of the program. From the programmer \ns point of view, the !within clause should not be necessary, but clearly it does have a very important \nimpact on the ultimate performance. Further\u00admore, there is still a signi.cant amount of overhead when \nwe com\u00adpare the hand-woven Java version (column labelled Hand-woven Java ) to the equivalent AspectJ \nversion (column labelled Pruned AspectJ ). In terms of runtime performance, the hand-woven Java version \nexecutes in 1.78 seconds whereas the Pruned AspectJ version exe\u00adcutes in 1.89 seconds, which is 6% slower. \nThis overhead is also re.ected in the number of instructions executed, 963 million for the Java version \nversus 1313 million for the AspectJ version. Ac\u00adcording to the ASPECTJTAG MIX metrics, most of the overhead \nis due to ADVICE ARG SETUP (16.13%) and AROUND PROCEED (6.64%). Furthermore, the Pruned AspectJ program \nloads more applica\u00adtion classes (48 vs. 22), because the AspectJ version must load many classes from \nthe AspectJ runtime library, and the aspect class itself. The AspectJ version has more instructions (7660 \nvs. 2421), which is due to code from the AspectJ runtime library, the inlining of multiple copies of \nadvice, and the fact that the inlining strat\u00adegy introduces many overhead instructions, as demonstrated \nby the ASPECTJTAG MIX metrics. Finally, the Pruned AspectJ version has signi.cantly more dead code (4872 \nvs 1042). The dead code comes from three sources: (1) methods in the AspectJ runtime library that are \nloaded, but never run, (2) the code in the never-taken branch of the advice which is inlined in many \nplaces, and (3) the presence of methods generated by the AspectJ compiler which are never needed (for \nexample, a method to deal with advice as closures is generated even if closures are not used). We believe \nAspectJ generates these dead methods for reasons of incremental compilation. After studying the null \ncheck aspect further, one can notice that the pruned version can be further improved by using after return\u00ading \nadvice instead of around advice, as follows. after() returning(Object lRetVal): methodsThatReturnObjects() \n{ if (lRetVal == null) { System.err.println( Detected null return value after calling + thisJoinPoint.getSignature().toShortString() \n+ in .le + thisJoinPoint.getSourceLocation().getFileName() + at line + thisJoinPoint.getSourceLocation().getLine()); \n  }} The measurements for this .nal version are given in the column labelled Best AspectJ . As indicated \nby the ASPECTJTAG MIX metrics, the overhead due to around in the Pruned version (0.95% for AROUND CONVERSIONand6.64%for \nAROUND PROCEED)is replaced by a smaller overhead due to after after returning (2.29% for AFTER RETURNING \nEXPOSURE). There are some important observations to be made with this bench\u00admark. First, even though \nthe pointcut in this example was very simple, it shows that it is very easy for a programmer to de.ne \na pointcut that applies to more places than absolutely necessary. Further, the decision of the AspectJ \ncompiler to use closures or in\u00adlining for around advice can have a huge impact on runtime, due to the \ngeneral, but heavy-weight, strategy used for closures. Pro\u00adgrammers may unwittingly trigger the use of \nclosures if they forget, or don t realize, the importance of avoiding pointcuts that apply in the aspect \nbody. The inlining strategy for around advice is much more ef.cient than the closure-based strategy, \nbut it can still lead to signi.cant overheads, particularly if applied to method calls that execute frequently. \nThus, we feel that this example shows that it would be worthwhile to further improve the approach to \ngenerating code for around advice. Finally, programmers should be aware of situations where after advice \ncould be used instead of around advice, since the overheads for after advice are lower. 5.3.2 Figure \nThe Figure benchmark illustrates the use of aspect-oriented pro\u00adgramming in a .gure editor [15]. Here \nwe have selected just one aspect from that example, namely to update the display whenever one of the \n.gure elements has been altered. There is an interface called FigureElement, and all shapes that the \neditor support implement that interface, for example the Point and Line classes. To capture any alterations \nto .gure elements, we de.ne a named pointcut: pointcut move(): call(void FigureElement.moveBy(int, int)) \n|| call(void Point.setX(int)) || call(void Point.setY(int)) || call(void Line.setP1(Point)) || call(void \nLine.setP2(Point)); The .rst use of call captures the moveBy operations on any of the implementations \nof FigureElement; the other disjuncts deal with alterations to individual classes. Now when do we want \nto update the display? Clearly whenever a move has occurred, but not when the move is part of a more \ncom\u00adplex operation that is itself a move. Furthermore we only want to update the display when the relevant \nmove has been successfully completed, not when it throws an exception. These considerations lead [15] \nto declare the following pointcut and advice: after() returning: move() &#38;&#38; !c.owbelow(move()) \n{Display.needsRepaint(); } The primitive c.owbelow checks that there is a move somewhere strictly below \nthe top of the call stack. One might argue that it is not necessary to use this primitive: it would be \npossible to explic\u00aditly write out all the composite operations. In that case, however, the pointcut depends \non intimate implementation detail, and is not robust to changes in that detail. In the present paper, \nthe purpose of the Figure benchmark is to examine the cost of using c.owbelow. We have thus disabled \nthe other aspects introduced in [15], using only the core .gure editor and the above advice. The core \nprogram is only a skeleton, and it does no interesting computation on its own. It is therefore to be \nexpected that there is a very high overhead as a proportion of the total computation time. This expectation \nis con.rmed by the .rst column of Table 4: the slowdown is about a factor of 32 compared to an equivalent, \nhand-coded version (where all the necessary calls to needsRepaint are inserted by hand into the core). \n  To understand this huge performance penalty, it is worthwhile to examine the numbers in more detail. \nIt appears that there is a great deal of allocation, as indicated by the EXECUTION SPACE MEASUREMENTS. \nFurthermore the tag mix reveals that the rel\u00adevant overheads lie in the administration of CFLOW ENTRY \nand CFLOW EXIT, as well as ADVICE TEST. The dynamic tests for c.owbelow are thus at the root of the problem. \nHowever, from the last row in our table we can conclude that all the dynamic tests are in fact runtime \nconstants so there is likely to be a signi.cant saving possible. As described in [23, 14], the AspectJ \ncompiler generates code to maintain a stack to keep track of each c.ow(P) pointcut. When a join point \nthat matches P is encountered, a new entry is pushed onto the stack; and when such a join point terminates, \nthe stack is popped. We examined the generated code using the Dava decom\u00adpiler to gain further insight. \nIn this example, the entries of the stack are zero length arrays of Object. In general these arrays are \nused to store variable bindings. A pointcut can bind variables through a number of primitives such as \nargs(x), which assigns the value of a join point parameter to x. If the pointcut P in c.ow(P) binds variables, \nwe need to keep track of them in the stack. In this benchmark program, the arrays have zero length because \nthere are no arguments to bind. Such a stack of zero length arrays could be more ef.ciently im\u00adplemented \nusing a counter; and the only check we need to make is that the argument of c.ow does not have variable \nbinders in it. This optimization was implemented by modifying the AspectJ compiler, and the results are \ndisplayed in the second column of Table 4. The results are a lot better, but there are still signi.cant \noverheads. The slowdown compared to the hand-woven version is a factor of 15.44. The overheads of this \ncounter-based implementation are due to the fact that it is necessary to maintain a counter for each \nc.ow in each thread. To this end the implementation keeps a mapping from threads to counters: upon each \npush, pop or is-empty operation, one .rst needs to retrieve the relevant counter for the current thread. \nTo improve upon this bookkeeping, note that the thread can be assumed to be the same throughout a method \nbody. It is therefore possible to retrieve the relevant counter once when the .rst c.ow operation is \ndone, store it in a .nal local variable, and then use the same counter throughout the method. To measure \nthe impact of this optimization, we decompiled the output of our modi.ed compiler, and applied the transformation \nby hand. The results are displayed in the third column of Table 4: the slowdown has now been brought \ndown to a factor of 3.78. Of course for this very simple benchmark, we know that there is only a single \nthread, and thus the thread-counter mapping is wholly unnecessary. The result of eliminating it from \nour code (again by editing the decompiled source) is shown in the penultimate column of Table 4. It further \nreduces the slowdown to a factor of 1.31. It would not be too dif.cult to implement this optimization, \nwith a conservative whole-program analysis to determine whether the application is single-threaded. In \n[30], it is argued that by building an accurate call graph that accounts for advice as well as ordinary \nmethod calls, one may often completely eliminate the dynamic tests for c.ow. That paper makes a lot of \nsimplifying assumptions, however, and in fact the language under consideration is a simple aspect-oriented \nvariant of Pascal. We expect, however, that the same techniques can be applied in the more general setting \nof Java, and we are working towards an im\u00adplementation using the Soot analysis framework [29], which \nwould truly be on a par with the hand-woven version. 5.3.3 LoD A very interesting application of AspectJ \nfor checking the Law of Demeter was proposed by Lieberherr, Lorenz and Wu [20] and the code to accompany \nthe paper is also available [21]. In the pa\u00adper they suggest two checkers, one for object form and another \nfor class form. We have used the object form checker as our bench\u00admark. The basic idea is that a program \nhas correct Law of Demeter object form when an object can only send messages to: itself, its arguments, \nits instance variables, a locally-constructed object or a returned object from a message sent to itself. \nTo achieve this check Lieberherr et al. have written a concise, but advanced collection of aspects which \nincludes relatively complex pointcuts, and the use of perc.ow, pertarget and c.ow. The basic idea behind \nthe checking code is that each calling con\u00adtext is associated with a hash table (through the use of perc.ow) \nand all valid (preferred) objects for that context are inserted into the hash table for that context. \nThen, at each method call, the checker veri.es that the method call uses only preferred objects, otherwise \nit is a violation of the Law of Demeter object form. In order to generate an interesting application \nof the checker, we applied it to the same simulator base code as used in Sections 5.3.1 and 5.2.1. We \nslightly modi.ed the Law of Demeter code so that each error would be reported only once (in the original \ncode an er\u00adror was reported once for each dynamic instance of the error, which led to large, dif.cult \nto read, output .les). After applying the Law of Demeter checker code (AspectJ code) to the simulator \ncode base (Java code), and executing the resulting woven code, the following three object form violations \nwere reported. !! LoD Object Violation !! call(double certrevsim.RevocationInfo. getNextUpdate()) at \nEndEntity.java:26 !! LoD Object Violation !! call(double certrevsim.RevocationInfo. getFirstDeltaUpdate()) \nat EndEntity.java:29 !! LoD Object Violation !! call(RevocationInfo certrevsim.Repository. requestRevocationInfo()) \nat Simulator.java:248 At .rst glance one might expect that the AspectJ overhead for this benchmark should \nbe small in relation to the amount of work done in each advice body (which includes inserting and testing \nfor membership in hash tables). However, as shown in Table 5 this was not the case. As demonstrated by \nthe column labelled Orig. , the original benchmark code has about 96% overhead, which is entirely unexpected. \nBy examining the ASPECTJTAG MIX metrics it is immediately obvious that c.ow is the problem, with 96% \nof the instructions and over 99% of the object allocations coming from CFLOW ENTRY and CFLOW EXIT. The \neffect of all these alloca\u00adtions has a huge impact on execution time, with garbage collection taking \n72.42 seconds, out of a total of 96.91 seconds. In order to examine this problem in more depth, we created \na second version of the benchmark using our modi.ed ajc which implements c.ow with counters instead of \nstacks ( Counters col\u00adumn in Table 5). As we saw in the Figure benchmark, the counters do improve performance \nsubstantially, reducing total running time  Orig. Counters Opt. Counters Single Thread Hand-woven 10 \n10 8 6 Instructions Loaded 616 645 642 347 189 Instructions Dead 233 246 270 89 25 Code Coverage (%) \n62 62 58 74 87   PROGRAM SIZE WITH JAVA LIBRARIES (WHOLE PROGRAM)     293 291 284 74948 74653 \n73325    # instr. (million bytecodes) 1623 495 299 229 114 Total time -client (sec) 8.90 4.23 1.03 \n0.36 0.27 JIT time -client (sec) 0.05 0.04 0.04 0.03 0.03 GC time -client (sec) 0.14 0.00 0.00 0.00 0.00 \nSlowdown vs. handcoded(\u00d7) 32.53 15.44 3.78 1.31 1.00 Time -client noinline (sec) 10.07 4.46 1.18 0.48 \n0.44 Slowdown vs. handcoded (\u00d7) 23.01 10.20 2.69 1.11 1.00 Time -interpreter (sec) 44.76 13.38 5.44 \n3.51 2.12 Slowdown vs. handcoded (\u00d7) 21.11 6.31 2.57 1.66 1.00 EXECUTION SPACE MEASUREMENTS (WHOLE PROGRAM) \nMem. Alloc. (million bytes) 374 1 1 1 1 Obj. Allocation Density (per kbc) 9.86 0.01 0.02 0.03 0.06 #Garbage \nCollections 488 0 0 0 0   ASPECTJMETRICS SUMMARIZING OVERHEAD  AspectJ Overhead % (whole) 92.54 75.55 \n#overhead/#advice (whole) 125.17 31.17 #advice/#total (whole) 0.01 0.02   ASPECTJTAG MIX FOR ALL \nINSTRUCTIONS (WHOLE PROG.) (%)  BASE CODE 6.72 22.02 ASPECT CODE 0.74 2.42 AspectJ Overhead (total) \n92.54 75.55    ADVICE EXECUTE 0.25 0.81 ADVICE ARG SETUP 0.62 2.02 ADVICE TEST 10.84 33.94 CFLOW \nENTRY 36.97 25.86 CFLOW EXIT 43.87 12.93      PEROBJECT ENTRY CLINIT INLINE ACCESS METHOD  ASPECTJTAG \nMIX FOR ALLOCATIONS ONLY (WHOLE PROG.) (%) BASE CODE 0.02 99.78 AspectJ Overhead (total) 99.98 0.22 \n ASPECTJMETRICS FOR SHADOWS (WHOLE PROGRAM) (%) Table 4: Figure Benchmark Measurements to 9.38 seconds \nand garbage collection time to 0.34 seconds. How\u00adever, there remains over 75% overhead due to CFLOW ENTRY \nand CFLOW EXIT, which is still higher than expected. We examined the benchmark and found that c.ow is \nused in two places, .rst in the de.nition of a pointcut, and second in a perc.ow clause. The pointcut \nde.nition is as follows. public pointcut scope(): !within(lawOfDemeter..*) &#38;&#38; !c.ow(withincode(* \nlawOfDemeter..*(..))) ; public pointcut StaticInitialization(): scope() &#38;&#38; staticinitialization(*); \npublic pointcut MethodCallSite(): scope() &#38;&#38; call(* *(..)); // ... followed by many other uses \nof scope() Note that the de.nition of the scope() pointcut contains a c.ow and then scope() is used within \nthe de.nition of many other point\u00adcuts. By examining the decompiled output of ajcwe determined that at \nleast 13 c.ow stacks are created for the same c.ow, pre\u00adsumably due to the inlining of the scope() pointcut \ninside the other pointcuts. Since all 13 stacks are updated on method entry and exit of some key methods, \nthis leads to enormous overheads. Since the states of all of these stacks are the same, there is clearly \nroom for improvement in the ajccode generation strategy, and further work will be needed to avoid the \ncreation of unneeded duplicate stacks. To show that most of the overhead is due to this use of c.ow and \nnot the perc.ow, we created a version of the benchmark that eliminated the c.ow clause in the de.nition \nof the scope() pointcut. This is safe for our benchmark because we know for our case it is not needed. \nThe performance measurements for this version are given in the column labelled No c.ow , and it is clear \nthat we have removed the majority of the c.ow overheads. Clearly programmers like to include c.ow pointcuts \nfor ease of speci.cation and for safety, so it seems important to work on ef.\u00adcient implementations for \nthem. By eliminating the multiple copies of stacks, and applying the ef.cient counter schemes presented \nin the previous section, it should be possible to greatly reduce the overheads due to c.ow. Even after \ndealing with the c.ow overheads, there still remains Orig. Counters No c.ow Classes Loaded 59 Instructions \nLoaded 33107 16606 11830 13742 7492 58 58 55    Classes Loaded 385 384 Instructions Loaded 122689 \n111450  EXECUTION TIME MEASUREMENTS (WHOLE PROGRAM) # instr. (million bytecodes) 2984 552 Total time \n-client (sec) 96.91 9.38 JIT time -client (sec) 0.55 0.40 111 1.16 0.22 GC time -client (sec) 72.42 0.34 \nTime -client noinline (sec) 97.18 9.17 0.06 1.10 Time -interpreter (sec) 158.78 15.14 2.18 EXECUTION \nSPACE MEASUREMENTS (WHOLE PROGRAM)   Mem. Alloc. (million bytes) 998 39 39 Obj. Allocation Density \n(per kbc) 11.78 0.68 3.40 #Garbage Collections 1104 42 42 ASPECTJMETRICS SUMMARIZING OVERHEAD AspectJ \nOverhead % (whole) 96.33 80.13 15.70 #overhead/#advice (whole) 30.68 4.71 0.19 #advice/#total (whole) \n0.03 0.17 0.81 AspectJ Runtime Lib % (whole) 90.33 25.60 12.75 ASPECTJTAG MIX FOR ALL INSTRUCTIONS (WHOLE \nPROG.) (%)   BASE CODE 0.53 2.84 2.83 ASPECT CODE 3.14 17.03 81.46 AspectJ Overhead (total) 96.33 80.13 \n15.70  ADVICE EXECUTE 0.008 0.04 0.21 ADVICE ARG SETUP 0.21 1.11 5.47 ADVICE TEST 0.22 1.53 2.24  \n AFTER RETURNING EXPOSURE 0.002 0.01 0.07 AFTER THROWING HANDLER 0.002 0.01 0.06   CFLOW ENTRY 44.06 \n51.80 4.04 CFLOW EXIT 51.83 25.55 3.26  PEROBJECT ENTRY CLINIT 0.001 0.005 0.02 ASPECTJTAG MIX FOR \nALLOCATIONS ONLY (WHOLE PROG.) (%)   BASE CODE 0.03 2.64 2.64 ASPECT CODE 0.27 25.83 25.84 AspectJ \nOverhead (total) 99.70 71.53 71.52  ADVICE ARG SETUP 0.27 25.71 25.71  CFLOW ENTRY 99.42 44.69 44.69 \n PEROBJECT ENTRY 0.009 0.89 0.89 CLINIT 0.002 0.23 0.22 Table 5: Law of Demeter Benchmark Measurements \nabout 16% overhead which is due mostly to the perc.ow and per\u00adtarget aspects. The pertarget overhead \nshows up in two ways. First, there are some signi.cant overheads for ADVICE ARG SET-UP(5.47%)andADVICE \nTEST(2.24%).Theseoverheadsarelarger than normal because the pertarget advice leads to extra code to be \ngenerated that checks if the aspect instance corresponding to the target already exists, and to allocate \na new aspect instance if one does not exist. Also, the space requirements for perc.ow and per\u00adtarget \nare signi.cant. The BASE CODE only accounts for 2.64% of the total allocations, whereas the perc.ow accounts \nfor 44.69% (shown in the bin for CFLOW ENTRY), and the pertarget accounts for 25.71% (shown in the bin \nfor ADVICE ARG SETUP, since this is where new aspect instances are created in the case of pertarget aspects). \nWe expect that at least some of these space overheads could be reduced.    5.4 Benchmark for performance \nimprovement The .nal benchmark in our set is somewhat different from the others in that the aspects used \nfor this benchmark were intended to improve upon the performance of an existing Java program. 5.4.1 *J \nPool This benchmark is drawn from our own tool set, namely the *J tool itself. The *J analyzer reads \nevents one-by-one from a trace .le (as described in Section 4.3). Each time it reads a new event, a new \nobject is allocated to hold this event; since there are potentially millions of events in a trace .le \nthis places signi.cant stress on the memory manager. However, it is a property of the implementation \nthat for any given trace .le, no more than the last two events will ever be in use at any one time, which \nmakes manual memory man\u00adagement of these objects possible (by reusing previously allocated ones that \nare guaranteed to no longer be in use, rather than allocat\u00ading new ones). This optimization is implemented \nby maintaining two pools of events, each pool containing one object of each of the various pos\u00adsible \nevent type. At any one moment, one pool is active and the other is inactive ; each time a new event would \nhave been allo\u00adcated, the appropriate type of event from the active pool is reused instead, and the active \nand inactive pools are swapped over. This guarantees that the last two events are always allocated from \ndiffer\u00adent pools, which ensures that events in use can never collide with each other. We wrote this optimization \nas a piece of around advice; in the original program a single method (newEvent) is used to allocate new \nevent objects, so this advice simply replaces calls to newEvent with code to reuse an object from the \nappropriate pool as described above. Of course, this could be implemented relatively simply by just replacing \nthe body of newEvent, but this would make it harder to disable the optimization easily if required. Multiple \ntrace .les can be read simultaneously by creating multiple objects of the ap\u00adpropriate class; therefore \nthe advice is implemented in a pertarget aspect, to ensure that different pools are used for each trace \n.le (the current implementation actually reads just one .le at a time, so the aspect could be implemented \nwithout using pertarget, but this would be rather more fragile). The results of this optimization are \ndetailed in Table 6. The .rst column, Aspect shows it implemented as a pertarget aspect as described \nabove; the second column ( Hand-woven ) is for a man\u00adual implementation. Finally the third column ( No \npooling ) shows the unoptimized version for comparison. In each case, the *J ana\u00adlyzer was run on a trace \ngenerated from a short run of a program to calculate the Fast Fourier Transform. 6 We have provided comparisons \nof running time with the unop\u00adtimized version; these show that introducing the aspect provides a speedup \nof about 23%. In fact, there is some overhead from weav\u00ading, since the version that applies pooling directly \nshows a speedup of about 52%. The amount of memory allocated drops by nearly a factor of 2, and the number \nof garbage collections and the total time spent garbage collection go down signi.cantly. 6. RELATED WORK \nMost work on dynamic metrics has focused on either address\u00ading a speci.c optimization problem such as \nmemory use (e.g., [7, 31]), or more generally (and voluminously) on software engineer\u00ading quality or \ncomplexity measures (e.g., [24, 34, 36]). More re\u00ad 6In order to reproduce the memory constraints imposed \nby a larger input, the total heap size is limited to 52 Mbytes for this benchmark. Aspect Hand-woven \nNo pool Classes Loaded 218 Instructions Loaded 46869 46305 28491 27269 27694 41 42 40    Classes Loaded \n 905 901 Instructions Loaded 184517 180992  EXECUTION TIME MEASUREMENTS (WHOLE PROGRAM)   # instr. \n(million bytecodes) 4232 4085 4185 Total time -client (sec) 24.52 19.84 30.18 JIT time -client (sec) \n0.66 0.65 0.64 GC time -client (sec) 7.41 3.42 13.47 Speedup vs. no pooling(\u00d7) 1.23 1.52 1.00 Time -client \nnoinline (sec) 26.44 20.95 31.58 Speedup vs. no pooling (\u00d7) 1.19 1.52 1.00 Time -interpreter (sec) 85.08 \n78.78 91.18 Speedup vs. no pooling (\u00d7) 1.08 1.16 1.00 EXECUTION SPACE MEASUREMENTS (WHOLE PROGRAM)  \n Mem. Alloc. (million bytes) 177 162 352 Obj. Allocation Density (per kbc) 0.49 0.51 1.03 #Garbage Collections \n46 45 81 ASPECTJMETRICS SUMMARIZING OVERHEAD AspectJ Overhead % (whole) 2.80 #overhead/#advice (whole) \n1.77 #advice/#total (whole) 0.02 AspectJ Runtime Lib % (whole) 0.00 ASPECTJTAG MIX FOR ALL INSTRUCTIONS \n(WHOLE PROG.) (%) BASE CODE 95.61 ASPECT CODE 1.59 AspectJ Overhead (total) 2.80  ADVICE EXECUTE \n0.11 ADVICE ARG SETUP  0.95 ADVICE TEST 0.64 PEROBJECT ENTRY  Table 6: *J Pool Benchmark Measurements \nlated work on analyzing programs through metrics is given in [8], along with a description of our overall \napproach. The performance of AspectJ programs has also been discussed and investigated in the literature, \nand typically it is assumed or demonstrated to some degree that aspects do not impose unreason\u00adable overhead. \nKiczales et al. s overview paper of AspectJ [16] for instance makes the pronouncement that (with respect \nto be\u00adfore/after advice) ...there should generally be no observable per\u00adformance overhead from these \nadditional method calls. Method calls inserted into code to support advice testing are assumed to be \nsimple and strict enough that the Just-In-Time compiler in most Java Virtual Machine implementations \nwill be able to inline the method call, and thus reduce any overhead to insigni.cance. The AspectJ FAQ \nreinforces that perception, claiming that most con\u00adstructions have little overhead, which could be optimized \naway by modern VM s. [35] (section 7.3). There are a few studies that actually measured the performance \nimpact of using aspects. Pace and Campo, for instance, analyzed regular and aspect-oriented versions \nof a temperature control bench\u00admark [6]. Although they found one style of implementation to be over 3 \ntimes slower than the original, a different aspect-oriented approach had only about 1% runtime overhead. \nThey attribute the former to the internal use of re.ection, and conclude that the impact may depend on \nthe problem under consideration. A more recent and larger study was done by Hilsdale and Hugunin [14], \nexamin\u00ading both runtime and compile-time performance issues. A naive implementation is shown to have \nquite poor performance (for a logging implementation they get a 2900% overhead versus a hand\u00adcoded implementation), \nbut they improve that to an unlikely to be noticeable 22% runtime overhead for an optimized version. \nAgain they attribute the former very poor performance largely to the use of re.ection.      In the \ncontext of middleware, Zhang and Jacobsen [37] demon\u00adstrate that an aspect version of a CORBA/ORB benchmark \nhas neg\u00adligible runtime overhead. They argue that an AspectJ implementa\u00adtion should have no overhead \nsince it is just specifying the same code in different ways (in the aspect versus in the program). In \ntheir case, however, an aspect-oriented approach signi.cantly sim\u00adpli.ed the program design (overall \ncode reduction of 9%, fewer methods per class on average, etc), so they are actually comparing an optimized \ndesign to an unoptimized design. The fact that the optimized design only achieves the same speed as the \nunoptimized is an argument that a signi.cant overhead may well be present. In their analysis, Zhang and \nJacobsen also give data for a number of software engineering complexity metrics, and use that data to \nshow that the aspect-oriented approach is quantitatively simpler. Complexity is also considered by Zhao, \nwho proposes a speci.c complexity metric suite for aspect oriented programming [38]. We are focusing \non performance and execution time costs, rather then complexity. Clearly particularities of the implementation \nof aspects have a large impact on the overhead. Sereni and de Moor describe a bet\u00adter implementation \nof pointcut designators as well as a compiler .ow analysis that can reduce the overhead by eliminating \nmany instances of runtime matching [30]. That paper is mostly a theoret\u00adical study, dealing with a small \ntoy language, and wholly without performance experiments. The results presented here suggest that such \noptimization techniques may be quite important in practice. Performance analyses have also been done \non dynamic weav\u00ading approaches where an aspect is applied to a running program. Dynamic weaving generally \naims to enhance capabilities, allow\u00ading for instant hot .xes to be applied to running code [27, 28]. \nPopovici et al. show an aspect-aware Java Virtual Machine that im\u00adposes relatively little overhead when \naspects are inactive (1.5% 8% slowdown over a regular JVM), though that increases dramatically for active \njoin points (1.3\u00d7 5\u00d7 slower than a statically-woven ver\u00adsion). Finally, more generic pro.ling methods \nhave been applied to As\u00adpectJ programs. Hall s CPPROFJ [11] for instance, does call-path pro.ling of \nboth pure Java and (limited) AspectJ programs, allow\u00ading the runtime cost of various method execution \nsequences to be determined. CPPROFJ is sampling-based and is naturally much more coarse-grained than \nour approach. 7. CONCLUSIONS We have presented a tool set and a systematic method for an\u00adalyzing the \ndynamic behaviour of AspectJ programs. The main technical contributions are the de.nition of new metrics, \nas well as a novel method of computing these metrics. In particular the idea of compile-time tags that \nare dynamically propagated allows us to accurately attribute costs to speci.c language features. As discussed \nin Section 4, the overall system for collecting our data is complex modi.cations to *J and ajc were non-trivial, \nand this system constitutes a contribution by itself. One of the more inter\u00adesting and dif.cult components \nof the system is the propagation strategy, which has to be carefully designed in order to attribute data \ncorrectly. The general paradigm could be transferred to similar situations, for example when compiling \nML to Java bytecode [4]. The same ideas could be integrated in a compiler that weaves the instrumentation \nwith the generated code, instead of using a tool like JVMPI, which was the route taken in this paper. \nOur benchmark set provides the .rst collection of programs suit\u00adable for discussing performance of AspectJ. \nThe benchmarks we have chosen provide a good cross section of different uses of the language. We are \ncontinuing to extend the collection, in particular using some of the examples from [19]. One small dif.culty \nconsists of programs that make use of re.ection: at present our propagation tools are unable to cope \nwith re.ective calls, and wrongly attribute the cost of such calls to the base program, never to the \naspect. This does not invalidate our measurements of overheads, only the num\u00adbers for BASE CODE and ASPECT \nCODE. The conventional wisdom that AspectJ does not introduce over\u00adheads seems to be explained by typical \naspect usage. First, advice generally applies to user code, yet typical Java programs spend most of their \ntime in library calls. As a percentage of the total execution time, the cost of advice is therefore insigni.cant \nin such applications. The Tetris benchmark illustrates this phenomenon. Some of our benchmarks (in particular \nDCM) show the opposite behaviour, where the advice is so expensive that the overheads of applying it \nare dwarfed. Finally, intertype declarations have very little overheads, except when it concerns the \nintroduction of new constructors. This is demonstrated by Bean and ProdLine respec\u00adtively. Contrary to \npopular belief, we did however also .nd signi.cant overheads. This has led to the following guidelines \nfor AspectJ usage, as well as promising areas for future compiler research: Loose pointcuts. It is easy \nto write a pointcut that matches too many join points. Even when some of the dynamic tests fail, such \nloose speci.cation can introduce signi.cant overheads. It is par\u00adticularly important to avoid around \nadvice that can apply to itself, as this forces the introduction of closures. This was illustrated in \nthe .rst two versions of the Nullcheck benchmark. Sometimes it is however not possible to tighten pointcuts \nto avoid this situation, so a more careful consideration of the use of closures is a fruitful topic for \nfuture research. Advice that is too generic. When using the very generic form of around, this causes \na signi.cant amount of boxing and unboxing to convert arguments to the right form. Unwarranted use of \naround. Because of the above, it is gener\u00adally preferable to eschew around in favour of after returning \nwhen possible. The most striking example we found of this phenomenon occurred in the .nal version of \nthe Nullcheck benchmark. In fact, that improvement was not noticed by a number of seasoned AspectJ users \nto whom we showed the original code, so this is an instance where our methods give new insights. C.ow. \nIt is tempting to write pointcuts using c.ow, but often this introduces signi.cant overheads. This was \nillustrated by three sep\u00adarate benchmarks, namely Nullcheck, Figure and LoD. Where pos\u00adsible, it is better \nto use withincode in lieu of c.ow, but this is ar\u00adguably less robust with respect to refactoring. Because \nit is not always possible to eliminate c.ow, we investigated various ways of improving its implementation: \n When there is no argument binding, the current use of stacks in ajc can be replaced by counters. We \nhave in fact imple\u00admented this optimization in ajc, and found it to be highly effective.  The use of \nsuch counters is still somewhat expensive due to the fact that we have to maintain one for each thread. \nIf the application is known to be single-threaded, signi.cant sav\u00adings are possible, as there is no need \nto maintain a mapping between threads and counters.   A whole-program analysis based on the call graph \ncan elimi\u00adnate all runtime overheads of c.ow. An initial study in this di\u00adrection, for a very small toy \nlanguage, was undertaken in [30]. Pertarget. The use of per clauses to control aspect creation carries \na non-negligible overhead, as demonstrated by the *J Pool bench\u00admark. It might be possible to devise \na static analysis which detects that only one instance will be created in a particular application. For \nall programmers with an interest in aspect-orientation, it is important to understand the implications \nof using aspects on the behaviour of their programs. The tools we have presented are an important step \ntowards this goal, but perhaps even more important is the construction of a representative set of benchmarks \nthat is ac\u00adcepted by the whole community. We hope that the benchmarks presented here provide a starting \npoint, and that others will join us in extending and improving it. We will be making a public release \nof the *J tool so that others can collect our Java-based metrics for their own programs. To ben\u00ade.t from \nthese tools, one also needs a compiler that assigns static tags; for now we are using a modi.ed version \nof the standard As\u00adpectJ compiler ajc. Inspired by the results of the present paper, we have begun the \nimplementation of an optimizing AspectJ compiler based on Soot [29], and this compiler includes that \ntagging scheme. 8. REFERENCES [1] Andr\u00b4e Arnes. Certi.cate revocations performance simulation project. \nhttp://www.pvv.ntnu.no/ andrearn/ certrev/sim.html, 2000. [2] R. Dale Asberry. Aspect oriented programming \n(AOP): Using AspectJ to implement and enforce coding standards. http://www.daleasberry.com/newsletters/ \n 200210/20021002.shtml, 2002. [3] AspectJ Eclipse Home. The AspectJ home page. http://eclipse.org/aspectj/, \n2003. [4] Nick Benton, Andrew Kennedy, and Claudio Russell. Compiling standard ML to Java bytecodes. \nIn 3rd ACM SIGPLAN conference on Functional Programming, 1998. [5] Curtis Clifton. MultiJava: Design, \nimplementation and evaluation of a Java-compatible language supporting modular open classes and symmetric \nmultiple dispatch. Technical Report 01-10, Department of Computer Science, Iowa State University, Ames, \nIowa, 2001. [6] J. Andr\u00b4es D\u00b4iaz Pace and Marcelo R. Campo. Analyzing the role of aspects in software \ndesign. Commun. ACM, 44(10):66 73, 2001. [7] Sylvia Dieckmann and Urs H\u00a8olzle. A study of the allocation \nbehavior of the SPECjvm98 Java benchmarks. In Proceedings of ECOOP 1999, LNCS 1628, pages 92 115, 1999. \n[8] Bruno Dufour, Karel Driesen, Laurie Hendren, and Clark Verbrugge. Dynamic metrics for Java. In Proceedings \nof the 18th ACM SIGPLAN conference on Object-oriented programing, systems, languages, and applications, \npages 149 168. ACM Press, 2003. [9] Gustav Evertsson. Tetris in AspectJ, 2003. http://www. guzzzt.com/coding/aspecttetris.shtml. \n[10] Joseph D. Gradecki and Nicholas Lesiecki. Mastering AspectJ: Aspect-Oriented Programming in Java. \nWiley, 2003. [11] Robert J. Hall. CPPROFJ: aspect-capable call path pro.ling of multi-threaded Java applications. \nIn Proceedings of the 17th IEEE Conference on Automated Software Engineering (ASE 02), pages 107 116, \nNovember 2002. [12] Youssef Hassoun, Roger Johnson, and Steve Counsell. A dynamic runtime coupling metric \nfor meta-level architectures. In Proceedings of the 8th European Conference on Software Maintenace and \nReengineering, page (to appear). IEEE Computer Society Press, March 2004. [13] Youssef Hassoun, Roger \nJohnson, and Steve Counsell. Emprical validation of a dynamic coupling metric. Technical Report BBKCS-04-03, \nBirbeck College London, March 2004. [14] Erik Hilsdale and Jim Hugunin. Advice weaving in AspectJ. In \nK. Lieberherr, editor, Aspect-oriented Software Development (AOSD 2004). ACM Press, 2004. [15] Gregor \nKiczales, Erik Hilsdale, Jim Hugunin, Mik Kersten, Jeffrey Palm, and William Griswold. Getting started \nwith AspectJ. Commun. ACM, 44(10):59 65, 2001. [16] Gregor Kiczales, Erik Hilsdale, Jim Hugunin, Mik \nKersten, Jeffrey Palm, and William G. Griswold. An overview of AspectJ. In J. Lindskov Knudsen, editor, \nEuropean Conference on Object-oriented Programming, volume 2072 of Lecture Notes in Computer Science, \npages 327 353. Springer, 2001. [17] Gregor Kiczales, John Lamping, Anurag Menhdekar, Chris Maeda, Cristina \nLopes, Jean-Marc Loingtier, and John Irwin. Aspect-oriented programming. In Mehmet Aksit and Satoshi \nMatsuoka, editors, European Conference on Object-oriented Programming, volume 1241 of Lecture Notes in \nComputer Science, pages 220 242. Springer, 1997. [18] I. Kiselev. Aspect-oriented programming with AspectJ. \nSAMS, 2002. [19] Ramnivas Laddad. AspectJ in Action. Manning, 2003. [20] Karl Lieberherr, David H. Lorenz, \nand Pengcheng Wu. A case for statically executable advice: checking the law of Demeter with AspectJ. \nIn Proceedings of the 2nd international conference on Aspect-oriented software development, pages 40 \n49. ACM Press, 2003. [21] Karl Lieberherr, David H. Lorenz, and Pengcheng Wu. A case for statically executable \nadvice: Checking the law of demeter with AspectJ. Code available from URL: http://www.ccs.neu.edu/home/lorenz/ \npapers/aosd2003lod/, 2003. [22] Roberto E. Lopez-Herrejon and Don Batory. Using AspectJ to implement \nproduct-lines: A case study. Technical report, University of Texis at Austin, September 2002. [23] Hidehiko \nMasuhara, Gregor Kiczales, and Chris Dutchyn. A compilation and optimization model for aspect-oriented \nprograms. In Compiler Construction, volume 2622 of Springer Lecture Notes in Computer Science, pages \n46 60, 2003. [24] Jean Mayrand, Jean-Franois Patenaude, Ettore Merlo, Michel Dagenais, and Bruno Lagu\u00a8e. \nSoftware assessment using metrics: A comparison across large C++ and Java systems. Ann. Softw. Eng., \n9(1-4):117 141, 2000. [25] Jerome Miecznikowski and Laurie Hendren. Decompiling Java bytecode: Problems, \ntraps and pitfalls. In Compiler Construction, 11th International Conference, volume 2304 of LNCS, pages \n111 127, April 2002. [26] Todd Millstein, Mark Reay, and Craig Chambers. Relaxed MultiJava: Balancing \nextensibility and modular typechecking. In OOPSLA 2003, pages 224 240. ACM Press, 2003. [27] Andrei Popovici, \nGustavo Alonso, and Thomas Gross. Just-in-time aspects: ef.cient dynamic weaving for Java. In Proceedings \nof the 2nd international conference on Aspect-oriented software development, pages 100 109. ACM Press, \n2003. [28] Andrei Popovici, Thomas Gross, and Gustavo Alonso. Dynamic weaving for aspect-oriented programming. \nIn Proceedings of the 1st international conference on Aspect-oriented software development, pages 141 \n147. ACM Press, 2002. [29] McGill University Sable Research Group. Soot: a Java optimization framework, \n1998-2003. [30] Damien Sereni and Oege de Moor. Static analysis of aspects. In Proceedings of the 2nd \nInternational Conference on Aspect-Oriented Software Development (AOSD), pages 30 39, 2002. [31] Ye.m \nShuf, Mauricio J. Serrano, Manish Gupta, and Jaswinder Pal Singh. Characterizing the memory behavior \nof Java workloads: a structured view and opportunities for optimizations. In Proceedings of the 2001 \nACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems, pages 194 205. \nACM Press, 2001. [32] Raja Vall\u00b4ee-Rai, Etienne Gagnon, Laurie J. Hendren, Patrick Lam, Patrice Pominville, \nand Vijay Sundaresan. Optimizing Java bytecode using the Soot framework: Is it feasible? In Compiler \nConstruction, 9th International Conference (CC 2000), pages 18 34, 2000. [33] Mitchell Wand, Gregor Kiczales, \nand Christopher Dutchyn. A semantics for advice and dynamic join points in aspect-oriented programming. \nIn Foundations of Aspect-Oriented Languages (FOAL), Workshop at AOSD 2002, Technical Report TR #02-06, \npages 1 8. Iowa State University, 2002. [34] Michalis Xenos, D. Stavrinoudis, K. Zikouli, and D. Christodoulakis. \nObject-oriented metrics-asurvey.In Proceedings of the FESMA 2000, Federation of European Software Measurement \nAssociations, 2000. [35] Xerox Corporation. Frequently asked questions about AspectJ, revision 1.8, 2003. \nhttp://dev.eclipse.org/viewcvs/ indextech.cgi/aspectj-home/doc/faq.html. [36] Sherif M. Yacoub, Hany \nH. Ammar, and Tom Robinson. Dynamic metrics for object oriented designs. In Proceedings of the 6th International \nSymposium on Software Metrics, page 50. IEEE Computer Society, 1999. [37] Charles Zhang and Hans-Arno. \nJacobsen. Quantifying aspects in middleware platforms. In Proceedings of the 2nd international conference \non Aspect-oriented software development, pages 130 139. ACM Press, 2003. [38] Jianjun Zhao. Towards a \nmetrics suite for aspect-oriented software. Technical Report SE-136-25, Information Processing Society \nof Japan (IPSJ), March 2002. http: //citeseer.nj.nec.com/zhao02towards.html. Appendix I: Tags General \nTags BASE CODE: This tag represents instructions that are not inter\u00ad preted as AspectJ overhead or are \nnot part of an advice body. They represent the base program that exists before weaving. ASPECT CODE: \nThis tag represents the default for any instruction that is executed from an aspect, regardless of where \nit was origi\u00adnally de.ned. It is propagated, so that, for example, the body of a method call from advice \nwill receive the ASPECT CODE tag. NO TAG: This is a special tag inserted by the compiler which is meant \nto be overwritten by a propagated tag during analysis. An instruction with this tag is to be interpreted \nequivalently to an in\u00adstruction with no tag at all. It is a necessary consequence of the way tags are \nencoded in a code attribute. Instructions in library classes, which have not been explicitly tagged, \nare also assumed to have NO TAG. Tags to support intertype declarations INTERMETHOD: An intertype method \ndeclaration results in the body of the new method being compiled into a method on the as\u00adpect class, \nand a dispatch method being added to the target class. The instructions in this dispatch method have \nthis tag. INTERFIELDGET, INTERFIELDSET: Some intertype .eld declara\u00adtions result in accessor methods \nbeing woven into the target class. The instructions in these accessor methods have these tags. INTERFIELDINIT: \nIntertype .eld declarations result in initializa\u00adtion code being woven into either the target class s \nconstructor, or its static initializer. These instructions invoke initialization meth\u00adods on the aspect \nto handle variable initialization. This initializa\u00adtion code has this tag. INTERCONSTRUCTOR PRE, INTERCONSTRUCTOR \nPOST: If an as\u00adpect has an intertype constructor declaration two methods are cre\u00adated on the aspect: \na preInterConstructor method and a postInter-Constructor method. A new constructor method is added to \nthe class, and it invokes both of these methods. The instructions that load these methods arguments and \ninvoke these methods have these tags. INTERCONSTRUCTOR CONVERSION: This represents overhead in\u00advolved \nin calling methods on org.aspectj.runtime.internal.Conver\u00adsions from within a constructor added by an \nintertype constructor declaration. Tags applying to all kinds of advice (before, after and around) ADVICE \nEXECUTE: This tag represents the overhead associated with executing the method implementing a piece of \nadvice. Ad\u00advice bodies are compiled as methods in the aspect class. When an aspect with advice is woven \ninto a base class, an invoke instruction for the advice method is added to the relevant join point shadows. \nADVICE ARG SETUP: This tag represents the overhead associated with acquiring an aspect instance at a \njoin point at which advice is to be executed, and exposing arguments to the advice body. At least one \ninstruction of this kind will precede an advice execution instruction. ADVICE TEST: When it cannot be \nstatically determined whether an advice body should be executed at all join points corresponding to the \njoin point shadow at which the advice invocation instructions have been added, then those invocation \ninstructions are wrapped in a test. The instructions corresponding to this test have this tag. Tags applying \nto around advice only AROUND CONVERSION: This represents the conversion of argu\u00adments and return values \nrelated to a proceed() call within around advice. This conversion is done by making calls to methods \non org.aspectj.runtime.internal.Conversions, which convert between primitive types and objects.   \n    AROUND CALLBACK, AROUND PROCEED: Both of these tags rep\u00adresent an overhead involved in making \na proceed() call from within around advice. One of these tags, AROUND CALLBACK, is spe\u00adci.c to the run \nmethod on closure classes. CLOSURE INIT: Advice advice may result in the creation of clo\u00adsure classes. \nWhen it does,the instructions in the constructors of these classes have this tag. Tags applying to after \nadvice only AFTER RETURNING EXPOSURE: This tag represents the overhead involved in exposing the value \nreturned at a join point to the body of a piece of after advice. AFTER THROWING HANDLER: In order to \nsupport after and after throwing advice, exception handling code is inserted which catches any exception, \nexecutes any pertinent advice, and then rethrows the original exception. The instructions responsible \nfor this have this tag. Tags to support the c.ow pointcuts and perc.ow aspects CFLOW ENTRY, CFLOW EXIT: \nThe c.ow and c.owbelow point\u00adcuts require that a representation of the call stack be managed dur\u00ading \nthe execution of the program. At every relevant join point shadow, this representation must be updated. \nInstructions for doing so receive one of these tags. An aspect that is declared with perc.ow or perc.owbelow \nclause will also lead to instructions with this tag. Tags to support perthis and pertarget aspects PEROBJECT \nENTRY: By default, aspect instances are singletons. They can however be associated on a per-object basis, \neither with the execution or target objects at join points selected by a given pointcut. The instructions \ninserted at join point shadows matched by the pointcut to manage these instances have this tag. PEROBJECT \nGET, PEROBJECT SET: These accessor methods are added to a class to acquire instances of an aspect that \nis declared pertarget or perthis. Tag for exception softening due to declare soft EXCEPTION SOFTENER: \nThis tag represents the overhead involved in softening exceptions. The declare soft declaration in an \nas\u00adpect results in exceptions of a given type, thrown from within join points selected by a given pointcut, \nbeing wrapped in the unchecked org.aspectj.SoftException, which is then thrown. Tags to handle privileged \naspects PRIV METHOD, PRIV FIELD GET, PRIV FIELD SET: In order to support privileged aspects, public wrapper \nmethods for the class s private methods, and public accessor methods for the class s private .elds, are \ninserted during weaving. The instructions in these new methods have these tags. Miscellaneous aspect \ntags CLINIT: The instructions in the static initializer of the aspect class have this tag. The static \ninitializer may setup the default singleton instance of the aspect or setup the c.ow stack, if necessary. \nIn\u00adstructions woven into the static initializer of a base program class, such as for initializing the \nstatic join point information, also have this tag. INLINE ACCESS METHOD: This tag represents the overhead \nin\u00advolved in calling a method de.ned on an aspect when there is a static dispatch method. The instructions \nof the static dispatch method have this tag.               \n\t\t\t", "proc_id": "1028976", "abstract": "<p>This paper proposes and implements a rigorous method for studying the dynamic behaviour of AspectJ programs. As part of this methodology several new metrics specific to AspectJ programs are proposed and tools for collecting the relevant metrics are presented. The major tools consist of: (1) a modified version of the AspectJ compiler that tags bytecode instructions with an indication of the cause of their generation, such as a particular feature of AspectJ; and (2) a modified version of the *J dynamic metrics collection tool which is composed of a JVMPI-based trace generator and an analyzer which propagates tags and computes the proposed metrics. This dynamic propagation is essential, and thus this paper contributes not only new metrics, but also non-trivial ways of computing them.</p> <p>We furthermore present a set of benchmarks that exercise a wide range of AspectJ's features, and the metrics that we measured on these benchmarks. The results provide guidance to AspectJ users on how to avoid efficiency pitfalls, to AspectJ implementors on promising areas for future optimization, and to tool builders on ways to understand the runtime behaviour of AspectJ.</p>", "authors": [{"name": "Bruno Dufour", "author_profile_id": "81100195102", "affiliation": "McGill University", "person_id": "PP33024229", "email_address": "", "orcid_id": ""}, {"name": "Christopher Goard", "author_profile_id": "81100333881", "affiliation": "McGill University", "person_id": "P698413", "email_address": "", "orcid_id": ""}, {"name": "Laurie Hendren", "author_profile_id": "81100646110", "affiliation": "McGill University", "person_id": "PP14221385", "email_address": "", "orcid_id": ""}, {"name": "Oege de Moor", "author_profile_id": "81100198102", "affiliation": "Oxford University", "person_id": "PP14078760", "email_address": "", "orcid_id": ""}, {"name": "Ganesh Sittampalam", "author_profile_id": "81100259191", "affiliation": "Oxford University", "person_id": "PP14098545", "email_address": "", "orcid_id": ""}, {"name": "Clark Verbrugge", "author_profile_id": "81100085529", "affiliation": "Oxford University", "person_id": "PP39026529", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1028976.1028990", "year": "2004", "article_id": "1028990", "conference": "OOPSLA", "title": "Measuring the dynamic behaviour of AspectJ programs", "url": "http://dl.acm.org/citation.cfm?id=1028990"}