{"article_publication_date": "10-01-2004", "fulltext": "\n Chianti: A Tool for Change Impact Analysis of Java Programs Xiaoxia Ren1, Fenil Shah2, Frank Tip3, \nBarbara G. Ryder1, and Ophelia Chesley1* Division of Computer and Information Sciences1 IBM Software \nGroup2 IBM T.J. Watson Research Center3 Rutgers University 17 Skyline Drive P.O. Box 704 110 Frelinghuysen \nRoad Hawthorne, NY 10532, USA Yorktown Heights, NY 10598, USA Piscataway, NJ 08854-8019, USA fenils@us.ibm.com \ntip@watson.ibm.com {xren,ryder,ochesley}@cs.rutgers.edu  ABSTRACT This paper reports on the design \nand implementation of Chianti, a change impact analysis tool for Java that is imple\u00admented in the context \nof the Eclipse environment. Chianti analyzes two versions of an application and decomposes their di.erence \ninto a set of atomic changes. Change impact is then reported in terms of a.ected (regression or unit) \ntests whose execution behavior may have been modi.ed by the applied changes. For each a.ected test, Chianti \nalso deter\u00admines a set of a.ecting changes that were responsible for the test s modi.ed behavior. This \nlatter step of isolating the changes that induce the failure of one speci.c test from those changes that \nonly a.ect other tests can be used as a debugging technique in situations where a test fails unex\u00adpectedly \nafter a long editing session. We evaluated Chianti on a year (2002) of CVS data from M. Ernst s Daikon \nsystem, and found that, on average, 52% of Daikon s unit tests are a.ected. Furthermore, each af\u00adfected \nunit test, on average, is a.ected by only 3.95% of the atomic changes. These .ndings suggest that our \nchange impact analysis is a promising technique for assisting devel\u00adopers with program understanding \nand debugging. Categories and Subject Descriptors D.2.5 [Software Engineering]: Testing and Debugging; \nD.2.6 [Software Engineering]: Programming Environ\u00adments; F.3.2 [Logics and Meanings of Programs]: Se\u00admantics \nof Programming Languages Program analysis  General Terms Algorithms, Measurement, Languages, Reliability \n* This research was supported by NSF grant CCR-0204410 and in part by REU supplement CCR-0331797. Permission \nto make digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior speci.c permission and/or a fee. OOPSLA 04, Oct. \n24-28, 2004, Vancouver, British Columbia, Canada. Copyright 2004 ACM 1-58113-831-8/04/0010 ...$5.00. \n Keywords Change impact analysis, regression test, unit test, analysis of object-oriented programs 1. \nINTRODUCTION The extensive use of subtyping and dynamic dispatch in object-oriented programming languages \nmake it di.cult to understand value .ow through a program. For example, adding the creation of an object \nmay a.ect the behavior of virtual method calls that are not lexically near the alloca\u00adtion site. Also, \nadding a new method de.nition that over\u00adrides an existing method can have a similar non-local e.ect. \nThis nonlocality of change impact is qualitatively di.erent and more important for object-oriented programs \nthan for imperative ones (e.g., in C programs a precise call graph can be derived from syntactic information \nalone, except for the typically few calls through function pointers [14]). Change impact analysis [3, \n13, 15, 21, 16] consists of a collection of techniques for determining the e.ects of source code modi.cations, \nand can improve programmer produc\u00adtivity by: (i) allowing programmers to experiment with dif\u00adferent edits, \nobserve the code fragments that they a.ect, and use this information to determine which edit to select \nand/or how to augment test suites, (ii) reducing the amount of time and e.ort needed in running regression1 \ntests, by de\u00adtermining that some tests are guaranteed not to be a.ected by a given set of changes, and \n(iii) reducing the amount of time and e.ort spent in debugging, by determining a safe approximation of \nthe changes responsible for a given test s failure [21, 19]. The change impact analysis method presented \nin this pa\u00adper presumes the existence of a suite T of regression tests associated with a Java program \nand access to the original and edited versions of the code. Our analysis comprises the following steps: \n1. A source code edit is analyzed to obtain a set of in\u00adterdependent atomic changes A, whose granularity \nis (roughly) at the method level. These atomic changes include all possible e.ects of the edit on dynamic \ndis\u00adpatch. 2. Then, a call graph is constructed for each test in T . Our method can use either dynamic \ncall graphs that have been obtained by tracing the execution of the  1In the rest of this paper, we \nwill use the term regression test to refer to unit tests and other regression tests. tests, or static \ncall graphs that have been constructed by a static analysis engine. In this paper, we use dy\u00adnamic call \ngraphs2 . 3. For a given set T of regression tests, the analysis de\u00adtermines a subset T . of T that is \npotentially a.ected by the changes in A, by correlating the changes in A against the call graphs for \nthe tests in T in the original version of the program. 4. Finally, for a given test ti .T ', the analysis \ncan de\u00adtermine a subset A. of A that contains all the changes that may have a.ected the behavior of ti. \nThis is ac\u00adcomplished by constructing a call graph for ti in the edited version of the program, and correlating \nthat call graph with the changes in A.  The primary goal of our research is to provide programmers with \ntool support that can help them understand why a test is suddenly failing after a long editing session \nby isolating the changes responsible for the failure. There are some interesting similarities and di.erences \nbe\u00adtween the work presented in this paper, and previous work on regression test selection and change \nimpact analysis. Step 3 above is similar in spirit to previous work on regres\u00adsion test selection. However, \nunlike previous approaches our technique does not rely on a pairwise comparison of high\u00adlevel program \nrepresentations such as control .ow graphs (see, e.g. [20]) or Java InterClass Graphs [10]. Our work \ndi.ers from previous approaches for dynamic change im\u00adpact analysis [13, 15, 16] in the sense that these \nprevious approaches are primarily concerned with the problem of de\u00adtermining a subset of the methods \nin a program that were a.ected by a given set of changes. In contrast, step 4 of our technique is concerned \nwith the problem of isolating a subset of the changes that a.ect a given test. In addition, our approach \ndecomposes the code edit into a set of seman\u00adtically meaningful, interdependent atomic changes which \ncan be used to generate intermediate program versions, in order to investigate the cause of unexpected \ntest behavior. These, and other connections to related work, will be further explored in Section 6. This \npaper reports on the engineering of Chianti, a proto\u00adtype change impact analysis tool, and its validation \nagainst the 2002 revision history (taken from the developers CVS repository) of Daikon, a realistic Java \nsystem developed by M. Ernst et al. [7]. Essentially, in this initial study we substituted CVS updates \nobtained at intervals throughout the year for programmer edits, thus acquiring enough data to make some \ninitial conclusions about our approach. We present both data measuring the overall e.ectiveness of the \nanalysis and some case studies of individual CVS updates. Since the primary goal of our research has \nbeen to assist pro\u00adgrammers during development, Chianti has been integrated closely with Eclipse, a widely \nused open-source development environment for Java (see www.eclipse.org). The main contributions of this \nresearch are as follows: Demonstration of the utility of the basic change impact analysis framework of \n[21], by implementing a proof-of\u00adconcept prototype, Chianti, and applying it to Daikon, a moderate-sized \nJava system built by others. 2Our previous study used static call graphs [19]. Extension of the originally \nspeci.ed techniques [21] to handle the entire Java language, including such con\u00adstructs as anonymous \nclasses and overloading. This work entailed extension of the model of atomic changes and their interdependences. \n Experimental validation of the utility of change im\u00adpact analysis by determining the percentages of \naf\u00adfected tests and a.ecting changes for 40 versions of Daikon in 2002. For the 39 sets of changes between \nthese versions, we found that, on average, 52% of the tests are potentially a.ected. Moreover, for each \npo\u00adtentially a.ected test, on average, only 3.95% of the atomic changes a.ected it. This is a promising \nresult with regard to the utility of our technique for enhanc\u00ading program understanding and debugging. \n In Section 2, we give intuition about our approach through an example. In Section 3, the model of atomic \nchanges is discussed, as well as engineering issues arising from handling Java constructs that were previously \nnot modeled. The im\u00adplementation of Chianti is described in Section 4. Section 5 describes the experimental \nsetup and presents the empirical .ndings of the Daikon study. Related work and conclusions are summarized \nin Sections 6 and 7, respectively.  2. OVERVIEW OF APPROACH This section gives an informal overview \nof the change im\u00adpact analysis methodology originally presented in [21]. Our approach .rst determines, \ngiven two versions of a program and a set of tests that execute parts of the program, the af\u00adfected tests \nwhose behavior may have changed. Our method is safe [20] in the sense that this set of a.ected tests \ncontains at least every test whose behavior may have been a.ected. Then, in a second step, for each test \nwhose behavior was a.ected, a set of a.ecting changes is determined that may have given rise to that \ntest s changed behavior. Our method is conservative in the sense that the computed set of a.ecting changes \nis guaranteed to contain at least every change that may have caused changes to the test s behavior. We \nwill use the example program of Figure 1(a) to il\u00adlustrate our approach. Figure 1(a) depicts two versions \nof a simple program comprising classes A, B, and C. The original version of the program consists of all \nthe program text except for the 7 program fragments shown in boxes; the edited version of the program \nconsists of all the pro\u00adgram text including the program fragments shown in boxes. Associated with the \nprogram are 3 tests, Tests.test1(), Tests.test2(), and Tests.test3(). Our change impact analysis relies \non the computation of a set of atomic changes that capture all source code mod\u00adi.cations at a semantic \nlevel that is amenable to analysis. We currently use a fairly coarse-grained model of atomic changes, \nwhere changes are categorized as added classes (AC), deleted classes (DC), added methods (AM), deleted \nmethods (DM), changed methods (CM), added .elds (AF), deleted .elds (DF), and lookup (i.e., dynamic dispatch) \nchanges (LC)3 . We also compute syntactic dependences between atomic changes. Intuitively, an atomic \nchange A1 is dependent on 3There are a few more categories of atomic changes that are not relevant for \nthe example under consideration that will be presented in Section 3. another atomic change A2 if applying \nA1 to the original ver\u00adsion of the program without also applying A2 results in a syntactically invalid \nprogram (i.e., A2 is a prerequisite for A1). These dependences can be used to determine that cer\u00adtain \nchanges are guaranteed not to a.ect a given test, and to construct syntactically valid intermediate versions \nof the program that contain some, but not all atomic changes. It is important to understand that the \nsyntactic dependences do not capture semantic dependences between changes (con\u00adsider, e.g., related changes \nto a variable de.nition and a variable use in two di.erent methods). This means that if two atomic changes, \nC1 and C2, a.ect a given test T , then the absence of a syntactic dependence between C1 and C2 does not \nimply the absence of a semantic dependence; that is, program behaviors resulting from applying C1 alone, \nC2 alone, or C1 and C2 together, may all be di.erent. If a set S of atomic changes is known to expose \na bug, then the knowledge that applying certain subsets of S does not lead to syntactically valid programs, \ncan be used to localize bugs more quickly. Figure 1(b) shows the atomic changes that de.ne the two versions \nof the example program, numbered 1 13 for conve\u00adnience. Each atomic change is shown as a box, where the \ntop half of the box shows the category of the atomic change (e.g., CM for changed method), and the bottom \nhalf shows the method or .eld involved (for LC changes, both the class and method involved are shown). \nAn arrow from an atomic change A1 to an atomic change A2 indicates that A2 is de\u00adpendent on A1. Consider, \nfor example, the addition of the call B.bar() in method B.foo(). This source code change resulted in \natomic change 8 in Figure 1(b). Observe that adding this call would lead to a syntactically invalid program \nunless method B.bar() is also added. Therefore, atomic change 8 is dependent on atomic change 6, which \nis an AM change for method B.bar(). The observant reader may have noticed that there is also a CM change \nfor method B.bar() (atomic change 9). This is the case because our method for deriving atomic changes \ndecomposes the source code change of adding method B.bar() into two steps: the addition of an empty method \nB.bar() (AM atomic change 6 in the .g\u00adure), and the insertion of the body of method B.bar() (CM atomic \nchange 9 in the .gure), where the latter is dependent on the former. Observe that addition of B.bar() \ns body re\u00adquires that .eld B.y be added to class B. Hence, there is a dependence of atomic change 9 on \nAF atomic change 7, which represents the addition of .eld B.y. Notice that our model of dependences between \natomic changes correctly cap\u00adtures the fact that adding the call to B.bar() to the body of B.foo() requires \nthat a method B.bar() is added, but not that .eld B.y is added. The LC atomic change category models \nchanges to the dynamic dispatch behavior of instance methods. In particu\u00adlar, an LC change (Y, X.m()) \nmodels the fact that a call to method X.m() on an object of type Y results in the selection of a di.erent \nmethod. Consider, for example, the addition of method C.foo() to the program of Figure 1(a). As a result \nof this change, a call to A.foo() on an object of type C will dispatch to C.foo() in the edited program, \nwhereas it used to dispatch to A.foo() in the original program. This change in dispatch behavior is captured \nby atomic change 4. LC changes are also generated in situations where a dispatch relationship is added \nor removed as a result of a source code change4 . For example, atomic changes 5 (de.ning the be\u00adhavior \nof a call to C.foo() on an object of type C) and 13 (de.ning the behavior of a call to C.baz() on an \nobject of type C) occur due to the addition of methods C.foo() and C.baz(), respectively. In order to \nidentify those tests that are a.ected by a set of atomic changes, we have to construct a call graph for \neach test. The call graphs used in this paper contain one node for each method, and edges between nodes \nto re.ect calling relationships between methods. Our analysis can work with call graphs that have been \nconstructed using static analysis, or with call graphs that have been obtained by observing the actual \nexecution of the tests. In the experiments reported in this paper, dynamic call graphs are used. Figure \n1(c) shows the call graphs for the 3 tests test1, test2, and test3, before the changes have been applied. \nIn these call graphs, edges corresponding to dynamic dis\u00adpatch are labeled with a pair < T,M >, where \nT is the run-time type of the receiver object, and M is the method shown as invoked at the call site. \nA test is determined to be a.ected if its call graph (in the original version of the pro\u00adgram) either \ncontains a node that corresponds to a changed method (CM) or deleted method (DM) change, or if its call \ngraph contains an edge that corresponds to a lookup change (LC). Using the call graphs in Figure 1(c), \nit is easy to see that: (i) test1 is not a.ected, (ii) test2 is a.ected because its call graph contains \na node for B.foo(), which corresponds to CM change 8, and (iii) test3 is a.ected because its call graph \ncontains an edge corresponding to a dispatch to method A.foo() on an object of type C, which corresponds \nto LC change 4. In order to compute the changes that a.ect a given af\u00adfected test, we need to construct \na call graph for that test in the edited version of the program. These call graphs for the tests are \nshown in Figure 1(d)5 . The set of atomic changes that a.ect a given a.ected test includes: (i) all atomic \nchanges for added methods (AM) and changed methods (CM) that correspond to a node in the call graph (in \nthe edited program), (ii) atomic changes in the lookup change (LC) category that correspond to an edge \nin the call graph (in the edited program), and (iii) their transitively prereq\u00aduisite atomic changes. \nAs an example, we can compute the a.ecting changes for test2 as follows. Observe, that the call graph \nfor test2 in the edited version of the program contains methods B.foo() and B.bar(). These nodes correspond \nto atomic changes 8 and 9 in Figure 1(b), respectively. Atomic change 8 re\u00adquires atomic change 6, and \natomic change 9 requires atomic changes 6 and 7. Therefore, the atomic changes a.ecting test2 are 6, \n7, 8, and 9. Informally, this means that we can automatically determine that test2 is a.ected by the \naddition of .eld B.y, the addition of method B.bar(), and the change to method B.foo(), but not on any \nof the other source code changes! In other words, we can safely rule out 9 of the 13 atomic changes as \nthe potential source for test2 s changed behavior. To conclude our discussion of the example program \nof Fig\u00ad 4Other scenarios that give rise to LC changes will be dis\u00ad cussed in Section 3. 5The call graph \nfor test1 in the edited version of the pro\u00adgram is not necessary for our analysis because test1 was not \na.ected by any of the changes, and is included in the .gure solely for completeness. = = = = = = (c) \n(d) Figure 1: (a) Example program with 3 tests. Added code fragments are shown in boxes. (b) Atomic changes \nfor the example program, with their interdependences. (c) Call graphs for the tests before the changes \nwere applied. (d) Call graphs for the tests after the changes were applied. A.ectedTests(T , A)= { ti \n| ti .T , Nodes(P, ti) n (CM . DM)) = \u00d8}. { ti | ti .T , n, A.m . Nodes(P, ti), n.B, X.mA.m . Edges(P, \nti), (B, X.m). LC, B< * X } A.ectingChanges(t, A)= { a' | a . Nodes(P ',t) n (CM . AM),a' .* a }. { a' \n| a =(B, X.m). LC, B< * X, ,t), n.B, X.mA.m . Edges(P ' for some n, A.m . Nodes(P ',t),a' .* a } Figure \n2: A.ected Tests and A.ecting Changes. ure 1, consider the atomic changes 10, 11, 12, and 13 corre\u00adsponding \nto the addition of .eld C.z and method C.baz(), respectively. These atomic changes do not a.ect any of \nthe tests, indicating that additional tests are needed. We will use the equations in Figure 2 [21] to \nmore formally de.ne how we .nd a.ected tests and their corresponding af\u00adfecting atomic changes, in general. \nAssume the original pro\u00ad ' gram P is edited to yield program P ', where both P and P are syntactically \ncorrect and compilable. Associated with P is a set of tests T = t1,...,tn. The call graph for test ti \non the original program, called Gti , is described by a subset of P s methods Nodes(P, ti) and a subset \nEdges(P, ti) of calling re\u00adlationships between P s methods. Likewise, Nodes(P ',ti) and Edges(P ',ti) \nform the call graph G'ti on the edited program P '. Here, a calling relationship is represented as D.n() \n.B,X.m() A.m(), indicating possible control .ow from method D.n() to method A.m() due to a virtual call \nto method X.m() on an object of type B. We implicitly make the usual assumptions [10] that program execution \nis deter\u00administic and that the library code used and the execution environment (e.g., JVM) itself remain \nunchanged.  3. ATOMIC CHANGES AND THEIR DEPENDENCES As previously mentioned, a key aspect of our analysis \nis the step of uniquely decomposing a source code edit into a set of interdependent atomic changes. In \nthe original formu\u00adlation [21], several kinds of changes, (e.g., changes to access rights of classes, \nmethods, and .elds and addition/deletion of comments) were not modeled. Section 3.1 discusses how these \nchanges are handled in Chianti. Table 1 lists the set of atomic changes in Chianti, which includes the \noriginal 8 cat\u00adegories [21] plus 8 new atomic changes (the bottom 8 rows of the table). Most of the atomic \nchanges are self-explanatory except for CM and LC. CM represents any change to a method s body. Some \nextensions to the original de.nition of CM are discussed in detail in Section 3.1. LC represents changes \nin dynamic dispatch behavior that may be caused by various kinds of source code changes (e.g., by the \naddi\u00adtion of methods, by the addition or deletion of inheritance relations, or by changes to the access \ncontrol modi.ers of methods). LC is de.ned as a set of pairs (Y, X.m()), in\u00addicating that the dynamic \ndispatch behavior for a call to X.m() on an object with run-time type Y has changed. 3.1 New and Modi.ed \nAtomic Changes Chianti handles the full Java programming language, which necessitated the modeling of \nseveral constructs not consid- AC Add an empty class DC Delete an empty class AM Add an empty method \nDM Delete an empty method CM Change body of a method LC Change virtual method lookup AF Add a .eld DF \nDelete a .eld CFI Change de.nition of a instance .eld initializer CSFI Change de.nition of a static .eld \ninitializer AI Add an empty instance initializer DI Delete an empty instance initializer CI Change de.nition \nof an instance initializer ASI Add an empty static initializer DSI Delete an empty static initializer \nCSI Change de.nition of an static initializer Table 1: Categories of atomic changes. ered in the original \nframework [21]. Some of these constructs required the de.nition of new sorts of atomic changes; oth\u00aders \nwere handled by augmenting the interpretation of atomic changes already de.ned. Initializers, Constructors, \nand Fields. Six of the newly added changes in Table 1 correspond to initializers. AI and DI denote the \nset of added and deleted instance initializers respectively, and ASI and DSI denote the set of added \nand deleted static initializers, respectively. CI and CSI capture any change to an instance or static \ninitializer, respectively. The other two new atomic changes, CFI and CSFI, capture any change to an instance \nor static .eld, in\u00adcluding (i) adding an initialization to a .eld, (ii) deleting an initialization of \na .eld, (iii) making changes to the initialized value of a .eld, and (iv) making changes to a .eld modi.er \n(e.g., changing a static .eld into a non-static .eld). Changes to initializer blocks and .eld initializers \nalso have repercussions for constructors or static initializer methods of a class. Speci.cally, if changes \nare made to initializers of in\u00adstance .elds or to instance initializer blocks of a class C, then there \nare two cases: (i) if constructors have been explicitly de.ned for class C, then Chianti will report \na CM for each such constructor, (ii) otherwise, Chianti will report a change to the implicitly declared \nmethod C.(init) that is generated by the Java compiler to invoke the superclass s construc\u00adtor without \nany arguments. Similarly, the class initializer C.(clinit) is used to represent the method being changed \nwhen there are changes to a static .eld (i.e., CSFI) or static initializer (i.e., CSI). Overloading. \nOverloading poses interesting issues for change impact analysis. Consider the introduction of an overloaded \nmethod as shown in Figure 3 (the added method is shown in a box). Note that there are no textual edits \nin Test.main(), and further, that there are no LC changes be\u00adcause all the methods are static. However, \nadding method R.foo(Y) changes the behavior of the program because the call of R.foo(y) in Test.main() \nnow resolves to R.foo(Y) instead of R.foo(X) [9]. Therefore, Chianti must report a CM change for method \nTest.main() despite the fact that no textual changes occur within this method6 . Hierarchy changes. It \nis also possible for changes to the 6However, the abstract syntax tree for Test.main() will be di.erent \nafter applying the edit, as overloading is resolved at compile time. class R } class X class Y class \nYy= }} Figure 3: Java such as in Figure 4(a), by a altered. = } (b) Figure 4: Hierarchy change that a.ects \na method whose code has not changed. Threads and Concurrency. Threads do not pose sig\u00adni.cant challenges \nfor our analysis. The addition/deletion of synchronized blocks inside methods and the addi\u00adtion/deletion \nof synchronized modi.ers on methods are both modeled as CM changes. Threads do not present signi.cant \nissues for the construction of call graphs either, because the analysis discussed in this paper does \nnot re\u00adquire knowledge about the particular thread that executes a method. The only information that \nis required are the meth\u00adods that have been executed and the calling relationships be\u00adtween them. If \ndynamic call graphs are used, as is the case in this paper, this information can be captured by tracing \nthe execution of the tests. If .ow-insensitive static analysis is used for constructing call graphs [19], \nthe only signi.cant issue related to threads is to model the implicit calling rela\u00adtionship between Thread.start() \nand Thread.run(). Exception handling. Exception handling is not a sig\u00adni.cant issue in our analysis. \nAny addition or deletion or statement-level changes to a try, catch or finally block will be reported \nas a CM change. Similarly, changes to the throws clause in a method declaration are also captured as \nCM changes. Possible interprocedural control .ow intro\u00adduced by exception handling is expressed implicitly \nin the call graph; however, our change impact analysis correctly captures e.ects of these exception-related \ncode changes. For example, if a method f() calls a method g(), which in turn calls a method h() and an \nexception of type E is thrown in h() and caught in g() before the edit, but in f() after the edit, then \nthere will be CM changes for both g() and f() representing the addition and deletion of the corresponding \ncatch blocks. These CM changes will result in all tests that execute either f() or g() to be identi.ed \nas a.ected. Therefore, all possible e.ects of this change are taken into account, even without the explicit \nrepresentation of .ow of control due to exceptions in our call graphs. Changes to CM and LC. Accommodating \nmethod access modi.er changes from non-abstract to abstract or vice-versa, and non-public to public or \nvice-versa, required extension of the original de.nition of CM. CM now com\u00adprises: (i) adding a body \nto a previously abstract method, (ii) removing the body of a non-abstract method and mak\u00ading it abstract, \nor (iii) making any number of statement\u00adlevel changes inside a method body or any method declara\u00adtion \nchanges (e.g., changing the access modi.er from public to private, adding a synchronized keyword or changing \na throws clause). In addition, in some cases, changing a method s access modi.er results in changes to \nthe dynamic dispatch in the program (i.e., LC changes). For example, there is no entry for private or \nstatic methods in the dynamic dispatch map (because they are not dynamically dispatched), but if a private \nmethod is changed into a public method, then an entry will be added, generating an LC change that is \ndependent on the access control change, which is represented as a CM. Additions and deletions of import \nstatements may also a.ect dynamic dispatch and are handled by Chianti. 3.2 Dependences Atomic changes \nhave interdependences which induce a partial ordering . on a set of them, with transitive closure ** \n. Speci.cally, C1 C2 denotes that C1 is a prerequi\u00adsite for C2. This ordering determines a safe order \nin which atomic changes can be applied to program P to obtain a syn\u00adtactically correct edited version \nP '' which, if we apply all the changes is P ' . Consider that one cannot extend a class X that does \nnot yet exist, by adding methods or .elds to it (i.e., AC(X) . AM(X.m()) and AC(X) . AF(X.f)). These \ndependences are intuitive as they involve how new code is added or deleted in the program. Other dependences \nare more subtle. For example, if we add a new method C.m() and then add a call to C.m() in method D.n(), \nthere will be a dependence AM(C.m()) . CM(D.n()). Figure 1(b) shows some examples of dependences among \natomic changes. Dependences involving LC changes can be caused by ed\u00adits that alter inheritance relations. \nLC changes can be classi.ed as (i) newly added dynamic dispatch tuples (e.g., caused by declaring a new \nclass/interface or method), (ii) deleted dynamic dispatch tuples (e.g., caused by deleting a class/interface \nor method), or (iii) dynamic dispatch tu\u00adples with changed targets (e.g., caused by adding/deleting a \nmethod or changing the access control of a class or method). For example, making an abstract class C \nnon-abstract will result in LC changes. In the original dynamic dispatch map, there is no entry with \nC as the run-time receiver type, but the new dispatch map will contain such an entry. Similar dependences \nresult when other access modi.ers are changed. 3.3 Engineering Issues One engineering problem encountered \nin building Chianti resulted from the absence of unique names for anonymous and local classes. In a JVM, \nanonymous classes are repre\u00adsented as EnclosingClassName$(num), where the number assigned represents \nthe lexical order of the anonymous class within its enclosing class. This naming strategy guaran\u00adtees \nthat all the class names in a Java program are unique. However, when Chianti compares and analyzes two \nrelated Java programs, it needs to establish a correspondence be\u00adtween classes and methods in each version \nto determine the set of atomic changes. The approach used is a match-by\u00adname strategy in which two components \nin di.erent pro\u00adgrams match if they have the same name; however, when there are changes to anonymous \nor local inner classes, this strategy requires further consideration. import java.io.*; class Lister \n{ static void listClassFiles(String dir){File f = new File(dir); String[] list = f.list( new FilenameFilter() \n{ //anonymous class boolean accept(File f, String s){return s.endsWith(\".class\"); } }); for(int i = 0; \ni < list.length; i++) System.out.println(list[i]); } static void listJavaFiles(String dir){File f = \nnew File(dir); String[] list = f.list( new FilenameFilter() { //anonymous class boolean accept(File f,String \ns){return s.endsWith(\".java\"); } }); for(int i = 0; i < list.length; i++) System.out.println(list[i]); \n } } Figure 5: Addition of an anonymous class. The added code fragments are shown inside a box. Figure \n5 shows a simple program using anonymous classes with the code added by the edit shown inside a box. \nIn this program, method listJavaFiles(String) lists all Java .les in a directory that is speci.ed by \nits pa\u00adrameter. Anonymous class Lister$1 implements interface java.io.FilenameFilter and is de.ned as \npart of a method call expression. Now, assume that the program is edited and a method listClassFiles(String) \nis added that lists all class .les in a directory. This new method declares another, similar anonymous \nclass. Now, in the edited version of the program, the Java compiler will name this new anonymous class \nLister$1 and the previous anonymous class, formerly named Lister$1, will become Lister$2. Clearly, the \nmatch\u00adby-name strategy cannot be based on compiler-generated names because the original anonymous class \nhas di.erent names before and after the edit. To solve this problem, Chianti uses a naming strategy for \nclasses that assigns each a unique internal name. For top-level classes or member classes, the internal \nname is the same as the class name. For anonymous classes and local inner classes, the unique name consists \nof four parts: enclosingClassName, enclosingElement-Name, selfSuperclassInterfacesName, sequenceNumber. \nFor the example in Figure 5, the unique internal name of the anonymous class in the original program \nis Lister$listJavaF iles(String)$java.io.F ilenameF ilter$1, while the unique internal name of the newly \nadded anonymous class in the edited program is Lister$listClassF iles(String)$java.io.F ilenameF ilter$1. \nSimilarly, the internal name of the origi\u00adnal anonymous class in the edited program is Lister$listJavaF \niles(String)$java.io.F ilenameF ilter$1. Notice that this original anonymous class whose compiler\u00adgenerated \nnames are Lister$1 in the original program and Lister$2 in the edited program, has the same unique internal \nname in both versions. With this new naming strategy, match-by-name can identify anonymous and local \ninner classes and report atomic changes involving them7 .  4. PROTOTYPE Chianti has been implemented \nin the context of the Java editor of Eclipse, a widely used extensible open-source de\u00advelopment environment \nfor Java. Our tool is designed as a combination of Eclipse views,a plugin, and a launch con\u00ad.guration \nthat together constitute a change impact analysis perspective8 . Chianti is built as a plugin of Eclipse \nand con\u00adceptually can be divided into three functional parts. One part is responsible for deriving a \nset of atomic changes from two versions of an Eclipse project (i.e., Java program), which is achieved \nvia a pairwise comparison of the abstract syn\u00adtax trees of the classes9 in the two project versions. \nAn\u00adother part reads test call graphs for the original and edited 7This naming scheme can only fail when \ntwo anonymous classes occur within the same scope and extend the same superclass. If this occurs due \nto an edit, however, Chianti generates a safe set of atomic changes corresponding to the edit. 8A perspective \nis Eclipse terminology for a collection of views that support a speci.c task, (e.g., the Java perspective \nis used for creating Java applications). 9 While Eclipse provides functionality for comparing source \n.les at a textual level, we found the amount of informa\u00adtion provided inadequate for our purposes. In \nparticular, the class hierarchy information provided by Eclipse does not currently include anonymous \nand local classes. Figure 6: Chianti architecture. projects, computes a.ected tests and their a.ecting \nchanges. The third part manages the views that allow the user to visualize change impact information. \nChianti s GUI also in\u00ad cludes a launch con.guration that allows users to select the project versions \nto be analyzed, the set of tests associated with the project and the call graphs to be used. Figure 6 \ndepicts Chianti s architecture. Although Chianti is intended for interactive use, we have been testing \nthe prototype using successive CVS versions of a program. Thus, a typical scenario of a Chianti ses\u00ad \nsion begins with the programmer extracting two versions of a project from a CVS version control repository \ninto the workspace. The programmer then starts the change impact analysis launch con.guration, and selects \nthe two projects of interest as well as the test suite associated with these projects. Currently, we \nallow tests that have a separate main() routine and JUnit tests10 . In order to enable the reuse of analysis \nresults, and to decouple the analysis from GUI-related tasks, both atomic change information and call \ngraphs are stored as XML .les. Chianti currently supports two mechanisms for obtaining the call graphs \nto be used in the analysis. When static call graphs are desired, Chianti invokes the Gnosis analysis \nen\u00ad gine11 to construct these [19]. In this case, users need to supply some additional information relevant \nto the analysis engine (e.g., the choice of call graph construction algorithm to be used and some policy \nsettings for dealing with re.ec\u00ad tion). Users can also point Chianti directly at an XML .le rep\u00ad resentation \nof the call graphs that are to be used, in order to enable the use of call graphs that have been constructed \nby external tools. The experiments with dynamic call graphs presented in this paper have been conducted \nusing an o.\u00ad line tool that instruments the class .les for an application. Executing an application that \nhas been instrumented by this tool produces an XML .le containing the application s dy\u00ad namic call graph12 \n. 10See www.junit.org. 11Gnosis is a static analysis framework that has been devel\u00ad oped at IBM Research \nas a test-bed for research on demand\u00ad driven and context-sensitive static analysis. 12We did not optimize \nthe gathering of the dynamic call in\u00ad formation; presently, the instrumented tests run, on aver- When \nthe analysis results are available, the Eclipse work\u00adbench changes to the change impact analysis perspective, \nwhich provides a number of views: The a.ecting changes view shows all tests in a tree view. Each a.ected \ntest can be expanded to show its set of a.ecting changes and their prerequisites. Fig\u00adure 7 shows a snapshot \nof this view; note how the prerequisite changes are shown. Each atomic change is the root of a tree that \ncan be expanded on demand to show prerequisite changes. This quickly provides an idea of the di.erent \nthreads of changes that have occurred.  The atomic-changes-by-category view shows the di.er\u00adent atomic \nchanges grouped by category.  Each of these user interface components is seamlessly inte\u00adgrated with \nthe standard Java editor in Eclipse (e.g., clicking on an atomic change in the a.ecting changes view \nopens an editor on the associated program fragment).  5. EVALUATION The experiments with Chianti were \nperformed on versions of the Daikon system by M. Ernst et al. [7], extracted from the developers CVS \nrepository. The Daikon CVS repository does not use version tags, so we partitioned the year-long version \nhistory arbitrarily at week boundaries. All modi\u00ad.cations checked in within a week were considered to \nbe within one edit whose impact was to be determined. How\u00adever, in cases where no editing activity took \nplace in a given week, we extended the interval by one week until it included changes. The data reported \nin this section covers the entire year 2002 (i.e., 52 weeks) of updates, during which there were 39 intervals \nwith editing activity. During the year under consideration, Daikon was actively being developed and increased \nin size from 48K to 123K lines of code. More signi.cant are the program-based measures of growth, from \n357 to 755 classes, 2878 to 7112 methods, and 937 to 2885 .elds. The number of unit tests associated \nwith Daikon grew from 40 to 62 during the time period under consideration. Figure 8 shows in detail the \ngrowth curves over this time period. Clearly, this is a moderate\u00adsized application that experienced considerable \ngrowth in size (and complexity) over the year 2002. 5.1 Atomic Changes Figure 9(a) shows the number \nof atomic changes between each pair of versions. The number of atomic changes per interval varies greatly \nbetween 1 and 11,698 during this pe\u00adriod, although only 11 edits involved more than 1,000 atomic changes. \nSection 5.3 gives more details about two speci.c intervals in our study. Figure 9(b) summarizes the relative \npercentages of kinds of atomic changes observed during 2002. The height of each bar indicates the frequency \nof the corresponding kind of atomic change; these values vary widely, by three orders of magnitude. Three \nof our atomic change categories were not seen in this data, namely addInitializer, changeInitial\u00ad age, \nabout 2 orders of magnitude more slowly than unin\u00adstrumented code, but we think we can reduce this overhead \nsigni.cantly with some e.ort.  Figure 7: Snapshot of Chianti s a.ecting changes view. opens an editor \non the associated source fragment. izer and deleteInitializer13 . Note that the 0.01% value for deleteStaticInitializer \nin the .gure represents the 5 atomic changes of that type out of a total of over 44,000 changes for the \nentire year! Figure 10 shows the proportion of atomic changes per in\u00ad terval, grouped by the program \nconstruct they a.ect, namely, classes, .elds, methods and dynamic dispatch. Clearly, the two most frequent \ngroups of atomic changes are changes to dynamic dispatch (i.e., LC) and changes to methods (i.e., CM); \ntheir relative amounts vary over the period.  5.2 Affected Tests and Affecting Changes Figure 11 shows \nthe percentage of a.ected tests for each of the Daikon versions. On average, 52% of the tests are af\u00adfected \nin each edit. Interestingly, there were several intervals over which no tests were a.ected, although \natomic changes did occur. For example, there were no a.ected tests for the interval between 04/01/02 \nand 04/08/02, despite the fact that there were 212 atomic changes during this time. Sim\u00adilarly, for the \ninterval between 8/26/02 and 9/02/02 there were 286 atomic changes, but no a.ected tests. This means \nthat the changed code for these intervals was not covered by any of the tests! In principle, a change \nimpact analysis  13This is not surprising because, in Java, instance initializers are only needed in \nthe rare event that an anonymous class needs to perform initialization actions that cannot be ex\u00adpressed \nusing .eld initializers. In non-anonymous classes, it is generally preferable to incorporate initialization \ncode in constructors or in .eld initializers. The arrow shows how clicking on an atomic change tool could \ninform the user that additional unit tests should be written when an observation of this kind is made. \nper a.ected test, for each of the Daikon versions. On av\u00ad erage, only 3.95% of the atomic changes impact \na given af\u00ad fected test. This means that our technique has the potential of dramatically reducing the \namount of time required for de\u00ad bugging when a test produces an erroneous result after an editing session. \nBy contrast, an earlier study performed with Chianti us\u00ad ing static call graphs for the same Daikon data, \nyielded on average 56% a.ected tests and 3.7% a.ecting changes per a.ected test [19]14 . The closeness \nof these results to those reported in the present paper suggests that we should inves\u00ad tigate the tradeo.s \nassociated with using static or dynamic call graphs. Our approach assumes that the test suite associated \nwith a Java program o.ers good coverage of the entire program. To verify this assumption, we used the \nJCoverage tool (see www.jcoverage.com) to determine how many methods in Daikon were actually exercised \nby its unit test suite. For each version of Daikon, we obtained the number of meth\u00ad 14Imprecision in \nthe static call graphs resulted in the detec\u00adtion of extra a.ected tests that had relatively small numbers \nof a.ecting changes. This skewed our averaging calculations to yield the counterintuitive result that \nthe a.ecting changes percentage obtained using static call graphs was lower than the percentage obtained \nusing the more precise dynamic call graphs.  Figure 8: Daikon growth statistics for the year 2002 ods \ncovered by the associated tests and the total number of (source code) methods in that version, yielding \nan average method coverage ratio. The overall average of these ratios on the entire Daikon system is \nquite low, at 21%. How\u00adever, this number is skewed by the fact that certain Daikon components have reasonable \ncoverage (e.g., for the utilMDE component we .nd an average coverage ratio over the year of 47%), whereas \nother components (e.g., the jtb compo\u00adnent) have virtually no coverage. Thus, while our change impact \nanalysis .ndings are promising, they would be more compelling with a test suite o.ering better coverage \nof the system.  5.3 Case Studies We conducted two detailed case studies to further inves\u00adtigate the \npossible applications of Chianti as it is intended to be used, namely in interactive environments with \nshort time intervals between versions. To this end, we selected two one-week intervals from the whole \nyear s data in which heavy editing activity occurred, and divided those intervals into subintervals of \none day each. Case Study 1 The .rst interval we decided to explore further is the one for which we found \nthe highest percentage of a.ected tests. This occurred between versions 07/08/02 and 07/15/02, when 88.7% \n(55 out of 62) of the tests were a.ected. We partitioned the version history of this interval into daily \nintervals so that we could obtain changes with .ner granularity. In cases where no editing activity took \nplace between two days, we extended the interval by one day, thus obtaining 5 intervals with editing \nactivity. Figure 13(a) shows the number of a.ected tests for each subinterval as well as the number of \na.ected tests for the original week-long interval (shown as the rightmost pair of bars). Before partitioning, \n55 of the 62 unit tests were af\u00adfected tests, but smaller numbers of a.ected tests, ranging from 1 to \n53, were reported for each of the subintervals (for example, in subinterval 07/10/02 07/11/02, there \nis only one a.ected test). Figure 13(b) shows the total number of atomic changes and the average number \nof a.ecting changes per a.ected test in each subinterval compared with the original inter\u00adval (again \nshown as the rightmost pair of bars). The use of smaller intervals resulted in smaller numbers of atomic \nchanges for each interval and also smaller numbers of a.ect\u00ading changes per a.ected test; this makes \nthe tracing of af\u00adfecting changes much easier. In addition, we found that 12 of the 55 a.ected tests \nfor the original, week-long interval were only a.ected in one of the smaller intervals, which means that \nwe can narrow down the range of a.ecting changes into a small set of atomic changes for these 12 tests. \nCase Study 2 The second interval we selected is the one with the highest average number of a.ecting changes. \nThis interval took place between versions 01/21/02 and 01/28/02, when 140 a.ecting changes occurred on \naverage (ranging from 3 to 217) for 32 a.ected tests. Similar to case study 1, we partitioned the original \nweek-long interval into several subintervals, obtaining 3 subintervals with editing activity. In Figure \n14(a) and (b) we can see similar results to those of case study 1, that is, we obtain smaller numbers \nof atomic changes, a.ected tests and a.ecting changes compared to the original week interval. In both \ncase studies, we found that the use of subin\u00adtervals with smaller numbers of a.ecting changes improves \nthe ability of Chianti to help programmers with under\u00adstanding the e.ects of an edit. Even in subintervals \nsuch as 01/21/02 01/25/02, where the number of atomic changes and the average number of a.ecting changes \nare (a) (b) Figure 9: (a) Number of atomic changes between each pair of Daikon versions in 2002 (note \nthe log scale). (b) Categorization of the atomic changes, aggregated over all Daikon edits in 2002. \nlarge relative to the corresponding numbers for the origi\u00adnal interval, Chianti can provide useful insights. \nFor ex\u00adample, consider one test with a large number of a.ect\u00ading changes: daikon.test.di..Di.Tester.testPpt4Ppt4 \nfrom subinterval 01/21/02 01/25/02. The a.ecting changes for this test are: 67 CM changes, 67 AF changes, \nand 69 CSFI changes. Among the 67 CM changes, 65 of them are asso\u00adciated with static initializers for \nsome class. These, in turn, are dependent on 68 of the CSFIs, whose own prerequisites are 66 of the AFs. \nA closer look revealed that all the added .elds have the same name, serialVersionUID, which is used to \nadd serialization-related functionality to Daikon. It is in\u00adteresting to observe that Chianti was able \nto determine that the changed behavior of this test was almost entirely due to this serialization-related \nchange, and that the other 800+ atomic changes that occurred during this interval did not contribute \nto the test s changed behavior.  5.4 Chianti Performance The performance of Chianti has thus far not \nbeen our primary focus, however, we have achieved acceptable per\u00adformance for a prototype. Deriving atomic \nchanges from two successive versions of Daikon takes, on average, approx\u00adimately 87 seconds. Computing \nthe set of a.ected tests for each version pair takes approximately 5 seconds on average, and computing \na.ecting changes takes on average approxi\u00admately 1.2 seconds per a.ected test. All measurements were \ntaken on a Pentium 4 PC at 2.8Ghz with 1Gb RAM.  6. RELATED WORK In previous papers, we presented the \nconceptual frame\u00adwork of our change impact analysis, without empirical ex\u00adperimentation [21], and reported \non experiments with a purely static version of Chianti using static call graphs generated by Gnosis, \non the same Daikon data used here [19]. We distinguish three broad categories of related work in the \ncommunity: (i) change impact analysis techniques, (ii) regression test selection techniques, and (iii) \ntechniques for controlling the way changes are made. 6.1 Change Impact Analysis Techniques Previous \nresearch in change impact analysis has varied from approaches relying completely on static information, \nincluding the early analyses of [3, 11], to approaches that only utilize dynamic information, such as \n[13]. There also are some methods [15] that use a combination of static and dynamic information. The \nmethod described in this paper is a combined approach, in that it uses (i) static analysis for .nding \nthe set of atomic changes comprising a program edit and (ii) dynamic call graphs to .nd the a.ected tests \nand their a.ecting changes. All of the impact analyses previous to ours focus on .nd\u00ading constructs of \nthe program potentially a.ected by code changes. In contrast, our change impact analysis aims to .nd \na subset of the changes that impact a test whose be\u00adhavior has (potentially) changed. First we will discuss \nthe Figure 11: Percentage of a.ected tests for each of the Daikon versions. Figure 12: Average percentage \nof a.ecting changes, per a.ected test, for each of the Daikon versions. (b)Number of atomic changes and \non average af\u00ad (a) Number of a.ected tests on large and smaller fecting changes on large interval and \nsmaller daily daily intervals. intervals (note log scale) Figure 14: Detailed analysis results for the \ninterval 1/21/02 1/28/02. previous static techniques and then address the combined and dynamic approaches. \nAn early form of change impact analysis used reachability on a call graph to measure impact. This technique15 \nwas presented by Bohner and Arnold [3] as intuitively appeal\u00ad ing and a starting point for implementing \nchange impact analysis tools. However, applying the Bohner-Arnold tech\u00ad nique is not only imprecise but \nalso unsound, because, by tracking only methods downstream from a changed method, it disregards callers \nof that changed method that can also be a.ected. Kung et al. [11] described various sorts of relationships \nbe\u00ad tween classes in an object relation diagram (i.e., ORD), clas\u00ad si.ed types of changes that can occur \nin an object-oriented program, and presented a technique for determining change impact using the transitive \nclosure of these relationships. Some of our atomic change types partially overlap with their class changes \nand class library changes. Tonella s impact analysis [27] determines if the compu\u00ad tation performed on \na variable x a.ects the computation on another variable y using a number of straightforward 15This is \nonly one of the static change impact analyses dis\u00adcussed. queries on a concept lattice that models the \ninclusion rela\u00adtionships between a program s decomposition (static) slices [8]. Tonella reports some \nmetrics of the computed lattices, but gives no assessment of the usefulness of his techniques. A number \nof tools in the Year 2000 analysis domain [5, 18] use type inference to determine the impact of a restricted \nset of changes (e.g., expanding the size of a date .eld) and per\u00adform them if they can be shown to be \nsemantics-preserving. Thione et al. [25, 24] wish to .nd possible semantic inter\u00adferences introduced \nby concurrent programmer insertions, deletions or modi.cations to code maintained with a version control \nsystem. In this work, a semantic interference is char\u00adacterized as a change that breaks a def-use relation. \nTheir unit of program change is a delta provided by the version control system, with no notion of subdividing \nthis delta into smaller units, such as our atomic changes. Their analysis, which uses program slicing, \nis performed at the statement level, not at the method level as in Chianti. No empirical experience with \nthe algorithm is given. The CoverageImpact change impact analysis technique by Orso et al. [15] uses \na combined methodology, by correlating a forward static slice [26] with respect to a changed program \nentity (i.e., a basic block or method) with execution data ob\u00adtained from instrumented applications. \nEach program en\u00ad tity change is thusly associated with a set of possibly a.ected program entities. Finally, \nthese sets are unioned to form the full change impact set corresponding to the program edit. There are \na number of important di.erences between our work and that by Orso et al. First, we di.er in the goals \nof the analysis. The method of Orso et al. [15] is focused on .nding those program entities that are \npossibly a.ected by a program edit. In contrast, our method is focused on .nding those changes that caused \nthe behavioral di.erences in a test whose behavior has changed. Second, the granularity of change expressed \nin their technique is a program entity, which can vary from a basic block to an entire method. In contrast, \nwe use a richer domain of changes more familiar to the programmer, by taking a program edit and decomposing \nit into interdependent, atomic changes identi.ed with the source code (e.g., add a class, delete a method, \nadd a .eld). Third, their technique is aimed at deployed codes, in that they are interested in obtaining \nuser patterns of program execution. In contrast, our techniques are intended for use during the earlier \nstages of software development, to give developers immediate feedback on changes they make. Law and Rothermel \n[13] present PathImpact, a dynamic impact analysis that is based on whole-path pro.ling [12]. In this \napproach, if a procedure p is changed, any procedure that is called after p, as well as any procedure \nthat is on the call stack after p returns, is included in the set of potentially impacted procedures. \nAlthough our analysis di.ers from that of Law and Rothermel in its goals (i.e., .nding a.ected program \nentities versus .nding changes a.ecting tests), both analyses use the same method-level granularity to \ndescribe change impact. A recent empirical comparison [16] of the dynamic impact analyses CoverageImpact \nby Orso et al. [15] and PathImpact by Law and Rothermel [13] revealed that the latter computes more precise \nimpact sets than the former in many cases, but uses considerably (7 to 30 times) more space to store \nexe\u00ad cution data. Based on the reported performance results, the practicality of PathImpact on programs \nthat generate large execution traces seems doubtful, whereas CoverageIm\u00ad pact [16] does appear to be \npractical, although it can be signi.cantly less precise. Another outcome of the study is that the relative \ndi.erence in precision between the two tech\u00ad niques varies considerably across (versions of) programs, \nand also depends strongly on the locations of the changes. Zeller [28] introduced the delta debugging \napproach for lo\u00ad calizing failure-inducing changes among large sets of textual changes. E.cient binary-search-like \ntechniques are used to partition changes into subsets, executing the programs re\u00ad sulting from applying \nthese subsets, and determining whether the result is correct, incorrect, or inconclusive. An impor\u00ad tant \ndi.erence with our work is that our atomic changes and interdependences take into account program structure \nand dependences between changes, whereas Zeller assumes all changes to be completely independent.  6.2 \nRegression Test Selection Selective regression testing16 aims at reducing the number of regression tests \nthat must be executed after a software change [20, 17]. These techniques typically determine the 16 \nWe use the term broadly here to indicate any methodology that tries to reduce the time needed for regression \ntesting after a program change, without missing any test that may be a.ected by that change. entities \nin user code that are covered by a given test, and correlate these against those that have undergone \nmodi.ca\u00adtion, to determine a minimal set of tests that are a.ected. Several notions of coverage have \nbeen used. For example, TestTube [4] uses a notion of module-level coverage, and De\u00adjaVu [20] uses a \nnotion of statement-level coverage. The em\u00adphasis in this work is mostly on reducing the cost of running \nregression tests, whereas our interest is primarily in assist\u00ading programmers with understanding the \nimpact of program edits. Bates and Horwitz [1] and Binkley [2] proposed .ne-grained notions of program \ncoverage based on program dependence graphs and program slices, with the goal of providing as\u00adsistance \nwith understanding the e.ects of program changes. In comparison to our work, this work uses more costly \nstatic analyses based on (interprocedural) program slicing and con\u00adsiders program changes at a lower-level \nof granularity, (e.g., changes in individual program statements). Our technique for change impact analysis \nuses a.ected tests to indicate to the user the functionality that has been a.ected by a program edit. \nOur analysis determines a sub\u00adset of those tests associated with a program which need to be rerun, but \nit does so in a very di.erent manner than pre\u00advious selective regression testing approaches, because \nthe set of a.ected tests is determined without needing information about test execution on both versions \nof the program. Rothermel and Harrold [20] present a regression test se\u00adlection technique that relies \non a simultaneous traversal of two program representations (control .ow graphs (CFGs) in [20]) to identify \nthose program entities (edges in [20]) that represent di.erences in program behavior. The tech\u00adnique \nthen selects any modi.cation-traversing test that is traversing at least one such dangerous entity. This \nregres\u00adsion test selection technique is safe in the sense that any test that may expose faults is guaranteed \nto be selected. Harrold et al. [10] present a safe regression test selection technique for Java that \nis an adaptation of the technique of Rothermel and Harrold [20]. In this work, Java Inter\u00adclass Graphs \n(JIGs) are used instead of control-.ow graphs. JIGs extend CFGs in several respects: Type and class hi\u00aderarchy \ninformation is encoded in the names of declaration nodes, a model of external (unanalyzed) code is used \nfor in\u00adcomplete applications, calling relationships between meth\u00adods are modeled using Class Hierarchy \nAnalysis, and addi\u00adtional nodes and edges are used for the modeling of exception handling constructs. \nThe method for .nding a.ected tests presented in this pa\u00adper is also safe in the sense that it is guaranteed \nto identify any test that reveals a fault. However, unlike the regres\u00adsion test selection techniques \nsuch as [20, 10], our method does not rely on a simultaneous traversal of two representa\u00adtions of the \nprogram to .nd semantic di.erences. Instead, we determine a.ected tests by .rst deriving from a source \ncode edit a set of atomic changes, and then correlating those changes with the nodes and edges in the \ncall graphs for the tests in the original version of the program. Investigating the cost/precision tradeo.s \nbetween these two approaches for .nding tests that are a.ected by a set of changes is a topic for further \nresearch. In the work by Elbaum et al. [6], a large suite of regres\u00adsion tests is assumed to be available, \nand the objective is to select a subset of tests that meets certain (e.g., cover\u00adage) criteria, as well \nas an order in which to run these tests that maximizes the rate of fault detection. The di.erence between \ntwo versions is used to determine the selection of tests, but unlike our work, the techniques are to \na large ex\u00adtent heuristics-based, and may result in missing tests that expose faults. The change impact \nanalysis of [15] can be used to provide a method for selecting a subset of regression tests to be rerun. \nFirst, all the tests that execute the changed program entities are selected. Then, there is a check if \nthe selected tests are adequate for those program changes. Intuitively, an adequate test set T implies \nthat every relationship between a program entity change and a corresponding a.ected entity is tested \nby a test in T . In their approach, they can determine which a.ected entities are not tested (if any). \nAccording to the authors, this is not a safe selective regression testing technique, but it can be used \nby developers, for example, to prioritize test cases and for test suite augmentation.  6.3 Controlling \nthe Change Process Palantir [22] is a tool that informs users of a con.gura\u00adtion management system when \nother users access the same modules and potentially create direct con.icts. Lucas et al [23] describes \nreuse contracts, a formalism to encapsulate design decisions made when constructing an ex\u00adtensible class \nhierarchy. Problems in reuse are avoided by checking proposed changes for consistency with a speci.ed \nset of possible operations on reuse contracts.  7. CONCLUSIONS AND FUTURE WORK We have presented our \nexperiences with Chianti, a change impact analysis tool that has been validated on a year of CVS data \nfrom Daikon. Our empirical results show that after a program edit, on average the set of a.ected tests \nis a bit more than half of all the possible tests (52%) and for each a.ected test, the number of a.ecting \nchanges is very small (3.95% of all atomic changes in that edit). These .nd\u00adings suggest that our change \nimpact analysis is a promising technique for both program understanding and debugging. Plans for future \nresearch include an in-depth evaluation of the cost/precision tradeo.s involved in using static ver\u00adsus \ndynamic call graphs, now that we have some experience with both. We also intend to experiment with smaller \nunits of change, to better describe change impact to a user, espe\u00adcially since we currently consider \nall changes to code within a method (i.e., CM) as one monolithic change. Acknowledgements. We would like \nto thank Michael Ernst and his research group at MIT for the use of their data. We are also grateful \nto the anonymous reviewers for their constructive feedback. 8. REFERENCES [1] Bates, S., and Horwitz, \nS. Incremental program testing using program dependence graphs. In Proc. of the ACM SIGPLAN-SIGACT Conf. \non Principles of Programming Languages (POPL 93) (Charleston, SC, 1993), pp. 384 396. [2] Binkley, D. \nSemantics guided regression test cost reduction. IEEE Trans. on Software Engineering 23,8 (August 1997). \n[3] Bohner, S. A., and Arnold, R. S. An introduction to software change impact analysis. In Software \nChange Impact Analysis, S. A. Bohner and R. S. Arnold, Eds. IEEE Computer Society Press, 1996, pp. 1 \n26. [4] Chen, Y., Rosenblum, D., and Vo, K. Testtube: A system for selective regression testing. In Proc. \nof the 16th Int. Conf. on Software Engineering (1994), pp. 211 220. [5] Eidorff, P. H., Henglein, F., \nMossin, C., Niss, H., Sorensen, M. H., and Tofte, M. AnnoDomini: From type theory to year 2000 conversion. \nIn Proc. of the ACM SIGPLAN-SIGACT Symp. on Principles of Programming Languages (January 1999), pp. 11 \n14. [6] Elbaum, S., Kallakuri, P., Malishevsky, A. G., Rothermel, G., and Kanduri, S. Understanding the \ne.ects of changes on the cost-e.ectiveness of regression testing techniques. Journal of Software Testing, \nVeri.cation, and Reliability (2003). To appear. [7] Ernst, M. D. Dynamically discovering likely program \ninvariants. PhD thesis, University of Washington, 2000. [8] Gallagher, K., and Lyle, J. R. Using program \nslicing in software maintenance. IEEE Trans. on Software Engineering 17 (1991). [9] Gosling, J., Joy, \nB., Steele, G., and Bracha, G. The Java Language Speci.cation (Second Edition). Addison-Wesley, 2000. \n[10] Harrold, M. J., Jones, J. A., Li, T., Liang, D., Orso, A., Pennings, M., Sinha, S., Spoon, S. A., \nand Gujarathi, A. Regression test selection for Java software. In Proc. of the ACM SIGPLAN Conf. on Object \nOriented Programming Languages and Systems (OOPSLA 01) (October 2001), pp. 312 326. [11] Kung, D. C., \nGao, J., Hsia, P., Wen, F., Toyoshima, Y., and Chen, C. Change impact identi.cation in object oriented \nsoftware maintenance. In Proc. of the International Conf. on Software Maintenance (1994), pp. 202 211. \n[12] Larus, J. Whole program paths. In Proc. of the ACM SIGPLAN Conf. on Programming Language Design \nand Implementation (May 1999), pp. 1 11. [13] Law, J., and Rothermel, G. Whole program path-based dynamic \nimpact analysis. In Proc. of the International Conf. on Software Engineering (2003), pp. 308 318. [14] \nMilanova, A., Rountev, A., and Ryder, B. G. Precise call graphs for C programs with function pointers. \nJournal for Automated Software Engineering (2004). Special issue on Source Code Analysis and Manipulation. \n[15] Orso, A., Apiwattanapong, T., and Harrold, M. J. Leveraging .eld data for impact analysis and regression \ntesting. In Proc. of European Software Engineering Conf. and ACM SIGSOFT Symp. on the Foundations of \nSoftware Engineering (ESEC/FSE 03) (Helsinki, Finland, September 2003). [16] Orso, A., Apiwattanapong, \nT., Law, J., Rothermel, G., and Harrold, M. J. An empirical comparison of dynamic impact analysis algorithms. \nIn Proc. of the International Conf. on Software Engineering (ICSE 04) (Edinburgh, Scotland, 2004), pp. \n491 500. [17] Orso, A., Shi, N., and Harrold, M. J. Scaling regression testing to large software systems. \nIn Proceedings of the 12th ACM SIGSOFT Symposium on the Foundations of Software Engineering (FSE 2004) \n(Newport Beach, CA, 2004). To appear. [18] Ramalingam, G., Field, J., and Tip, F. Aggregate structure \nidenti.cation and its application to program analysis. In Proc. of the ACM SIGPLAN-SIGACT Symp. on Principles \nof Programming Languages (January 1999), pp. 119 132. [19] Ren, X., Shah, F., Tip, F., Ryder, B. G., \nChesley, O., and Dolby, J. Chianti: A prototype change impact analysis tool for Java. Tech. Rep. DCS-TR-533, \nRutgers University Department of Computer Science, September 2003. [20] Rothermel, G., and Harrold, M. \nJ. A safe, e.cient regression test selection technique. ACM Trans. on Software Engineering and Methodology \n6,2 (April 1997), 173 210. [21] Ryder, B. G., and Tip, F. Change impact for object oriented programs. \nIn Proc. of the ACM SIGPLAN/SIGSOFT Workshop on Program Analysis and Software Testing (PASTE01) (June \n2001). [22] Sarma, A., Noroozi, Z., and van der Hoek, A. Palantir: Raising awareness among con.guration \nmanagement workspaces. In Proc. of the International Conf. on Software Engineering (2003), pp. 444 454. \n[23] Steyaert, P., Lucas, C., Mens, K., and D Hondt, T. Reuse contracts: Managing the evolution of reusable \nassets. In Proc. of the Conf. on Object-Oriented Programming, Systems, Languages and Applications (1996), \npp. 268 285. [24] Thione, G. L. Detecting semantic con.icts in parallel changes, December 2002. Masters \nThesis, Department of Electrical and Computer Engineering, University of Texas, Austin. [25] Thione, \nG. L., and Perry, D. E. Parallel changes: Detecting semantic interference. Tech. Rep. ESEL-2003-DSI-1, \nExperimental Software Engineering Laboratory, University of Texas, Austin, September 2003. [26] Tip, \nF. A survey of program slicing techniques. J. of Programming Languages 3, 3 (1995), 121 189. [27] Tonella, \nP. Using a concept lattice of decomposition slices for program understanding and impact analysis. IEEE \nTrans. on Software Engineering 29, 6 (2003), 495 509. [28] Zeller, A. Yesterday my program worked. Today, \nit does not. Why? In Proc. of the 7th European Software Engineering Conf./7th ACM SIGSOFT Symp. on the \nFoundations of Software Engineering (ESEC/FSE 99) (Toulouse, France, 1999), pp. 253 267.  \n\t\t\t", "proc_id": "1028976", "abstract": "<p>This paper reports on the design and implementation of Chianti, a change impact analysis tool for Java that is implemented in the context of the Eclipse environment. Chianti analyzes two versions of an application and decomposes their difference into a set of atomic changes. Change impact is then reported in terms of affected (regression or unit) tests whose execution behavior may have been modified by the applied changes. For each affected test, Chianti also determines a set of affecting changes that were responsible for the test's modified behavior. This latter step of isolating the changes that induce the failure of one specific test from those changes that only affect other tests can be used as a debugging technique in situations where a test fails unexpectedly after a long editing session. We evaluated Chianti on a year (2002) of CVS data from M. Ernst's Daikon system, and found that, on average, 52% of Daikon's unit tests are affected. Furthermore, each affected unit test, on average, is affected by only 3.95% of the atomic changes. These findings suggest that our change impact analysis is a promising technique for assisting developers with program understanding and debugging.</p>", "authors": [{"name": "Xiaoxia Ren", "author_profile_id": "81100597887", "affiliation": "Rutgers University, Piscataway, NJ", "person_id": "P698424", "email_address": "", "orcid_id": ""}, {"name": "Fenil Shah", "author_profile_id": "81100650965", "affiliation": "IBM Software Group, Hawthorne, NY", "person_id": "P698416", "email_address": "", "orcid_id": ""}, {"name": "Frank Tip", "author_profile_id": "81100333471", "affiliation": "IBM T.J. Watson Research Center, Yorktown Heights, NY", "person_id": "PP15029416", "email_address": "", "orcid_id": ""}, {"name": "Barbara G. Ryder", "author_profile_id": "81100632248", "affiliation": "Rutgers University, Piscataway, NJ", "person_id": "PP14217204", "email_address": "", "orcid_id": ""}, {"name": "Ophelia Chesley", "author_profile_id": "81100299986", "affiliation": "Rutgers University, Piscataway, NJ", "person_id": "P698421", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1028976.1029012", "year": "2004", "article_id": "1029012", "conference": "OOPSLA", "title": "Chianti: a tool for change impact analysis of java programs", "url": "http://dl.acm.org/citation.cfm?id=1029012"}