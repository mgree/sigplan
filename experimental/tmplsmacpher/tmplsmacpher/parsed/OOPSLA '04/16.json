{"article_publication_date": "10-01-2004", "fulltext": "\n Method-Level Phase Behavior in Java Workloads Andy Georges Dries Buytaert Lieven Eeckhout Koen De Bosschere \nDepartment of Electronics and Information Systems (ELIS), Ghent University St.-Pietersnieuwstraat 41, \nB-9000 Gent, Belgium {ageorges,dbuytaer,leeckhou,kdb}@elis.ugent.be ABSTRACT Java workloads are becoming \nmore and more prominent on various computing devices. Understanding the behavior of a Java workload which \nincludes the interaction between the application and the virtual machine (VM), is thus of primary importance \nduring performance analysis and optimization. Moreover, as contemporary software projects are increasing \nin complexity, automatic performance analysis techniques are indispensable. This paper proposes an o.-line \nmethod\u00adlevel phase analysis approach for Java workloads that con\u00adsists of three steps. In the .rst step, \nthe execution time is computed for each method invocation. Using an o.-line tool, we subsequently analyze \nthe dynamic call graph (that is annotated with the method invocations execution times) to identify method-level \nphases. Finally, we measure perfor\u00admance characteristics for each of the selected phases. This is done \nusing hardware performance monitors. As such, our approach allows for linking microprocessor-level infor\u00admation \nat the individual methods in the Java application s source code. This is extremely interesting information \ndur\u00ading performance analysis and optimization as programmers can use this information to optimize their \ncode. We eval\u00aduate our approach in the Jikes RVM on an IA-32 platform using the SPECjvm98 and SPECjbb2000 \nbenchmarks. This is done according to a number of important criteria: the overhead during pro.ling, the \nvariability within and between the phases, its applicability in Java workload characteriza\u00adtion (measuring \nperformance characteristics of the various VM components) and application bottleneck identi.cation. \nCategories and Subject Descriptors C.4 [Performance of Systems]: design studies, measure\u00adment techniques, \nperformance attributes General Terms Measurement, Performance, Experimentation Permission to make digital \nor hard copies of all or part of this work for personal or classroom use is granted without fee provided \nthat copies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. OOPSLA 04, Oct. 24-28, 2004, Vancouver, British \nColumbia, Canada. Copyright 2004 ACM 1-58113-831-8/04/0010 ...$5.00.  Keywords workload characterization, \nperformance analysis, Java work\u00adloads, virtual machine technology 1. INTRODUCTION The execution of a \nJava application involves a complex interaction between the Java code and the virtual machine (VM). Consequently \nthe behavior that is observed at the micro-architectural level when executing Java workloads is not just \na function of the application or the VM, but of the interaction between both. In addition to that, the \nappli\u00adcations themselves are growing in size and complexity and VM s are complex as well consisting of \na number of sub\u00adcomponents to drive the managed run-time system, such as the interpreter, compiler, optimizer, \ngarbage collector, class loader, .nalizer, thread scheduler, etc. As a result of that, understanding \nthe behavior of a Java workload is non-trivial which increases the demand for automatic approaches to \nan\u00adalyze Java workload behavior. The purpose of this paper is to study method-level phase behavior in \nJava workloads. The notion of a phase is de\u00ad.ned as a set of parts of the program execution with sim\u00adilar \nbehavior which do not necessarily need to be tempo\u00adrally adjacent. The underlying assumption of method-level \nphase behavior is that phases of execution correspond to the code that gets executed. In particular, \ndi.erent meth\u00adods are likely to result in dissimilar behavior and di.erent invocations of the same method \nare likely to result in sim\u00adilar behavior. There has been some work in the literature that studies whether \nmethods are of an appropriate granu\u00adlarity to detect program phase behavior [6, 17, 20]. Those studies \ndid show that the method level is at least as good as lower levels (basic block level and loop level) \nespecially for applications with lots of method calls in which each method is quite small [20]. And this \nis the case for object-oriented workloads, such as Java [24]. In other words, the granularity of method-level \nphases is course-grained enough to identify major phases, and at the same time is .ne-grained enough \nto provide su.cient detail. Our method-level phase behavior analysis is an o.-line analysis that consists \nof three steps. In a .rst step, we de\u00adtermine how much time the Java workload spends in dif\u00adferent portions \nor methods of the application. This is done by instrumenting all methods to read microprocessor per\u00adformance \ncounter values to track the amount of time that is spent in each method. The result of this .rst step \nis an an\u00adnotated dynamic call graph. Using an o.ine analysis (step 2) we then determine the methods in \nwhich the application component subcomponent description memory hierarchy L1 I-cache L1 D-cache L2 cache \nL1 I-TLB L2 I-TLB L1 D-TLB L2 D-TLB 64KB two-way set-associative, 64-byte lines, LRU replacement with \nnext line prefetching 64KB two-way set-associative, 8 banks with 8-byte lines, LRU write-allocate, write-back, \ntwo access ports 64 bits each 256KB 16-way set-associative, uni.ed, on-chip, exclusive 24 entries, fully \nassociative 256 entries, four-way set-associative 32 entries, fully associative 256 entries, four-way \nset-associative branch prediction BTB RAS taken/not-taken branch target bu.er, two-way set-associative, \n2048 entries return address stack, 12 entries gshare 2048-entry branch predictor with 2-bit counters \nsystem design bus 266MHz, 2.1GiB per second pipeline stages integer .oating-point 10 cycles 15 cycles \ninteger pipeline pipeline 1 pipeline 2 pipeline 3 integer execution unit and address generation unit \nalso allows integer multiply integer execution unit and address generation unit idem .oating-point pipeline \npipeline 1 pipeline 2 pipeline 3 3DNow! add, MMX ALU/shifter and .oating-point add 3DNow!/MMX multiply/reciprocate, \nMMX ALU and .oating-point multiply/divide/square root .oating-point constant loads and stores Table \n1: The AMD Athlon XP microprocessor summary. spends a signi.cant portion of its total execution time \nwith the additional constraint that one invocation of the method takes a signi.cant portion of the total \nexecution time as well. This is to avoid selecting methods that are too small. Dur\u00ading a second run of \nthe application (step 3), these selected methods are instrumented and performance characteristics are \nmeasured. Measuring these performance characteristics is done using the hardware performance counters. \nIn this step, we measure a number of characteristics such as branch misprediction rate, cache miss rate, \nnumber of retired in\u00adstructions per cycle, etc. As such, we obtain detailed perfor\u00admance characteristics \nfor the major method-level execution phases of the Java application. In addition to the method\u00adlevel \nphases, we also measure performance characteristics for major parts of the VM, such as the compiler/optimizer, \nthe garbage collector, the class loader, the .nalizer, etc. There are several interesting applications \nfor this work. First, for application programmers it is important to under\u00adstand the behavior of the \nJava workload in all its complex\u00adity in order to optimize its performance. Using automatic techniques \nto characterize Java workloads can be helpful to identify performance bottlenecks with limited e.ort. \nSec\u00adond, for VM developers, automatic workload characteriza\u00adtion helps to get insight in how a Java application \ninteracts with its VM, which allows improving the performance of the VM under development. Third, our \napproach also provides interesting insights into phase behavior. Detecting program execution phases and \nexploiting them has received increased attention in recent literature. Various authors have pro\u00adposed \nways of exploiting phase behavior. One example is to adapt the available hardware resources to reduce \nenergy consumption while sustaining the same performance [6, 10, 26, 17]. Another example is to use phase \ninformation to guide simulation-driven processor design [25]. The idea is to select one single sample \nfrom each phase for simulation in\u00adstead of the complete benchmark execution. On the software side, JIT \ncompilers in VM s [3, 4] and dynamic optimization frameworks [5, 22] heavily rely on implicit phase behavior \nto optimize code. Fourth, in future work we plan to build a performance model to estimate Java performance. \nIn such a model, performance models of di.erent Java components need to be combined to form an overall \nperformance model of the Java application. We believe that the granularity of method-level phases that \nare identi.ed in this paper will be the right choice for this purpose. This paper is organized as follows. \nThe next section de\u00adtails on our experimental setup. Section 3 discusses our o.\u00adline approach for identifying \nmethod-level phase behavior. Section 4 discusses the statistical data analysis techniques that we have \nused to quantify the variability between and within phases. The results of our phase analysis are pre\u00adsented \nin section 5. Section 6 discusses related work. Fi\u00adnally, we conclude in section 7.  2. EXPERIMENTAL \nSETUP In this section we discuss the experimental setup: our hardware platform, the use of performance \ncounters, Jikes RVM in which all experiments are done, and the Java appli\u00adcations that are used in the \nevaluation section of this paper. 2.1 Hardware platform In this paper we use the AMD Athlon XP microproces\u00adsor \nfor our measurements, see Table 1. The AMD Athlon XP is a superscalar microprocessor implementing the \nIA-32 instruction set architecture (ISA). It has a pipelined mi\u00adcroarchitecture in which up to three \nx86 instructions can be fetched. These instructions are fetched from a large prede\u00adcoded 64KB L1 instruction \ncache (I-cache). For dealing with the branches in the instruction stream, branch prediction is done using \na global history (gshare) based taken/not-taken branch predictor, a branch target bu.er (BTB) and a return \naddress stack (RAS). Once fetched, each (variable-length) x86 instruction is decoded into a number of \nsimpler (and .xed-length) macro-ops. Up to three x86 instructions can be translated per cycle. These \nmacro-ops are then passed to the next stage in the pipeline, the instruction control unit (ICU) which \nba\u00adsically consists of a 72-entry reorder bu.er. From this re\u00adorder bu.er, macro-ops are scheduled into \nan 18-entry in\u00adteger scheduler and a 36-entry .oating-point scheduler for integer and .oating-point operations, \nrespectively. The 18\u00adentry integer scheduler is organized as a collection of three 6-entry deep reservation \nstations, each reservation station serving an integer execution unit and an address generation unit. \nThe 36-entry .oating-point scheduler (FPU: .oating\u00adpoint unit) serves three .oating-point pipelines executing \nx87, MMX and 3DNow! operations. In the schedulers, the macro-ops are broken down to ops which can execute \nout\u00adof-order. Next to these schedulers, the AMD K7 microarchi\u00adtecture also has a 44-entry load-store \nunit. The load-store unit consists of two queues, a 12-entry queue for L1 D-cache load and store accesses \nand a 32-entry queue for L2 cache and memory load and store accesses requests that missed in the L1 D-cache. \nThe L1 D-cache is organized as an eight\u00adbank cache having two 64-bit access ports. Another interesting \naspect of the AMD Athlon microar\u00adchitecture is the fact that the L2 uni.ed cache is an exclusive cache. \nThis means that cache blocks that were previously held by the L1 caches but had to be evicted from L1, \nare held in L2. If the newer cache block that is to be stored in L1 previously resided in L2, that cache \nblock will be evicted from L2 to make room for the L1 block, i.e., a swap oper\u00adation is done between \nL1 and L2. If the newer cache block that is to be stored in L1 did not previously reside in L2, a cache \nblock will need to be evicted from L2 to memory. 2.2 Performance counters Modern processors are often \nequipped with a set of per\u00adformance counter registers. These registers are designed to count microprocessor \nevents that occur during the execution of a program. They allow to keep track of the number of re\u00adtired \ninstructions, elapsed clock cycles, cache misses, branch mispredictions, etc. Generally, there are only \na limited num\u00adber of performance counter registers available on the chip. On the AMD Athlon, there are \nfour such registers. How\u00adever, the total number of microprocessor events that can be traced using these \nperformance counters exceeds 60 in to\u00adtal. As a result, these registers need to be programmed to measure \na particular event. The events that are traced for this study are given in Table 2. These events are \ncommonly used in architectural studies to analyze program execution behavior. For most of the analyzes \ndone in this paper, we use derived performance metrics. These performance met\u00adrics are obtained by dividing \nthe number of events by the number of retired instructions. As such, we use events that occurred per \ninstruction. We deem this to be more mean\u00adingful than the often-used miss rates. For example, we will \nuse the number of cache misses per instruction instead of the number of cache misses per cache access. \nThe reason is that the number of cache misses per instruction relates more di\u00adrectly to performance than \nclassical cache miss rate since it mnemonic description cycles elapsed clock cycles during execution \nret instr retired instructions L1-D-misses L1 D-cache misses L2-D-misses L2 D-cache misses L1-I-misses \nL1 I-cache misses L2-I-misses L2 I-cache misses L1-L-misses L1 load misses L1-S-misses L1 store misses \nL2-L-misses L2 load misses L2-S-misses L2 store misses I-TLB-misses Instruction TLB misses D-TLB-misses \nData TLB misses br mpred branches mispredicted res stall resource stalls Table 2: Performance counter \nevents traced on the AMD Athlon XP. also incorporates the number of cache accesses per instruc\u00adtion. \nThus, the performance metrics derived from the events shown in Table 2, include e.g. CPI (clock cycles \nper retired instruction), L1 D-cache misses per retired instruction, etc. Performance counters have several \nimportant bene.ts over other characterization methods: less slowdown since mea\u00adsurements happen at native \nexecution speed, their ease of use, and their high accuracy compared to simulation-based and instrumentation-based \napproaches. However, there are also a number of issues that need further attention. First, measuring \nmore than 4 events in our setup is impossible. As such, multiple runs are required to measure more than \n4 events. Second, non-determinism can lead to slightly dif\u00adferent performance counter values when running \nthe same program multiple times. To address this issue, we measure each performance counter four times \nand use the average during analysis. In this study, the performance counter values are accessed through \nthe VM, see the next section. In turn, the VM makes use of the following tools: (i) the perfctr1 Linux \nkernel patch, which provides a kernel module to access the processor hardware, and (ii) Performance API \n(PAPI) [7], a high-level library presenting a uniform interface to the per\u00adformance counters on multiple \nplatforms. The kernel patch allows tracing a single process, maintaining the state of the performance \ncounters across kernel thread switches. The PAPI library presents a uniform manner for accessing the \nperformance counters through the kernel module. Not all PAPI de.ned events are available on every platform, \nand not all native AMD events can be accessed through PAPI. However, for our purposes, it provides a \nsu.cient set of events. 2.3 Jikes RVM We use the Jikes Research Virtual Machine (RVM) [2, 3, 8] in this \nstudy. Jikes RVM is mainly targeted at server side applications. It is written (almost) entirely in Java. \nThe Jikes RVM uses a compilation-only scheme for translating Java bytecodes to native machine instructions. \nThe Jikes RVM comes with several compilation strategies: baseline, optimizing and adaptive. This paper \nuses the most advanced 1 http://user.it.uu.se/~mikpe/linux/perfctr/ Figure 1: Overview of the Jikes \nRVM tracing system compilation strategy, namely adaptive. In this scheme, Jikes RVM compiles each method \non its .rst invocation using the baseline compiler and adaptively optimizes hot meth\u00adods. Multiple recompilations \nare possible, each time using more optimizations. The Jikes RVM also supports di.erent garbage collection \nmechanisms. The garbage collector in our experiments implements a generational garbage collection (GC) \nstrategy with Mark-Sweep to clean the mature-object space, i.e. the CopyMS scheme. To build the Jikes \nRVM, we used (i) the Jikes2 Java source-to-bytecode compiler (version 1.18) to compile the Jikes RVM \nsource code class .les, (ii) the Blackdown VM to build the boot image with the Jikes RVM optimizing compiler \nframework, and (iii) the GNU C and C++ compilers to compile the few C/C++ source .les. The Jikes RVM \nitself is the CVS head (development) version from January 2004. The threading system multiplexes n Java \nthreads (appli\u00adcation and VM) onto m native (kernel) threads that are scheduled by the operating system. \nA command line option speci.es the number of kernel threads that are created by the Jikes RVM. Usually, \nthere is one kernel thread used for each physical processor, also referred to as a virtual proces\u00adsor \nbecause multiple Java threads can be scheduled by the VM within the single kernel thread. In our setup, \nwe have used a single virtual processor. Current implementations of the Jikes RVM include sup\u00adport for \nhardware performance counters on both the IA-32 and PowerPC platforms. On the IA-32 platform, access \nto the processor hardware is done through the PAPI library (discussed above), see Figure 1. The Hardware \nPerformance Monitor (HPM) subsystem of the Jikes RVM de.nes a set of methods to access the PAPI functions, \nsuch as starting, stopping, and resuming the counting of events, as well as reading the counter values. \nKeeping track of the events in the Jikes RVM is done as follows. Essentially, each Java thread keeps \ntrack of the performance counter events that occur while it is the executing thread on the virtual proces\u00adsor. \nEach time the VM schedules a virtual context switch, the removed thread reads the counter values, accumulates \nthem with its existing values, and resets the counters. As 2http://www-124.ibm.com/developerworks/opensource/ \njikes/ such, a scheduled thread only observes counter values for the events that occur while it is executing. \nThis mecha\u00adnism for reading performance counter values is the standard implementation within the Jikes \nRVM. For a more detailed description on this, we refer to [28]. In section 3, we will detail how we extended \nthis approach for measuring perfor\u00admance counter values on a per-method basis. 2.4 Java applications \nWe use the SPECjvm98 and SPECjbb2000 benchmark suites in this study. SPECjvm983 is a client-side Java \nbench\u00admark suite consisting of seven benchmarks. For each of these, SPECjvm98 provides three inputs sets: \ns1, s10 and s100. Contradictory to what their names suggest, the size of the input set does not increase \nexponentially. For some benchmarks, a larger input indeed increases the problem size. For other benchmarks, \na larger input executes a smaller input multiple times. SPECjvm98 was designed to evaluate combined hardware \n(CPU, caches, memory, etc.) and soft\u00adware aspects (virtual machine, kernel activity, etc.) of a Java \nenvironment. However, they do not include graphics, networking or AWT (window management). In this study, \nwe used the s100 input set. The VM was set to use 64MiB4 heap for SPECjvm98 benchmarks. SPECjbb2000 (Java \nBusiness Benchmark)5 is a server\u00adside benchmark suite focusing on the middle-tier, the busi\u00adness logic, \nof a three-tier system. We have used a modi.ed version of this benchmark, known as PseudoJBB, which exe\u00adcutes \na .xed amount of transactions, instead of running for a predetermined period of time. The benchmark was \nrun with 8 warehouses. The VM s heap size was set to 384MiB.  3. METHOD-LEVEL PHASES As stated in the \nintroduction, Java applications are com\u00adplex workloads due to the close interaction between the Java \napplication and the VM on which it is running. To gain in\u00adsight in how Java workloads behave we identify \nthree major issues that need to be addressed. First, a distinction needs to be made between the Java \napplication and the VM, i.e. the behavior of the application should be separated from the behavior of \nthe VM. Second, the VM itself is a complex piece of software in which various components interact with \neach other to implement a managed run-time system. Several important VM components can be identi.ed: \nclass loader, compiler, optimizer, thread scheduler, .nalizer, garbage col\u00adlector, etc. As such, each \nof these components need to be pro.led to extract performance characteristics. Third, the application \nitself may consist of several phases of execution. Previous work has extensively shown that applications \nex\u00adhibit phase behavior as a function of time, i.e. programs go from one phase to another during execution. \nRecall that we consider a phase to be a set of parts of a program exe\u00adcution that exhibit similar characteristics; \nand these parts of similar behavior do not need to be temporally adjacent. In this paper, we study method-level \nphase behavior, more in particular we consider a method including all its callees. 3http://www.spec.org/jvm98/ \n4The notations KiB, MiB, and GiB used in this pa\u00adper are SI standard notations for binary multiples, \n 210 220 230 and denote , and , respectively. See also http://physics.nist.gov/cuu/Units/binary.html. \n5http://www.spec.org/jbb2000/ This includes all the methods called by the selected method, i.e. all the \nmethods in the call graph of which the selected method is the root. There are two motivations for doing \nthis. First of all, the granularity of a method including its callees is not too small to introduce too \nmuch overhead dur\u00ading pro.ling. Second, the granularity is .ne-grained enough to identify phases of execution. \nPrevious work on phase clas\u00adsi.cation [20] has considered various methods for identifying phases, ranging \nfrom the instruction level, the basic block level, the loop level up to the method level. From this re\u00adsearch, \nthe authors conclude that phase classi.cation using method level information is .ne-grained enough to \nidentify phases reliably. Especially when the application has small methods. This is generally the case \nfor applications written in object-oriented languages, of which Java is an example. The use of methods \nas a unit for phase classi.cation was also studied by Balasabrumonian et al. [6] and Huang et al. [17]. \nIt is important to state that the purpose of this paper is to identify method-level phase behavior using \no.-line tech\u00adniques. This is particularly useful for Java and VM develop\u00aders during performance analysis \nof their software. Method\u00adlevel phase behavior allows them to improve their software since the performance \ncharacteristics that are obtained from the hardware performance monitors are linked directly to the source \ncode of the application and virtual machine. For comparison, Sweeney et al. [28] read performance counter \nvalues on virtual context switches, and output the method ID of the method that is executing at that \ntime to a trace .le. In our methodology, we speci.cally link performance counter values to the methods. \nThis is an important di.er\u00adence because the method executing at the virtual context switch is not necessarily \nthe method that was executed dur\u00ading a major fraction of the time slice just before the virtual context \nswitch. A second motivation for studying o.-line phase behavior is that it can be used as a reference \nfor dy\u00adnamic (on-line) phase classi.cation approaches. In other words, the statically identi.ed phases \ncan be used for eval\u00aduation purposes of dynamic phase classi.cation techniques. Obviously, to identify \nrecurring phases, static phase analysis has the advantage over dynamic phase analysis as it can look \nat the future by looking ahead in the trace .le. A dynamic approach on the other hand has to anticipate \nphase behavior and as such, can result in suboptimal phase identi.cation. In addition, the resources \nthat are available during o.-line analysis are much larger than in case of on-line analysis, ir\u00adrespective \nwhether phase classi.cation is done in software or in hardware. Yet another motivation for studying o.\u00adline \nmethod-level phase classi.cation is for embedded and specialized environments in which the a priori information \nconcerning the phase behavior in the Java application can be useful. The following issues are some of \nthe more speci.c goals we want to meet using our o.-line phase analysis approach. We want to gather \ninformation from the start to the end of the program s execution. We want maximal coverage with no gaps. \n The overhead when pro.ling methods should be small enough not to interfere with normal program execu\u00adtion. \nThis means that tracing all executed methods is not a viable option. Also, we want the volume of the \ntrace data to be acceptable.  We want to gather as much information as possible. At a minimum, the collected \ninformation should be su.ciently .ne-grained such that transitions in Java performance characteristics \ncan readily be identi.ed. Such transitions can be caused by thread switches, e.g. the garbage collector \nis activated, or because the appli\u00adcation enters a method that shows di.erent behavior from previously \nexecuted methods. To meet the goals of this paper, we use the following o.\u00adline phase analysis methodology. \nDuring a .rst run of the Java application, we measure the number of elapsed clock cycles in each method \nexecution. This information is col\u00adlected in a trace .le that is subsequently used to annotate a dynamic \ncall graph. A dynamic call graph is a tree that shows the various method invocations during a program \nex\u00adecution when traversed in depth-.rst order [1]. In a second step of our methodology, we use an o.-line \ntool that an\u00adalyzes this annotated dynamic call graph and determines the major phases of execution. The \noutput of this second step is a Java class .le that describes which methods are responsible for the major \nexecution phases of the Java ap\u00adplication. In the third (and last) step, we link this class .le to the \nVM and execute the Java application once again. The Java class .le that is linked to the VM forces the \nVM to measure performance characteristics using the hardware performance monitors for the selected methods. \nThe result of this run is a set of detailed performance characteristics for each method-level phase. \n3.1 Mechanism This section details on how a Java workload is pro.led in our methodology. We .rst discuss \nhow the methods in the Java application are instrumented. This mechanism will be used during two steps \nof our methodology: when measuring the execution time of each method execution during the .rst run (step \n1), and when measuring the performance charac\u00adteristics of the selected methods (step 3). The only di.er\u00adence \nbetween both cases is that in step 1 we instrument all methods. In step 3, we only pro.le the selected \nmethods. In the second subsection, we detail on what information is col\u00adlected during pro.ling. Subsequently, \nwe address pro.ling the components of the VM. 3.1.1 Instrumenting the application methods Methods compiled \nby the VM compilers consist of three parts: (i) the prologue, (ii) the main body of the method, and (iii) \nthe epilogue. The prologue and epilogue handle the calling conventions, pushing and popping the callee \ns stack frame, yielding at a thread switch, etc. The goal is to cap\u00adture as many of the generated events \nduring the execution of a method. To achieve this, we add our instrumentation to the method s prologue \nand to the beginning of the method s epilogue. Methods are instrumented on-line by all the Jikes RVM \ncompilers, i.e. the baseline compiler as well as the op\u00adtimizing compiler. Extending the baseline compiler \nto instrument methods is quite straightforward. It involves emitting the machine code to call the instrumentation \nfunctionality in the Jikes RVM run-time facilities. Calling the instrumentation functional\u00adity is done \nusing the Jikes RVM HPM API. Adding calls to baseline compiled methods introduces new yield points. As \neach yield point is a potential GC point (or safe point), it is necessary to update the stack maps accordingly. \nIf not, Figure 2: Tracing the performance counter events at the prologue and epilogue of a method. referenced \nobjects might not be reachable for the GC and risk being erroneously collected. For the optimizing compiler, \nthings are slightly more com\u00adplicated. Optimizations are directed by the optimization planner, and involve \nmultiple layers, from a high level rep\u00adresentation to a machine code level representation. Our in\u00adstrumentation \ninvolves adding an extra compiler phase to the compiler plan in the High Intermediate Representation \n(HIR) optimization set. Basically, we walk through the con\u00adtrol .ow graph of each method and add similar \ncall instruc\u00adtions to the prologue and epilogue as we did for baseline compiled methods. Next to the \nbaseline and optimizing compiler, Jikes RVM also employs an on-stack replacement (OSR) scheme [16]. OSR \nallows the machine code of methods that are execut\u00ading to be replaced by an optimized version of that \nmachine code. This is especially useful for long-running methods, as the VM does not need to wait until \nthe method .nishes ex\u00adecuting to replace the code that is currently executing with the newer optimized \ncode. For this, certain OSR safe-points are available in the compiled code. At these points, the OSR \nmechanism can interrupt the thread executing the code and replace it with the optimized version. Our \nimplementation also supports OSR. Regardless of the compiler that was used to generate the executable \nmachine code, we call our tracing framework as soon as the new frame for the callee has been established, \nsee Figure 2. At this point, the thread executing the method up\u00addates its counter values, and suspends \ncounting through the Jikes RVM HPM interface. As such, no events are counted during the logging of the \ncounter values. When the trace values have been stored into a trace bu.er, counting is re\u00adsumed. To enhance \nthroughput, the trace data is stored in a bu.er. We maintain a per-thread cyclic list, which con\u00adtains \ntwo 128KiB bu.ers. To ensure that these bu.ers are never touched by the garbage collector, they are allocated \noutside of the Java heap. Each time one of the bu.ers for a thread is full, it is scheduled to be written \nto disk, and the other bu.er is used to store the trace data of that thread. A separate thread6 stores \nthe full bu.er contents to disk in a properly synchronized manner. The same action sequence occurs at \nthe method epilogue, just before the return value is loaded into the return register(s) and the stack \nframe is popped. When a method frame is popped because of an uncaught exception, we also log the counter \nvalues at that point. In summary, this approach reads the performance monitor values when entering the \nmethod and when exiting the method. The di.erence between both values gives us the performance metric \nof the method including its callees. For computing the performance metrics of the method itself, i.e. \nexcluding its callees, the performance metrics of the callees need to be subtracted from the caller method. \nFrom Figure 2, it can be observed that the events occur\u00adring before reading the counter values in the \nprologue and the events in the epilogue of a method are attributed to the calling method. However, this \ninaccuracy is negligible for our purposes. In our methodology, we need to pay special attention to exceptions \nwhich can be thrown either implicitly or explic\u00aditly. The former are thrown by the VM itself whereas \nthe latter are thrown by the program. In both cases, whenever an exception is thrown, control must be \ntransferred from the code that caused the exception to the nearest dynamically\u00adenclosing exception handler. \nTo do so, Jikes RVM uses stack unwinding: stack frames are popped one at a time until an exception handler \nis reached. When a frame is popped by the exception handling mechanism, the normal (instru\u00admented) epilogue \nis not executed, i.e. a mismatch in prologue versus epilogue. To solve this problem, we instrumented \nthe exception handling mechanism as well to assure that the trace always contains records for methods \nthat terminate because of an uncaught exception. 3.1.2 Logging the trace data An instrumented run of \nour application results in multiple traces, one with the IDs for the compiled methods (baseline, optimized \nand OSR-compiled), and the others with the rel\u00adevant counter information per thread. Each record in the \nlatter requires 35 bytes at most, and provides the following information: A 1-byte tag, indicating the \nrecord type (high nibble) and the number of counter .elds (1 up to 4) used in the record (low nibble). \nThe record type denotes whether the record concerns data for a method entry, a method exit, a method \nexit through an exception, an entry into the compiler, a virtual thread switch, etc.  Four bytes holding \nthe method ID. This should prove more than su.cient for even very large applications.  6This is an OS-level \nPOSIX thread, not a VM thread. This ensures that storing the trace data does not block the Vir\u00adtual Processor \nPOSIX thread on which the Jikes RVM exe\u00adcutes. Eight bytes per counter .eld in the record. We can measure \nup to four hardware performance monitor val\u00adues at a time. It is possible to use a scheme in which the \ntraces for each thread are written to a single .le. In this case, we add extra synchronization to ensure \nthe order of the entries in the trace is the same as the execution order. The disadvantage here is that \nthere occurs a sequentialization during the pro.ling run, which can be bothersome when using multiple \nvirtual processors on a multi-processor system. Also, in this case, each record will contain two extra \nbytes for the thread ID. The total trace .le size is thus a function of the number of method invocations, \nthe number of virtual context switches and the number of traced events. Again, for clari.cation, the \nsame structure is used for both the .rst step of our methodology (measuring execution times for each \nmethod) and the third step (measuring performance characteristics for the selected phases). However, \nfor the .rst step we apply a heuristic so that we do not need to instrument all methods; this reduces \nthe run-time overhead and prevents selecting wrapper methods as the starting point of a phase. A method \nis instrumented if the bytecode size of its body is larger than a given threshold (50 bytes), or if the \nmethod contains a backward branch, i.e. can contain a loop. 3.1.3 Instrumenting VM routines As mentioned \nearlier, a VM consists of various compo\u00adnents, such as class loader, compiler, optimizer, garbage collector, \n.nalizer, thread scheduler, etc. To gain insight in Java workload behavior, it is thus of primary importance \nto pro.le these components. For most of these, this is eas\u00adily done using the available Jikes RVM HPM \ninfrastructure since they are run in separate VM threads. This is the case for the .nalizer, the garbage \ncollector and the optimizer (which uses six separate threads). To be able to capture the behavior of \nthe compiler, we had to undertake special ac\u00adtion since calling the compiler and optimizer is done in \nthe Java application threads. In case of the baseline compiler, the compilation is done in the Java application \nthread itself. In case of the optimizing compiler, the method is queued to be optimized by the optimizer \nthreads. These two cases were handled in our modi.ed Jikes RVM implementation by manually instrumenting \nthe runtimeCompiler and compile methods from VM Runtime.  3.2 Phase identi.cation This section discusses \nhow we identify phases using our o.-line phase identi.cation tool. Our tool takes a trace .le with timing \ninformation about method calls and thread switches (see section 3.1.2) as input, analyzes it, and outputs \na list of unique method names that represent the phases of the application. To select method-level phases, \nwe use the algorithm pro\u00adposed by Huang et al. [17] which requires two parameters, called .weight and \n.grain . The algorithm selects methods or phases as follows. Suppose the execution time of a pro\u00adgram \ntakes cT clock cycles. Consider a method m for which the total execution time in this method including \nits callees equals cm = ptotal \u00b7 cT clock cycles, with ptotal < 1. This is the total execution time consumed \nby this method over all its invocations. The average execution time consumed by this method (including \nits callees) equals paverage \u00b7 cm. Method m will then be selected as a phase if the following method \ninformation name total time time/call calls main 1800 1800 1 init 30 30 1 readData 300 200 1 readElement \n200 4 50 print 30 30 1 sortData 1300 1300 1 compare 600 2 300 swap 500 2 250 printData 170 170 1 printElement \n150 3 50 Figure 3: Phase identi.cation example. conditions are met ptotal >.weight paverage >.grain. \nThe basic idea of this de.nition is to select methods in which the program spends a substantial portion \nof its total execu\u00adtion time (this is done through .weight ), and in which the program spends a su.ciently \nlong period of time on each invocation (this eliminates short methods and is realized through .grain \n). As a result, the maximum number of se\u00adlected methods equals 1/.weight and the maximum number of method \ninvocations pro.led during our third step (mea\u00adsuring performance metrics using the hardware performance \nmonitors) equals 1/(.weight \u00b7 .grain). To illustrate the phase identi.cation algorithm, consider the \ncall graph in Figure 3. It depicts a call tree that is the result of analyzing the trace .le of a .ctive \nsort program. The sort program reads the data to be sorted, prints an in\u00adtermediate status message to \nthe screen, sorts the data, and .nally prints the sorted data before terminating. For sim\u00adplicity, abstract \ntime units are used. The table in Figure 3 also shows the total time spent in each method, as well as \nthe time spent per invocation. To identify program phases, our tool .rst computes the total and average \nexecution times spent in each method. For all methods, these times include the time spent in their callees. \nIn order for a method to be selected as a program phase, its total execution time needs to be at least \na frac\u00adtion .weight of the program s total execution time, and the average execution time should take \nat least a fraction .grain of the program s total execution time on average. In our running example, \n.weight = 10% and .grain = 5%, would se\u00adlect methods whose total execution time is more than 180 and \nwhose average execution time is more than 90 main, readData and sortData, respectively. benchmark con.guration \noverhead number of phases trace .le size (4 counters) .weight .grain estimated measured static (total) \ndynamic (total) compress 8 \u00d7 10-6% 6 \u00d7 10-6% 1.84% 1.82% 49 (54) 2,664 (19,726,311) 211KiB jess 1.0 % \n1.0% 1.22% 1.27% 10 (211) 23 (22,693,249) 68KiB db 8 \u00d7 10-6% 6 \u00d7 10-6% 7.17% 5.61% 52 (57) 32,223 (1,484,605) \n2590KiB javac 2 \u00d7 10-2% 6 \u00d7 10-3% 2.61% 2.11% 29 (503) 9,864 (23,388,699) 871KiB mpegaudio 2 \u00d7 10-2% \n2 \u00d7 10-3% 10.75% 3.52% 23 (191) 40,064 (29,338,068) 753KiB mtrt 10-2% 10-3% 24.68% 7.83% 30 (94) 88,719 \n(14,859,306) 2,680KiB jack 1.0% 10-2% 3.98% 4.28% 18 (182) 2,528 (4,292,580) 244KiB PseudoJBB 2 \u00d7 10-1% \n2 \u00d7 10-4% 3.69% 6.65% 52 (381) 29,599 (16,224,804) 2,766KiB Table 3: Summary of the selected method-level \nphases for the chosen .weight and .grain values: overhead (esti\u00admated versus real), the number of static \nand dynamic phases, and the size of the trace .le.  4. STATISTICAL TECHNIQUES To quantify the variability \nwithin phases, we use the Co\u00ade.cient of Variation (CoV). We .rst measure the CoV for a given performance \nmetric within a phase. This is de.ned as the standard deviation divided by the mean average value for \nthat metric within the given phase. The overall CoV is then obtained by averaging over the various phases \naf\u00adter weighting with the execution time spent in each of the phases. The smaller the CoV the less variability \nis observed within a phase. For quantifying the variability within phases versus the variability between \nphases, we employ the ANOVA tech\u00adnique [18]. ANOVA allows us to verify whether the mean values for a \ngiven set of characteristics are signi.cantly dif\u00adferent for each phase by looking at the variability \nin the observations. In other words, our goal is to verify whether the variability is larger between \nthe phases than within each phase. If this is the case, our methodology succeeds in iden\u00adtifying recurring \nphases in which the behavior is stable and similar. An ANOVA analysis is done as follows. First, the \ntotal variability in the observations Xij (phase i =1 ...k and measurement j =1 ...ni for phase i) can \nbe expressed \u00af in terms of the deviations from the overall mean X, i.e. Xij - X\u00af. Second, each measurement \nfor a given phase i can be expressed in terms of the deviation from the mean for the \u00af\u00af phase Xi, i.e. \nXij - Xi. Finally, the mean for each phase i can be expressed in terms of the deviation from the overall \nmean, i.e. X\u00afi - X\u00af. This can be expressed by the following formula ` \u00b4`\u00b4` \u00b4 \u00af\u00af Xij - X = X\u00afi - X + Xij \n- X\u00afi |{z}| {z } ti .i,j which is equivalent to XX` XXX \u00b4 22 2 \u00af Xij - X = niti + .i,j ij iij Clearly, \nif the mean values for the various phases are signif\u00adicantly di.erent, the ti will be signi.cantly larger \nthan the .i,j . If this is the case, the F-statistic P i niti 2/(k - 1) PP P .2 /( ni - k) i ji,j i that \nis used to compare both values with respect to all phases will yield a large value. The corresponding \np-value will be smaller than 0.05 for a 95% level of signi.cance or smaller than 0.01 for a 99% level \nof signi.cance. Applying the ANOVA analysis in this paper is done using R [23], an S dialect.  5. RESULTS \n 5.1 Identifying method-level phases Tracing all methods at their entry and exit points is very intrusive. \nThus, it is important to determine a set of method\u00adlevel phases such that the incurred overhead is relatively \nlow, but such that we still get a detailed picture on what hap\u00adpens at the level of the methods being \nexecuted. This is done by choosing appropriate values for .weight and .grain . These values depend on \nthree parameters: (i) the maximum acceptable overhead, (ii) the required level of information, and (iii) \nthe application itself. The o.-line analysis tool aids in selecting values for .weight and .grain by \nproviding an es\u00adtimate for both the overhead and the information yielded by each possible con.guration. \nThe left column of Figure 4 presents the number of selected method-level phases as a function of .weight \nand .grain . The right column of Figure 4 shows the corresponding estimated overhead which is de\u00ad.ned \nas the number of pro.led method invocations (corre\u00adsponding to method-level phases) divided by the total \nnum\u00adber of method invocations. Note that this is not the same as coverage, since selected methods also \ninclude their callees. The coverage is always 100% in our methodology. Figure 4 only presents data for \nthree (jess, jack, and PseudoJBB) out of the eight benchmarks we analyzed because the remaining .ve benchmarks \nshowed similar results. Clearly, the larger .weight , the fewer methods will be selected. The higher \nthe value of .grain , the less short-running methods will be se\u00adlected. Using the plots in Figure 4 we \ncan now choose appropri\u00adate values for .weight and .grain for each benchmark. This is done by inspecting \nthe curves with the estimated overhead. We choose .weight and .grain in such a way that the estimated \noverhead is smaller than 1%, i.e. we want less than 1% of all method invocations to be instrumented. \nThe results for each benchmark are shown in Table 3. Note that the user has some .exibility for determining \nappropriate values for .weight and .grain ; this allows the user to determine the num\u00adber of selected \nmethod-level phases according to his interest. So far, we have used an estimated overhead which is de\u00ad.ned \nas the number of pro.led method invocations versus the total number of method invocations of the complete \npro\u00adgram execution. To validate these estimated overheads, i.e. to compare with the actual overheads, \nwe proceed as fol\u00ad  (a) (b) (c) (d) (e) (f)  Figure 4: Estimating the overhead as a function of \n.weight and .grain for jess (a,b), jack (c,d), and PseudoJBB (e,f). The .gures on the left present the \nnumber of selected method-level phases; the .gures on the right present the estimated overhead. Figure \n5: Accumulated weighted CoV values for the various benchmarks for four characteristics: (a) CPI, (b) \nbranch mispredictions (c) L1 D-cache misses, and (d) L1 I-cache misses. lows. We measure the total execution \ntime of each bench\u00admark (without any pro.ling enabled) and compare this with the total execution time \nwhen pro.ling is enabled for the selected methods. The actual overhead is de.ned as the in\u00adcrease in \nexecution time due to adding pro.ling. Measuring the wall clock execution time is done using the GNU/Linux \ntime command. Table 3 compares the estimated overhead and the actual overhead. We observe from these \nresults that the actual overhead is usually quite small and mostly tracks the estimated overhead very \nwell. This is important since determining the estimated overhead is more convenient than measuring the \nactual overhead. In two cases the estimate is signi.cantly larger than the measured overhead, i.e. for \nmpegaudio and for mtrt. For completeness, Table 3 also presents the number of static method-level phases \nas well as the number of phase invocations or dynamic phases. For reference, the total number of static \nmethods as well as the total number of dynamic method invocations are shown. The rightmost col\u00adumn in \nTable 3 presents the .le size of the trace .le ob\u00adtained from running the Java application while pro.ling \nthe selected method-level phases. Recall that besides applica\u00adtion method records, the trace also contains \ndata regarding thread switches, GC, and compiler activity.  5.2 Variability within and between phases \nThe purpose of this section is to evaluate the variability within and between the phases. Our .rst metric \nis the Co\u00ade.cient of Variation (CoV) which quanti.es the variability within a phase, see section 4. Figure \n5 shows the CoV for the various benchmarks over various characteristics (CPI, L1 D-cache misses, L1 I-cache \nmisses, and branch mispre\u00addictions). We observe that for the CPI and the branch mis\u00adpredictions, the \nCoV is quite small. This indicates that the behavior within a phase is quite stable for these character\u00adistics. \nThe I-cache behavior on the other hand, is not very stable within the phases for mpegaudio. This can \nbe due to the low I-cache miss rate for the mpegaudio for which a similarly small variation exists. Indeed, \na small variation in absolute terms, e.g. 0.0001 versus 0.0002, can result in large variations in relative \nterms (100% in this example). As such, we do not consider this as a major problem since analysts do not \ncare about code having very low cache miss rates. Furthermore, if unstable behavior for a given characteris\u00adtic \nis undesirable, .weight and .grain can be adjusted so that more phases are selected and less variability \nwill be observed within the phases. To get a better view on the performance characteristics within and \nbetween phases, we use boxplots. Figure 6 shows  Figure 6: Boxplots showing the distribution for the \nphases of PseudoJBB on the following characteristics: (a) L1 D-cache misses, (b) L1 I-cache misses, (c) \nbranch mispredictions, and (d) IPC. (a) (b) Figure 7: Performance characteristics for the application \nversus the VM components for PseudoJBB (a) and jack (b). benchmark con.guration ANOVA results .grain \n.weight F -value degrees of freedom p-value compress jess db javac mpegaudio mtrt jack PseudoJBB 8 \u00d7 \n10-6% 1.0 % 8 \u00d7 10-6% 2 \u00d7 10-2% 2 \u00d7 10-2% 10-2% 1.0% 2 \u00d7 10-1% 6 \u00d7 10-6% 1.0% 6 \u00d7 10-6% 6 \u00d7 10-3% 2 \u00d7 \n10-3% 10-3% 10-2% 2 \u00d7 10-4% 37.94 1.74 43.03 117.34 495.95 39.23 63.56 288.24 (48, 2640) (10, 664) (49, \n32339) (20, 10749) (26, 102894) (26, 89004) (14, 2934) (19, 32206) < 10-16 0.067 < 10-16 < 10-16 < 10-16 \n< 10-16 < 10-16 < 10-16 Table 4: Results for ANOVA comparing the means for the observed characteristics. \nthe performance characteristics for the various phases for PseudoJBB. On the X axis of this graph, we \ndisplay all the phases. The Y axes represent the various metrics that were measured: IPC, L1 I-cache \nmiss rate, L1 D-cache miss rate and the branch misprediction rate. For each phase, we dis\u00adplay the mean \nvalue as the middle of the rectangular box. The borders of each box represent the standard deviation \nand the individual points above and under these boxes rep\u00adresent outliers, i.e. not within one standard \ndeviation from the mean. This .gure clearly shows that the performance characteristics can be quite di.erent \nbetween phases. The variability within each phase on the other hand, is usually small. Notable exceptions \nto this are the GC and VM com\u00adpiler. We now quantify the variability between the phases ver\u00adsus the variability \nwithin the phases in a more rigorous way. This is done using an ANOVA test, see section 4. In Ta\u00adble \n4, we show the maximum p-value per benchmark over all characteristics. Recall that the lower p, the better. \nIn our results, p is smaller than 10-16 for nearly all bench\u00admarks, from which we conclude that the mean \nvalues for each characteristic for the various phases are di.erent at a signi.cance level that almost \nreaches certainty. This means that the variability between the phases is signi.cantly larger than the \nvariability within the phases which proves that our o.-line technique reliably identi.es phases with \ndissimilar inter-phase behavior and similar intra-phase behavior. For jess however, the p value reported \nin Table 4 is larger than for the other benchmarks. The higher p-values are due to the I-TLB and D-TLB \nmiss rates, which do not show that much variability between the phases.  5.3 Analysis of method-level \nphase behavior A programmer analyzing application behavior will typi\u00adcally start from a high-level view \nof the program. Two of the .rst things one wants to analyze are where the time is spent, and whether \npotential bottlenecks are due to the ap\u00adplication or the VM. In the .rst subsection, we look at the high-level \nbehavior of Java applications and compare it with the behavior of the VM (GC, compiler, etc.). Once the \nhigh\u00adlevel behavior has been understood, the next logical step is to investigate parts of the application \ninto more detail. The subsequent subsection shows how the programmer can use the information collected \nby our framework to gain insight about the low-level behavior of his program, and how our data can help \nidentify and explain performance bottlenecks. 5.3.1 VM versus application behavior Figure 7(a) shows \nthe number of events occurring in the application versus the VM. This is done here for PseudoJBB. We \nobserve that most of the events occur in the application and not in the VM. Indeed, the total program \nexecution spends 73% of its total execution time in the application; the remaining 27% is spent in the \nVM. The time spent in the VM is partitioned in the time spent in the various VM components: compiler, \noptimizer, garbage collector, .nal\u00adizer and others. We observe that the most dominant part benchmark \napplication garbage collector .nalizer compiler other baseline optimizing compress 89.136% 9.377% 0.006% \n0.205% 0.696% 0.580% jess 56.835% 39.641% 0.003% 0.919% 1.914% 0.688% db 92.211% 6.991% 0.002% 0.128% \n0.455% 0.213% javac 65.463% 28.987% 0.008% 0.940% 3.618% 0.984% mpegaudio 85.000% 7.999% 0.002% 0.559% \n5.821% 0.620% mtrt 65.802% 28.039% 0.005% 0.485% 4.687% 0.982% jack 53.905% 41.556% 0.015% 0.941% 2.317% \n1.265% PseudoJBB 73.348% 22.974% < 0.001% 0.091% 3.532% 0.063% Table 5: The time spent in the application \nand the di.erent VM components. of the VM routines is due to the optimizer (3.5%) and the garbage collector \n(23%). This graph reveals several inter\u00adesting observations. For example, although the optimizer is responsible \nfor only 3.5% of the total execution time, it is responsible for a signi.cantly larger proportion of \nthe L1 D-cache load misses (6.3%) and L2 I-cache misses (13.7%). The garbage collector on the other hand, \naccounts for signif\u00adicantly more L2 D-cache misses (28%) than it accounts for execution time (21%). Another \ninteresting result is that the garbage collector accounts for a negligible fraction of the L1 and L2 \nI-cache and I-TLB misses. This is due to the fact that the garbage collector has a small instruction \nfootprint while accessing a large data set. Figure 7(b) presents a similar graph for jack. The per\u00adcentage \nof the total execution time spent in the application is 54%. Of the 46% spent in the VM, 41.5% is spent \nin the garbage collector, 2.3% in the optimizer, 0.9% in the baseline compiler and 1.3% in other VM routines, \nsuch as the thread scheduler, class loader, etc. These results con\u00ad.rm the speci.c behavior of the garbage \ncollector previously observed for PseudoJBB: low L1 and L2 I-cache and I-TLB miss rates and high L2 D-cache \nand D-TLB miss rates (due to writes). The baseline compiler and the optimizer show high L2 I-cache miss \nrates. Table 5 presents the time spent in the application versus the time spent in the VM components \nfor the SPECjvm98 and PseudoJBB benchmarks. The time spent in the applica\u00adtion varies between 54% and \n92% of the total execution time; the time spent in the garbage collector varies between 7% and 42% and \nthe time spent in the optimizer varies between 0.4% and 5.8%. The execution time in the other VM compo\u00adnents \nis negligible. We conclude that Java workloads spend a signi.cant fraction of their total execution time \nin the VM, up to 46% for the long-running applications included in our study. For short-running applications, \nfor example SPECjvm98 with the s1 input set, this fraction will be even larger. It is interesting to \nnote that the three benchmarks (compress, db, and mpegaudio) for which the total execution time spent \nin the application is signi.cantly larger than the average case (89%, 92% and 85%, respectively), were \nde\u00adnoted as simple benchmarks by Shuf et al. [27].  5.3.2 Application bottleneck analysis Pro.lers provide \na means for programmers to perceive the way their programs are performing. Our technique provides an \neasy way to the programmer to gain insight about the performance of their application at the micro-architectural \nlevel. That is, hardware performance counters can be linked to the methods in the source code. Conventional \nmethods on the other hand, are much more labor-intensive and error\u00adprone for the following reasons: (i) \nthe huge amount of data gathered from a pro.ling run, and (ii) the presentation of this huge amount of \ndata usually prevents quick insight. This section shows how our technique can help answer three fundamental \nquestions programmers might ask when optimizing their application: (i) what is the application s bottleneck, \n(ii) why does the bottleneck occur, and (iii) when does the bottleneck occur? To answer the .rst question \n(what is the bottleneck?), we compile a list of phases with the highest CPI values. The methods with \nthe highest CPI are most likely to represent a bottleneck. To answer the second question (why does the \nbottleneck occur?) we investigate the corresponding HPM counters of the bottleneck phase(s). To answer \nthe last ques\u00adtion (when does my bottleneck occur?) we can plot the CPI over time. Table 6 presents the \nmajor bottleneck phases for both the SPECjvm98 benchmarks and for PseudoJBB. Due to space constraints \nTable 6 depicts a subset of all phases only: we show the phases of which the total execution time takes \nmore than 1% or 2% of the program execution time and of which the CPI is above the average CPI, or which \nother\u00adwise display bad behavior for a shown characteristic. This table shows the percentage of the total \nexecution time that is spent in each phase, the average CPI in each phase, the cache miss rates and the \nbranch misprediction rate. This information can be helpful in identifying why these phases su.er from \nsuch a high CPI. For example, high D-cache miss rates suggest that the programmer should try to improve \nthe data memory behavior for the given phases. We can make the following interesting observations these \nare just a few examples to clarify the usefulness of linking microprocessor level information to source-code \nlevel methods. The Compressor.compress method in compress su.ers from high D-cache miss rates. Optimization \nof the data memory behavior can be achieved by applying prefetching.  From all the benchmarks, mtrt \nhas a method with the most mispredicted branches: Scene.RenderScene. This method contains two nested \nloops, iterating over all pixels in the scene to be rendered. Inside the loop there are a number of conditional \nbranches and a call to e.g. Scene.Shade. In turn, the latter shows bad branch behavior due to numerous \n(nested) tests that are conducted to decide on the color of the pixel that is being rendered. This behavior \ncan be optimized by  phase time CPI L1-D L1-I L2-D L2-I br mpred compress garbage collector Compressor.compress \nDecompressor.decompress 9.3773% 58.3916% 25.2042% 1.7778 1.7447 0.9242 4.04 22.91 2.53 0.02 0.01 < 0.01 \n2.59 4.36 0.12 0.01 < 0.01 < 0.01 4.39 7.28 4.83 benchmark average n/a 1.4830 12.25 0.10 2.01 0.04 5.69 \njess Jesp.parse garbage collector Rete.Run 1.1021% 39.6413% 53.8732% 1.8701 1.7647 1.1796 4.52 4.02 4.92 \n5.91 0.02 0.51 1.04 2.58 0.45 1.71 0.01 0.05 14.55 4.33 4.51 benchmark average n/a 1.3959 4.66 0.68 1.12 \n0.17 4.73 db Database.shell sort Database.remove garbage collector Database.set index 85.5593% 4.5821% \n6.9912% 2.3873% 5.1134 2.7155 1.7989 1.5749 26.42 11.33 3.92 5.74 0.02 0.10 0.03 0.08 18.01 6.28 2.45 \n3.38 0.01 0.06 0.01 0.04 4.87 1.36 4.23 0.08 benchmark average n/a 3.9847 4.71 0.05 3.13 0.01 1.07 javac \nSourceClass.check SwitchStatement.check garbage collector SourceClass.compileClass Assembler.collect \nParser.parseClass ConstantPool.write 7.0644% 1.4973% 28.9874% 26.0361% 1.8285% 22.6849% 5.3863% 2.2501 \n1.9129 1.8059 1.7408 1.6994 1.4789 1.0231 8.06 7.91 4.48 5.14 4.96 2.93 1.20 13.13 13.21 0.02 6.53 4.58 \n5.52 0.19 1.74 0.76 2.55 0.98 1.23 0.54 0.33 1.75 0.49 0.01 0.85 0.52 0.44 0.08 23.2 22.87 4.76 16.26 \n13.64 18.26 6.26 benchmark average n/a 1.6747 4.28 4.48 1.32 0.66 13.26 mpegaudio garbage collector lb.read \nt.O 7.9994% 20.7281% 75.1119% 1.7795 0.8602 0.8430 4.15 1.41 0.82 0.03 1.17 0.49 2.62 0.01 < 0.01 0.01 \n< 0.01 < 0.01 4.71 6.38 2.29 benchmark average n/a 0.8157 1.07 0.47 0.03 0.02 3.25 mtrt Scene.RenderScene \ngarbage collector Scene.GetLightColor Scene.Shade Scene.ReadPoly 1.9640% 28.0391% 23.7782% 36.2558% 2.4275% \n2.3249 1.7829 1.4919 1.3496 1.2909 12.32 3.73 9.74 7.21 1.52 18.74 0.02 3.29 2.84 3.32 0.21 2.40 0.68 \n0.42 0.12 0.25 < 0.01 0.03 0.05 0.10 35.98 4.47 8.95 11.42 8.88 benchmark average n/a 1.5389 8.27 2.66 \n1.03 0.07 7.18 jack Jack the Parser Generator. Jack3 1 garbage collector Jack the Parser Generator.production \nJack the Parser Generator.jack input Jack the Parser Generator.expansion choices Jack the Parser Generator.java \ndeclarations and code Jack the Parser Generator Internals.db process ParseGen.build 2.34092% 41.5561% \n1.87112% 2.8412% 20.5098% 19.4081% 2.78693% 2.6689% 1.9655 1.7741 1.7039 1.6014 1.5546 1.3648 1.2737 \n1.1157 5.84 4.04 4.38 3.59 4.46 2.70 2.61 2.27 5.19 0.02 7.41 6.26 7.54 5.1 1.61 0.37 0.57 2.59 0.51 \n0.38 0.22 0.11 0.59 0.69 0.22 0.01 0.73 0.56 0.23 0.09 0.28 0.06 11.53 4.36 15.16 13.69 15.94 12.64 4.74 \n2.41 benchmark average n/a 1.5976 3.83 3.58 1.19 0.24 9.58 PseudoJBB DeliveryTransaction.process garbage \ncollector TransactionManager.go 2.7597% 22.9744% 57.9074% 3.0722 2.1581 2.1219 8.74 5.76 6.77 9.95 0.03 \n7.77 6.45 3.59 2.91 2.61 < 0.01 0.75 17.32 4.35 11.08 benchmark average n/a 2.046 6.02 5.02 2.69 0.57 \n9.13 Table 6: Interesting methods from the SPECjvm98 and SPECjbb2000 (as observed in PseudoJBB) benchmark \nsuites. The L1 and L2 I-cache miss rates, L1 and L2 D-cache miss rates and the branch misprediction rate \nare given as the number of events per 1,000 instructions. changing the code layout to improve the branch \npre\u00addictability. Poor I-cache behavior can be observed for e.g. the expansion choices method in jack. \n For the SPECjvm98 benchmarks, the GC shows a very consistent behavior, with a CPI that remains around \n 1.77. Also, GC shows a very good I-cache behavior both on L1 and L2. This due to the fact that GC (i) \nusually can take quite some time, hence the initial cache misses can be made up for by a longer execution \ntime, and (ii) GC code is usually quite compact.   6. RELATED WORK The .rst subsection details on related \nwork done on char\u00adacterizing Java workloads. In the second subsection, we dis\u00adcuss phase classi.cation \nand detection techniques. 6.1 Java workload characterization Cain et al. [9] characterize the Transaction \nProcessing Council s TPC-W web benchmark which is implemented in Java. TPC-W is designed to exercise \nthe web server and transaction processing system of a typical e-commerce web site. They used both hardware \nexecution (on an IBM RS/6000 S80 server with 8 RS64-III processors) and simu\u00adlation in their analysis. \nKarlsson et al. [19] study the memory system behavior of Java-based middleware. To this end, they study \nthe SPECjbb2000 and SPECjAppServer2001 benchmarks on real hardware as well as through simulation. For \nthe real hard\u00adware measurements, they use the hardware counters on a 16\u00adprocessor Sun Enterprise 6000 \nmultiprocessor server. They measure performance characteristics over the complete bench\u00admark run and \nmake no distinction between the VM and the execution phases of the application. Luo et al. [21] compare \nSPECjbb2000 versus SPECweb99, VolanoMark and SPEC CPU2000 on three di.erent hard\u00adware platforms: the \nIBM RS64-III, the IBM POWER3-II and the Intel Pentium III. All measurements were done us\u00ading performance \ncounters and measure aggregate behavior. Eeckhout et al. [15] measure various performance counter events \nand use statistical data analysis techniques to analyze Java workload behavior at the microprocessor \nlevel. One particular statistical data analysis technique that is used in that paper is principal components \nanalysis which allows to reduce the dimensionality of the data set. This reduced data set allows for \neasier reasoning. In that work, the authors also measured aggregate performance characteristics and made \nno distinction between phases of execution. Dufour et al. [14] present a set of architecture-independent \nmetrics for describing dynamic characteristics of Java ap\u00adplications. All these metrics are bytecode-level \nprogram characteristics and measure program size, the intensiveness of various data structures (arrays, \npointers, .oating-point operations), memory use, concurrency, synchronization and the degree of polymorphism. \nDmitriev [12] presents a bytecode-level pro.ling tool for Java applications, called JFluid. During a \ntypical JFluid session, the VM is started with the Java application without any special preparation. \nSubsequently, the tool is attached to the VM, the application is instrumented, the results are collected \nand analyzed on-line, and the tool is detached from the VM. The instrumentation is done by injecting \ninstru\u00admentation bytecodes into methods of a running program. In JFluid, the user needs to specify which \ncall subgraph, called a task by Dmitriev, from an arbitrary root method is to be instrumented. This method \nhas two major di.erences with our approach: (i) we do not operate at the bytecode level but at the lower \nmicroprocessor level, and (ii) we provide a means to automatically detect these tasks . This relieves \nthe user from manually selecting major tasks of execution. Sweeney et al. [28] present a system to measure \nmicro\u00adprocessor level behavior of Java workloads. To this end, they generate traces of hardware performance \ncounter val\u00adues while executing Java applications. This is done for each Java thread and for each microprocessor \non which the thread is running. The latter can be useful in case of a multipro\u00adcessor environment. The \ninfrastructure for reading perfor\u00admance counter values used by Sweeney et al. is exactly the same as \nthe one used in this paper using HPM in the Jikes RVM except for the fact that our measurements are done \non an IA-32 ISA platform opposed to the PowerPC ISA plat\u00adform. Sweeney et al. read the performance counter \nvalues on every virtual context switch in the VM. This informa\u00adtion is collected for each virtual processor \nand for each Java thread, and written in a per virtual processor record bu.er. Sweeney et al. also present \na tool for graphically exploring the performance counter traces. The major di.erence be\u00adtween the work \nby Sweeney et al. and this paper, is that we collect performance counter values on a per-phase basis \nas opposed to the timing-driven approach of taking one sample on every virtual context switch. The bene.t \nof measuring performance counter values on a per-phase basis is that per\u00adformance counter values can \nbe easily linked to the code that is executed in the phase. We believe this is particularly useful for \nanalysis in general, and for application and VM developers in particular. Moreover, our approach is more \ngeneral than the approach by Sweeney et al. since the infor\u00admation we obtain can be easily transformed \nto behavioral information over time. This can be done by ordering our information on a time-line basis. \nAdditionally, Sweeney et al. use an on-line approach, while we essentially perform an o.ine analysis. \n 6.2 Program phases Several techniques that have been proposed in the recent literature to detect program \nphases divide the total program execution in .xed intervals. For each interval, program char\u00adacteristics \nare measured during program execution. When the di.erence in program characteristics between two con\u00adsecutive \nintervals exceeds a given threshold, the algorithm detects a phase change. These approaches are often \nre\u00adferred to as temporal techniques. The proposed temporal techniques all di.er in what program characteristics \nneed to be measured over the .xed interval. Balasubramonian et al. [6] compute the number of dynamic \nconditional branches executed. A phase change is detected when the di.erence in branch counts between \nconsecutive intervals exceeds a threshold. This threshold is adjusted dynamically during program execution \nto match the program s execution behav\u00adior. Dhodapkar and Smith [11] use the instruction working set \nor the instructions executed at least once. Since repre\u00adsenting a complete working set is impractical, \nespecially in hardware, the authors propose working set signatures which are lossy-compressed representations \nof working sets. Work\u00ading set signatures are compared using a relative signature distance. A program \nphase change is detected when the relative signature distance between consecutive intervals ex\u00adceeds \na given threshold. Sherwood et al. [25, 26] use basic block vectors (BBVs) to identify phases. A BBV \nis a vec\u00adtor in which the elements count the number of times each static basic block is executed in the \n.xed interval. These BBVs are weighted by the number of instructions in the given basic block. A phase \nchange is detected when the Manhattan distance between two consecutive intervals ex\u00adceeds a given threshold. \nThey consider both static and dy\u00adnamic methods for identifying phases in [25] and [26], re\u00adspectively. \nThe purpose of their static phase classi.cation approach was to identify equally behaving intervals through\u00adout \na program execution so that one single representative interval for each phase can be used for e.cient \nsimulation studies. In a follow-up study, Lau et al. [20] study several structures for classifying program \nexecution phases. They study approaches using basic blocks, loops, procedures, op\u00adcodes, register usage \nand memory address information. In contrast to the previously mentioned approaches which all use micro-architecture-independent \ncharacteristics i.e. the metrics are only dependent on the instruction set architec\u00adture (ISA) and not \non the micro-architecture Duesterwald et al. [13] use micro-architecture-dependent characteristics to \ndetect phases. The metrics used by them are the instruc\u00adtion mix, the branch prediction accuracy, the \ncache miss rate and the number of instructions executed per cycle (IPC). These metrics are measured using \nperformance counters over .xed intervals of 10 milliseconds. Next to temporal phase detection approaches, \nthere exist a number of approaches that do not use .xed intervals. Bala\u00adsubramonian et al. [6] consider \nprocedures to identify phases. They consider non-nested and nested procedures as phases. A non-nested \nprocedure is a procedure that includes its com\u00adplete call graph, i.e., including all the methods it calls, \nas is done in this paper. A nested procedure does not include its callees. They concluded that non-nested \nprocedures are better performing than nested procedures. Huang et al. [17] also use procedures to identify \nphases. The method used in our work to identify method-level phases of execution using .weight and .grain \nis based on the approach proposed by Huang et al. Next to this static approach, they also pro\u00adpose a \nhardware-based call stack mechanism to identify pro\u00adgram phase changes. This paper di.ers from the one \nby Huang et al. for at least three reasons. First, we explore the technique for detecting phases in more \ndetail by quantifying the overhead and coverage as a function of .weight and .grain . Huang et al. chose \n.xed .weight = 5% and .grain = 1,000 cy\u00adcles in their experiments. Second, we study Java workloads whereas \nHuang et al. studied SPEC CPU2000 benchmarks. Java workloads provide several additional challenges over \nC-style workloads because of the managed run-time envi\u00adronment. Third, the focus of the work by Huang \net al. was on exploiting phase behavior for energy-e.cient computing. The focus of our work is on using \nphase behavior to increase the understanding during whole-program analysis  7. CONCLUSIONS AND FUTURE \nWORK Java workloads are complex pieces of software in which the Java application closely interacts with \nthe VM. In addi\u00adtion, the applications themselves are becoming increasingly complex due to the ever-increasing \nprocessing power of cur\u00adrent microprocessor systems. Because of this, automatic tools for characterizing \nsuch software systems are becoming paramount during performance analysis. The purpose of this paper was \nto study method-level phase behavior of Java applications. In other words, our goal was to identify methods \n(including their callees) that ex\u00adhibit similar behavior within the phase and dissimilar be\u00adhavior between \nthe phases. The phase analysis framework presented in this paper consists of three steps. In the .rst \nstep, we measure the execution time for each method in\u00advocation using hardware performance monitors which \nare made available through the Jikes RVM HPM API. The sec\u00adond step analyzes this information using an \no.-line tool and selects a number of phases. These phases are subsequently characterized in the third \nstep using performance counters. This characterization includes measuring a number of micro\u00adprocessor \nevents such as cache miss rates, TLB miss rates, branch misprediction rates, etc. for each selected phase. \nUsing this framework, we investigated the phase behav\u00adior of both the SPECjvm98 and SPECjbb2000 benchmark \nsuite. In a .rst set of experiments, we have compared the characteristics of the Java application versus \nthe various VM components. We concluded that Java workloads spend a signi.cant portion of their total \nexecution time (up to 46%) in the VM, more speci.cally in the garbage collector. In addition, the VM \nexhibits a signi.cantly di.erent behavior from the Java application, and this can vary widely over di.erent \napplications. In a second set of experiments, we have focused on the method-level phase behavior of the \nJava application itself. We have shown that our phase analy\u00adsis technique is capable of reliably discriminating \nmethod\u00adlevel phases since a larger variability is observed between the phases than within the phases. \nWe have also shown that the overhead incurred during pro.ling is small. Particularly novel compared to \nexisting work is the fact that our framework can link the microprocessor-level in\u00adformation to the methods \nin the Java application s source code. This provides a way for programmers to identify per\u00adformance bottlenecks \nautomatically which can guide them while optimizing their software. In future work we will extend the \nideas presented in this paper as several future research directions open up. First, we will explore on-line \nphase identi.cation. Detecting and selecting phases at run-time is interesting for program anal\u00adysis \nbecause we could eliminate the required training run. A second future direction is the development of \na framework capable of both visualizing and replaying the pro.le infor\u00admation captured by our tool. Such \na framework could result in new contributions that help gain deep understanding of program behavior of \nJava applications. A third possible di\u00adrection is to develop techniques that are capable of exploit\u00ading \nthe collected phase information. The insights obtained in this paper can be bene.cial for VM, garbage \ncollector and compiler optimizations. Furthermore, the information could lend itself to predict the behavior \nof Java applications based on the method-level phase behavior. Acknowledgments Andy Georges is supported \nby the Institute for the Pro\u00admotion of Innovation by Science and Technology in Flan\u00adders (IWT) in the \nCoDAMoS project, and Dries Buytaert is supported by an IWT grant. Lieven Eeckhout is a Postdoc\u00adtoral \nFellow of the Fund for Scienti.c Research Flanders (Belgium) (F.W.O. Vlaanderen). This research was also \nfunded by Ghent University. 8. REFERENCES [1] Alfred V. Aho, Ravi Sethi, and Je.rey D. Ullman. Compilers: \nprinciples, techniques, and tools. Addison-Wesley Longman Publishing Co., Inc., 1986. [2] B. Alpern, \nB. Alpern, C. R. Attanasio, J. J. Barton, M. G. Burke, P. Cheng, J.-D. Choi, A. Cocchi, S. J. Fink, D. \nGrove, M. Hind, S. F. Hummel, D. Lieber, V. Litvinov, M. F. Mergen, T. Ngo, J. R. Russell, V. Sarkar, \nM. J. Serrano, J. C. Shepherd, S. E. Smith, V. C. Sreedhar, H. Srinivasan, and J. Whaley. The Jalape \nno Virtual Machine. IBM Systems Journal, 39(1):211 238, February 2000. [3] Matthew Arnold, Stephen Fink, \nDavid Grove, Michael Hind, and Peter F. Sweeney. Adaptive optimization in the Jalape no JVM. In Proceedings \nof the 15th ACM SIGPLAN conference on Object-oriented programming, systems, languages, and applications \n(OOPSLA) 2000, pages 47 65. ACM, 2000. [4] Matthew Arnold, Michael Hind, and Barbara G. Ryder. Online \nfeedback-directed optimization of Java. In Proceedings of the 17th ACM SIGPLAN conference on Object-oriented \nProgramming, Systems, Languages, and Applications (OOPSLA) 2002, pages 111 129. ACM, 2002. [5] Vasanth \nBala, Evelyn Duesterwald, and Sanjeev Banerjia. Dynamo: a transparent dynamic optimization system. In \nProceedings of the ACM SIGPLAN 2000 conference on Programming Language Design and Implementation (PLDI) \n2000, pages 1 12. ACM, 2000. [6] Rajeev Balasubramonian, David Albonesi, Alper Buyuktosunoglu, and Sandhya \nDwarkadas. Memory hierarchy recon.guration for energy and performance in general-purpose processor architectures. \nIn Proceedings of the 33rd annual ACM/IEEE international symposium on Microarchitecture, pages 245 257. \nACM, 2000. [7] S. Browne, J. Dongarra, N. Garner, G. Ho, and P. Mucci. A portable programming interface \nfor performance evaluation on modern processors. The International Journal of High Performance Computing \nApplications, 14(3):189 204, 2000. [8] Michael G. Burke, Jong-Deok Choi, Stephen Fink, David Grove, Michael \nHind, Vivek Sarkar, Mauricio J. Serrano, V. C. Sreedhar, Harini Srinivasan, and John Whaley. The Jalape \nno dynamic optimizing compiler for Java. In Proceedings of the ACM 1999 conference on Java Grande, pages \n129 141. ACM, 1999. [9] Harold W. Cain, Ravi Rajwar, Morris Marden, and Mikko H. Lipasti. An architectural \nevaluation of Java TPC-W. In Proceedings of the IEEE International Symposium on High-Performance Computer \nArchitecture. IEEE Computer Society, 2001. [10] Ashutosh S. Dhodapkar and James E. Smith. Managing multi-con.guration \nhardware via dynamic working set analysis. In Proceedings of the 29th annual international symposium \non Computer architecture, pages 233 244. IEEE Computer Society, 2002. [11] Ashutosh S. Dhodapkar and \nJames E. Smith. Comparing program phase detection techniques. In Proceedings of the 36th Annual IEEE/ACM \nInternational Symposium on Microarchitecture, page 217. IEEE Computer Society, 2003. [12] Mikhail Dmitriev. \nSelective pro.ling of Java applications using dynamic bytecode instrumentation. In Proceedings of the \nIEEE International Symposium on Performance Analysis of Systems and Software. IEEE Computer Society, \n2004. [13] Evelyn Duesterwald, Calin Cascaval, and Sandhya Dwarkadas. Characterizing and predicting program \nbehavior and its variability. In Proceedings of the 12th International Conference on Parallel Architectures \nand Compilation Techniques, page 220. IEEE Computer Society, 2003. [14] Bruno Dufour, Karel Driesen, \nLaurie Hendren, and Clark Verbrugge. Dynamic metrics for Java. In Proceedings of the 18th ACM SIGPLAN \nconference on Object-oriented programing, systems, languages, and applications, pages 149 168. ACM, 2003. \n[15] Lieven Eeckhout, Andy Georges, and Koen De Bosschere. How Java programs interact with virtual machines \nat the microarchitectural level. In Proceedings of the 18th ACM SIGPLAN conference on Object-oriented \nprogramming, systems, languages, and applications (OOPSLA) 2003, pages 169 186. ACM, 2003. [16] Stephen \nJ. Fink and Feng Qian. Design, implementation and evaluation of adaptive recompilation with on-stack \nreplacement. In Proceedings of the international symposium on Code generation and optimization, pages \n241 252. IEEE Computer Society, 2003. [17] Michael C. Huang, Jose Renau, and Josep Torrellas. Positional \nadaptation of processors: application to energy reduction. In Proceedings of the 30th annual international \nsymposium on Computer architecture, pages 157 168. ACM, 2003. [18] R. A. Johnson and D.W. Wichern. Applied \nMultivariate Statistical Analysis. Prentice Hall, .fth edition, 2002. [19] Martin Karlsson, Kevin E. \nMoore, Erik Hagersten, and David A. Wood. Memory system behavior of Java-based middleware. In Proceedings \nof the IEEE International Symposium on High-Performance Computer Architecture. IEEE Computer Society, \n2003. [20] Jeremy Lau, Stefan Schoenmackers, and Brad Calder. Structures for phase classi.cation. In \nProceedings of the IEEE International Symposium on Performance Analysis of Systems and Software. IEEE \nComputer Society, 2004. [21] Yue Luo, Juan Rubio, Lizy Kurian John, Pattabi Seshadri, and Alex Mericas. \nBenchmarking internet servers on superscalar machines. Computer, 36(2):34 40, 2003. [22] Matthew C. Merten, \nAndrew R. Trick, Ronald D. Barnes, Erik M. Nystrom, Christopher N. George, John C. Gyllenhaal, and Wen \nmei W. Hwu. An architectural framework for run-time optimization. IEEE Transactions on Computers, 50(6):567 \n589, 2001. [23] R Development Core Team. R: A language and environment for statistical computing. R Foundation \nfor Statistical Computing, Vienna, Austria, 2003. ISBN 3-900051-00-3. [24] Ramesh Radhakrishnan, N. Vijaykrishnan, \nLizy Kurian John, Anand Sivasubramaniam, Juan Rubio, and Jyotsna Sabarinathan. Java runtime systems: \nCharacterization and architectural implications. IEEE Transactions on Computers, 50(2):131 146, 2001. \n[25] Timothy Sherwood, Erez Perelman, Greg Hamerly, and Brad Calder. Automatically characterizing large \nscale program behavior. In Proceedings of the 10th international conference on architectural support \nfor programming languages and operating systems (ASPLOS-X), pages 45 57. ACM, 2002. [26] Timothy Sherwood, \nSuleyman Sair, and Brad Calder. Phase tracking and prediction. In Proceedings of the 30th annual international \nsymposium on Computer architecture, pages 336 349. ACM, 2003. [27] Ye.m Shuf, Mauricio J. Serrano, Manish \nGupta, and Jaswinder Pal Singh. Characterizing the memory behavior of Java workloads: a structured view \nand opportunities for optimizations. In Proceedings of the 2001 ACM SIGMETRICS international conference \non Measurement and modeling of computer systems, pages 194 205. ACM, 2001. [28] Peter F. Sweeney, Matthias \nHauswirth, Brendon Cahoon, Perry Cheng, Amer Diwan, David Grove, and Michael Hind. Using hardware performance \nmonitors to understand the behavior of Java applications. In Proceedings of the Third Virtual Machine \nResearch and Technology Symposium, pages 57 72. USENIX, 2004.  \n\t\t\t", "proc_id": "1028976", "abstract": "<p>Java workloads are becoming more and more prominent on various computing devices. Understanding the behavior of a Java workload which includes the interaction between the application and the virtual machine (VM), is thus of primary importance during performance analysis and optimization. Moreover, as contemporary software projects are increasing in complexity, automatic performance analysis techniques are indispensable. This paper proposes an off-line method-level phase analysis approach for Java workloads that consists of three steps. In the first step, the execution time is computed for each method invocation. Using an off-line tool, we subsequently analyze the dynamic call graph (that is annotated with the method invocations' execution times) to identify method-level phases. Finally, we measure performance characteristics for each of the selected phases. This is done using hardware performance monitors. As such, our approach allows for linking microprocessor-level information at the individual methods in the Java application's source code. This is extremely interesting information during performance analysis and optimization as programmers can use this information to optimize their code. We evaluate our approach in the Jikes RVM on an IA-32 platform using the SPECjvm98 and SPECjbb2000 benchmarks. This is done according to a number of important criteria: the overhead during profiling, the variability within and between the phases, its applicability in Java workload characterization (measuring performance characteristics of the various VM components) and application bottleneck identification.</p>", "authors": [{"name": "Andy Georges", "author_profile_id": "81100487568", "affiliation": "Ghent University, Gent, Belgium", "person_id": "P643379", "email_address": "", "orcid_id": ""}, {"name": "Dries Buytaert", "author_profile_id": "81100468396", "affiliation": "Ghent University, Gent, Belgium", "person_id": "P698113", "email_address": "", "orcid_id": ""}, {"name": "Lieven Eeckhout", "author_profile_id": "81330490198", "affiliation": "Ghent University, Gent, Belgium", "person_id": "PP45025842", "email_address": "", "orcid_id": ""}, {"name": "Koen De Bosschere", "author_profile_id": "81100123309", "affiliation": "Ghent University, Gent, Belgium", "person_id": "PP14054039", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1028976.1028999", "year": "2004", "article_id": "1028999", "conference": "OOPSLA", "title": "Method-level phase behavior in java workloads", "url": "http://dl.acm.org/citation.cfm?id=1028999"}