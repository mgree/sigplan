{"article_publication_date": "10-01-2004", "fulltext": "\n Vertical Pro.ling: Understanding the Behavior of Object-Oriented Applications Matthias Hauswirth Peter \nF. Sweeney University of Colorado at Boulder IBM Thomas J. Watson Research Center Matthias.Hauswirth@colorado.edu \npfs@us.ibm.com Amer Diwan Michael Hind University of Colorado at Boulder IBM Thomas J. Watson Research \nCenter diwan@colorado.edu hindm@us.ibm.com  ABSTRACT Object-oriented programming languages provide a \nrich set of features that provide signi.cant software engineering ben\u00ade.ts. The increased productivity \nprovided by these features comes at a justi.able cost in a more sophisticated runtime system whose responsibility \nis to implement these features e.ciently. However, the virtualization introduced by this so\u00adphistication \nprovides a signi.cant challenge to understand\u00ading complete system performance, not found in traditionally \ncompiled languages, such as C or C++. Thus, understand\u00ading system performance of such a system requires \npro.ling that spans all levels of the execution stack, such as the hard\u00adware, operating system, virtual \nmachine, and application. In this work, we suggest an approach, called vertical pro\u00ad.ling, that enables \nthis level of understanding. We illustrate the e.cacy of this approach by providing deep understand\u00adings \nof performance problems of Java applications run on a VM with vertical pro.ling support. By incorporating \nverti\u00adcal pro.ling into a programming environment, the program\u00admer will be able to understand how their \nprogram interacts with the underlying abstraction levels, such as application server, VM, operating system, \nand hardware. Categories and Subject Descriptors B.8.2 [Performance and Reliability]: Performance Anal\u00adysis \nand Design Aids; C.4 [Computer Systems Organi\u00adzation]: Performance of Systems measurement techniques, \nperformance attributes General Terms measurement, performance, experimentation Permission to make digital \nor hard copies of all or part of this work for personal or classroom use is granted without fee provided \nthat copies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. OOPSLA 04, Oct. 24-28, 2004, Vancouver, British \nColumbia, Canada. Copyright 2004 ACM 1-58113-831-8/04/0010 ...$5.00.  Keywords vertical pro.ling, whole-system \nanalysis, perturbation, hard\u00adware performance monitors, software performance monitors 1. INTRODUCTION \nCompared to imperative languages, such as C/C++, mod\u00adern object-oriented programming languages, such \nas Java, o.er the bene.t of increased runtime .exibility (e.g., re.ec\u00adtion, automatic memory management), \nimproved security properties (null pointer and array bounds checks, security policies), and a portable \ndeployment representation. How\u00adever, these features, and several others, require a sophisti\u00adcated runtime \nsystem that introduces an additional layer of virtualization, such as a JVM, between the application \nand the operating system. Higher level programming models, such as J2EE, require a further level of virtualization \nin the form of an application server. Thus, in today s commercial system the application can be separated \nfrom the native hardware by several layers of virtualization: an operating system, a virtual machine, \nand an application server. Although this virtualization provides several software en\u00adgineering advantages, \nthe support of this virtualization in\u00adtroduces obstacles to understanding application performance. For \nexample, popular implementation techniques, such as dynamic recompilation and garbage collection, in.uence \nap\u00adplication behavior in a way that makes correlation of hard\u00adware performance to source code challenging. \nWith dynamic recompilation, the same source code statement can be trans\u00adlated to di.erent machine instructions \nat di.erent memory locations throughout the execution of the program. Like\u00adwise, garbage collection can \nboth relocate objects and reuse addresses, resulting in a dynamic mapping between objects and addresses. \nThe above issues indicate that there is a need to gather more complete pro.les, containing information \nabout system behavior on various levels (see Figure 1). We call this ap\u00adproach vertical pro.ling. The \nmain goal of vertical pro.ling is to further the understanding of system behavior through correlation \nof pro.le information from di.erent levels. This paper introduces the concept of vertical pro.ling and \ndemonstrates its use for understanding performance phe\u00adnomena. We demonstrate that it is necessary to \nuse vertical pro.ling to understand existing performance problems, and further demonstrate that vertical \npro.ling can indeed ex\u00ad Figure 1: Layers of Virtualization plain these problems. Through insight gained \nfrom .ve cases studies, we demonstrate that achieving the goal of vertical pro.ling requires more than \ncapturing the mapping between virtualized and real entities. The main contributions of this paper are \n a novel approach, vertical pro.ling, for capturing and correlating performance problems across multiple \nexe\u00adcution layers;  .ve case studies that demonstrate how vertical pro.l\u00ading can be used to understand \nperformance phenom\u00adena; and  insight into issues that can impact the success of this performance understanding. \n By incorporating vertical pro.ling into a programming en\u00advironment the programmer will be able to understand \nhow their program interacts with the underlying abstraction lev\u00adels, such as application server, VM, \noperating system, and hardware. The structure of this paper is as follows. Section 2 de\u00adscribes our infrastructure. \nSection 3 describes the method\u00adology we use for our case studies and evaluates the over\u00adhead caused by \nour vertical pro.ler. Section 4 presents .ve detailed case studies that demonstrate how the infrastruc\u00adture \ncan be used to better understand program performance. Section 5 presents lessons we learned while using \nvertical pro.ling in our case studies. Section 6 covers work related to vertical pro.ling, and Section \n7 concludes.  2. INFRASTRUCTURE This section describes our infrastructure. Section 2.1 in\u00adtroduces the \nfundamental concepts of our vertical pro.ling approach. Section 2.2 describes the software performance \nmonitors we introduced to capture the behavior of the soft\u00adware layers. Section 2.3 presents the implementation \nof the sampling infrastructure that gathers traces of performance monitor values. Section 2.4 describes \nour approach for ana\u00adlyzing those traces. 2.1 Events, States, Monitors, and Counters The fundamental \ncomponents of our vertical pro.ler are performance monitors. They observe (monitor) events and states \nof the system on various levels. 2.1.1 Events and States A computer system can be viewed as a large \nstate ma\u00adchine, where a state captures the values in all of memory (including registers, caches, main \nmemory, and persistent storage). An event is an atomic occurrence in time that does not have any duration. \nAn event usually causes the system to change to a di.erent state. For example, an ob\u00adject allocation \nevent means that the system now has less free memory, but an additional new object. Or an instruction \ncompleted event means that the processor pipeline now has one less instruction in it. Some events can \nhave attributes. For example, an object allocation event can have attributes specifying the object size, \naddress, type, etc. Conversely, a cycle event (a clock tick in the CPU) has no attributes. 2.1.2 Monitors \nand Counters A performance monitor observes the behavior of a system. We use the hardware performance \nmonitors available in the processor, and we add software performance monitors to the native libraries, \nthe virtual machine, and the Java libraries. A performance monitor is a scalar variable with a value \nthat changes over time. A performance counter is a special kind of performance monitor. The value of \na performance counter re.ects a count, usually a count of events.  2.2 Software Performance Monitors \nIn previous work [33] we used hardware performance mon\u00aditors to analyze the behavior of applications. \nWe found that hardware performance monitors are not enough for a complete understanding of certain performance \nphenomena. We concluded that we also needed information from higher layers (Figure 1) of the system. \nThis paper thus intro\u00adduces vertical pro.ling, adding software performance mon\u00aditors (SPMs) to observe \nthe behaviour in the layers above the hardware. Our software performance monitors cover the following \nparts of the system: Application. We provide a mechanism for pro.ling appli\u00adcation behavior by adding \nsoftware performance mon\u00aditors to applications. Virtual Machine. We monitor several subsystems of the \nvirtual machine: Memory Manager. Our infrastructure implements monitors to pro.le allocations of arrays \nand ob\u00adjects, and growing and shrinking of the virtual machine s heap. Runtime Compilers. We provide \nsoftware performance monitors for dynamic compilations that count the number of methods compiled, and \nthe number of byte code and machine code instructions gener\u00adated by the di.erent compilers, and captures \nthe compiled method ID and optimization level of the last method that is recompiled. Synchronization. \nWe pro.le synchronization opera\u00adtions such as locks and unlocks, notify and waits, the number of thread \nyields because of the need to wait for a lock, and the number of attempted and succeeded test-and-set \noperations. Figure 2: Scheduling in Jikes RVM Operating System. We observe the interactions between \nvirtual machine and operating system. Those interac\u00ad tions go both ways, system calls into the OS, and \nsig\u00ad nals sent to the virtual machine. We provide monitors that observe virtual memory management requests, \nand signals caused by segmentation violations and arith\u00ad metic errors.  2.3 Implementation Our vertical \npro.ling infrastructure is based on the hard\u00adware performance monitor capability that exists in Jikes \nRVM [33]. This section summarizes the existing infrastruc\u00adture and describes our extensions. Jikes RVM \n[19] is an open source research virtual machine that executes Java bytecodes. The system is implemented \nin the Java programming language [2] and uses Java threads to implement several subsystems, such as the \ngarbage collec\u00adtor [6] and the adaptive optimization system [4]. Figure 2 il\u00adlustrates how Jikes RVM \ns thread scheduler maps its M Java threads (application and VM) onto N Pthreads (user level POSIX threads). \nThere is a 1-to-1 mapping from Pthreads to OS kernel threads. The operating system schedules the kernel \nthreads on available processors. Typically, Jikes RVM creates a small number of Pthreads (on the order \nof one per physical processor). Each Pthread is called a virtual pro\u00adcessor because it represents an \nexecution resource that the virtual machine can use to execute Java threads. To implement M-to-N threading, \nJikes RVM uses com\u00adpiler-supported quasi-preemptive scheduling by having the two compilers (baseline \nand optimizing) insert yieldpoints into method prologues, epilogues, and loop heads. The yieldpoint code \nsequence checks a .ag on the virtual pro\u00adcessor object; if the .ag is set, then the yieldpoint invokes \nJikes RVM s thread scheduler. The .ag can be set by a timer interrupt handler (signifying that the 10ms \nschedul\u00ading quantum has expired) or by some other system service (for example, the need to initiate a \ngarbage collection) that needs to preempt the Java thread to schedule one of its own daemon threads. \nThis work builds on existing infrastructure [33] to capture hardware performance monitors, such as processor \ncycles, instructions completed, and L1 cache misses. The existing infrastructure generates a trace .le \nfor each Jikes RVM vir\u00adtual processor, and one meta .le. A trace .le contains a series of trace records, \nwhich capture performance monitor information for a measurement period in which exactly one Java thread \nwas executing on that virtual processor. A trace record contains the following data: Virtual Processor \nID. This .eld contains the unique ID of the virtual processor that executed during the mea\u00adsurement period. \nThread ID. This .eld contains the unique ID of the Java thread that executed during the measurement period. \nThread Yield Status. This boolean .eld captures if a thread yielded before its scheduling quantum expired. \nReal Time. This .eld contains the value of the PowerPC time base register at the start of this measurement \nperiod. The time base register contains a 64-bit un\u00adsigned quantity that is incremented periodically \n(at an implementation-de.ned interval) [23]. Real Time Duration. This .eld contains the duration of the \nmeasurement period using the time base register. Thread Switch Status. The existing infrastructure col\u00adlected \na trace record only at thread switch time. Since our extensions can also generate a record for other \nrea\u00adsons, we add a boolean .eld that describes what trig\u00adgered the trace record creation. Compiled Method \nIDs. The existing infrastructure cap\u00adtured the top two method IDs on the call stack. We ex\u00adtended this \nto capture the top n = 0 compiled method IDs on the call stack. A compiled method s ID iden\u00adti.es a compiled \nmethod, which is a piece of machine code that has been created by compiling a method with a given compiler, \nat a given optimization level. The value of n is a runtime parameter to the VM. Monitor Values. In the \nexisting infrastructure this array contained the values of the selected hardware perfor\u00admance monitors, \nwhere the selection of monitors was a runtime parameter to the VM. In this work we gener\u00adalize this to \napply to software as well as hardware per\u00adformance monitors. For a counter the captured value corresponds \nto the number of events that occurred dur\u00ading the measurement period. Our new software performance monitors \nare implemented on two levels: in native code and in Java code. In native code, each Pthread has its \nprivate array of software per\u00adformance monitors. A pointer to this array is stored as Pthread-speci.c \ndata. This way, instrumentations in na\u00adtive code can update the monitor of the currently running thread. \nOn the Java level, we keep a reference to a Java array of software performance monitors in the VirtualPro\u00adcessor \nobject. Jikes RVM provides cheap access to the Vir\u00adtualProcessor object of the virtual processor (Pthread) \non which the current Java thread is scheduled. The existing infrastructure creates one trace .le for \neach virtual processor. The real time values in the trace records are used to merge together multiple \ntrace .les to accurately model concurrent events on multiple processors. The values are also used to \ndetect when the measurement period has been shared with other non-VM Pthreads. A meta .le is generated \nin conjunction with a benchmark s trace .les. The meta .le speci.es the number of hardware and software \nperformance monitor values and the number of compiled method IDs captured from the call stack in each \ntrace record, and provides the following mappings: moni\u00adtor .eld number to event name, thread ID to thread \nname, method ID to method signature, type ID to type name, and compiled method ID to method ID and optimization \nlevel. 2.4 Performance Analysis Given a set of traces that contain trace records that pro\u00advide the performance \nmonitor values for each time slice, we want to investigate performance phenomena and problems. This subsection \nbrie.y describes the concepts of our perfor\u00admance visualization and analysis tool, the Performance Ex\u00adplorer, \nwhich is described in more detail in previous work [33]. We also describe the correlation facilities \nwe added to Per\u00adformance Explorer for this work. 2.4.1 Metrics Metrics are a concept we use for performance \nanalysis. They compute values given the values of performance moni\u00adtors. A metric computes a value given \na sample. Each sam\u00adple contains the values of all recorded hardware and software monitors. Each of those \nmonitors has an associated metric (e.g., the Cyc metric produces the value of the Cyc moni\u00adtor of the \ngiven sample). Besides those monitor metrics we also support computed metrics, which represent arbitrary \narithmetic expressions involving other metrics and constant values. This allows the computation of the \nInstCmpl/Cyc metric, given the InstCmpl and Cyc monitors. 2.4.2 Sample Lists We provide a .exible and \ninteractive way to declaratively construct sample lists, i.e., a list of samples, using .lters on sample \nattributes and metrics, and using set operations on other sample lists. We also allow the direct manual \nselection of speci.c samples to include or exclude from sample lists. 2.4.3 Statistics We provide descriptive \nstatistics, such as min, max, aver\u00adage, and median, and multivariate statistics, such as cross correlation \ncoe.cient and correlation matrix. 2.4.4 Visualizations For manual investigations we present straightforward \nplots of a metric over time. In addition, we also provide scatter plots for plotting two metrics against \neach other. Further\u00admore we provide a matrix of scatter plots, for the pair-wise comparison of any number \nof metrics. This matrix is actu\u00adally a correlation matrix, showing a scatter plot in addition to the \ncorrelation coe.cient in each cell. Examples of such visualizations are given in subsequent sections. \n  3. METHODOLOGY This section describes the platform and benchmarks used for our case studies, and it \nquanti.es the overhead intro\u00adduced by our vertical pro.ler. The goal of vertical pro.ling is to .nd cause-e.ect \nrelations between performance phenom\u00adena. Since this approach depends on the instrumentation of code \non various levels, we need to verify that the phenomena we observe using vertical pro.ling are not caused \nby our in\u00adstrumentation. This section describes the two components to this veri.cation: pertubation analysis \nand validation. Benchmark Production Vertical Pro.ling compress 9.77 10.15 3.6% db 22.42 23.85 6.4% jack \n13.38 14.41 7.8% javac 19.14 20.57 7.5% jess 8.23 8.64 5.0% mpegaudio 8.52 9.69 13.8% mtrt 7.64 7.99 \n4.6% jbb 27.17 31.84 17.2% hsql 19.19 19.39 1.1% Average 7.4% Table 2: Vertical Pro.ling Overhead 3.1 \nPlatform We run all experiments on a 4-processor IBM POWER4 [18] machine running the AIX 5.1 operating \nsystem. Our virtual machine is an extended version of the Jikes RVM CVS head as of January 19, 2004. \n 3.2 Benchmarks Table 1 presents our benchmark suite, which consists of the SPECjvm98 suite [10], a modi.ed \nversion of SPEC\u00adjbb2000 [9], which we refer to as jbb, and the HSQL server [28]. The upper part of the \ntable contains the single-threaded benchmarks, whereas the last three rows contain the multi\u00adthreaded \napplications. For the SPECjvm98 benchmarks we use the reference in\u00adputs (size 100). We use a modi.ed \nversion of SPECjbb2000 that is able to process a .xed number of transactions (in\u00adstead of running for \na predetermined amount of time). We run the HSQL database server (version 1.7.1) in in-memory mode, using \na modi.ed version of the JDBCBench included with the HSQL distribution. 3.3 Overhead This section demonstrates \nthat our implementation of ver\u00adtical pro.ling is fast enough to be useful. Table 2 summa\u00adrizes vertical \npro.ling overhead. For each benchmark, the best of ten runs was chosen. All times are in seconds. The \nProduction column gives the total time needed to execute the benchmark without any vertical pro.ling. \nThe two Ver\u00adtical Pro.ling columns show the total time needed to execute the benchmark with vertical \npro.ling (tracing both HPMs an SPMs), and the overhead as a percentage of the produc\u00adtion run. The overhead \nfor vertical pro.ling ranges from as little as 3.6% for compress to as much as 17.2% for jbb. For this \nmeasurement, all 148 software performance moni\u00adtors were enabled. Often a user might be interested in \nonly a small subset of the monitors, and would thus be able to reduce overhead even more. 3.4 Perturbation \nAnalysis Because vertical pro.ling adds instrumentation to the sys\u00adtem, it can perturb the very behavior \nthat it is trying to un\u00adderstand. For example, instrumentation for collecting data on a software performance \nmonitor may change the cache behavior of the application. Our perturbation analysis as\u00adsures that the \ndata collection is not perturbing the behavior of interest. The following structure provides the framework \nwe will use for the perturbation analysis in each of our case studies: Benchmark Functionality Threads \nReference Input compress File compressor/decompressor 1 main 5 iterations compressing and decompressing \n.les 213x.tar (3.1MB), 209.tar (2.8MB), 239.tar (0.9MB), 211.tar (1.2MB), 202.tar (1.1MB) db Simple in-memory \naddress database 1 main perform operations in scr6 (221 adds, 292 deletes, 8 modi.es, 15 .nds, 67 sorts) \non address database in db6 (1.1MB, 15,332 records) jack Parser generator 1 main 17 iterations of generating \nthe parser for grammar Jack.jack (17kB, 17 productions) javac Java compiler 1 main 4 iterations of compiling \nJavaLex.java (1.8MB, contain\u00ading 144 classes) with -O jess Inference system 1 main solve wordgames.clp \n(12kB, containing 2 puzzles: GERALD+DONALD=ROBERT and 5 houses, 5 attributes, 25 constraints ) mpegaudio \nMPEG Layer 3 audio decoder 1 main decode track2.mp3 (3.2MB) mtrt Multithreaded raytracer 1 main 2 renderers \nrender scene time-test.model (0.3MB, 2 lights, 5 spheres, 1,407 polygons with totally 4,233 vertices) \ninto a 200x200 pixel image (each renderer renders a 100x200 section) jbb In-memory 3-tier application \nserver 1 main 2 warehouses execute 120,000 transactions per warehouse hsql In-memory SQL database server \n1 main 2 clients execute 4*10,000 JDBC queries per client Table 1: Benchmarks HPM perturbation analysis. \n End-to-end perturbation analysis of HPMs. Because HPMs are implemented in the hardware they do not perturb \nthe behavior of the application. How\u00adever, our mechanism for recording HPMs at each thread switch may \nperturb behavior. To see if this is the case, we conduct an additional run that does not collect any \ndata at each thread switch, but instead records the end-to-end di.erence be\u00adtween the HPM values at the \nend and beginning of the run. We compare the end-to-end di.er\u00adences with the aggregate HPM values that \nare recorded at each thread switch. If these two sets of values are close, we have some con.dence that \nour mechanism for recording HPMs does not per\u00adturb behavior at the macro level. To minimize inter-run \nvariations due to things outside of our control (e.g., context switches at the operating system level) \nwe use .ve runs for each con.gura\u00adtion and use the average of the runs. To enable us to distinguish perturbation \nfrom measurement noise, we also compute the standard deviations for the .ve runs. Temporal impact of \nHPMs. While the end-to-end perturbation analysis gives us con.dence that we have not perturbed the application \nbehavior at the macro level, it does not tell us if we have per\u00adturbed behavior at the micro-level. For \nexample, the original application may experience a .xed cache miss rate throughout its run whereas the \nrun that records HPMs at Java thread switches may experience varying miss rates during its run. If both \nruns end up with the same number of total misses, our end-to-end perturbation analysis will not recognize \nthat we have perturbed the appli\u00adcation. We use qualitative analysis based on our knowledge of our HPM \nimplementation to argue that vertical pro.ling is not perturbing the micro\u00adlevel behavior of the application. \n SPM perturbation analysis. Impact of SPMs on HPMs. The instrumentation re\u00adquired to capture SPMs may \nalso perturb sys\u00adtem behavior. To verify that the mechanism for recording SPMs is not perturbing HPMs, \nwe vi\u00adsually compare the HPM signals collected with and without SPM tracing enabled. If the signals visually \ncorrelate then we have some con.dence that SPMs are not changing HPMs in a signi.\u00adcant way. Impact of \nSPMs on SPMs. We argue from knowl\u00adedge of our SPM implementation that one SPM does not signi.cantly perturb \nanother SPM of in\u00adterest to our case study.  3.5 Validation A vertical pro.ling session results in a \nhypothesis about the cause of the observed performance phenomenon. In our studies we validate these hypotheses \nby eliminating the cause and running an additional experiment to verify that the phenomenon is gone. \nEach case study in Section 4 has a Validation subsection presenting the veri.cation of our hy\u00adpothesis. \n 4. CASE STUDIES So far, the paper has claimed that vertical pro.ling is nec\u00adessary for understanding \nthe performance of Java programs. In this section we provide evidence for this claim by describ\u00ading .ve \ncase studies. Each case study shows how we used our infrastructure to explain a performance anomaly in \na benchmark program. We selected anomalies by gathering the IPC (Instructions completed Per Cycle) of \nour bench\u00admarks over time (see Figure 3), and choosing the most sig\u00adni.cant patterns.1 We found that \nin all the case studies we needed pro.le information from more than one system layer (Figure 1) to explain \nthe anomaly. Figure 3 shows the IPC over time for our benchmark pro\u00adgrams. In this .gure the multithreaded \nbenchmarks use only one worker thread. The number to the right of each graph gives the IPC for all worker \nand main thread time slices. We see that not only do di.erent benchmarks have vastly di.erent IPC (factor \nof 2.3 between lowest and highest), but also the IPC changes dramatically over each benchmark run (up \nto factor of 7.8 between lowest and highest time slice in a benchmark). We have marked the four most \ndistinct patterns in Fig\u00adure 3 as sudden increase, gradual increase, dip before GC, and periodic pattern. \nFour of our case studies use vertical pro.ling to explain these patterns. Our .fth case study uses vertical \npro.ling to investigate the scalability of the three multithreaded benchmarks. 4.1 Gradual Increase \nin Jbb From Figure 3 we see that the IPC of many benchmarks increases over time (gradual increase pattern). \nThis section uses vertical pro.ling to explain the cause of this pattern in jbb. We picked jbb for this \ncase study since the gradual increase pattern is particularly visible in jbb and moreover the pattern \nrecurs several times during the run. For this case study we focus on the marked instance in Figure 3 \nwhich happens when the worker thread starts running (after the main thread is done setting up the application). \n4.1.1 Background The worker thread of jbb performs about 50 transactions per 10 ms time slice. Our vertical \npro.ler reports perfor\u00admance monitor values at the end of each time slice. The changes in application \nbehavior during a transaction are not captured in the signal, because our sampling rate is too low to \nobserve individual transactions. Thus, we expect that the gradual increase in IPC is not due to the transaction \nbehavior, but due to some other lower-level behavior. In prior work [33] we speculated that jbb s gradual \nincrease in IPC occurs because as jbb runs, more and more of its code gets optimized. When a method runs \nfor the .rst time, Jikes RVM uses its baseline (non-optimizing) compiler to compile the code; when a \nmethod becomes hot, Jikes RVM uses its optimizing compiler to recompile the code. We came to the above \nconclusion in prior work because of two observations: We found that optimized code had a higher IPC \n(by more than 32%) than unoptimized code. Thus one could expect that the IPC of the application would \nincrease with an increase in the amount of executed optimized code.  We found that the number of .ushes \nin the load/store unit (LSU) went down when the IPC increased. LSU  1Although IPC does not necessarily \nre.ect the amount of useful work completed (e.g. noop instructions completed are also counted) it is \na popular metric used to measure CPU utilization of an application running on a particular software stack. \n.ushes happen when the POWER4 s LSU specula\u00adtively reorders a load and a store that access the same address. \nThe optimizing compiler uses a register allo\u00adcator to reduce the number of loads and stores while the \nnon-optimizing compiler simulates the Java expres\u00adsion stack, and issues frequent loads and stores to \nthe top of the stack. If the POWER4 misspeculates the frequent loads and stores issued by unoptimized \ncode, it incurs LSU .ushes. We found that LSU .ushes hap\u00adpen at 15.2% of the instructions completed for \nunop\u00adtimized code and 0.1% of the instructions completed for optimized code.  4.1.2 Findings We were \nable to demonstrate that the gradual increase in IPC for jbb is indeed caused by the Jikes RVM optimizing \nmore methods as the run progresses. 4.1.3 Exploration Our prior work, which used only hardware performance \nmonitors, hypothesized that the IPC for jbb increased grad\u00adually over time because as the program ran, \nmore and more of the code got optimized. However, using only the hardware performance monitors we could \nnot con.rm this hypothesis. To test this hypothesis, we needed to measure the amount of time spent in \noptimized code and unoptimized code for each time slice. Measuring this information directly would add \noverhead (and perturbation) to every call and return and thus we settled on an indirect way of capturing \nthe in\u00adformation. We identi.ed JVM lock acquisition performance monitors that would approximate this \ninformation. The Java compiler issues MonitorEnter bytecode instructions at each call to a synchronized \nmethod and at each entry into a synchronized block. Jikes RVM s baseline compiler ex\u00adpands MonitorEnter \ninto a call to a lock acquisition method and the optimizing compiler expands MonitorEnter into an inlined \nbody of a di.erent lock acquisition method. We added two JVM performance monitors: UnoptMonitorEnter \nand OptMonitorEnter. UnoptMonitorEnter is incremented in the lock acquisition method used by baseline \ncompiled code, while OptMonitorEnter is incremented by the inlined lock acquisition method used in optimized \ncode. These JVM performance monitors told us how many MonitorEnters were executed in unoptimized and \nin optimized code. Since jbb executes many synchronized methods throughout its ex\u00adecution, these counts \nprovide us with useful information. For benchmarks that do not execute synchronized methods throughout \ntheir execution, we would need di.erent moni\u00adtors to determine the time spent in optimized and unopti\u00admized \ncode. Figure 4 shows the IPC, LsuFlush/Cyc, and the percentage of MonitorEnter byte code instructions \nexecuted that were unoptimized2, over time. We can see that in the beginning the IPC is low, while the \ntwo other metrics are high. Over time the IPC increases while the other metrics decrease. The LsuFlush/Cyc \nand the percentage of UnoptMonitorEnters decrease at almost the same rate. We use the correlation feature \nof the Performance Ex\u00adplorer to determine the cross correlation coe.cient between these signals. Figure \n5 shows two scatter plots. The left scatter plot correlates Cyc/InstCmpl3 and LsuFlush/Cyc. The 2UnoptMonitorEnter/(UnoptMonitorEnter+OptMonitorEnter). \n3We use the CPI, inverse of IPC, because the correlation Figure 3: InstCmpl/Cyc over Time for All Benchmarks \n Figure 4: Behavior of Jbb s Worker Thread over Time Figure 5: Correlation of Metrics for Jbb s Worker \nThread plot shows that overall higher .ush rates come with higher CPI rates. Also, the cross correlation \ncoe.cient of 0.726 indicates a signi.cant correlation, which con.rms the vi\u00adsual correlation. The right \nscatter plot correlates the Lsu\u00adFlush/Cyc with the percentage of unoptimized MonitorEnters. Since the \ndata points in this plot almost form a straight line4it must be the case that the execution of unoptimized \ncode is strongly correlated to LsuFlush/Cyc (correlation co\u00ade.cient 0.963), which in turn is correlated \nto Cyc/InstCmpl. Thus, our investigation con.rms our hypothesis: the IPC of jbb increase gradually over \ntime as more and more of the code it executes is optimized.  4.1.4 Perturbation Analysis This case study \nexamines the Cyc, InstCmpl, and LsuFlush HPMs. The end-to-end impact of HPMs is within measure\u00adment noise \nwith the three HPMs decreasing with aggregate HPMs enabled: -0.92% for Cyc, -0.26% for InstCmpl/Cyc, \nand -2.57% for LsuFlush. Their standard deviations for .ve runs without vertical pro.ling enabled are \n0.73% for Cyc, 0.63% for InstCmpl, and 1.48% for LsuFlush. It is highly unlikely that the huge gradual \ndrop in LsuFlush over the coe.cient indicates a linear relationship between two met\u00adrics, and we expect \na linear relationship between the overall cost of executing an instruction, CPI (Cyc/InstCmpl), and the \namount of load store unit .ushing, LsuFlush/Cyc. 4The curved segment in the right half of the graph consists \nof a small fraction of the points. The correlation coe.cient is dominated by the vast majority of the \npoints forming a straight line in the lower left corner. course of the whole run is caused by HPMs as \nthe HPM perturbation occurs consistently throughout the run at each Java thread switch. We were able \nto visually correlate the gradual increase in IPC and drop in LsuFlush/Cyc with and without SPM updating. \nThe UnoptMonitorEnter and Opt-MonitorEnter SPMs can not perturb each other. We conclude that the perturbation \nby our instrumenta\u00adtions is not signi.cation enough to perturb our .ndings. 4.1.5 Validation We validated \nour explanation for the gradual increase pat\u00adtern in jbb by disabling the adaptive optimization system \n(aos), i.e. we used either the baseline or optimizing com\u00adpiler exclusively without any recompilation. \nIf our expla\u00adnation is correct then (i) at the beginning of the run the aos system has no methods optimized \nand thus its metrics (such as IPC) will be similar to the run with the baseline compiler; and (ii) at \nthe end of the run the aos system has optimized all the hot methods and thus its metrics will be similar \nto the run with the optimizing compiler. We found both properties to be true, thus validating our explanation. \nAs further evidence we found that the runs with the base\u00adline and optimizing compilers did not exhibit \nthe gradual increase pattern, indicating that the aos is the cause of the pattern. 4.2 Sudden Increase \nin Compress At the start compress s execution, we .nd a variation of the gradual increase pattern presented \nin Section 4.1. In this case, the IPC suddenly jumps from 0.3 to 1.0.  4.2.1 Background The execution \nof compress is dominated by the alternat\u00ading invocation of two long running methods: compress and decompress. \n 4.2.2 Findings The two long-running methods in compress account for most of its execution time. Since \nthese methods dominate the execution time of compress, optimizing them results in a jump in IPC (which \nas we discussed earlier is higher with optimized code than with unoptimized code). 4.2.3 Exploration \nIn our previous case study (Section 4.1) we found that when the amount of executed optimized code increases \nthe IPC also increases. However, unlike the previous case study, the IPC increase in compress is not \ngradual. We suspected that the rapid increase in IPC may happen when a key method gets optimized. To \ninvestigate this, we used two software performance monitors at the JVM level. The .rst monitor, TopOfStack-MethodId, \ncaptured and recorded the activation records at the top of the call stack. The second monitor, Optimized-MethodId, \nrecorded the identity of the most recently opti\u00admized method at the end of each sample. The .rst monitor \ntold us that most of the time during the run of compress either the compress or the decompress method \nis executing. When we superimposed the trace from this monitor with the IPC trace we found that when \nthe IPC jumps up for the .rst time, the method compress is executing and when the IPC jumps for the second \ntime, the method decompress is executing. Looking at the second monitor we .nd that the .rst jump in \nIPC corresponds to the optimization of compress and the second to the optimization of decompress. These \ntwo pieces of information tell us that it is likely that the two sudden increases in IPC are caused by \nthe optimization of the two methods that account for most of the execution of the benchmark. 4.2.4 Perturbation \nAnalysis Similar to the previous case study, this case study ex\u00adamines the Cyc, InstCmpl, and LsuFlush \nHPMs. For com\u00adpress, the end-to-end perturbation of HPMs is less than the standard deviation and, therefore, \nwithin measurement noise. In particular, end-to-end perturbations are 0.68% for Cyc, 0.5% for InstCmpl/Cyc, \nand 0.4% for LsuFlush, which is less than the corresponding standard deviations of 1.47% for Cyc, 0.22% \nfor InstCmpl, and 6.74% for LsuFlush. It is highly unlikely that the temporal impact of HPMs cause the \nisolated jump in InstCmpl/Cyc. We were able to visually cor\u00adrelate the sudden increase in IPC and drop \nin LSU .ushes with and without SPM updating. The SPMs TopOfStack-MethodId, and OptimizedMethodId have \nno impact on one another as they count di.erent events. We conclude that vertical pro.ling does not invalidate \nour hypothesis.  4.2.5 Validation To validate our explanation we reran compress with OSR (on-stack replacement \n[17, 16]) disabled. OSR replaces an unoptimized version of a method with an optimized version while the \nmethod is running. When OSR is disabled we ex\u00adpect the .rst execution of both methods to complete before \nthe IPC jumps up. Figure 6: InstCmpl/Cyc and LsuFlush/Cyc for Com\u00adpress with and without On-Stack Replacement \nFigure 6 shows IPC and LsuFlush/Cyc over time for the two runs. The top two graphs are with OSR enabled \nand the bottom two graphs are with OSR disabled. The segment la\u00adbeled 1 is the .rst execution of the \ncompress method and the segment labeled 2 is the .rst execution of the decompress method. We can see \nthat the .rst invocations of compress and decompress take about 2.5 times longer without OSR than with \nOSR. This is because the baseline compiled meth\u00adods cannot be replaced by optimized code until after \ntheir .rst invocation .nishes. This data con.rms our expectation and thus validates our explanation. \n 4.3 Scalability of Multithreaded Benchmarks Three of our benchmarks, mtrt, jbb, and hsql, are multi\u00adthreaded \napplications. Each of them allows us to arbitrarily set the number of worker threads. When we .rst analyzed \nthe traces of runs with more than one worker thread, we were surprised by the large number of time slices \nin those traces. We observed almost an order of magnitude more time slices (10,409 instead of 2,221 for \njbb), and a much shorter average time slice duration, for a multithreaded run than for a single threaded \nrun of the same benchmark with the same amount of work. This case study uses our vertical pro.ler to \n.nd the reason for this increase in the number of time slices. 4.3.1 Background Each of the three benchmarks \nprovides worker threads that can run in parallel. If we hand a .xed amount of work to a benchmark with \ntwo worker threads executing on a ma\u00adchine with two processors, we might expect the work to be done in \nabout half the time it would take a benchmark with only one worker thread executing on one processor. \nThis expectation is of course unrealistic because of the synchro\u00adnization overhead associated with parallelization. \nThe fundamental synchronization feature in Java are crit\u00adical sections. In Java a critical section corresponds \nto a synchronized block or a synchronized method. On entry to a critical section, Java executes the MonitorEnter \nbytecode. In Jikes RVM MonitorEnter is implemented as a call to the VM Thread.lock() method. When calling \nlock, a thread ei\u00adther immediately gets the lock, retries a few times in a tight loop, or yields if neither \nof the former is successful. The number of yields happening from within the lock method are an indication \nof lock contention, and thus a measure of parallelization overhead. In Java each object has an associated \nlock. A call to a synchronized instance method acquires the lock associated with the instance. A call \nto a synchronized class method acquires the lock associated with the java.lang.Class ob\u00adject describing \nthe class. And an entry to a synchronized block acquires the lock associated with the object explicitly \npassed to the synchronized statement. Knowing the type (the Java class) of a contended lock s object \ncan be very helpful for understanding the cause of lock contention in a parallel program. 4.3.2 Findings \nWe have found that a large number of time slices in multi\u00adthreaded runs is caused by lock contention. \nWorker threads try to acquire a lock that is already held by a di.erent thread, and thus they yield. \nA thread that yields ends its time slice prematurely, and thus the length of the time slice is shorter \nthan the scheduler s time quantum. 4.3.3 Exploration Since our machine has four processors we run the \nbench\u00admarks in the following con.gurations: 1 worker thread on 1 processor, 2 worker threads on 2 processors, \nand 4 worker threads on 4 processors. The only aspect that varies over the three experiments is the number \nof available processors and worker threads. The overall amount of work (mtrt: size of the picture; jbb \nand hsql: total number of transac\u00adtions) stays the same. In the following analysis we focus on the behavior \nof the worker threads and we omit the sys\u00adtem threads (compiler, garbage collector, ...) and the main \nthread (which generally just sets up the benchmark during the startup phase). Table 3 presents our measurements \nfor the three di.erent levels of parallelism of the three multithreaded benchmarks. The .rst column shows \nthe benchmark. The Scale column shows the level of parallelism (how many worker threads are executing \non how many processors). The Wall Time column show the wall clock time from the point where the .rst \nworker thread starts running to the point where the last worker thread stops running. The .rst subcolumn \nshows the time in million ticks (on the POWER4, a tick corresponds to 8 cycles). The second subcolumn \nshows the time as a percentage of the 1 on 1 time. The CPU Time column shows the sum of CPU ticks used \nby the worker threads. The Samples column shows the sum of the number of time slices used by all worker \nthreads, and the Sample D. column shows the average duration (in million ticks) of the worker thread \ntime slices. The last column gives the number of yields during lock acquisitions in the worker threads. \nWe expect the wall time to drop (the work to be completed sooner) with higher levels of parallelism, \nand the table shows that it generally does. The CPU time ideally would be con\u00adstant over the various \nlevels of parallelism. We .nd that the number increases, meaning that we actually use more CPU time for \nthe same amount of work when we increase parallelism. Only hsql on 4 processors shows an increase in \nwall time over the 2 processor run. This is either because of the synchronization overhead growing very \nbig, because of secondary e.ects (like decreased cache performance) caused by shorter time slices, or \ndue to perturbation caused by in\u00adcrementing our software performance counters. We gather the number of \nlock yields using a software per\u00adformance monitor, LockYieldCount. We can see that this value is 0 for \nall single-threaded runs. This is because there is no lock contention in this situation. For parallel \nruns, the value is greater than zero. We can also see that the number of time slices of a parallel run \nis almost equal to the num\u00adber of time slices of the corresponding single threaded run, plus the number \nof lock yields in the parallel run. There is a slight di.erence because a thread can also yield in situations \nother than a lock acquisition. We currently cannot explain the big discrepancy (72,256 samples, but only \n51,363 lock yields) observed in the hsql 4 on 4 run. In the hsql 2 on 2 and 4 on 4 runs the time slices \nshrink to less than 1% of the scheduler s scheduling quantum (of about 1.5 million ticks, or 10 ms). \nWe expect such short time slices to negatively a.ect memory performance due to the frequent context switches. \nOur vertical pro.ler is the ideal environment for continuing the study in this direction, and we plan \nto do so in future work. We have seen that the number of time slices goes up, and their length shrinks, \nas we increase parallelism, and that Benchmark Lock Yields Library VM App mtrt 0 0 0 0 mtrt 40 40 0 0 \nmtrt 350 134 216 0 jbb 0 1 0 0 jbb 2,704 0 2,703 1 jbb 8,013 113 7,843 57 hsql 0 0 0 0 hsql 28,600 2 \n0 28,598 hsql 72,012 143 149 71,720 Table 4: Classes Used for MonitorEnter this change is correlated \nwith the number of yields due to locking. Next we use another software performance moni\u00adtor, LockYieldTypeId, \nto investigate which locks are causing the observed contention. This monitor is set to the type id (a \nunique integer identi.er for each Java type) whenever a thread yields due to a lock acquisition (since \nit has to wait for the lock to be released by another thread). Table 4 gives a breakdown for the lock \nyields, by the sub\u00adsystem in which the lock type belongs. A MonitorEnter on a java.lang.String, for example, \nwould show up in the li\u00adbrary column. In this table we report all lock yields, not just the ones in the \nworker threads. As can be seen when comparing the lock yields column in this table with the same column \nin Table 3, the total number of lock yields is almost equal to the number of lock yields by the worker \nthreads. We can see that for mtrt contended locks are in the li\u00adbrary and virtual machine classes. For \njbb most contention is caused by the VM, whereas for hsql the contention is al\u00admost exclusively caused \nby the application. In jbb, the ma\u00adjority of the locks are associated with com.ibm.JikesRVM.\u00adclassloader.VM \nNormalMethod, and we suspect that this in\u00addicates contention in the runtime compilation subsystem. All \nof the 71,720 locks in hsql are associated with just one class, org.hsqldb.Database, which seems to be \nlocked for a large part of the time, and thus causes a lot of contention. Even though the primary goal \nof a vertical pro.ler is to correlate temporal behavior over di.erent levels, this case study shows that \nit can also be used to analyze classical problems, like lock contention. We used two software per\u00adformance \nmonitors to gather the information we needed to not only .nd out that the short time slices are caused \nby lock yields, but also to further investigate what locks the application was yielding for. 4.3.4 Perturbation \nAnalysis Unlike the other four case studies, this study does not analyze a temporal pattern. Thus our \nperturbation analysis methodology is not fully applicable. The SPMs never acquire any Java locks, nor \ndo they cause a yield for any other reason. Thus they do not directly a.ect the duration of time slices, \nor the number of locks acquired. But the perturbation caused by the SPMs still is signi.cant. It prohibited \nus from correlating the locking behavior to low level performance characteristics. In the future, we \nplan on reducing this perturbation by updating only the necessary monitors, and by reducing the overhead \nof a SPMs update. Benchmark Scale Wall time (M,%) CPU time (M,%) Samples Sample D. (M) Lock Yields mtrt \n1 on 1 1,070 100% 828 100% 575 1.440 0 mtrt 2 on 2 754 70% 957 116% 694 1.379 40 mtrt 4 on 4 666 62% \n1,224 148% 1,129 1.084 302 jbb 1 on 1 4,276 100% 3,227 100% 2,221 1.453 0 jbb 2 on 2 2,401 56% 3,509 \n109% 4,979 0.705 2,703 jbb 4 on 4 1,346 31% 3,836 119% 10,409 0.369 7,940 hsql 1 on 1 434 100% 265 100% \n192 1.380 0 hsql 2 on 2 313 72% 277 105% 28,849 0.010 28,600 hsql 4 on 4 367 85% 317 120% 72,256 0.004 \n51,363 Table 3: Levels of Parallelism in Multithreaded Benchmarks 4.3.5 Validation To validate our \nexplanation we would need to change the amount of lock contention in the application. Since this requires \nsigni.cant changes to the application, we did not perform this validation and instead defer it for future \nwork.  4.4 Dip before GC in Hsql From Figure 3 we see that for many benchmarks the IPC dips immediately \nbefore a garbage collection (GC). GCs show up as gaps in the curves since GC runs in separate threads \nwhile all application threads are suspended, and Figure 3 presents only the data for the main and worker \nthreads. In this study we analyze the pre-GC dip in hsql in more detail. The hsql run invokes GC .fteen \ntimes and for all except for the .rst two GCs, the IPC drops considerably, from about 0.65 to about 0.55, \njust before the GC. 4.4.1 Background We .rst identi.ed the pre-GC dip in prior work [33]. How\u00adever, at \nthat time we did not have our vertical pro.ling in\u00adfrastructure and therefore were unable to .nd the \ncause for the dips. We use a semispace GC in our con.guration. The collector uses adaptive heap resizing, \nwhich means that it can increase or decrease the size of the heap based on the application s allocation \nbehavior. At startup the heap size is set to a user\u00adspeci.ed initial value. When the application needs \nmore space, the garbage collector grows the heap up to a user\u00adspeci.ed maximum value. If the application \nneeds less space than the heap size, the garbage collector can also shrink the heap. In this way an application \ntakes only as much memory as it needs from the operating system. 4.4.2 Findings We found the pre-GC \ndip to be caused by page faults to newly allocated pages. Since object allocation .rst uses ex\u00adisting \nmemory before allocating from new memory obtained from the operating system, the dip happens immediately \nbe\u00adfore a GC (which is triggered when the application runs out of memory).  4.4.3 Exploration We started \nby using our tool to .nd metrics that corre\u00adlated with the IPC. The top two graphs in Figure 7 show the \nIPC and a correlating OS-level metric, EeO./Cyc. Ee-O./Cyc gives the fraction of cycles when CPU exceptions \nare disabled (i.e., an interrupt handler is executing in the OS kernel). We see that exceptions are rarely \ndisabled (0.2% of the cycles) except during the pre-GC dip, when exceptions are disabled for 7% to 19% \nof the cycles.  Figure 7: Behavior of Hsql s Main and Worker Threads over Time We initially thought \nthat the pre-GC dips must be due to allocation patterns: after all, it is more likely that GC will be \ntriggered in periods of intense allocation. To see if this was the case, we added the AllocBytes software \nperformance monitor to capture the number of bytes allocated in Java. We found that the rate of allocation \nwas actually lower during dips than during normal behavior. Moreover, if the pre-GC dip was caused by \na high rate of allocation, one would expect the dip to continue after the GC ended; we did not see that \nto be the case. We continued our investigation to .nd the connection be\u00adtween garbage collection (on \nthe JVM level) and exception handling (on the OS level). We added new performance monitors to record \ntransitions from Java code to native code (the only ways through which the execution could invoke an \nOS system call). We found that one system call, mmap, correlated with the pre-GC dips. The third graph \nof Fig\u00adure 7 gives data for the MmapCalls counter, which counts the number of mmap calls. We see that \nthere are no calls to mmap except during the GC dip. Moreover, we found (using the MmapBytes monitor) \nthat all calls to mmap map exactly 1 MB. Table 5 summarizes the values of relevant performance monitors. \nEach row gives the data for one dip. The GC column identi.es the GC for which the row reports pre-GC \nGC AllocBytes(M) MmapCalls MmapBytes(M) AllocB/MmapB MmapPages EeO.(M) EeO./MmapPages 1 44.2 45 45 0.98 \n11,520 298.3 25,894 2 41.7 42 42 0.99 10,752 272.9 25,381 3 3.3 3 3 1.11 768 22.9 29,818 4 5.4 5 5 1.09 \n1,280 35.2 27,500 5 7.8 8 8 0.98 2,048 48.9 23,877 6 8.6 9 9 0.95 2,304 53.9 23,394 7 9.3 9 9 1.03 2,304 \n59.7 25,911 8 10.9 10 10 1.09 2,560 70.2 27,422 9 11.3 11 11 1.02 2,816 72.5 25,746 10 11.1 11 11 1.01 \n2,816 73.2 25,994 11 11.4 11 11 1.04 2,816 70.9 25,178 12 10.8 11 11 0.98 2,816 69.9 24,822 13 10.9 11 \n11 0.99 2,816 70.9 25,178 14 11.2 11 11 1.01 2,816 73.0 25,923 15 11.3 12 12 0.95 3,072 75.4 24,544 \nTable 5: Information about Pre-GC Dips in Hsql dip numbers. Even though GCs 1 and 2 do not have a visible \ndip, we observe a considerable number of mmap calls before these GCs. Thus, we conclude that even though \nthere is no visible dip before GCs 1 and 2, it is because the dip is much wider and in fact includes \nthe entire execution before these GCs. For GCs after the .rst two, we see that there are about 10 calls \nto mmap during each GC dip. From our visualizer we determined that mmap calls do not take longer than \na single time slice, which corresponds to a single point in the IPC graphs. How, then, can 10 calls to \nmmap cause such a considerable dip in IPC? An mmap call occurs every fourth time slice during a dip but \nevery time slice in a dip has a bad IPC. We hypothesized that the pre-GC dips were related to adaptive \nheap resizing. When the garbage collector wishes to increase the size of the heap, it does not immediately \ngo out and get the space. Instead, the allocator of the garbage collector obtains the space (via mmap) \nwhen the application actually needs it. Of course, once the space is allocated, the .rst accesses to \npages in it will cause page faults. Those page faults are handled in an interrupt handler in the kernel, \nwhich has a considerably di.erent IPC than the application. The pre-GC dip is much longer before the \n.rst two GCs because the application has not yet accessed any memory at the start of the run and thus \nall accesses before the second  Figure 8: Correlation of Dips Before GC Figure 8 provides evidence for \nour hypothesis. The left scatter plot illustrates the correlation between AllocBytes and MmapBytes over \nthe 13 dips plus the full periods be\u00adfore the .rst two GCs. Not only are the two metrics almost perfectly \ncorrelated (correlation coe.cient 0.9995), but the scatter plot shows that the application allocates \nalmost ex\u00adactly the number of bytes that are also mmapped during the dip. Thus, the JVM calls mmap to \nkeep pace with the allo\u00adcation requests of the application. Second, Table 5 shows that MmapPages and \nEeO. are probably also correlated. We verify this using the right scatter plot in Figure 8. We see that \nthe number of cycles spent in the exception dis\u00adabled state during a dip is linear in the number of mmapped \npages (MmapPages)5 . In fact we can see in column Ee\u00adO./MmapPages that we spend approximately 25,000 \ncycles for each mmapped page in an interrupt handler. Thus we con\u00adclude that the page faults due to mmapped \npages cause the processor to execute a considerable amount of cycles in the page fault handler, which \nin turn impacts the IPC.  4.4.4 Perturbation Analysis This case study examines the Cyc, InstCmpl and \nEeO. HPMs. The end-to-end perturbation for HPMs is within the standard deviation of the executions without \nHPMs. In particular, end-to-end perturbation is 0.23% for Cyc, 0.26% for InstCmpl, and 0.74% for EeO., \nwhich is less than the standard deviation for executions without HPMs: 0.39% for Cyc, 0.41% for InstCmpl, \nand 1.02% for EeO.. Overhead caused by HPMs and preGC dips are not temporarily cor\u00adrelated. We were able \nto visually correlate the dips in IPC and bursts of EeO. before GC with and without SPMs. Fi\u00adnally, the \nMmapCalls, MmapBytes and AllocBytes SPMs do not impact each other. We conclude that hypotheses that we \nderive from vertical pro.ling also hold for hsql without vertical pro.ling. 4.4.5 Validation To validate \nour .nding that the dip in IPC before GC is caused by page faults due to the mmaps needed for adaptive \nheap growth, we reran hsql with Jikes RVM s adaptive heap resizing turned o.. We found that the run without \nadaptive heap resizing did not exhibit the dips in IPC or the bursts of mmaps, thus validating our explanation. \n5The metric MmapPages is computed by dividing the mmaped bytes (MmapBytes) by the size of a page (4 kB). \n4.5 Periodic Pattern in Db From Figure 3 we see that db exhibits periodic behavior. The IPC of a typical \nperiod starts at 0.338, then rises to 0.568, and .nally drops back to 0.338. Based on a visual inspection \nof the graph we counted more than 60 periods. This section uses vertical pro.ling to explain why db exhibits \nperiodic behavior.  4.5.1 Background db uses a java.util.Vector of Entry class instances to store its \ndatabase. The Entry class has an instance vari\u00adable of type java.util.Vector that stores the .elds of \nthe database record. Before performing a major operation on the database (e.g., sort or remove), db copies \nthe vector that contains all the records of the database to an array. db uses shell sort, an O(n 2) algorithm, \nto sort the array representation of the database. Shell sort divides up the database into sets and uses \ninsertion sort on each set. The .rst iteration of shell sort uses sets of size 2, the second uses sets \nof size 4, and so on, until the last iteration uses a set size that includes the entire database. The \nelements in a set are not adjacent in the database (except for the last iteration); instead, shell sort \ninterleaves the elements of all the sets. For example, in the .rst iteration for a database of size n, \nthe .rst set has elements at index 0 and n/2 - 1, the second set has elements at index 1 and n/2 etc. \n 4.5.2 Findings Our exploration revealed that the periodic patterns in the IPC graph correspond to the \n67 shell sorts in the db run. When the set size for the shell sort is small, the entire set .ts in the \nL2 cache, yielding good IPC. Moreover, since the number of times shell sort touches each element increases \nwith the size of the set, as long as the set .ts in the cache, the IPC improves with increasing set size. \nWhen the set size exceeds 2178 entries, a set is too large to .t in the L2 cache, thus causing a drop \nin the IPC.  4.5.3 Exploration Since the number of periods that we counted manually was the same as \nthe number of times db sorts the database (Table 1) we immediately suspected that each period cor\u00adresponded \nto a sort. We used the Performance Explorer to determine if there was a correlation between the IPC and \nany of the other existing performance monitors. We found an immediate high correlation (correlation coe.cient \n-0.916) with a performance monitor at the processor level: L2 cache miss rate. We also determined that \nthe L2 cache miss rate varied from 0.1 misses per 100 completed instructions at the troughs to 0.8 misses \nper 100 completed instructions at the peaks. Given that L2 cache misses are often expensive this swing \nin the L2 miss rate can account for the swing in the IPC. While the processor-level performance monitor \ntold us that L2 misses were responsible for the swings in the IPC, we still did not know what phenomenon \nactually caused the be\u00adhavior. Moreover, we did not even know whether the troughs or the peaks signaled \nthe start of a sort. To answer these questions, we added an application level performance moni\u00adtor, SetSize, \nthat keeps track of the set size of the shell sort. Figure 9 presents the signals for three metrics: \nIPC, log2(set size), and L2 cache miss rate. Rather than presenting the signals for the entire duration \nof the run, Figure 9 zooms in on one part of the run. We noted three interesting phenomena. First, once \nthe set size exceeds about 2178, the IPC starts to drop. This phe\u00adnomenon occurs because once the set \nsize increases beyond 2178, it is too large to .t in the 1.5MB L2 cache. To access a single entry in \nthe database, shell sort needs to perform six loads to entry speci.c data, with each load being to a \ndi.erent object (there are other loads also, for example, ones needed to perform array bounds checks \nbut those are likely to have good locality in the cache and thus not relevant to this discussion): (i) \nload a reference to the entry object by indexing into the array of Entry instances; (ii) load a ref\u00aderence \nto the Vector inside the Entry instance; (iii) Load reference to the Object array inside Vector; (iv) \nload the element of the Object array to obtain a reference to the key on which to sort; (v) load the \nChar array that contains the contents of the key (which is a String); (vi) load contents of the key to \nuse in the comparisons (this may actually be more than one load but most likely most of these loads will \nbe to the same cache line). Load (i) is always to the same array, however, because of the nature of shell \nsort it is typically to elements that are far apart (e.g. element 0 followed by element n/2-1). Loads \n(ii) (vi) are to distinct objects. In other words, there will be little spatial locality between two \nreferences to the same set. Given this and the 128-byte L2 line size on the POWER4, a 1.6MB6 L2 cache \nwill be large enough to hold a set of size 2178 even if there is no spatial locality between two references \nto the same set. The next larger set size (which will be about twice the size) will not .t in the 1.5MB \nL2 cache. Thus, when shell sort increases the set size beyond 2178, its working set does not .t in the \nL2 cache, which degrades performance. The second phenomenon we observe is that for set sizes ranging \nfrom 2 to 2178, the IPC increases as the set size increases. This phenomenon occurs because as the set \nsize increases, shell sort has more temporal locality. For exam\u00adple, with a set size of 2, shell sort \nloads each element in the set only once. With a set size of 4, shell sort loads the .rst two elements \nthrice, the third element twice, and the fourth element once. Thus, as long as the set .ts in the cache, \nthe ever-increasing temporal locality leads to better performance. The third phenomenon is that, contrary \nto what we ex\u00adpected, the shell sort does not start at either the trough or at the peak of the IPC curve. \nInstead it starts at a point that occurs immediately before the trough (circled in Figure 9). This happens \nbecause each sort is preceded by a phase that copies the contents of the vector that stores all the records \nin the database to an array. This phase has good locality because it reads sequentially from one data \nstructure and writes sequentially into another data structure. 4.5.4 Perturbation Analysis This case \nstudy examines the Cyc, InstCmpl, and L2Misses HPMs. The end-to-end perturbation for HPMs is either within \nor close to the standard deviation of the executions without HPMs. In particular, end-to-end perturbation \nis 2.77% for Cyc and 0.11% for InstCmpl, which is less than the standard deviation for executions without \nHPMs: 2.92% for Cyc, 0.23% for InstCmpl. Whereas, L2Misses HPM has only slightly more perturbation at \n7.49% than its standard 62178 \u00b7 6 \u00b7 128 = 1, 672, 704 Figure 9: Behavior of Db over Time deviation at \n5.97%. Tracing and periodic pattern are not correlated. We were able to visually correlate the IPC and \nL2Misses patterns with and without SPMs. The SetSize ap\u00adplication SPM is not impacted by any other SPMs. \nWe conclude that vertical pro.ling does not invalidate the hypothesis that we derived.  4.5.5 Validation \nWe determined, using vertical pro.ling, that the IPC of a shell sort in db depends on whether the set \nit is sorting .ts in the L2 cache. To validate this explanation, we man\u00adually performed object inlining \n(moving a referenced object into the referring object) on db s main data structures. We expected that \nobject inlining would enable larger sets to .t in the cache because it reduces the number of accesses \nto distinct objects in favor of more accesses to some objects. If object inlining halves the memory required \nto hold a set, then the IPC curve should start to drop at one higher set size than before. After object \ninlining we found that for some of the sorts IPC did not start to drop until the set size reached about \n5081 entries (as opposed to 2178 entries before). We believe that we did not see this phenomenon for \nall sorts because: (i) Object inlining reduced the memory required to hold a set by less than a factor \nof two. Since db performs insertions and deletions between sorts the set sizes used in two di.er\u00adent \nsorts may be di.erent and thus di.erent sorts will have slightly di.erent behavior; (ii) Our sampling \nfrequency was not high enough to take at least one sample for every change in set size. In other words, \nif we did not have a sample at set size of 5081 we could not con.rm that the IPC did indeed start to \ndrop at 5081. Overall, the object inlining optimization, guided by ver\u00adtical pro.ling, decreased db s \nrun time by 66%. Part of this decrease is due to better cache locality (as discussed above) while part \nis due to a reduction in the number of instructions (after object inlining, one needs fewer pointer dereferences). \nThus, the above data supports our explanation for db s behavior.  5. LESSONS LEARNED Our .ve case studies \nexhibited di.erent causes of signif\u00adicant performance impacts. We found that not only can performance \nbe directly a.ected by the algorithms of the application, but also can other, less obvious causes, on \ndif\u00adferent levels of the system, lead to signi.cant performance changes. We have used a vertical pro.ler \nand a performance analysis tool to .nd the causes of such performance phenom\u00adena. This section summarizes \nthe lessons we have learned while applying our tools and techniques. 5.1 Layers We mentioned in the \nintroduction that for a full under\u00adstanding of system behavior one needs to pro.le the system on multiple \nlayers. Here we show what layers we actually had to observe for our case studies. Table 6 lists, for \neach case study, what monitors we used on what layer. We can see that we needed information from multiple \nlay\u00aders for each case study. For example, we could not have explained the Gradual Increase without the \nLsuFlush moni\u00adtor that observes the hardware layer, and the MonitorEnter monitor observing the virtual \nmachine layer. The classi.cation of a monitor into a layer depends on the context. In Table 6 we list \nthe EeO. monitor in the operating system layer, even though EeO. is implemented as a hardware performance \nmonitor. This is because we assigned monitors to layers based on what behavior they observe, not on the \nlocation of the instrumentation. Table 6 also shows that we needed to use both, hardware and software \nperformance monitors. It would not have been possible to observe the .ushing of the LSU without the Lsu-Flush \nhardware performance monitor. And it would not have been possible to count the number of entries into \nsynchro\u00adnized code without the MonitorEnter software performance monitor. But sometimes it is possible \nto monitor the same behavior with both, a hardware and a software monitor. For example the EeO., measuring \nthe time spent with CPU exceptions disabled, is equivalent to a software performance monitor that measures \nthe time spent in interrupt handlers (e.g. by instrumenting the entry and exit of those handlers). In \nsuch a situation it is bene.cial to use the hardware performance monitor, because it causes less perturbation. \n 5.2 Approach We have been using two di.erent but complementary tech\u00adniques in our explorations of the \ncauses of performance prob\u00adlems: browsing and searching. 5.2.1 Browsing Whenever we were not sure about \nwhat the cause of the problem might be, browsing through the signals of all avail\u00adable performance metrics \nhelped us .nd new hints. We thus depended on the set of available metrics to cover as many subsystems \n(and thus as many causes) as possible. In our previous work on using hardware performance counters to \nunderstand Java performance, browsing only through all available hardware performance metrics helped \nus .nd the initial hints for the dip before GC (the EeO./Cyc metric) and the gradual increase (the LsuFlush/Cyc \nmetric) patterns. The browsing process can be improved by semi-automatic support for detecting correlated \nsignals, but there are some issues that prevent complete automation of this process (see Subsection 5.4). \n 5.2.2 Searching Whenever we had a hypothesis, we needed to search for a performance metric we could \nuse to test the hypothesis. Since we had a hypothesis, we knew what information we Case Study Layer Gradual \nIncrease in jbb Sudden Increase in compress Scalability in mtrt, jbb, hsql Dip Before GC in hsql Periodic \nPattern in db Application SetSize Framework Java Libraries Virtual Machine OptMonitorEnter UnoptMonitorEnter \nTopOfStackMethodId OptimizedMethodId LockYieldCount LockYieldTypeId AllocBytes Native Libraries Operating \nSystem EeO. MmapCalls MmapBytes Hardware Cyc InstCmpl LsuFlush Cyc InstCmpl Cyc Cyc InstCmpl Cyc InstCmpl \nL2Misses Table 6: Case Studies, Layers and Monitors needed to test it. But often that information was \nnot di\u00adrectly available as a performance metric. Sometimes we could solve this problem by creating a \ncomputed metric (e.g. in our approximation of the percentage of the time spent in executing unoptimized \ncode using MonitorEnter counters). At other times it was necessary to add a new software per\u00adformance \nmonitor (e.g. in observing the shell sort set size in Db). This requires a extensible system where the \naddi\u00adtion of a new counter is a simple and straightforward task. In contrast, there is no option for \nadding hardware perfor\u00admance counters, short of in.uencing the design of future microprocessors.  5.3 \nPerturbation A vertical pro.ler needs to add instrumentations to the executed code in order to update \nits software performance monitors. Depending on the frequencies of those updates, the perturbation caused \nby these additional instructions can become signi.cant. It is impossible to evaluate the pertur\u00adbation \noutside the context of a speci.c exploration. There are too many possible combinations of software performance \nmonitors that can be enabled and disabled. Furthermore, for some explorations there is a need to create \nadditional application-speci.c monitors. We have found that an ex\u00adploration based on a vertical pro.le \nneeds to be backed by a perturbation analysis. Our approach is to .rst complete the exploration, without \nany concern for perturbation. Once we have found the root cause of a performance phenomenon, we then \nverify that the results have not been in.uenced by perturbation.  5.4 Correlation The key issue in .nding \nthe cause of a performance phe\u00adnomenon is correlation. This can be correlation between a performance \nmetric and source code (e.g. program point X causes many i-cache misses), between performance metrics \non di.erent levels (e.g. high allocation rate leads to bad d\u00adcache performance), or between performance \nmetrics on the same level (e.g. high pipeline .ush rate leads to bad IPC). Correlation can be done visually \n(e.g. by comparing two time charts or by looking at a scatter plot) or statistically (e.g. by computing \nthe cross correlation coe.cient [30]). Statistical correlation requires a set of two dimensional data \npoints, where each dimension corresponds to one metric. In our case studies we have correlated di.erent \nentities (di.er\u00adent kinds of data points): samples (time slices) and patterns. In the temporal correlation \nwe used for the gradual increase of IPC , we correlated two metrics across all samples in a sample list. \nIn the pattern correlation we used for the dips before GC, we correlated two metrics across all instances \nof a pattern (e.g. pre-GC dip). Another option would be to do benchmark correlation, where we would correlate \ntwo metrics across all benchmarks. We have identi.ed several issues when using statistical correlation \nin our case studies. Sometimes the visual in\u00adspection of the two data sets would lead to the conclusion \nthat they are highly correlated, even though the (linear) cross correlation coe.cient is very small. \nHere we present a short summary of each of them. 5.4.1 Low Event Frequency Given that we have a sequence \nof values for two metrics, we can apply a simple statistic such as Pearson s cross cor\u00adrelation coe.cient \nr to determine how well those two met\u00adrics correlate. This approach works well for metrics with event \nfrequencies that are much higher than the sampling frequency used to generate the sequence of values. \nIf the event frequency (e.g. for SysMMapCalls) is close to the sam\u00adpling frequency, or even lower, then \nthe two signals resemble those of Figure 10, and the cross correlation coe.cient does not indicate any \nsigni.cant correlation. (r =0.388) Figure 11 shows the distribution of event frequencies of the 240 \nperformance counters that were incremented at least once in a jbb run with 120000 transactions and one \nware\u00adhouse thread. Each bar represents a performance counter (hardware or software), and its height represents \nthe number of events that counter observed over the whole benchmark. The leftmost bar represents the \nInstDisp counter (67 billion instructions dispatched), and the second bar from the left is the Cyc counter \n(46 billion cycles). We can see that we have counters with event frequencies distributed all over the \nfrequency spectrum, from more than once per cycle, to once per 10 billion cycles. We also have 22 performance \ncoun\u00adters that were never incremented throughout this run (not shown in the .gure). Figure 12 shows \nthe distribution of event frequencies for each of four levels of the system: hardware, operating sys\u00adtem, \nnative libraries, and virtual machine. We can see that on each level the total values of the monitors \nare distributed over a large range. Even on the VM level we can still ob\u00adserve monitors that represent \nvery high frequency events, and even on the hardware level we can see monitors for very rare events. \n  5.4.2 Non-linear Relationships The cross correlation coe.cient is a measure of the linear\u00adity of a \nrelationship between two signals. If the two signals are not linearly related (e.g. if one of them is \nan exponen\u00adtial function of the other), the cross correlation coe.cient will potentially indicate no \nsigni.cant correlation. Thus it is important to correlate only metrics that have a mostly linear relationship. \nIf the relationship between two metrics is suspected to be nonlinear, and if there is a hypothesis of \nwhat the function of the relationship is, then it can help to transform the metrics before correlation \n(e.g. to take the logarithm of one of the metrics).  5.4.3 Leverage Points A further problem is the \nin.uence of extreme data points on the correlation coe.cient. It is a well-known fact in statistics, \nthat a single so-called leverage point [30] can highly in.uence correlation, as seen in Figure 13. The \ntwo se\u00adquences of values seem to be very well correlated, except for the last data point (the leverage \npoint). But the corre\u00adlation coe.cient r is only 0.268. With the leverage point removed, the correlation \ncoe.cient increases to 0.929. Figure 13: Correlation with Leverage Point (r = 0.268) We have primarily \nfound such leverage points due to the inconsistent duration of our samples. Even though Jikes RVM s scheduler \npreempts threads every 10 ms, some of our samples represent very long time slices (e.g. the garbage collection \nappears as one time slice, since it cannot be inter\u00adrupted), while others represent very short time slices \n(some system threads that periodically wake up to perform a very small amount of work and then yield, \nor time slices in multi\u00adthreaded programs experiencing heavy lock contention and thus premature yields). \nIn shorter samples an e.ect with a small absolute magnitude can have a large impact on a relative metric \n(see sub-subsection 5.4.5). Our solution was to .lter out such time slices, if they signi.cantly but \ninap\u00adpropriately a.ected the correlation coe.cient. 5.4.4 Direction of Causality When investigating \nperformance phenomena, we are gen\u00aderally interested in .nding the cause of bad performance. Using our \ninfrastructure, we can .nd out whether two sig\u00adnals are correlated. But the infrastructure does not tell \nus which signal is the cause, and which signal is the e.ect, or whether there is a causal relationship \nat all. We have ob\u00adserved cases where the IPC goes down when the cache miss rate goes up (which probably \nmeans that the frequent cache misses stalled the CPU). But we have also observed the op\u00adposite, where \nthe IPC goes down and when the cache miss rate goes down (which probably means that something else caused \nthe bad IPC, and the bad IPC caused fewer cache misses during the same amount of time). We have found \nthat in correlating the IPC with some other metric, the magnitude and the direction of the change in \nthe other metric can indicate the direction of causality. If both IPC and the other metric change by \nabout the same percentage and in the same direction, and the other metric is a measure of work, this \noften indicates that the IPC is the cause for the change in the other metric (less work can be done because \nless instructions are completed per cycle). If the change in IPC is moderate, but the change in the other \nmetric is extreme and in the opposite direction, this can indicate that the other metric is the cause \nfor the change in the IPC (an extreme amount of extra work had to be done, and thus less instructions \ncould be completed per cycle). 5.4.5 Absolute Magnitude of Metrics in a Ratio It can be misleading to \ncorrelate a computed metric, like LdMissL1/LdRefL1 (L1 data cache load miss rate), with some other metric. \nThe miss rate could be very high, up to 1.0, but the number of load references might be so low (maybe \njust 1) that the in.uence on any other metric is negligible. Thus a high correlation between two metric \ndoes not neces\u00adsarily entail a cause-e.ect relationship.  6. RELATED WORK This section surveys work \nrelated to vertical pro.ling. This includes work on gathering performance monitor data, pro.ling Java \nworkloads, performance visualization, and sta\u00adtistical approaches to performance analysis. 6.1 Gathering \nPerformance Monitor Data Several library packages provide access to hardware per\u00adformance monitor information, \nincluding the HPM toolkit [12], PAPI [7], PCL [5], and OPro.le [27]. These libraries provide facilities \nto instrument programs, record hardware counter data, and analyze the results. The Digital Continuous \nPro\u00ad.ling Infrastructure provides a powerful set of tools to ana\u00adlyze and collect hardware performance \ncounter data on Al\u00adpha processors [3]. VTune [36] and SpeedShop [39] are sim\u00adilar tools from Intel and \nSGI, respectively. Microsoft s Win\u00addows Management Instrumentation (WMI) [25] provides a framework for \ngathering software performance counter data over time. IBM s Performance Inspector [34] is a collection \nof pro.ling tools. It includes a patch to the Linux kernel providing a trace facility and hooks to record \nscheduling, interrupt, and other kernel events. Our work di.ers in that we are gathering information \nabout several levels of hardware and software simultane\u00adously. We show in our case studies that this \ndata is use\u00adful and often necessary for .nding the root cause of a per\u00adformance issue in a complex multilayered \nsystem. And we provide insight into the problems associated in correlating performance metrics across \ndi.erent levels.  6.2 Pro.ling Java Workloads The Java Virtual Machine Pro.ler Interface (JVMPI) de\u00ad.nes \na general purpose mechanism to obtain pro.le data from a Java VM [35]. JVMPI supports CPU time pro\u00ad.ling \n(using statistical sampling or code instrumentation) for threads and methods, heap pro.ling, and monitor \ncon\u00adtention pro.ling. Our work di.ers in that we are interested in infrastructure that is capable of \nmeasuring the architec\u00adtural level performance of Java applications as well as the software level performance. \nFurthermore, our performance monitors can also measure e.ects that are not observable by using the JVMPI. \nJava middleware and server applications are an important class of emerging workloads. Existing research \nuses simula\u00adtion and/or hardware performance counters to characterize these workloads. Cain et al. [8] \nevaluate the performance of a Java implementation of the TPC-W benchmark and com\u00adpare the results to \nSPECweb99 and SPECjbb2000. Shuf et al. [32] analyze the memory performance of SPECjvm98 and pBOB on an \nIBM PowerPC processor using simulation and hardware performance counters. Luo and John [22] evalu\u00adate \nSPECjbb2000 and VolanoMark on a Pentium III pro\u00adcessor using the Intel hardware performance counters. \nSe\u00adshadri, John, and Mericas [31] use hardware performance counters to characterize the performance of \nSPECjbb2000 and VolanoMark running on two PowerPC architectures. Karlsson et al. [20] characterize the \nmemory performance of Java server applications using real hardware and a simu\u00adlator. They measure the \nperformance of SPECjbb2000 and ECPerf on a 16-processor Sun Enterprise 6000 server. Other studies focus \non behavior impacting speci.c subsystems, like Dieckmann et al. [13], who investigate memory performance \nmetrics of interest for garbage collection designers. These studies generally focus on the overall characteristics \nof the workloads. We are interested in the causes of temporal per\u00adformance phenomena, we present a system \nto gather the necessary information, and we introduce techniques for cor\u00adrelating performance information \nacross di.erent levels of a system. Dufour et al. [14] introduce a set of architecture-and vir\u00adtual \nmachine-independent Java bytecode-level metrics for describing the dynamic characteristics of Java applications. \nTheir metrics give an objective measure of aspects such as array or pointer intensiveness, degree of \npolymorphism, allo\u00adcation density, degree of concurrency, and synchronization. Our work analyzes workload \ncharacteristics on the architec\u00adtural level, and combines it with performance information from multiple \nsoftware levels. While their work focuses on abstracting away from the hardware, we focus on connecting \nsoftware behavior back to hardware performance. 6.3 Performance Visualization A large body of work exists \non performance visualization. Kimelman et al. [21] introduce PV, a performance visual\u00adizer focused on \npresenting temporal information from vari\u00adous levels of the system. PV shows only a subsection of the \nwhole trace, but it allows scrolling through the whole trace, thereby continually updating the subsection \ncurrently visu\u00adalized. Mellor-Crummey et al. [24] present HPCView, a per\u00adformance visualization tool \ntogether with a toolkit to gather hardware performance counter traces. They use sampling to attribute \nperformance events to instructions, and then hierarchically aggregate the counts, following the loop \nnest\u00ading structure of the program. Their focus is on attributing performance counts to source code areas. \nMiller et al. [26] present Paradyn, a performance measurement infrastructure for parallel and distributed \nprograms. Paradyn uses dy\u00adnamic instrumentation to count events or to time fragments of code. It can \nadd or remove instrumentations on request, reducing the pro.ling overhead. Metrics in Paradyn corre\u00adspond \nto everything that can be counted or timed through instrumentations. The original Paradyn does not support \nmultithreading, but Xu et al. [38] introduce extensions to Paradyn to support the instrumentation of \nmultithreaded applications. Zaki et al. [40] introduce an infrastructure to gather traces of message-passing \nprograms running on par\u00adallel distributed systems. They describe Jumpshot, a trace visualization tool, \nwhich is capable of displaying traces of programs running on a large number of processors for a long \ntime. They visualize di.erent (possibly nested) pro\u00adgram states, and communication activity between processes \nrunning on di.erent nodes. The newer version by Wu et al. [37] is also capable of correctly tracing multithreaded \nprograms. Pablo, introduced by Reed et al. [29], is another performance analysis infrastructure focusing \non parallel dis\u00adtributed systems. It supports interactive source code in\u00adstrumentation, provides data \nreduction through adaptively switching to aggregation when tracing becomes too expen\u00adsive, and introduces \nthe idea of clustering for trace data re\u00adduction. DeRose et al. [11] describe SvPablo (Source View Pablo), \nloosely based on the Pablo infrastructure, which sup\u00adports both interactive and automatic software instrumenta\u00adtion \nand hardware performance counters, to gather aggre\u00adgate performance data. They visualize this data for \nC and Fortran programs by attributing the metric values to speci.c source code lines. Our work combines \nthe virtues of these tools. We sample an extensive set of hardware performance monitors, combine this \nwith software performance monitors injected at many levels of the software system, including in a virtual \nmachine, and use our performance analysis tool to detect correlations, and ultimately cause-e.ect relations \nbetween performance phenomena across multiple levels.  6.4 Statistical Performance Analysis Recent work \nuses statistical techniques to analyze per\u00adformance counter data. Eeckhout et al. [15] analyze the hardware \nperformance of Java programs. They use principal component analysis to reduce the dimensionality of the \ndata from 34 performance counters to 4 principal components. Then they use hierarchical clustering to \ngroup workloads with similar behaviors. They gather only aggregate per\u00adformance counts, and they divide \nall performance counter values by the number of clock cycles. Ahn and Vetter [1] hand-instrument several \ncode regions in a set of applica\u00adtions. They gather data from 23 performance counters for three benchmarks \non two di.erent parallel machines with 16 and 68 nodes. Then they analyze that data using dif\u00adferent \nclustering algorithms and factor analysis, focusing on parallelism and load balancing. In our case studies \nwe have found that the straightforward application of statistical techniques that rely on linear rela\u00adtionships \nbetween phenomena (such as cross correlation) did not help us in discovering cause-e.ect relationships \nin com\u00adplex non-linear systems. We heavily relied on visualizations of the captured performance metrics \nover time, on identi\u00adfying data points that were not related to the phenomena under investigation but \ndue to their high leverage in.uenced the statistical computation, before verifying a correlation us\u00ading \nstatistics.  7. CONCLUSIONS This paper introduces vertical pro.ling, an approach for understanding performance \nphenomena in modern systems. This approach captures behavioral information about mul\u00adtiple layers of \na system and correlates that information to .nd the causes of performance phenomena. We have built an \ninfrastructure to capture, visualize, and correlate verti\u00adcal pro.les which we have applied to .ve case \nstudies. Each case study represents some performance anomaly that we explained using vertical pro.ling. \nThe case studies demon\u00adstrate that vertical pro.ling is an e.ective method for un\u00adderstanding the behavior \nof Java applications. To explain each performance anomaly, we used the fol\u00adlowing methodology. First, \nwe performed a perturbation analysis to con.rm that the performance anomaly actually existed. In other \nwords, we con.rmed that the performance anomaly was not an artifact of vertical pro.ling. Second, we \nused our visualizer to .nd correlations (both visually and using statistical metrics) between the anomalous \nbehavior and other metrics. Third, we followed the leads uncovered by the correlation to identify a potential \nexplanation for the anomaly. Fourth, we performed a validation to make sure that we had indeed uncovered \nthe correct reason for the performance anomaly. In the future, we intend to look at ways to further \nauto\u00admate our methodology. To our knowledge this is the .rst work that has system\u00adatically used vertical \npro.ling for performance analysis. 8. ACKNOWLEDGEMENTS We thank Sam Guyer for the idea of using, and \nimplemen\u00adtation of, object inlining to improve performance of db. We also thank Urs Hoelzle, Martin Hirzel, \nand Laureen Treacy for their feedback on the paper. This work was supported in parts by the Defense Ad\u00advanced \nResearch Project Agency under contract NBCH30390004. Matthias Hauswirth and Amer Diwan were also supported \nby NSF ITR grant CCR-0085792, an NSF Career Award CCR-0133457, and an IBM faculty partnership award. \nAny opinions, .ndings and conclusions or recommendations ex\u00adpressed in this material are the authors \nand do not neces\u00adsarily re.ect those of the sponsors. 9. REFERENCES [1] Dong H. Ahn and Je.rey S. Vetter. \nScalable analysis techniques for microprocessor performance counter metrics. In Proceedings of the 2002 \nACM/IEEE conference on Supercomputing, pages 1 16. IEEE Computer Society Press, 2002. [2] Bowen Alpern, \nC. R. Attanasio, Anthony Cocchi, Derek Lieber, Stephen Smith, Ton Ngo, John J. Barton, Susan Flynn Hummel, \nJanice C. Sheperd, and Mark Mergen. Implementing Jalape no in Java. ACM SIGPLAN Notices, 34(10):314 324, \nOctober 1999. Published as part of the proceedings of OOPSLA 99. [3] Jennifer M. Anderson, Lance M. \nBerc, Je.rey Dean, Sanjay Ghemawat, Monika R. Henzinger, Sun tak A. Leung, Richard L. Sites, Mark T. \nVandevoorde, Carl A. Waldspurger, and William E. Weihl. Continuous pro.ling: Where have all the cycles \ngone? ACM Transactions on Computer Systems, 15(4):357 390, November 1997. [4] Matthew Arnold, Stephen \nFink, David Grove, Michael Hind, and Peter F. Sweeney. Adaptive optimization in the Jalape no JVM. ACM \nSIGPLAN Notices, 35(10):47 65, October 2000. Proceedings of the 2000 ACM SIGPLAN Conference on Object \nOriented Programming, Systems, Languages and Applications (OOPSLA 00). [5] Rudolf Berrendorf, Heinz \nZiegler, and Bernd Mohr. PCL \u00adthe performance counter library. http://www.fz-juelich.de/zam/PCL. [6] \nStephen Blackburn, Perry Cheng, and Kathryn McKinley. Oil and Water? High performance garbage collection \nin Java with JMTk. In 26th International Conference on Software Engineering, May 2004. [7] S. Browne, \nJ. Dongarra, N. Garner, K. London, and P. Mucci. A scalable cross-platform infrastructure for application \nperformance tuning using hardware counters. In Proceedings of the 2000 ACM/IEEE Conference on Supercomputing, \nDallas, TX, November 2000.  [8] Harold W. Cain, Ravi Rajwar, Morris Marden, and Mikko H. Lipasti. An \narchitectural evaluation of Java TPC-W. In Proceedings of the Seventh International Symposium on High-Performance \nComputer Architecture, pages 229 240, Nuevo Leone, Mexico, January 2001. [9] Standard Performance Evaluation \nCorporation. SPECjbb2000 (Java Business Benchmark). http://www.spec.org/jbb2000. [10] Standard Performance \nEvaluation Corporation. Specjvm98 benchmarks. http://www.spec.org/jvm98. [11] Luiz DeRose and Daniel \nA. Reed. Svpablo: A multi-language architecture-independent performance analysis system. In Proceedings \nof the International Conference on Parallel Processing, Fukushima, Japan, September 1999. [12] Luiz A. \nDeRose. The hardware performance monitor toolkit. In Rizos Sakellariou, John Keane, John Gurd, and Len \nFreeman, editors, Proceedings of the 7th International Euro-Par Conference, number 2150 in Lecture Notes \nin Computer Science, pages 122 131, Manchester, UK, August 2001. Springer-Verlag. [13] Sylvia Dieckmann \nand Urs H\u00a8olzle. A study of the allocation behavior of the SPECjvm98 Java benchmarks. In Proceedings \nof the European Conference on Object-Oriented Programming (ECOOP). Springer Verlag, June 1999. [14] Bruno \nDufour, Karel Driesen, Laurie Hendren, and Clark Verbrugge. Dynamic metrics for Java. In Proceedings \nof the 18th Annual ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications, \npages 149 168, 2003. [15] Lieven Eeckhout, Andy Georges, and Koen De Bosschere. How Java programs interact \nwith virtual machines at the microarchitectural level. In Proceedings of the 18th Annual ACM SIGPLAN \nConference on Object-Oriented Programming, Systems, Languages and Applications (OOPSLA), pages 169 186, \n2003. [16] Stephen J. Fink and Feng Qian. Design, implementation and evaluation of adaptive recompilation \nwith on-stack replacement. In International Symposium on Code Generation and Optimization, pages 241 \n252, 2003. [17] Urs H\u00a8olzle, Craig Chambers, and David Ungar. Debugging optimized code with dynamic deoptimization. \nIn Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI), \npages 32 43. ACM Press, 1992. [18] IBM. Power4 system microarchitecture. http://www-1.ibm.com/servers/eserver/pseries/hardware/\u00adwhitepapers/power4.html, \n2001. [19] Jikes Research Virtual Machine (RVM). http://www.ibm.com/developerworks/oss/jikesrvm. [20] \nMartin Karlsson, Kevin E. Moore, Erik Hagersten, and David A. Wood. Memory system behavior of Java-based \nmiddleware. In Proceedings of the Ninth International Symposium on High Performance Computer Architecture, \npages 217 228, Anaheim, CA, February 2003. [21] Doug Kimelman, Bryan Rosenburg, and Tova Roth. Strata-various: \nMulti-layer visualization of dynamics in software system behavior. In Proceedings of the conference on \nVisualization (VIS 94), pages 172 178. IEEE Computer Society Press, October 1994. [22] Yue Luo and Lizy \nKurian John. Workload characterization of multithreaded Java servers. In Proceedings of the 2001 IEEE \nInternational Symposium on Performance Analysis of Systems and Software, pages 128 136, Tucson, AZ, November \n2001. [23] Cathy May, Ed Silha, Rick Simpson, and Hank Warren. The PowerPC Architecture. Morgan Kaufmann \nPublishers, Inc., San Francisco, California, 1994. [24] John Mellor-Crummey, Robert Fowler, and Gabriel \nMarin. HPCView: A tool for top-down analysis of node performance. In Proceedings of the Los Alamos Computer \nScience Institute Second Annual Symposium, Santa Fe, NM, October 2001. [25] Microsoft. Windows management \ninstrumentation. http://msdn.microsoft.com/library/en\u00adus/wmisdk/wmi/wmi start page.asp, 2004. [26] Barton \nP. Miller, Mark D. Callaghan, Joanthan M. Cargille, Je.rey K. Hollingsworth, R. Bruce Irvin, Karen L. \nKaravanic, Krishna Kunchithapadam, and Tia Newhall. The Paradyn parallel performance measurement tool. \nIEEE Computer, 28(11):37 46, 1995. [27] Opro.le. http://opro.le.sourceforge.net, 2003. [28] HSQL Database \nEngine (HSQLDB) Project. Hsql database engine. http://hsqldb.sourceforge.net. [29] Daniel A. Reed, Ruth. \nA. Aydt, Roger J. Noe, Philip C. Roth, Keith A. Shields, Bradley Schwartz, and Luis F. Tavera. Scalable \nperformance analysis: The Pablo performance analysis environment. In Proceedings of the Scalable Parallel \nLibraries Conference, October 1993. [30] Ashish Sen and Muni Srivastava. Regression Analysis: Theory, \nMethods and Applications. Springer-Verlag, 1997. [31] Pattabi Seshadri, Lizy John, and Alex Mericas. \nWorkload characterization of Java server applications on two PowerPC processors. In Proceedings of the \nThird Annual Austin Center for Advanced Studies Conference, Austin, TX, February 2002. [32] Ye.m Shuf, \nMauricio J. Serrano, Manish Gupta, and Jaswinder Pal Singh. Characterizing the memory behavior of Java \nworkloads: A structured view and opportunities for optimizations. In Proceedings of the ACM SIGMETRICS \nInternational Conference on Measurement and Modeling of Computer Systems, pages 194 205. ACM Press, 2001. \n[33] Peter F. Sweeney, Matthias Hauswirth, Brendon Cahoon, Perry Cheng, Amer Diwan, David Grove, and \nMichael Hind. Using hardware performance monitors to understand the behavior of Java applications. In \nProceedings of the 3rd Virtual Machine Research and Technology Symposium (VM 04), May 2004. [34] Bob \nUrquhart, Enio Pineda, Scott Jones, Frank Levine, Ron Cadima, Jimmy DeWitt, Theresa Halloran, and Aakash \nParekh. Performance inspector. http://www-124.ibm.com/developerworks/oss/pi, 2004. [35] D. Viswanathan \nand S. Liang. Java Virtual Machine Pro.ler Interface. IBM Systems Journal, 39(1):82 95, February 2000. \n[36] Intel VTune performance analyzers. http://www.intel.com/software/products/vtune. [37] C. Eric Wu, \nAnthony Bolmarcich, Marc Snir, David Wootton, Farid Parpia, Anthony Chan, Ewing Lusk, and William Gropp. \nFrom trace generation to visualization: A performance framework for distributed parallel systems. In \nProc. of SC2000: High Performance Networking and Computing, November 2000. [38] Zhichen Xu, Barton P. \nMiller, and Oscar Naim. Dynamic instrumentation of threaded applications. In Principles Practice of Parallel \nProgramming, pages 49 59, 1999. [39] Marco Zagha, Brond Larson, Steve Turner, and Marty Itzkowitz. Performance \nanalysis using the MIPS R10000 performance counters. In Proceedings of the 1996 ACM/IEEE Conference on \nSupercomputing, November 1996. [40] Omer Zaki, Ewing Lusk, William Gropp, and Deborah Swider. Toward \nscalable performance visualization with Jumpshot. High Performance Computing Applications, 13(2):277 \n288, Fall 1999.  \n\t\t\t", "proc_id": "1028976", "abstract": "<p>Object-oriented programming languages provide a rich set of features that provide significant software engineering benefits. The increased productivity provided by these features comes at a justifiable cost in a more sophisticated runtime system whose responsibility is to implement these features efficiently. However, the virtualization introduced by this sophistication provides a significant challenge to understanding complete system performance, not found in traditionally compiled languages, such as C or C++. Thus, understanding system performance of such a system requires profiling that spans all levels of the execution stack, such as the hardware, operating system, virtual machine, and application.</p> <p>In this work, we suggest an approach, called &#60;i>vertical profiling&#60;/i>, that enables this level of understanding. We illustrate the efficacy of this approach by providing deep understandings of performance problems of Java applications run on a VM with vertical profiling support. By incorporating vertical profiling into a programming environment, the programmer will be able to understand how their program interacts with the underlying abstraction levels, such as application server, VM, operating system, and hardware.</p>", "authors": [{"name": "Matthias Hauswirth", "author_profile_id": "81332503330", "affiliation": "University of Colorado at Boulder", "person_id": "PP43124501", "email_address": "", "orcid_id": ""}, {"name": "Peter F. Sweeney", "author_profile_id": "81332530743", "affiliation": "IBM Thomas J. Watson Research Center", "person_id": "PP43124869", "email_address": "", "orcid_id": ""}, {"name": "Amer Diwan", "author_profile_id": "81100202872", "affiliation": "University of Colorado at Boulder", "person_id": "PP15025608", "email_address": "", "orcid_id": ""}, {"name": "Michael Hind", "author_profile_id": "81339504521", "affiliation": "IBM Thomas J. Watson Research Center", "person_id": "PP43136124", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1028976.1028998", "year": "2004", "article_id": "1028998", "conference": "OOPSLA", "title": "Vertical profiling: understanding the behavior of object-priented applications", "url": "http://dl.acm.org/citation.cfm?id=1028998"}