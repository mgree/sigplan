{"article_publication_date": "10-01-2004", "fulltext": "\n Finding Your Cronies: Static Analysis for Dynamic Object Colocation. Samuel Z. Guyer Kathryn S. McKinley \nThe University of Texas at Austin The University of Texas at Austin Austin, TX, 78712 Austin, TX, 78712 \nsammy@cs.utexas.edu mckinley@cs.utexas.edu ABSTRACT This paper introduces dynamic object colocation, \nan optimization to reduce copying costs in generational and other incremental garbage collectors by allocating \nconnected objects together in the same space. Previous work indicates that connected objects belong together \nbe\u00adcause they often have similar lifetimes. Generational collectors, however, allocate all new objects \nin a nursery space. If these ob\u00adjects are connected to data structures residing in the mature space, \nthe collector must copy them. Our solution is a cooperative opti\u00admization that exploits compiler analysis \nto make runtime allocation decisions. The compiler analysis discovers potential object con\u00adnectivity \nfor newly allocated objects. It then replaces these alloca\u00adtions with calls to coalloc, which takes an \nextra parameter called the colocator object. At runtime, coalloc determines the loca\u00adtion of the colocator \nand allocates the new object together with it in either the nursery or mature space. Unlike pretenuring, \ncolocation makes precise per-object allocation decisions and does not require lifetime analysis or allocation \nsite homogeneity. Experimental re\u00adsults for SPEC Java benchmarks using Jikes RVM show colocation can \nreduce garbage collection time by 50% to 75%, and total per\u00adformance by up to 10%. Categories and Subject \nDescriptors D.3.4 [Programming Languages]: Processors Compilers, Mem\u00adory management (garbage collection) \n General Terms Languages, Performance, Experimentation, Algorithms  Keywords Cooperative optimization, \nstatic analysis, compiler-assisted mem\u00adory management .This work is supported by NSF ITR CCR-0085792, \nNSF CCR\u00ad0311829, NSF EIA-0303609, DARPA F33615-03-C-4106, and IBM. Any opinions, .ndings and conclusions \nexpressed herein are the authors and do not necessarily re.ect those of the sponsors. Permission to make \ndigital or hard copies of all or part of this work for personal or classroom use is granted without fee \nprovided that copies are not made or distributed for pro.t or commercial advantage and that copies bear \nthis notice and the full citation on the .rst page. To copy otherwise, to republish, to post on servers \nor to redistribute to lists, requires prior speci.c permission and/or a fee. OOPSLA 04, Oct. 24-28, 2004, \nVancouver, British Columbia, Canada. Copyright 2004 ACM 1-58113-831-8/04/0010 ...$5.00. 1. INTRODUCTION \nThis work introduces the colocation (or crony) optimization for garbage collectors that divide the heap \ninto separately collected re\u00adgions, such as generational collectors. A problem for these collec\u00adtors \nis that they record and treat as live pointers into independently collected regions. Pointers in connected \ndata structures that cross regions will cause the collector to retain their referents. For ex\u00adample, \nFigure 1 depicts a connected data structure that straddles two collection regions. Since collectors must \nassume that point\u00aders between separately collected regions are live, they will retain this data structure, \nperhaps needlessly. Furthermore, previous work shows that connected objects usually die together [21, \n22]. The goal of colocation is to allocate a new object directly into the same region as an object that \nwill reference it. Grouping and collect\u00ading connected objects together will thus avoid the work of tracking \nand processing pointers from different regions, and more promptly reclaim objects when they die. Colocation \nis a cooperative opti\u00admization because it uses compiler analysis to selectively introduce dynamic allocation \ndecisions. Figure 1: Connected structures belong in the same space. Colocation uses the following static \nand runtime components. (1) A static compiler analysis .nds old objects, called colocators that will \nreference newly allocated objects and that the program does not immediately overwrite. (2) A new allocation \nroutine, coalloc, which takes a colocator object parameter. (3) At run\u00adtime, coalloc puts new objects \nin the same region as the colo\u00adcator object. The analysis .nds connections between newly allo\u00adcated objects \nand existing objects, with special provisions to ex\u00adclude volatile (quickly overwritten) connections. \nThe interprocedu\u00adral compiler analysis is .ow-insensitive and exploits static-single\u00adassignment (SSA) \nform. The compiler analysis need not be sound since the location of the object does not affect correctness, \nand we exploit this feature to make our analysis fast. We implement this analysis in the IBM Jikes RVM \n[2, 3] optimizing compiler. At runtime, coalloc makes precise per-allocation decisions based on the current \nlocation of existing colocator objects. Un\u00adlike pretenuring [9, 12, 20] or proli.c types [30], colocation \ndoes not require lifetime analysis or call site homogeneity. Homogene\u00adity limits applicability, especially \nfor library routines with many different client uses. Colocation and pretenuring are most likely synergistic, \nbut that study is beyond the scope of this paper. Colo\u00adcation goes beyond age-based copying algorithms \n[8, 27, 33, 36] to exploit dynamic connectivity, but does not require strict analysis correctness as \ndoes connectivity-based collection [21]. We discuss related work in more detail in Section 2. Although \nwe evaluate colocation in two generational collectors, it will work for any col\u00adlector that divides the \nheap into independently collected regions. Using MMTk [6, 7], a Memory Management Toolkit for Java in \nIBM Jikes RVM [2, 3], we evaluate the overhead of colocation, the potential reduction from connected \nobjects in two generational col\u00adlectors, and the performance impact. The current implementation produces \nits best results when all methods are compiled ahead of time in this setting our analysis only increases \ncompilation cost by about 10%. It is considerably less expensive than previous alias and escape analysis \nfor Java [10, 15], and we believe we can fur\u00adther reduce this cost. We evaluate colocation using a copying \nnurs\u00adery with both copying and mark-sweep mature spaces. It reduces garbage collection time for all the \nSPEC JVM benchmarks by up to a factor of 2. For a few benchmarks, the generational collectors augmented \nwith colocation can execute in a smaller heap sizes than without it because colocation helps them to \nuse heap space more ef.ciently. For most programs, the collection time improvements translate into total \ntime improvements, of up to 10%, since garbage collection time is a fraction of total time. The rest \nof this paper is organized as follows. In Section 2 we review related work. In Section 3 we describe \nthe overall system for performing cooperative object colocation. In Sections 4 and 5 we describe our \nstatic analysis and run-time system. In Sections 6 and 7 we present our experiments and results. 2. \nRELATED WORK This section overviews generational collectors and the opportuni\u00adties they expose for colocation. \nWe then compare our work to con\u00adnectivity analysis, allocation for locality, and static and dynamic lifetime \nprediction and its use by pretenuring. Colocation is unique from previous work because it does not require \ncall site homogene\u00adity or lifetime pro.ling or prediction. We also compare our com\u00adpiler analysis to \nother static heap analyses. All generational collectors exploit the weak-generational hypoth\u00adesis [27, \n36], that young object die quickly. Copying generational collectors divide the heap, allocate the youngest \nobjects into the nursery, and most frequently collect the nursery [5, 36]. To avoid scanning the mature \nspace when collecting the nursery, the write barrier tests all heap pointer stores, and remembers the \nsource of pointers that point from the mature space into the nursery at run\u00adtime. At collection time, \nit copies objects reachable from the stacks, registers, global variables, and these remembered pointers \ninto the mature space. The collector re-examines remembered pointers .rst, since a later update may have \noverwritten the nursery target. Recent work on connectivity-based garbage collection [21, 22] shows that \nconnected objects often die together, which bolsters our hypothesis. However, their collector organization \ncompletely elim\u00adinates write barriers with a static analysis that allocates objects into static partitions \nthat contain connected objects, forming a hierar\u00adchical directed acyclic graph of partitions. The collector \nmust al\u00adways collect ancestors together with descendents, and thus does not exploit dynamic connections. \nThe colocation optimization exploits object connectivity in a much less restricted setting, and is thus \nable to couple itself with the high performance generational collectors, and could also improve other \nincremental collectors [8, 24, 29]. Research that uses copying collection to improve object local\u00adity \n[14, 23, 25, 37] can also exploit connectivity. This work locates objects that are frequently accessed \ntogether on the same cache line. Object colocation may have a positive bene.t on page locality since \nit will tend to allocate connected objects closer together, but it will not generally improve cache line \nlocality. Locality and colocation optimizations are thus orthogonal, but could work well together. The \npretenuring optimization uses static pro.ling to classify ob\u00adjects as long lived, and then directly allocates \nthem into the older generation [9, 12, 34, 35], or uses dynamic samples [1, 17, 20, 28] through weak \npointers and write barriers. All of these techniques require call site lifetime homogeneity, which is \nrestrictive. For ex\u00adample, the top allocation site in javac creates entry nodes for a hash table and \nthe lifetimes are split 55-45 short-lived versus long-lived. Object colocation works without call site \nhomogeneity because it asks on a per-instance basis: Is the existing colocator in the nurs\u00adery or mature \nspace? Our compiler analysis is similar in spirit to work that .nds con\u00adnected heap objects [11, 19], \nbut is much faster and less precise. In fact, colocation analysis need not be conservative, since the \nalloca\u00adtion of an object in the mature space or the nursery does not affect correctness, only performance. \nWe use a .ow insensitive, single pass analysis, and experiment with intraprocedural and interproce\u00addural \npropagation. This approach makes the compiler analysis vi\u00adable for a just-in-time compiler, where as \nescape [10, 15] and other pointer analyses are too costly in this context.  3. DYNAMIC OBJECT COLOCATION \nThe goal of dynamic object colocation is to allocate connected ob\u00adjects in the same garbage collection \nspace. Since collectors use connectivity to determine survivors, the lifetimes of connected ob\u00adjects \nare correlated [22], and placing them in the same space can improve collector ef.ciency. Colocation produces \nthis effect dy\u00adnamically by determining in which space the source of a pointer resides, and then allocating \nthe target of the pointer in the same space. Similar to pretenuring, colocation tends to put short-lived \ncon\u00adnected objects in the nursery, and long-lived ones in the mature space. In contrast to pretenuring, \nhowever, a given allocation site can allocate to either the old or young space, depending on the objects \ninvolved. This .exibility is particularly important for allo\u00adcation sites inside widely reused code, \nsuch as the Java container classes. For example, the LinkedList class contains an inter\u00adnal link class \nthat makes up the backbone of the list, and the add()method allocates instances of this class in order \nto accom\u00admodate new elements. Pretenuring schemes that are triggered by types or allocation sites must \ndecide, for all linked lists, in which space add() will place new instances of the link. This decision \npresents a dif.cult tradeoff in programs that create both short-lived and long-lived lists. In contrast, \ncolocation avoids this tradeoff by making the decision dynamically: it allows add() to place new link \nelements into whichever space contains the existing elements of the list. Our approach uses a new memory \nallocation routine, which we call coalloc, that takes an addition argument of type Object, which we call \nthe colocator. Coallocallocates the new object into the same space as the colocator. Unlike previous \napproaches to colo\u00adcation for locality, we do not expose the allocation interface to the user [13]. Our \nsystem automatically identi.es candidate allocation sites and computes appropriate colocators for them. \nSince .nding a colocator requires knowledge of the future use of a new object (e.g., its incorporation \ninto a list), our system performs this task at com\u00adpile time using static analysis. Since the particular \nspace in which the colocator resides is only known at run-time (and is collector\u00adspeci.c), our system \nmakes allocation decisions at run-time using a dynamic test. Our system consists of two parts.  Figure \n2: Simple example: the newly allocated B object is stored in the A object parameter, therefore we convert \nthe new into a call to coalloc, passing aas the colocator. Figure 3: More complex example: we cannot \nuse newCas the colocator for the new B because the new C is created later; however we can safely use \naas the colocator for both. Compile-time analysis identi.es colocators and inserts calls to coalloc. \nRun-time system provides the coallocallocation routine. Figure 2 shows a simple example of how colocation \nworks. The code fragment on the left shows a method called Simplethat cre\u00adates a new object of type B \nand stores a reference to it in the object of type A pointed to by the variable a. The graph in the middle \nof the .gure depicts the resulting data structure. Unless the program subsequently overwrites the reference, \nthe newly allocated B object will live at least as long as A. Therefore, the variable a is a good choice \nfor the colocator of the new B object. The code on the right shows the result of our compiler pass. Our \nanalysis automatically identi.es a as a suitable colocator and replaces the new construct with a call \nto coalloc, passing in the variable a. The coalloc routine makes an allocation decision based on a:if \na refers to an object in the mature space, then coallocputs the new B object in the mature space, otherwise \ncoalloc puts it in the nursery. The key is that this decision depends on the run-time value of a, which \ncan vary from one invocation of the method to another. Unfortunately, it is not always possible to colocate \na new object with the object that directly references it. The reason is that pro\u00adgrams need not create \nobjects in the same order that they connect them together. The method BottomUp in Figure 3 demonstrates \nthis problem: newC is the logical choice of colocator for the al\u00adlocation of newB,but newC does not yet \nexist at that point. One solution is to abandon colocation is such cases. Another solution is to attempt \nto transform the code so that the order of creation matches the order of connection. Both of these solutions \nrequire some form of dependence analysis and may still miss colocation opportunities. Our solution is \nto use only the formal parameters of a method (in\u00adcluding the receiver object) as potential colocators. \nThe advantage of this solution is that the formal parameters are guaranteed to exist before any allocations \ntake place in the method, so no dependence analysis is needed. This solution works because connectivity, \nand therefore survival, is transitive: since newBlives as long as newC, and newC lives as long as a, \nwe can conclude that newB lives as long as a. Therefore, a is a suitable colocator for both allocation \nsites. The code fragment on the right in Figure 3 shows the appli\u00adcation of this policy to the BottomUpmethod. \nMore formally, we select colocators as follows: given an alloca\u00adtion site A inside a method M, we choose \na colocator C from among the formal parameters of M (including the receiver) such that dur\u00ading the execution \nof M the object or objects created at A become reachable, by some sequence of pointers, from C. If no \nparameter is suitable, then the allocation site has no colocator.  4. FINDING COLOCATORS This section \ndescribes our static analysis for .nding colocators. The goal of this analysis is to .nd a suitable colocator \nfor each allocation site. We start by presenting our basic analysis algorithm, followed by two interprocedural \nenhancements. This algorithm can discover most of the potential connectivity between objects in a program. \nThis information, however, is often overly aggressive for colocation because some of the connections \nare volatile and short\u00adlived at runtime. Examples of volatile connections include refer\u00adences that are \ncreated conditionally and containers that are cleared. Therefore we add to our algorithm a set of heuristics \nthat prune out potentially volatile connections. Our colocation analysis resembles existing algorithms \nfor pointer analysis, but differs in several unique ways. Central to these differ\u00adences is that our analysis \nis unsound: its results do not necessarily represent all the connections that might occur at runtime. \nTherefore it cannot be used for traditional optimizations, which require sound analysis in order to preserve \nprogram correctness. For colocation, however, our analysis need not be sound because changing the al\u00adlocation \nspace is always safe, even if it s not always pro.table. We exploit our exemption from soundness in several \nways. First, the volatility heuristics mentioned above intentionally ignore cer\u00adtain connections between \nobjects. We present these heuristics in detail in the last part of this section. Second, we signi.cantly \nsim\u00adplify our algorithm. For example, since colocation is only con\u00adcerned with overall reachability of \none object from another (as op\u00adposed to pointer aliasing), we do not need to accurately model the number \nof pointer hops between objects. This .exibility allows us to employ a simple and compact representation \nfor the method summaries used in our interprocedural analysis. Finally, by giv\u00ading up soundness we avoid \nthe problems presented by certain Java language features, such as dynamic class loading, re.ection, and \nnative methods. These features present signi.cant challenges for sound pointer analysis algorithms [?]. \nOur analysis can safely ig\u00adnore these features even in programs that use them, obviating the need for \nthe so-called closed-world assumption . In the remainder of this section we present our analysis in de\u00adtail \nand point out its unique features, particularly those that make it unsound. We implement this analysis \nusing the Jikes RVM op\u00adtimizing compiler, which includes an internal representation based on static-single \nassignment form (SSA) [16]. Each method consists of a list of simple operations applied to temporary \nvariables (virtual registers). Our analysis algorithm is .ow insensitive in that it does not associate \nanalysis information with particular program points. However, since only one de.nition of a variable \nreaches each use, SSA form provides some .ow sensitivity. 4.1 Basic colocation analysis The basic colocation \nanalysis builds a graph that represents a con\u00adservative approximation of the connectivity between objects \nin a method. This algorithm resembles Andersen-style pointer analy\u00adsis [4, 18]: it is .ow-insensitive \nand inclusion-based. The graph it creates captures connectivity among the objects allocated by the method \nand connectivity from local variables to those objects. The compiler then searches this graph to identify \npotential colocators. The analysis starts by identifying the relevant components of each method: S Set \nof statements in the method (in compiler IR) V Set of variables in the method pi EV Formal parameters \n indexed by parameter number i as ES Allocation sites indexed by statement s Each node in the connectivity \ngraph represents a heap-allocated object: either an old object (pointed to by a parameter) or a new object \n(generated by an allocation site). For example, Figures 2 and 3 depict the connection graph for the two \nexample code frag\u00adments where a represents an old node, and B and Crepresent new nodes. The graph nodes \nare identi.ed as follows: oi ENold A node for each parameter pi old objects ns ENnew A node for each \nalloc site as new objects N =Nold UNnew Set of all graph nodes Edges in the graph are directed and \nrepresent possible points-to relationships. An edge between two nodes represents a pointer be\u00adtween two \nheap-allocated objects. Our analysis does not distin\u00adguish between the different .elds of an object. \nThere are also edges from elements of V to nodes in the graph, which represent pointers from the method \ns local variables into the heap. We initialize the points-to graph with an edge from each of the formal \nparameters to its corresponding old object. This initial structure implies that parameters do not alias \neach other, which is not generally a safe assumption. points-to: {N UV )-2N Graph edges a mapping from \na variable or node to its possible targets Vi :points-to{pi) = oi Initialize parameter variables to point \nto old objects The analyzer takes one pass over the statements in a method, adding edges to the points-to \ngraph according to the analysis rules shown below. The rules for allocation, assignment, and SSA f functions \nare straight-forward: they just transfer the points-to sets from the right-hand side expression to the \nleft-hand side variable. Op Statement Effect new s:v = new O(); points-to{v)=ns assign v= y; points-to{v)U=points-to{y) \nphi v= f(v0,...); points-to{v)U=Vi,points-to{vi) getfield aload v = y.f; v = y[i]; points-to{v)U=points-to{y) \nputfield astore v.f = y; v[i] = y; Vm Epoints-to{v): points-to{m)U=points-to{y) Unlike other pointer \nanalysis algorithms, our rule for getfield (and aload) does not dereference the right-hand side variable. \nIn\u00adstead, it ignores the .eld altogether and just treats the statement as an assignment. Skipping the \ndereference operation further simpli\u00ad.es analysis and it does not affect overall reachability. For example, \nwe can treat v=y.fas v=ybecause anything reachable from y.f is also reachable from y. After it builds \nthe points-to graph, the analysis simply computes which, if any, allocation nodes are reachable from \neach parameter in the graph. We test reachability by computing the closure over the points-to function \nfor each parameter. A parameter is a potential colocator for an allocation site if that node is in the \nclosure. reach{p) = {m I m Epoints-to{p)V m Ereach(points-to{p))} coloc{as) = {pi I ns Ereach{pi)} Notice \nthat the analysis may .nd multiple suitable colocators for a single allocation site. In early experiments \nwe compared the ef\u00adfects of choosing different colocation policies: (1) taking the .rst colocator, in \nparameter order, (2) only using coallocif there is a single colocator, (3) combining multiple colocators \nat run-time by taking the conjunction or disjunction of the colocation decisions. We found, however, \nno signi.cant difference in the run-time effect of the different policies. Most of the time, the colocators \nagree on the colocation decision. Therefore, all the results shown in Sec\u00adtion 7 use policy (1). 4.2 \nInterprocedural algorithm The analysis algorithm described so far is intraprocedural: it only considers \nallocations and connections that occur within a single method. It is common, however, for programs to \ncreate objects in one method and assemble them in a different method. To han\u00addle this case, we compute \na simple summary for each method and apply the summary wherever the method is called. Since our anal\u00adysis \ndoes not require soundness, we can safely ignore method calls when no summary is available. In practice, \nthough, we .nd the summaries are critical for effective colocation and we quantify these bene.ts in Section \n7. Method summaries cover two programming constructs found frequently in object-oriented programs: factories \nand containers. 4.2.1 Factory methods Factory methods are a common design pattern in object-oriented \nprogramming: a factory method just creates and returns objects on behalf of other methods, and thus behaves \nas an allocation routine. Our solution is to detect these methods, and then treat them as al\u00adlocation \nsites in their callers. We describe the modi.cations to our analysis below, and Section 5 shows the run-time \ninstrumentation for colocation in a factory method. To detect factory methods we add the following analysis \nrule to collect the set of variables R that might be returned from a method: Op Statement Effect return \nreturn v; R U=points-to{v) At the end of the analysis, the analyzer checks to see if any alloca\u00adtion \nnodes are reachable from returned variables. If so, it marks the method as a factory and records the \nallocation sites that generate the returned objects. Section 5 describes how we provide coloca\u00adtors for \nthese sites. is f actory{m)= true if :as : as Ereach{Rm) false otherwise We also need a rule to handle \nthe factory call sites. This rule mirrors the existing rule for regular allocation sites: Op Statement \nEffect call s: v = obj.m(); if is f actory{m): Create node ns as alloc site, points-to{v)=ns 4.2.2 Connector \nmethods Another common programming practice is the use of container classes, such as the standard Java \nlibrary. Container classes present a problem for our intraprocedural analysis because they encapsulate \nthe code that connects new objects to their containers. For example, at a call to Vector.addElement()our \nintraprocedural analy\u00adsis cannot determine that this method creates a connection between the vector and \nthe input argument. Our solution is to provide this additional information in the form of method summaries. \nWe compute a connection summary for a method while comput\u00ading colocators. In addition to detecting allocation \nnodes that are reachable from the parameters, we compute reachability between parameters. For each parameter \npi, if some other parameter pj is reachable from pi then we record the parameter numbers as a pair: Summary{m)={{i,j)Ipj \nEreach{pi)} For example, our analysis generates a summary consisting of {0,1) for the Vector.addElement() \nmethod because the new ele\u00adment (parameter 1) is attached to the receiver Vector (parameter 0). During \nanalysis, we use the method summary at a call site by ap\u00adplying the putfieldrule to each of the integer \npairs {i,j). Note that the edges created by this rule might represent many edges in the callee method, \nbut collapsing those edges into a single edge does not affect overall reachability. Op Statement Effect \ncall obj.m(v0,...); V{i,j)ESummary{m): apply vi.f = vj  4.3 Volatility heuristics This section describes \nour volatility heuristics, which help prevent overly aggressive colocation. The colocation analysis described \nabove detects almost all potential connectivity between the objects in a program. However, this analysis \nis too aggressive because pro\u00adgrams often introduce volatile or unstable connectivity. For exam\u00adple, \nprograms sometimes quickly overwrite a connection, or only install connections under special conditions. \nExcessive colocation can force objects with dramatically different lifetimes into the same space, hurting \nthe ef.ciency of collection. In our experiments using generational collectors this effect manifests itself \nas an excess of short-lived objects colocated in the mature space, requiring costly full-heap collections \nto recover. These heuristics are conservative in the sense that if a reference appears to be volatile \nthen we exclude it from colocation. For some programs these additions are overly conservative, but in \nseveral cases they prevent pathological behavior. The code fragment in Figure 4 shows two examples of \nvolatile connections. The .rst example creates a new string, but only adds it to the container if it \nis not already there. At compile time, we do not know how frequently that condition might be true, so \nwe act conservatively and avoid colocating the string with the container. In the second example, the \nloop .lls the container with new objects, but then immediately clears it. Again, to conservatively prevent \nexcessive colocation, the analysis prohibits colocation. To capture 1 void Volatile(Container c, Value \nv) { 2 // --The newly created string is not always 3 // stored in the container: 4 String value_name \n= v.toString(); 5 if ( ! c.contains(value_name)) 6 c.add(value_name); 7 8 // --Objects don t remain \nin the container 9 // for long... 10 for (...) { 11 c.add(new String(...)); 12 } 13 c.clear(); \n14 } Figure 4: Examples of volatile connectivity. this notion of volatility, we place two additional \nconditions on the analysis rules and modify the reachability computation. We place two restrictions on \nthe putfield rule in order to avoid volatile or uncertain connections. First, we prohibit colo\u00adcation \nwhen the putfield that connects a new object to an old one is guarded by a condition, but the creation \nof the new object is not. We test for this case using post-dominance: We only apply the putfieldrule \nwhen the putfieldpost-dominates the creation of the stored object. Second, we prohibit colocation when \nthe program stores the re\u00adsults of a get.eld operation. Our reasoning is that the object pro\u00adduced by \na get.eld is already connected to some other data struc\u00adture, so the additional connectivity is unlikely \nto help colocation. We can imagine cases where this condition would help colocation. For example, if \nan object is stored in a temporary object before being connected to another data structure. We prefer \nto act conser\u00advatively, and .nd this opportunity is rare. Our third heuristic is designed to detect cleared \ndata structures. During the analysis we compute the set of objects C into which the program explicitly \nstores null. We add the following analysis rules: Op Statement Effect putfield null astore null v.f = \nnull; v[i] = null; C U=points-to{v) During the closure computation we do not follow the outgoing edges \nfrom cleared objects: reach{p)={m IEC . m .( m Epoints-to{p)V m Ereach(points-to{p))}  5. RUNTIME SYSTEM \nThis section describes the run-time components of dynamic object colocation: (1) The coalloc routine, \nwhich replaces the regular memory allocation routine and performs the run-time colocation test, (2) the \nmechanism for passing colocators down through fac\u00adtory methods to the underlying allocation sites, and \n(3) an exten\u00adsion to coallocthat speculative colocates objects based on their relative ages. This last \nfeature is more aggressive than the standard colocation system, but can improve colocation in collectors \nthat use an unbounded nursery. 5.1 Coalloc The compiler replaces calls to the regular memory allocation \nrou\u00adtine with a coalloc call only when the analysis .nds a suitable colocator. Figure 5 shows an abstraction \nof the original code and its replacement. Since we use two-generational collectors for all of our experiments, \ncoalloc tests the colocator to decide whether to allocate the new object in the nursery or in the mature \nspace. Our VM assigns speci.c address ranges to each of these spaces, so we can determine which space \nthe colocator occupies by a simple address comparison. In our collectors the nursery space resides at \na higher range than the mature space, so the less-than test in Figure 5 returns true if the colocator \nis not in the nursery. Since allocation time typically represents less than 1% of total time [23], and \nsince these values are usually in registers, this overhead is negligible. 1 public VM_Address alloc(int \nbytes) { 2 return nursery.alloc(bytes); 3 } (a) Original allocation. 1 public VM_Address coalloc(int \nbytes, 2 VM_Address colocator) { 3 if (! colocator.isZero() &#38;&#38; 4 colocator.LT(NURSERY_START)) \n5 return matureAlloc(bytes); 6 else 7 return nursery.alloc(bytes); 1 8 } 2 (b) Coalloc. 3 4 5 6 Figure \n5: The colocator argument selects the allocation space in coalloc. 7 8  5.2 Factory methods 9 10 \nFactory methods colocate objects based on the use of objects in the calling method. Therefore, we provide \na mechanism for the caller to pass a special factory colocator down into the factory method. Ideally, \nwe might alter the factory method interface to accept an additional object argument. However, this strategy \nrequires us to make sure that any potential callers and any factory subclasses are properly modi.ed to \nre.ect the new interface. Therefore, the cur\u00adrent system instruments the caller to store the factory \ncolocator in\u00adside the VM, and the callee retrieves the value and holds it in a local variable. This strategy \nis easy to implement and is correct even if the caller does not recognize the callee as a factory method. \nFigure 6 shows an example of this instrumentation. We avoid con\u00adfusion and contention across threads \nby allowing the VM to save factory colocators on a per-thread basis. 1 void someMethod(Container c) { \n 2 3 VM_save_factory_colocator(c); 4 Element e = Factory.makeElement(); 5 c.add(e); 6} (a) Caller. \n1 class Factory { 2 Element makeElement() { 3 Object factory_colocator = 4 VM_get_factory_colocator(); \n5 return coalloc(..., factory_colocator); 6} 7} (b) Callee. Figure 6: The caller passes colocators to \nFactory methods.  5.3 Speculative age-based colocation In generational collectors, dynamic object colocation \nprimarily serves to allocate new objects into the mature space only when the coloca\u00adtor is in the mature \nspace. However, we can also look at the relative age of a colocator, even if it currently resides in \nthe nursery. Fig\u00adure 7 shows a diagram of the nursery space with a colocator close to the older end of \nthe nursery and the current bump pointer at the young end of the nursery. The colocator is almost certainly \nlive at this point, and thus likely to survive the next collection, especially for large nurseries. Therefore \nwe can speculatively place the new object in the mature space when the colocator is old, but still in \nthe nursery. Relative age colocator bump-ptr Figure 7: Age-based colocation: even if the colocator is \nnot in the mature space, if it is old enough we can allocate the new object in the mature space. public \nVM_Address coalloc(int bytes, VM_Address colocator) { int age = nursery.cursor.diff(colocator).toInt(); \n if (! colocator.isZero() &#38;&#38; (colocator.LT(NURSERY_START) || (age > AGE_THRESHOLD))) return \nmatureAlloc(bytes); else return nursery.alloc(bytes); } Figure 8: Coalloc routine with age-base speculative \npromotion. The implementation of this feature involves adding an address\u00adrelative test to the coallocroutine. \nFigure 8 shows the modi.ed coallocroutine with an age-relative colocation test. In Section 7 we show \nthe effect of this policy under an unbounded Appel-style nursery using a 4 MB age threshold. This nursery \ncon.guration delays collection of the nursery as long as possible, resulting in relatively old objects \nresiding in the nursery instead of in the ma\u00adture space. Age-based colocation places new objects in the \nmature space when their colocators are old enough, but not yet in the ma\u00adture space.  6. METHODOLOGY \nThis section brie.y describes our experimental methodology, in\u00adcluding our generational collectors, MMTk, \nJikes RVM, compile\u00adtime strategy, and platform. 6.1 Generational Collectors We perform our experiments \nin an ef.cient, composable Java mem\u00adory management toolkit that implements a wide variety of high performance \ncollectors that reuse shared components [7]. MMTk manages large objects (8K or bigger) separately in \na non-copy space, and puts the compiler and a few other system pieces in the boot im\u00adage, an immortal \nspace. We apply colocation to two generational collectors with different mature space policies, and two \ndifferent nursery con.gurations. The .rst collector is a generational copying collector (GenCopy) that \ndivides the heap into two parts, a copying nursery for newly allocated objects, and a mature space that \nis managed using semi\u00adspace collection [5, 36]. A write barrier remembers pointers from the mature space \nto the young space. For every pointer store, the compiler inserts write-barrier code. At execution time, \nit condi\u00adtionally records pointers depending on the collector policy. Gen-Copy collects the nursery when \nit is full (see nursery policy dis\u00adcussion below). It .nds all reachable objects by tracing from the \nroots (stacks, registers, statics, and remembered set) and promot\u00ading survivors into the mature space. \nWe use a variant of depth-.rst copying order that attains good mutator locality [23]. Since the mature \nspace is a semi-space, it must reserve half of its space for copying. The second collector is a generational \ncollector with a mark\u00adsweep mature space (GenMS). The mark-sweep space uses a seg\u00adregated free-list modeled \nafter Lea s allocator [26]. The system collects this space by tracing and marking the live objects using \nbit maps, and lazily .nds free slots during allocation. Tracing is thus proportional to the number of \nlive objects, and reclamation is incremental and proportional to allocation. MMTk uses 51 size classes \nthat attain a worst case internal fragmentation of 1/8. Col\u00adlection of the nursery proceeds in the same \nmanner as GenCopy. Since GenMS need not reserve half the heap for copying, it is more space ef.cient \nthan GenCopy. However, our results con.rm recent work showing that copying collectors produce better \nmutator lo\u00adcality, which outweighs space ef.ciency in some cases [23]. See Blackburn et al. for additional \nMMTk details [6, 7]. We test both GenCopy and GenMS under two nursery con.g\u00adurations: a bounded 4 MB \nnursery and an unbounded Appel [5] nursery. In MMTk, the bounded nursery takes a command line parameter \nas the initial nursery size, collects when the nursery is full, and resizes the nursery below the bound \nonly when the ma\u00adture space cannot accommodate a nursery of survivors. When the nursery size falls below \na lower bound (we use 256KB), it triggers a mature space collection. An Appel nursery uses the same dis\u00adcipline, \nbut with the heap size as the upper bound. Previous work .nds that these two have similar performance, \nbut the Appel con.g\u00aduration is sometimes slightly faster and the bounded 4 MB nursery has lower average \npause times [6]. Colocation is sensitive to the nursery con.guration because it determines which objects \nend up in the mature space and when. For example, with an unbounded nursery the .rst collection only \noccurs after the whole heap has been exhausted, which delays the initiation of colocation. With a 4 MB \nnursery, collection occurs earlier, allowing colocation to start working earlier. For this reason, we \nfocus on the 4 MB bounded nursery. 6.2 IBM Jikes RVM and compiler Jikes RVM (v 2.3.0.1) is a high-performance \nVM written in Java with an aggressive adaptive just-in-time optimizing compiler [2, 3]. We use con.gurations \nthat pre-compile as much as possible, including key libraries and the optimizing compiler itself (the \nFast build-time con.guration), and turn off assertion checking. Our experiments direct the compiler to \noptimize all methods in the application before executing the program and measuring perfor\u00admance. While \nthis strategy is not strictly necessary, it signi.cantly improves the effectiveness of colocation. We \nadded our colocation analysis and instrumentation phase to the sequence of high-level optimizations that \ntake place in SSA form. The compiler analysis goes bottom up on call graph to obtain interprocedural \nsummaries for all methods (see Section 4). The overhead of optimizing the entire application is quite \nhigh, but the fraction of this overhead added by the colocation analysis is only 5% to 10%. By compari\u00adson, \nother pointer analysis and escape analysis, appear to be signif\u00adicantly more costly [10, 15]. In addition \nto the analysis, the more complex allocation routine places a heavier load on the optimizing compiler. \nFor now we view colocation as an ahead-of-time opti\u00admization, which might be suitable for a Java-to-bytecode \ncompiler. 6.3 Experimental Platform We perform all of our experiments on a 3.2 GHz Intel Pentium 4 with \nhyper-threading enabled, with an 8KB 4-way set associative L1 data cache, a 12K\u00b5ops L1 instruction trace \ncache, a 512KB uni\u00ad.ed 8-way set associative L2 on-chip cache, 1GB main memory, and runs Linux 2.6.0. \n  7. RESULTS This section presents our .ndings from applying dynamic coloca\u00adtion to the SPEC JVM98 benchmarks, \nunder the four generational garbage collector con.gurations (see Section 6.1). In this setting, the primary \nbene.t of colocation is to reduce the cost of nursery collections by allocating some objects directly \nin the mature space. We start by describing the potential for colocation: the amount of memory copied \nfrom the nursery by the unmodi.ed collectors. We quantify runtime overhead of coalloc, which is on average \nless than 1%, using a con.guration that includes all colocation instru\u00admentation, but always allocates \nobjects in the nursery. We examine the tradeoff between accuracy and ef.cacy of colocation, .nding that \nour analysis .nds much of the potential while making some, but not many errors. We then present the central \nresult of the paper. We examine the impact of colocation on performance by measuring garbage col\u00adlection \ntime, mutator time, and overall execution time. Colocation substantially reduces copying from the nursery \nwithout overbur\u00addening the mature space. The resulting collection time improve\u00adments translate into total \nexecution time improvements. Coloca\u00adtion is particularly effective on the three benchmarks with high \nnursery survival rates. Finally, we explore the design space of the colocation analysis by showing the \nimpact of turning off various features, including the volatility heuristics and the interprocedural summaries. \n7.1 Potential of colocation Table 2 presents the allocation characteristics of our benchmarks: the total \nallocation in MB (Total) and the amount the collector pro\u00admotes from the nursery to the mature space \n(Copy) in MB and as a percentage. We order programs by their nursery survival rate. These base statistics \nshow that colocation in this generational set\u00adting has the potential to improve pseudojbb, javac, and \ndb since a signi.cant fraction of their nursery objects survive. We omit com\u00adpress because it allocates \nonly 3 MB into the nursery, and thus it never triggers a nursery collection. Table 1 shows the compile-time \nproperties of the benchmarks: the numbers of methods and allocation sites, as well as the spe\u00adci.c colocation \ndecisions the compiler generates. The second col\u00adumn shows the number of methods identi.ed as factory \nmethods. The third column shows the total number of allocation sites. The last three columns show the \nnumber of allocation sites converted to coalloc. In general, the compiler .nds many opportunities for \ncolocation, but these opportunities comprise less than half of all the allocation sites. For factories, \nwe separate out the uses of coallocinside the factory method itself from uses of the factory method in \nthe caller. Methods Allocation sites  Factory Benchmark total Factory Total coalloc inside caller \n     pseudojbb 598 89 1404 120 77 155 javac 919 121 1953 512 274 431 db 192 18 683 83 25 36 mtrt \n322 18 747 138 24 35 jack 430 22 1386 198 35 90 raytrace 324 19 751 138 25 35 jess 605 49 1266 255 159 \n88 Table 1: Compile-time colocation decisions  7.2 Colocation overhead Colocation incurs a small runtime \noverhead that results from the ad\u00additional test on the colocator object in each call to coalloc.We measure \nthis overhead using a version of coalloc that includes the test, but still allocates all objects in the \nnursery. This con.gu\u00adration separates the direct overhead of coalloc from secondary effects of colocation, \nsuch as changes in locality. Figure 9 shows that that the overhead of the additional test is on average \nless than 1%, and thus a negligible consideration.  7.3 Accuracy and ef.cacy of colocation In our current \nimplementation, colocation reduces copying from the nursery by allocating some new objects in the mature \nspace, by\u00adpassing the nursery. This effect, however, only yields a bene.t un\u00adder two conditions: .rst, \ncolocation must select the right objects to place in the mature space, and second, it must do so often \nenough to signi.cantly lower nursery survival rate. These two requirements, which we refer to as accuracy \nand ef.cacy respectively, are com\u00adpeting forces. For example, we could increase ef.cacy arbitrarily by \nallocating most or all new objects in the mature space, but since many of these objects do not belong \nthere the resulting inaccuracy would force many expensive full-heap collections. Similarly, we could \nimprove accuracy by using a more conservative colocation analysis, but the low ef.cacy would yield little \nimprovement in per\u00adformance. The following measurements show that our formulation effectively balances \nthese two requirements. Ideally colocation would always select exactly those objects that would have \nsurvived nursery collection, so that no objects are copied perfect accuracy and ef.cacy. For a number \nof reasons, however, colocation cannot attain this goal. First, some nursery survivors Heap size (MB) \n40 60 80 100 120 140 1.2 are not connected to objects in the mature space but are instead reachable from \nthe stacks and global variables. Second, coloca\u00adtion can only start to place new objects directly in \nthe mature space once some initial set of colocators is already there. Therefore colo\u00adcation requires \na warm up of at least one nursery collection to produce these initial colocators. Third, some allocation \nsites pro\u00adduce objects whose lifetimes are not accurately predicted by their connectivity. In these cases, \nour volatility heuristics conservatively place these objects in the nursery to avoid triggering excess \nfull\u00adheap collections. Finally, even with these heuristics, colocation can mistakenly place objects in \nthe mature space. Figure 10 shows the effects of colocation on the allocation and copying of each benchmark \nas compared to the unmodi.ed collec\u00adtors. To focus on accuracy, we measure these values using specially \ninstrumented collectors con.gured with a 4 MB nursery with an in\u00ad.nite mature space. (Table 2 also presents \nthese raw numbers and adds percentages.) Each bar shows the amount of memory that ends up in the mature \nspace, broken down into two parts: the dark part represents memory copied from the nursery and the light \npart represents memory allocated directly in the mature space. The bar labeled Base shows the behavior \nof the unmodi.ed collections, which allocate all objects in the nursery. We normalize the graph to this \nvalue because it represents the potential for colocation. The bar labeled Coloc shows the result of colocation. \nOur goal is to push down the dark bar (reduce copying) without allowing the total size of the bar (copying \nplus mature space allocation) to signi.cantly exceed the base value. For all but one benchmark, colocation \nreduces copying by 50% to 75%. Colocation is usually accurate as well, increasing mature space allocation \nby 1 to 6% on four programs, but is not accurate on pseudojbb. We discuss the impact of pseudojbb s behavior \non performance below. jess has the fewest nursery survivors, and the smallest reduction. Furthermore, \ncolocation increases mature space promotion usage by 28%, but in absolute terms the amount of memory \n(0.6 MB) is so low that it has little impact on performance.  1 The most signi.cant reductions are for \njavac, pseudojbb, and db 0.9 which are non-trivial applications that allocate large amounts of 1.15 0.8 \nNormalized Mutator Time memory and have high nursery survival rates.  7.4 Write barrier Colocation also \nhas the potential to reduce intergenerational point- GC Time (sec) GC Time (sec) 0.7 1.1 0.6 0.5 1.05 \n0.4 0.3 ers, and therefore reduce the number of write barriers. The last two 1 0.2 columns of Table \n2 show the percent of all writes that the write barrier records in the remembered set (remset). We observe \nthis Heap size relative to minimum heap size secondary reduction for pseudojbb, javac, and db, which \nis not (a) Mean Mutator Time surprising since these are the benchmarks for which colocation is most effective. \nFor jess and jack colocation slightly increases the Heap size (MB) number of remset entries. For raytrace \nand mtrt (which are closely 40 60 80 100 120 140 1 related programs), however, the number of remset \nentries grows 0.9 considerably. As an absolute percentage, the number is still low. 0.8 These could \nbe errors, but Figure 11 shows how colocation can in\u00ad 0.7 0.6 0.5 crease remset entries, while still \nimproving overall performance. If only part of a data structure is colocated in the mature space then \na broad slice of it may span the boundary between spaces. 0.4 0.3  7.5 Performance 0.2 11.5 22.533.544.55 \nHeap size relative to minimum heap size (b) Mean Total Time Figure 9: Colocation overhead (mean over \nall benchmarks): colocation instrumentation has a low overhead. We present the geometric mean for collection, \nmutator, and total time (Figure 12) using a 4 MB bounded nursery, and the individual program results \n(Figures 13, 14, and 15) with and without coloca\u00adtion. Figure 12 shows that colocation consistently reduces \ncollector work in a bounded 4 MB nursery, reducing collection times from 40% to 60% lower in large heaps. \nIn overall time, colocation pro\u00ad Allocation (MB) Write barriers Base Colocation % taken Total Copy % \nSurv % Copy % Surv % Mature space Base Colocation pseudojbb 216 59.8 27.7 23.1 10.7 63.6 5.72 3.51 javac \n185 47.7 25.8 13.8 7.5 34.9 2.70 0.99 db 82 7.7 9.4 4.1 5.0 3.7 1.22 0.17 mtrt 142 6.4 4.5 3.3 2.3 3.2 \n0.07 0.36 jack 231 6.7 2.9 3.6 1.6 4.2 8.19 8.57 raytrace 135 3.2 2.3 0.9 0.7 2.4 0.01 0.33 jess 261 \n2.1 0.8 2.0 0.8 0.7 0.09 0.17 Table 2: Benchmark Characteristics. Copying and colocation are measured \nusing a 4 MB nursery and in.nite mature space. pseudojbb javac db mtrt jack raytrace jess Memory put \nin mature space(as % of base) 150 100 50 0 MB Colocate Copy Figure 10: Colocation reduces copying without \nsigni.cantly increasing mature-space allocation in all but two cases. 1.2 1.25  1 1 1 1 1.5 2 2.5 3 \n3.5 4 4.5 5 Heap size relative to minimum heap size (a) Mean GC Time 1 1.5 2 2.5 3 3.5 4 4.5 5 Heap size \nrelative to minimum heap size (b) Mean Mutator Time 1 1.5 2 2.5 3 3.5 4 4.5 5 Heap size relative to minimum \nheap size (c) Mean Total Time  1.2 1.15 1.1 1.05 1.15 1.1 1.05 Normalized GC Time Figure 12: Colocation \nresults with 4MB nursery: (a) Colocation reduces work for the garbage collector, (b) improves locality \nfor copying mature space, (c) these bene.ts are re.ected in overall execution time. duces reasonable \nimprovements for the GenCopy collector, but not for the GenMS collector. This result is explained by \nthe muta\u00adtor time graph. In a copying mature space, colocation improves locality without signi.cant allocation \noverhead. In a mark-sweep mature space, however, locality is poor and allocation is more ex\u00adpensive \nobjects placed directly in the mature space cannot even bene.t from .eeting nursery locality [23]. This \neffect is likely to hurt the mutator time in any scheme that allocates objects directly in the mature \nspace. Figure 13 reports the reduction in collection time for the indi\u00advidual benchmark programs. These \nresults show four kinds of be\u00adhavior under colocation. First, for javac, raytrace, jack, and mtrt colocation \nchooses the right objects to allocate in the mature space, reducing collection time for both collectors \nand across a range of heap sizes. Second, for jess and jack, our analysis detects the po\u00adtentially high \nmutation rate in the mature space (using the volatility heuristics described in Section 4) and prevents \nincorrect colocation. Without this special case, colocation can cause collection time in jess to grow \nby a factor of three or four. Third, for db colocation chooses the right objects to allocate in the mature \nspace, but the performance improvement is not due to the reduction in garbage collection time, but due \nto the reduction in mutator time. We only see this improvement in GenCopy, which suggests that it is \na result of locality: colocation places critical data structures together in the mature space in allocation \norder. Finally, for pseudojbb colocation places many objects in the mature space that would not have \nsurvived a nursery collection. At larger heap sizes the cost of these incorrect decisions is hidden \npseudojbb even shows a measurable improvement. In small heaps excess allocation in the mature space triggers \nwhole-heap collec\u00adtions more frequently, and degrades performance. Heap size (MB) Heap size (MB) Heap \nsize (MB) 50 100 150 200 250 40 60 80 100 120 140 160 180 40 60 80 100 120 140 7 7 7 1.61.4 1.46 61.2 \n1.25 5 1 1 4 4   Normalized GC TimeNormalized GC Time GC Time (sec) GC Time (sec) Normalized GC TimeNormalized \nGC Time GC Time (sec) GC Time (sec) Normalized GC TimeNormalized GC Time GC Time (sec) GC Time (sec) \n0.8 0.6 0.4 0.8 0.6 3 2 3 2 0.4 1 10.2 1 0.2 0.1 Heap size relative to minimum heap size Heap size relative \nto minimum heap size Heap size relative to minimum heap size (a) pseudojbb GC Time (b) javac GC Time \n(c) db GC Time Heap size (MB) Heap size (MB) Heap size (MB) 20 40 60 80 100120 20 40 60 80 100120 20 \n40 60 80 100120 7 7 7   0.5 0.4 0.3 0.2 6 5 4 3 2 6 1 5 4 0.8 0.6 0.4 3 2 0.1 0.1 0.2 1 1 1 0.05 \nHeap size relative to minimum heap size Heap size relative to minimum heap size Heap size relative to \nminimum heap size (d) mtrt GC Time (e) jack GC Time (f) raytrace GC Time Heap size (MB) 20 30 40 50 60 \n70 80 90 100110  (g) jess GC Time Figure 13: GC Time with and without Colocation, 4 MB bounded nursery. \n 7.6 Design space analysis We explore the analysis design space by turning off components of the interprocedural \nanalysis and volatility heuristics. These experi\u00adments use an unbounded mature space to isolate nursery \nbehavior. The results, shown in Figures 16, 17, and 18, use the same axes as Figure 10 described at the \nbeginning of this section. We also present performance numbers using the unbounded nursery. Figure 16 \nshows the effects of removing the interprocedural com\u00adponents of the analysis. The left bar shows the \nfull colocation algo\u00adrithm (same as Figure 10.) The middle bar shows the results with\u00adout any interprocedural \nsummaries. The effect is most detrimen\u00adtal to the larger benchmarks such as javac, jack, and pseudojbb, \nwhich rely on complex data structures to store their data. The right bar shows the results of excluding \nfactory colocation: this con.gu\u00adration effectively disables colocation for programs that rely heavily \non factories, such as pseudojbb. In fact, pseudojbb allocates al\u00admost all important data structures through \na single factory. Figure 17 shows the effects of turning off the analysis heuris\u00adtics that prune volatile \nreferences out of the connectivity graph. For some benchmarks eliminating the put.eld and cleared-object \ntests actually improves performance these programs contain ref\u00aderences that appear volatile, but are \nin fact stable. However, the heuristics are critical for many of the programs, which would oth\u00aderwise \nrapidly .ll the mature space with garbage. jess and jack,in particular, use container classes to hold \nephemeral objects. Figure 18 shows the effects of using speculative age-based colo\u00adcation (see Section \n5). This feature primarily bene.ts smaller bench\u00admarks, such as db and mtrt, which allocate many long-lived \nobjects in the .rst nursery collection. Speculative colocation allows these programs to put objects in \nthe mature space before the .rst nursery collection. This feature also helps colocation work more effectively \nin an unbounded Appel nursery [5]. Figure 19 shows the geometric means of collector time, mutator time, \nand overall time using an Appel nursery. Colocation is less effective in this nursery con.guration, but \nstill yields 15% to 25% improvement in collection time for GenMS. These improvements occur for the smaller \nheap sizes, where performance improvements are harder to obtain. Unfortunately, colocation degrades locality \nin GenMS, which overwhelms this ben\u00ade.t and results in a net slowdown for the overall runtimes. Heap \nsize (MB) Heap size (MB) Heap size (MB) 50 100 150 200 250 40 60 80 100 120 140 160 180 40 60 80 100 \n120 140 1.2 1.2 7.6 5.41.15 1.157.4 7.2 5.2 1.1   Normalized Mutator TimeNormalized Mutator Time Mutator \nTime (sec) Mutator Time (sec) Normalized Mutator TimeNormalized Mutator Time Mutator Time (sec) Mutator \nTime (sec) Normalized Mutator TimeNormalized Mutator Time Mutator Time (sec) Mutator Time (sec) 1.1 \n7 5 4.8 6.8 1.05 1.05 6.6 1 1 1 116.4  4.6 Heap size relative to minimum heap size Heap size relative \nto minimum heap size Heap size relative to minimum heap size (a) pseudojbb Mutator Time (b) javac Mutator \nTime (c) db Mutator Time Heap size (MB) Heap size (MB) Heap size (MB) 20 40 60 80 100120 20 40 60 80 \n100120 20 40 60 80 100120 1.2 1.2 1.2   2.35 2.3 2.25 2.2 2.15 2.1 2.05 3.3 3.2 3.1 3 2.9 1.15 1.1 \n1.05 1.15 1.1 1.05 2 2.8 1 1 1 1.85  1.95 Heap size relative to minimum heap size Heap size relative \nto minimum heap size Heap size relative to minimum heap size (d) mtrt Mutator Time (e) jack Mutator Time \n(f) raytrace Mutator Time Heap size (MB) 20 30 40 50 60 70 80 90 100110  (g) jess Mutator Time Figure \n14: Mutator time with and without Colocation, 4 MB bounded nursery.  8. CONCLUSION This paper introduces \ndynamic object colocation, a new coopera\u00adtive compiler and runtime optimization to improve the performance \nof generational and other incremental garbage collectors. We demon\u00adstrate a practical compiler analysis \nthat computes object connectiv\u00adity information and passes it to the garbage collector so that con\u00adnected \ndata structures can be colocated in the same garbage col\u00adlection space. Our analysis .nds many opportunities \nfor pro.table colocation and reduces garbage collection time, sometimes dramat\u00adically, on our benchmarks \nfor two generational collectors. These improvements translate to improvements in total execution time \nas well. Colocation makes a unique use of static and dynamic infor\u00admation, and should play well with \nother optimizations to further improve performance. Previous work suggests heap organizations that segregate \nobjects by connectivity, but with the restriction that the objects must never install cross region pointers \n[21, 22]. The success of colocation instead suggests collector organizations that group connected objects \ninto separately collected regions where a write barrier handles cross region pointers.  9. REFERENCES \n[1] O. Agesen and A. Garthwaite. Ef.cient object sampling via weak references. In ACM International Symposium \non Memory Management, pages 121 127, Minneapolis, MN, Oct. 2000. [2] B. Alpern et al. Implementing Jalape \nno in Java. In ACM Conference on Object-Oriented Programming Systems, Languages, and Applications, pages \n314 324, Denver, CO, Nov. 1999. [3] B. Alpern et al. The Jalape no virtual machine. IBM Systems Journal, \n39(1):211 238, February 2000. [4] L. O. Andersen. Program Analysis and Specialization for the C Programming \nLanguage. PhD thesis, DIKU, University of Copenhagen, May 1994. [5] A. W. Appel. Simple generational \ngarbage collection and fast allocation. Software Practice and Experience, 19(2):171 183, 1989. [6] S. \nM. Blackburn, P. Cheng, and K. S. McKinley. Myths and realities: The performance impact of garbage collection. \nIn ACM Conference on Measurement &#38; Modeling Computer Systems, pages 25 36, New York, NY, June 2004. \n Heap size (MB) Heap size (MB) Heap size (MB) 50 100 150 200 250 40 60 80 100 120 140 160 180 40 60 80 \n100 120 140 1.4 9.5 1.4 1.35 1.35 6.5 9 1.3 1.3 14.5 14 8.5 1.25 1.25 6 1.2 1.2   Normalized TimeNormalized \nTime Time (sec) Time (sec) Normalized TimeNormalized Time Time (sec) Time (sec) Normalized TimeNormalized \nTime Time (sec) Time (sec) 8 1.15 1.1 1.15 5.5 7.5 1.1 1.05 1.05 1.05 11.5 7 5 1 1 1 11 6.5 Heap size \nrelative to minimum heap size Heap size relative to minimum heap size Heap size relative to minimum heap \nsize (a) pseudojbb Total Time (b) javac Total Time (c) db Total Time Heap size (MB) Heap size (MB) Heap \nsize (MB) 20 40 60 80 100120 20 40 60 80 100120 20 40 60 80 100120 1.4 1.4 1.4 2.7   2.8 2.7 4 3.8 \n3.6 1.35 1.3 1.25 1.2 1.35 1.3 2.6 1.25 2.5 1.2 2.4 1.15 3.4 1.15 2.3 1.1 1.1 3.2 2.2 1.05 1.05 2.1 3 \n1 1 2 Heap size relative to minimum heap size Heap size relative to minimum heap size Heap size relative \nto minimum heap size (d) mtrt Total Time (e) jack Total Time (f) raytrace Total Time Heap size (MB) 20 \n30 40 50 60 70 80 90 100110  11.5 22.533.544.55 Heap size relative to minimum heap size (g) jess Total \nTime Figure 15: Total Time with and without Colocation, 4 MB bounded nursery. [7] S. M. Blackburn, P. \nCheng, and K. S. McKinley. Oil and water? High performance garbage collection in Java with JMTk. In Proceedings \nof the International Conference on Software Engineering, pages 137 146, Scotland, UK, May 2004. [8] S. \nM. Blackburn, R. E. Jones, K. S. McKinley, and J. E. B. Moss. Beltway: Getting around garbage collection \ngridlock. In ACM Conference on Programming Languages Design and Implementation, pages 153 164, Berlin, \nGermany, June 2002. [9] S. M. Blackburn, S. Singhai, M. Hertz, , K. S. McKinley, and J. E. B. Moss. Pretenuring \nfor Java. In ACM Conference on Object-Oriented Programming Systems, Languages, and Applications, pages \n342 352, Tampa, FL, Oct. 2001. ACM. [10] B. Blanchet. Escape analysis for Java: Theory and practice. \nACM Transactions on Programming Languages and Systems, 25(6):713 775, Nov. 2003. [11] D. R. Chase, M. \nWegman, and F. K. Zadeck. Analysis of pointers and structures. In ACM Conference on Programming Languages \nDesign and Implementation, pages 296 310, White Plains, NY, June 1990. [12] P. Cheng, R. Harper, and \nP. Lee. Generational stack collection and pro.le-driven pretenuring. In ACM Conference on Programming \nLanguages Design and Implementation, pages 162 173, Montreal, Canada, May 1998. [13] T. M. Chilimbi, \nM. D. Hill, and J. R. Larus. Cache-conscious structure layout. In ACM Conference on Programming Languages \nDesign and Implementation, pages 1 12, Atlanta, GA, May 1999. [14] T. M. Chilimbi and J. R. Larus. Using \ngenerational garbage collection to implement cache-conscious data placement. In ACM International Symposium \non Memory Management, pages 37 48, Vancouver, BC, Oct. 1998. [15] J. Choi, M. Gupta, M. J. Serrano, V. \nC. Sreedhar, and S. P. Midkiff. Stack allocation and synchronization optimizations for Java using escape \nanalysis. ACM Transactions on Programming Languages and Systems, 25(6):876 910, Nov. 2003. [16] R. Cytron, \nJ. Ferrante, B. Rosen, M. Wegman, and K. Zadeck. Ef.ciently computing static single assignment form and \nthe control dependence graph. ACM Transactions pseudojbb javac db mtrt jack raytrace jess 150 MB Colocate \n Copy Memory put in mature space (as % of base) 100 50 0  Figure 16: Interprocedural analysis is important, \nparticularly for the larger benchmarks such as javac, jack, and pseudojbb. pseudojbb javac db mtrt jack \nraytrace jess Memory put in mature space (as % of base) 200 150 100 50 0  MB Colocate Copy Figure \n17: Volatility heuristics effectively prevent excessive colocation, particular in jess which otherwise \nputs 150 MB in the mature space. pseudojbb javac db mtrt jack raytrace jess Memory put in mature space \n(as % of base) 150 100 50 0  MB Colocate Copy Figure 18: Speculative age-based-colocation helps the \nsmaller benchmarks such as db and mtrt which need to start colocation before the .rst nursery collection. \non Programming Languages and Systems, 13(4):451 490, pointers. In ACM Conference on Programming Languages \nOct. 1991. Design and Implementation, pages 242 256, June 1994. [17] T. Domani, G. Goldshtein, E. Kolodner, \nE. Lewis, [19] R. Ghiya and L. J. Hendren. Is it a tree, a DAG, or a cyclic E. Petrank, and D. Sheinwald. \nThread-local heaps for Java. graph? A shape analysis for heap-directed pointers in C. In In ACM International \nSymposium on Memory Management, ACM Symposium on the Principles of Programming pages 76 87, Berlin, Germany, \nJune 2002. Languages, St. Petersburg Beach, FL, Jan. 1996. [18] M. Emami, R. Ghiya, and L. J. Hendren. \nContext-sensitive [20] T. L. Harris. Dynamic adaptive pre-tenuring. In ACM interprocedural Points-to \nanalysis in the presence of function International Symposium on Memory Management, pages 1.2 1.25 1.2 \n1.15 1.1 1.15   Normalized GC Time 1.1 1.05 1.05 1.5 1 1 1 Heap size relative to minimum heap size \nHeap size relative to minimum heap size Heap size relative to minimum heap size (a) Mean GC Time (b) \nMean Mutator Time (c) Mean Total Time Figure 19: Colocation results with unbounded nursery: Colocation \nyields less of a bene.t for collection time, and incurs a higher mutator time overhead. Old space Nursery \n 1992. Springer-Verlag. [26] D. Lea. A memory allocator. http://gee.cs.oswego.edu/dl/html/malloc.html, \n1997. [27] H. Lieberman and C. E. Hewitt. A real time garbage collector based on the lifetimes of objects. \nCommunications of the ACM, 26(6):419 429, 1983. [28] F. Qian and L. Hendren. An adaptive, region-based \nallocator for Java. In ACM International Symposium on Memory (a) No colocation Old space Nursery Figure \n11: Colocation can increase remset sizes even when it is working correctly.  Management, Berlin, Germany, \nJune 2002. [29] N. Sachindran and J. E. B. Moss. Mark-Copy: Fast copying GC with less space overhead. \nIn ACM Conference on Object-Oriented Programming Systems, Languages, and Applications, pages 326 343, \nAnahiem, CA, Oct. 2003. [30] Y. Shuf, M. Gupta, R. Bordawekar, and J. P. Singh. Exploiting proli.c types \nfor memory management and optimzations. In ACM Symposium on the Principles of Programming Languages, \npages 295 306, Portland, OR, Jan. (b) With colocation 127 136, Minneapolis, MN, Oct. 2000. [21] M. Hirzel, \nA. Diwan, and M. Hertz. Connectivity-based garbage collection. In ACM Conference on Object-Oriented Programming \nSystems, Languages, and Applications, pages 359 373, Anaheim, CA, Oct. 2003. [22] M. Hirzel, J. Hinkel, \nA. Diwan, and M. Hind. Understanding the connectivity of heap objects. In ACM International Symposium \non Memory Management, pages 36 49, Berlin, Germany, June 2002. [23] X. Huang, Z. Wang, S. M. Blackburn, \nK. S. McKinley, J. E. B. Moss, and P. Cheng. The garbage collection advantage: Improving mutator locality. \nIn ACM Conference on Object-Oriented Programming Systems, Languages, and Applications, Vancouver, BC, \n2004. To appear. [24] R. L. Hudson and J. E. B. Moss. Incremental garbage collection for mature objects. \nIn Y. Bekkers and J. Cohen, editors, International Workshop on Memory Management, St. Malo, France, volume \n637 of Lecture Notes in Computer Science. Springer-Verlag, 1992. [25] M. S. Lam, P. R. Wilson, and T. \nG. Moher. Object type directed garbage collection to improve locality. In Y. Bekkers and J. Cohen, editors, \nACM International Workshop on Memory Management, number 637 in Lecture Notes in Computer Science, pages \n404 425, St. Malo, France, Sept. 2002. [31] Standard Performance Evaluation Corporation. SPECjvm98 Documentation, \nrelease 1.03 edition, March 1999. [32] Standard Performance Evaluation Corporation. SPECjbb2000 (Java \nBusiness Benchmark) Documentation, release 1.01 edition, 2001. [33] D. Stefanovi\u00b4c, K. McKinley, and \nJ. Moss. Age-based garbage collection. In ACM Conference on Object-Oriented Programming Systems, Languages, \nand Applications, pages 370 381, Denver, CO, Nov. 1999. [34] D. Ungar and F. Jackson. Tenuring policies \nfor generation-based storage reclamation. In ACM Conference on Object-Oriented Programming Systems, Languages, \nand Applications, pages 1 17, San Diego, California, Nov. 1988. [35] D. Ungar and F. Jackson. An adaptive \ntenuring policy for generation scavengers. ACM Transactions on Programming Languages and Systems, 14(1):1 \n27, 1992. [36] D. M. Ungar. Generation scavenging: A non-disruptive high performance storage reclamation \nalgorithm. In ACM Software Engineering Symposium on Practical Software Development Environments, pages \n157 167, April 1984. [37] P. R. Wilson, M. S. Lam, and T. G. Moher. Effective static-graph reorganization \nto improve locality in garbage-collected systems. In ACM SIGPLAN Conference on Programming Languages \nDesign and Implementation, pages 177 191, Toronto, Canada, June 1991.   \n\t\t\t", "proc_id": "1028976", "abstract": "<p>This paper introduces &#60;i>dynamic&#60;/i> object colocation, an optimization to reduce copying costs in generational and other incremental garbage collectors by allocating connected objects together in the same space. Previous work indicates that connected objects belong together because they often have similar lifetimes. Generational collectors, however, allocate all new objects in a &#60;i>nursery&#60;/i> space. If these objects are connected to data structures residing in the &#60;i>mature&#60;/i> space, the collector must copy them. Our solution is a cooperative optimization that exploits compiler analysis to make runtime allocation decisions. The compiler analysis discovers potential object connectivity for newly allocated objects. It then replaces these allocations with calls to &#60;i>coalloc&#60;/i>, which takes an extra parameter called the &#60;i>colocator&#60;/i> object. At runtime, coalloc determines the location of the colocator and allocates the new object together with it in either the nursery or mature space. Unlike pretenuring, colocation makes precise per-object allocation decisions and does not require lifetime analysis or allocation site homogeneity. Experimental results for SPEC Java benchmarks using Jikes RVM show colocation can reduce garbage collection time by 50% to 75%, and total performance by up to 1%.</p>", "authors": [{"name": "Samuel Z. Guyer", "author_profile_id": "81332502517", "affiliation": "The University of Texas at Austin, Austin, TX", "person_id": "PP42049875", "email_address": "", "orcid_id": ""}, {"name": "Kathryn S. McKinley", "author_profile_id": "81100402805", "affiliation": "The University of Texas at Austin, Austin, TX", "person_id": "P157900", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1028976.1028996", "year": "2004", "article_id": "1028996", "conference": "OOPSLA", "title": "Finding your cronies: static analysis for dynamic object colocation", "url": "http://dl.acm.org/citation.cfm?id=1028996"}