{"article_publication_date": "10-01-2004", "fulltext": "\n MC2: High-Performance Garbage Collection for Memory-Constrained Environments Narendran Sachindran J. \nEliot B. Moss Emery D. Berger Department of Computer Science University of Massachusetts Amherst, MA \n01003, USA {naren, moss, emery}@cs.umass.edu ABSTRACT Java is becoming an important platform for memory-constrained \ncon\u00adsumer devices such as PDAs and cellular phones, because it provides safety and portability. Since \nJava uses garbage collection, ef.cient garbage collectors that run in constrained memory are essential. \nTyp\u00adical collection techniques used on these devices are mark-sweep and mark-compact. Mark-sweep collectors \ncan provide good throughput and pause times but suffer from fragmentation. Mark-compact col\u00adlectors prevent \nfragmentation, have low space overheads, and provide good throughput. However, they can suffer from long \npause times. Copying collectors can provide higher throughput than either of these techniques, but because \nof their high space overhead, they pre\u00adviously were unsuitable for memory-constrained devices. This pa\u00adper \npresents MC2 (Memory-Constrained Copying), a copying gener\u00adational garbage collector that meets the needs \nof memory-constrained devices with soft real-time requirements. MC2 has low space over\u00adhead and tight \nspace bounds, prevents fragmentation, provides good throughput, and yields short pause times. These qualities \nmake MC2 attractive for other environments, including desktops and servers.  Categories and Subject \nDescriptors D.3.4 [Programming Languages]: Processors Memory manage\u00adment (garbage collection)  General \nTerms Algorithms, Design, Experimentation, Performance  Keywords Java, copying collector, generational \ncollector, mark-sweep, mark\u00adcompact, mark-copy, memory-constrained copying 1. INTRODUCTION Handheld consumer \ndevices such as cellular phones and PDAs are becoming increasingly popular. These devices tend to have \nlimited amounts of memory. They also run on a tight power budget, and Permission to make digital or hard \ncopies of all or part of this work for personal or classroom use is granted without fee provided that \ncopies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. OOPSLA 04, Oct. 24-28, 2004, Vancouver, British \nColumbia, Canada. Copyright 2004 ACM 1-58113-831-8/04/0010 ...$5.00. the memory subsystem consumes a \nconsiderable fraction of the total power. As a result, it is extremely important to be able to run applica\u00adtions \nin constrained memory, and to minimize memory consumption during execution. Java is becoming a popular \nplatform for these handheld devices. It allows developers to focus on writing applications, without having \nto be concerned with the diverse and rapidly evolving hardware and op\u00aderating systems of these devices. \nSince Java uses garbage collection to manage memory, ef.cient garbage collection in constrained memory \nhas become a necessity. While a small memory footprint is an essential requirement, ap\u00adplications in \nthe handheld space make other demands on the garbage collector. Cellular phones now contain built-in \ndigital cameras, and run multimedia applications such as games and streaming audio and video. PDAs run \nscaled-down versions of desktop applications such as web browsers and e-mail clients. While all these \napplications re\u00adquire good throughput from the garbage collector, many interactive and communications \napplications also require the collector to have short pause times. For instance, cellular phones need \nto code, de\u00adcode, and transmit voice data continuously without delay or distortion. Hence, in order to \nmeet the demands of applications on handheld de\u00advices, modern garbage collectors must be able to satisfy \nall three of these requirements: bounded and low space overhead; good through\u00adput; and reliably short \npause times. Java implementations on handheld devices [23, 29] typically use mark-sweep (MS) [22, 25], \nmark-(sweep)-compact (MSC) [13], or generational variants of these collectors [20, 30] to manage dynami\u00adcally \nallocated memory. MS collectors can provide excellent through\u00adput, and they can be made fully incremental \n(provide short pause times consistently). However, they suffer from fragmentation, which affects both \nspace utilization and locality. MS collectors typically need to use additional compaction techniques \nto lower the impact of these prob\u00adlems. MSC collectors can provide good throughput and their space utilization \nis excellent. However, they tend to have long pauses mak\u00ading them unsuitable for a range of applications \nthat require good re\u00adsponse times. We present in this paper a memory-constrained copying collec\u00adtor, \nMC2, that addresses the problems that the above collectors face. MC2 provides good throughput and short \npause times with low space overheads. The collector is based on the mark-copy collection tech\u00adnique [24]. \nMC2 runs in bounded space, thus making it suitable for devices with constrained memory. Since the collector \nregularly copies data, it prevents fragmentation, minimizes memory requirements, and yields good program \nlocality. The collector also limits the amount of data copied in every collection, thus obtaining short \npause times. We organize the remainder of this paper as follows. We .rst de\u00adscribe related work in Section \n2. In Section 3, we describe the ba\u00adsic mark-copy technique and its limitations. We explain in Section \n4 how the new collector overcomes these limitations, bounding space utilization and providing short pause \ntimes. To explore the space of collectors appropriate for memory-constrained environments, we built a \ngenerational mark-compact collector. We describe the implementa\u00adtion of this collector, a generational \nmark-sweep collector, and MC2 in Section 5. We then compare the throughput and pause time charac\u00adteristics \nacross the collectors and a range of benchmarks in Section 6, and conclude in Section 7. 2. RELATED \nWORK We classify related work according to collector algorithm, discussing in turn related work in mark-sweep, \nmark-(sweep)-compact, copying, and generational collection. 2.1 Mark-Sweep A number of incremental collection \ntechniques use mark-sweep col\u00adlection. Examples of collectors in this category (which in fact run con\u00adcurrently \nwith the application) are the collector by Dijkstra et al. [14] and Yuasa s collector [32]. The problem \nwith mark-sweep collectors is that they suffer from fragmentation. Johnstone and Wilson [18] show that \nfragmentation is not a problem for carefully designed allo\u00adcators, for a range of C and C++ benchmarks. \nHowever, they do not demonstrate their results for long running systems, and our experience indicates \nthat this property is not necessarily true with Java programs. Fragmentation makes purely mark-sweep \ncollectors unsuitable for de\u00advices with constrained memory. Researchers and implementors have also proposed \nmark-sweep col\u00adlectors that use copying or compaction techniques to combat fragmen\u00adtation. Ben-Yitzhak \net al. [7] describe a scheme that incrementally compacts small regions of the heap via copying. However, \nthey re\u00adquire additional space during compaction to store remembered set en\u00adtries, and do not address \nthe problem of large remembered sets. Fur\u00adther, for a heap containing n regions, they require n marking \npasses over the heap in order to compact it completely. This can lead to poor performance when the heap \nis highly fragmented. In order to com\u00adpact the heap completely our collector requires only a single round \nof marking. The only collector we are aware of that meets all the requirements we lay out for handheld \ndevices is the real-time collector implemented by Bacon et al. [5]. They use mark-sweep collection and \ncompact the heap using an incremental copying technique that relies on a read bar\u00adrier. They demonstrate \ngood throughput and utilization in constrained memory. However, in order to make their read barrier ef.cient, \nthey require advanced compiler optimization techniques. Our collector does not require compiler support \nbeyond that generally available, such as support for write barriers, and therefore is simpler to imple\u00adment, \nespecially in the absence of a signi.cant compiler optimization infrastructure. 2.2 Mark-Compact Mark-(sweep)-compact \n(MSC) collectors [13, 15, 19, 21] use bump pointer allocation, and compact data during every collection. \nThey prevent fragmentation and typically preserve the order of allocation of objects, thus yielding good \nlocality. Compaction typically requires two or more passes over the heap. However, since these heap traver\u00adsals \nexhibit good locality of reference, they are ef.cient. MSC collec\u00adtors can provide good throughput and \ntheir space utilization is excel\u00adlent. They run ef.ciently in very small heaps. However, they tend to \nhave long pauses. MC2 is similar in many ways to MSC collection, but because its copying is incremental \nit gains the added bene.t of shorter pauses. 2.3 Copying Purely copying techniques also have incremental \nversions. The most well-known of these are Baker s collector [6], Brooks s collector [10], and the Train \ncollector [16]. Baker s and Brooks s techniques use semi-space copying and hence have a minimum space \nrequirement equal to twice the live data size of a program. Also, they use a read barrier, which is not \nvery ef.cient. The Train collector can run with very low space overheads. It can suffer from large remembered \nsets, though there are proposals on limiting that space overhead. However, our experiments with the Train \ncollector show that it tends to copy large amounts of data, especially when programs have large, cyclic \nstructures. In order to obtain good throughput, we found that the col\u00adlector typically requires space \noverheads of a factor of 3 or higher. We conclude that the Train algorithm is not well-suited to memory\u00adconstrained \nenvironments. 2.4 Generational collection Generational collectors divide the heap into multiple regions \ncalled generations. Generations segregate objects in the heap by age. A two-generation collector divides \nthe heap into two regions, an allo\u00adcation region called the nursery, and a promotion region called the \nold generation. Generational collectors trigger a collection every time the nursery .lls up. During a \nnursery collection, they copy reachable nursery objects into the old generation. When the space in the \nold generation .lls up, they perform a full collection and collect objects in the old generation. Generational \ncollection is based on the hypothesis that, for many applications, a large fraction of objects have very \nshort lifetimes, while a small fraction live much longer. The frequent nursery collections weed out the \nshort-lived objects, and less frequent older generation collections reclaim the space occupied by dead \nlong-lived objects. While generational collectors can provide good throughput and short average pause \ntimes, the limitations of the collection technique used in the old generation (namely, fragmentation, \nminimum space over\u00adhead being twice the maximum live size, and large maximum pause time) determine the \noverall space requirements and pause time char\u00adacteristics of the collector. The drawbacks of MS, copying, \nand MSC therefore carry over to generational collection.  3. BACKGROUND We introduced the basic mark-copy \nalgorithm, MC, in a previous pa\u00adper [24]. In this section we summarize MC and describe its limita\u00adtions \nwith respect to the requirements of memory-constrained envi\u00adronments. 3.1 Mark-Copy MC extends generational \ncopying collection. It divides the heap into two regions, a nursery, and an old generation. The nursery \nis identical to a generational copying collector s nursery. MC further divides the old generation into \na number of equal size subregions called windows. Each window corresponds to the smallest increment that \ncan be copied in the old generation. The windows are numbered from 1 to n, with lower numbered windows \ncollected before higher numbered windows. MC performs all allocation in the nursery, and promotes nursery \nsurvivors into the old generation. When the old generation becomes full (only one window remains empty), \nMC performs a full heap col\u00adlection in two phases: a mark phase followed by a copy phase. During the \nmark phase, MC traverses live objects in the heap, and marks them as reachable. It also performs two \nother tasks while mark\u00ading. First, it counts the total volume of live data in each old generation window. \nSecond, it constructs remembered sets for each of the win\u00addows. The remembered sets are unidirectional: \nthey record slots in higher numbered windows that point to objects in lower numbered Root1 Root2   \nLow Region High Region Low Region High Region Old Generation Nursery Old Generation Nursery  (a) Heap \nlayout before a nursery collection (b) Heap layout after a nursery collection   Low Region High Region \nLow Region High Region Old Generation Nursery Old Generation Nursery (c) Heap layout after a full heap \nmark (d) Heap layout after one copy pass Root1 Root2 Root1, Root2 - stack/static roots  Old Generation \nNursery (e) Heap layout after two copy passes Figure 1: Mark-Copy: An example of a nursery collection \nfollowed by a full collection windows. The requirement is to record pointers whose target may be copied \n(moved) before the source. An implication of using uni\u00addirectional remembered sets is that the collector \ncannot change the order of collection of windows once the mark phase starts. While MC can overcome this \nlimitation by using bidirectional remembered sets (recording all cross-window pointers), this is not \ngenerally desirable since bidirectional sets tend to occupy much more space. Once the mark phase completes, \nthe collector performs the copy phase. The copy phase is broken down into a number of passes. Each pass \ncopies data out of a subset of the windows in the old generation. Since the collector knows the exact \namount of live data in each win\u00addow, and the total amount of free space in the heap, it can copy data \nout of multiple windows in each pass. After each copy pass, MC un\u00admaps the pages occupied by the windows \nevacuated in that pass, thus limiting the total virtual memory mapped at any time during the col\u00adlection. \nAfter .nishing all copy passes, the collector resumes nursery allocation and collection. Figure 1 offers \nan example of a nursery collection followed by a full collection using MC. For this example, we assume \nthat all objects allocated in the heap are the same size, and that the heap can accom\u00admodate at most \n10 objects. The heap consists of an old generation with 5 windows. Each of these windows can hold exactly \n2 objects. Root1 and Root2 are root (stack/static) pointers. Figure 1(a) shows a nursery collection, \nwhich results in objects G and H being copied into the old generation. (G is copied because it is reachable \nfrom a root, and H is copied because it is reachable from an object (E) in the old generation.) At this \npoint, the old generation is full (Figure 1(b)). MC then performs a full heap mark and .nds objects B, \nC, D, and G to be live. During the mark phase it builds a unidirectional remembered set for each window. \nAfter the mark phase (Figure 1(c)), the remembered set for the .rst window contains a single entry (D.B). \nAll other re\u00admembered sets are empty, since there are no pointers into the windows from live objects \nin higher numbered windows. In the .rst copy pass, there is enough space to copy two objects. Since the \n.rst window contains one live object (B) and the second window contains two live objects (C, D), MC can \ncopy only the .rst window in this pass. It copies B to the next free window and then unmaps the space \noccu\u00adpied by the .rst window (Figure 1(d)). It also adds a remembered set entry to the second window \nto record the pointer from B to D (since B is now in a higher-numbered window than D, and B needs to \nbe updated when D is moved). The old generation now contains enough free space to copy 3 objects. In \nthe next copying pass, MC copies the other 3 live objects and then frees up the space occupied by windows \n2, 3, and 4 (Figure 1(e)).  3.2 Limitations of Mark-Copy MC suffers from several limitations. First, \nit maps and unmaps pages in windows while performing the copying phase. It thus depends on the presence \nof virtual memory hardware, which may not always be available on handheld devices. Second, the collector \nalways copies all live data in the old generation. This is clearly not ef.cient when the old generation \ncontains large amounts of long-lived data. Third, and perhaps most signi.cantly for many applications, \nthe marking and copying phases can be time-consuming, leading to large pauses. The MC paper [24] describes \ntechniques to make the collector incremen\u00adtal, but demonstrates only incremental copying. MC2 uses the \nsame basic algorithms, but makes several enhancements. Finally, although the collector usually has low \nspace overheads, it occasionally suffers from large remembered sets. In the worst case, the remembered \nset size can grow to be as large as the heap. The original paper describes a technique that can be used \nto bound space overhead by using an ex\u00adtra word per object. However, it is not possible to make that \ntechnique incremental.  4. MC2 The new collector, memory-constrained copying (MC2) uses the basic MC \ntechnique to partition the heap: there is a nursery, and an old generation divided into a number of windows. \nA full collection marks live objects in the heap, followed by a copy phase that copies and compacts live \ndata. However, MC2 overcomes the cited limitations of MC. We describe below a series of features of the \ncollector that allow it to obtain high throughput and low pause times in bounded space. 4.1 Old generation \nlayout As previously stated, MC2 divides the heap into equal size windows. It requires that the address \nspace within each window be contiguous. However, the windows themselves need not occupy contiguous mem\u00adory \nlocations: MC2 maintains a free list of windows and assigns a new window to the old generation whenever \na previously-assigned window cannot accommodate an object about to be copied. Unlike MC, which uses object \naddresses to determine the relative location of objects in the heap, MC2 uses indirect addressing to \ndetermine this information in order to decouple actual addresses from logical window numbers. It uses \na byte array, indexed by high order bits of addresses, to indicate the logical window number for each \nwindow. While this indirection adds a small cost to the mark phase, it has several advantages. First, \nit removes the need to map and unmap memory every time MC2 evacuates a window, in order to maintain MC2 \ns space bound. Second, the indirection removes the need to copy data out of every window; we can assign \nthe window a new logical number and it will be as if the data had been copied. This is important for \nprograms with large amounts of long-lived data. Third, it allows the collector to change the order of \ncollection of windows across multiple collections. We describe the details of these features in Section \n4.3. MC2 also differentiates between physical windows and collection windows. It divides the heap into \na number of small windows (typi\u00adcally 100) called physical windows. MC2 maintains remembered sets for \neach of these windows. A collection window de.nes the maxi\u00admum amount of live data that MC2 normally \ncollects during a copying increment. Collection windows are usually larger than physical win\u00addows. This \nscheme allows MC2 to occasionally copy smaller amounts of data (e.g. when a physical window contains \nhighly referenced ob\u00adjects). In the following sections we use the term window to refer to a physical \nwindow. A B (a) (b) (c) Figure 2: Example of an error during incremental marking 4.2 Incremental \nmarking MC marks the heap when the free space drops to a single window. While this gives good throughput, \nit tends to be disruptive: when MC performs a full collection, the pause caused by marking can be long. \nIn order to minimize the pauses caused by marking, MC2 triggers the mark phase sooner than MC, and spreads \nout the marking work by interleaving it with nursery allocation. After every nursery collection, MC2 \nchecks the occupancy of the old generation. If the occupancy exceeds a prede.ned threshold (typ\u00adically \n80%), MC2 triggers a full collection and starts the mark phase. It .rst associates a bit map with each \nwindow currently in the old gen\u00aderation (the collector has previously reserved space on the heap for \nthe bit maps). Marking uses these bit maps to mark reachable objects, and to check if an object has already \nbeen marked. Apart from the bene.t to marking locality, the bit maps also serve other purposes, described \nin Section 4.3. MC2 then assigns logical addresses to each of the windows. Mark\u00ading uses these addresses \nto determine the relative positions of objects in the old generation. MC2 marks data only in windows \nthat are in the old generation when a full collection is triggered. It considers any data promoted into \nthe old generation during the mark phase to be live, and collects it only in the next full collection. \nAfter MC2 assigns addresses to the windows, it cannot alter the order in which they will be collected \nfor the duration of the current collection. Finally, MC2 allocates a new window in the old generation, \ninto which it performs subsequent allocation. After triggering the mark phase, MC2 allows nursery allocation \nto resume. Every time a 32KB block in the nursery .lls up, MC2 marks a small portion of the heap.1 In \norder to compute the volume of data that needs to be marked in each mark increment, MC2 maintains an \naverage of the nursery survival rate (NSR). It computes the mark in\u00adcrement size using the following \nformulae: 1We chose 32KB as a good compromise in terms of interrupting mu\u00adtator work and allocation often \nenough, but not too often. This value determines the incrementality of marking. numMarkIncrements = availSpace/(NSR \n* nurserySize) markIncrementSize = totalBytesToMark/numMarkIncrements totalBytesToMark = totalBytesToMark \n- markIncrementSize MC2 initializes totalBytesToMark to the sum of the size of the win\u00addows being marked, \nbecause in the worst case all the data in the win\u00addows may be live. If the heap occupancy reaches a prede.ned \ncopy threshold (typically 95% occupancy) during the mark phase, MC2 will perform all remaining marking \nwork without allowing further alloca\u00adtion. MC2 maintains the state of marking in a work queue, speci.cally \na list of the objects marked but not yet scanned. When it succeeds in emptying that list, marking is \ncomplete. Write barrier A problem with incremental marking is that the mutator modi.es ob\u00adjects in the \nheap while the collector is marking them. This can lead to a situation where the collector does not mark \nan object that is actually reachable. Using the tri-color invariant [14], we can classify each ob\u00adject \nin the heap as white (unmarked), gray (marked but not scanned), or black (marked and scanned). The problem \narises when the mutator changes a black object to refer to a white object, destroys the original pointer \nto the white object, and no other pointer to the white object exists. Figure 2 shows an example illustrating \nthe problem. In Figure 2(a), the collector has marked and scanned objects A and C, and it has colored \nthem black. The collector has not yet reached objects B and D. At this point, the program swaps the pointers \nin A and B (Figure 2(b)). When the collector resumes marking, it marks B (Figure 2(c)). Since B points \nto C, and the collector has already processed C, the collector wrongly concludes that the mark phase \nis complete, although it has not marked a reachable object (D). Two techniques exist to handle this problem, \ntermed by Wilson [31] as snapshot-at-the-beginning and incremental update. Snapshot col\u00adlectors [32] \ntend to be more conservative. They consider any object that is live at the start of the mark phase to \nbe live for the dura\u00adtion of the collection. They collect objects that die during the mark phase only \nin a subsequent collection. Whenever a pointer is over\u00adwritten, a snapshot collector records the original \ntarget and marks it gray, thus ensuring that all reachable objects are marked. Incremental update collectors \n[14, 28] are less conservative than snapshot collec\u00adtors. When the mutator creates a black-to-white pointer, \nthey record either the source or the new target of the mutation. Recording the source causes the collector \nto rescan the black object, while recording the new target causes the white object to be grayed, thus \nmaking it reachable. Figure 3 shows pseudo-code for the write barrier that MC2 uses. The write barrier \nserves two purposes. First, it records pointer stores writeBarrier(srcObject, srcSlot, tgtObject){ if \n(srcObject not in nursery) { if (tgtObject in nursery) record srcSlot in nursery remset else if (tgtObject \nin old generation) { if (srcObject is not mutated) { set mutated bit in srcObject header record srcObject \nin mutated object list } } } } Figure 3: MC2 write barrier that point from outside the nursery to objects \nwithin the nursery (in order to be able to locate live nursery objects during a nursery col\u00adlection). \nSecond, it uses an incremental update technique to record mutations to objects in the old generation. \nWhen an object muta\u00adtion occurs, and the target is an old generation object, the write bar\u00adrier checks \nif the source object is already recorded as mutated. If so, MC2 ignores the pointer store. If not, it \nrecords the object as mutated. When MC2 performs a mark increment, it .rst processes the mutated objects, \npossibly adding additional objects to the list of those need\u00ading to be scanned. After processing the \nmutated objects, it resumes regular marking.   4.3 Incremental copying When MC performs a full collection, \nit copies data out of all win\u00addows, without allowing any allocation in between. While this is good for \nthroughput, the pause caused by copying can be long. MC2 over\u00adcomes this problem by spreading the copying \nwork over multiple nurs\u00adery collections. (The MC paper [24] described and offered prelim\u00adinary results \nfor a version of incremental copying. It did not offer incremental marking, the bounded space guarantee, \nor the short pause times of MC2.) High occupancy windows At the end of the mark phase, MC2 knows the \nvolume of data marked in each window. At the start of the copy phase, MC2 uses this informa\u00adtion to classify \nthe windows. MC2 uses a mostly-copying technique. It classi.es any window that has a large volume of \nmarked data (e.g., > 98%) as a high occupancy window, and does not copy data out of the window. Once \nMC2 classi.es a window as high occupancy, it discards the remembered set for the window, since no slots \npointing to that window will be updated in the current collection. Copying data After MC2 identi.es the \nhigh occupancy windows, it resumes nursery allocation. At every subsequent nursery collection, it piggybacks \nthe processing of one old generation group.MC2 groups windows based on the amount of data they contain. \nEach group consists of one or more old generation windows, with the condition that the total amount of \nmarked data in a group is less than or equal to the size of a collection window. Since MC2 scans high-occupancy \nwindows sequentially, and the processing does not involve copying and updating slots, it treats a high-occupancy \nwindow as equivalent to copying and updating slots forhalfawindow oflivedata. MC2 also allows one to \nspecify a limit on the total number of remembered set entries in a group. If the addition of a window \nto a group causes the group remembered set size to exceed the limit, MC2 places the window in the next \ngroup. In order to pace old generation collection, MC2 uses information it has about the total space \nthat will be reclaimed by the copy phase. The target for the copy phase is to reduce the heap occupancy \nbelow the mark phase threshold. To achieve this goal, MC2 resizes the nursery at every nursery collection, \nbased on the average survival rate of the nursery and the space that will be reclaimed by compacting \nthe old generation data. When MC2 processes an old generation group, it .rst checks if the group contains \nhigh occupancy windows. If so, MC2 uses the mark bit map for the windows to locate marked objects within \nthem. It scans these objects to .nd slots that point into windows that have not yet been copied, and \nadds the slots to remembered sets for those windows. It then logically moves the windows into to-space. \nIn subsequent col\u00adlections, MC2 places these high occupancy windows at the end of the list of windows \nto be collected. If they still contain large volumes of live data, they do not even have to be scanned, \nand the copy phase can  (a) Start of copy phase (b) After .rst copy pass (c) After second copy pass \n(d) Third copy pass after copying (e) After third copy pass Figure 4: Stages in the copy phase for \nMC2  terminate when it reaches these windows. This technique turns out to be especially helpful for \nSPECjvm98 benchmarks such as db and pseudojbb, which allocate large amounts of permanent data. If a window \ngroup contains objects that need to be copied, MC2 locates marked objects in the window by scanning the \nroot set and re\u00admembered set entries. It copies these objects into free windows using a regular Cheney \nscan [11]. While scanning to-space objects, MC2 adds slots that reference objects in uncopied windows \nto the corre\u00adsponding remembered set. Write barrier As in the mark phase, the write barrier keeps track \nof mutations to old generation objects during the copy phase. It records mutated objects at most once. \nAt every copy phase collection, MC2 updates mutated slots that reference objects in windows being copied, \nand adds mu\u00adtated slots that reference objects in uncopied windows to the corre\u00adsponding remembered sets. \nFigure 4 shows an example of the copy phase of the collector. The example assumes that the physical window \nand collection window sizes are identical i.e. MC2 will copy at most one physical window worth of data \nin each pass. Figure 4(a) shows the old generation lay\u00adout before the copy phase starts. The old generation \ncontains four marked windows (W0 W3). MC2 classi.es W1 and W3 as high oc\u00adcupancy since they contain 100% \nand 98% marked data respectively and places them in separate groups. It also places W0 and W2 in sep\u00adarate \ngroups. In the .rst copy pass (Figure 4(b)), MC2 copies data from the nursery and W0 into W4. It then \nadds W0 to the list of free windows. In the second pass (Figure 4(c)), MC2 scans objects in W1 and adds \nW1 to the end of to-space. It copies nursery survivors into W4 and W0. In the third pass (Figure 4(d)), \nMC2 copies objects out of the nursery and W2 into windows W0 and W5. It then adds W2 to the list of free \nwindows. At this point, the only remaining window, W3, is high occupancy, so MC2 adds it to the end of \nto-space and ends the copy phase. 4.4 Bounding space overhead The remembered sets created during MC collection \nare typically small (less than 5% of the heap space). However, they occasionally grow to be large. There \nare two typical causes for such large remembered sets: popular objects (objects heavily referenced in \na program), and large arrays of pointers. MC2 uses two strategies to reduce the remembered set overhead, \nboth of which involve coarsening remembered set en\u00adtries. Large remembered sets MC2 sets a limit on the \ntotal amount of space that remembered sets can occupy. When the space overhead reaches the limit, it \ncoarsens remembered sets starting from the largest, until the space overhead is below the limit. Also, \nwhen the size of a single remembered set exceeds a prede.ned limit, MC2 coarsens that particular remembered \nset. This coarsening involves converting the remembered set represen\u00adtation from a sequential store buffer \n(SSB) to a card table. (Our usual representation for a remembered set for a window W is a sequential \nlist of addresses of slots containing pointers into W. Whenever we de\u00adtect a reference into W that needs \nrecording, we simply add it to the end of this list. The list may contain duplicates as well as stale \nentries (slots that used to point into W but no longer do). The physical repre\u00adsentation of the list \nis a chain of .xed sized chunks of memory, where each chunk is an array of slot addresses.) We use the \nscheme described by Azagury et al. [3] to manage card tables. While they describe the scheme for the \nTrain collector, it works well with MC2.MC2 divides every window into cards of size 128 bytes. For every \nwindow, it creates a card table that contains a byte for each card in the window. The byte corresponding \nto a card stores the logical address of the .rst window containing an object that is referenced from \nthe card. The collector also maintains a window level summary table (that stores the lowest logical address \ncontained in each window s card table) . When it is time to collect a target window TW whose remembered \nset is a card table the collector proceeds as follows. It .rst scans the summary table to identify windows \nthat contain references into TW . For each source window SW that contains a reference into TW ,the collector \nscans the card table of SW to .nd cards within SW that con\u00adtain references into TW . It then scans every \nobject in each of these cards, to .nd the exact slots that point into TW . If a particular slot does \nnot point into TW , and it points to a window whose remembered set is a card table, MC2 records the window \nnumber of the slot ref\u00aderence. It uses this information to update the card table with a new value at \nthe end of the scan. The process of converting the remembered set representation for a window TW is straightforward. \nMC2 scans the SSB sequentially, and for every recorded slot, it checks if the contents still refer to \nan object in TW . If so, it .nds the source window and card corresponding to the object that contains \nthe slot. If the current entry for the source window card is larger than the logical address of TW ,MC2 \noverwrites the entry with the lower logical address. With a card size of 128 bytes, the size of the card \ntable is about 0.78% of the total heap space. MC2 ensures that the total sum of the space taken by the \nSSB remembered sets and the card table does not exceed a speci.ed limit. For example, if the limit is \nset at 5% of the total space, MC2 starts coarsening remembered sets when their occupancy reaches 4.2% \nof the total space. (Another possibility is to use only a card table, and not use SSBs. We brie.y compare \nthe performance of MC2 with both remembering schemes in the results section.) The bounded space overhead \ncan come at a run-time cost. Setting a byte in a card table is more expensive than inserting into a sequen\u00adtial \nstore buffer. Also, scanning a card table and objects in a card to identify slots that point into a target \nwindow is more expensive than simply traversing a buffer sequentially.2 However, large remembered 2While \nour platform (Jikes RVM) uses an object format that precludes sets are relatively rare, and when MC2 \ncreates a large remembered set, we use a technique described below to prevent the situation from recurring. \nPopular objects Very often, large remembered sets arise because of a few very highly referenced objects. \nFor example, in javac, occasionally a large re\u00admembered set occurs when a small number of objects (representing \nbasic types and identi.ers of Java) appear in the same window. These objects can account for over 90% \nof all references in a remembered set of size 600KB (about 4.5% of the live data size). MC2 identi.es \npopular objects during the process of converting an SSB to a card ta\u00adble. While performing the conversion, \nit uses a byte array to maintain a count of the number of references to each object in the window. (Since \nthe collector always reserves at least one window worth of free space, there is always enough space for \nthe byte array without ex\u00adceeding our space budget.) As MC2 scans the SSB, it calculates the offset of \neach referenced object, and increments the corresponding en\u00adtry in the byte array. When the count for \nany object exceeds 100, MC2 marks it as popular. During the copy phase, MC2 treats windows containing \npopular objects specially. Any window that has a coarsened remembered set (and hence popular objects), \nis placed in a separate collection group, even if the amount of live data in the window is less than \none collec\u00adtion window. This helps reduce the amount of data copied and focuses collection effort on \nupdating the large number of references into the window, hence lowering pause time. MC2 copies popular \nobjects into a special window. It treats ob\u00adjects in this window as immortal and does not maintain a \nremembered set for this window in subsequent collections. However, if the occu\u00adpancy of the window grows \nto be high, MC2 can add it back to the list of collected windows. So, if popular objects exist, MC2 may \ntake a slight hit on pause time occasionally. However, it ensures that these objects are isolated and \ndo not cause another disruption. MC2 cannot avoid this problem completely. To be able to do so, it would \nhave to know in advance (at the point when a popular object is copied out of the nursery), that a large \nnumber of references will be created to the object. Large reference arrays MC2 divides large reference \narrays (larger than 8 KB) into 128-byte cards. Rather than store every array slot in the remembered sets, \nMC2 always uses a card table for large objects. When MC2 allocates a large object, it allocates additional \nspace (one byte for every 128 bytes in the object) at the end of the object. These bytes function as \na per-object card table in the exact same manner as the technique described in the previous section. \nMC2 also adds a word to the header of the object to store the logical address of the .rst window referenced \nfrom the object. The card scan and update is identical to the scheme described in the previous section. \n4.5 Worst case performance It is important to note that MC2 is a soft real time collector, and cannot \nprovide hard guarantees on maximum pause time and CPU utilization. In theworst caseMC2 behaves like a \nnon-incremental collector. For instance, if an application allocates a very big object immediately af\u00adter \nthe mark phase commences, causing the heap occupancy to cross the heap size limit, MC2 must perform all \nmarking and copying non\u00adincrementally in order to reclaim space to satisfy the allocation. If such a \nsituation arises, then the program will experience a long pause. sequential scanning of a region by simple \nexamination of its contents, we use the mark bit map to .nd (marked) objects within a card. Only the \nmarked objects are relevant.  (a) Before threading (b) After threading Figure 5: An example of pointer \nthreading performed by the Mark-Compact collector MC2 does not assume any knowledge about peak allocation \nrates of the running program and provides a best effort service based on statis\u00adtics it can compute as \nthe program runs. Programs with very high al\u00adlocation rates during a full collection will experience \nlonger pauses than programs with lower allocation rates. As described in the previous section, popular \nobjects can also cause MC2 to exhibit poor performance occasionally. In the worst case, every word on \nthe heap points to a single object, and moving the object requires updating the entire heap, causing \na long pause. However, these situations are rare and MC2 provides good pause time behavior under most \nconditions. 5. METHODOLOGY In order to evaluate garbage collection for memory-constrained en\u00advironments, \nwe needed to implement garbage collectors described in the literature that are appropriate to that environment. \nIn this section, we describe the implementation of these collectors, followed by our experimental setup. \n5.1 Implementation Details We used the Jikes RVM Java virtual machine [1, 2], release 2.2.3, to implement \nand evaluate the collectors. We used the Java memory management toolkit (MMTk [8]), standard with Jikes \nRVM 2.2.3, as the base collector framework. MMTk includes a generational mark\u00adsweep collector, and it \nprovided us with most of the generic function\u00adality required for a copying collector. While none of the \ncollectors requires virtual memory mapping support, they happen to use map\u00adping because the MMTk framework \nuses it. This support speeds up the performance of all collectors by allowing a faster write barrier \n(no indirection in mapping addresses to logical regions). MMTk divides the available virtual address \nspace into a number of regions. The region with the lowest addresses stores the virtual ma\u00adchine boot \nimage . The region adjacent to the boot region stores im\u00admortal objects objects never considered for \ncollection. The immor\u00adtal region uses bump pointer allocation and maps memory on demand in blocks of \nsize 32KB. MMTk allocates all system objects created at run time in the immortal region. It also allocates \ntype information block (TIB) objects, which include virtual method dispatch vectors, etc., in immortal \nspace. Additionally, we allocate all type objects and reference offset arrays for class objects into \nimmortal space, since the mark-compact collector requires that these objects not move during collection. \nNext to the immortal region is the region that stores large objects. All collectors allocate objects \nlarger than 8KB into this region. MMTk rounds up the size of large objects to whole pages (4 KB), and \nallo\u00adcates and frees them in page-grained units. The remainder of the ad\u00address space can be used by the \ncollectors to manage regular objects allocated in the heap. All collectors we evaluate in this paper \nare generational collectors. We implement the collectors with a bounded nursery: the nursery is bounded \nby a maximum and minimum size. When the size of the nursery reaches the upper bound, even if there is \nfree space available, we trigger a nursery collection, and if the nursery shrinks to the min\u00adimum size, \nwe trigger a full heap collection. The unusual aspect is the upper bound, because it triggers collection \nearlier than necessary on grounds of available space. This early triggering is important in bounding \npause time. It is important to realize that we consider here only the bounded nursery variants of each \nof the collectors we com\u00adpare. 5.1.1 Generational mark-sweep The MMTk MS collector divides the heap space \ninto two regions. The region with lower addresses contains the old generation, and the re\u00adgion with higher \naddresses contains the nursery. The write barrier records, in a remembered set, pointers that point from \noutside the nursery into the nursery. The write barrier is partially inlined [9]: the code that tests \nfor a store of an interesting pointer is inlined, while the code that inserts interesting pointers into \nthe remembered set is out of line. The nursery uses bump pointer allocation, and the collector copies \nnursery survivors into an old generation managed by mark\u00adsweep collection. The mark-sweep collector uses \nsegregated free lists to manage the heap memory. The collector divides the entire heap into a pool of \nblocks, each of which can be assigned to a free list for any of the size classes. An object is allocated \nin the free list for the smallest size class that can accommodate it. After garbage collection, if a \nblock becomes empty, the collector returns it to the pool of free blocks. 5.1.2 Generational mark-compact \nWe implemented a mark-compact generational collector (MSC), based on the threaded algorithm described \nby Martin [21]. Threaded com\u00adpaction does not require any additional space in the heap, except when handling \ninternal pointers. While this is not a problem for Java ob\u00adjects, since Java does not allow internal \npointers, Jikes RVM allocates code on the heap, which contains internal code pointers. However, the space \nrequirement for these pointers is not very high, since there is only one code pointer per stack frame. \nMSC also requires a bit map (space overhead of about 3%) in Jikes RVM, because the ob\u00adject layout (scalars \nand arrays in opposite directions) does not allow a sequential heap scan. MSC divides the heap into nursery, \nold gener\u00adation, and bit map regions. It uses the same write barrier as MS. Its compaction operates in \ntwo phases. During the mark phase, the col\u00adlector marks reachable objects. At the same time it identi.es \npointers that point from higher to lower addresses. These pointers are chained to their target object \nstarting from the status word of the target (Jikes RVM uses a status word in every object that stores \nlock, hash and Old Write Barrier New Write Barrier Figure 6: Heap layout in a MMTk generational collector. \nThe old write barrier records pointers into the nursery from objects outside the nursery. The new write \nbarrier additionally records mutations to boot image objects. GC information). For internal pointers, \nwe use extra space to store an additional offset into the target object. Figure 5 shows an example illustrating \nhow MSC performs thread\u00ading during the .rst phase. A, B, and C are three objects in the heap. A contains \na self-referential pointer (considered a backward pointer), and B and C contain one pointer each to A. \nThe collector creates a linked list starting from the status word for A. The status (which is distinguished \nby having its low bits non-zero) is stored in the slot at the end of the linked list. At the end of the \nmark phase, the collector has identi.ed all live objects. Also, it has chained to each object all backward \npointers to that object, and they can be reached by traversing a linked list starting from the object \nheader. During the second phase, MSC performs the actual compaction. As it moves an object, it updates \nall pointers refer\u00adring to the object and copies the original status word into the header of the new \ncopy. Also, it chains all (forward) pointers from the object to target objects not yet moved, so that \nit will update these pointers when it moves the target object later in the phase. 5.1.3 Non-Incremental \nCollector Improvements We describe here a couple of re.nements we make to the MMTk MS collector and our \nMSC collector described above. These re.nements help to improve execution and pause times signi.cantly \nin small and moderate size heaps. Boot image remembered sets: During a full collection, the MMTk MS collector \nscans the entire boot image to .nd pointers from boot image objects into the heap. The collector needs \nto do this since it does not record mutations to boot image objects. While this is simple to implement \nand makes the write barrier fast, it is inef.cient for small and moderate size heaps, when full collections \nare frequent. This is because most pointers within the boot image reference boot image private final \nvoid writeBarrier( VM_Address srcSlot, VM_Address tgtObj) throws VM_PragmaInline { // Record pointers \nfrom outside the nursery // that point to objects in the nursery if (srcSlot.LT(NURSERY_START) &#38;&#38; \n tgtObj.GE(NURSERY_START)) nurseryRemset.outOfLineInsert(srcSlot); VM_Magic.setMemoryAddress(srcSlot, \ntgtObj); } Figure 7: Original MMTk generational write barrier private final void writeBarrier(VM_Address \nsrcObj, VM_Address srcSlot, VM_Address tgtObj) throws VM_PragmaInline { if (srcObj.LT(NURSERY_START) \n&#38;&#38; tgtObj.GE(LOS_START)) slowPathWriteBarrier(srcObj, srcSlot, tgtObj); VM_Magic.setMemoryAddress(src, \ntgtObj); } private final void slowPathWriteBarrier( VM_Address srcObj, VM_Address srcSlot, VM_Address \ntgtObj) throws VM_PragmaNoInline { // If source object is in the boot image if (srcObj.LT(IMMORTAL_START)) \n{ // Check if object has already been recorded. // If not, insert object address into boot // remset \nand mark the object VM_Word status = VM_Interface.readAvailableBitsWord(srcObj); if (status.and(Hdr.MUTATE_BIT).isZero()) \n{ VM_Interface.writeAvailableBitsWord(srcObj, status.or(Hdr.MUTATE_BIT)); bootRemset.inlinedInsert(srcObj); \n } } // Record slots outside nursery that point to // nursery objects if (tgt.GE(NURSERY_START)) nurseryRemset.inlinedInsert(src); \n} Figure 8: New generational write barrier objects, and the collector spends a good portion of the collection \ntime following these pointers. Our measurements show that only about 0.4% of all boot image pointers \nreference objects in the heap. This issue with boot image scanning has also been discussed by Bacon et \nal. [4]. We modi.ed the MMTk MS collector to avoid having to scan the boot image during full collections. \nThe modi.ed MS collector uses a new write barrier. Figure 6 shows the layout of the heap in MMTk and \nthe pointers that are recorded by the old and new write barriers. Figure 7 shows the old MMTk MS write \nbarrier and Figure 8 shows the new write barrier (for a uniprocessor environment). The old write barrier \nrecords pointers into the nursery from objects that lie outside the nursery. The new barrier records \nall boot image objects that con\u00adtain pointers into the heap, in addition to recording pointers into the \nnursery. During a full collection, the mutated boot objects are scanned to .nd heap pointers, and the \nrest of the boot image is not touched. The modi.ed MS collector improves the full collection pause time \nby up to a factor of 4 for small benchmarks (since boot image scanning dominates collection time for \nthese benchmarks). It usually has lower execution times in small and moderate size heaps. In large heaps, \nwhen fewer collections occur, the execution time is about the same since the reduction in collection \ntime is not signi.cantly larger than the increase in mutator time (due to the more expensive write barrier). \nWe also used this technique in a modi.ed version of the genera\u00adtional mark-compact collector, and found \nimprovements in execution and pause times. We present in the results section the performance of both \nversions of MS and MSC. Code Region: MMTk collectors place compiled code and data in the same space. \nWe found that this can cause signi.cant degradation in the performance of MSC and MC2 due to poor code \nlocality. We modi.ed the collectors to allocate code and data in different spaces. Total size (MB) (log) \nTotal size (MB) (log) 6184 7730 9276 10822 12368 13914 15460 18552 21644 24736 13364 16705 20046 23387 \n26728 30069 33410 40092 46774 53456 4 5.87 4 10.51  GC time relative to best (log) Total GC time (seconds) \n(log) GC time relative to best (log) Total GC time (seconds) (log) 2 2.94 2 5.26 1.47   2.63 1 1.25 \n1.5 1.75 2 2.25 2.5 2.75 3 3.5 4 1 1.25 1.5 1.75 2 2.25 2.5 2.75 3 3.5 4 Size relative to max. live size \n(log) Size relative to max. live size (log) (a) jack (b) javac Figure 9: GC time relative to best GC \ntime for MSC, MSC with a separate code region (MSC-CODE), and MSC with a separate code region and a new \nwrite barrier (MSC-CODE-NEW-WB). The new write barrier lowers GC time signi.cantly. Total size (MB) (log) \nTotal size (MB) (log) 6184 7730 9276 10822 12368 13914 15460 18552 21644 24736 13364 16705 20046 23387 \n26728 30069 33410 40092 46774 53456 2 15.80 231.12  1.05 8.29 1.05 16.34 1 1 1.25 1.5 1.75 2 2.25 2.5 \n2.75 3 3.5 4 7.90 1 1 1.25 1.5 1.75 2 2.25 2.5 2.75 3 3.5 4 15.56 Size relative to max. live size (log) \nSize relative to max. live size (log) (a) jack (b) javac  Execution time relative to best (log) Total \nexecution time (seconds) (log) Execution time relative to best (log) Total execution time (seconds) (log) \n1.5 11.85 1.5 23.34 1.3 10.27 1.3 20.23 1.2 9.48 1.2 18.67 1.1 8.69 1.1 17.12 Figure 10: Execution time \nrelative to best execution time for MSC, MSC with a separate code region (MSC-CODE), and MSC with a separate \ncode region and a new write barrier (MSC-CODE-NEW-WB). The separate code region lowers execution time \nsigni.cantly by reducing mutator time. The lower GC times with the new write barrier reduce execution \ntime signi.cantly. For MSC, we achieved this by maintaining a separate code space to store code objects \nwhich we compact in place. For MC2, we use sepa\u00adrate code windows to hold code objects. We do not use \nthis technique for MS, since the use of a separate MS space for code would not help signi.cantly (objects \nof different size are not placed together), and would probably increase fragmentation. While the write \nbarrier tech\u00adnique improves GC time (at a small expense in mutator time), the sep\u00adarate code region improves \nmutator time for the copying collectors due to improved code locality. Figure 9 shows MSC GC times for \ntwo benchmarks. The three curves represent MSC with a common space for data and code, with a separate \ncode space (MSC-CODE), and MSC with a separate code space and the new write barrier(MSC-CODE-NEW-WB). \nFigure 10 shows execution times for the three MSC variants. The GC times for MSC and MSC-CODE are almost \nidentical. Our performance counter measurements show that MSC-CODE performs better due to improved code \nlocality (fewer ITLB misses). The graphs also show that MSC-CODE-NEW-WB performs better than MSC-CODE \nin small and moderate heaps because of lower GC times. In large heaps, the two collectors have equivalent \nperformance. 5.1.4 MC2 Our MC2 implementation divides the virtual address space for the old generation \ninto a number of .xed size frames. A frame is the largest contiguous chunk of memory into which MC2 performs \nallocation. We use a frame size of 8MB (the heap sizes for our benchmarks vary from 6MB 120MB). Each \nframe accommodates one old generation physical window. Windows are usually smaller than a frame, and \nthe portion of the address space within a frame that is not occupied by a window is left unused (unmapped). \nThe frames are power-of-two aligned, so we need to perform only a single shift to .nd the physical window \nnumber for an old generation object during GC; we use the physical window number as an index into a byte \narray to obtain the logical address of the window, as previously described. Each window has an associated \nremembered set, implemented using a sequential store buffer. Benchmark Description Maximum Live size \n(KB) Total Allocation (MB) 202 jess a Java expert system shell 5872 319 209 db a small data management \nprogram 12800 93 213 javac aJavacompiler 13364 280 227 mtrt a dual-threaded ray tracer 12788 163 228 \njack a parser generator 6184 279 pseudojbb SPECjbb2000 with a .xed number of transactions 30488 384 \nTable 1: Description of the benchmarks used in the experiments Our implementation tags each remembered \nset entry to indicate whether the entry belongs to a scalar object or an array (this informa\u00adtion is \nrequired to locate the object containing a slot while converting remembered set representations). If \nthe overall metadata size grows close to 4.2% of the heap, MC2 converts the largest remembered sets to \ncard tables (we use a card table with a granularity of 128 bytes and place a 5% limit on remembered set \nsize). In our current implementa\u00adtion, we do not coarsen boot image slots since the number of entries \nis small and limited. 5.2 Experimental Setup Jikes RVM compiles all bytecode to native code before execution. \nIt has two compilers, a baseline compiler that essentially macro-expands each bytecode into non-optimized \nmachine code, and an optimizing compiler. It also has an adaptive run-time system that .rst baseline \ncompiles all methods and later optimizes methods that execute fre\u00adquently. It optimizes methods at three \ndifferent levels depending on the execution frequency. However, the adaptive system does not pro\u00adduce \nreproducible results, because it uses timers and may optimize different methods in different runs. We \nused a pseudo-adaptive con.guration to run our experiments with reproducible results. We .rst ran each \nbenchmark 7 times with the adaptive run-time system, logging the names of methods that were optimized \nand their optimization levels. We then determined the meth\u00adods that were optimized in a majority of the \nruns, and the highest level to which each of these methods was optimized in a majority of runs. We ran \nour experiments with exactly these methods optimized (to that optimization level) and all other methods \nbaseline compiled. The re\u00adsulting system behavior is repeatable, and does very nearly the same total \nallocation as a typical adaptive system run. Jikes RVM is itself written in Java, and some system classes \ncan be compiled either at run time or at system build time. We compiled all the system classes at build \ntime to avoid any non-application compila\u00adtion at run time. The system classes appear in a region called \nthe boot image that is separate from the program heap. 6. RESULTS We compare space overheads, GC times, \ntotal execution times, and pause times for .ve collectors (MC2, MS, MS with the new write barrier, MSC, \nand MSC with the new write barrier) across six bench\u00admarks. The MSC collectors and MC2 use separate regions \nfor code and data. Five benchmarks come from the SPECjvm98 suite [26], and the sixth is pseudojbb, a \nmodi.ed version of the SPECjbb2000 benchmark [27]. pseudojbb executes a .xed number of transactions (70000), \nwhich allows better comparison of the performance of the different collectors. We ran all SPEC benchmarks \nusing the default parameters (size 100), and ignoring explicit GC requests. Table 1 de\u00adscribes each of \nthe benchmarks we used. We compute the live size by running each benchmark in the smallest heap for MSC, \nand recording the largest amount of live data in the old generation after a full col\u00adlection. (MMTk uses \na resource table that occupies 4MB in immortal space. We move it to the boot image and do not include \nit in live size measurements, since it skews the value considerably for small bench\u00admarks.) We ran our \nexperiments on a system with a Pentium P4 1.7 GHz processor, 8KB on-chip L1 cache, 12KB on-chip ETC (instruction \ncache), 256KB on-chip uni.ed L2 cache, and 512 MB of memory, running RedHat Linux 2.4.20-31.9 (with the \nperfctr patch applied). We performed our experiments with the machine in single user mode to maximize \nrepeatability. 6.1 MC2 Space Overheads MC2 implemented using an SSB remembered set incurs the following \nspace overheads for its metadata: 3.12% of the total heap space for a bit map that is used both to mark \nobjects and locate objects during a window scan.  At most 5% of the total heap space for window remembered \nsets. A card table for 128 byte cards occupies 0.78% of the to\u00adtal space. MC2 coarsens SSB remembered \nsets when their total size reaches 4.2% of the total space. Table 2 shows the max\u00adimum remembered set \noverhead (without coarsening) for the six benchmarks, for heap sizes ranging from 1.5 2.5 times the program \nlive size. The overheads are low, and jess, jack, javac (and mtrt at 1.5 times the live size) require \nsome coarsening in small heaps. The overheads shown here do not include pointers from large and immortal \nobjects into the windows. We always use a card table for these objects.  Work Queue overhead. Our current \nimplementation does not bound the total work queue overhead, but we account for the space taken up by \nthe queue, which is small. Table 3 shows the maximum queue overheads for our benchmark suite.  6.2 Execution \ntimes and pause times Figure 11 shows GC times for the collectors relative to the best GC time. Figure \n12 shows total execution times for the collectors rela\u00adtive to the best execution time. The x-axis on \nall graphs represents the heap size relative to the maximum live size, and the y-axis repre\u00adsents the \nrelative times. Table 4 shows the maximum pause times for the collectors and relative execution times \nfor the MS and MSC col\u00adlectors in a heap that is 1.8 times the program s maximum live size. This is the \nsmallest heap in which MC2 provides a combination of high throughput and low pause times for 5 of the \n6 benchmarks, and is the typical overhead to be expected for good performance. Table 5 shows minimum, \nmaximum and geometric means of the pause times across all benchmarks for MC2 in heaps ranging from 1.5 \n2.5 times the live size. It also shows geometric means of pause times and av\u00aderage relative execution \ntimes for MS and MSC. All results are for con.gurations using a nursery with a maximum size of 1MB and \na minimum size of 256KB. MC2 uses 100 physical windows and 30 collection windows in the old generation. \nBenchmark Heap Size relative to maximum live size 1.5 1.75 1.8 2 2.25 2.5 202 jess 7.95 4.86 6.63 6.91 \n3.88 3.32 209 db 1.64 1.37 1.26 1.14 1.01 0.90 213 javac 8.52 7.06 6.91 5.90 5.91 5.49 227 mtrt 4.51 \n3.07 2.74 3.14 2.08 0.51 228 jack 6.84 5.03 5.00 4.54 3.18 2.20 pseudojbb 2.24 1.81 1.78 1.54 1.38 1.23 \n Benchmark Heap Size relative to maximum live size 1.5 1.75 1.8 2 2.25 2.5 202 jess 0.61 0.53 0.51 0.48 \n0.41 0.37 209 db 0.46 0.40 0.38 0.34 0.31 0.27 213 javac 0.70 0.57 0.54 0.32 0.47 0.43 227 mtrt 0.17 \n0.15 0.15 0.13 0.12 228 jack 0.46 0.39 0.38 0.34 0.31 0.28 pseudojbb 0.13 0.11 0.11 0.10 0.08 0.08  \nTable 2: Remembered set size (without coarsening) as per-Table 3: Maximum mark queue size as percent \nof heap size cent of heap size (%), for MC2 using 100 physical windows (%), for MC2 using 100 physical \nwindows and 30 collection and 30 collection windows windows Benchmark MC2 MPT MS MS (New WB) MSC MSC \n(New WB) MPT ET MPT ET MPT ET MPT ET 202 jess 17.15 202.20 1.16 53.15 1.11 244.53 1.06 65.69 1.04 209 \ndb 19.89 278.00 1.10 123.02 1.11 353.55 0.96 198.51 0.96 213 javac 40.39 317.04 0.96 171.88 0.92 458.09 \n0.92 308.95 0.89 227 mtrt 29.57 284.40 1.06 138.05 1.02 379.55 1.01 225.24 0.96 228 jack 23.89 210.04 \n1.07 59.69 1.03 234.04 1.03 92.89 0.99 pseudojbb 41.47 322.97 1.12 168.20 1.08 445.99 1.01 314.53 0.98 \nGeo. Mean 27.18 264.68 1.08 107.67 1.04 340.88 1.00 172.68 0.97 Table 4: Maximum Pause Times (MPT, all \nin milliseconds) and execution times relative to MC2 (ET) for MC2, MS, and MSC, in a heap that is 1.8 \ntimes the maximum live size. MSC and MC2 use separate data and code regions. All collectors use a 1MB \nnursery and MC2 uses 100 physical windows and 30 collection windows. Rel. Heap. MC2 MS MS (New WB) MSC \nMSC (New WB) Min. MPT Max. MPT Mean MPT Mean MPT Mean ET Mean MPT Mean ET Mean MPT Mean ET Mean MPT Mean \nET 1.50 15.97 119.47 37.92 263.43 1.06 104.65 1.00 338.04 0.97 166.85 0.92 1.75 16.18 48.40 26.98 261.56 \n1.06 102.85 1.03 338.23 0.98 165.01 0.95 2.00 18.01 40.70 26.93 254.45 1.07 95.62 1.04 328.20 0.99 149.68 \n0.97 2.25 17.50 47.46 30.11 254.96 1.06 94.24 1.04 321.15 0.99 153.59 0.97 2.50 18.26 48.64 27.61 255.71 \n1.08 99.25 1.05 334.05 0.99 163.36 0.99 Table 5: Min., max., and geometric mean of max. pause times \n(milliseconds) across all benchmarks for MC2 . Geometric mean of max. pause times (milliseconds) and \ngeometric mean of execution times relative to MC2 across all benchmarks for MS and MSC. For each heap \nsize, we consider only benchmarks that cause invocation of at least one full collection for all collectors. \nMSC and MC2 use separate data and code regions. All collectors use a 1MB nursery and MC2 uses 100 physical \nwindows and 30 collection windows. The execution times for MSC with the new write barrier are almost \nalways lower than MSC with the boot image scan. The GC plots show that a signi.cant reduction in GC time \ncauses the performance im\u00adprovement. Only for db is performance with the new barrier slightly worse in \nmoderate and large heaps. This is because db mutates a large number of pointers into the old generation \n(90% of all stores), causing invocation of the slow path barrier each time. The old barrier ignores these \nwrites since it records only pointers into the nursery. Similarly, for MS the performance with the new \nbarrier is usually better in small and moderate size heaps. As with MSC, the perfor\u00admance with the new \nbarrier is slightly worse for db in moderate and large heaps. In the following discussion, we consider \nMS and MSC only with the new barrier. MSC almost always has the best execution times among the col\u00adlectors. \nIt always outperforms MS in small and moderate size heaps. MC2 also generally performs better than MS \nin small and moderate size heaps. The exceptions are db and javac,where MC2 performs worse than MS in \nsmall heaps. In large heaps, MC2 and MS have equivalent performance for all benchmarks apart from db \nand pseu\u00addojbb.MC2 performs signi.cantly better for these benchmarks. MC2 has lower GC times than MS \nfor jess and jack. For all other benchmarks, GC times for MC2 are slightly worse than those for MS. However, \nthe overall performance of MC2 is usually better due to improved locality. For all benchmarks, the performance \nfor MC2 is usually within 5% of of MSC in heaps that are 1.5 1.8 times the program live size or larger. \njavac again is an exception and MC2 is within 5-6% of MSC only in heaps that are twice the live size \nor larger. MSC performs better than the other collectors for a couple of rea\u00adsons. First, the GC cost \nfor MSC is almost always the lowest. Second, the collector preserves allocation order, which yields better \nlocality and thus lower mutator times. Both db and pseudojbb contain signi.cant amounts of permanent \ndata, which is advantageous to MC2. Since it uses a mostly-copying technique, it does not copy large \nportions of data. It compacts a smaller portion of the heap, which contains transient data. In spite \nof copying little data, MC2 does not have lower GC times than MSC, because MSC has the same property. \nIt also does not repeatedly copy permanent data, since permanent data .ows toward the lower end of the \nheap, and MSC does not move any live data at the start of the heap. Total size (MB) (log) Total size \n(MB) (log) 5872 7340 8808 10276 11744 13212 14680 17616 20552 23488 12800 16000 19200 22400 25600 28800 \n32000 38400 44800 51200 4 5.57 4 1.66  Total GC time (seconds) (log) GC time relative to best (log) \nTotal GC time (seconds) (log) Total GC time (seconds) (log) GC time relative to best (log)GC time relative \nto best (log) 2.78 2 1.39 0.41 1 1.25 1.5 1.75 2 2.25 2.5 2.75 3 3.5 4 1 1.25 1.5 1.75 2 2.25 2.5 2.75 \n3 3.5 4 Size relative to max. live size (log) Size relative to max. live size (log) (a) jess(b) db Total \nsize (MB) (log) Total size (MB) (log) 13364 16705 20046 23387 26728 30069 33410 40092 46774 53456 12788 \n15985 19182 22379 25576 28773 31970 38364 44758 51152 4 10.51 4 3.52  Total GC time (seconds) (log) \nGC time relative to best (log) Total GC time (seconds) (log) GC time relative to best (log) 5.26 2 2.63 \n0.88 1 1.25 1.5 1.75 2 2.25 2.5 2.75 3 3.5 4 1 1.25 1.5 1.75 2 2.25 2.5 2.75 3 3.5 4 Size relative to \nmax. live size (log) Size relative to max. live size (log) (c) javac(d) mtrtTotal size (MB) (log) Total \nsize (MB) (log) 6184 7730 9276 10822 12368 13914 15460 18552 21644 24736 30488 38110 45732 53354 60976 \n68598 76220 91464 106708 121952 4 5.87 4 13.21  Total GC time (seconds) (log) GC time relative to best \n(log) 2.94 2 1.47   3.30 1 1.25 1.5 1.75 2 2.25 2.5 2.75 3 3.5 4 1 1.25 1.5 1.75 2 2.25 2.5 2.75 3 \n3.5 4 Size relative to max. live size (log) Size relative to max. live size (log) (e) jack (f) pseudojbb \n The data in table 4 show that for all benchmarks, the maximum but has a mean pause time that is a factor \nof 4 higher, with a mean pause times for MC2 are signi.cantly lower than those for MS and performance \ndegradation of 4%. MSC, even in a tight heap. The geometric mean of the pause times for MSC with the \nboot image scan performs as well as MC2 on aver-MC2 is 27.18ms. The mean pause time for the MMTk MS collector \nis age, with an average pause time that is almost a factor of 12 higher. 264.68, almost a factor of 10 \nhigher, with a mean performance degra-With the new write barrier, MC2 is 3% slower on average but its \npause dation of 8%. MS with the new write barrier performs much better, time is up to an order magnitude \nlower than MSC, with an average Total size (MB) (log) Total size (MB) (log) 5872 7340 8808 10276 11744 \n13212 14680 17616 20552 23488 12800 16000 19200 22400 25600 28800 32000 38400 44800 51200 2 13.36 2 47.10 \n Execution time relative to best (log)Execution time relative to best (log) Total execution time (seconds) \n(log) Execution time relative to best (log) Total execution time (seconds) (log) Total execution time \n(seconds) (log) 10.02 1.5 8.68 1.3 8.02 1.2 7.35 1.1 1.05 7.01 1.05 24.73 1 6.68 1 23.55 Size relative \nto max. live size (log) Size relative to max. live size (log) (a) jess(b) db Total size (MB) (log) Total \nsize (MB) (log) 13364 16705 20046 23387 26728 30069 33410 40092 46774 53456 12788 15985 19182 22379 \n25576 28773 31970 38364 44758 51152 2 31.12 2 12.18  Execution time relative to best (log) Total execution \ntime (seconds) (log) Execution time relative to best (log) Total execution time (seconds) (log) 1.5 \n23.34 1.5 9.13 1.3 20.23 1.3 7.92 1.2 18.67 1.2 7.31 1.1 17.12 1.1 6.70 1.05 16.34 1.05 6.39 1 15.56 \n1 6.09 Size relative to max. live size (log) Size relative to max. live size (log) (c) javac(d) mtrt \nTotal size (MB) (log) Total size (MB) (log) 6184 7730 9276 10822 12368 13914 15460 18552 21644 24736 \n30488 38110 45732 53354 60976 68598 76220 91464 106708 121952 2 15.80 2 45.24  1.05 8.29 1.05 23.75 \n1 1 1.25 1.5 1.75 2 2.25 2.5 2.75 3 3.5 4 7.90 1 1 1.25 1.5 1.75 2 2.25 2.5 2.75 3 3.5 4 22.62 Size relative \nto max. live size (log) Size relative to max. live size (log) (e) jack (f) pseudojbb  Total execution \ntime (seconds) (log) Execution time relative to best (log) 11.85 1.5 10.27 1.3 9.48 1.2 8.69 1.1 pause \ntime that is a factor of 6 lower. The only benchmark for which about 8% slower than MSC on average and \nhas a maximum pause time the performance of MC2 is signi.cantly worse is javac.MC2 requires of 119.47ms \n(this long pause occurs for javac because a few copying a slightly larger heap for javac and is usually \nabout 5% worse than passes are clustered together). In heaps between 1.75 2.5 times the MSC in heaps \nthat are twice the live size or larger. live size, the maximum pause time for MC2 is under 50ms for all \nTable 5 summarizes results for the collectors in heaps from 1.5 benchmarks. The average pause time varies \nfrom 27 30ms. MC2 is 2.5 times the live size. In a heap that is 1.5 times the live size, MC2 is 3 5% \nfaster than MS and 1 5% slower than MSC on average. We also compared the performance of MC2 (using SSBs \nwith coars\u00adening) with a version of MC2 that uses only a card table. In very small heaps (1.25 times \nthe live size or lower) the card table technique usu\u00adally performs better than the SSB technique. This \nis because it has a smaller space overhead (always 0.78%) than the SSB collector. How\u00adever, in heaps \nthat are larger than 1.25 times the live size, the SSB technique performs slightly better or about the \nsame as the card tech\u00adnique. The pause times for the SSB collector also tend to be slightly lower than \nthe card technique since it knows the exact location of slots pointing into windows. The card technique \nmust examine ev\u00adery pointer in every object in all cards that contain an object into the window being \ncollected, which can be more expensive. Summary: The results show that MC2 can obtain low pause times \nand good throughput in constrained heaps. The average pause time for MC2 is 27.18ms in a heap that is \n1.8 times the program live size. Importantly, the execution times for MC2 are good about 4% better than \na well tuned mark-sweep collector and about 3% worse than a well tuned mark-compact collector. 6.3 Pause \ntime distribution Figure 13 shows the distribution of MC2 pause times for the six bench\u00admarks in a heap \nthat is 1.8 times the benchmark s maximum live size. The .gure contains three plots for each benchmark: \nthe .rst contains all pauses (nursery collection, mark phase, and copy phase), the sec\u00adond only mark \nphase pauses, and the third only copy phase (nursery and old generation window copying) pauses. The graphs \nalso show the durations of the median pause, and of the 90th and 95th percentile pauses. The x-axis on \nall graphs shows the actual pause times and the y-axis shows the frequency of occurrence of each pause \ntime. The y-axis is on a logarithmic scale, allowing one to see clearly the less frequently-occurring \nlonger pauses. For all benchmarks, a majority of the pauses are 10ms or less. 80% of all pauses for javac \nare 10ms or less, and 92% of pauses for pseu\u00addojbb are in the 0 10ms range. (For jess, db, mtrt and jack,pauses \nthat are 10ms or less account for 97%, 93%, 87% and 93% of all pauses). Most of these pauses are caused \nby the mark phase, which performs small amounts of marking interleaved with allocation. These pauses \ncause the median pause time value to be low. The graphs containing mark-only pauses show that the maximum \nduration of a mark pause is 9ms, and this occurs for jack. jess, db, mtrt and pseudojbb have mark pauses \nthat are 5ms or less. All mark pauses for javac are atmost7ms long. The less frequent, longer pauses \n(up to 41ms long) typically result from the copy phase. These collections copy objects out of both the \nnursery and a subset of the old generation windows. The average copy phase collection time for javac \nis 26ms, and the average copy phase pause time for pseudojbb is 19ms. The longest copy pause time (41ms) \nis for pseudojbb, and 88% of all copy pauses are shorter than 30ms in duration. One possible technique \nwe could use to reduce copy phase pause times further is to collect the nursery and old generation windows \nsep\u00adarately. We call this a split phase technique. Using split-phase, MC2 would alternate between nursery \ncollections and old generation win\u00addow copying, with data from the windows copied when the nursery is \nhalf full. However, this technique adds a cost to the write barrier, to keep track of pointers from the \nnursery into the next set of old generation windows being copied. We have not yet implemented and evaluated \nthis technique for MC2. 6.4 Bounded mutator utilization We now look more closely at the pause time characteristics \nof the collectors. We consider more than just the maximum pause times that occurred, since these do not \nindicate how the collection pauses are distributed over the running of the program. For example, a collector \nmight cause a series of short pauses whose effect is similar to a long pause, which cannot be detected \nby looking only at the maximum pause time of the collector (or the distributions). We present mutator \nutilization curves for the collectors, following the methodology of Cheng and Blelloch [12]. They de.ne \nthe utiliza\u00adtion for any time window to be the fraction of the time that the mutator (the program, as \nopposed to the collector) executes within the win\u00addow. The minimum utilization across all windows of \nthe same size is called the minimum mutator utilization (MMU) for that window size. An interesting property \nof this de.nition is that a larger window can actually have lower utilization than a smaller one. To \navoid this, we extend the de.nition of MMU to what we call the bounded minimum mutator utilization (BMU). \nThe BMU for a given window size is the minimum mutator utilization for all windows of that size or greater. \nFigure 14 shows BMU curves for the six benchmarks for a heap size equal to 1.8 times the benchmark live \nsize. The x-intercept of the curves indicates the maximum pause time, and the asymptotic y\u00advalue indicates \nthe fraction of the total time used for mutator execution (average mutator utilization). Since it is \ndif.cult to factor out the write barrier cost, the curves actually represent utilization inclusive of \nthe write barrier. The real mutator utilization will be a little lower. These graphs also do not show \nthe effects of locality on the overall performance. For instance, for db,MC2 and MS have lower throughput. \nHowever, since this is caused by higher mutator times (possibly because of locality effects), and not \nbecause of higher GC times, the BMU curves do not re.ect the consequences. The three curves in each graph \nare for MC2, MS, and MSC (all for versions using the new write barrier). The curve with the smallest \nx\u00adintercept is for MC2. MSC has the curve with the largest x-intercept; the MS curve has an x-intercept \nin between the other two. For all benchmarks, MC2 allows some mutator utilization even for very small \nwindows. This is because of the low pause times for the collector. For most benchmarks, the mutator can \nexecute for up to 10 25% of the total time in the worst case, for time windows that are about 50ms long. \nThe non-incremental collectors, on the other hand, allow non-zero utilization in the worst case only \nfor much larger windows, since they have large maximum pause times. MC2 can provide higher utilization \nthan MS in windows of time up to 7 seconds for jess, windows up to 600ms for db,300ms for javac, mtrt,and \npseudojbb, and windows up to 200ms for jack.When compared with MSC, utilization is higher for windows \nup to 7s, 3s, 800ms, 500ms, 400ms and 300ms for jess, javac, pseudojbb, mtrt, db, and jack respectively. \nBeyond that, the utilization provided by MC2 for most benchmarks is about the same, and the asymptotic \ny-values for the curves are very close. For jack, utilization provided by MC2 is lower than MSC for windows \nlarger than 300ms but overall utilization is close. Only for javac does MC2 provide signi.cantly lower \noverall utilization (the overall utilization for javac is much closer in heaps that are twice the live \nsize or higher). Summary: The mutator utilization curves for the collectors show that MC2 not only provides \nshorter pause times, it also provides higher mutator utilization for small windows of time, i.e., it \nspreads its pauses out well. This holds even for windows of time that are larger than the maximum pause \ntimes for the non-incremental collectors. In windows of time that are larger than one second, the utilization \nprovided by all collectors tends to be about the same. 1000 1000 1000 100 100 100   Frequency Frequency \nFrequency Frequency Frequency Frequency Frequency Frequency Frequency Frequency Frequency Frequency Frequency \nFrequency Frequency Frequency Frequency Frequency 10 10 10 1 1 1 Pause time (ms) Pause time (ms) Pause \ntime (ms) (a) jess (all pauses)(b) jess (mark pauses)(c) jess (copy pauses) 1000 1000 1000   100 100 \n100 10 10 10 1 1 1 Pause time (ms) Pause time (ms) Pause time (ms) (d) db (all pauses)(e) db (mark pauses)(f) \ndb (copy pauses) 1000 1000 1000   100 100 100 10 10 10 1 1 1 Pause time (ms) Pause time (ms) Pause \ntime (ms) (g) javac (all pauses)(h) javac (mark pauses)(i) javac (copy pauses) 1000 1000 1000   100 \n100 100 10 10 10 1 1 1 Pause time (ms) Pause time (ms) Pause time (ms) (j) mtrt (all pauses)(k) mtrt \n(mark pauses)(l) mtrt (copy pauses) 1000 1000 1000   100 100 100 10 10 10 1 1 1 Pause time (ms) Pause \ntime (ms) Pause time (ms) (m) jack (all pauses)(n) jack (mark pauses)(o) jack (copy pauses) 1000 1000 \n1000  10 10 10 1 1 1 0 5 10 15 20 25 30 35 40 45Pause time (ms) 50 0 5 10 15 20 25 30 35 40 45Pause \ntime (ms) 50 0 5 10 15 20 25 30 35 40 45 50 Pause time (ms) (p) pseudojbb (all pauses) (q) pseudojbb \n(mark pauses) (r) pseudojbb (copy pauses) Figure 13: MC2 pause time distributions, in a heap that is \n1.8 times the program live size. The .rst column shows all pauses. Mark pauses (second column) are 9ms \nor less. Copy phase pauses (third column) are longer (4 41ms).   100 100 100  10000 100000 1e+06 1e+07 \n10000 100000 1e+06 1e+07 Window Size (\u00b5seconds) (log) Window Size (\u00b5seconds) (log) (a) jess (b) db  \n Window Size (\u00b5seconds) (log) Window Size (\u00b5seconds) (log) (c) javac (d) mtrt  Window Size (\u00b5seconds) \n(log) Window Size (\u00b5seconds) (log) (e) jack (f) pseudojbb 7. CONCLUSIONS predictable footprint makes \nbetter use of available memory. We com\u00adpared the performance of MC2 with a non-incremental generational \nWe have presented an incremental copying garbage collector, MC2 mark-sweep (MS) collector and a generational \nmark-compact (MSC) (Memory-Constrained Copying), that runs in constrained memory and collector, and showed \nthat MC2 provides throughput comparable to provides both good throughput and short pause times. These \nprop\u00adthat of both of those collectors. We also showed that the pause times erties make the collector \nsuitable for applications running on hand\u00ad held devices that have soft real-time requirements. It is \nalso attrac-of MC2 are signi.cantly lower than those for MSC and MS in con\u00adstrained memory. tive for \ndesktop and server environments, where its smaller and more 8. ACKNOWLEDGMENTS This material is based \nupon work supported by the National Science Foundation under grant number CCR-0085792. Additionally, \nEmery Berger was supported by NSF CAREER award CNS-0347339. Any opinions, .ndings, conclusions, or recommendations \nexpressed in this material are those of the authors and do not necessarily re.ect the views of the NSF. \nWe are also grateful to IBM Research for making the Jikes RVM system available under open source terms. \nThe MMTk component of Jikes RVM was particularly helpful in this work. In addition, we thank Rick Hudson \nfor the original idea that led to the MC and MC2 collectors, and Kathryn McKinley and Csaba Andras Moritz \nfor discussions that helped shape the paper. 9. REFERENCES [1] Bowen Alpern, C. R. Attanasio, Anthony \nCocchi, Derek Lieber, Stephen Smith, Ton Ngo, John J. Barton, Susan Flynn Hummel, Janice C. Sheperd, \nand Mark Mergen. Implementing Jalape no in Java. In OOPSLA 99 ACM Conference on Object-Oriented Systems, \nLanguages and Applications, volume 34(10) of ACM SIGPLAN Notices, pages 314 324, Denver, CO, October \n1999. ACM Press. [2] Bowen Alpern, Dick Attanasio, John J. Barton, M. G. Burke, P. Cheng, J.-D. Choi, \nAnthony Cocchi, Stephen J. Fink, David Grove, Michael Hind, Susan Flynn Hummel, D. Lieber, V. Litvinov, \nMark Mergen, Ton Ngo, J. R. Russell, Vivek Sarkar, Manuel J. Serrano, Janice Shepherd, S. Smith, V. C. \nSreedhar, H. Srinivasan, and J. Whaley. The Jalape no virtual machine. IBM System Journal, 39(1):211 \n238, February 2000. [3] Alain Azagury, Elliot K. Kolodner, Erez Petrank, and Zvi Yehudai. Combining card \nmarking with remembered sets: How to save scanning time. In Richard Jones, editor, ISMM 98 Proceedings \nof the First International Symposium on Memory Management, volume 34(3) of ACM SIGPLAN Notices, pages \n10 19, Vancouver, October 1998. ACM Press. [4] David F. Bacon, Perry Cheng, and V.T. Rajan. Controlling \nfragmentation and space consumption in the Metronome, a real-time garbage collector for Java. In ACM \nSIGPLAN 2003 Conference on Languages, Compilers, and Tools for Embedded Systems (LCTES 2003), pages 81 \n92, San Diego, CA, June 2003. ACM Press. [5] David F. Bacon, Perry Cheng, and V.T. Rajan. A real-time \ngarbage collecor with low overhead and consistent utilization. In Conference Record of the Thirtieth \nAnnual ACM Symposium on Principles of Programming Languages, ACM SIGPLAN Notices, pages 285 298, New \nOrleans, LA, January 2003. ACM Press. [6] Henry G. Baker. List processing in real-time on a serial computer. \nCommunications of the ACM, 21(4):280 294, 1978. Also AI Laboratory Working Paper 139, 1977. [7] Ori \nBen-Yitzhak, Irit Goft, Elliot Kolodner, Kean Kuiper, and Victor Leikehman. An algorithm for parallel \nincremental compaction. In ISMM 02 [17], pages 100 105. [8] Stephen M. Blackburn, Perry Cheng, and Kathryn \nS. McKinley. Oil and Water? High Performance Garbage Collection in Java with MMTk. In ICSE 2004, 26th \nInternational Conference on Software Engineering, pages 137 146, Edinburgh, May 2004. [9] Stephen M. \nBlackburn and Kathryn S. McKinley. In or out? Putting write barriers in their place. In ISMM 02 [17], \npages 175 184. [10] Rodney A. Brooks. Trading data space for reduced time and code space in real-time \ngarbage collection on stock hardware. In Guy L. Steele, editor, Conference Record of the 1984 ACM Symposium \non Lisp and Functional Programming, pages 256 262, Austin, TX, August 1984. ACM Press. [11] C. J. Cheney. \nA non-recursive list compacting algorithm. Communications of the ACM, 13(11):677 8, November 1970. [12] \nPerry Cheng and Guy Blelloch. A parallel, real-time garbage collector. In Proceedings of SIGPLAN 2001 \nConference on Programming Languages Design and Implementation, ACM SIGPLAN Notices, pages 125 136, Snowbird, \nUtah, June 2001. ACM Press. [13] Jacques Cohen and Alexandru Nicolau. Comparison of compacting algorithms \nfor garbage collection. ACM Transactions on Programming Languages and Systems, 5(4):532 553, 1983. [14] \nEdsgar W. Dijkstra, Leslie Lamport, A. J. Martin, C. S. Scholten, and E. F. M. Steffens. On-the-.y garbage \ncollection: An exercise in cooperation. Communications of the ACM, 21(11):965 975, November 1978. [15] \nB. K. Haddon and W. M. Waite. A compaction procedure for variable length storage elements. Computer Journal, \n10:162 165, August 1967. [16] Richard L. Hudson and J. Eliot B. Moss. Incremental garbage collection \nfor mature objects. In Yves Bekkers and Jacques Cohen, editors, Proceedings of International Workshop \non Memory Management, volume 637 of Lecture Notes in Computer Science, pages 388 403, University of Massachusetts, \nUSA, 16 18 September 1992. Springer-Verlag. [17] ISMM 02 Proceedings of the Third International Symposium \non Memory Management, ACM SIGPLAN Notices, Berlin, June 2002. ACM Press. [18] Mark S. Johnstone and Paul \nR. Wilson. The memory fragmentation problem: Solved? In Peter Dickman and Paul R. Wilson, editors, OOPSLA \n97 Workshop on Garbage Collection and Memory Management, pages 26 36, October 1997. [19] H. B. M. Jonkers. \nA fast garbage compaction algorithm. Information Processing Letters, 9(1):25 30, July 1979. [20] Henry \nLieberman and Carl E. Hewitt. A real-time garbage collector based on the lifetimes of objects. Communications \nof the ACM, 26(6):419 429, 1983. Also report TM 184, Laboratory for Computer Science, MIT, Cambridge, \nMA, July 1980 and AI Lab Memo 569, 1981. [21] Johannes J. Martin. An ef.cient garbage compaction algorithm. \nCommunications of the ACM, 25(8):571 581, August 1982. [22] John McCarthy. Recursive functions of symbolic \nexpressions and their computation by machine. Communications of the ACM, 3:184 195, 1960. [23] NewMonics \nInc., PERC virtual machine. http://www.newmonics.com/perc/info.shtml. [24] Narendran Sachindran and J. \nEliot B. Moss. Mark-Copy: Fast copying GC with less space overhead. In OOPSLA 03 ACM Conference on Object-Oriented \nSystems, Languages and Applications,ACM SIGPLAN Notices, pages 326 343, Anaheim, CA, November 2003. ACM \nPress. [25] H. Schorr and W. Waite. An ef.cient machine independent procedure for garbage collection \nin various list structures. Communications of the ACM, 10(8):501 506, August 1967. [26] Standard Performance \nEvaluation Corporation. SPECjvm98 Documentation, release 1.03 edition, March 1999. [27] Standard Performance \nEvaluation Corporation. SPECjbb2000 (Java Business Benchmark) Documentation, release 1.01 edition, 2001. \n[28] Guy L. Steele. Multiprocessing compactifying garbage collection. Communications of the ACM, 18(9):495 \n508, September 1975. [29] Sun Microsystems, The CLDC HotSpot Implementation Virtual Machine, Java 2 Platform, \nMicro Edition J2ME Technology, March 2004. http://java.sun.com/products/cldc/wp/CLDC-HI whitepaper-March \n2004.pdf. [30] David M. Ungar. Generation scavenging: A non-disruptive high performance storage reclamation \nalgorithm. ACM SIGPLAN Notices, 19(5):157 167, April 1984. Also published as ACM Software Engineering \nNotes 9, 3 (May 1984) Proceedings of the ACM/SIGSOFT/SIGPLAN Software Engineering Symposium on Practical \nSoftware Development Environments, 157 167, April 1984. [31] Paul R. Wilson. Uniprocessor garbage collection \ntechniques. Technical report, University of Texas, January 1994. Expanded version of the IWMM92 paper. \n[32] Taichi Yuasa. Real-time garbage collection on general-purpose machines. Journal of Software and \nSystems, 11(3):181 198, 1990.    \n\t\t\t", "proc_id": "1028976", "abstract": "<p>Java is becoming an important platform for memory-constrained consumer devices such as PDAs and cellular phones, because it provides safety and portability. Since Java uses garbage collection, efficient garbage collectors that run in constrained memory are essential. Typical collection techniques used on these devices are mark-sweep and mark-compact. Mark-sweep collectors can provide good throughput and pause times but suffer from fragmentation. Mark-compact collectors prevent fragmentation, have low space overheads, and provide good throughput. However, they can suffer from long pause times. Copying collectors can provide higher throughput than either of these techniques, but because of their high space overhead, they previously were unsuitable for memory-constrained devices. This paper presents MC&#60;sup>2&#60;/sup> (Memory-Constrained Copying), a copying generational garbage collector that meets the needs of memory-constrained devices with soft real-time requirements. MC&#60;sup>2&#60;/sup> has low space over-head and tight space bounds, prevents fragmentation, provides good throughput, and yields short pause times. These qualities make MC&#60;sup>2&#60;/sup> attractive for other environments, including desktops and servers.</p>", "authors": [{"name": "Narendran Sachindran", "author_profile_id": "81100025271", "affiliation": "University of Massachusetts, Amherst, MA", "person_id": "P643391", "email_address": "", "orcid_id": ""}, {"name": "J. Eliot B. Moss", "author_profile_id": "81406593781", "affiliation": "University of Massachusetts, Amherst, MA", "person_id": "P129799", "email_address": "", "orcid_id": ""}, {"name": "Emery D. Berger", "author_profile_id": "81100228645", "affiliation": "University of Massachusetts, Amherst, MA", "person_id": "PP14089241", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1028976.1028984", "year": "2004", "article_id": "1028984", "conference": "OOPSLA", "title": "MC<sup>2</sup>: high-performance garbage collection for memory-constrained environments", "url": "http://dl.acm.org/citation.cfm?id=1028984"}