{"article_publication_date": "10-01-2004", "fulltext": "\n An Ef.cient Parallel Heap Compaction Algorithm Diab Abuaiadh Yoav Ossia Erez Petrank* Uri Silbershtein \nIBM Haifa Research Laboratory Haifa University, Mount Carmel, Haifa 31905, Israel. {diab, yossia,erezp,uris}@il.ibm.com \nABSTRACT We propose a heap compaction algorithm appropriate for modern computing environments. Our algorithm \nis targeted at SMP platforms. It demonstrates high scalability when running in parallel but is also extremely \ne.cient when run\u00adning single-threaded on a uniprocessor. Instead of using the standard forwarding pointer \nmechanism for updating point\u00aders to moved objects, the algorithm saves information for a pack of objects. \nIt then does a small computation to process this information and determine each object s new location. \nIn addition, using a smart parallel moving strategy, the al\u00adgorithm achieves (almost) perfect compaction \nin the lower addresses of the heap, whereas previous algorithms achieved parallelism by compacting within \nseveral predetermined seg\u00adments. Next, we investigate a method that trades compaction quality for a further \nreduction in time and space overhead. Finally, we propose a modern version of the two-.nger com\u00adpaction \nalgorithm. This algorithm fails, thus, re-validating traditional wisdom asserting that retaining the \norder of live objects signi.cantly improves the quality of the compaction. The parallel compaction algorithm \nwas implemented on the IBM production Java Virtual Machine. We provide measurements demonstrating high \ne.ciency and scalability. Subsequently, this algorithm has been incorporated into the IBM production \nJVM. Categories and Subject Descriptors D.3.4 [Programming Languages]: Processors Memory management \n(garbage collection) General Terms Languages, Performance, Algorithms * Dept. of Computer Science, Technion, \nHaifa 3200, Israel. This work was done while the author was at the IBM Haifa Research Lab. E-mail: erez@cs.technion.ac.il. \nPermission to make digital or hard copies of all or part of this work for personal or classroom use is \ngranted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. OOPSLA \n04, Oct. 24-28, 2004, Vancouver, British Columbia, Canada. Copyright 2004 ACM 1-58113-831-8/04/0010 ...$5.00. \n Keywords Garbage collection, Java, JVM, Parallel garbage collection, Compaction, Parallel Compaction. \n 1. INTRODUCTION Mark-sweep and reference-counting collectors are suscep\u00adtible to fragmentation. Because \nthe garbage collector does not move objects during the collection, but only reclaims unreachable objects, \nthe heap becomes more and more frag\u00admented as holes are created in between objects that survive the collections. \nAs allocation within these holes is more costly than allocation in a large contiguous space, program \nactivity is hindered. Furthermore, the program may even\u00adtually fail to allocate a large object simply \nbecause the free space is not consecutive in memory. To ameliorate this prob\u00adlem, compaction algorithms \nare used to group the live ob\u00adjects and create large contiguous available spaces for alloca\u00adtion. Originally, \ncompaction was used in systems with tight memory. When compaction was triggered, space had been virtually \nexhausted and auxiliary space could not be as\u00adsumed. Some clever algorithms were designed for this set\u00adting \nand could compact the heap without using any space overhead, most notably, the threaded algorithm of \nJonkers [9] and Morris [12], also described in [8]. Today, memories are larger and cheaper, and modern \ncomputer systems allow (reasonable) auxiliary space usage for improving compaction e.ciency. An interesting \nchallenge is to design an e.cient and scaleable compaction algorithm appropriate for modern environments, \nincluding multi-threaded operating systems and SMP platforms. In this work, we present a new compaction \nalgorithm that achieves high e.ciency, high scalability, almost perfect com\u00adpaction quality, and low \nspace overhead. Our algorithm demonstrates e.ciency by substantially outperforming the threaded algorithm \ncurrently employed by the IBM produc\u00adtion JVM [6]. It demonstrates high scalability by yielding a speedup \nfactor of about 6.6 onan8-way SMP. In con\u00adtrast to previously suggested parallel algorithms, which ob\u00adtained \nparallelism by producing several piles of compacted objects within the heap, the proposed algorithm obtains \na compaction of high quality by compacting the whole heap to the lower addresses. Compaction has been \nnotoriously known to cause large pause times. Even with concurrent garbage collectors, a compaction may \neventually be triggered causing infrequent, yet, long pauses. Thus, it is important to improve com\u00adpaction \ne.ciency in order to reduce the maximum pause times. On an 8-way SMP, the new algorithm reduces this \npause time by a factor of more than 8 (being more e.cient and scalable), thus making the longest pause \ntimes bearable. 1.1 Algorithmic overview A compaction algorithm consists of two parts: moving the objects \nand .xing up the pointers in the heap to point to the new locations. Let us start with the .x up phase. \nThe simplest .x up method is to keep forwarding pointers in the objects and use them to .x pointers. \nThis implies a space overhead of one pointer per object and forces a read operation in the heap for each \npointer update. We use a di.erent approach, dividing the heap into small blocks (typ\u00adically, 256 bytes) \nand maintaining .x up information for each block (rather than for each object). This information allows \n.xing up pointers to live objects within the block. This new approach requires a small amount of computation \nduring each pointer .x up, but restricts the memory accesses to a relatively small block table, rather \nthan reading random locations in the heap. A second issue is the quality of compaction. We would like \nto move all live objects to the lower addresses in the heap, even though the work is done in parallel \nby several threads. The problem is that the naive approach to moving all objects to the lower addresses \nin parallel requires synchronization for every move operation. Frequent synchronization has a high cost \nin e.ciency. The most prominent previous solution was to split the heap into N areas and let N threads \nwork in parallel, each compacting its own area locally [7]. This method yields several piles of compacted \nobjects. Our goal is to obtain a perfect compaction, moving all live objects to lower addresses as is \ndone in a sequential compaction algorithm. To this end, we divide the heap into many areas. Typically, \nfor a Gigabyte heap, the size of an area is a few megabytes. Objects in the .rst few areas are compacted \nwithin the area space to lower addresses. This results in a bunch of compacted areas with contiguous \nempty spaces. The re\u00admaining areas are compacted into the free spaces created within areas previously \ncompacted. This allows moving all objects to the lower addresses. Using the above idea, the quality of \ncompaction does not depend on the number of areas. But now an additional bene.t is obtained. Using a \nlarge number of areas allows excellent load balancing. Indeed with this improved load balancing, the \nalgorithm scales well to a large number of processors.  1.2 Quality versus overhead Next, we observe \nthat it is possible to save time and space by somewhat relaxing the compaction quality. Instead of full \ncompaction, we can run a compaction without executing intra-block compaction, but only inter-block compaction. \nNamely, each block is moved as a whole, without condensing holes within the objects inside it. This allows \nsaving two thirds of the space overhead required by our compaction algorithm. From our measurements of \nSPECjbb2000, we found it also reduces the (already short) time required to do the compaction by as much \nas 20%. The area into which the objects are compacted increases by only a few percent due to the holes \nwe do not eliminate.  1.3 Verifying traditional wisdom It is widely believed that preserving the order \nof objects during compaction is important. We have made an attempt to design and run a simple compaction \nalgorithm that does not follow this guideline. The proposed algorithm is a mod\u00adern variant of the two-.nger \nalgorithm (see [8]) applica\u00adble for the Java heap. The advantage of this algorithm is that it substantially \nreduces the number of moved objects (typically, by a factor of 2). The cost is that it does not preserve \nthe order of objects. The hope was that maybe modern benchmarks are not severely harmed by reorder\u00ading \ntheir objects in this manner. However, it turned out that the folklore belief strongly holds, at least \nwhen running the SPECjbb2000 benchmark. Application throughput was drastically reduced with this compaction \nalgorithm. 1.4 Implementation and measurements We have implemented our algorithms on top of the mark\u00adsweep-compact \ncollector that is part of the IBM production JVM 1.4.0. This collector is described in [5, 6]. The com\u00adpact \npart of this collector implements the threaded algo\u00adrithm of Jonkers [9] and Morris [12]. We have replaced \nthis compactor with our parallel compaction, and ran it on N threads in parallel, using an N-way machine. \nIn order to compare our algorithm with the original threaded algorithm, we restrict our algorithm to \nuse a single thread. It turns out that our algorithm typically reduces compaction time by 30%. We then \ncheck how the compaction scales on a multiprocessor, when run in parallel, and .nd that it has good scalability, \nobtaining (on an 8-way AIX machine) a speedup factor of 2.0 on two threads, 3.7 on four threads, 5.2 \non six threads, and 6.6 on eight threads. With this new algorithm, compaction ceases to be a major problem \non a multiprocessor. For example, using temporary access to a 24-way AIX machine, we ran SPECjbb (from \n1 to 48 ware\u00adhouses) on a 20 GB heap. The last compactions (the most intensive ones) ran for about 15 \nseconds with the threaded algorithm and only 600 ms with our new parallel algorithm. 1.5 Related work \nAs far as we know, Flood et al [7] o.er the best algorithm known today for parallel compaction. They \npresent an al\u00adgorithm to compact the (old generation part of the) heap in parallel. Their algorithm uses \na designated word per object to keep a forwarding reference. On the HotSpot platform, such a word can \nbe easily obtained in the header of most objects (non-hashed, old-generation objects). The rest of the \nobjects require the use of more space (either free young generation space or more allocated space). The \ncompaction runs three passes over the heap. First, it determines a new location for each object and installs \na forwarding pointer, second, it .xes all pointers in the heap to point to the new locations, and .nally, \nit moves all objects. To make the al\u00adgorithm run in parallel, the heap is split into N areas (where N \nis the number of processors) and N threads are used to compact the heap into N/2 chunks of live objects. \nIn the algorithm proposed in this paper, we attempt to improve over this algorithm in e.ciency, scalability, \nspace consumption, and quality of compaction. In terms of ef\u00ad.ciency, our algorithm requires only two \n(e.cient) passes over the heap. Note that we have eliminated the pass that installs forwarding pointers \nand we have improved the pass that .xes the pointers. In particular, the pass that .xes all pointers \nin the heap does not need to access the target object (to read a forwarding pointer). Instead, it accesses \na small auxiliary table. We also do not assume a header structure that can designate a word to keep the \nforwarding reference. This reduces space consumption on a platform that does not contain a redundant \nword per object. In par\u00adticular, our additional tables consume 2.3% or 3.9% of the heap size (depending \non whether the mark bit vector is in use by the garbage collector). Sparing a word per object is likely \nto require more space [3]. In addition, we improve the load balancing in order to obtain better scalability. \nWe do this by splitting the heap into a large number of small areas, each being handled as a job unit \nfor a compacting thread. This .ne load balancing yields excellent speedup. Finally, we improve the quality \nof compaction (to obtain better ap\u00adplication behavior) by moving all live objects to a single pile of \nobjects at the low end of the heap. To do that, instead of compacting each area into its own space, we \nsometimes allow areas to be compacted into space freed in other areas. Using all these enhancements we \nare able to obtain improved e.ciency, scalability and compaction quality. Some compaction algorithms \n(e.g., [11, 2]) use handles to provide an extra level of indirection when accessing objects. Since only \nthe handles need to be updated when an object moves, the .x up phase, in which pointers are updated to \npoint to the new location, is not required. However, a design employing handles is not appropriate for \na high performance language runtime system. Incremental compaction was suggested by Lang and Dupont [10] \nand a modern variant was presented by Ben Yitzhak et al [4]. The idea is to split the heap into regions, \nand com\u00adpact one region at a time by evacuating all objects from the selected region. Extending these \nworks to compact the full heap does not yield the e.cient parallel compaction al\u00adgorithm we need. Extending \nthe .rst algorithm yields a standard copying collector (that keeps half the heap empty for collection \nuse). Extending the latter is also problem\u00adatic, since it creates a list of all pointers pointing into \nthe evacuated area; this is not appropriate for the full heap. Also, objects cannot be moved into the \nevacuated area, since forwarding pointers are kept in place of evacuated objects. We note that our algorithm \ncan be modi.ed to perform in\u00adcremental compaction. One may determine an area to be compacted and then \ncompact within the area. After the compaction, the entire heap must be traversed to .x point\u00aders, but \npointers that do not point into the compacted area can be easily identi.ed and skipped, without incurring \na signi.cant overhead. Finally, the algorithm that we compare against (in the measurements) is the one \ncurrently implemented in the IBM production JVM. This is a compaction algorithm based on the threaded \nalgorithm of Jonkers [9] and Morris [12]. These algorithms use elegant algorithmic ideas to completely \navoid any space overhead during the compaction. While saving space overhead was a crucial goal at times \nwhen memory was small and costly, it is less critical today. Trading ef\u00ad.ciency for space saving seems \nless appealing with today s large memory and heaps. Even though the threaded algo\u00adrithm requires only \ntwo passes over the heap, these passes are not e.cient. Our measurements demonstrate that our method \nshows a substantial improvement in e.ciency over the threaded compaction, even on a uniprocessor without \nparallelization. When SMPs are used, the threaded algo\u00adrithms do not tend to parallelize easily. We are \nnot aware of any reported parallel threaded compaction algorithm and our own attempts to design parallel \nextensions of the threaded compaction algorithms do not scale as well as the algorithm presented in this \npaper.  1.6 Organization The rest of the paper is organized as follows. In Section 2 we present a sequential \nversion of the new compaction al\u00adgorithm. In Section 3, we show how to make it parallel. In Section 4, \nwe present a trade-o. between the quality of the compaction and the overhead of the algorithm in time \nand space. In Section 5, we present an adaptation of the two-.nger compaction algorithm adequate for \nJava. In Sec\u00adtions 6 and 7 we present the implementation and the mea\u00adsurements. We conclude in Section \n8.  2. THE COMPACTION ALGORITHM In this section we describe the parallel compaction algo\u00adrithm. We start \nwith the data structure and then proceed to describe the algorithm. 2.1 Accompanying data structures \nOur compaction algorithm requires three vectors: two of them are bit vectors that are large enough to \ncontain a bit for each object in the heap and the third is an o.set vector large enough to contain an \naddress for each block in the heap. Blocks will be de.ned later, and the size of a block is a parameter \nB to be chosen. In our implementation, we chose B = 256 bytes. Many collectors use such two bit vectors \nand the com\u00adpaction can share them with the collector. Speci.cally, such vectors exist in the IBM production \nJVM collector that we used. The .rst vector is an input to the compaction algo\u00adrithm, output by the marking \nphase of the mark-compact algorithm. This vector has a bit set for any live (reachable) object, and we \ndenote it the mark bit vector. The second vector is the alloc bit vector, which is the output of the \ncom\u00adpaction algorithm. Similar to the mark bit vector, the alloc vector has a bit set for each live object \nafter the compaction. Namely, a bit corresponding to an object is set if, after the compaction, the corresponding \nobject is alive. We remark that in the JVM we use, these vectors exist for collection use. When they \ndo not exist for collector use, these two vectors should be allocated for the compaction. Our garbage \ncollector uses the mark bit vector during the marking phase to mark all reachable objects. The alloc \nbit vector signi.es which objects are allocated (whether reach\u00adable or not). This vector is also used \nduring the marking phase since our JVM is non-accurate (i.e., it is not possible to always tell whether \na root is a pointer or not). See more on this issue in section 6. Thus, when a pointer candidate is found \nin the roots, one test the collector runs to check whether it may be a valid pointer, is a check if it \npoints to an allocated object. Here the alloc bit vector becomes useful. To summarize, we assume that \nthe mark bit vector cor\u00adrectly signi.es the live objects on entry to the compaction algorithm. The compaction \nalgorithm guarantees that as compaction ends and the program resumes, the alloc bit vector is properly \nconstructed to signify the location of all allocated live objects after compaction. The third vector \nis allocated speci.cally for use during the compaction; we call it the o.set vector. The length of the \n(mark and alloc) bit vectors is proportional to the heap size and depends on object alignment. For example, \nin our implementation, the object alignment is eight bytes and so in order to map each bit in the bit \nvectors to a potential be\u00adginning of an object in memory, each bit should correspond to an 8-byte slot \nin memory. The overhead is 1/64 of the heap for each of these vectors. The length of the third vec\u00adtor \ndepends on the platform (32 or 64-bit addressing) and on the block size, as explained in Section 2.2 \nbelow. In our implementation, we chose a block size of 256 bytes, making this vector equal in length \nto (each of) the other two vectors on 32-bit platforms.  2.2 The basic algorithm In this section, we \ndescribe the basic algorithm without parallelization. In Section 3 below, we modify the basic al\u00adgorithm \nmaking it run on several threads in parallel. We run the compaction in two phases. In the .rst phase, \nthe moving phase, the actual compaction is executed, moving objects to the low end of the heap and recording \nsome in\u00adformation that will later let us know where each object has moved. In the second phase, the .x \nup phase,we go over the pointers in the heap (and roots) and modify them to point to the new location \nof the referent object. The infor\u00admation recorded in the .rst phase, must allow the .xing of all pointers \nin the second phase. The moving phase. The actual moving is simply done with two pointers. The .rst pointer, \ndenoted from, runs consecu\u00adtively on the heap (more accurately, on the mark bit vector) to .nd live objects \nto be moved. The second pointer, de\u00adnoted to, starts at the beginning of the heap. Whenever a live object \nis found, it is copied to the location pointed by the to pointer, and the to pointer is incremented by \nthe size of the copied object to point to the next word available for copying. This is done until all \nobjects in the heap have been copied. The main issue in this phase is how to record information properly \nwhen moving an object, so that in the second phase, we can .x pointers that reference this object. For \neach relocated object, we update the alloc bit vector to signify that an object exists in its new position. \n(The alloc bit vector is zeroed before the beginning of the compaction, so there is no need to erase \nthe bits signifying previous object locations.) In addition, we divide the heap into .xed size blocks. \nThe size of the blocks is denoted by Band in our implementation, we chose B= 256 bytes. For each .rst \nobject in a block, we also update the o.set vector to record the address to which this .rst object is \nmoved. Note that the o.set vector has a pointer for each block, thus, it must be long enough to hold \na pointer per block. This sums up the moving phase. The .x up phase. During the .x up phase, we go over \nall pointers in the heap (and also in the roots, such as threads stack and registers) and .x each pointer \nto reference the modi.ed location of the referent object after the move. Given a pointer to the previous \nlocation of an object A,we can easily compute in which block the object Aresides (by dividing the address \nby B, or shifting left eight bit positions if B= 256). For that block, we start by reading all mark bits \nof the objects in the block. These bits determine the location of the objects before they were moved. \nFrom the mark bits, we can compute the ordinal number of this object in the block. We do not elaborate \non the bit manipulations required to .nd the ordinal number, but note that this is an e.cient computation \nsince there are not many objects in a block. For example, if a block is 256 bytes and an object is at \nleast 16 bytes (as is the case in our system), then there exist at most eight objects in a block; therefore, \nour object may be .rst in the block, second in the block, and so forth, but at most the eighth object \nin the block. The number of live objects in a block is usually much smaller than eight, since many objects \nare not alive and since most objects are longer than 16 bytes. We actually found out that many of the \nsearches end up outputting 1 as the ordinal number. Returning to the algorithm, we denote the ordinal \nnumber of the referent object to be i. We now turn to the o.set vector and .nd the location to which \nthe .rst object in the block has moved. Let this o.set be O.If i= 1, we are done and the address Ois \nthe new address of the object A.If i>1, we turn to look at the alloc bit that corresponds to the heap \naddress O. Note that this alloc bit must be set since the .rst object in the block has moved to address \nO. From that bit, we proceed to search the alloc bit vector until we .nd the i-th set bit in the alloc \nbit vector, starting at the set bit that corresponds to the address O. Because we keep the order of objects \nwhen we move them, this i-th set bit in the alloc bit vector must be the bit that corresponds to our \nreferent A. Thus, from the alloc bit vector we can compute the new address of Aand .x the given pointer. \nInterestingly, this computation never touches the heap. We .rst check the mark bit vector, next we check \nthe o.set vector, and last, if the ordinal number is not one, we check the alloc bit vector. The fact \nthat the .x up only touches relatively small tables, may have a good e.ect on the cache behavior of the \ncompaction. The above explanation assumes that pointers reference an object. Namely, they point to the \nbeginning (or to the header) of the object. This assumption is correct at garbage collection times for \nthe JVM we used. However, it is easy to modify the algorithm to handle inner-object references by searching \nthe mark bit vector to .nd the object head. A pseudo code of the .x up routine is depicted in Fig\u00adure \n1. It assumes a couple of routines or macros that per\u00adform simple bit operations and address translations. \nThe mark bit location macro takes an object and returns the lo\u00ad Procedure Fix Up(A:Object) begin 1. MA= \nmark bit location(A) 2. MB = mark bit block location(A) 3. i = ordinal(MA, MB ) 4. O =o.set table(MB \n) 5. if(i == 1)  new = O 6. else A1 = alloc bit lcation(O) Ai =locationof i-th set bit in alloc vector \nstarting at A1 new = alloc to heap(Ai)  7. return(new) end Figure 1: A pseudo code of the .x up algorithm \ncation of its corresponding mark bit; the alloc bit location returns the corresponding location in the \nalloc bit vector. The alloc to heap macro receives a location in the alloc bit vector and returns the \nheap location of the object that cor\u00adresponds to this location. The mark bit block location macro returns \nthe location in the mark bit vector that corresponds to the beginning of the block that contains the \nobject A. The routine ordinal receives two locations in the mark bit vector and counts the number of \nones between them. In particular, it returns the ordinal number of the object cor\u00adresponding to the .rst \nparameter in the block corresponding to the second parameter. The o.set table of a given block location \nreturns the location to which the .rst object in this block has been moved. A possible optimization, \nthat we adopt in our JVM, is applicable when the minimal size of objects, denoted S,ex\u00adceeds the alignment \nrestriction, denoted A. For example, in our system, objects are 8-bytes aligned and their minimal size \nis 16, i.e., A =8 and S = 16. It is then possible to keep a bit per S bytes rather than per A bytes, \nas described above. Information kept in this manner allows .nding the ordinal number, which is enough \nfor the mark bit vector, but does not allow computing the address of an object from the location of its \ncorresponding bit; therefore, this cannot be done with the alloc bit vector. Using this observation, \nwe may save 1 - A/S (half, in our setting) of the space used by the mark bits. Our measurements show \nthat performance is not harmed by employing this space saving. 3. MAKING THE COMPACTION PARALLEL We start \nby splitting the heap into areas so the com\u00adpaction work can be split between threads. As always, there \nis a tradeo. in determining the number of areas. For good load balancing (and therefore good scalability) \nit is better to use a large number of small areas. But for low synchroniza\u00adtion and low management overhead, \nit is better to have a small number of large areas. Synchronization does not play a substantial role \nin the tradeo., since synchronizing to get the next area does not create a noticeable delay in execution \ntime. However, the quality of compaction and the overhead of managing these areas are signi.cant. A typical \nchoice in our collector is to let the number of areas be 16 times the number of processors, but the area \nsize is limited to be at least 4 MB. An important new idea in our parallel version of the com\u00adpaction \nistoallow objects to move from oneareatoanother. In the beginning, a compacting thread pushes objects \nto the lower addresses of the area it is working in. However, when some areas have been compacted, the \nthreads start to com\u00adpact other areas into the remaining space within the areas that have already been \ncompacted. This method is di.erent from previous schemes [7] that partitioned the heap into a small number \nof large areas and compacted each area sepa\u00adrately. In those schemes, the heap is compacted into several \ndense piles of objects, depending on the number of areas. Therefore, a good quality of compaction dictates \na small number of areas, which may be bad for load balancing. Us\u00ading our idea, the entire heap is compacted \nto the low ad\u00addresses, except for some negligible holes remaining in the last areas, which were not completely \n.lled. Using this idea, we get the best of both worlds: better compaction quality as objects are all \nmoved to the lower end of the heap and better scalability as small areas allow better load balancing. \nGetting more into the details, we keep an index (or a pointer) that signi.es the next area to be compacted. \nA thread A obtains a task by using a compare and swap syn\u00adchronization to increment this index. If successful, \nthe thread A has obtained an area to compact. If the compare and swap fails, another thread has obtained \nthis area and A tries the next area. After obtaining an area to compact, A also looks for a target area \ninto which it can output the com\u00adpacted area; it only tries to compact into areas with lower addresses. \nA compaction is never made to an area with higher addresses. This invariant may be kept, because an area \ncan always be compacted into itself. We keep a table, denoted the target table of areas, that provides \na pointer to the beginning of the free space for each area. Initially, the target table contains nulls, \nimplying that no area may be compacted into another area. However, whenever a thread compacts an area \ninto itself (or similarly into another area), it updates the values of the compacted area entry (or the \ntarget area entry, if di.erent) in the target table to point to the beginning of the remaining free space \nin the area. Re\u00adturning toathread A that is looking for a target area, once A has found a lower area \nwith free space, it tries to lock the target area by using a compare and swap instruction to null the \nentry of the area in the target table. If this operation succeeds, A can compact into the selected target \narea. No thread will try to use this area as a target, as long as it has a null entry. After .nishing \nthe compaction, A updates the free space pointer of the target area in the target ta\u00adble. At that time, \nthis area may be used as a target area by other threads. On the other hand, if the target area gets .lled \nduring the compaction, its remaining space entry will remain null and thread A will use the above mechanism \nto .nd another target area to compact into. Procedure Parallel Move() begin 1. S= fetchAndAdd(lastCompacted) \n 2. If(S == N)exit. 3. for(T =0; T<S; T ++)  pCopy= fetchAndSet(P (T ),NULL)  if (pCopy != NULL) break \n  4. if(T == S) Compact S internally  P (S)= resulting free space pointer  goto step 1  5. else \nCompact remaining S objects into T using pCopy If (T becomes full) goto step 3  If (S becomes empty) \n P(S) = S  P(T) = (updated) pCopy   goto step 1 end Figure 2: A per-thread pseudo code of the parallel \nmove algorithm Normally, the size of a block is much smaller than the size of an area. (In our implementation \nit was 256 bytes and 4MB respectively.) When moving a block, we never split its objects to more than \none area. In other words, if the remaining free space in the end of an area does not su.ce to move all \nlive objects that start in a block, then we abandon the area and start moving blocks to a new area. Thus, \nthe live objects of a block are always moved consecutively. Figure 2 illustrates our parallel compaction \nalgorithm. Suppose the heap is divided to N areas (0 .. N-1), and for each area A we keep a pointer (denoted \nP (A)) to its free space (initially set to NULL). We also maintain a global variable with the index of \nthe last area that is compacted (denoted lastCompacted). We de.ne the atomic operations of fetchAndAdd(V \n)(whichincrements V and returns its old value) and fetchAndSet(V ,newV alue)(whichsets V to newV alue \nand returns its old value) 1 . 4. TRADING QUALITY OF COMPACTION FOR HIGHER EFFICIENCY In our algorithm \n(presented in Sections 2 and 3 above), the heap is (almost) perfectly compacted into the low addresses \nof the heap so that the compacted objects are dense with no waste of space. We now suggest trading this \nquality of compaction for reduced overhead both in execution time as well as in the amount of auxiliary \nspace used. 1Both operations may be constructed using the compare and swap atomic operation, which is \ngiven a variable, its expected old value, and a required new value. It succeeds to set the variable to \nthe new value only if the old value was found. In the case of a few threads trying to perform this operation \nsimultaneously, only one will return as successful. In our main algorithms, we use blocks of length \nB bytes and we spend a considerable e.ort compacting the objects within a block. In terms of space overhead, \nthis intra-block compaction necessitates the use of the mark bit vector. In terms of runtime overhead, \nthe full compaction requires up\u00addating and using the mark bit vector and the alloc bit vector for the \npointer .x up. The main idea behind the relaxed compaction algorithm that we present now, is to perform \ncompaction between blocks, but not within blocks. Thus, for each block s we keep only an o.set .(s) signifying \nthat all objects within the block s are moved .(s) words back\u00adwards. The value of .(s) is determined \nso that the .rst live object in the block moves to the .rst available word after the previous blocks \nhave been compacted. In partic\u00adular, .(s + 1) is set so that the .rst live object in Block s + 1 moves \njust after the last live object in block s ends. If a block has no live objects, it consumes no space \nin the compacted heap. We simply let the next non-empty block move its .rst live object to the next available \nword. Figure 3 depicts a compaction of two possible .rst blocks in a heap. Figure Inter-blockCompaction: \nspaces within the blockare not compacted Clearly, if a block has only one live object or no live ob\u00adjects, \nit is compacted perfectly, leaving no wasted space. Also, if a block is full of objects with no space \nbetween them, it is also perfectly compacted. Thus, dense live objects and sparse live objects may yield \ngood compaction quality. How\u00adever, in-between these two cases, we lose space. The worst placement of \nobjects in a block for this method is the place\u00adment of two small objects, one residing in the beginning \nof the block and the other in the end of the block. We have implemented this optimization and measured \nits behavior. Interestingly, we found that with the SPECjbb2000 benchmark and heap size inducing 60% \nheap consumption2 , the cost of this optimization is as low as 3% increment in the size of the compacted \narea, the reduced compaction quality did not change the overall throughput, compaction time was reduced \nby approximately 20%, and the space overhead (for compaction data structures) was reduced by 66%. These \nmeasurements are presented in Section 7.3. 2Namely, the accumulated size of the live objects is 60% of \nthe heap size.  5. THE ORDER OF OBJECTS MATTERS The two-.nger compaction algorithm was designed for \ncompacting LISP heaps containing objects of one size only. The algorithm may be visualized via two .ngers \n(i.e., point\u00aders). One .nger traverses the heap from the beginning up\u00adwards searching for an empty location, \nwhile the other .nger moves from the end of the heap to the beginning looking for a live object. When \nthe two .ngers stop their move, the lower .nger points to a hole and the upper .nger points to an object. \nSince all objects are of the same size, all holes are also of the same size and we can move the object \nof the upper .nger to the hole found by the lower .nger. Then, the two .ngers go on searching. When the \ntwo .ngers meet each other, the compaction is done. Forwarding pointers are left in the moved objects \nso that pointers to moved ob\u00adjects can later be updated. (For more details see [8]). The big advantage \nof this method is that it typically moves only half of the objects, whereas other methods move all objects. \nThis also implies less work on .xing up the pointers, since many pointers (referencing addresses below \nthe point where the two .ngers meet) do not need to be updated. Also, this algorithm can be easily made \nparallel; the .x up is simple and e.cient, and no space overhead is required. The disadvantage of this \ncompaction method is that it does not preserve the order of live objects. According to folklore belief, \nthis does not yield a good compaction. First, it is believed that locality of reference holds with respect \nto allocation order; thus, the paging and caching behavior of the program deteriorates when the order \nof objects is not preserved by the compaction. Second, objects allocated to\u00adgether tend to die together \nand thus, not keeping objects with their allocated neighbors may increase future fragmen\u00adtation. In spite \nof this folklore belief, we made an attempt to implement an adaptation of the two-.nger method for Java \nand to check its behavior. An adaptation is required since the objects in a Java heap di.er in size. \nWe also tried to design an adaptation that can later run in parallel threads. The hope was that the negative \ne.ects would be small on a modern system and that the advantages of the algorithm would win. As it turned \nout, the throughput of the program using this compaction algorithm deteriorated and the com\u00adpacted heap \nobtained by this algorithm became fragmented by subsequent program allocations and garbage collections. \nThis validates the folklore belief. The modi.ed algorithm is now described brie.y. In this description, \nwe do not care about the e.ciency of the com\u00adpaction itself. As the compaction quality is the focus of \nthis discussion, we simplify the presentation by describing a naive and ine.cient algorithm. We will \nlater measure the throughput of the application without taking into account the running time of the compaction3 \n. Also, the quality of the compaction does not depend on the running time of the compaction itself. Getting \ninto the algorithmic details, Java objects are par\u00adtitioned into small and big objects. The threshold \nsize is a tunable parameter, and a typical threshold setting is 1 kilo\u00ad 3This is the default when measuring \nthe SPECjbb2000 benchmark. Compaction happens during ramp-up while changing the number of warehouses. \nThat time is not ac\u00adcounted for when computing the throughput. byte. The algorithm starts by .nding \nthe lowest address . in the heap that ensures that all small objects in addresses above . can be moved \nto holes below . Naively looking for . may require a lot of computation but, as stated above, in this \nshort description we ignore all algorithmic e.ciency issues. After .nding the address , we move small \nobjects from addresses above . to addresses below .We start by mov\u00ading objects to holes of exactly the \nsame size, where both holes (starting from the beginning of the heap) and objects (starting from ) are \nprocessed by their order in the heap. After .tting exactly matching objects and holes, we try to .t smaller \nobjects into bigger holes; still, all objects and holes are small. Next, we put small objects in large \nholes. Finally, if large holes remain, we also move large objects into large holes, wherever possible. \nMoving large objects to large holes is done using the following strategy: go over the holes according \nto their sizes in ascending order; for each hole, go over objects smaller than the hole according to \ntheir size in descending order and move the objects into the hole. For each object moved to a hole, we \nleave a forwarding pointer. At this stage, we are done with moving objects to holes, but some large objects \nmay still be scattered above address . and need to be moved. These objects are compacted by sliding them \ndown to the address . This compaction op\u00aderation requires a small auxiliary table for a later .x up of \nthe heap pointers. Before starting the slide, we run an up\u00addate on all pointers in the heap. This .x \nup process starts by computing the .nal address for each large object that has not yet been moved. This \naddress is written into the small auxiliary table. Next, the .x up process goes over all pointers in \nthe heap and updates pointers according to the forwarding pointer in the object or the information stored \nin the auxiliary table. Finally, the remaining large live objects above address . are compacted towards \nthe address . and we are done. In Section 7.4 below, we provide measurements of this algorithm. It turns \nout that fragmentation quickly recurs after several garbage collections and that program through\u00adput \nis reduced with this compaction algorithm. This does not happen when using the original compaction algorithm \nor the compaction algorithm described in this paper (in Sec\u00adtion 2). 6. IMPLEMENTATIONS We have implemented \nour algorithms on top of the mark\u00adsweep-compact collector that is part of the IBM production JVM 1.4.0. \nThis collector is described in [5, 6]. The com\u00adpact part of this collector implements the threaded algo\u00adrithm \nof Jonkers [9] and Morris [12]. We have replaced this compactor with our parallel compaction, which is \nrun on n threads in parallel, when using an n-way machine. In order to compare between our algorithm \nand the original threaded algorithm, we sometimes restricted our algorithm to use a single thread. Non-movable \nobjects. Our platform is not type accurate. Although each .eld in the heap can be correctly identi.ed \nas a pointer or non-pointer, we cannot make the distinc\u00adtion between pointer and non-pointer .elds in \nthe stack and registers (the roots). Thus, some objects cannot be moved, and are left in their original \nlocation during the compaction. This complicates any compaction algorithm and speci.cally, forced us \nto deal with (normally irrelevant) technicalities. We do not get into the details in this paper. We just \nnote that the number of such objects is small (around 1% by our measurements) and thus we do not expect \nto see them in.uencing the compaction behavior. Furthermore, we com\u00adpare our algorithm to a base algorithm \nthat deals with the same problem, in a similar manner, and thus, there are no hidden advantages favoring \nour algorithm in the measured comparison. 7. MEASUREMENTS In this section, we present the results of \nimplementing our parallel compaction algorithm into the IBM production JVM. We start by describing our \ntest environment. We then compare the measurements of the new compaction algorithm to those of the threaded \nalgorithm previously used with the IBM production JVM. The threaded compaction algorithm was presented \nin [9, 12] and is also described in [8]. Next, we present measurements for our inter-block compaction \nthat trades compaction quality with time and space overhead (as described in Section 4 above). Finally, \nwe present the mea\u00adsurements for the extended two-.nger compaction algorithm that does not preserve the \norder of compacted objects. (This algorithm was described in Section 5 above.) 7.1 Platform and benchmarks \nOur measurements were taken primarily on a pSeries server, with eight 600 MHz PowerPC RS64 III processors \n(64-bit) and 5 GB of RAM running AIX 5.1. Measurements of the extended two-.nger compaction al\u00adgorithm \nwere done on an IBM Net.nity 7000 server, with four 550 MHz Pentium III XeonTM processors (32-bit) and \n2 GB of RAM, running Windows 2000. Measurements of our compaction on a Web server application were done \non an IBM xSeries 440 Server with four 2.0 GHz Intel XeonTM MP processors (32-bit, with Hyper-Threading \nenabled) and 3.5 GB of RAM, running Windows 2000. Benchmarks. We used three benchmarks: SPECjbb2000, \nTrade3,and the SPECjvm98 benchmark suite. SPECjbb2000 [14] is a Java business benchmark inspired by TPC-C. \nIt emulates a 3-tier system, concentrating on the middle tier. SPECjbb is throughput oriented: it measures \nthe amount of work (i.e., number of transactions ) done during a given time. The result is given in TPM \n(transac\u00adtions per minute). On an 8-way multiprocessor, an o.cial run of SPECjbb includes sixteen short \ncycles of two min\u00adutes each. The .rst cycle creates a single warehouse, and each successive cycle increases \nthe number of warehouses by one. Adding warehouses increases the amount of live ob\u00adjects. SPECjbb issues \na TPM score for each cycle, and a total score for the entire run. SPECjbb2000 initiates a forced GC (or \nSystem.gc() call [1]) before each cycle. With the IBM JVM 1.4.0, compaction oc\u00adcurs if a System.GC has \nbeen requested and the last GC did not compact [6]. So IBM JVM 1.4.0 initiates a compaction before each \nSPECjbb cycle. As each cycle increases the heap residency, this gives us an excellent opportunity to \ntest com\u00adpaction in di.erent heap conditions. On the other hand, throughput measurements are paused when \nthese compactions occur, so compaction time does not in.uence the SPECjbb2000 scores. Therefore, we provide \nmeasurements of the actual compaction times as well as the throughput. Another measurement we ran was \none that executes a compaction once every 10 or 20 garbage collections. That gave us an indication of \nthe e.ect of compaction on the throughput when both its bene.t to the order of the heap and its cost \nin time a.ects the throughput scores. Trade3 [16] is the third generation of the WebSphere [13] end-to-end \nbenchmark and performance sample application. This benchmark models an online stock brokerage applica\u00adtion \nand uses a real world workload, driving WebSphere s implementation of J2EE 1.3, Web Services, and other \nkey Web application server components. Trade3 execution in\u00advolves three components: a client application \nwhich gener\u00adate the workload, a database, and a Web Application Server. The database and the Web Application \nServer were installed on separate machines. The Web Performance Tool (WPT) was used to generate the workload. \nWPT is an HTTP en\u00adgine capable of simulating hundreds of HTTP clients. We used its default test driver \n(AKstress) and test script, but instead of simulating a single client issuing 2000 HTTP re\u00adquests, we \nhave simulated 300 clients and let it run for 30 minutes. We have measured the performance of the main \ncomponent (the Web Application Server), and relate to the JVM of this application in the rest of this \nsection. The performance metrics we display is requests per seconds. Trade3 is interesting for compaction \nresearch, as it creates a signi.cant amount of fragmentation in the Java heap. SPECjvm98 [15] is a benchmark \nsuite that measures com\u00adputer system performance for JVM client platforms. It con\u00adsists mostly of single-threaded \nbenchmarks that use rela\u00adtively small heaps (typically, less than 50 MB). We mea\u00adsured this benchmark \nsuite in order to provide some insight into the behavior of our compaction algorithm for small ap\u00adplications. \nAll benchmarks in this suite initiate a forced GC (and hence a compaction) before and after their measured \nperiod. One of the benchmarks (javac) also initiates a few compactions inside the measured period. Test \nrules. All measurements presented in this section are results averaged from at least .ve runs, except \nfor fragmen\u00adtation results, which were computed from one typical run. In order to test the compaction \nmechanism under a reasonably heavy load when running SPECjbb, we aimed at achieving a 60% heap residency \nat sixteen warehouses, and therefore used a 600 MB heap. For the same reason, Trade3 was run with a 180 \nMB heap. The entire SPECjvm98 suite was run with a32MB heap. 7.2 The main parallel algorithm We start \nwith the measurements of the main parallel algo\u00adrithm. We measure its e.ciency versus the original threaded \nalgorithm that was used with the IBM production JVM be\u00adfore this algorithm was incorporated. When evaluating \na compaction algorithm, the interesting parameters are: compaction time (i.e., the time it takes to compact \nthe entire heap), program throughput, com\u00adpaction quality, and (for a parallel compaction algorithm) \nthe speedup of the compaction on a multiprocessor. In this section, we present detailed comparisons of \nthe original collector (denoted the threaded compaction) with a single-threaded version of our new compaction \n(denoted parallel-restricted compaction), and with our parallel com\u00adpaction (denoted parallel compaction), \nwhen running SPEC\u00adjbb2000. Compaction times. Starting with compaction time, the running times of our \nparallel-restricted algorithm are com\u00adpared with the running times of the threaded compaction algorithm. \nFigure 4 shows the compaction times of each of the algorithms as a function of the number of warehouses, \nwhich implies the heap residency of the compacted heap. It is easy to see that the larger the size of \nthe live objects, the more saving we get when running the new compaction algo\u00adrithm. We stress that this \nis done without parallelism. Thus, 3000 2500 Time (ms) 2000 1500 1000 500 0 Threaded Parallel-restricted \nFigure 4: SPECjbb: Compaction time comparison between the threaded compaction and our new com\u00adpaction \n(single-threaded implementation) the superiority of our method is fairly measured against the original \nnon-parallel threaded compaction algorithm. When using parallelism, our method outperforms the original \ncom\u00adpacting algorithm by far. The advantage of the parallel algo\u00adrithm (even when executed by a single \nthread) is presented by the reduction in compaction time in Figure 5. 40% 30% do not report the maximum \ntimes, as the results are quite stable. In particular, for SPECjbb, the maximal compact time with the \nnew parallel compaction algorithm never ex\u00adceeds 6% above the average time. On the other hand, with the \noriginal threaded compactor the maximum compaction time never exceeded an increase of 12% above the average. \nThroughput. As mentioned above, the SPECjbb2000 does not take compaction time into account in its throughput \nmeasurements. Thus, an extremely long compaction would not make a di.erence in the throughput measurements. \nThe throughput measurements only measure the e.ect of the compaction on program e.ciency. As a sanity \ncheck, we measured the throughput with the new parallel compaction algorithm and compared it with the \nthroughput of the JVM when using the threaded compaction algorithm. The re\u00adsults indeed show no noticeable \ndi.erence and are presented in Figure 6. 90 80 TPM (thousands) 70 60 50 40 30 20 10 0 Parallel Threaded \n Figure 6: SPECjbb: Throughput comparison be\u00adtween the threaded compaction and the parallel compaction \nSpeedup. Finally, we check the speedup of the parallel com\u00adpaction when varying the number of threads \nrunning the compaction. We usedan8-way multiprocessor and ranthe compaction on two, four, six or eight \nthreads. The speedup computed is the compaction running time with a single thread divided by the compaction \ntime with several threads. The results are presented in Figure 7. It turns out that the scal\u00adability \nof this algorithm is high, yielding speedup factor of 2.0 on two threads, 3.7 on four threads, 5.2 on \nsix threads, Change and 6.6 on eight threads. 20% Compaction s effect on throughput. Running compaction \n10% poses a trade-o. to the JVM implementer. On one hand, the compaction improves the heap order, reduces \nfragmenta\u00ad0% tion and increases allocation e.ciency. On the other hand, compaction takes time. We measured \nthe throughput of the SPECjbb benchmark when additional compaction exe\u00ad Figure 5: SPECjbb: Compaction \ntime reduction with our new compaction (single-threaded imple\u00admentation) relative to the threaded compaction \nThe reported measure is the average compaction time. We cutions were forced on top of the original SPECjbb \ninvoca\u00adtions of compaction. We ran this measurement twice. Once, forcing compactions every 10 collections, \nand again, forcing compactions every 20 collections. This test was run with the threaded compaction, \nwith the restricted version of our Speedup factor with this benchmark. In our measurements of runs \nof Trade3, 8 the original IBM JVM 1.4.0 triggered compaction about ev\u00ad 7 ery 90 GC cycles. In addition, \nwe forced compaction every 6 20 GC cycles. The average increase in free space due to com\u00ad 5 paction \nwas around 8% of the size of the heap, for both the 4 threaded and parallel compaction algorithms. Without \nforc\u00ad ing compaction every 20 GC cycles, the fewer compactions 3 resulted in a higher increase in free \nspace; around 13% of the size of the heap, for both the threaded and the parallel 2 1 compaction algorithms \n 8 compacting threads 4 compacting threads 6 compacting threads 2 compacting threads Figure 7: SPECjbb: \nSpeedup comparison when run\u00adning the parallel compaction with di.erent number of threads Figure 8: SPECjbb: \nThroughput comparison. Forc\u00ading compaction every 10 and 20 collections with the threaded compaction, \nthe parallel-restricted (single threaded) compaction, and the parallel compaction (8 threads). All scores \nare compared with a normal run of threaded compaction parallel compaction that runs on only a single \nthread , and .nally, with the parallel compaction (on eight threads). Fig\u00adure 8 depicts the throughput \nchange relative to the regular execution (without additional compactions) of the threaded compaction. \n#10 denotes forcing a compaction every 10 col\u00adlections and #20 every 20 collections. We can see that \nthe threaded compaction is too slow to allow additional com\u00adpactions to yield an overall throughput improvement. \nAc\u00adtually, running the original threaded algorithm reduces the overall throughput by more than 8%. Only \nthe parallel com\u00adpaction is fast enough to improve the overall time when run every 10 or 20 collections. \nThe Trade3 benchmark. Fragmentation reduction is one of the major motivations for compaction [8]. In \nthe SPECjbb benchmark, fragmentation does not appear to be signi.\u00adcant, although forcing compaction frequently \nmay still im\u00adprove the throughput. The Trade3 benchmark creates more fragmentation in the Java heap and \nallocates large objects that require large consecutive heap spaces. Thus, the built\u00adin (conservative) \ntriggering mechanism triggers compactions Compaction Type Compact time (ms) Improvement over Threaded \ndefault trigger #20 trigger default trigger #20 trigger Threaded Parallel-restricted (single threaded) \nParallel 1698 1387 499 1671 1251 440 18.3% 70.6% 25.1% 64.8 % Table 1: Trade3: Compact times for the \nthreaded compaction, the Parallel-restricted, and the parallel compaction; with and without forcing compaction \nevery 20 GC cycles Compaction Type Requests per second Improvement over Threaded default trigger #20 \ntrigger default trigger #20 trigger Threaded Parallel-restricted (single threaded) Parallel 219.8 221.7 \n222.4 224.5 226.1 229.1 0.9 % 1.2 % 0.7% 2.0% Table 2: Trade3: Throughput for the threaded compaction, \nthe Parallel-restricted, and the parallel compaction; with and without forcing compaction every 20 GC \ncycles In what follows, we present measurements results for Trade3. In Tables 1-2 the threaded algorithm \nis compared with the restricted (single threaded) version of the parallel algorithm and with the (unrestricted) \nparallel algorithm running in parallel on all the processors on the 4-way xSeries 440 Server. Two measurements \nare reported. First, the compaction times, when the algorithms are run on the Java heap as induced by \nthe Trade3 benchmark. This relates to applica\u00adtion s pause times. Measurements were taken both on the \noriginal built-in compaction triggering mechanism (denoted default trigger), which executed compaction \nabout once ev\u00adery 90 GC cycles, and also when forcing additional com\u00adpactions every 20 collections (denoted \n#20 trigger). Second, we report the overall throughput of the application when us\u00ading each of these compaction \nmethods. In contrast to SPECjbb, in which compaction e.ort is not accounted for in the throughput measure, \nwe can see a throughput increase when the more e.cient compaction is used. The compaction time is reduced \neven when using our algorithm on a single thread. When it is run on all four Benchmarks Improve with \nImprove with 4% add. space restricted-parallel (threaded alg) (no add. space) SPECjbb #10 1.46% 3.76% \nSPECjbb #20 0.72% 2.33% Trade3 #20 -0.02% 0.73%  Table 4: Throughput improvement when adding 4% processors, \nthe speedup obtained is more than 2.8, which is smaller than the speedup observed on the PowerPC plat\u00adform. \nAs with SPECjbb, the compaction times did not vary too much. The maximum compaction time exceeded the \naverage compaction time by at most 22% with the paral\u00adlel compactor and by at most 14% with the threaded \ncom\u00adpactor. Thus, the new collector is still faster even on a single thread and even when considering \nthe maximum compaction time. Behavior with client applications. As a sanity check, we ran our compaction \nalgorithm with the SPECjvm98 bench\u00admark suite on a uniprocessor. In these benchmarks, there is a minor \nneed for compaction and usually no need for paral\u00adlelism. Nevertheless, our new algorithm behaves well \neven in this setting. We compared the compaction running times to those of the threaded compaction. The \nresults, which are presented in Table 3, re.ect the average running time of a compaction run towards \nthe end of the run of the bench\u00admark. On most benchmarks, the parallel compaction run\u00adning on a uniprocessor \ndoes better than the threaded com\u00adpaction. extra heap space to the basic collector (employ\u00ading the threaded \ncompaction), and throughput im\u00adprovement when replacing the threaded compaction with the new parallel \nalgorithm restricted to run\u00adning with a single thread. better compaction is the reduction in the pause \ntime, as compaction creates long pauses. In this respect, using a larger heap only increases the pause \ntime, whereas the new parallel algorithm reduces the pause times substantially (see Figures 4,5,and 7). \n 7.3 Trading quality for ef.ciency Next, we move to checking the inter-block compaction Compact time \n(ms) Total time (sec) Benchmarks Threaded Parallel Threaded Parallel compress jess db javac mtrt jack \nraytrace 28.3 19.2 14.6 21.2 16.0 80.8 15.7 21.1 19.1 13.9 24.4 16.3 77.8 15.9 16.7 12.0 22.4 24.5 7.8 \n15.1 7.5 16.6 11.9 22.3 24.4 7.9 14.0 7.5  variant proposed in Section 4. This variant is denoted inter\u00adblock \nin the .gures. We start by measuring the compaction times for this method. We expect to see lower running \ntimes and indeed this is the case, as shown in Figure 9. The im\u00ad 300 250 Time (ms) 200 150 Table 3: \nSPECjvm98: Compact times and total 100 times for both the threaded compaction and the par\u00adallel compaction \nWe also checked the in.uence of the compaction on the program throughput. As the heaps are relatively \nsmall, compaction did not signi.cantly in.uence the overall times. Thus, both JVMs gave similar throughput \nresults. Auxiliary space usage. Our algorithm uses three auxil\u00adiary vectors, which require 2.3% or 3.9% \nadditional space over the heap. To check whether this extra space buys us any of the improvements shown \nearlier, we tried to provide the threaded algorithm with 4% more heap space and check the improvement \nin throughput. The results are presented in Table 4. We did not include a normal run of SPECjbb since \ncompaction time is arti.cially not included in the mea\u00adsurement. SPECjbb #10 refers to a run of SPECjbb \nin which we forced additional compactions every 10 collections. SPECjbb #20 refers to forcing additional \ncompactions ev\u00adery 20 collections. Trade3 #20 refers to a run of Trade3 in which we force additional \ncompactions every 20 collections. As in all other runs, SPECjbb was run on a heap size of 600MB and Trade3 \nwas run on a 180MB heap. It turns out that adding more space to the heap does not yield the bene.ts of \na good algorithm that uses additional auxiliary space. Also, note that the main advantage of a 50 0 Inter-block \n Parallel Figure 9: SPECjbb: Compaction time comparison between the parallel compaction and its inter-block \nvariant provement in running times is presented in Figure 10 be\u00adlow. As one can see, the improvement \nis around 25%. All these measurements are done while running the compaction in parallel, in its typical \nform. It is interesting to check, in this case, how the throughput is in.uenced by the change. Because \nthe compaction qual\u00adity is reduced, this may a.ect the throughput of the pro\u00adgram. Our SPECjbb2000 measurements \nshow no through\u00adput change between the parallel compaction and its inter\u00adblock variant; di.erences, in \nall warehouses, are smaller than 0.25%. Finally, it is interesting to measure how much the com\u00adpaction \nquality was reduced by the inter-block variant. For each number of warehouses, we check the size of the \ncom\u00adpacted area of the live objects just after compaction. For 35% 28 24 30% 20 25% 20% 16  TPM (thousands) \nChange 15% 12 8 10% 4 0 5% 0% Threaded Extended-2-fingers Figure 10: SPECjbb: Compaction time reduction \nFigure 11: SPECjbb on a 4-way: Throughput com\u00adwith the inter-blockvariant relatively to the parallel \nparison between the threaded compaction and the compaction extended-two-.nger compaction Size limit After \nCompact After Compact +10 GCs Threaded 2-.ngers Threaded 2-.ngers 32 bytes 64 bytes 128 bytes 256 bytes \n0.0% 0.0% 0.0% 0.0% 0.2% 0.3% 0.3% 0.3% 1.2% 2.7% 3.2% 3.8% 1.4% 5.3% 8.6% 9.9% the inter-block variant, \nthis should be higher since some of the compaction was not performed. The increase in the size of the \ncompacted area is presented in Table 5. Warehouses W4 W8 W12 W16 Compacted area size (MB): parallel 107.7 \n199.7 276.5 351.5 Compacted area size (MB): inter-block 111.8 205.0 282.6 358.3 Compacted area size increase \n3.8% 2.7% 2.2% 1.9%  Table 6: SPECJbb on a 4-way: the percentile of free space in all chunks below given \nsizes, after com\u00adpaction and ten GCs later, for the threaded com\u00adpaction and the extended-two-.nger compaction \nTable 5: SPECjbb: Size (and increase in size) of the compacted areas for the parallel compaction and \nits inter-blockvariant, for selected warehouses  7.4 The extended two-.nger algorithm We now turn to \nthe extended two-.nger algorithm, de\u00adnoted extended-two-.nger in the.gures. Theextended two .ngers algorithm \ndoes not preserve the order of objects. The most interesting measure is how this in.uences the through\u00adput. \nAs compaction time is not accounted for, the through\u00adput di.erences do not re.ect the time it takes to \nrun a com\u00adpaction, but only the e.ect of ordering objects arbitrarily. The results, compared to the original \nthreaded algorithm are presented in Figure 11 below. It is easy to see that as the heap residency grows, \nthe throughput is dramatically reduced. We have also measured the distribution of the lengths of free \nchunks, to try and account for the throughput di.er\u00adence by fragmentation (i.e., the fact that free chunks \nbe\u00adcome smaller on average). We present these measurements in Table 6. In this table, we report the percentile \nof free space that resides in all chunks that are smaller than a given chunk size. We do that for a few \nchunk sizes. For example, the row labeled 64 bytes shows the percentile of free space that resides in \nall chunks smaller than 64 bytes. We compare the threaded compaction with the extended-two-.nger com\u00adpaction. \nMeasurements were taken immediately after the compaction that preceded the cycle with eight warehouses, \nand ten GCs later. The table shows that immediately after the compaction, the distribution of small \nchunks is similar in both com\u00adpaction methods. Yet, when using the extended-two-.nger method, the amount \nof small chunks increases much faster than when using the threaded method. We believe that this occurred \nbecause jointly allocated objects are also freed together. As the extended-two-.nger takes blocks of \nsuch objects and sprinkles them all over the heap, the cured frag\u00admentation reappears as these objects \nare freed. From the table it is clear that this phenomena happens, yet, it does not seem strong enough \nto justify all the reduced through\u00adput. We thus tend to believe that the main reason for the reduced \nperformance is the reduced locality of reference dur\u00ading the program run. We remark, without presenting \nthe actual table, that a similar e.ect is not seen with the inter-block method. There, as the order of \nobjects is maintained, the throughput is steady and the distribution of free chunks looks similar to \nthat of the parallel compaction.  8. CONCLUSIONS We have presented a parallel compaction algorithm ade\u00adquate \nboth for server SMPs as well as for client uniproces\u00adsors. Our algorithm is highly e.cient; it scales \nwell with good load balancing and provides a good quality of com\u00adpaction, as all objects are being pushed \nto the bottom of the heap, rather than being collected in several piles to al\u00adlow parallelism. This algorithm \nhas been implemented and measured on the IBM production JVM and has been incor\u00adporated into the production \nJVM due to its qualities. We further suggested a trade-o. between compaction qual\u00adity and overhead, demonstrating \nhow a somewhat reduced compaction quality can substantially improve performance. Finally, we have revalidated \ntraditional wisdom, by check\u00ading a variant of the two-.nger algorithm on the Java plat\u00adform. The two-.nger \nalgorithm does not preserve object ordering. We have con.rmed the wide belief that such com\u00adpaction hinders \nprogram e.ciency and eventually creates more fragmentation. 9. ACKNOWLEDGMENTS We thank Ori Ben Yitzhak, \nElliot (Hillel) Kolodner, Vic\u00adtor Leikehman (from the IBM Research Laboratory in Haifa), and Avi Owshanko \n(from the Dept. of Computer Science, Technion, Haifa) for useful discussions. We are particu\u00adlarly grateful \nto Sam Borman, Martin Trotter, and Robert Berry from the IBM Java Technology Center in Hursley, for their \ncollaboration and support. We are also grateful to Rob Geiner from the IBM WebSphere Performance group \nin Raleigh and Nikola Grcevski from the IBM Java JIT Perfor\u00admance in Toronto for their generous help \nin enabling some of the measurements included in this paper. 10. REFERENCES [1] java.lang class system. \n http://java.sun.com/j2se/1.3/docs/api/java/lang/System.html. [2] David F. Bacon, Perry Cheng, and V.T. \nRajan. A real-time garbage collecor with low overhead and consistent utilization. In Conference Record \nof the Thirtieth Annual ACM Symposium on Principles of Programming Languages, ACM SIGPLAN Notices, New \nOrleans, LA, January 2003. ACM Press. [3] David F. Bacon, Stephen J. Fink, and David Grove. Space-and \ntime-e.cient implementation of the java object model. In Proceedings of the 16th European Conference \non Object-Oriented Programming, pages 111 132. Springer-Verlag, 2002. [4] Ori Ben-Yitzhak, Irit Goft, \nElliot Kolodner, Kean Kuiper, and Victor Leikehman. An algorithm for parallel incremental compaction. \nIn David Detlefs, editor, ISMM 02 Proceedings of the Third International Symposium on Memory Management, \nACM SIGPLAN Notices, pages 100 105, Berlin, June 2002. ACM Press. [5] Sam Borman. Sensible Sanitation \n-Understanding the IBM Java Garbage Collector (Part 1: Object allocation). http://www.ibm.com/developerworks/ibm/library/i\u00adgarbage1. \n[6] Sam Borman. Sensible Sanitation -Understanding the IBM Java Garbage Collector (Part 2: Garbage collection). \nhttp://www.ibm.com/developerworks/ibm/library/i\u00adgarbage2. [7] Christine Flood, Dave Detlefs, Nir Shavit, \nand Catherine Zhang. Parallel garbage collection for shared memory multiprocessors. In Usenix Java Virtual \nMachine Research and Technology Symposium (JVM 01), Monterey, CA, April 2001. [8] Richard E. Jones. Garbage \nCollection: Algorithms for Automatic Dynamic Memory Management. Wiley, July 1996. With a chapter on Distributed \nGarbage Collection by R. Lins. [9] H. B. M. Jonkers. A fast garbage compaction algorithm. Information \nProcessing Letters, 9(1):25 30, July 1979. [10] Bernard Lang and Francis Dupont. Incremental incrementally \ncompacting garbage collection. In SIGPLAN 87 Symposium on Interpreters and Interpretive Techniques, volume \n22(7) of ACM SIGPLAN Notices, pages 253 263. ACM Press, 1987. [11] Martin Larose and Marc Feeley. A compacting \nincremental collector and its performance in a production quality compiler. In Richard Jones, editor, \nISMM 98 Proceedings of the First International Symposium on Memory Management, volume 34(3) of ACM SIGPLAN \nNotices, pages 1 9, Vancouver, October 1998. ACM Press. [12] F. Lockwood Morris. A time-and space-e.cient \ngarbage compaction algorithm. Communications of the ACM, 21(8):662 5, 1978. [13] WebSphere Software platform. \nIbm software products. http://www.ibm.com/software/info1/websphere/index.jsp. [14] SPECjbb2000 Java Business \nBenchmark. Standard Performance Evaluation Corporation (SPEC), Fairfax, VA, 1998. Available at http://www.spec.org/osg/jbb2000/. \n[15] SPECjvm98 Benchmarks. Standard Performance Evaluation Corporation (SPEC), Fairfax, VA, 1998. Available \nat http://www.spec.org/osg/jvm98/. [16] Trade3 Web Application Server Benchmark. IBM Software. Available \nat http://www.ibm.com/software/webservers/appserv/\u00adbenchmark3.html.  \n\t\t\t", "proc_id": "1028976", "abstract": "<p>We propose a heap compaction algorithm appropriate for modern computing environments. Our algorithm is targeted at SMP platforms. It demonstrates high scalability when running in parallel but is also extremely efficient when running single-threaded on a uniprocessor. Instead of using the standard forwarding pointer mechanism for updating pointers to moved objects, the algorithm saves information for a pack of objects. It then does a small computation to process this information and determine each object's new location. In addition, using a smart parallel moving strategy, the algorithm achieves (almost) perfect compaction in the lower addresses of the heap, whereas previous algorithms achieved parallelism by compacting within several predetermined segments.</p> <p>Next, we investigate a method that trades compaction quality for a further reduction in time and space overhead. Finally, we propose a modern version of the two-finger compaction algorithm. This algorithm fails, thus, re-validating traditional wisdom asserting that retaining the order of live objects significantly improves the quality of the compaction.</p> <p>The parallel compaction algorithm was implemented on the IBM production Java Virtual Machine. We provide measurements demonstrating high efficiency and scalability. Subsequently, this algorithm has been incorporated into the IBM production JVM.</p>", "authors": [{"name": "Diab Abuaiadh", "author_profile_id": "81100339417", "affiliation": "IBM Haifa Research Laboratory, Mount Carmel, Haifa, ISRAEL", "person_id": "P452698", "email_address": "", "orcid_id": ""}, {"name": "Yoav Ossia", "author_profile_id": "81100376653", "affiliation": "IBM Haifa Research Laboratory, Mount Carmel, Haifa, ISRAEL", "person_id": "PP14133338", "email_address": "", "orcid_id": ""}, {"name": "Erez Petrank", "author_profile_id": "81100377919", "affiliation": "Dept. of Computer Science, Technion - Israel Institute of Technology, Haifa, Israel", "person_id": "PP39040138", "email_address": "", "orcid_id": ""}, {"name": "Uri Silbershtein", "author_profile_id": "81100259054", "affiliation": "IBM Haifa Research Laboratory, Mount Carmel, Haifa, ISRAEL", "person_id": "P698423", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1028976.1028995", "year": "2004", "article_id": "1028995", "conference": "OOPSLA", "title": "An efficient parallel heap compaction algorithm", "url": "http://dl.acm.org/citation.cfm?id=1028995"}