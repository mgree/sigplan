{"article_publication_date": "10-01-2004", "fulltext": "\n Refactoring Class Hierarchies with KABA Mirko Streckenbach, Gregor Snelting Universit\u00a8at Passau Fakult\u00a8at \nf\u00a8ur Informatik, Innstr. 33, 94032 Passau, Germany strecken@infosun.fmi.uni-passau.de ABSTRACT gram \nbehavior is preserved) may involve non-trivial program anal\u00adysis. Another limiting factor is the fact \nthat refactoring is a manual KABA is an innovative system for refactoring Java class hierar\u00adprocess even \nwith tool support: the refactorer must know which chies. It uses the Snelting/Tip algorithm [13] in order \nto determine refactorings to apply and why. a behavior-preserving refactoring which is optimal with respect \nto In this paper, we present a different approach to refactoring. We a given set of client programs. \nKABA can be based on dynamic assume that a hierarchy is given together with a set of client pro\u00adas well \nas static program analysis. The static variant will preserve grams using this hierarchy. We generate \na refactoring proposal program behavior for all possible input values; the dynamic version automatically, \nand this proposal is based on the usage of the hi\u00adguarantees preservation of behavior for all runs in \na given test suite. erarchy by the client programs. The refactoring is guaranteed to KABA offers automatic \nrefactoring as well as manual refactoring be semantically equivalent to the original program (with respect \nto using a dedicated editor. client behavior). In this contribution, we recapitulate the Snelting/Tip \nalgorithm, The transformed hierarchy can then be subject to further man\u00adpresent the new dynamic version, \nand explain new extensions which ual refactorings, while preservation of semantics is still guaran\u00adallow \nto handle full Java. We then present .ve case studies which teed. Eventually, code can be generated. \nThe new code contains discuss the KABA refactoring proposals for programs such as javac the same statements \nas the original code, except that the hierarchy and antlr. KABA proved that javacdoes not need refactoring, \nhas changed and for all variables a new type (i.e. class) has been but generated semantics-based refactoring \nproposals for antlr. computed.Categories and Subject Descriptors: D.2.7 [Software Engineer- Preservation \nof semantics is achieved by a combination of pro\u00ading]: Distribution, Maintenance, and Enhancement Restructur\u00ad \n gram analysis, type constraints and concept lattices. But the true ing, reverse engineering, and reengineering; \nD.3.3 [Programming value of the technique lies in the possibility to automatically refac-Languages]: \nLanguage Constructs and Features Classes and ob\u00ad tor with respect to a given purpose represented by \na given set of jects; Inheritance; F.3.2 [Logics and meanings of Programms]: Se\u00ad client programs. For \nthe given clients, the refactored hierarchy is mantics of Programming Languages Program analysis optimal \nin the sense that every object contains only those methods General Terms: Algorithms, Languages or .elds \nit really accesses. In fact, we determine the most .ne\u00ad grained refactoring which still preserves behavior \nof all clients. Keywords: Refactoring The refactoring editor KABA1 offers two variants of this ap\u00adproach. \nThe static approach requires static program analysis and 1. INTRODUCTION guarantees behavior preservation \nfor all analyzed client programs. Refactoring transforms a given class hierarchy in order to im-The dynamic \napproach requires dynamic program analysis and guar\u00adprove its structure and evolution. Refactoring and, \nmore generally, antees behavior preservation for all client runs of a given test suite. program transformation \nhas been a popular research topic for some Of course, the static analysis is a conservative approximation \nof any time, and has recently gained much interest due to the emergence of dynamic analysis. KABA also \noffers semantic support for man\u00adlight-weight design methodologies such as Extreme Programming ual refactoring, \nintended not only for post-processing an automatic [2] that advocate continuous refactorings. The book \nby Fowler [5] refactoring, but for refactoring any class hierarchy. presents a comprehensive list of \nrefactoring transformations which has been implemented in some refactoring tools. Organization of this \npaper But automated generation of refactoring proposals is still in its The primary purpose of this contribution \nis the presentation of the infancy. One of the key limiting factors is the fact that verifying the innovative \nKABA refactoring system, as well as the discussion of preconditions for many refactorings (in order to \nensure that pro-case studies for its application. But in order to make KABA work for full Java, the original \nrefactoring algorithm had to be consider\u00adably expanded. We therefore have chosen a non-standard organization: \nthe main Permission to make digital or hard copies of all or part of this work for paper presents KABA \nfrom an application-oriented, software engi\u00ad personal or classroom use is granted without fee provided \nthat copies are neering view. All technical and algorithmic details and innovations not made or distributed \nfor pro.t or commercial advantage and that copies are presented in a series of appendices. bear this \nnotice and the full citation on the .rst page. To copy otherwise, to republish, to post on servers or \nto redistribute to lists, requires prior speci.c permission and/or a fee. 1KABA stands for KlassenAnalyse \nmit BegriffsAnalyse (class OOPSLA 04, Oct. 24-28, 2004, Vancouver, British Columbia, Canada. analysis \nby concept analysis). KABA is also a popular chocolate Copyright 2004 ACM 1-58113-831-8/04/0010 ...$5.00. \ndrink in Germany.  2. THE REFACTORING ALGORITHM In this section, we describe the Snelting/Tip algorithm \nfrom a user s viewpoint. The algorithm has been described in detail in [13]. Appendix 1 presents a more \ntechnical recapitulation of the most important steps of the algorithm, as well as technical refer\u00adences. \n2.1 Basic steps of the algorithm 2.1.1 Collection of member accesses The .rst analysis step collects \nall .eld and method accesses in the given source hierarchy and its client set. The algorithm sets up \na table, where the rows are labeled with variable names (includ\u00ading parameters, this-pointers etc), and \nthe columns are labeled with .elds and methods from the hierarchy. An access o.m() from an object will \nlead to a table entry for (o, C :: m) (where C is the static class for m). There are two variants of \nthe collection process, one based on dynamic program analysis and one based on static program anal\u00adysis. \nIn the dynamic variant, the JVM is instrumented such that every call o.m() at runtime leads to a corresponding \ntable entry. In the static variant, points-to analysis is used to approximate the effects of dynamic \ndispatch. For an object reference x, its points-to set pt(x)= {o1,...,on} is used to collect possible \nmethod calls by x.m(): a table entry is generated for any (oi,C :: m). 2.1.2 Incorporation of type constraints \nIn order to guarantee preservation of behavior, a set of type con\u00adstraints is extracted from the source \nwhich must also be respected in the refactored hierarchy. For example, every assignment a= b; in the \noriginal source requires that type(a) = type(b), and this must hold in the refactored hierarchy as well. \nFurthermore, if A = B are classes which both de.ne a method m, the constraint type(A :: m) = type(B :: \nm) must sometimes be retained in order to avoid ambiguous accesses in the refactored hierarchy. Once \nall type constraints have been extracted, they are incorpo\u00adrated into the table from phase 1: more entries \nare added, until a minimal table is obtained which respects all constraints. It is not obvious that this \nis always possible, and even less obvious that these constraints are enough to guarantee preservation \nof behavior. The details of constraint generation and incorporation are quite complex (see appendix 1). \n 2.1.3 Generation of concept lattice Tables are nice, but the true reason for extracting a table from \nthe source is that a class hierarchy can be generated automatically from the table using the well-known \nmethod of mathematical con\u00adcept analysis. Concept analysis (see appendix 1) generates a lat\u00adtice from \nthe table, which represents exactly the same information as the table, but organized into a completely \ndifferent, hierarchi\u00adcal view. Concept lattices are natural inheritance structures: every lattice element \nrepresents a class, and common .elds or methods are factored out into super-classes. The refactored hierarchy \nis thus obtained as a concept lattice generated from the .nal table. The statements in this refactored \nhierarchy are the same as in the original hierarchy. But every variable or object obtains a new type, \nand this type (i.e. refactored class) will contain all .elds and methods needed by the variable. More \nprecisely: every object that might be generated at runtime has a new type containing all meth\u00adods and \n.elds it may access at runtime; while methods or .elds not accessed by an object resp. variable are not \nmembers of its new type. Thus the original lattice provides .ne-grained insight into the member access \npatterns of all objects and variables a feature very valuable for program analysis and understanding. \n 2.1.4 Simpli.cation of concept lattice For practical refactoring purposes, the lattice must be simpli.ed \nin order to be useful. Typically the lattice size can be condensed by 80% without affecting preservation \nof behaviour. The user may contribute background knowledge during lattice simpli.cation in order to control \nthe structure of the .nal refactoring. Even the simpli.ed lattice may contain multiple inheritance.2 \nAs an option, automatic elimination of multiple inheritance can be ap\u00adplied (see appendix 1). \"From a \nsoftware engineering viewpoint, the resulting .nal refactorings are the most useful ones.  2.2 Properties \nof the algorithm The algorithm has several outstanding properties, which can be summarized as follows: \n it generates a refactoring proposal automatically;  it guarantees preservation of behavior (for all \npossible runs in the static variant, for the given test suite in the dynamic variant);  it refactors \nwith respect to a given purpose, that is, with re\u00adspect to a given set of clients;  it thus implements \na certain form of program specialization but as the number or size of clients grows, the refactorings \nwill be more general in nature;  it is optimal with respect to member distribution: all objects can \naccess only members they really need (thus they usually become smaller);  it identi.es dead methods \nor .elds as a by-product;  it provides two levels of granularity: the raw lattice, acting like a spectral \nanalysis tool allows remarkable insight into program behavior; the simpli.ed lattice provides practical \nrefactoring proposals.  We will illustrate the algorithm by a small example in the next subsection, \nand will discuss various case studies later in the paper. Right here, we would like to point out an important \nobservation: The Snelting/Tip algorithm is not a software engineer. It is an anal\u00adysis tool which shows \nwhat can be done without destroying behav\u00adior. KABA refactorings are proposals, and the software engineer \ndecides about their application. 2.3 An example As a simple example, consider the program in .gure 1. \nB, being a subclass of A, rede.nes f() and accesses the inherited .elds x, y. The main program creates \ntwo objects of type A and two objects of type B, and performs some .eld accesses and method calls. Figure \n2 presents the KABA refactoring proposal, that is, the simpli.ed concept lattice (the intermediate tables \nas well as the raw lattice are shown in appendix 1; in fact the simpli.ed lattice is only a partial order). \nLattice elements are the classes of the new hierarchy. They are marked with class members above, and \nwith variables or objects below. The members above an element (i.e. a new class) de.ne the new class \nmembers; variables below an ele\u00adment will obtain this element as their new type. 2The Snelting/Tip algorithm \nwas originally designed with C++ in mind, where occasional multiple inheritance does not pose a prac\u00adtical \nproblem. class A { intx, y,z; void f() { class Client { y= x; public static void } main(String[] \nargs) { } A a1 = new A(); // A1 A a2 = new A(); // A2 class B extends A { B b1 = new B(); // B1 \nvoid f() { B b2 = new B(); // B2 y++; } a1.x = 17; void g() { a2.x = 42; x++; if (...){a2 =b2; \n} f(); a2.f(); } b1.g(); void h() { b2.h(); f(); } x--; } } } Figure 1: A small example program \n Figure 2: KABA refactoring for .gure 1 The refactoring basically reacts to the different member access \npatterns of the objects in the program. Typically, a class is split into new unrelated classes if there \nare objects which access one subset of its members, and other objects which access another, disjoint \nsubset of its members. New subclasses and inheritance relations are introduced if there are objects accessing \nonly a subset of a mem\u00adber set accessed by other objects. In the example, we observe the following: \nThe two objects of original type B have different behavior, as one calls g and the other calls h. Therefore, \nthe original B class is split into two unrelated classes.  The two objects of original type A have related \nbehavior, as A2 accesses everything accessed by A1, plus A.f(). There\u00adfore, the original A class is split \ninto a class and a subclass.  A1 does only contain A.x and not A.y. A.z is dead any\u00adway, as it does \nnot appear in the refactored hierarchy. Thus objects become smaller in general, as unused members are \nphysically absent in objects of the new hierarchy.  One might think of simplifying even further by merging \nthe two topmost elements in .gure 2, but that would make A1 bigger than necessary by including A.y as \na member. It is the refactorer s deci\u00adsion whether this disadvantage is out-weighted by a simpler lattice \nstructure. If so, the refactoring editor must guarantee that behavior of all clients is still preserved \nafter simpli.cation.  3. THE KABA SYSTEM KABA is an implementation of the approach described so far. \nKABA currently consists of four components: the static analysis, the dynamic analysis, the class hierarchy \neditor and the byte-code transformation tool KRS. 3.1 Program analysis The static analysis can handle \nfull Java byte-code and includes a full points-to analysis. Intraprocedural points-to analysis is .ow\u00adsensitive \nand can be parametrized to be context-and object-sensitive.3 Stubs are necessary to simulate the behavior \nof native methods. Stubs are provided for the most commonly used native functions of JDK 1.3, additional \nstubs can be added easily. The analysis can handle re.ection features like the .classoperator precisely, \nand uses a heuristic if a program loads classes with Class.forName. The techniques used to deal with \nthis aspects of re.ection could be extended to handle all of re.ection. Although mainly tested with class \n.les generated by javac and jikes, KABA should be able to handle other Java and non-Java com\u00adpilers as \nwell. Besides the class .les, the analysis needs one or more starting methods (main). All code reachable \nfrom these methods is included in the analysis. The main limitation of the analysis is its memory requirement. \nWith 2GB of memory programs like javac (as of JDK 1.3, 28000 LOC) can be analyzed. Unfortunately 2GB \nis a technical barrier not easily overcome on most systems. The dynamic analysis consists of the JVM \nKaffe4, whose byte\u00adcode interpreter has been modi.ed to track all member accesses during program execution. \nIt supports the same amount of re.ec\u00adtion as the static analysis. No stubs for native programs have been \nprovided because experience with the static analysis demonstrated that stubs are important for correct \ncontrol .ow, but not for mem\u00adber accesses. For the instrumented JVM, control .ow is correct without stubs. \nThe output from different program runs are merged into the ta\u00adble format used by the static analysis. \nWe have not observed any limitations in terms of program size, however general limitations of automated \ntesting apply.  3.2 The refactoring editor The KABA editor computes the (raw or simpli.ed) concept lat\u00adtice \nand displays it graphically. Figure 3 presents the lattice for .gure 1 in form of a KABA screen shot. \nEvery box represents one class, its name is printed in bold font in the center (nodes contain\u00ading only \nmembers from the same original class C are named C. n; users have the option to manually rename classes). \nMembers are displayed above the class name, variables below it. To reduce the screen space requirements, \nattributes and objects are not displayed by default; little arrows next to the class name allow to expand \nthem if necessary.5 In contrast to the simpli.ed lattice in .gure 2 this screen shot features all the \nugly details necessary in practice: full method sig\u00adnatures, constructors, and unique object identi.ers. \nAlso noticeable are two different displays of methods, as method names are pre.xed by either dcl or def. \nThe dcl represents an abstract declaration of a method, whereas a def represents the implementation (see \nAp\u00adpendix 1). In the example, a method s dcl and def are always at 3By default, context-sensitive analysis \nis used only for object con\u00adstructors, while object-sensivity is used for the Java collection classes. \nThese parameters can be tuned individually to achieve best performance. 4http://www.kaffe.org/ 5We plan \nto use UML notation in the future, but right now there is no UML layouter satisfying KABA s needs.  \nthe same node, but this need not be the case if interfaces or abstract methods are present. Additional \nviews help users to browse and refactor the hierar\u00adchy. The .rst view (.gure 4 upper) shows what happened \nto an original class. It shows all members and their new home classes , making it easy to see how a class \nwas restructured. Additional util\u00adity functions to mark certain members are also provided. The second \nview (.gure 4 lower) gives an overview of a new class. In addition to members located in the class itself \nit includes members from parent classes. KABA displays all members of a class; for inherited members, \ntheir new home class is given. For every object shown in the class hierarchy, the source code of its \ncreation site can be seen on mouse click. Besides browsing, the user may modify the class hierarchy. \nThe following basic operations are provided: Attributes and objects can be moved in a cut-and-paste \nfash\u00adion. Any number of them can be selected (cut) and moved to any class (paste).  A class can be split \ninto two. Incoming edges are attached to the .rst new class, outgoing edges are attached to the sec\u00adond, \nand the second inherits from the .rst. All members are moved to the .rst, all variables to the second \nclass; they may be redistributed later.  Two classes can be merged into one class, and a class can be \nmade a subclass of another class.  These operations should be suf.cient to refactor the class hier\u00adarchy \nin every possible way. Additional convenience functions are provided to make common tasks more easy (e.g. \napplication of the lattice simpli.cation algorithm). Modi.cation of the class hierarchy is only allowed \nif it will not affect client behavior. The user is given detailed feedback, if a cer\u00adtain refactoring \nis not allowed. Figure 5 shows an example: in order to merge classes A 1and A 6from .gure 3, the user \ntried to move the pointer a1 from class A 1 to the subclass. KABA refused as Figure 4: Browser for original \ntypes (upper); Browser for class content (lower). Figure 5: KABA reaction to an illegal refactoring \nA 1also contains an object (A2in terms of .gure 2) and there is an assignment between the pointer and \nthe object in the program: the proposed pointer move would make this assignment type-incorrect. It would \nhowever be valid to move pointer and object to the sub\u00adclass in one step or just to merge the two classes. \nAs refactored hierarchies can contain multiple inheritance, the KABA editor will (on demand) mark classes \nwhich really inherit method de.nitions from different superclasses6. It offers to move method de.nitions \nfrom one of these superclasses further up in the hierarchy, thereby eliminating multiple inheritance. \n6Note that the type constraints prevent that the same method is in\u00adherited from different superclasses \n 3.3 Code generation The .nal part of KABA is the byte-code transformation tool KRS [12]. Using transformation \ninstructions generated by the editor, it transforms the original byte-code into a new set of classes \nmatching the new hierarchy. The byte-code can be run directly or fed into one of the various byte-code \ndecompilers to regain Java source code. Code generation consists of two major tasks: .rst, all .elds \nand methods must be reordered according to the new class hierarchy. The second task is more subtle: Every \nclass name in the program must be replaced by a new one. This affects new expressions as well as types \nof local variables, method parameters, .elds, ins\u00adtanceof operators, type casts and exception handlers. \nFor re\u00ad.ection calls, parameters passed to certain methods are modi.ed to match the new class names. \nIn addition to the type changes, all dead code must be removed as it becomes non-typeable in general. \nCurrently only the static analysis provides all the information required for code generation, as the \ndynamic analysis omits infor\u00admation about pointers. 3.4 Application of KABA KABA is most useful when \na library is analyzed together with all clients using this library. It guarantees preservation of behavior \nfor the clients (static analysis) or all test runs (dynamic analysis): for any input, visible program \noutput remains the same.7 Client code which was not analyzed by KABA may still work with the refactored \nversion, but there is no guarantee. It is however possible to automatically check for preservation of \nbehavior. For both variants of the analysis, the full byte code must be avail\u00adable. If some methods are \nimplemented as native methods, stubs are necessary which emulate member access in the same way the native \nmethods would. The static analysis already contains stubs for the most commonly used native methods of \nJDK 1.3. The user must decide which code is subject to refactoring (user code) and which part must remain \nunchanged (library code). Usu\u00adally the analyzed program is user code and the classes from the JDK are \nlibrary code. If a program requires third party libraries, they are most likely to be included in the \nlibrary code. Library code must be self-contained, it may not contain references to the user code. Once \nthe analysis is complete, the KABA editor can be used to browse and modify the transformed class hierarchy. \nIf the static analysis was used, the original byte code can be automatically trans\u00adformed into code conforming \nto the refactored classes hierarchy.  4. CASE STUDIES We have applied KABA to several real world programs. \nWe use a condensed view for the resulting hierarchies in the .gures be\u00adlow. Nodes will only contain the \nnames of the original classes from which it contains members. Numbers at the left hand side of a node \nare used to reference a speci.c class. The right hand side contains two numbers: the number of pointers \n(upper half) and the number of objects (lower half) who have that speci.c type. Common pack\u00adage names \nare abbreviated ... . The presented hierarchies are not the raw lattices, but have been processed with \nthe algorithm from appendix 1 to reduce their complexity without changing behavior. Interfaces consisting \nonly of constants are omitted. 4.1 The javac compiler Our .rst example is the Java compiler from Sun \ns JDK 1.3.1. It was analyzed using the dynamic analysis. The Java library itself 7As objects become smaller, \nKABA may act as a space optimizer; as the number of classes usually increases, some runtime overhead \nmay be introduced. Both effects have not yet been evaluated. was used as test suite, creating 1878 individual \ncompiler runs. All refactorings shown below are guaranteed to preserve at least the behavior of these \n1878 test cases. 4.1.1 The symbol table Figure 6 presents a speci.c sub-hierarchy from javac, namely \nthe symbol table. The topmost symbol table class is Symbolwith 4 subclasses, 3 of them have further subclasses.8 \nFor this case study, the rest of the compiler is considered to be client code. Figure 7 shows the KABA \nrefactoring of the symbol table. The overall structure of the hierarchy is not affected by the transfor\u00admation: \nthere is still one top class with four subclasses. In two cases even their subclasses remain unchanged \n(2 , 3 , 15 ). The class 4 has some of is members moved into new subclasses 11 and 17 . Class 11 also \ncontains members that were previously in the top class. Hence these examples illustrate automatic extract \nclass and move member / move method refactorings. Something similar happens to the two subclasses of \n6 . They are split into a hierarchy containing members of the same origi\u00adnal class. Additionally members \nof the former top class are moved down. For these members multiple inheritance is introduced, as they \nare used by different subclass branches, but not in their com\u00admon ancestor class. Only multiple inheritance \ncan visualize this phenomenon in the hierarchy. But for a practical Java refactoring, multiple inheritance \ncan be easily removed by merging class 8 and 13 with their superclass 6 . The Result can be seen in .gure \n8. In general, refactored objects become smaller. The original class Symbol had 27 members, only 14 are \nleft in class 1 . Thus the number of members in the top class was reduced by about 50 per\u00adcent. The number \ncould be further reduced if the constructor would not initialize all data members. Let us now discuss \nthe KABA refactoring proposal from a soft\u00adware engineering viewpoint. Even after removal of multiple \ninheri\u00adtance, new subclasses are proposed in the KABA refactoring. Intro\u00adducing new subclasses will in \ngeneral improve information hiding, as certain secrets are moved into less-visible subclasses. Con\u00adsidering \nclass cohesion, KABA will introduce new classes only if their methods are executed together and thus \nimproves functional cohesion. But the source code may provide other arguments against splitting classes. \nRemember: KABA shows what can be done with\u00adout affecting behavior, but is not a software engineer. The \nKABA editor is used to modify a refactoring proposal according to soft\u00adware engineering criteria. In \nthe example, the new subclasses are certainly justi.ed from a software engineering viewpoint, as functional \ncohesion is much improved. But the more fundamental insight from this case study is that KABA basically \nreproduced the overall hierarchy structure. We therefore conclude that the original javac design was \nquite good. This example demonstrates that KABA can also be used as a design metrics: it is good if the \noriginal hierarchy is more or less reproduced, it is bad if the original hierarchy is completely refactored. \n 4.1.2 The tree visitor The second example from javacis shown in .gure 9; it is the tree visitor sub-hierarchy. \nThe refactored KABA version of the sub\u00adhierarchy (.gure 10) is completely unchanged. However the move \nmember / move method refactoring has been applied: several members have been moved into subclasses. The \nnew class 1 con\u00adsists of only 3 methods, 6 other methods from the original class are 8Symbol$VarSymbol \nindicates this class is an inner class of Symbol. An inner class may be a subclass of its outer class. \nObject  Object  10 ...code.Symbol ...code.Symbol$ClassSymbol 1  Figure 7: KABA refactoring for .gure \n6 Object  Figure 8: Final refactoring for .gure 6 after automatic elimination of multiple inheritance \nObject Figure 9: Original class hierarchy for javactree visitor Object distributed to classes 8 ,9 \n,10 and 11 , while 28 methods are pre\u00adsumed dead. As this is a very high number, we manually inspected \nthe code and found that the original class is an abstract visitor with one method overloaded for 36 different \nsyntactical units. But only 8 of them are actually called, the used versions are usually over\u00adwritten \nin a subclass. If the dead (!) code is removed, the program produces a run-time error! This design seems \nquestionable. Having dead code for future extensibility may be ok, but having dead code which breaks \nthe program if removed, is a pitfall for programmers not fully familiar with the code. The visitor pattern \nshould be implemented in such a way that removing dead code produces a compile-time error instead of \nthrowing an exception. In general, it must be decided individually weather the inclusion of dead code \nfor future extensibility is a good idea. In some cases it will cause maintenance problems, as unused \ncode is especially hard to maintain. In other cases omitting it may prevent all future extensions (e.g. \nremoving the only public or protected constructor). Thus KABA replicated the given hierarchy, assuring \nthat the original design was basically good. But KABA still uncovered a minor design problem, as KABA \nidenti.es dead members as a by\u00adproduct.  4.1.3 The syntax tree Figure 11 shows the abstract syntax tree \nsub-hierarchy. It is ex\u00adactly reproduced by KABA, even without automatic elimination of multiple inheritance. \nThe top class contains just data members and access methods for these members. The subclasses represent \nthe 36 syntactical units mentioned in the previous example. The exact hierarchy reproduction shows that \nthe members are used in the same way by all subclasses. Hence cohesion for this particular class is high. \nThis indicates a good design which should be left unchanged. KABA con.rms that javac was designed by \nexperienced software engineers.    2 ...Token ...CommonToken 1  3 ...CommonToken 7 5 ...Token 1 \n Figure 16: Fine-grained refactoring for .gure 15 (upper); .nal refactoring after removal of multiple \ninheritance (lower). 4.2 The antlr parser generator Our next case study is antlr, a popular parser generator. \nFor the dynamic analysis, all example grammars from the antlrdis\u00ad tribution were used, creating 84 test \nruns. The antlr Java run\u00adtime classes were not analyzed.  4.2.1 First Example Figure 12 shows part \nof the Antlr original hierarchy and .gure 13 the .ne-grained KABA refactoring. It is obvious that this \nhierarchy is much more changed than any of the javacexamples: 1. 6 out of 20 classes have their members \ndistributed to more than one class; most noticeable AlternateBlock, which was split into 9 classes, AlternativeElement(split \ninto 6) and GrammarElement(split into 5). 2. The original hierarchy has GrammarElementand Alter\u00adnativeElement \nas super classes of all other classes. In the new hierarchy a topmost class still exists (1 ), but it \ncon\u00adtains members from both classes. This indicates that the dis\u00adtinction between the two classes is \nredundant. 3. Originally AlternativeBlockwas a subclass of Alter\u00adnativeElement. This relation is weaker \nin the new hier\u00adarchy, as a lot of members from AlternativeElement are no longer contained in the classes \nwhich have members from AlternativeBlock. This indicates that the inher\u00aditance between these two classes \nis a candidate for further inspection. 4. The isolated class 23 contains only a static member. As static \nmembers are not in.uenced by member accesses, they appear as individual nodes in the new hierarchy and \nmay be manu\u00adally moved to any other class.  The .ne-grained refactoring proposal looks pretty complex \nand is not realistic anyway, as it contains a lot of multiple inheritance. Still, KABA already demonstrated \nthat the natural design from .gure 12 does not stick to the principle of functional cohesion, and that \nrefactoring as sketched in items 1.-4. can be done without affecting behavior. Figure 14 shows the result \nof automatically removing multiple inheritance as described in appendix 1. Now the refactoring looks \nmuch more convincing! Note that preservation of behaviour is still guaranteed; items 1.-4. above still \napply, but the overall structure is much more similar to the original hierarchy. But classes have been \nmerged or splitted, new subclasses have been introduced, and members have been moved. The result is a \ndramatic increase in functional cohesion. 4.2.2 Second Example, Dynamic Variant For our second example, \nthe original hierarchy is shown in .gure 15, the refactored version can be seen in .gure 16. The original \nhi\u00aderarchy consists of 3 classes, but the .ne-grained refactoring triples this number: 1. No members \nof the original class CommonHiddenStream-Token are in the refactored version, revealing the original \nclass to be dead code. 2. Some members of the former top class Token were moved into subclasses 7 and \n8 . Manual inspection reveals that these members are methods called getTypeand setType, which are accessor \nfunctions for a data member type. This indicates some functionality of Tokenconcerning typemay be moved \nto a subclass. typeitself however is contained in class 1, as it is accessed by the default constructor. \n The multiple inheritance introduced by KABA can be removed by collapsing the diamond of classes 4 , \n3 , 2 and 5 . As a result, the refactored hierarchy will be a chain just like the original hierarchy, \nbut more .ne grained (see .gure 16). Thus the KABA refactoring improves locality and cohesion.  4.2.3 \nSecond Example, Static Variant The .ne-grained class hierarchy created by the static analysis can be \nseen in .gure 17. The hierarchy has more than thrice as many classes as the dynamic variant. It is not \nuseful as a practi\u00adcal refactoring, but presents a detailed spectral analysis of true object behaviour. \nThis feature is valuable for various program un\u00adderstanding tasks. In particular, the static analysis \nis useful in ar\u00adeas where automated testing is dif.cult or the transformed code is needed. The static \nvariant also generates refactoring proposals for interfaces which is not possible with the dynamic analysis. \nFor example, 10 classes are not below Object. While in Java everything is a subclass of Object, in KABA \nthis need not al\u00adways be the case. Classes not below Object may contain data  Object 214 3   objects \n3 1 1 5 1 1 1 1 1 1 dynamic 1 1 3 5 5 5 6 8 9 10 static 22 23 20 20 32 33 11 36 34 35 Figure 19: Position \nof objects in the dynamic and static analysis members, but are never instantiated, as an instance would \nrequire access to the constructor of Object. Non-instantiated classes are good candidates for the create \ninterface refactoring. The static variant, being a conservative approximation, contains some additional \nlattice elements due to imprecision of the underly\u00ading points-to analysis. But the main reason for the \nhigher number of classes in the static refactoring is the inclusion of pointers (which are left out by \nthe dynamic analysis). Often, lattice elements are created just because pointers of the same original \ntype access differ\u00adent member subsets, even though this does not hold for the objects pointed to. This \nis the reason that including pointers is interesting for program understanding, but not for practical \nrefactoring. In order to compare precision of the static and dynamic analysis, Figure 19 correlates individual \nobjects for the static and dynamic analysis. For example the .rst column means that 3 objects which are \nlocated in class 1 in the dynamic analysis are in class 22 in the static analysis. Remarkably, there \nis an example where the dynamic analysis is more detailed (columns 3 and 4; these objects are in the \nsame class in the static analysis and in different classes in the dynamic analysis) as well as examples \nwhere the static analysis is more detailed (columns 4 to 6). This means that the level of detail of the \nrefactored class hierarchy does not always stem from static vs. dynamic analysis, but may come from the \nanalyzed code itself. In rare cases, the conservative approximation generates artifacts, in particular \nif re.ection is used. For example, the static refac\u00adtoring contains members of CommonHiddenStreamToken \nin class 37 , which were presumed dead by the dynamic analysis. It seems that the static analysis includes \ncode which was not covered by the test suite of the dynamic analysis. But this is not the case! Class \n37 (as well as 14 and 12 ) are results of the approximation of Java s re.ection capabilities. antlruses \nre.ection, but not within classes considered in this example. Thus the inclusion of members from class \n37 is an artifact of statically approximating re.ection. The .nal static refactoring, based on agressive \nsimpli.cation of .gure 17, can be seen in .gure 18. It is even more coarse-grained than the dynamic version. \nIn fact, the static refactoring can be ob\u00adtained from the dynamic variant by merging classes 1/2 resp. \n5/6, which preserves behaviour. It is the engineers task to decide which refactoring is more appropriate. \nIn any case, the original distinction between Token and CommonToken was not designed properly, and KABA \nshows how to improve the hierarchy.  4.3 Discussion We have seen that KABA can distinguish designs which \nrespect actual member access patterns (and thus have high cohesion and good locality) from designs where \nthis is not the case; in the latter situation, KABA provides practically useful proposals for refactor\u00ading. \nFor practical refactorings, aggresive lattice simpli.cation and automatic elimination of multiple inheritance \nmust be used. The case studies have shown that the .nal refactorings de.nitely im\u00adprove the quality of \nthe design. Still, the .ne-grained lattice offers another important KABA fea\u00adture: the possibility to \nobtain .ne-grained insight into member ac\u00adcess patters. KABA can act as a spectral analysis for a hierarchy, \ntelling the engineer what the objects really do. Fine-grained analy\u00adsis is also useful if the KABA lattice \nis used as a quality metrics for old or new programs: simple lattices are better than complex ones; lattices \nreplicating the original design are better than lattices intro\u00adducing many new classes and inheritance \nrelationships. Whenever KABA proposes a refactoring which substantially differs from the manual design, \nclasses and objects do not stick to the principle of functional cohesion. During development of new code, \ndesigners can react to this lack of software quality by applying some or all KABA refactorings. Let us \n.nally repeat a fact which was mentioned before: a KABA refactoring is just a proposal; it presents changes \nwhich can be ap\u00adplied without changing behavior. There may be good reasons not to apply some or all of \nthese changes, like future extensibility or cohe\u00adsive grouping of members into classes. Similarly, subsequent \nman\u00adual refactorings should obey software engineering criteria: what can be done is not always the same \nas what should be done . Naturally, the decision to apply a refactoring requires a certain fa\u00admiliarity \nwith the code.  5. RELATED WORK In the recent Dagstuhl seminar Program Analysis for Object-Oriented \nEvolution , analysis researchers met with refactoring re\u00adsearchers in order to explore the potential \nfor program analysis in refactoring [16]. One insight of this workshop was that in a refac\u00adtoring context, \n100% preservation of behaviour is not always an objective. KABA reacts to this insight by offering a \nstatic variant which preserves the behavior of all clients, and a dynamic variant which preserves only \nthe behavior of a given test suite. Other authors proposed methods for automated refactoring. None of \nthese are client-speci.c, which means they are valid for all clients, but also prevents client-speci.c \nrefactorings. Opdyke [10] origi\u00adnally introduced the concept of refactoring a class hierarchy. Casais \n[4] was among the .rst authors to investigate automated refactor\u00ading. Unfortunately, his algorithm does \nnot provide semantic guar\u00adantees. Moore [9] not only refactors classes, but also statements e.g. by extracting \ncommon subexpressions. His algorithm is for the dynamically-typed language self. Naturally, behavior \nguaran\u00adtees are not provided, and realistic applications have not been re\u00adported. Bowdidge and Griswold \n[3] present semantics-preserving restructuring transformations for procedural programs based on so\u00adcalled \nstar diagrams; as it happens, star diagrams have similarities to concept lattices. Kataoka et al. [7] \nautomatically extract simple invariants from Java source code and use these to derive code-level refactorings \nsuch as Encapsulate Downcast or Remove Parameter. Tip et al. [15] also uses type constraints in order \nto support the refactorings Extract Interface, and Pull up / Push down member. Tip does guarantee behavior \npreservation, and offers an interactive tool similar to the KABA editor as part of Eclipse. Automatic \ngen\u00aderation of refactorings is however not supported. Earlier work by Tip described an algorithm to specialize \na class hierarchy with re\u00adspect to one client [17]; this method also in.uenced KABA, but to our knowledge \nwas never implemented. Identi.cation of dead .elds and methods is usually based on static analysis such \nas RTA [1], and often used in practical tools such as JAX [18]. KABA is a more general analysis which \nincludes dead members as a by-product. 6. CONCLUSION The original version of the Snelting/Tip algorithm \nwas already published in 1998, but it took us several years to develop KABA on the basis of this algorithm, \nre.ne the foundations in order to handle full Java, add the dynamic analysis variant, .nd good lattice \nsimpli.cations, and make KABA work for realistic applications. Compared to [13] our achievements can \nbe summarized as follows. Today, the static KABA variant can handle up to 30000 LOC, while the new dynamic \nvariant has no program size limita\u00adtion.  The innovative KABA refactoring editor guarantees behav\u00adior \npreservation for client-speci.c as well as general refac\u00adtorings.  Case studies have shown that KABA \nis helpful for analyzing class hierarchies, and generates practically useful refactor\u00adings.  It is interesting \nto note that KABA based on dynamic analysis works .ne in practice, hence the much higher cost of static \nanalysis and full behavior preservation seems questionable. Future work must show whether this observation \nrepresents a new trend in program analysis for software tools. Acknowledgments. This work was funded \nby Deutsche Forschungs\u00adgemeinschaft, grants DFG Sn11/7-1 and Sn11/7-2. The example from .gure 1 was worked \nout by C. Schneider. Frank Tip provided valuable comments. 7. REFERENCES [1] D. F. Bacon and P. F. Sweeney. \nFast static analysis of C++ virtual function calls. In Proc. 11th Conference on Object-Oriented Programming \nSystems, Languages, and Applications (OOPSLA 96), pages 324 341, 1996. [2] K. Beck. Extreme Programming \nExplained. Longman Higher Education, 2000. [3] R. Bowdidge and W. Griswold. Supporting the restructuring \nof data abstractions through manipulation of a program visualization. ACM Transactions on Software Engineering \nand Methodology (TOSEM), 7:109 157, 1998. [4] E. Casais. Automatic reorganization of object-oriented \nhierarchies: A case study. Object-Oriented Systems, 1(2):95 115, 1994. [5] M. Fowler. Refactoring. Addison-Wesley, \n1999. [6] B. Ganter and R. Wille. Formal Concept Analysis - Mathematical Foundations. Springer Verlag, \n1999. [7] Y. Kataoka, M. Ernst, W. Griswold, and D. Notkin. Automated support for program refactoring \nusing invariants. In Proc. International Conference of Software Maintenance (ICSM 01), 2001. [8] O. Lhotak \nand L. Hendren. Scaling Java points-to using Sparc. In Compiler Construction, 12th International Conference, \nLNCS, pages 153 169, 2003. [9] I. Moore. Automatic inheritance hierarchy restructuring and method refactoring. \nIn Proc. 11th Conference on Object-Oriented Programming Systems, Languages, and Applications (OOPSLA \n96), pages 235 250, 1996. [10] W. Opdyke and R. Johnson. Creating abstract superclasses by refactoring. \nIn ACM 1993 Computer Science Conference, 1993. [11] A. Rountev, A. Milanova, and B. G. Ryder. Points-to \nanalysis for Java using annotated constraints. In Proc. 16th Conference on Object-Oriented Programming \nSystems, Languages, and Applications (OOPSLA 01), pages 43 55, 2001. [12] P. Schneider. Umsetzung von \nTransformationen an Klassenhierarchien in der Sprache JAVA. Master s thesis, Universit\u00a8at Passau, 2003. \n[13] G. Snelting and F. Tip. Understanding class hierarchies using concept analysis. ACM Transactions \non Programming Languages and Systems, pages 540 582, May 2000. [14] M. Streckenbach. Points-to-Analyse \nf\u00a8ur Java. Technical Report MIP-0011, Fakult\u00a8at f\u00a8ur Mathematik und Informatik, Universit\u00a8at Passau, \n2000. [15] F. Tip, A. Kiezun, and D. Baeumer. Refactoring for generalization using type constraints. \nIn Proc. 18th Conference on Object-Oriented Programming Systems, Languages, and Applications (OOPSLA \n03), pages 13 26, 2003. [16] F. Tip, G. Snelting, and R. Johnson. Program analysis for object-oriented \nevolution. Technical report, Dagstuhl Seminar Report 03091, 2003. [17] F. Tip and P. F. Sweeney. Class \nhierarchy specialization. Acta Informatica, 36:927 982, 2000. [18] F. Tip, P. F. Sweeney, C. Laffra, \nA. Eisma, and D. Streeter. Practical extraction techniques for java. ACM Trans. Prog. Lang. Syst., 24(6):625 \n666, November 2002. Appendix 1: The Snelting/Tip Algorithm In order to keep this contribution self-contained, \nthis appendix ex\u00adplains the main steps of the refactoring algorithms in a more tech\u00adnical way. Full details \ncan be found in [13]. Collecting member accesses The algorithm is based on a .ne-grained analysis of \nobject ac\u00adcesses. For all objects or object references o, it determines whether member m from class C \nis required in o. This information is ex\u00adtracted from a given hierarchy and its clients by (static or \ndynamic) program analysis. The result is a binary relation, coded in form of a table T . For the example \nin .gure 1, table T contains rows for object ref\u00aderences a1, a2, b1, b2, A.f.this, B.f.this, B.g.this, \nB.h.this,as well as for object creation sites A1, A2, B1, B2.9 Columns are la\u00adbeled with .elds and methods \nA.x, A.y, A.z, A.f(), B.f(), B.g(), B.h(). For methods, there is a distinction between declarations and \nde.nitions (i.e. implementations), that is, between dcl(C.f()) and def (C.f()), which makes the analysis \nmore precise [13]. Dynamic Variant KABA offers two variants of table construction, a static and a dy\u00adnamic \none. The dynamic variant analyzes member accesses for a given test suite. The JVM is instrumented such \nthat every member access O.x from a true object O (resp. its creation site) gives rise to a table entry \n(O, C.m) where O =(C, m, i) is the object creation site at instruction i in method m of class C. Method \ncalls O.f() give rise to table entry (O, def (C.f())) For references, no entries are generated. Static \nVariant In the static variant, points-to analysis is used to determine for an object reference o to which \nobject creation sites it might point to at runtime; this set is denoted pt(o)= {O1,O2,...} pt(o) may \nbe too big (i.e. imprecise), but never too small (i.e. pt is a conservative approximation). Today, reasonable \nef.cient and precise points-to analysis exists for Java, e.g. [14, 11, 8]. Now let Type(o)= C be the \nstatic type of o, and let member accesses o.m resp. o.f() be given. Table T will contain entries (o, \nC.m) resp. (o, dcl(C.f()) Furthermore, entries (O, def (C.f())) are added for all O . pt(o) where C = \nStaticLookup(Type(O),f). For the above example, the resulting table is shown in .gure 20. Note that it \ncontains some additional entries for this-pointers which are explained in [13]. In this simple example, \nstatic and dynamic 9Program analysis usually does not distinguish runtime objects cre\u00adated by the same \nnew statement, so it is a standard technique to identify objects with the same creation site. Thus in \nthe following object O in fact stands for O s creation site, coded as a triple O =(C, m, i) of class \nC, method m and instruction address i.  A.y A.z dcl(A.f) def(A.f) dcl(B.f) def(B.f) dcl(B.g) \n def(B.g) dcl(B.h) def(B.h) Once all type constraints have been extracted, they are incor\u00adporated into \ntable T . To achieve this, we exploit the fact that a a1 \u00d7 a2 \u00d7 \u00d7 b1 \u00d7 b2 \u00d7 \u00d7 \u00d7 \u00d7 A1 \u00d7 A2 \u00d7 \u00d7 \u00d7 \u00d7 B1 \n\u00d7 \u00d7 \u00d7 \u00d7 \u00d7 \u00d7 \u00d7 B2 \u00d7 \u00d7 \u00d7 \u00d7 \u00d7 \u00d7 \u00d7 A.f.this \u00d7 \u00d7 \u00d7 \u00d7 B.f.this \u00d7 \u00d7 \u00d7 \u00d7 B.g.this \u00d7 \u00d7 \u00d7 \u00d7 \u00d7 \u00d7 \u00d7 B.h.this \u00d7 \u00d7 \n\u00d7 \u00d7 \u00d7 \u00d7 \u00d7 Figure 21: Table after incorporating type constraints table are identical, but in general \nthis is of course not the case: due to the principle of conservative approximation, the entries of the \nstatic table are a superset of the entries of any dynamic table. Type constraints In a second step, \na set of type constraints is extracted from the program, which are necessary and suf.cient for preservation \nof be\u00adhavior. The refactoring algorithm computes a new type (i.e. class) for every variable or class-typed \nmember .eld, and a new home class for every member. Therefore, constraints for a variable or .eld x are \nexpressed over the (to be determined) new type of x in the refactored hierarchy, type(x); constraints \nfor a member or method C.m are expressed over its (to be determined) new home class , def (C.m). There \nare basically two kinds of type constraints. First, any assignment x=y;gives rise to a type constraint \ntype(y) = type(x). Such a constraint will be generated not only for explicit assignments, but also for \nimplicit assignments due to parameter and return values, and even for implicit assignments to this-pointers. \nAnother simple type constraint requires that for all methods C.f, def (C.f) = dcl(C.f). The second set \nof type constraints is more dif.cult to understand. These dominance constraints are necessary whenever \na member m is de.ned in a class A and a subclass B = A. In order to avoid ambiguous member access in \nthe refactored hierarchy, sometimes B = A must be retained. More precisely, if subclass B of A rede\u00ad.nes \na member or method m, and some object x accesses both A.m and B.m (that is, .x :(x, def (A.m)) . T .(x, \ndef (B.m)) . T ), then def (B.m) < def (A.m) must be retained in order to avoid ambiguous access to m \nfrom x [13]. a1 \u00d7 a2 \u00d7 \u00d7 b1  \u00d7 b2  \u00d7 \u00d7 A1   A2 \u00d7 B1  \u00d7 \u00d7 B2  \u00d7 \u00d7 A.f.this \u00d7 \u00d7 \u00d7 B.f.this \u00d7 \n\u00d7 B.g.this \u00d7 \u00d7 \u00d7 B.h.this \u00d7 \u00d7 \u00d7 Figure 20: Member access table for .gure 1   A.x  A.y  A.z  \n dcl(A.f)  def(A.f)  dcl(B.f)  def(B.f)  dcl(B.g)   def(B.g)  dcl(B.h)  def(B.h) def (B.f) \n. def (A.f), dcl(B.f) . dcl(A.f) def (A.f) . dcl(A.f), def (B.f) . dcl(B.f), def (B.g) . dcl(B.g), def \n(B.h) . dcl(B.h), These implications are easily incorporated into table 20 by copying row entries from \nrow y to row x resp. column entries from column def (A.f) to column def (B.f) etc. Note that in general \nthere may be cyclic and mutual dependences between row and/or column im\u00adplications, thus a .x-point iteration \nis required to incorporate all constraints into the table. The .nal table for .gure 20 is presented in \n.gure 21.  Concept lattices In a .nal step, concept analysis [6] is used to construct the refac\u00adtored \nhierarchy from table T . Concept analysis can always be ap\u00adplied whenever hidden hierarchical structures \nhave to be extracted from a given binary relation. The standard example is shown in .gure 22. The table \nencodes a binary relation between objects O (in our case the planets) and attributes A, thus T . O \u00d7 \nA. From the table, the corresponding concept lattice is computed by some smart algorithm [6]. The elements \nof this lattice are labeled with objects and attributes ; .(o) is the lattice element labeled with o \n. O, \u00b5(a) is the element labeled with a . A. The lattice has the following characteristic properties: \n1. (o, a) . T i. .(o) = \u00b5(a), that is object o has attribute a iff o appears below a in the lattice; \n2. The supremum .(o1) n .(o2) represents all attributes com\u00admon to both o1 and o2; 3. The in.mum \u00b5(a1)U\u00b5(a2) \nrepresents all objects having both attributes a1 and a2; 4. \u00b5(a1) = \u00b5(a2)i. a1 . a2 (a1 implies a2), \nthat is, if a1 appears below a2 in the lattice, all objects having attribute a1 also have attribute a2. \n 5. .(o1) = .(o2)i. o2 . o1, that is, if o1 appears below o2, all attributes .tting to o2 will also .t \nto o1.  Note that in big tables, common attributes (suprema), common ob\u00adjects (in.ma), and implications \nare not at all obvious from the table alone. One reason is that the concept lattice for a table is invariant \nagainst row or column permutations. 10Note that x . y is an implication between row labels, while def \n(B.m) . def (A.m) is an implication between column labels. Therefore the direction of the second implication \nis reversed . But the effect is the same: if x . y holds in T , then y appears below B.g.this . b1,B.h.this \n. b2,a1 . A1, a2 . A2,b1 . B1,b2 . B2,a2 . b2 Furthermore, the following dominance constraints are collected: \nconstraint can be seen as an implication between table rows resp. columns, and that there is an algorithm \nto incorporate any given set of implications into a table [6]. First we observe that even in the refactored \nhierarchy, a subtype inherits all members from its super-type. Therefore type(y) = type(x) enforces that \nany table entry for x must also be present for y; that is .m :(x, m) . T . (y, m) . T ,or x . y for short. \nSecond, def (B.m) < def (A.m) enforces that any table entry for def (B.m) must also be present for def \n(A.m), which is written as def (B.m) . def (A.m).10 Reconsidering .gure 1, the following assignment constraints \nare collected in form of implications: A.y . A.x, A.f.this . a2,B.f.this . a2,  The concept lattice \nfor .gure 1, as constructed from table 21, is given in .gure 23. Concept lattices can naturally be interpreted \nas inheritance hierarchies as follows. Every lattice element represents a class in the refactored hierarchy. \nMethod or .eld names above an element represent the members of this class. Objects or references below \nan element will have that element (i.e. class) as its new type. In particular, all objects now have a \nnew type which contains only the members the object really accesses. Typically, original classes are \nsplit and new subclasses are intro\u00adduced. This is particularly true for .gure 23, where the raw lattice \nintroduces 12 refactored classes instead of the original two. These new classes represent object behavior \npatterns: a1 and A1 use A.x but nothing else, which is clearly visible in the lattice. a2 addition\u00adally \ncalls a.f() and thus needs the declaration of this method. b2 calls B.h(),B.f() plus anything called \nby a2. The real objects A2,B2,B1 are located far down in the lattice and use various sub\u00adsets of the \noriginal members. B2 in particular not only accesses everything accessed by b2, but also calls B.f() \nand thus needs def (B.f()) (references need dcl , objects need def ), which causes one of the multiple \ninheritances in the raw lattice. Note that the raw lattice clearly distinguishes between a class and \nits interface: several new classes (e.g. the one labeled dcl(B.g()) in .gure 23) contain only dcl(...) \nentries, but no (inherited) def (...) entries or .elds, meaning that they are interfaces. x in the refactored \nhierarchy; if def (B.m) . def (A.m) holds in T , then def (B.m) appears below def (A.m) in the refactored \nhierarchy [6]. Figure 23: Concept lattice for .gure 1, generated from .gure 21 The lattice guarantees \npreservation of behavior for all clients [13]. It is rather .ne-grained, and in its raw form represents \nthe most .ne-grained refactoring which respects client behavior.  Lattice simpli.cation and elimination \nof multi\u00adple inheritance \"From a software engineering viewpoint, the lattice must be simpli\u00ad.ed in order \nto be useful. For example, empty elements (i.e. new classes without own members) such as the top element \nin .gure 23 can be removed; multiple inheritance can often be eliminated, and lattice elements can be \nmerged according to certain (behavior\u00adpreserving) rules. In particular, the distinction between a class \nand its interface can be removed by merging lattice elements. For simpli.cation of the class hierarchy, \nwe apply several trans\u00adformations: If q is the only subclass of p and there are no instances of q, merge \np and q.  If p is the only superclass of q and q does not contain any members, merge p and q.  If p \nis the only superclass of q and both classes contain only members of the same original class, merge p \nand q.  These transformations are repeated until a .xpoint is reached. They simplify the structure, \nbut never affect the semantics of the hierar\u00adchy. Repeated application of the transformations to .gure \n23 re\u00adsulted in .gure 2. For example, the .rst transformation can be ap\u00adplied to the nodes labelled a2 \nand A.y, as the latter has no instances (no variables appear below the node). The new hierarchy may exhibit \nmultiple inheritance between the new classes. Many of these classes can be made interfaces (and the lattice \nbe simpli.ed as above), but cases remain in which a class in\u00adherits a non-abstract method from more than \none superclass. These can always be removed manually without changing behavior. In order to eliminate \nmultiple inheritance automatically, more aggressive transformations are needed. It must be checked ex\u00adplicitely \nwhether their application affects the semantics, that is, whether all type constraints are still valid \nafter application. If q inherits members from p and p ., and the only superclass of p . is ., make p \n. a subclass of p.  If q inherits members from p and p . with p . p . and p . . p: Let r be a superclass \nof p with r =p ..If r is the only su\u00adperclass of p, merge r and p, else move all members of p to r. \n If q inherits members from p and p . with p . p . and p . . p, and p is a superclass of q and there \nare no instances of p, merge p and q  Repeated application of these transformations to .gures 7, 13, \n17 resulted in .gures 8, 14, 18 respectively. In these examples, the application of the transformations \nhad to be checked excessively for preservation of semantics, making removal of multiple inheritance expensive. \n Dead variables, .elds and methods In the refactored hierarchy the top element is called ., which is \ndifferent from the node for java.lang.Object. One might argue they should be identical as in Java everything \nis derived from Object. The difference is that objects directly below .are dead. Similarly, .elds and \nmethods appearing directly above .are dead as well. In .gure 23, .eld A.z is dead. Appendix 2: Language \nDetails Several Java features require additional treatment [13], which will be sketched in the following. \nWe would like to point out that full Java can be handled.  Libraries We distinguish objects whose type \nis de.ned in user code, and ob\u00ad jects whose type is de.ned in library or API code. Library code is never \nrefactored. Nevertheless, all objects created (even those cre\u00ad ated inside library code) must be taken \ninto account for the static analysis (in particular points-to analysis), as they impact the control .ow \nof the analyzed program and may in.uence which members of the relevant objects are accessed. The Java \nAPI also contains native code. These methods can ac\u00ad cess members too and do so in practice. For each \nof these meth\u00ad ods a stub must be provided, which must be equivalent in terms of member access or dynamic \ntype checks. The effects of library code should not be underestimated. Even small Java programs load \na huge amount of library code11, provid\u00ad ing big problems for the scalability of the static analysis. \nBut anal\u00ad ysis of this code and careful handling of native code is absolutely necessary when it comes \nto preservation of behavior. 12 11As of JDK 1.4.2, a hello world example loads 248 library classes. \n12Here is a small example: class Main { String toString() {return \"Hello, World\"; }public static void \nmain(String args[]) {System.out.println(new Main()); }} Without handling the effects of library code \nand native methods, the method toStringwill be declared dead, obviously breaking the behavior of the \nprogram. This is no esoteric example, code like this can be found in many Java programs.  Treatment \nof instanceof Like object creation sites, different uses of the instanceofop\u00aderator in the program are \ndistinguished by their byte-code address. For an occurrence x instanceof T at site C in the program, \ntwo additional attributes (table columns) are generated: C =true and C =false. For every o .pt(x)a table \nentry (o, C =true) is generated if type(x)=T , a table entry (o, C =false)other\u00adwise. In the class hierarchy, \nall objects returning true for the expression in the program will appear below \u00b5(C = true). When code \nis regenerated, \u00b5(C = true)is the new type for T in the original expression. The attribute \u00b5(C =false)only \nbecomes relevant for editing the class hierarchy. Variables below \u00b5(C = false)may never be below \u00b5(C \n= true)as well, because the transformed instanceof operator will match every object of classes below \n\u00b5(C =true). The result of the instanceof may be always true (indicated by \u00b5(C =false)=.) or never true \n(\u00b5(C =true)=.). In the latter case, the whole operator could be replaced by false(more aggressive dead \ncode elimination is also possible). Unfortunately this is not possible for the always true case, as x \nmay be null, causing the operator to return false, so the operator could be replaced by x!=null, also \nenabling further optimizations. Treatment of Type Casts Type casts are handled in a similar fashion. \nIf (T)xis in the pro\u00adgram at site C, two attributes, C =true and C =false are gen\u00aderated, objects o .pt(x)passing \nthe type cast will create a table entry (o, C =true), if the cast is not possible (o, C =false)is generated. \nThe new cast then can be rewritten to (\u00b5(C =true))x. But for type casts the situation that the cast is \nnever successful is more complicated. A little example illustrates this: class A { A a=new A(); void \nf() {} a.f(); } B b=(B)a; class B extends A { b.f(); } In this example no object gets a table entry \nat column C =true because the cast always fails, so \u00b5(C =true)=.. But .is not a type and cannot be used \nin the transformed program. To handle this for every cast, an additional pointer x/T , representing the \nresult of cast, is created and the objects successfully casted are assigned to it (o .pt(x).Type(o)=T \n.x/T =o). This pointer is further used to collect the member accesses from the casted value (e.g. a table \nentry (a/B, dcl(B.f))would be generated for the example program). Because of the assignment, \u00b5(C =true)=.(a/B)is \nalways valid. If \u00b5(C =true)=., .(a/B)can be used as type for the result, but not for the typecast as \nthere is no guarantee that no object is below .(a/B). For this special case the recreated code would \nbe: B b=null; if(a!=null) throw new ClassCastException(); b.f(); Without the call a.f();, the new class \nhierarchy would have a class containing only a declaration of f, without subclasses or in\u00adstances. Treatment \nof Exceptions To preserve the behavior of exceptions, the analysis must guaran\u00adtee that every object \nthrown as exception shows the same behavior against every exception handler testing it while thrown. \nException handlers are listed as a table in byte-code, so they can be identi.ed by a method name and \na number referring to a table entry. The han\u00addlers a thrown object is tested against can be inferred \nfrom control .ow information intraprocedural and from the call graph interpro\u00adcedural. Again, attributes \nH = false and H = true are created for every exception handler H in the program. An object o tested against \nH raises a table entry (o, H = true) if the exception is caught by that handler and (o, H = false) else. \nThe new type for an exception handler H is the class \u00b5(H = true). In case \u00b5(H = true)= ., the handler \nis never used and can be removed from the code. The necessary table entries for handlers are currently \nnot created by the dynamic analysis, making it impossible to refactor exception hierarchies. This does \nnot affect the analysis of objects not used as exceptions. Signatures of Overloaded Methods As the analysis \nreduces every type in the program to its minimum, this may cause unwanted results in the context of overloaded \nmeth\u00adods. For the following example class A {}class B {}... void f(A a) { System.out.println(\"an A\"); \n} void f(B b) { System.out.println(\"a B\"); }... f(new A()); f(new B()); Both parameters will be reduced \nto type Object, giving the overloaded methods equal signatures, which is not allowed in Java. This can \nbe detected automatically and one or both variants of the method can be renamed. Signatures of Overwritten \nMethods Exactly the opposite will happen to the parameters of methods overwritten in a subclass. Here \nis a small example: class A { void f(C c) { c.g(); } A a=new A(); } if(...) class B extends A { a=new \nB(); void f(C c) { c.h(); } a.f(null); } ... For the parameter of f in A, a type containing only dcl(C.g) \nwill be calculated, for f in B a type containing only dcl(C.h). But with different signatures Java would \ntreat these as overloaded methods and no longer apply dynamic binding. It seems possible to take the \nin.mum \u00b5(dcl(C.g)) U \u00b5(dcl(C.h)) as type for the parameter, but this may be . (like in this example) \nand in general is not type correct. Instead assignments between all those parameters are added, forcing \nthem to have the same type in the new class hierarchy (and giving all actual parameters the necessary \ntype). The same process is applied to the return type of overwritten methods. Appendix 3: Preserving \nbehavior This appendix describes how the KABA editor guarantees preser\u00advation of behavior when manually \nmodifying a refactoring pro\u00adposal. The initial refactoring proposal preserves behavior. Subsequent refactoring \nsteps must only guarantee that the behavior of the new class hierarchy is identical to the behavior of \nthe initially generated concept lattice. Behavior preservation is guaranteed by two groups of constraints. \nThe .rst group consists of global constraints which must be ful.lled in order to interpret the graph \nas a class hierarchy. Constraints in the second group concern individual objects. The .rst group comprises: \n The graph must not contain cycles, and a class may not con\u00adtain two method de.nitions with equal signature. \n All assignments in the program must still be type-correct. If p =q; was in the analyzed program, then \n.(q) = .(p) must be valid.  Dominance constraints must not be violated. If A . B is a dominance constraint, \nthen \u00b5(A) = \u00b5(B) must be valid.  The additional constrains for type checks and exception han\u00addling must \nbe respected. If C is the site of a type check or represents an exception handler, C, C = false . C = \ntrue must be valid.  The second group comprises individual constraints for a true ob\u00adject O: If Type(O) \ninitially contained a statically bound member m, .(O) must also contain m.  If Type(O) initially contained \na dynamically bound method m, the lookup must yield the same implementation: StaticLookup(Type(O),m)= \nStaticLookup(.(O),m).  The class of O, .(O), must not become an abstract class.  If the table for the \ninitial graph had an entry (o, C = true), .(o) = C = true must be valid.  If the table for the initial \ngraph had an entry (o, C = false), .(o) . C = false must be valid.  Thus these constraints are the interactive \nversion of the domi\u00adnance constraints from appendix 1.  \n\t\t\t", "proc_id": "1028976", "abstract": "<p>KABA is an innovative system for refactoring Java class hierar-chies. It uses the Snelting/Tip algorithm [13] in order to determine a behavior-preserving refactoring which is optimal with respect to a given set of client programs. KABA can be based on dynamic as well as static program analysis. The static variant will preserve program behavior for all possible input values; the dynamic version guarantees preservation of behavior for all runs in a given test suite. KABA offers automatic refactoring as well as manual refactoring using a dedicated editor.</p> <p>In this contribution, we recapitulate the Snelting/Tip algorithm, present the new dynamic version, and explain new extensions which allow to handle full Java. We then present five case studies which discuss the KABA refactoring proposals for programs such as javac and antlr. KABA proved that javac does not need refactoring, but generated semantics-based refactoring proposals for antlr.</p>", "authors": [{"name": "Mirko Streckenbach", "author_profile_id": "81100122602", "affiliation": "Universit&#228;t Passau, Passau, Germany", "person_id": "P698420", "email_address": "", "orcid_id": ""}, {"name": "Gregor Snelting", "author_profile_id": "81100482381", "affiliation": "Universit&#228;t Passau, Passau, Germany", "person_id": "PP43121149", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1028976.1029003", "year": "2004", "article_id": "1029003", "conference": "OOPSLA", "title": "Refactoring class hierarchies with KABA", "url": "http://dl.acm.org/citation.cfm?id=1029003"}