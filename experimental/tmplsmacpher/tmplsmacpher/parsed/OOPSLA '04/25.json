{"article_publication_date": "10-01-2004", "fulltext": "\n Finding and Preventing Run-Time Error Handling Mistakes Westley Weimer George C. Necula University \nof California, Berkeley * {weimer,necula}@cs.berkeley.edu ABSTRACT It is di.cult to write programs \nthat behave correctly in the presence of run-time errors. Existing programming language features often \nprovide poor support for executing clean-up code and for restoring invariants in such exceptional situa\u00adtions. \nWe present a data.ow analysis for .nding a certain class of error-handling mistakes:those that arise \nfrom a failure to release resources or to clean up properly along all paths. Many real-world programs \nviolate such resource safety policies because of incorrect error handling. Our .ow-sensitive analysis \nkeeps track of outstanding obligations along program paths and does a precise modeling of control .ow \nin the presence of exceptions. Using it, we have found over 800 error handling mistakes almost 4 million \nlines of Java code. The analysis is unsound and produces false posi\u00adtives, but a few simple .ltering \nrules su.ce to remove them in practice. The remaining mistakes were manually veri.ed. These mistakes \ncause sockets, .les and database handles to be leaked along some paths. We present a characterization \nof the most common causes of those errors and discuss the limitations of exception handling, .nalizers \nand destructors in addressing them. Based on those errors, we propose a pro\u00adgramming language feature \nthat keeps track of obligations at run time and ensures that they are discharged. Finally, we present \ncase studies to demonstrate that this feature is natural, e.cient, and can improve reliability; for example, \nretro.tting a 34kLOC program with it resulted in a 0.5% code size decrease, a surprising 17% speed increase \n(from correctly deallocating resources in the presence of excep\u00adtions), and more consistent behavior. \n* This research was supported in part by the National Science Foundation Grants CCR-9875171, CCR-0085949, \nCCR-0081588, CCR-0234689, CCR-0326577, CCR-00225610, and gifts from Mi\u00adcrosoft Research. The information \npresented here does not nec\u00adessarily re.ect the position or the policy of the Government and no o.cial \nendorsement should be inferred. Permission to make digital or hard copies of all or part of this work \nfor personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior \nspeci.c permission and/or a fee. OOPSLA 04, Oct. 24-28, 2004, Vancouver, British Columbia, Canada. Copyright \n2004 ACM 1-58113-831-8/04/0010 ...$5.00.  Categories and Subject Descriptors D.2.4 [Software Engineering]:Program \nVeri.cation; D.2.5 [Software Engineering]:Testing and Debugging; D.3.3 [Programming Languages]:Constructs \nand Fea\u00adtures  General Terms Experimentation, Languages, Measurement, Reliability, Veri.cation Keywords \nData.ow, Exceptions, Finalizers, Destructors, Try-Finally 1. INTRODUCTION An IBM survey [12] reports \nthat up to two-thirds of a program may be devoted to error handling. Our experi\u00adments with a suite of \nopen-source Java programs ranging in size from 4,000 to 1,600,000 lines of code suggest that error handling \nis a lesser fraction of all source code but that it is still signi.cant. Between 1% and 5% of program \ntext in our experiments was comprised of error-handling catch and finally blocks. Between 3% and 46% \nof the program text was transitively reachable from catch and finally blocks, which often contain calls \nto cleanup methods. Aside from programs speci.cally designed from the ground up for reli\u00adability (e.g., \n[7]), these proportions grow with program size and age. These broad numbers suggest that error handling \nis an important part of such programs and that much e.ort is devoted to it. Despite the importance of \naddressing run-time errors, poor handling abounds. Here we are not concerned with the frequency of run-time \nerrors but instead with how the program reacts to such exceptional situations. Error han\u00addling mistakes \nin which applications don t properly handle error conditions that occur during normal operation are one \nof the top ten causes of Java web application security risks [1]. Exception handling itself is insu.cient:along \nwith others [2], we observe that the two most common exception handling strategies are (1) do nothing, \nand (2) abort the program. In addition, existing exception handling mecha\u00adnisms in programming languages \nare too low-level:higher\u00adlevel machinery for error-handling is needed. In particular, attention is rarely \npaid to restoring invariants and adhering to interface requirements (e.g., releasing previously-acquired \nresources). Based on a static analysis, we will present exper\u00adimental evidence that many Java programs \nmake mistakes of this nature. The biggest problem is that programs fail to account for all possible execution \npaths in the presence of run-time errors. It is hard to restore invariants correctly on all paths. We \npropose a mechanism where program actions can be asso\u00adciated with compensations, code that semantically \nundoes e.ects and restores invariants. This mechanism is similar to destructors and .nalizers. This system \nensures that if an action is taken, the program cannot halt without .rst exe\u00adcuting the compensation. \nThus a program that acquires a resource protected by this mechanism will release that re\u00adsource along \nall execution paths, including those on which run-time errors occur. The compensating actions themselves \nare recorded and executed at run time. Compensations are stored in special run-time stacks. Our system \nuses stacks because dependencies between important resources make it desirable to execute the most recent \ncompensation .rst. The contributions of this paper include: A static analysis for locating mistakes \nin resource man\u00adagement run-time error handling. We report on such mistakes in a large number of programs. \n A language-level mechanism for associating compensa\u00adtions with actions and guaranteeing that the compen\u00adsations \nare executed along all paths.  Section 2 provides a motivating example and shows com\u00admon practices in \nerror handling. We discuss our analysis for .nding error-handling mistakes in Section 3. In Section 4, \nwe apply the analysis and show that many programs make mistakes in their error handling. We attempt to \ncharacter\u00adize common mistakes in Section 5. In Section 6 we examine problems with error-handling via \ndestructors and .nalizers. Section 7 describes formally our proposed language features for run-time error \nhandling. Case studies arguing that our proposed features are e.cient and natural are given in Sec\u00adtion \n8. We place this work in context in Section 9 and compare it to existing techniques. In Section 10 we \nmention future research directions. Section 11 concludes.  2. MOTIVATING EXAMPLE Consider this code, \ntaken from Ohioedge CRM, the largest open-source customer relations management project [41]. 01: Connection \ncn; PreparedStatement ps; ResultSet rs; 02: try { 03: cn = ConnectionFactory.getConnection(/* ... */); \n04: StringBuffer qry = ...; // do some work 05: ps = cn.prepareStatement(qry.toString()); 06: rs = ps.executeQuery(); \n07: ... // do I/O-related work with rs 08: rs.close(); ps.close(); 09: } finally { 10: try { cn.close(); \n} 11: catch (Exception e1) { } 12: } This program uses language features to facilitate run-time error \nhandling (i.e., Java s finally), but many problems re\u00admain. Connections, PreparedStatementsand ResultSets \nrepresent global resources associated with an external database, so the program should close each one \nas quickly as possible. If a run-time error occurs on line 4, the runtime system will raise an exception, \nand the program will close the open Connection on line 10. However, if a run-time er\u00adror occurs on line \n7 (or 6 or 8), the resources associated with ps and rs will not be freed. Moving the close calls from \nline 8 into the finally block is insu.cient for at least two reasons. First, the close method itself \ncan raise exceptions (as indicated by lines 10 and 11), so a failure while closing rs might leave ps \ndan\u00adgling. Second, if an error occurs on line 5, an attempt will be made to close the never-opened and \nstill-null rs,caus\u00ading a null-pointer exception. This sort of code is quite common and highlights a num\u00adber \nof important observations. First, the programmer is aware of the safety policies: try and close abound. \nSec\u00adond, there are many paths where error handling is poor. Third, there are a few control-.ow paths \nwhere the error handling works correctly, so the programmer is aware of the correct policy. Finally, \n.xing the problem typically has soft\u00adware engineering disadvantages:the distance between any resource \nacquisition and its associated release increases, and extra control .ow used only for error-handling \nmust be in\u00adcluded. In addition, if another procedure wishes to make use of Connections, it must duplicate \nall of this error handling code. This duplication is frequent in practice; the source .le containing \nthe above example also contains two similar procedures that make the same mistakes. Developers have cited \nthis required repetition to explain why error handling is sometimes ignored (e.g., [7]). In general, \ncorrectly dealing with N resources requires N nested try-finally statements or a number of run-time checks \n(e.g., checking each vari\u00adable against null or keeping track of progress in a counter variable). Handling \nsuch problems is complicated and error\u00adprone in practice, and we claim it could be made easier with more \nsupport from the programming language. In the next section we will discuss an analysis for au\u00adtomatically \ndiscovering such error-handling mistakes. For example, this analysis will report three paths in the example \nabove. If an exception occurs on line 6, a PreparedStatement is leaked. If an error occurs on line 7, \nboth the PreparedStatement and the ResultSet are for\u00adgotten. Finally, if the .rst call to close on line \n8 raises an exception, the PreparedStatement is again leaked. 3. ERROR-HANDLING ANALYSIS We present \na static analysis for locating mistakes in re\u00adsource management run-time error handling. This analysis \nyields paths through methods on which mistakes may oc\u00adcur and can be used to direct changes to the source \ncode to improve error handling. The analysis may report false pos\u00aditives and may miss real errors. We \nhave chosen to take a fully static approach to avoid the problems of test case gen\u00aderation and the unavailability \nof third-party libraries. Path coverage and test case generation are particularly thorny problems in \nthe context of run-time errors and exceptions, which are typically rare and di.cult to trigger. We consider \neach method body in turn, symbolically ex\u00adecuting all code paths, abstracting away data values but paying \nspecial attention to control .ow and exceptions. The .rst step is to create the control-.ow graph. Con\u00adstructing \na control-.ow graph that explicitly accounts for exceptional control .ow is non-trivial in Java. While \ntry\u00adcatch-finally is conceptually simple, it has the most com\u00adplicated execution description in the language \nspeci.ca\u00adtion [24] and requires four levels of nested if s in its English description. In short, it contains \na large number of corner cases that programmers often overlook. 3.1 Fault Model Modeling exceptional \ncontrol-.ow requires determining where exceptions can be raised. We treat throw statements directly, \nbut all other expressions fall under a speci.c fault model we have adopted. In Java, method declarations \nex\u00adplicitly list all checked exceptions that they can (transi\u00adtively) raise. We assume that all method \nand constructor in\u00advocations can either return normally or raise any of their de\u00adclared exceptions. This \nchoice is motivated by experiments demonstrating that actual failures (e.g., pulling the plug on a remote \nmachine) do map to application-visible checked exceptions at call sites [8]. In addition to such checked \nexceptions, Java also contains unchecked exceptions for sit\u00aduations like null-pointer dereferences and \ndivision by zero. Thus we could treat any division expression, for example, as terminating normally or \nraising a DivisionByZero ex\u00adception. In our fault model we do not consider such implicit unchecked exceptions \nbecause they do not necessarily cor\u00adrespond to the run-time errors that concern us. Finally, there is \nthe practical issue of unavailable code (in particular, public domain code written against commer\u00adcial \ndatabase libraries). When a method in an unavailable library is invoked, its signature is also unavailable \nso we can\u00adnot determine what exceptions it may raise. Our fault model is to assume that it can raise \nany exception mentioned in a lexically-enclosing catch clause or raised by the enclosing method. This \nfault model gives the programmer the bene.t of the doubt and also concentrates the analysis on mistakes \nin existing error handling (which is common in our expe\u00adrience), not mistakes caused by completely forgetting \nerror handling (rare in our experience). Once an exception has been raised, type-checking is required \nin order to determine control .ow. Barring finally clauses, execution transfers to the nearest enclosing \ncatch clause with a declared excep\u00adtion parameter that is a supertype of the raised exception s type. \nNote that our fault model is Java-speci.c but that our data.ow analysis is language-independent.  3.2 \nData.ow Analysis Given the control-.ow graph, our .ow-sensitive, intrapro\u00adcedural data.ow analysis [29, \n13, 18] is designed to .nd paths along which programs forget to discharge obligations in the presence \nof run-time errors. We abstract away data values, and retain as symbolic state a path through the pro\u00adgram \nand a multiset of outstanding resource types for that path. That is, rather than keeping track of which \nvariables hold important resources we merely keep track of a set of ac\u00adquired resource types. We begin \nthe analysis of each method body with an empty path and no obligations. If a symbolic state at the end \nof method contains outstanding obligations, we term it a violation and report it. The analysis is parametric \nwith respect to a safety policy. The safety policy enumerates the abstract obligations (e.g., Socket \nand ResultSet are di.erent resources that must be tracked separately) and lists method invocations (by \nreceiver class and name) that create and discharge such obligations. We formalize the safety policy as \na set of triples:method names, an add-or-remove annotation, and a unique resource identi.er. For example, \nthese triples are a subset of our  sthen = visit(s, L) selse = visit(s, L) sn se == { add(s, o, L)if \n(meth, add,o) . Policy del(s, o, L) if (meth, del,o) . Policy visit(s, L) otherwise {del(s, o, L)if (meth, \ndel,o) . Policy visit(s, L) otherwise sother = visit(s, L) sjoin = {visit(shorter(s, s'),L) if s.obs \n= s'.obs visit(s, L) .visit(s',L)otherwise visit((obs, path),L)= {(obs, path L)} add((obs, path),o,L)= \n{({o}.obs, path L)} del({o}.obs, path),o, L)= {(obs, path L)} { (o, p) if |p|=|p'| shorter((o, p), (o, \np'))= (o, p') otherwise Figure 1: Analysis .ow functions. obs is a multiset of obligations, path is \na sequence of locations. Each in\u00adcoming state from s (an (obs, path) pair) is considered individually \nand produces a set of outgoing states. database safety policy and handle ResultSets: (java.sql.Statement.executeQuery \n, add, ResultSet) (java.sql.PreparedStatement.executeQuery , add, ResultSet) (java.sql.ResultSet.close \n, del, ResultSet) Given such a safety policy we must still determine what state information to propagate \non the graph, and give .ow and grouping functions. Much like the ESP [13] and Meta\u00adcompilation [18] projects, \nwe combine a degree of symbolic execution with data.ow and often keep state associated with multiple \ndistinct paths that pass through the same program point. The symbolic state s = (obs, path) propagated \nfrom node to node is a multiset of outstanding obligations, obs,and a sequence of program point labels, \npath.Thus at the be\u00adginning of line 6 in the example in the previous section, we would carry the information \nthat a Connection and a PreparedStatement obligation are present (but not which variables contain them) \nand that lines 1, 2, 3, 4 and 5 have been executed. 3.3 Flow Functions The .ow functions are determined \nby the safety policy and are given in Figure 1. The four main types of control .ow nodes are branches, \nmethod invocations, other statements and join points. We handle normal and conditional control .ow by \nab\u00adstracting away data values:control can .ow from an if to both the then and the else branch (assuming \nthat the guard does not raise an exception, etc.) and our symbolic state propagates directly from the \nincoming edge to both outgoing edges. We write visit(s, L) to mean the symbolic state s with location \nL appended to its path. A method invocation may terminate normally, repre\u00adsented by the sn edge in Figure \n1. Such an invocation of an obligation-creating method adds the appropriate obligation to the symbolic \nstate (and is propagated to the next pro\u00adgram point). We write add(s, o, L) to mean a new symbolic state \nlike s that includes obligation o and has L appended to its path. A method invocation may also discharge \nan obliga\u00adtion depending on the safety policy, and we use del(s, o, L) to mean a new symbolic state like \ns with obligation o deleted and L appended to the path. An attempt to discharge an obligation that is \nnot present is reported as a violation, but that never occurred in our experiments. All other successful \nmethod invocations do not change the outstanding obliga\u00adtions. A method invocation may also raise a declared \nexception, represented by the se edge in Figure 1. Note that unlike the successful invocation case and \nas per our fault model, the symbolic state does not accrue obligations since the method did not complete \nnormally. However, an attempt to dis\u00adcharge an obligation that raises an exception still removes that \nobligation. Thus we do not require that programs loop around close functions, invoking them until they \nsucceed. Since close functions can generally not be retried and al\u00admost no programs we have observed \ndo so, it would create unnecessary false positives. In any event, the resulting sym\u00adbolic state is propagated \nto the nearest appropriate handler. Continuing the above example, evaluating line 6 would propagate Connection, \nPreparedStatement and ResultSet to line 7 and Connection and PreparedStatement to line 10 (assuming that \nexecuteQuery is declared to raise an excep\u00adtion). As described above, any invocation of an obligation\u00addischarging \nmethod discharges the obligation, but we still consider both normal and exceptional control .ow. For \nex\u00adample, rs.close() on line 8 always removes the ResultSet from the set of obligations, but may propagate \nto both the next statement on line 8 and the finally block on line 10. The grouping (or join) function \ntracks separate paths through the same program point provided that they have di.erent obligation lists. \nOur join function uses the prop\u00aderty simulation approach [13] to grouping sets of symbolic states. We \nmerge states with identical obligations by retain\u00ading only the shorter path for error reporting (modeled \nhere with the function shorter(s1,s2)). To ensure termination we stop the analysis and .ag an error when \na program point occurs twice in a path with di.erent obligation sets (e.g., if a program acquires obli\u00adgations \ninside a loop). For the safety policies we consid\u00adered, that never occurred. The analysis is exponential \nin the worst case (e.g., sequential if statements with every path containing a di.erent obligation list) \nbut quite e.cient in practice. For example, performing this analysis on the 57k LOC hibernate program, \nincluding parsing, typechecking and printing out the resulting error traces, took 104 seconds and 46 \nMB of memory on a 1.6 GHz machine.  3.4 Error Report Filtering Finally, we use heuristics as a post-processing \nstep to .lter reported violations and avoid false positives. Based on an exhaustive analysis of the false \npositives reported by this analysis, we designed three simple .ltering rules. When a violation is reported, \nwe examine its path.Every time it passes through a conditional of the form t = null we remove an outstanding \nobligation that has the same type as t. This addresses the very common case of checking for null resources: \nif (sock != null) { try { sock.close(); } catch (Exception e) { } } Since we abstract away data values, \nwe would report a false positive in such cases. Intuitively, the resource is not leaked along this path \nbecause the program has checked and ensured thatitwas notallocated. Second, we examine the path for assignments \nof the form field = t. For each such assignment we remove one out\u00adstanding obligation with the same type \nas t.When impor\u00adtant resources are assigned to object .elds, the object al\u00admost invariably contains a \nseparate cleanup method that is charged with releasing those resources. As we shall dis\u00adcuss in Section \n6, this cleanup method is almost never an actual .nalizer. Finally, if the path contains a return t, \nweremoveone outstanding resource of type t. Such functions are e.ectively wrappers around the standard \nlibrary constructors and the obligation for discharging the resource falls to the caller. We did not \nobserve wrappers for standard library close functions, so we do not similarly remove obligations based \non values passed as function arguments. Our .rst heuristic helps to reduce false positives intro\u00adduced \nby data abstraction. The second and third heuristics help to address false positives caused by the intraprocedural \nnature of our analysis. These three simple .lters eliminate all false positives we encountered but could \ncause this anal\u00adysis to miss real errors. We discuss the rami.cations along with the results in the next \nsection. 3.5 Analysis Summary Our fault model is speci.c to Java, and we use it to con\u00adstruct a control-.ow \ngraph where method invocations can raise declared exceptions. We chose Java because experi\u00adments show \nthat exceptions and run-time errors are corre\u00adlated and because method signatures include exception in\u00adformation. \nOur data.ow analysis is language-independent. Theanalysis is.ow-sensitive becausewewanttoconsider control \n.ow and because the abstract state of a resource (e.g., acquired or released) can change from program \npoint to program point. The analysis is intraprocedural for e.\u00adciency since we track separate execution \npaths. This leads to false positives, which we can eliminate easily in practice, but our heuristics for \ndoing so may also mask real errors. The analysis abstracts away data values, keeping instead a set of \noutstanding resource types as the per-path symbolic state. This abstraction can also lead to false positives \nand false negatives, but stylized usage patterns allow us to elim\u00adinate the false positives in practice. \nAt join points we keep symbolic states separate if they have distinct sets of obliga\u00adtions.1 We report \na violation when a path leaves a method (normally or exceptionally) with an outstanding obligation. 1In \nthe analysis presented, keeping two states will usually yield a violation later. We present the general \njoin so that if the analysis abstraction is made more precise (e.g., if it captures correlated conditionals) \nthe join will work unchanged. Lines Methods paths with errors Program of with per safety policy Code \nErrors DB File Strm   javad 2000 4k 1 0 0 1 javacc 3.0 13k 4 0 36 0 jtar 1.21 17k 5 0 7 4 jatlite \n3.5.97 18k 6 0 4 0 toba 1.1c 19k 6 0 1 20 osage 1.0p10 20k 3 15 0 0 jcc 0.02 26k 0 0 0 0 quartz 1.0.6 \n27k 17 46 5 20 infinity 1.28 28k 14 0 165 1 ejbca 2.0b2 33k 31 0 39 117 ohioedge 1.3.1 40k 15 23 5 0 \njogg 1.1.3 47k 7 0 11 2 staf 2.4.5 55k 12 0 76 0 hibernate 2.0b4 57k 13 34 6 19 jaxme 1.54 58k 6 1 12 \n0 axion 1.0m2 65k 15 1 61 5 hsqldb 1.7.1 71k 18 22 8 13 cayenne 1.0b4 86k 7 2 27 6 sablecc 2.17.4 99k \n3 0 0 6 jboss 3.0.6 107k 40 134 5 53 mckoi-sql 1.0.2 118k 37 37 6 190 portal 1.8.0 162k 39 99 20 13 pcgen \n4.3.5 178k 17 0 120 0 compiere 2.4.4 230k 322 715 10 9 aspectj 1.1 319k 27 0 50 48 ptolemy2 3.0.2 362k \n27 0 504 46 eclipse 5.25.03 1.6M 126 0 181 252  total 3.9M 818 1129 1359 825 Figure 2: Error handling \nmistakes by program and policy. The Methods column indicates the to\u00adtal number of distinct methods that \ncontain viola\u00adtions. The DB , File , and Stream columns give the total number of acyclic control-.ow \npaths within those methods that violate the given policy.  4. POOR HANDLING ABOUNDS In this section \nwe apply the analysis from the previous sec\u00adtion and show that many programs make mistakes in their run-time \nerror handling. Safety Policies. We surveyed catch blocks, finally blocks, and object .nalizers in order \nto see what error han\u00addling existing programs found important. In order to make our analysis as applicable \nas possible, we looked for safety with respect to some standard Java library resources. These policies \nhave an acquire-release .avor:if a resource has been acquired, a special function must later be called \nto release it. We arrived at a set of resources and policies by sys\u00adtematically inspecting Java programs \nand then checking the o.cial de.nition in Java Platform API Speci.cation. In our experiments, any program \nthat used a resource of a certain type had at least some paths along which it adhered to that resource \ns safety policy. Correctly handling these resources is important but di.cult to do perfectly in the presence \nof run-time errors. Figure 2 shows results from this analysis. The Methods column shows the number of \nmethods that violate at least one policy. The DB policy refers to an API for linking Java programs to \nSQL databases as mentioned in Section 2. Java programs consider this policy to be particularly impor\u00adtant:the \nvast majority of finally blocks tried to deal with it. In includes ten method calls governing three resources: \nConnections, Statements, and ResultSets. The Stream policy deals with any class (even a user-de.ned one) \nthat in\u00adherits from java.io.InputStream.The Java Platform API indicates that system resources may be \nassociated with such streams. The File policy covers acquiring and releas\u00ading java.io.FileInputStreams. \nIn addition, we applied a simple Socket policy (not detailed in Figure 2) that cov\u00aders the Socket and \nServerSocket constructors and close methods and found 14 paths with violations in 4 of the pro\u00adgrams. \n Programs. The programs were taken from open source repositories (e.g., [41]), ranging from business \nsoft\u00adware (compiere) to music players (jogg), from databases (hsqldb) to games (pcgen) to heterogeneous \nconcurrent modeling (ptolemy2 [4]). In the larger programs, much of the application logic did not interact \nwith our safety policies. For example, in eclipse and ptolemy2 only 10% of the .les mentioned resources \ncovered by these safety policies, and in aspectj only 16% of the .les did, making them behave like smaller \nprograms. False Positives. Figure 2 includes every violation re\u00adported by the analysis that was not automatically \n.ltered out using the heuristic techniques presented in the previous section. All of the methods with \nerrors were then manually inspected to verify that they contained at least one error. This manual inspection \nassumed that a method could raise any of its declared exceptions. The heuristics eliminate all false \npositives that the analysis would report on these pro\u00adgrams. The heuristics reduce the number of reported \nmethods by 20% (from 1034 to 818) and the number of reported paths by 15% (from 3922 to 3320). The applicability \nof a heuris\u00adtic depends on the coding practices of the program. For example, in ejbca, which favors populating \ncatch blocks with statements like if (c != null) c.close(),there are 10 methods that are not reported \nbecause of the if .lter and 4 that are not reported because of a combination of the if and return .lters. \nIn mckoi-sql,which makes use of wrappers and accessors like getInputStream(), 25 methods are elided by \nthe return .lter, 2 are not reported because of the assignment .lter, and 1 is suppressed because of \na combination of .lters. From our perspective, such false positives are worth men\u00adtioning because they \nrepresent places where code quality could be improved by other language-level mechanisms; if an analysis \ncannot reason about the code, the programmer may not be able to either. Error Paths. All paths in Figure \n2 arose in the presence of exceptions the program did not handle correctly. More than half of these paths \nfeatured some sort of exception handling (i.e., the exception was caught), but the resource was still \nleaked. This result demonstrates that existing exception handlers contain mistakes. Java s IOException, \nSQLException and SecurityException were the three most common exceptions that programs handled poorly \nin this manner. A single path may violate multiple safety policies:for ex\u00adample, along an exceptional \npath the program might forget to close a Socket and a ResultSet. For simplicity, such cases are categorized \nin favor of the leftmost policy in Fig\u00adure 2. To give one example, of the 59 possible error paths reported \nin hibernate, 34 involved violating multiple poli\u00adcies along a single path with up to 4 forgotten resources \nat once. Errors that cross safety policies argue strongly for the need to have an error-handling mechanism \nthat supports multiple resources in sequence. In the next section we will summarize trends in error-handling \nmistakes. Finally, some programs contain some methods that never close these resources at all and others \nthat close them care\u00adfully. For example, in ejbca s HttpGetCert.sendHttpReq method, a BufferedReader \nis created but not closed (al\u00adthough two other resources are closed in that method). However, in ejbca \ns RemoveVerifyServlet.loadUserDB,a BufferedReader is given its own try-finally statement and its close \ncall is given its own exception handler within that finally block. We report sendHttpReq as a method \nwith an error-handling mistake, following Engler et al. [19], since the ejbca program takes care to handle \nBufferedReadersin some cases and is thus inconsistent with itself.  5. MISTAKE CHARACTERIZATION In this \nsection we attempt to characterize some of the er\u00adrors found by our analysis, paying special attention \nto the qualities a handling mechanism should have in order to ad\u00address these errors naturally. In some \ncases, try-finally handling is skipped entirely, as in this example from axion s ObjectBTree class: 01: \npublic void read() throws IOException, /* ... */ { 02: File idxFile = getFileById(getFileId()); 03: // \n... 04: FileInputStream fin = new FileInputStream(idxFile); 05: ObjectInputStream in = new ObjectInputStream(fin); \n06: // ... 07: in.close(); 08: fin.close(); 09: } This happens even though the annotation on line 1 and \nex\u00adtant handling in other methods mean that the programmer is aware of the possibility of run-time errors. \nSuch examples show that it would be useful to have an automatic mech\u00adanism that does the right thing \nin common cases with no programmer intervention. It is also common for try-finally statements to protect \nsome, but not all, operations, as in this fragment from staf s STAXMonitor class: 01: ObjectInputStream \nois = null; 02: try { 03: ois = new ObjectInputStream(/* ... */); 04: // ... 05: } catch (StreamCorruptedException \nex) { 06: if (ois != null) { ois.close(); } 07: showErrorDialog(/* ... */); 08: return false; 09: } 10: \nObject obj = ois.readObject(); // no try 11: ois.close(); // no finally Care is taken to deal with run-time \nerrors that occur on lines 3 4 when ois is created and used, but reading from ois on line 10 is done \nwithout an enclosing try-finally.These examples show that it would be useful to have a mechanism that \nallows .ne-grained control for some error handling but automatic behavior for others. A single project \nwill often re-use an error-handling design pattern that contains .aws. In the rest of the discussion, \nwe assume that (1) if method a1() is called then method c1() should be called, (2) c1() should not be \ncalled unless a1() succeeds (similarly for all ai() and ci()), and (3) that all methods can raise exceptions \nand su.er from run-time errors. Thus the previous example would be rendered: try { a1(); wrk(); } catch \n{ c1(); return; } wrk(); c1(); In osage multiple methods use this form: try { a1(); a2(); } finally \n{ c2(); c1(); } Such handling can fail if a2 or c2 raises an exception. Some programs, like compiere, \ntreat multiple resources se\u00adquentially but still fail to handle errors perfectly: a1(); c1(); a2(); c2(); \n// no try-finally The quartz program contains a number of instances of: try { a1(); a2(); } finally \n{ c1(); } Such partial handling covers some of the resources, but not all. The ohioedge program contains \nexamples like: for (...) { a1(); wrk(); c1(); } Such handling can be di.cult to reason about statically, \nespecially if the important resources are not variables local to the loop body. Finally, various programs \noften use .ags (and often use them correctly) to track resources and free them early: try {a1();f=0;if(...){f=1; \nc1(); }wrk(); } finally { if (!f) { c1(); } } In many cases, like the example in Section 2, error han\u00addling \nwith multiple resources contains an insu.cient num\u00adber of try statements to handle all paths. One common \napproach to handling this problem is to introduce a .ag variable (or check individual objects against \nnull), as the following examples (adapted from [7]) illustrate: 01: intf=0; 02: try { 03: a1(); f = 1; \n04: a2(); f = 2; 05: a3(); f = 3; 06: } finally { 07: switch (f) { 08: case 3: try { c3(); } catch (Exception \ne) {} 09: case 2: try { c2(); } catch (Exception e) {} 10: case 1: try { c1(); } catch (Exception e) \n{} 11: } 12: } This approach has a number of software engineering disad\u00advantages. One is that the cleanup \ncode is distant from the action code. Another is that control-.ow that determines the actions must be \nduplicated in reverse for the cleanup. Every distinct path of normal control .ow must have a cor\u00adresponding \npath in the exceptional error-handling control .ow. The following code fragment demonstrates this com\u00adplexity: \n01: intf=0; 02: try { 03: a1(); f = 1; 04: if (p2){a2();f=2; did_a2 = true; } 05: a3(); f = 3; 06: } \nfinally { 07: switch (f) { 08: case 3: try { c3(); } catch (Exception e) {} 09: case 2: if (did_a2) try \n{ c2(); } catch // ... 10: case 1: try { c1(); } catch (Exception e) {} 11: } 12: } Thus attempting to \ndeal with the issue introduces addi\u00adtional logic into the program that must be maintained (and reproduced \nat every resource use). If the control-.ow is non\u00adtrivial (e.g., a while loop or a visitor that performs \nactions on btree elements) it might not even be desirable to repro\u00adduce the control .ow (e.g., in the \nbtree case it would involve jumping to the middle of the tree and then traversing it in reverse). In \nsuch general cases it makes more sense to record which actions were taken at run-time and then clean \nup ex\u00adactly what is required. A mechanism that does not require the programmer to reproduce control .ow \nor introduce ex\u00adtra bookkeeping is desired here. In the next section we will examine destructors and \n.nalizers, which are modern pro\u00adgramming language features that could be used to address such concerns, \nand argue that they are not su.cient. 6. DESTRUCTORS AND FINALIZERS Destructors and .nalizers are existing \nprogramming lan\u00adguage features that can help programs deal with resources in the presence of run-time \nerrors. Destructors provide guaranteed cleanup actions for stack\u00adallocated objects even in the presence \nof exceptions. How\u00adever, for heap-allocated objects the programmer must still remember to explicitly \ndelete the object along all paths. We would like to generalize the notion of destructors:rather than \none implicit stack tied to the call stack, programmers should be allowed to manipulate .rst-class collections \nof obligations. In addition, programmers should have guaran\u00adtees about managing objects and actions that \ndo not have their lifetimes bound to the call stack (such objects are com\u00admon in practice see e.g., \n[22]). In many domains, multiple stacks are a more natural .t with the application. For exam\u00adple, a web \nserver might store one such stack for each concur\u00adrent request. If the normal request encounters an error \nand must abort and release its resources, there is generally no reason that another request cannot continue. \nDestructors can be invoked early, but would typically have to include a .ag to ensure that actions are \nnot duplicated when it is called again. We believe such bookkeeping should be auto\u00admatic. Destructors \nare tied to objects and there are many cases where a program would want to change the state of the object, \nrather than destroying it. We shall return to that consideration in Section 7.1. Compared to pure .nalizers, \nmost programmer-speci.ed error handling must be more immediate and more determin\u00adistic. Finalizers are \narguably well-suited to resources like .le descriptors that must be collected but need not be collected \nright away.2 In contrast, the database locks from the exam\u00adple in Section 2 should be released as quickly \nas possible, making .nalizers an awkward .t for performance reasons. We want a mechanism that is well-suited \nto being invoked early, and while .nalizers can be called in advance they su.er from the same disadvantages \nas destructors in that regard. Like destructors, .nalizers can be invoked early but doing so typically \nrequires additional bookkeeping. More importantly, .nalizers in Java come with no order guarantees [24]. \nFor example, a Stream built on (and ref\u00aderencing) a Socket mightbe.nalized after thatsocketif they are \nboth found unreachable in the same garbage col\u00adlection pass. If the arbitrary cleanup actions above were \nto be handled by .nalizers on dependent objects, the natural trick of adding an extra pointer .eld to \nthe child object pointing to the parent object in order to ensure that the 2Even this use of .nalizers \nis often discouraged because programs have a limited number of .le descriptors and can easily race with \nthe garbage collector to exhaust them.  child action is called before the parent action would not be \nsound. Thus we desire an error handling mechanism that can strictly enforce such dependencies and provide \na more intuitive ordering for cleanup actions. While such depen\u00addencies could be encoded in a .nalizer \nsystem, we did not observe such a system in any of the programs we examined in Section 4. Finally, it \nis worth noting that Java programmers do not make even a sparing use of .nalizers to address these prob\u00adlems. \nSome Java implementations do not implement .naliz\u00aders correctly [5], .nalizers are often viewed as unpredictable \nor dangerous, and the delay between .nishing with the re\u00adsource and having the .nalizer called may be \ntoo great. In all of the code surveyed in Section 4, there were only 13 user\u00adde.ned .nalizers (hibernate \nhad 4; osage had 3; jboss and eclipse had 2; javad and aspectj had 1). In our experi\u00adence, Java programmers \nbasically do not use .nalizers. One might also hope that standard libraries would make use of .\u00adnalizers, \nbut this is not always the case. The GNU Classpath 0.05 implementation of the Java Standard Library does \nnot use .nalizers for any of the resources governed by the safety policies in Section 4. Sun s JDK 1.3.1 \n07 does use them, but only in some situations (e.g., for database connections but not for sockets). While \nother or newer Standard Libraries may well use .nalizers for all such important resources, one cannot \ncurrently portably count on the Library to do so. We would like to make something like .nalizers more \nuseful to Java programmers by making them easier to use and giving them destructor-like properties. The \nresults in Section 4 argue that language support is necessary:merely making a better Socket library will \nnot help if Sockets, databases, and user-de.ned resources must be dealt with together. Using exception \nhandling to han\u00addle run-time errors is di.cult. In the next section, we will describe language mechanisms \nthat make it easy to do the right thing:all of the mistakes presented here could have been avoided using \nour proposed language extensions. In addition, the analysis presented in this section can easily ver\u00adify \nthat programs using our mechanisms are handling these resources correctly. 7. COMPENSATIONS Based on \nour characterization of existing mistakes and coding practices in Section 5 and existing programming \nlan\u00adguage techniques in Section 6, we propose a language ex\u00adtension where program actions and interfaces \nare annotated with compensations, which are closures containing arbi\u00adtrary code. At run-time, these compensations \nare stored in .rst-class stacks. Compensation stacks can be thought of as generalized destructors, but \nwe emphasize that they can be used to execute arbitrary code and not just call functions upon object \ndestruction. Our compensation stacks are an adaptation of the database notions of compensating transactions \nand linear sagas [21]. A compensating transaction semantically un\u00addoes the e.ect of another transaction \nafter that transaction has committed. A saga is a long-lived transaction seen as a sequence of atomic \nactions a1...an with compensating trans\u00adactions c1...cn. This system guarantees that either a1...an executes \nor a1...akck...c1 executes. Note that the compen\u00adsations are applied in reverse order. We have found \nthis model to be a good .t for this sort of run-time error han\u00addling. Many conceptually simple program \nactions actually require that multiple resources be handled in sequence. Our system allows programmers \nto link actions with com\u00adpensations, and guarantees that if an action is taken, the program cannot terminate \nwithout executing the associated compensation. Compensation stacks are .rst-class objects that store \nclosures. They may be passed to methods or stored in object .elds. The Java language syntax is ex\u00adtended \nto allow arbitrary closures to be pushed onto com\u00adpensation stacks. These closures are later executed \nin a last-in, .rst-out order. Closures may be run early by the programmer, but they are usually run automatically \nwhen a stack-allocated compensation stack goes out of scope or when a heap-allocated compensation stack \nis .nalized. If a compensating action raises an exception while executing, the exception is logged but \ncompensation execution contin\u00adues. 3 When a compensation terminates (either normally or exceptionally), \nit is removed from the compensation stack. Compensation stacks normally behave like generalized de\u00adstructors, \ndeallocating resources based on lexical scoping, but they are also .rst-class collections that can be \nput in the heap and that make use of .nalizers to ensure that their contents are eventually executed. \nThe ability to execute some compensations early is important and allows the com\u00admon programming idiom \nwhere critical shared resources are freed as early as possible along each given path. In addition, the \nprogram can explicitly discharge an obligation without executing its code (presumably based on outside \nknowledge not directly encoded in the safety policy). This .exibility allows compensations that truly \nundo e.ects to be avoided on successful executions, and it requires that the program\u00admer annotate a small \nnumber of success paths rather than every possible error path. Additional compensation stacks may be \ndeclared to create a nested transaction e.ect. Fi\u00adnally, the analysis in Section 3 can be easily modi.ed \nto show that programs that make use of compensation stacks do not forget obligations. 7.1 Implementation \nWe implemented compensation stacks using a source-level transformation for Java programs. This entails \nde.ning a CompensationStack class, adding support for closures (as in [37]), and adding convenient syntactic \nsugar for lexically\u00adscoped compensation stacks. In our system, the client code from Section 2 looks like \nthis: 01: Connection cn; PreparedStatement ps; ResultSet rs; 02: cn = ConnectionFactory.getConnection(/* \n... */); 03: StringBuffer qry = ...; // do some work 04: ps = cn.prepareStatement(qry.toString()); 05: \nrs = ps.executeQuery(S); 06: ... // do I/O-related work with rs 3Neither Java .nalizers nor POSIX cleanup \nhandlers propagate such exceptions. Lisp s unwind-protect may not execute all cleanup actions if one \nraises an exception. In analogous situ\u00adations, C++ aborts the program. Since our goal is to keep the \nprogram running and restore invariants, we choose to log such ex\u00adceptions. Ideally, error-prone compensations \nwould contain their own internal compensation stacks for error handling. A second option would be to \nhave the type system statically verify that a compensation cannot raise an exception. In the particular \nexam\u00adple of Java, this solution is not desirable. First, it would require checking unchecked exceptions, \nwhich is non-intuitive to most Java programmers. Second, most compensations can, in fact, raise exceptions \n(e.g., close can raise an IOException). All of the release actions are handled automatically, even in \nthe presence of run-time errors. An implicit CompensationStack based on the method scope is being used \nand the resource-acquiring methods have been anno\u00adtated to use such stacks. We will now elaborate those \nde\u00adtails and develop our system to the point where such code behaves correctly along all paths. The .rst \nstep in such an approach is to annotate the in\u00adterface of methods that acquire important resources. For \nexample, we would associate with the action getConnection the compensation close at the interface level \nso that all uses of Connections can be a.ected. Consider this code: public Connection getConnection() \nthrows SQLException { // ... do work ... } We would change it so that a CompensationStack argu\u00adment is \nrequired. The syntax compensate { a } with { c } using (S) corresponds to executing the action a and \nthen pushing the compensation code c on the stack S if a com\u00adpleted normally. The modi.ed de.nition follows: \npublic Connection getConnection(CompensationStack S) throws SQLException { compensate { /* ... do work \n... */ } with { this.close(); } using (S); } As we mentioned in Section 6, this mechanism has the ad\u00advantages \nof early release and proper ordering over just using .nalizers. Not all actions and compensations must \nbe asso\u00adciated at the function-call level; arbitrary code can be placed in compensations. After annotating \nthe database interface with compensation information, the client code might look like this: 01: Connection \ncn; PreparedStatement ps; ResultSet rs; 02: CompensationStack S = new CompensationStack(); 03: try { \n04: cn = ConnectionFactory.getConnection(S, /* ... */); 05: StringBuffer qry = ...; // do some work 06: \nps = cn.prepareStatement(S, qry.toString()); 07: rs = ps.executeQuery(S); 08: ... // do I/O-related work \nwith rs 09: } finally { 10: S.run(); 11: } As the program executes, closures containing compensa\u00adtion \ncode are pushed onto the CompensationStack S.Com\u00adpensations are recorded at run-time, so resources can \nbe ac\u00adquired in loops or other procedures. Before a stack becomes inaccessible, all of the associated \ncompensations must be ex\u00adecuted. A particularly common use involves lexically scoped compensation stacks \nthat essentially mimic the behavior of destructors. We add syntactic sugar allowing a keyword (e.g., \nmethodScopeStack) to stand for a compensation stack that is allocated at the beginning of the enclosing \nscope and finally executed at the end of it. In addition, we optionally allow that special stack to be \nused for omitted compensation stack parameters. We thus arrive at the six-line version at the beginning \nof this section for the common case. Compensations can contain arbitrary code, not just method calls. \nFor example, consider this code fragment adapted from [7]: 01: try { 02: StartDate = new Date(); 03: \ntry { 04: StartLSN = log.getLastLSN(); 05: ... // do work 1 06: try { 07: DB.getWriteLock(); 08: ... \n// do work 2 09: } finally { 10: DB.releaseWriteLock(); 11: ... // do work 3 12: } 13: } finally { 14: \nStartLSN = -1; 15: } 16: } finally { 17: StartDate = null; 18: } We might rewrite it as follows, using \nexplicit CompensationStacks: 01: CompensationStack S = new CompensationStack(); 02: try { 03: compensate \n{ StartDate = new Date(); } 04: with { StartDate = null; } using (S); 05: compensate { StartLSN = log.getLastLSN(); \n} 06: with { StartLSN = -1; } using (S); 07: ... // do work 1 08: compensate { DB.getWriteLock(); } 09: \nwith { DB.releaseWriteLock(); 10: ... (*dowork3*) } 11: ... // do work 2 12: } finally { 13: S.run(); \n14: } Resource .nalization and state changes are thus handled by the same mechanism and bene.t from the \nsame order\u00ading. Traditional destructors are tied to objects, and there are many cases where a program \nwould want to change the state of the object rather than destroying it. Destructors could be used here \nby creating arti.cial objects that are stack-allocated and perform the appropriate state changes on the \nenclosing object. However, such a solution would not be natural. For example, the program from which \nthe last example was taken had 17 unique compensations (i.e., error\u00adhandling code that was site-speci.c \nand never duplicated) with an average length of 8 lines and a maximum length of 34 lines. Creating a \nnew arti.cial object for each unique bit of error-handling logic would be burdensome, especially since \nmany of the compensations had more than one free variable (which would generally have to be passed as \nextra arguments to the helper constructor). Nested try-finally blocks could also be used but are error-prone \n(see Section 2 and Section 4). Previous approaches to similar problems can be vast and restrictive departures \nfrom standard semantics (e.g., linear types or transactions) or lack support for common idioms (e.g., \nrunning or discharging obligations early). We designed this mechanism to integrate easily with new and \nexisting programs, and we needed all of its features for our case stud\u00adies. With this feature, we found \nit easy to avoid the mistakes that were reported hundreds of times in Section 4. In the common case of \na lexically-scoped linear saga of resources, the error handling logic needs to be written only once with \nan interface, rather than every time a resource is acquired. In more complicated cases (e.g., storing \ncompensations in heap variables and associating them with long-lived objects) extra .exibility is available \nwhen it is needed.  8. CASE STUDIES We hand-annotated two programs to show that it is easy to modify \nexisting programs to use compensation stacks (and by implication that it would not be di.cult to write \na new program from scratch using them) and to demon\u00adstrate that the run-time overhead is low. Guided \nby the data.ow analysis in Section 3, the programs were modi\u00ad.ed so that their existing error-handling \nmakes use of com\u00adpensation stacks; no truly new error handling was added (even when inspection revealed \nit to be missing) and the be\u00adhavior was otherwise unchanged. In the common case this amounted to removing \nan existing close call (and possi\u00adbly its guarding finally) and using a CompensationStack instead (possibly \nwith a method that had been annotated to take a compensation stack parameter). Maintaining the stacks \nand the closures takes time, but that overhead was dwarfed by the I/O latency in our case studies. The \n.rst case study, Aaron Brown s undoable email store [7], can be viewed as an SMTP and IMAP proxy that \npro\u00advides operators with system-level time travel. The origi\u00adnal version was 35,412 lines of Java code. \nAnnotating the program took about four hours and involved updating 128 sites with code to use compensations \nas well as annotat\u00ading the interfaces for some standard library methods (e.g., sockets and databases). \nThe resulting program was 225 lines shorter (about 1%) because redundant error-handling code and control-.ow \nwere removed. The program contains non-trivial error handling, including one .ve-step saga of actions \nand compensations and one three-step saga. Sin\u00adgle compensating actions ranged from simple close calls \nto 34-line code blocks with internal exception handling and synchronization. Using .fty microbenchmarks \nand one ex\u00adample workload (all provided by the original author), the annotated program s performance \nwas almost identical to the original. Performance was measured to be within one standard deviation of \nthe original, and was generally within one half of a standard deviation; the run-time overhead as\u00adsociated \nwith keeping track of obligations at run-time was dwarfed by I/O and other processing times. Compensa\u00adtions \nwere used to handle every request answered by the program. Finally, by changing a method invocation in \nsome insu.ciently-guarded cleanup code to always raise one of its declared run-time errors in both versions \nof the program, we were able to cause the unmodi.ed version of the program to drop all SMTP requests. \nThe version using compensations handled that cleanup failure correctly and proceeded nor\u00admally. While \nthis sort of targeted fault injection is hardly representative, it does show that the errors we are address\u00ading \nwith compensations can have an impact on reliability. The second case study, Sun s Pet Store 1.3.2 [42], \nis a web-based, database-backed retailing program. The original version was 34,608 lines of Java code. \nAnnotations to 123 sites took about two hours. The resulting program was 168 lines smaller (about 0.5%). \nMost error handling annotations centered around database Connections. Using an indepen\u00addent workload \n[11, 8], the original version raises 150 ex\u00adceptions from the PurchaseOrderHelper s processInvoice method \nover the course of 3,900 requests. The exceptions signal run-time errors related to RelationSets being \nheld too long (e.g., because they are not cleared along with their connections on some paths) and are \ncaught by a middle\u00adware layer which restarts the application.4 The annotated 4While updating a purchase \norder to re.ect items shipped, the processInvoice method creates an Iterator from a RelationSet Collection \nthat deals with persistent data in a database. Un\u00adfortunately, the transaction associated with the RelationSet \nhas version of the program raises no such exceptions:compen\u00adsation stacks ensure that the database objects \nare handled correctly. The average response times for the original pro\u00adgram (over multiple runs) is 52.06 \nmilliseconds (ms), with a standard deviation of 100 ms. The average response time for the annotated program \nis 43.44 ms with a standard de\u00adviation of 77 ms. The annotated program is both 17% faster and also more \nconsistent because less middleware interven\u00adtion was necessary. Together, these case studies suggest \nthat stacks of com\u00adpensations are a natural and e.cient model for this sort of run-time error handling. \nThe decrease in code size argues that common idioms are captured nicely by this formal\u00adism. The unchanging \nor improved performance indicates that leaving some checks to run time is quite reasonable. Finally, \nthe checks ensure that cleanup code is invoked cor\u00adrectly along all paths through the program. 9. RELATED \nWORK Beyond destructors and .nalizers, previous related work falls into .ve main categories:type systems, \nregions, ex\u00adception schemes, ideas on error handling, and transactional models. Type systems. Flow-sensitive \ntype systems check many of the same safety properties that our system enforces. The key di.erence is \nthat a strong type system will reject a pro\u00adgram that cannot be statically shown to adhere to the safety \npolicy, whereas our system will use run-time instrumenta\u00adtion to ensure compliance. In addition, most \nsuch type sys\u00adtems work at the level of the resources themselves. DeLine and F\u00a8ahndrich [15] propose \nthe Vault language and static linear type system for enforcing high-level soft\u00adware protocols. Vault \nrepresents a di.erent point in the design space, with more powerful properties but a more dif\u00ad.cult programming \nmodel. It can verify that operations are performed on resources in a certain order (e.g., that open is \ncalled before read), while we cannot. It can also en\u00adsure that an operation is in a thread s computational \nfuture (e.g., that an opened resource is closed by the end of the method). Vault s variant keys (e.g., \nspecial objects that are either empty or contain a key) can be used to free an object early on one path \nand free it later on another. These vari\u00adants require the programmer to make an explicit run-time check \nto determine if the key has already been freed. Our system handles this aspect slightly more naturally \nby per\u00adforming that check automatically. On the other hand, our system lacks stateful keys. Perhaps the \ngreatest drawback of Vault is that it requires much of the program to adhere to a linear type system. \nLinear type systems are generally considered to be di.cult to work with, and structuring a program to \n.t a linear type system is often a herculean task. Later work [20] extends the Vault type system with \nadditional features that ease the burden of programming with linear types, but aliasing can still be \ndi.cult. Regions. Gay and Aiken [22] propose a system for mem\u00adory management using explicit .rst-class \nregions [43]. Their regions are conceptually similar to our compensation stacks. In their system, reference \ncounts keep programs from delet\u00ading regions too early. In our system, stacks keep programs from forgetting \nto perform compensations. Regions allow already been completed. one to express data locality, whereas \nPutting compensations in the same stack allows the programmer to express the con\u00adceptual locality of \na compound transaction. Exceptions. Most modern programming languages fea\u00adture exceptions that behave \naccordingtothe replacement model [23, 45] (see also [31, 14, 36, 26, 38]). Alonso et al. [2] believe \nthat poor support for exception handling is a major obstacle for large-scale and mission-critical systems. \nHagen et al. [27] claim that exception handling must be separated from normal code if processes are to \nbe reused like libraries. This separation is similar to our goal of annotating interfaces with compensation \ninformation. Common Lisp s unwind-protect body cleanup behaves like try-finally and ensures that cleanup \nwill be ex\u00adecuted no matter how control leaves body. To han\u00addle a common case, the macro with-open-file \nstream body opens andcloses stream automatically as appro\u00adpriate. Since Lisp comes with .rst-class funcitons \nand macros, unwind-protect can be used more conveniently than Java s try-finally with respect to duplicate \nand unique error handling. However, it still su.ers from many of the same limitations (e.g., no easy \nway to discharge obli\u00adgations early, one nesting level per resource, one global stack). In Scheme dynamic-wind \nbefore work after and call-with-open-file serve similar purposes, although dynamic-wind is complicated \nby the presence of continua\u00adtions (e.g., the dynamic extent of work may not be a single time period). \nDony [17] describes an object-oriented exception handling system where all exception handlers have a \ndynamic call\u00adstack scope. Dony s form of unwind-protect is similar to our approach, although it o.ers \nno support for discharging obligations early or for a .rst-class handling of the current set of pending \nobligations. Cargill [10] argues that without extraordinary care ex\u00adceptions actually diminish the overall \nreliability of software. The hard part of exception handling is not raising exceptions but writing the \nsupport code so that errors are handled cor\u00adrectly. Our technique is particularly well-suited to handling \nthe matched acquire-free behavior in his presentation. Error handling. Valetto and Kaiser [44] note that \nadap\u00adtation to errors usually involves several conditional or de\u00adpendent activities that may fail; the \nlinear saga model we support is rich enough to capture many dependent activities. Cardelli and Davies \n[9] present a language for writing pro\u00adgrams with an explicit notion of failure. We have a less holistic \nnotion of run-time errors but have an easier time integrating with existing code. Demsky and Rinard [16] \nallow defects in key data struc\u00adtures to be repaired at run-time based on speci.cations. Their technique \nworks at the level of data structures and not at the level of program actions, and it may be viewed as \naddressing an orthogonal problem. For example, their ap\u00adproach does not lend itself naturally to I/O-based \nrepairs and ours does not handle logical errors in compensation code. The POSIX thread library (IEEE \n1003.1c-1995) provides a per-thread cancellation cleanup stack (pthread cleanup push and pop). The cleanup \nrou\u00adtines are executed when the thread exits or is canceled. However, the cleanup stack is not a .rst-class \nobject, so cleanup code must be associated with the thread and not with an object. In addition, only \nthe most recently-added cleanup code can be executed early or removed from the stack. Also, those two \nactions may only be taken inside the same lexical scope as their corresponding push.The stack uses C-style \nfunction pointers, so general error-handling (like that of undo in Section 8) requires the creation of \nseparate functions. Finally, the mechanism can only be used safely in deferred cancellation mode because \nperforming the action and pushing the cleanup code are not done atomically with respect to thread cancellation. \nOur compensate-with expression handles this issue in Java, where thread cancellation is signaled via \nexceptions. The VINO operating system [39] uses software fault iso\u00adlation and lightweight transactions \nto address problems like resource hoarding in user-de.ned kernel extensions. This form is similar to \nour approach in that an interface has been annotated with compensations that are called if a fatal error \noccurs. However, in VINO there is only one compensation stack per extension, and it is not a .rst-class \nobject. In ad\u00addition, there is no support for nested transactions without de.ning additional extensions. \nTransactions. Database transactions provide a strong and well-founded approach to error handling [25]. \nHowever, many .nd the consistency and durability of transactions to be too heavyweight for most programming \npurposes (e.g., [2, 32, 14]). Restructuring a program to make use of transactions can be a large, invasive \nchange. Borg et al. [6] describe a check\u00adpointing system that allows unmodi.ed programs to survive hardware \nfailures. Essentially, every system call is inter\u00adcepted and logged. Others (e.g., [35, 40]) provide \nsimilar services. Our compensation annotations are a much less drastic change to the program semantics \nthan the incorpo\u00adration of full-.edged transactions. In addition, these transaction techniques address \nan or\u00adthogonal error handling issue. In Borg et al. s system, a buggy process that acquires a lock twice \nand deadlocks on initialization will continue to deadlock no matter how many times it is recovered. Lowell \net al. [34] formalize this point by noting that the desire to log all events actually con.icts with the \nability to recover from all errors. Such systems are very good at masking hardware failures and quite \npoor at masking software failures; Lowell et al. suggest that 85 95% of application bugs cause crashes \nthat would not be prevented by a failure-transparent operating system. Our technique hopes to address \nthose sorts of bugs, although it is less automatic. Many researchers have found that advanced transac\u00adtional \nconcepts .t closely with language-level error handling (e.g., [33]). One such concept, the compensating \ntransac\u00adtion, semantically undoes the e.ects of another transaction after that transaction has been committed \n[30]. Designing a full compensating transaction that completely undoes the e.ects of a previous action \nis often di.cult. Our system relaxes this requirement. Our system is also slightly more general than \na pure linear saga [30] and more closely resem\u00adbles a form of nested or interleaved linear sagas. 10. \nFUTURE WORK Stack inference. In order to make this language fea\u00adture as useful as possible to existing \nprograms, we hope to reduce the annotation burden on the programmer. We can manually annotate the interfaces \nfor several generally appli\u00adcable safety policies, as in Section 4. Such interfaces would then be useful \nfor any program that uses those libraries. A further step would be to devise an inference algorithm for \ncompensation stack placement, similar to algorithms for re\u00adgion inference [43]. The goal would be to \nplace compensation stack declarations so as to give them the smallest lifetime possible without freeing \nany resources while they are still in use (or otherwise invoking compensation code too early). Function \ncalls that require compensation stacks would use the nearest enclosing stack. Given such an inference \nalgo\u00adrithm and annotated interfaces, this technique could be ap\u00adplied automatically to existing programs \nfor certain safety policies. Safety policies. We also plan to move to more interest\u00ading safety policies. \nSince such policies are usually program speci.c, we hope to mine speci.cations from the program source \ncode. Our experiments indicate that when programs consider a resource, they handle it correctly at least \nsome of the time. We hope to leverage existing techniques (e.g., 3, 28, 19]) to allow us to infer possible \nspeci.cations. Such speci.cations can then be presented to the programmer and, if accepted, can be applied \nautomatically. Speci.cation min\u00ading based on the source code generally yields an unaccept\u00adable number \nof false positives. In this sort of application domain, however, where extra code will be executed at \nrun\u00adtime rather than having the project rejected at compile\u00adtime, such false positives may be slightly \nmore acceptable. Theoretical work. Finally, much theoretical work re\u00admains to be done. We would like \nto have a formal proof of our safety guarantees for a more .exible system:if a pro\u00adgram uses our features \nand type-checks according to our rules, then it can be guaranteed that its actions will be paired with \ncompensations. We also hope to examine the interactions between this language feature and other fea\u00adtures. \nFor example, there are a number of known issues re\u00adlating .nalizers, concurrency and deadlocks [5]. We \nhope to prove, for example, that using this method (or a restriction to it) to keep track of compensations \nwill not introduce any concurrency problems that were not present in the original code. We would also \nlike to present a unifying framework for compensation stacks and regions. 11. CONCLUSIONS We have presented \nan analysis that discovers when ex\u00adisting programs fail to restore invariants or invoke cleanup code \nin the presence of run-time errors. Using this analysis we have discovered over 800 methods with mistakes \nin their error-handling in almost 4 million lines of code. We then analyzed those mistakes qualitatively \nand we have discussed the strengths and weaknesses of exceptions, destructors and .nalizers for run-time \nerror handling. Based on that analy\u00adsis, we have proposed a programming language feature based on advanced \ntransaction models. Stacks of compensations are .rst-class objects that can be manipulated by the pro\u00adgram \nand used to store compensating actions. They behave much like destructors but provide a more general \nstack struc\u00adture, guarantees on heap objects, and easy early execution and bookkeeping. Compensations \nthemselves are recorded and executed at run time. Case studies show that such a feature can be used to \nachieve improved reliability with minimal overhead. Since error handling is a large and im\u00adportant part \nof programs, .nding error-handling mistakes and suggesting features that would help to prevent them is \nan important step toward making more robust programs. 12. ACKNOWLEDGMENTS We thank Aaron Brown for providing \nan explanation of and workload for his undo program, as well as fruitful dis\u00adcussion about Java error \nhandling. Mark Brody, George Candea and Tom Martell generously provided their infras\u00adtructure and their \nworkload generator for Pet Store.We thank Christopher Hylands Brooks for an insightful discus\u00adsion into \nerror handling in general and in ptolemy2.The anonymous reviewers provided useful suggestions, especially \nwith respect to experimental results. Finally, one of the au\u00adthors would like to thank Dr. Loren Rhodes \nfor very early encouragement. 13. REFERENCES [1] Advisor. Beware:10 common web application security \nrisks. Technical Report Doc 11756, Security Advisor Portal, Jan. 2003. [2] G. Alonso, C. Hagen, D. Agrawal, \nA. E. Abbadi, and C. Mohan. Enhancing the fault tolerance of work.ow management systems. IEEE Concurrency, \n8(3):74 81, July 2000.  [3] G.Ammons,R.Bodik,and J. R. Larus. Mining speci.cations. In ACM Symposium \non Principles of Programming Languages, pages 4 16, 2002. [4] P. Baldwin, S. Kohli, E. A. Lee, X. Liu, \nand Y. Zhao. Modeling of sensor nets in Ptolemy II. In Proceedings of Information Processing in Sensor \nNetworks, Apr. 2004. [5] H.-J. Boehm. Destructors, .nalizers and synchronization. In ACM Symposium on \nPrinciples of Programming Languages. ACM, Jan. 2003. [6] A. Borg, W. Blau, W. Graetsch, F. Herrmann, \nand W. Oberle. Fault tolerance under UNIX. ACM Transactions on Computer Systems, 7(1), Feb. 1989.  \n[7] A. Brown and D. Patterson. Undo for operators: Building an undoable e-mail store. In USENIX Annual \nTechnical Conference, 2003. [8] G. Candea, M. Delgado, M. Chen, and A. Fox. Automatic failure-path inference:A \ngeneric introspection technique for internet applications. In IEEE Workshop on Internet Applications.San \nJose, California, June 2003. [9] L. Cardelli and R. Davies. Service combinators for web computing. Software \nEngineering, 25(3):309 316, 1999. [10] T. Cargill. Exception handling:a false sense of security. C++ \nReport, 6(9), 1994. [11] M. Chen,E. Kiciman,E.Fratkin,E.Brewer, and A. Fox. Pinpoint:Problem determination \nin large, dynamic, internet services. In International Conference on Dependable Systems and Networks, \nWashington D.C., 2002.  [12] F. Cristian. Exception handling. Technical Report RJ5724, IBM Research, \n1987. [13] M. Das, S. Lerner, and M. Seigle. Esp:path-sensitive program veri.cation in polynomial time. \nSIGPLAN Not., 37(5):57 68, 2002. [14] U. Dayal, M. Hsu, and R. Ladin. Organizing long-running activities \nwith triggers and transactions. In Proceedings of ACM SIGMOD, pages 204 214. Atlantic City, May 1990. \n[15] R. DeLine and M. F\u00a8ahndrich. Enforcing high-level protocols in low-level software. In ACM Conference \non Programming Language Design and Implementation, pages 59 69, 2001. [16] B. Demsky and M. C. Rinard. \nAutomatic data structure repair for self-healing systems. In ACM Conference on Object-Oriented Programming, \nSystems, Languages, and Applications, 2003. [17] C. Dony. A fully object-oriented exception handling \nsystem. In Advances in Exception Handling Techniques, volume 2022 of Lecture Notes in Computer Science, \npages 18 38, 2001. [18] D. Engler, B. Chelf, A. Chou, and S. Hallem. Checking system rules using system-speci.c, \nprogrammer-written compiler extensions. In Symposium on Operating Systems Design and Implementation, \n2000. [19] D. R. Engler, D. Y. Chen, and A. Chou. Bugs as deviant behavior:A general approach to inferring \nerrors in systems code. In Symposium on Operating Systems Principles, pages 57 72, 2001. [20] M. F\u00a8ahndrich \nand R. DeLine. Adoption and focus: Practical linear types for imperative programming. In ACM Conference \non Programming Language Design and Implementation, June 2002. [21] H. Garcia-Molina and K. Salem. Sagas. \nIn ACM Conference on Management of Data, pages 249 259, 1987. [22] D. Gay and A. Aiken. Memory management \nwith explicit regions. In ACM Conference on Programming Language Design and Implementation, pages 313 \n323, 1998. [23] J. B. Goodenough. Exception handling:issues and a proposed notation. Communications of \nthe ACM, 18(12):683 696, 1975. [24] J. Gosling, B. Joy, and G. L. Steele. The Java Language Speci.cation. \nThe Java Series. Addison-Wesley, Reading, MA, USA, 1996. [25] J. Gray. The transaction concept:virtues \nand limitations. In International Conference on Very Large Data Bases, pages 144 154. Cannes, France, \nSept. 1981. [26] C. Hagen and G. Alonso. Flexible exception handling in the OPERA process support system. \nIn International Conference on Distributed Computing Systems, pages 526 533, 1998. [27] C. Hagen and \nG. Alonso. Exception handling in work.ow management systems. IEEE Transactions on Software Engineering, \n26(9):943 959, Sept. 2000. [28] S. Hangal and M. S. Lam. Tracking down software bugs using automatic \nanomaly detection. In International Conference on Software Engineering,May 2002. [29] G. A. Kildall. \nA uni.ed approach to global program optimization. In Proceedings of the 1st annual ACM SIGACT-SIGPLAN \nsymposium on Principles of programming languages, pages 194 206. ACM Press, 1973. [30] H. F. Korth, E. \nLevy, and A. Silberschatz. A formal approach to recovery by compensating transactions. In The VLDB Journal, \npages 95 106, 1990. [31] R. Levin. Program structures for exceptional condition handling. PhD thesis, \nCarnegie Mellon University, June 1977. [32] B. Liskov and R. Schei.er. Guardians and actions: Linguistic \nsupport for robust, distributed programs. ACM Transactions on Programming Languages and Systems, 5(3):381 \n404, July 1983. [33] C. Liu, M. E. Orlowska, X. Lin, and X. Zhou. Improving backward recovery in work.ow \nsystems. In Conference on Database Systems for Advanced Applications, Apr. 2001. [34] D. E. Lowell, S. \nChandra, and P. M. Chen. Exploring failure transparency and the limits of generic recovery. In USENIX \nSymposium on Operating Systems Design and Implementation, Oct. 2000. [35] D. E. Lowell and P. M. Chen. \nDiscount checking: transparent, low-overhead recovery for general applications. Technical Report CSE-TR-410-99, \nUniversity of Michigan, Nov. 1998. [36] R. Miller and A. Tripathi. Issues with exception handling in \nobject-oriented systems. In Object-Oriented Programming, 11th European Conference (ECOOP), pages 85 103, \n1997. [37] M. Odersky and P. Wadler. Pizza into Java: Translating theory into practice. In ACM Symposium \non Principles of Programming Languages, pages 146 159, 1997. [38] M. P. Robillard and G. C. Murphy. \nRegaining control of exception handling. Technical Report TR-99-14, Dept. of Computer Science, University \nof British Columbia, 1, 1999. [39] M. I. Seltzer, Y. Endo, C. Small, and K. A. Smith. Dealing with disaster:Surviving \nmisbehaved kernel extensions. In Symposium on Operating Systems Design and Implementation, pages 213 \n227, Seattle, Washington, 1996. [40] J. S. Shapiro, J. M. Smith, and D. J. Farber. EROS:a fast capability \nsystem. In Symposium on Operating Systems Principles, pages 170 185, 1999. [41] SourceForge.net. About \nSourceForge.net (document A1). http://sourceforge.net. Technical report, 2003. [42] Sun Microsystems. \nJava pet store 1.1.2 blueprint application. http://java.sun.com/blueprints/code/. Technical report, 2001. \n[43] M. Tofte and J.-P. Talpin. Region-based memory management. Information and Computation, 1997. [44] \nG. Valetto and G. Kaiser. A case study in software adaptation. In ACM Workshop on Self-Healing Systems \n(WOSS 02), pages 73 78, Nov. 2002. [45] S. Yemini and D. Berry. A modular veri.able exception handling \nmechanism. ACM Transactions on Programming Languages and Systems, 7(2), Apr. 1985.  \n\t\t\t", "proc_id": "1028976", "abstract": "<p>It is difficult to write programs that behave correctly in the presence of run-time errors. Existing programming language features often provide poor support for executing clean-up code and for restoring invariants in such exceptional situations. We present a dataflow analysis for finding a certain class of error-handling mistakes: those that arise from a failure to release resources or to clean up properly along all paths. Many real-world programs violate such resource safety policies because of incorrect error handling. Our flow-sensitive analysis keeps track of outstanding obligations along program paths and does a precise modeling of control flow in the presence of exceptions. Using it, we have found over 800 error handling mistakes almost 4 million lines of Java code. The analysis is unsound and produces false positives, but a few simple filtering rules suffice to remove them in practice. The remaining mistakes were manually verified. These mistakes cause sockets, files and database handles to be leaked along some paths. We present a characterization of the most common causes of those errors and discuss the limitations of exception handling, finalizers and destructors in addressing them. Based on those errors, we propose a programming language feature that keeps track of obligations at run time and ensures that they are discharged. Finally, we present case studies to demonstrate that this feature is natural, efficient, and can improve reliability; for example, retrofitting a 34kLOC program with it resulted in a 0.5% code size decrease, a surprising 17% speed increase (from correctly deallocating resources in the presence of exceptions), and more consistent behavior.</p>", "authors": [{"name": "Westley Weimer", "author_profile_id": "81100631608", "affiliation": "University of California - Berkeley", "person_id": "PP18002460", "email_address": "", "orcid_id": ""}, {"name": "George C. Necula", "author_profile_id": "81100295630", "affiliation": "University of California - Berkeley", "person_id": "PP14109324", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1028976.1029011", "year": "2004", "article_id": "1029011", "conference": "OOPSLA", "title": "Finding and preventing run-time error handling mistakes", "url": "http://dl.acm.org/citation.cfm?id=1029011"}