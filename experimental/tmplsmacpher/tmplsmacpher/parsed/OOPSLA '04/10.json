{"article_publication_date": "10-01-2004", "fulltext": "\n Decentralizing Execution of Composite Web Services Mangala Gowri Nanda Satish Chandra Vivek Sarkar \nIBM India Research IBM India Research IBM T. J. Watson Research Laboratory Laboratory Center mgowri@in.ibm.com \nsatishchandra@in.ibm.com vsarkar@us.ibm.com ABSTRACT Distributed enterprise applications today are increasingly \nbeing built from services available over the web. A unit of functionality in this framework is a web \nservice, a soft\u00adware application that exposes a set of typed connections that can be accessed over the \nweb using standard protocols. These units can then be composed into a composite web service. BPEL (Business \nProcess Execution Language) is a high-level distributed programming language for creating composite web \nservices. Although a BPEL program invokes services distributed over several servers, the orchestration \nof these services is typically under centralized control. Because performance and throughput are major \nconcerns in enterprise applica\u00adtions, it is important to remove the ine.ciencies introduced by the centralized \ncontrol. In a distributed, or decentralized orchestration, the BPEL program is partitioned into inde\u00adpendent \nsub-programs that interact with each other without any centralized control. Decentralization can increase \npar\u00adallelism and reduce the amount of network tra.c required for an application. This paper presents \na technique to partition a compos\u00adite web service written as a single BPEL program into an equivalent \nset of decentralized processes. It gives a new code partitioning algorithm to partition a BPEL program \nrepre\u00adsented as a program dependence graph, with the goal of min\u00adimizing communication costs and maximizing \nthe throughput of multiple concurrent instances of the input program. In contrast, much of the past work \non dependence-based par\u00adtitioning and scheduling seeks to minimize the completion time of a single instance \nof a program running in isolation. The paper also gives a cost model to estimate the through\u00adput of a \ngiven code partition. Experimental results show that decentralized execution can substantially increase \nthe throughput of example composite services, with improve\u00adments of approximately 30% under normal system \nloads and by a factor of two under high system loads. Permission to make digital or hard copies of all \nor part of this work for personal or classroom use is granted without fee provided that copies are not \nmade or distributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. OOPSLA 04, Oct. 24-28, 2004, Vancouver, British Columbia, Canada. \nCopyright 2004 ACM 1-58113-831-8/04/0010 ...$5.00. Categories and Subject Descriptors D.3.4 [Programming \nLanguages]: Processors optimiza\u00adtion General Terms Languages, Performance  Keywords Work.ow, Business \nProcess, Program Dependence Graph 1. INTRODUCTION The idea of software as a service has recently gained \ntremendous importance because of standardization of the way in which software may be delivered as a service \nover the network. For example, the Web Services standard [9] pro\u00advides primitives for communication, \nmessaging, and naming of software applications (or services) available over the In\u00adternet. Once software \napplications are available as a service, a composite service can be created by accessing a set of ser\u00advices \nprogrammatically using some scripting language. A composite service expresses a business process, which \ncap\u00adtures a particular intra or inter enterprise work.ow. This paper is concerned with e.cient execution \nof such enter\u00adprise applications. Business Process Execution Language (BPEL)[4, 11] is a language that \nis used to specify a composite web service. The language consists of standard .ow constructs for se\u00adquential, \nconditional and concurrent execution of activities such as invoking a service or standard assignment \nand arith\u00admetic operations. The language under consideration is sum\u00admarized in Table 1. BPEL is a standard \nbeing developed jointly by IBM, Microsoft, BEA and other companies, and is rapidly gaining importance \nin the enterprise computing landscape. Note that Web Services and BPEL are particu\u00adlar instances of generic \nconcepts of software-as-service, ser\u00advice composition and a service composition language, and our techniques \napply to other instances as well. Figure 1(a) illustrates BPEL concepts via a small example of a composite \nservice. The composite service, FindRoute is built from two address book services, AddrBook(1) and AddrBook(2),and \na TrainRoute service. The AddrBook ser\u00advices take as input a name and return the address of that individual. \nThe TrainRoute service takes as input two ad\u00addresses and returns the train schedules from one address \nto the other. The FindRoute service sends, in parallel, name1 as input to service AddrBook(1),and name2 \nas input to service AddrBook(2). This parallelism is expressed using Table 1: Summary of BPEL constructs \nand notation BPEL construct Description Notation Control Flow Constructs sequence sequential .ow sequence \n... end-sequence switch conditional .ow switch .. . end-switch while iterative .ow while ... end-while \npick non-deterministic conditional .ow pick ... end-pick flow concurrent .ow similar to cobegin-coend \nflow ... end-flow link wait-notify type of synchronization source(linkId), target(linkId) Data Structures \nvariable variables include a set of parts analogous to .elds variableName { part1, part2, .. . partn \n} Activities invoke synchronous (blocking) invocation on a partner P, sending data from an input variable \nin and receiving the response in the output variable out invoke(P, in, out) send1 asynchronous (oneway, \nnonblocking) invocation on a partner P, sending data using an input variable in (no response variable) \nsend(P, in) receive blocking receive of data from a partner P into avariable var receive(P, var) reply \nsend response to a partner P from a variable var reply(P, var) assign assignment. Multiple assignments \ncan be speci.ed in a single assign statement, which executes atomically var1.p1.g1 = var2.p1.g3 compute \narithmetic or logical operation BPEL s flow construct; the availability of the .ow construct allows \na programmer to identify parts of the process that can run concurrently without depending on the implementation \nto extract the parallelism automatically. The two addresses are returned to the FindRoute service, the \ncity and zip code of each address is extracted and then sent to the TrainRoute service which returns \nthe train routes from the .rst address to the second. The BPEL code for the FindRoute service is shown \nin slightly sugared form (C0) in Figure 1(a). This code is interpreted/executed by a BPEL engine (such \nas BPWS4J [5]) that acts as a centralized coordinator for all interactions among the component services. \nThis type of execution is known as centralized orchestration. Note that the requests from and replies \nto the client are handled by the central server, in this case node C0. Note also that the invoked services, \nnamely AddrBook(1), Addr\u00adBook(2) and TrainRoute are available only on speci.c nodes A1, A2, and TR respectively. \nThe rest of the code, which may include some business logic, is really portable code or glue code that \nin principle can be run anywhere, not necessarily on the central server. Now consider the same example \nin Figure 1(b). Here the original BPEL code (C0) has been partitioned into four com\u00adponents that are \nexecuted by four distributed engines (D0, D1, D2 and D3). Together, the four engines perform the role \nof C0. This form of orchestration is termed decentralized or\u00adchestration. In decentralized orchestration, \nmessages can be sent directly from a component where the data is produced to a component where the data \nis consumed, without using a centralized coordinator. For example, the addresses gen\u00ad 1The correct BPEL \nsyntax for oneway messaging is invoke(P, in). However, in this paper we use the keyword send to highlight \nthe di.erence between oneway, non\u00adblocking, asynchronous invoke and synchronous invoke. erated at AddrBook(1) \nand AddrBook(2) can be forwarded directly to TrainRoute (via D3), as shown in Figure 1(b). Note that \nwhile speci.c services such as TrainRoute still re\u00adside on the same nodes as before, the glue code is \nrun on di.erent nodes, compared to Figure 1(a). Decentralization may lead to increased parallelism in \nexe\u00adcuting glue code and reduced message overhead since fewer messages are sent. Decentralized orchestration \nbrings per\u00adformance enhancements, namely better response time and throughput, to the composite service \nexecution. However, decentralization does require a BPEL engine at all partici\u00adpating nodes. This is \na reasonable assumption to make in the context of modern enterprise servers for two reasons. One reason \nis that the ability of executing BPEL (or an\u00adother service speci.cation) has become standard software \ninfrastructure in application servers such as WebSphere [1]. The second reason is that the application \nthat the server ex\u00adports as a web service may itself be implemented as a BPEL program behind the scenes, \nrequiring a BPEL execution en\u00advironment. Problem Statement. It is clearly desirable to run compos\u00adite \nservices in a decentralized manner, assuming of course that the infrastructure supports this execution \nmodel. Yet creating and deploying decentralized versions is burdensome. Given a BPEL program for centralized \norchestration, can one decentralize it automatically? On the surface, this prob\u00adlem has many similarities \nwith automatic partitioning of programs for multiprocessor execution. However, there are some important \ndi.erences, as explained in the technical overview below. Technical Overview. Figure 2(a) shows the example \nin Figure 1(a) as a control-.ow graph (CFG). We designate C0 receive(client, c{name1,name2}) flow sequence \nn1.name = c.name1 invoke(A1, n1{name}, a1{ph,street,city,zip}) end-sequenceA1 AddressBook(1) sequence \nn2.name = c.name2 AddressBook(2) invoke(A2, n2{name}, a2{ph,street,city,zip}) C0  A2 end-sequence name2 \n end-flow r.city1 = a1.city Client r.city2 = a2.city Centralized r.zip1 = a1.zip Composite r.zip2 = \na2.zip Service invoke(TR, r{city1,city2,zip1,zip2}, dir{routes}) (FindRoute) reply(client, dir{routes}) \n(a) D0 D2 receive(client, c{name1,name2}) receive(D0, n2{name}) n1.name = c.name1 invoke(A2, n2{name}, \na2{ph,street,city,zip}) n2.name = c.name2 r2.city = a2.city flow r2.zip = a2.zip send(D1, n1{name}) \nsend(D3, r2{city,zip}) AddressBook(2) send(D2, n2{name}) D2 A2  D0 end-flow receive(D3, dir{routes}) \n D3 name2 reply(client, dir{routes}) flow Client receive(D1, r1{city,zip}) receive(D2, r2{city,zip}) \n Decentralized D1 end-flow Composite receive(D0, n1{name}) r.city1 = r1.city Service invoke(A1, n1{name}, \na1{ph,street,city,zip}) r.zip1 = r1.zip (FindRoute) D3 TR r1.city = a1.city r.city2 = r2.city TrainRoute \nr1.zip = a1.zip r.zip2 = r2.zip send(D3, r1{city,zip}) invoke(TR, r{city1,city2,zip1,zip2}, dir{routes}) \n send(D0, dir{routes})(b) Figure 1: Centralized and Decentralized Architecture receive and reply nodes \nof the CFG as .xed nodes that must execute at the central server; invoke nodes are also designated as \n.xed nodes that must be colocated with the corresponding web service; all other activities are desig\u00adnated \nas portable nodes. We use the convention that the .xed nodes are represented by rectangular boxes and \nthe portable nodes by rounded boxes. A program dependence graph (PDG) [6] consists of control dependence \nand data dependence edges superimposed on the same set of nodes which denote statements and predicate \nexpressions of the CFG. The PDG corresponding to Figure 2(a) is shown in Figure 2(e). Unlike [12], we \ndo not work directly with con\u00adcurrent PDGs. Instead, a conventional PDG with a few extra edges can represent \nall the information we need for our purposes. We show how to create these extra edges in Section 2. In \na centralized execution, all portable nodes of a PDG are mapped to the central server, leaving .xed nodes \nat whichever servers they belong. We would like to explore al\u00adternative partitioning of the nodes of \nthe PDG in which some of the portable nodes are assigned to nodes other than the central servers, corresponding \nto a decentralized execution. For the PDG of Figure 2(e), Figure 2(b) gives the partition\u00adinginwhich \nallportable nodes runonthe centralserver (centralized execution), and Figures 2(c) and 2(d) give two \npossible partitionings in which we have grouped portable nodes with .xed nodes other than the central \nserver (de\u00adcentralized execution). The data dependence edges in Fig\u00adures 2(c) and 2(d) refer to messages \nbetween partitions, for which send/receive pairs need to be generated (see code in Figure 1(b) which \ncorresponds to Figure 2(d)). We can see that the number of messages in 2(c) and 2(d) is lower than in \n2(b). The experimental results (Section 5) validate that the performance of 2(b) is inferior to the decentralized \noptions (2(c) and 2(d)). In principle, we could use a PDG-based code partition\u00ading algorithm designed \nfor multiprocessor execution. Such an algorithm creates independently schedulable tasks at the granularity \nof partitions of a PDG. To reduce overhead, such algorithms try to merge several PDG nodes to create \na larger partition, possibly sacri.cing parallelism. An example of a merging algorithm that iteratively \nmerges nodes that have the same control dependence condition can be found in [16]. However, the problem \nof partitioning PDGs for composite services, has an additional constraint that the node merging algorithm \nmust create partitionings such that each parti\u00adtion has exactly one .xed node and zero or more portable \nnodes. By de.nition, a .xed node cannot be merged with another .xed node. Portable nodes cannot form \na partition without a .xed node, because .xed nodes are the ones that have execution resources. Further, \nthe objective function for composite web services is to maximize throughput, whereas the objective function \ntypically used for multiprocessor exe\u00adcution is to minimize completion time. Due to these di.er\u00ad Fixed \nnode Portable node Fixed partition Control dependence/control flow Data dependence Entry   flow p1 \nn1.name = c.name1 p2 n2.name = c.name2 end-flow p3 r.city1 = a1.city p4 r.city2 = a2.city     \n (e) (f)  g)   h) (i) =r.city1 =r.city2 =r.zip1 =r.zip2 dir.route= Figure 2: For the example in \nFigure 1: (a) the Threaded CFG (TCFG); (b) a partitioning of the PDG that corresponds to centralized \nexecution; (c) and (d) two partitionings of the PDG that correspond to decentralized execution; (e) the \nPDG corresponding to the TCFG; (f) a partitioning generated by merging lexical siblings in the PDG; (g) \nthe desired merging scheme where siblings connected by data dependence edges are merged; (h) the reordered \nPDG corresponding to (g); (i) the partitioning generated by merging neighboring siblings in the reordered \nPDG in (h). Note that every partition has exactly one .xed node and zero or more portable nodes. ences, \na node merging algorithm designed for multiprocessor execution may not perform well in the case of partitioning \nfor composite web services. Returning to our example of Figure 2, while the number of inter-partition \nedges in both (c) and (d) is the same, it canbe seen that the data passedonthe wire is larger in Figure \n2(c). In Figure 2(d), for example, the city and zip information is extracted at partition F1 and F2 and \nsent to F3, whereas in Figure 2(c), the entire address is sent from F1 and F2 to F3, and the relevant \ndata is then extracted at F3. The partitioning of Figure 2(c) corresponds to the parti\u00adtion con.guration \nshown in Figure 2(f) and the partitioning of Figure 2(d) corresponds to the partition con.guration of \nFigure 2(g). In Figure 2(f), each partition merges only sib\u00adlings in the PDG that are also lexical neighbors, \nhowever, Figure 2(g) tries to merge siblings that are not lexical neigh\u00adbors. A partitioning algorithm \nthat performs only lexical neighbor sibling merges will miss out on this more e.cient partitioning. As \none might expect, in our experiments we see a noticeable performance di.erence between (c) and (d) when \nthe size of the entire address relative to city and zip is signi.cant. Our premise is that merging nodes \nalong .ow-dependence edges will result in a more communication-e.cient parti\u00adtioning. Since the source \nand destination nodes of a .ow dependence may not be lexically adjacent, merging along .ow-dependence \nedges is equivalent to .rst reordering the nodes so that the source and destination become adjacent, \nand then merging the nodes. Consider the PDG in Fig\u00adure 2(h), in which some of the nodes have been reordered \nfrom Figure 2(e). On this new PDG, it is now possible to generate the partitioning of Figure 2(d) just \nby combining neighboring siblings as shown in Figure 2(i). However, be\u00adfore we generate this partitioning, \nwe need to validate that the reordered PDG is isomorphic [10] with the original PDG. The main contributions \nof this paper are as follows: 1. We give a heuristic solution to the problem of decen\u00adtralization of \nBPEL programs. To the best of our knowledge, this is the .rst proposed solution to the decentralization \nproblem for BPEL programs. 2. We introduce a cost model to guide the decentraliza\u00adtion algorithm that \nis based on throughput as the pri\u00admary performance metric, for multiple instances of a program running \non a server. In contrast, much of the past work on dependence-based partitioning and scheduling seeks \nto minimize the completion time of a single instance of a program running in isolation. 3. Our experimental \nresults show signi.cant bene.ts from decentralization for four sample composite services. For the same \nhardware resource, the decentralized ver\u00adsions performed better than the centralized version across a \nrange of parameters for request rates and message sizes. The results show that decentralized ex\u00adecution \ncan substantially increase the throughput of example composite services, with improvements of ap\u00adproximately \n30% under normal system loads and by a factor of two under high system loads.  Organization. The rest \nof the paper is organized as fol\u00adlows. Section 2 describes how we construct PDGs for our program model \n(a subset of BPEL), and how we test for legality of node reordering. Section 3 describes our parti\u00adtioning \nalgorithm and Section 4 describes our cost function. Section 5 contains experimental results for decentralization \nof four example composite services. Finally, Section 6 dis\u00adcusses related work, and Section 7 contains \nour conclusions.  2. PDGS AND NODE REORDERING In this section, we describe how we build the program \nde\u00adpendence graph (PDG) representation assumed in our work. We start with a Threaded Control Flow Graph \n(TCFG) rep\u00adresentation as shown in Figure 3(a). (We had also seen an example of a TCFG earlier in Figure \n2(a).) To obtain a PDG representation of this parallel program representation, we need to insert extra \ncontrol and data dependences that model the parallel constructs. For the control dependence edges, each \nsequence node representing a parallel section is control dependent on its flow node, and each node within \na parallel section is control dependent on its sequence node. After these control dependence relationships \nare established, the sequence and flow nodes are eliminated as follows: let Nc be the node on which the \nflow node is control dependent. Then, every node that is control dependent on a sequence node below the \nflow is made control dependent on Nc and the sequence and flow nodes are eliminated. In this way, we \nobtain a single integrated PDG from the TCFG, unlike (say) the Threaded PDG (TPDG) approach [12] in which \nPDGs for parallel tasks are represented as separate sub-PDGs em\u00adbedded in the parent PDG. Our motivation \nfor working with a single integrated PDG is that it enables the partitioning algorithm to treat statements \nfrom within and across paral\u00adlel tasks uniformly. The PDG for the TCFG in Figure 3(a) is showninFigure \n3(b). For the data dependence edges, we must preserve the or\u00addering constraints implicit in the TCFG. \nTo accomplish this task, we .rst compute all data dependences (.ow depen\u00addences, output dependences and \nanti dependences) neces\u00adsary to preserve correct execution order within each parallel section. Next, \nwe insert additional dependence edges to cap\u00adture the ordering constraints across parallel sections. \nThere are two cases that require insertion of these extra edges in the PDG: 1. Dependences with statements \noutside a .ow construct these dependences occur between a statement in a parallel section and a statement \noutside its .ow con\u00adstruct that precedes or succeeds the .ow construct con\u00adtaining the parallel section. \n2. Dependences among parallel sections these depen\u00addences capture explicit synchronization-based order\u00ading \nbetween two parallel sections (as speci.ed by the BPEL link construct). We discuss these two cases in \nmore detail below. These ex\u00adtra dependence edges are analogous to the constraint edges introduced by \nHorwitz et al [10] in the context of merging variants of PDGs into a single PDG, and to the synchroniza\u00adtion \nedges introduced by Sarkar [17] for extending PDGs to parallel programs. Our code partitioning algorithm \n(Sec\u00adtion 3) relies on the correctness of these extra dependence edges, because it attempts to reorder \nnodes in a region of a PDG in search of a more e.cient partitioning and it must ensure that all reorderings \nconsidered are legal.   Figure 3: (a) A Threaded Control Flow Graph (TCFG), (b) its PDG representation \nwhere parallel sections have been merged into a single PDG with\u00adout parallel sections, (c) a PDG isomorphic \nto that shown in (b). Dependences with statements outside a .ow construct. Consider a statement Sj in \na parallel section. If there is a statement Si outside the .ow construct containing the par\u00adallelsection,such \nthat Si precedes the .ow construct and Si and Sj perform interfering (write-read, read-write, or write\u00adwrite) \naccesses on the same variable, then a corresponding (.ow, anti or output) dependence edge is inserted \nin the PDG from Si to Sj . Likewise, if there is a statement Sk out\u00adside the .ow construct containing \nthe parallel section, such that Sk follows the .ow construct and Sj and Sk perform interfering (write-read, \nread-write, or write-write) accesses on the same variable, then a corresponding (.ow, anti or output) \ndependence edge is inserted in the PDG from Sj to Sk. In Figure 3(b), we see an example of four such \ndepen\u00addence edges being inserted in the PDG, due to dependences on variables x, y,and z. Figure 3(c) \nshows another legal ordering of nodes of this PDG. Dependences among parallel sections. By default, par\u00adallel \nsections execute independently and there is no need to insert any dependence edges between two parallel \nsections in the same .ow construct. However, dependence edges do need to be inserted to capture explicit \nsynchronization-based ordering between two parallel sections, as speci.ed by the BPEL link construct. \nSpeci.cally, if there is an explicit synchronization link from source in parallel section .1 to target \nin parallel section .2, then the semantics imply that every node that precedes the source node must execute \nbe\u00adfore any node that follows the target node. There is no inter-process ordering implication for nodes \nthat follow the source node or precede the target node. To enforce le\u00adgal orderings, we .rst insert a \nsynchronization edge from source to target. Then, for every de.nition of x that pre\u00adcedes source in .1 \nwe create a dependence edge to a use or de.nition of x that follows target in .2; and for every use of \nx that precedes source in .1 we create a dependence edge to a de.nition of x that follows target in .2. \nFor example, in Figure 4(a) source . target is a synchronization edge. (Note that in the PDG, we do not \ndi.erentiate between the  (b) n1 a=y b=x (c) Figure 4: (a) Threaded CFG showing Synchroniza\u00adtion Dependence, \n(b) and (c) Isomorphic PDG vari\u00adants produced after merging the parallel sections di.erent forms of dependence \nedges for the purpose of deter\u00admining legal orderings.) The semantics of synchronization enforces the \nfollowing dependence edges across parallel sec\u00adtions: n1 . n5, n2 . n6,and n2 . n7. In Figure 4(b), n1 \nis legally topologically ordered after n2 as this does not violate any dependence edges. Figure 4(b) \nand (c) show two topological orderings where the value of x at n7 is 30 and 40 respectively. One question \nthat may arise is: what should be done in cases when two parallel sections have interfering accesses \nto a variable that are not ordered by an explicit synchroniza\u00adtion link? As mentioned earlier, the semantics \nof parallel sections does not require us to insert a dependence edge in this case. This case represents \na nondeterministic parallel program, where the program can exhibit di.erent behaviors depending on the \nrelative execution order of the interfer\u00ading accesses. Note that, since the BPEL speci.cation states \nthat each assign activity is an atomic activity akin to a syn\u00adchronized block in Java, this case does \nnot represent a data race. If an assign activity contains multiple copy state\u00adments, the BPEL speci.cation \ndictates that the entire set of assignments will be performed atomically.  3. CODE PARTITIONING In general, \nthere is a bounded but exponential number of ways to distribute the portable code amongst the par\u00adtitions. \nHence, an exhaustive search algorithm that tries every possible placement of portable nodes is intractable. \nIn this section we .rst describe a simple heuristic called the merge-by-def-use heuristic. The aim of \nthe merge-by-def-use partitioning algorithm is to determine the best partitions at which each portable \ntask must be executed in order to optimize the throughput of the decentralized program. As mentioned \nin the introduction, our code partitioning algorithm is based on the idea of merging tasks along loop\u00adindependent \n.ow dependence edges. This idea is appealing not only for its performance implications, but also because \nit serves as a heuristic to prune the space of possible solutions. However, merging along .ow dependence \nedges must take into account several considerations including: (1) not all combinations of merges will \nbe legal (a legal combination must not create a dependence cycle among partitions), and (2) a .ow dependence \nedge between two nodes with di.erent control dependences cannot be merged (without introducing guards/predicates). \nStarting at the bottom of the control dependence tree we identify sibling nodes that have the same control \ndependence condition and perform a merge on these nodes. Two sibling nodes in the PDG that have the same \ncontrol dependence condition may be merged if there is a def-use dependence re\u00adlationship between them, \nprovided the following conditions hold -(1) the reordering along the .ow dependence edge does not violate \nany other dependences, and (2) each parti\u00adtion has at most one .xed node. Using a cost function, we exhaustively \nevaluate all possible merges along .ow depen\u00addence edges and compute a local minimum for each region. \nOnce a region has been evaluated, the algorithm is recur\u00adsively applied to the parent node and its siblings. \nMerging Portable Code. An informal description of the merging algorithm is as follows: 1. Locate a control \nnode, Tc in the PDG whose child nodes are all leaf nodes. For all nodes that have the same control dependence \ncondition on Tc repeat steps 2 through 8. Continue till all control nodes have been processed. 2. Identify \nthe set of .ow dependence edges, E,that per\u00adtain to a .ow dependence between siblings with the control \ndependence condition chosen in step 1, such that at least one of the siblings is a portable task. Pick \nan edge in E and merge the source and destina\u00adtion tasks of the edge. The resultant dependences of the \nmerged task is the union of the component tasks. 3. When a portable task gets merged with a .xed task \nthe combined task is a .xed task. When a portable task gets merged with another portable task the combined \ntask is also marked as a portable task. 4. When a node is merged with a sibling that is not its lexical \nneighbor, we need to ensure that no dependence conditions are violated. To determine whether the merge \nmay violate a dependence condition, we check if the merge can introduce a dependence cycle. 5. Exhaustively \nconsider all merging con.gurations of sib\u00adlings that can be generated by merging some subset of the .ow \ndependence edges in E. Since the size of E for a single region is usually small, this exhaustive search \nis usually feasible in practice. (Later we describe a more complex heuristic that further reduces the \nnum\u00adber of partitionings evaluated.)  6. Choose the merging con.guration from step 5 that is likely \nto yield the best overall throughput value, us\u00ading the cost model discussed in Section 4. Though in Section \n4, the cost function is de.ned for a com\u00adplete partition, it can be adapted for an intermediate partition \nin which some portable tasks have yet to be merged with .xed tasks. This is a greedy heuristic that gives \na locally optimal solution, but does not nec\u00adessarily guarantee global optimality. 7. Any remaining \nportable tasks that are not merged with a .xed task are merged with the parent. At this point, the parent \nhas only .xed tasks (if any) for children. The parent node is now marked as a leaf node. 8. Once a region \n(subgraph) has been merged, we treat the whole subgraph as a single node for the purpose of merging at \nthe next higher level. The dependences of the merge is a union of all dependences in the child nodes \nas well as the parent node.  Example. Figure 5(a) gives the CFG for an example Loan-Approval BPEL service, \nwhere the client sends in his pro.le and the required loan amount. If the amount is less than $10,000, \nthe web service sets a risk factor to zero, else it in\u00advokes web service F1 to get a risk assessment. \nThen the web service sends the risk and amount to two banks F2 and F3 whichreturnthe rate of interest. \nAdditionally F2 returns a document with details about the loan scheme. Finally the web service returns \nthe lower rate and the loan scheme in\u00adformation (which may be null) to the client. The PDG for the example \nis shown in Figure 5(b). All .xed tasks are labeled Fi and portable tasks are labeled pi. We follow the \nconvention that a partition is referred by the label of the .xed task it contains (if it contains a .xed \ntask) or by the label of the portable task with the lowest index. We give here a partial walk-through \nof the algorithm. The merging algorithm .rst merges nodes at the bottom of the control dependence tree. \n For the nodes control dependent on p1: F1 and p3 have the same control dependence condition. The algorithm \nstarts with p3, .nds that there is a dependence edge F1 . p3 and merges F1 and p3. No other merges are \npossible. p2 has no siblings and so it gets merged with the parent node p1.  For nodes control dependent \non p5: p6 and p7 have no siblings and hence get merged with p5.  The resultant intermediate partitioning \nis shown in Fig\u00adure 5(c). The set of edges in and out of p1 is the union of edges in and out of p1, p2, \nF1 and p3, and similarly for p5. Next we consider the nodes that are control dependent on Entry. Let \nthe algorithm start with the portable task p5 which is source or destination in the def-use edges F2 \n. p5, F3 . p5,and p5 . F4.Let us merge p5 with F3 as shown in Figure 5(d1).  Then the algorithm considers \np4 which has the option of merging with one of F2, p1 or F0.Let us merge p4 with F2 as showninFigure \n5(d2).  Entry   p1 if(req.amt<10000) g.risk=0 invoke(F1,req,r) =req.amt (c)  =req.profile r.risk= \n p3 g.risk=r.risk endif p4 b.risk=g.risk b.amt=req.amt flow  end-flow F4 (a)  data dependence control \nflow / control dependence (e5) Figure 5: LoanApproval Example -Merging Portable Tasks Finally the algorithm \nmerges p1 with F0 as shown in Figure 5(d3). Now there are no more portable nodes left and so this is \none of the .nal partitionings. (We happened to use only lexical siblings in this partitioning.) As part \nof enumerating all the merging con.gurations out\u00adlined in step 5, the algorithm backtracks from Figure \n5(d1). We do not show all the steps performed by this recursive algorithm but show only some interesting \ncombinations gen\u00aderated. Let the algorithm merge p5 with F2 (instead of F3)as showninFigure 5(e1). \n Even though this partitioning has an edge F3 . F2 which points right-to-left, it is legal because it \nhas not introduced a dependence cycle. To make the acyclic structure clearer, we reorder the partitions \ntopolog\u00adically, resulting in Figure 5(e2). This partition has the interesting property that the edge \nF2 . p5 is in\u00adternalized within a partition. Thus the assignment of res.scheme in task p6 happens (if \nit does) at the same node as F2, avoiding a large message. Note that this partitioning could not have \nbeen created by lexical merges alone. By contrast, in the partitioning shown in Figure 5(d3), F2 . p5 \nis an inter-partition edge.  Next let the algorithm merge p1 with F0 resulting in Figure 5(e3). To complete \nthe partitioning, p4 can be merged with one of F0, F3 or F2 as shown in Fig\u00adure 5(e4-a), (e4-b) and (e4-c) \nrespectively. Of these, (e4-a) and (e4-b) are valid partitionings, but (e4-c) is not valid as it has \ngenerated a dependence cycle. Hence, the partitioning of Figure 5(e4-c) will be dis\u00adcarded as an infeasible \nPDG.  The cycle detected in the example in Figure 5(e4-c) can be broken by the judicious use of code \nreplication. In the example the cycle arises because p4 needs to be executed before either F2 or F3 executes. \nTherefore p4 is replicated at both F2 and F3, giving the partitioning in Figure 5(e5). This code may \nbe replicated if it is a pure computation and hence has no side e.ects. Note, however, that code replication \nmay not improve performance. Complexity and Heuristics. An exhaustive search algo\u00adrithm that tries every \npossible placement of portable nodes would have a complexity of O(fp), where p is the maximum number \nof portable nodes that are siblings in the PDG and have the same control dependence condition; and f \nis the corresponding number of .xed nodes. The merge-by-def-use algorithm described in this section applies \na heuristic that attempts to reduce this search space while trying to reduce the data on the network. \nThe complexity of the merge-by\u00addef-use algorithm is O(ep), where e is the maximum number of def-use edges \nthat enter or exit a portable node, and p is the maximum number of portable nodes that are siblings in \nthe PDG and have the same control dependence condi\u00adtions. For example, in the program in Figure 2, there \nare 6 portable nodes, each has two def-use edges and so the num\u00adber of possible partitionings are 26 \nwhich is 64. The program in Figure 5(c) has portable nodes p1 with 2 edges, p4 with 4 edges, and p5 with \n3 edges. Hence the number of possible partitionings is 2 * 4 * 3 which is 24. Many BPEL programs F0 \nF4 (a) F0 F4F3  (b)    F0 F4F3  (c) F4 F3  (d) Figure 6: Applying the pooling heuristic to \nthe ex\u00adampleof Figure2-(a) ThePDG of Figure 2; (b), (c) Two possible poolings; (d) A partitioning gener\u00adated \nfrom (c) by merging the pool (p1, p2)with F1 and the pool (p3, p4, p5, p6)with F2. are very small and \nhence they can be analyzed exhaustively. However, we did sample some BPEL programs that could not be \nanalyzed in a reasonable amount of time without ap\u00adplying heuristics. We apply two more heuristics -(1) \nthe greedy-merge heuristic is a re.nement of the merge-by-def\u00aduse heuristic that further tries to minimize \nthe data on the network and (2) the pooling heuristic tries to minimize the total number of messages. \nIn the greedy-merge heuristic we examine every portable node, pi, that has exactly one incoming def-use \nedge from pd and one outgoing def-use edge to pu.Then pi is merged with pd if the volume of data between \npi and pd is less than the volume of data between pi and pu, else it is merged with pu. This halves the \noptions for pi. Applying this heuristic to the example in Figure 2, reduces the number of partitionings \nto exactly one and the partitioning generated is the one shown in Figure 2(i) which we will show experimentally \nto be the best. Applying this heuristic to the example in Figure 5, reduces the number of partitionings \nto 12. The pooling heuristic is as follows: if two or more portable nodes have the same def-use source \nor if two or more portable nodes have the same def-use destination, then pool them together .rst and \ntreat them as a single portable node with the combined dependencies of the merge. Then apply the merge-by-def-use \nalgorithm as before along with the greedy\u00admerge heuristic. The pooling is, of course, subject to the \ncondition of correctness. This heuristic tends to combine all data before entering or exiting a node \nand hence min\u00adimizes the inter-component messages. Consider the exam\u00adple of Figure 2. The PDG is redrawn \nin Figure 6(a). The possible .rst level poolings are shown in Figure 6(b) and 6(c). In Figure 6(b), p1 \nand p2 are pooled as they have a common source F0; p3 and p5 are pooled as they have a common source \nF1;and p4 and p6 are pooled as they have a common source F2. In Figure 6(c), p1 and p2 are pooled as \nthey have a common source F0;and p3, p4, p5 and p6 are pooledas they have a commondestination F3.These \nare the only possible poolings of portables for this exam\u00adple. The number of initial portable nodes has \nreduced from 6 in Figure 6(a) to 3 in Figure 6(b) and 2 in Figure 6(c), thus reducing the overall complexity \nof the algorithm. The possible number of merges is 3 * 1 * 1 for Figure 6(b) and 3 * 3 for Figure 6(c), \ngiving 3 + 9 = 12 partitionings, and a total of 13 partitionings including the partitioning gener\u00adated \nby the greedy-merge heuristic. This is a vast reduction from the original 64 partitionings. Note the \npartitioning in Figure 6(d) which is derived from the initial pooling in Fig\u00adure 6(c) and which minimizes \nthe number of communication edges at the expense of sending extra data. In this case F0 sends both the \nnames to F1. F1 uses the .rst name to get the .rst address and sends the .rst address along with the \nsecond name to F2. F2 obtains the second address and sends both the addresses to F3. Thus the input for \nF2 is pipelined through F1 and the output of F1 is pipelined through F2.At low message rates this is \nquite an e.cient method of com\u00admunication due to the lack of synchronization overheads. Note, that the \nset of partitionings generated by the greedy\u00admerge heuristic is a subset of the partitionings generated \nby the merge-by-def-use heuristic, but the partitionings gener\u00adated by the pooling heuristic are di.erent \nfrom those gener\u00adated by the merge-by-def-use heuristic. Neither is a subset of the other and they may \nhave a non-null intersection. In gen\u00aderal, the number of partitionings generated by the pooling heuristic \nis fewer than the number of partitionings generated by the merge-by-def-use heuristic.  4. COST MODEL \nMuch of the past work on partitioning and scheduling has focused on minimizing the completion time of \na sin\u00adgle instance of a program running in isolation. Application servers instead use multiple threads \nto overlap the execution of multiple instances of one or more programs, and are usu\u00adally more concerned \nwith optimizing the throughput perfor\u00admance that can be delivered for a given a hardware capacity. In \ncontrast, completion time is only of interest as a quality of service threshold, and can be ignored as \nan optimiza\u00adtion metric in cases when the request rates on server nodes can be satis.ed by available \ncapacities. Therefore, the cost model developed in our work is focused on throughput as its objective \nfunction. As in standard queueing system models, if R is the num\u00adber of requests sent to a service per \nunit time, the over\u00adall throughput of the service can be modeled as a function T(R) representing the \naverage number of requests processed per unit time. Typically, T(R) ramps up with increases in R until \na steady-state plateau is reached when one or more resources is fully utilized; eventually a breakdown \nphase is reached when a backlog accumulates and the system throughput may decline dramatically. Real \nsystems gener\u00adally use some form of admission control to avoid the break\u00addown situation. Our system model \nfor decentralized execution is a sys\u00adtem consisting of a set of communicating server nodes, S = {S1,...,Sk}, \neach of which implements a portion of the overall service as dictated by the task partition. The through\u00adput \ndelivered by each individual server node contributes to an upper bound on the overall throughput as follows, \nT(S) = min(T(S1),...,T(Sk)) For convenience, we use the same notation, Si to refer to both a server node, \nand the request rate that it receives. It is therefore important to balance the throughput across the \nnodes, because the overall throughput will be bounded by that of the slowest node. (This is akin to the \nimportance of balancing stages in a pipelined system.) Consequently, we need a model to .gure the rate \nat which each participat\u00ading server can process (its portion of) client requests. This rate depends on \nthe capacity of the server as well as the amount of work it is required to carry out per client request. \nNotice that in the cost model, we explicitly consider the fact that multiple instances of the same application, \nby way of concurrent requests, are running on the same server. We factor in the presence of other independent \nprocesses by as\u00adsuming a reduction in available capacity which re.ects the fraction of a server dedicated \nto (all instances of) a given application. In practice, application servers partition re\u00adsources statically \namong independent applications. Participating servers are assigned work in the following manner. The \nreceive and reply .xed nodes, generally F0 and Fmax are mapped to the central server. The invoke .xed \nnodes are mapped to their corresponding servers, ex\u00adcept in the centralized execution in which all the \nnodes, .xed or portable, are located on the central server. A portable node is mapped to the same server \nas the .xed node in its partition. Figure 7 gives work assignment to servers as well as the data dependencies \ncorresponding to three di.er\u00adent partitioned con.gurations of the loan approval process (Figure 5): Figure \n7(a) shows the centralized partition con\u00ad.guration and Figures 7(b) and 7(c) show con.gurations for two \ndecentralizations, corresponding to Figure 5(e4-a) and Figure 5(e4-b) respectively. The straight arrows \ndepict messages across servers while the squiggly arrows show wait times within the thread that is working \non (that node s por\u00adtion of) a particular client request. A client request arrives at F0 on the central \nserver (node C0 for the centralized or\u00adchestration in Figure 7(a) and node D0 or D0. for the decen\u00adtralized \norchestrations of Figure 7(b) and 7(c)) and works its way through the servers following inter-partition \nedges. It may fork activity along two edges concurrently (shown as a and \u00df in the .gure). The request \nterminates at Fmax on the central server, which responds to the client. In computing the load per request \non a server, we do not need to account for thread waiting time, because it will not have an impact on \nthroughput the server would instead switch to processing its part of another client request, for which \nthe assigned thread is ready to run. Therefore the load is simply the aggregation of all activity at \neach server. With this reasoning, we can abstract out the wait times from the pictures in Figure 7, considering \neach node simply as a collection of .xed and portable tasks. Only the aggregate compute and messaging \ncosts at each server are relevant for the estimation of throughput. Since our cost function is an upper \nbound on throughput, we assume both sides of a conditional are executed when estimating the cost of con\u00additionals. \nFurther re.nement of the cost functions to use execution pro.le information is a subject for future work. \nThe cost of computation per client request required at a server includes the cost of running all the \n.xed and portable tasks stationed at the server. (Note that the service itself  Synchronous invocation \n Asynchronous send/receive Figure 7: Partition con.gurations of the Loan-Approval Example -(a) The centralized \norchestra\u00adtion; (b), (c) Decentralized orchestrations corre\u00adsponding to Figure 5(e4-a) and Figure 5(e4-b) \nre\u00adspectively. accessed by an invoke may not be adding to the cost in case it is implemented by another \nbackend server.) For each portable task, Pi, we assign a cost cPi .The cost of the .xed task receive, \ncR, includes in addition to the cost of data handling, the cost of setting up a new process if it is \nthe .rst receive or the cost of correlation for every sub\u00adsequent receive.The cost of a reply, cL, includes \nonly data handling cost for sending over an existing channel. A synchronous invoke .xed task needs communication \nwith a backend server, as shown by double-headed arrows in Fig\u00adure 7. The cost of an invoke, cI , involves \nmessage setup in addition to marshalling and unmarshalling data sent and received. At places where .ow \ndependence edges have their source in one partition and destination in another, decentralization introduces \nasynchronous communication, which is shown by single-headed arrows in Figure 7. This communication is \nhandled by introducing a send at the source partition and a receive at the destination partition. These \ntasks have no direct correspondence with the original code, but are generated to support decentralization. \nThus at the source of an inter-partition edge we need to add the cost of a send and at the destination \nof an inter-partition edge we need to add the cost of a receive.The cost of a send, cS includes only \nthe cost of message setup plus marshalling data, and the cost of a receive, cR, includes the cost of \nunmarshalling data plus correlation overheads. The cost of send or receive is typically less than that \nof an invoke. The cost of each task depends on various factors including the size of data being handled \nand the complexity of data being manipulated. For example, the cost of an assign is independent of the \nsize of the data if it involves moving sim\u00adple strings, but the cost of an assign is extremely sensitive \nto data size if it involves an XPath expression. The cost of invokes, sendsand receives are dependent \non size of data as well as complexity of data since these tasks involve mar\u00adshalling and unmarshalling \nof data. These costs need to be determined empirically using micro-benchmarking, which is explained in \ngreater detail in Section 5. Example. Given the cost of each task, the peak rate at node C0 in Figure \n7(a) is Capacity C0 cR+cL+cP1 +cP2 +...+cP7 +3*cI As another example, consider the cost of node D3 in \nFig\u00adure 7(b). In addition to the obvious cost of .xed node F2 (which is an invoke) and the portable tasks \np5, p6 and p7, we need to add the cost of two receives (for the two incom\u00ading inter-partition edges) \nand the cost of one send (for the outgoing inter-partition edge). Hence the peak rate at D3 is Capacity \nD3 cP5 +cP6 +cP7 +cI +cS+2*cR Likewise, the peak rates for other nodes in Figure 7(b) and (c) can be \ncomputed. The minimum rate across nodes would be the upper bound on the throughput that this con.gura\u00adtion \nwould sustain. In this example we can see that the critical node is D0 for (b) and node D0. for (c). \nAssuming all portable tasks have the same cost cp: Capacity C0 peak rate(C0) = cR+cL+7*cp+3*cI Capacity \nD0 peak rate(D0) = cR+cL+3*cP +3*cS+2*cR Capacity peak rate(D0)= D0. cR+cL+2*cP +2*cS+cR Assuming C0, \nD0 and D0. have the same capacity, the peak rate of the con.guration in Figure 7(c) is clearly higher \nthan that in Figure 7(b). However, the relative peak rate of C0 and D0 is not immediately obvious and \nin Section 5.3 we show how to determine the values of the individual costs (cR, cL, etc.) using micro-benchmarking. \nIn this case, the peak rate of Figure 7(b) turns out to be higher than that of Figure 7(a). The experimental \nresults are given in Sec\u00adtion 5.1. Note that although in our example programs, the portable tasks seem \nto be trivial assignments, the assignment state\u00adments actually represent extraction of data from contain\u00aders \nin BPEL, because in general messages need to be re\u00adconstituted as they .ow between .xed nodes. This can, \nin turn, involve complicated operations such as XPath queries. Thus, they are more expensive than assignment \nstatements in standard programming languages. Finally, we outline how our cost function can be extended \nto determine the bounds on throughput that occur when a server is unable to consume the available compute \ncapacity due to other reasons. These cases can occur if the available capacity such as the number of \nthreads in a thread pool on theservernodeis too low. Thecost function discussed thus far estimates an \nupper bound of the throughput on a single server node Si as T (Si) = Capacity/Cost,where Capacity is \ntheraw computecapacity of node Si and Cost is the total amount of work that needs to be performed on \nnode Si for a single request. The bound on throughput due to limited number of threads in the server \ncan be modeled as T (Si) = Number of Threads/CritPath where CritPath is the thread occupancy time along \nthe crit\u00adical path. Then we can estimate an upper bound of the throughput, T (Si) on a single server \nnode Si as follows, Capacity Number of Threads T (Si) = min( , ) Cost CritPath Similarly, there may exist \nother bounds based on bandwidth availability or memory usage.  5. EXPERIMENTAL RESULTS 5.1 Runtime Performance \nExperimental Setup. Our experimental setup for testing decentralized orchestration is as follows. We \nuse a cluster of Intel Pentium based Linux machines (2.2 GHz, 2 GBRAM) connected by a 100Mb/s LAN. For \nthe results reported in this paper, two of the machines were used as clients, and up to four machines \nwere used as servers. The clients execute multithreaded Java programs that run a total of 10 to 200 threads \ngenerating a steady request rate of 1 request/second (or 60 requests/minute) to 20 re\u00adquests/second (1200 \nrequests/minute). The test message sizes varied from 256 bytes to 24 KB. In the centralized setup (as \nshown in Figure 1(a)), the clients send requests to an HTTP server that hosts a BPWS4J engine. The requests \nare synchronous and sent using SOAP over HTTP. Each component web service is deployed on an HTTP server \nrunning on a separate machine. The BPWS4J server internally invokes the component web services using \nSOAP over HTTP. While the centralized setup uses only HTTP servers, the decentralized setup uses a combination \nof HTTP and EJBservers. EJBservers are necessary as HTTP servers cannot handle asynchronous messaging. \nIn the decentralized setup (Figure 1(b)), the clients send requests to an EJBserver that hosts a BPWS4J \nengine. The requests are sent using SOAP over JMS. However, the component web services continue to run \non HTTP servers, each running on a separate machine. An EJBserver hosting a BPWS4J engine is co-located \nwith each of the web services. Test Examples. Since BPEL is a relatively new language, there are currently \nno standardized BPEL benchmarks that we could use in our performance evaluation. However, we have tested \nour algorithm on all examples that come with the BPWS4J distribution as well as several other applica\u00adtions \nculled o. the web. Although there are many possible partitionings for each example, for visual clarity \nwe show only a few illustrative partitionings for each example. We .rst present performance results for \nthe LoanApproval example introduced in Section 3 and the FindRoute example introduced in Section 1. Then \nwe show results for two more examples. The TranslateArticle service locates an article of interest, gets \nit translated into the required language, formats both the original and translated article and returns \nit to the client. The NearestRestaurant service receives a restaurant preference from a client along \nwith his mobile cell number and a radius parameter. The service locates a restaurant of the speci.ed \ncuisine, gets the location of the client using a GPS service, then invokes a map service that takes the \nclient location, the list of restaurants and shortlists the restaurants that are within the speci.ed \nradius of the client. This list is returned to the client2 . 2All the BPEL examples are available on \nrequest from the Request Rate (Requests sent per minute) Figure 8: LoanApproval -Throughput Variation \nwith Request Rate. Message Size in KBytes Figure 9: LoanApproval -Throughput Variation with Message \nSize. LoanApproval Example. In Figure 8, we show the through\u00adput observed for the three partitionings \nof the LoanApproval example shown in Figure 7, as a function of the request rate. This example uses three \ncomponent web services which were accessed using the standard SOAP over HTTP protocol. For Figure 8, \nthe service time for each web service was .xed at 4000 ms, and the message size .xed at 512 bytes. We \nvar\u00adied the client request rate from 60 requests/minute to 1200 requests/minute. At lower rates the requests \ndo not ex\u00adceed the capacity of the system and hence the throughput is equal to the request rate. As mentioned \nin Section 4, the partitioning in case (a) was the .rst to reach its capacity limit, followed by case \n(b), with case (c) delivering the best throughput. In each case, the throughput begins to decline when \nthe request rate exceeds the capacity. In Figure 9, we show how the throughput for the Loan-Approval \nexample varies with message sizes of 512B, 4KB, 8KB, 16KB, and 24KB. To highlight the di.erences among \nthe three cases, we set the y-axis origin for Figure 9 to 400 requests/minute instead of zero. In this \nexperiment, we .xed the client request rate at 600 requests/minute at which authors Request Rate (Requests \nsent per minute) 0 5 10 15 20 25 Message Size in KBytes case(a) and case(b) are just within their capacity \nlimits while case(c) is well within its capacity limits (see Figure 8). The service time was .xed at \n4000 ms as before. Increasing the message size has a di.erent impact for the three di.erent partitionings. \nIn case (a), C0 has to send/receive six mes\u00adsages per request, but the relative impact of message size \nis low compared to the load imbalance of a centralized parti\u00adtioning. In case (b), D0 has to send/receive \n.ve messages per request, and increasing the message size from 16KBto 24KBcauses the throughput to fall \ndue to a decrease in pro\u00adcessing capacity. In case(c), D0. has to send/receive three messages per request, \nbut the request load did not cause its capacity to fall short within the parameters set by this experiment. \nFindRoute Example. For the FindRoute example Figure 10 shows throughput results for variation in request \nrate and Figure 11 shows throughput results for variation with mes\u00adsage size. We show performance results \nfor the three parti\u00adtionings labeled (b), (c) and (d) in Figure 2 and also for the partitioning in Figure \n6(d) where the number of intercom\u00adponent messages has been minimized but some data gets pipelined causing \nexcess data to .ow in the network. Recall that partitioning 2(b) corresponds to centralized orchestra\u00adtion, \nwhereas 2(c) and 2(d) are two decentralized orchestra\u00adtions. We stated that 2(d) is likely to give better \nperfor\u00admance since it communicates less data on the network. Ex\u00adperimentally we observe that when the \nmessage size is small, all the decentralized orchestrations (including the pipelined case) perform equally \nwell (Figure 10), but show variations when the message size increases (Figure 11). In our exper\u00adiments \nwe ensure that the fraction of the address required by the TrainRoute service is very small. Partitioning \n2(c) puts the entire data onto the network and hence shows a deterioration in performance as the message \nsize increases. Partitioning 2(d) puts only the required fraction of data on the network and hence is \nuna.ected by increasing size of ad\u00addresses. The pipelined version (which carries the maximum data) deteriorates \nthe most. TranslateArticle Example. The BPEL code, the PDG and two partitionings for the TranslateArticle \nservice are given in Figure 12. The partitionings have been selected as case (c) that minimizes data \non the network (using the greedy\u00admerge heuristic) and case (d) that minimizes the number of hops on the \nnetwork (using the pooling heuristic). Both the decentralized versions perform well (Figure 13) compared \nto the centralized version. The partitioning generated by the pooling heuristic performs marginally better \nthan the partitioning generated by the greedy-merge heuristic as the greedy-merge version has higher \nsynchronization overheads. Neither orchestration shows deterioration (Figure 14) in per\u00adformance with \nincreasing message size within the experi\u00admented range. NearestRestaurant Example. The BPEL code, the \nPDG and two partitionings for the NearestRestaurant service are given in Figure 15. Here also the partitionings \nhave been selected as case (c), a partitioning generated by the greedy\u00admerge heuristic and case (d), \na partitioning generated by the pooling heuristic. Both the decentralized versions perform well (Figure \n16) compared to the centralized version. The partitioning in case (c) minimizes data on the network and \nperforms marginally better than the partitioning of case (d) as case (d) has higher data overheads without \nmuch bene.t of reduced synchronization overheads. Neither orchestration shows deterioration (Figure 17) \nin performance with increas\u00ading message size within the experimented range. However although we do not \nshow the results here, we observed that the response time in case (c) was substantially better than that \nin case (d) due to the higher parallelism.  5.2 Compile-time Performance We evaluated three di.erent \nalgorithms -one which does an exhaustive search of the space of all possible merges, one which uses the \nmerge-by-def-use heuristic of merging along def-use edges and one which combines the greedy-merge and \npooling heuristics described in Section 3. For the three al\u00adgorithms we report in Table 2, the total \nnumber of con.g\u00adurations that each algorithm explores and the number of con.gurations that are valid. \nIn Table 3 we report the time taken to run the algorithms. In these tables, we include one extra example, \nthe BioAnnotator composite service. BioAn\u00adnotator creates a chain of web services, in which each web \nservice adds some set of annotations to an input .le and F0 receive(client,req{lang,item}) f2.lang=req.lang \np2 f1.item = req.item F1 invoke(F1, f1{item}, f1Res{article}) f2.article=f1Res.article F2 invoke(F2, \nf2{lang,article}, f2Res{translated}) f3.translated=f2Res.translated p5 f3.article=f1Res.article F3 invoke(F3, \nf3{article,translated}, f3Res{output}) res.output = f3Res.output F4 reply(client,res{output}) (a) F0 \nreceive(client,req{cuisine,id,radius}) f1.cuisine=req.cuisine p2 f2.id = req.id flow invoke(F2, f2{id}, \nf2Res{location}) invoke(F1, f1{cuisine}, f1Res{list}) end-flow f3.radius=req.radius p4 f3.list=f1Res.list \np5 f3.location=f2Res.location F3 invoke(F3, f3{list,budget,location}, f3Res{list}) res.list = f3Res.list \nF4 reply(client,res{list}) (a)  (b)  F4 (c) F4 F0   (d)  (d) F4 PDG, (c) and (d) Two topologies \nThe PDG, (c) and (d) Two topologies Average Throughput (Requests served per minute) 1000 800 600 400 \n200 0 0 200 400 600 800 1000 1200 Request Rate (Requests sent per minute) 0 200 400 600 800 1000 1200 \nRequest Rate (Requests sent per minute) Figure 13: Throughput Variation with Request Rate Figure 16: \nThroughput Variation with Request Rate 650 Average Throughput (Requests served per minute) 600 550 500 \n450 400  Message Size in KBytes 0 5 10 15 20 25 Message Size in KBytes Figure 14: Throughput Variation \nwith Message Size Table 2: Number of Con.gurations Generated. Total is the total number of con.gurations \nexplored, Valid is the number of valid con.gura\u00adtions found. (LA=LoanApproval, FR=FindRoute, TA=TranslateArticle, \nNR=NearestRestaurant, BA=BioAnnotator) Test Exhaustive Merge Greedy-merge Case search by def-use + Pooling \nTotal Valid Total Valid Total Valid LA641124 111211 FR 78125 736 64 64 13 10 TA 15625 144 64 64 27 12 \nNR 15625 448 64 64 24 14 BA --6553665536 1 1 Table 3: Compute Time in millisec\u00adonds. (LA=LoanApproval, \nFR=FindRoute, TA=TranslateArticle, NR=NearestRestaurant, BA=BioAnnotator) Example Exhaustive Merge Greedy-merge \nsearch by def-use + Pooling LA 3039 2983 2975 FR 6576 2265 1959 TA 4080 2028 1932 NR 4185 2050 1954 BA \n- 71880 3554 passes the annotated .le to the next annotator web service. This is a fairly large program \nbut with a completely linear structure. An exhaustive search is clearly very expensive compared to the \nheuristics presented in this paper. For the BioAnno\u00adtator example, we were unable to run the exhaustive \nsearch algorithm. The merge-by-def-use heuristic generated more than 65000 partitionings, but the greedy-merge \nand pooling heuristics together generated exactly one partitioning which in fact has the best performance \ncharacteristics. The merge\u00adby-def-use heuristic generates many partitionings that are similar and hence \ngenerates a much larger number of par\u00adtitionings. The time to compute the exhaustive search al\u00adgorithm \nis also very large compared to the heuristics. For smaller programs, the time to run the greedy-merge \nand pool\u00ading heuristic algorithms is comparable with the time to run the merge-by-def-use heuristic, \nbut as the program size in\u00adcreases as in the case of BioAnnotator, the time to compute the merge-by-def-use \nheuristic is much larger.  5.3 Micro-benchmarking Micro-benchmarking is required to compute the cost \nof primitive activities used in BPEL programs. As mentioned in Section 4 these costs need to be determined \nempirically for di.erent data sizes as well as for di.erent operations on data. For brevity, we show \nthe cost variance with data size only for the following four activities: receive, reply, assign and invoke. \nTo compute the cost of these activities we ran four micro-benchmark programs -(1) the receive BPEL program \nwhich contains exactly one receive activ\u00adity and nothing else; (2) the receive-reply BPEL pro\u00ad Request \nRate (Requests/minute) Request Rate (Requests/minute) gram which receives a string and echoes it back; \n(3) the receive-assign-reply BPEL program which receives a str\u00ading in one variable, copies it to another \nvariable and sends it back; and (4) the receive-invoke-reply BPEL program which receives a string in \none variable, invokes a web service using the same data and returns the response to the client. The invoked \nweb service is an echo service. Figure 18 gives the percentage CPU utilization for each program as a \nfunction of request rate when the data sizes are all 512 bytes and Figure 19 gives the percentage CPU \nutilization for each program as a function of request rate when the data sizes are 24 KB. Since CPU utilization \nvaries linearly with request rate we can compute the cost of an activity from the slope of the plot. \nFor the receive-assign-reply benchmark for 512 byte messages, from the slope of the corresponding CPU \nutilization plot we .nd that the cost is 0.019 work units computed as CPU utilization (40%) divided \nby request rate (2100). Similarly, the cost of the receive-reply BPEL program is 0.012 units. Hence the \ncost of an assign is 0.019 - 0.012 = 0.007 units. Similarly, Message Size (KBytes) the cost of each \nactivity can be benchmarked and computed. The cost of an assign is 0.007 at message sizes of 24KB showing \nthat simple assigns are not a.ected by message size. When the assign contains an XPath expression, the \ncost is signi.cantly higher and also sensitive to message size. The cost of invoke however is 0.025 for \n512 bytes and 0.045 for 24KBmessages, indicating that invokes are sensitive to message sizes. The costs \nhave been plotted for message sizes of 256 bytes, 1KB, 4KB, 8KB, 16KB and 24KB in Figure 20. Once the \nmicro-benchmark costs have been computed, they can be used to predict the throughput of a given parti\u00adtion \nas follows: The available capacity (e.g., CapacityC0 in the example in Section 4) is set to 100 units \nif 100% CPU is alloted to the process else appropriately to a smaller value. Then our cost function computes \nthe request rate at which a partitioning will deliver peak throughput and beyond which throughput is \nexpected to deteriorate. Example. If we assume a 90% capacity availability, then applying the cost values \nin Figure 20 to the example in Sec\u00adtion 4, we get the peak rates in Requests/minute as: peak rate(C0) \n= 90 = 396 .007+.005+7*.20+3*.25 peak rate(D0) = 90 = 865 .007+.005+3*.20+3*.006+2*.007 peak rate(D0)= \n90 = 1267 .007+0.005+2*.20+2*.006+.007 While these values as absolute values are not important, they \nare indicative of the relative performance characteris\u00adtics of di.erent partitions. Discussion and Limitations. \nWe have benchmarked sev\u00aderal Linux machines with di.erent CPU, memory and kernel con.gurations. The trends \nin all the machines is the same though the actual values di.er. However, machines with the same con.guration \ngive the same results. Thus it is impor\u00adtant to benchmark only one of each type of machine in the system. \nWe have not benchmarked machines running di.er\u00adent operating systems. The cost function works on the \nassumption that the size of the data handled by each activity is known a priori. A WSDL (Web Services \nDescription Language [9]) document is a signature of a web service, providing input/output de\u00adtails of \nthe web service. Data size information can some\u00adtimes be estimated by the WSDL descriptors of the BPEL \nprograms and the WSDL descriptors of the invoked web services. However, in general, this information \nneeds to be gathered by pro.ling. Our micro-benchmarking computes an upper bound based on CPU and memory \nutilization. However, as mentioned in Section 4, there may be other upper bounds: the number of available \nthreads in the server or network bandwidth. We have not benchmarked these parameter. For this paper, \nwe assume that plentiful threads are available, although real application servers obviously limit the \nnumber of threads that can be con.gured. We also assume that network band\u00adwidth is not a bottleneck which \nis not unrealistic for LAN scenarios.  6. RELATED WORK Much work has been done on automatic parallelization \nof sequential programs based on PDGs e.g., [2, 6]. In contrast, the focus in this paper is on the use \nof PDGs in partition\u00ading of composite web service applications for decentralized orchestration. There \nare many references in the literature that are relevant to partitioning and clustering algorithms for \nparallel programs e.g., [3, 15, 16, 7, 23]. Though we leverage the results from past work on program \npartition\u00ading, we observe that there are some key characteristics that distinguishes our problem statement \nfrom the problem state\u00adments considered in past work. Speci.cally, decentralization of composite web \nservices presents a partitioning problem with the additional constraint that tasks can be either .xed \nor portable. In addition, most previous work on partition\u00ading focused on minimizing the completion time \nof a single instance of the program or for parallel tasks. The works by Graham [8] and Reiter [14] show \nhow to determine bounds on execution times and throughput in acyclic and cyclic de\u00adpendence graphs respectively \nfor parallel computation. The goal of this work is to maximize the throughput for the case when multiple \ninstances of the parallel program (composite web service) are executed. Our work uses and expands on \ntechniques for merging PDGs [10]. Singhai [19] uses a similar idea in the area of loop fusion, where \ntwo loops are merged subject to the condition that the merge does not violate dependences. His algorithm \nalso uses the notion of maximizing dependence edges withina fusedloopto increase reuse of cached vari\u00adables. \nHowever neither of them consider the dependence constraints that must be applied when merging two explic\u00aditly \nparallel sections of a program. Work by Subhlok et al [20, 21] merges adjacent tasks in a task graph \nand solve the problem of assignment of tasks to a set of processors. Their problem is di.erent in that \nthey need to determine the optimal number of processors for an application based on certain constraints \nand cost models. In our case, the number of processors is pre-determined, some tasks (.xed tasks) are \npreallocated to speci.c processors and the remaining tasks need to be assigned. Their applications are \ntypically pipelines of tasks where the output of one task feeds into the input of the next and hence \nit su.ces to merge only adjacent tasks. Our applications have much more com\u00adplex communication patterns \nand hence bene.t from task reordering. Partitioning problems in distributed computing have been tackled \nin many ways. Singh and Pande [18] give a solution to code migration based on mobile agents. A mobile \nagent represents a single .ow of control (albeit decentralized) that determines what code is executed \nat what location. Our de\u00adcentralized solution may have many parallel threads of exe\u00adcution that interact \nand synchronize and hence the problems we solve are di.erent. Zhou et al [24] give a static analy\u00adsis \nsolution for method partitioning. However, their solu\u00adtion lies in partitioning the message handling \ncode between the sender and receiver. While we do something similar, we also generate and evaluate di.erent \npartition con.gura\u00adtions, whereas they do not attempt to change the topology of a given decentralized \napplication. Tilevich and Smaragdakis [22] give a related partitioning algorithm that uses the notion \nof anchored and mobile tasks. Certain classes are anchored to .xed locations while others may be allowed \nto migrate. Their algorithm gener\u00adates proxies to access the migrated classes and determines an e.cient \npartitioning of the code. However, the .nal orches\u00adtration is centralized over RMI calls unlike our orchestration \nwhich is decentralized with asynchronous messaging. Finally, there exist other techniques for partitioning \nwork\u00ad.ows such as state and activity chart-based techniques to enable distributed execution according \nto the original se\u00admantics [13]. These techniques do not alter the invocation order of activities, and \nalso do not consider load balanc\u00ading issues in mapping activities to work.ow servers. Our work does not \nhave these limitations because we model de\u00adcentralized execution as a general partitioning of a program \ndependence graph. 7. CONCLUSIONS AND FUTURE WORK In this paper we have given a new code partitioning \nalgo\u00adrithm that is applicable to decentralization of composite web services. The algorithm depends on \na technique for testing for legality of reordering of PDG nodes, and on a technique to estimate the throughput \nof a network of servers executing a business process. Our experimental results show that de\u00adcentralization \ncan increase the throughput of example com\u00adposite services substantially, easily doubling it under high \nsystem load. In the near future we plan to build a feedback control mechanism that will determine the \ncorrect runtime values to the parameters in the cost function. Depending on the feedback we will enable \nswitching between di.erent partition con.gurations based on runtime conditions. From an algorithmic standpoint, \nwe plan to enhance our algorithm in the future to consider merges of .xed nodes i.e., when multiple .xed \nnodes are placed in the same partition. 8. ACKNOWLEDGMENTS We would like to thank Girish Cha.e, Sunil \nChandra and Vijay Mann of IBM India Research Laboratory for their their suggestions, ideas, and for their \nenthusiastic support in implementing the tool. We would also like to thank the BPWS4J team at IBM T. \nJ. Watson Research Center for their support in using the BPWS4J engine. 9. REFERENCES [1] WebSphere \nApplication Server. http://www\u00ad3.ibm.com/software/info1/websphere/index.jsp? tab=products/appserv. [2] \nW. Baxter and I. H. R. Bauer. The program dependence graph and vectorization. In Proceedings of the ACM \nSymposium on Principles of Programming Languages, 1989. [3] S. H. Bokhari. Partitioning Problems in Parallel, \nPipelined, and Distributed Computing. IEEE Transactions on Computers, C-37:48 57, January 1988. [4] \nBusiness Process Execution Language for Web Services Version 1.1. http://www.ibm.com/developerworks/library/ws\u00adbpel/. \n [5] BPWS4J: Java Run Time for BPEL4WS. http://www.alphaworks.ibm.com/tech/bpws4j. [6] J. Ferrante, \nK. Ottenstein, and J. Warren. The program dependence graph and its use in optimization. ACM Transactions \non Programming Languages and Systems, 9:319 349, July 1987. [7] A. Gerasoulis, S. Venugopal, and T. \nYang. Clustering Task Graphs for Message Passing Architectures. Proceedings of the ACM 1990 International \nConference on Supercomputing, pages 447 456, June 1990. Amsterdam, the Netherlands. [8] R. L. Graham. \nBounds on Multiprocessing Timing Anomalies. SIAM Journal on Applied Mathematics, 17(2):416 429, March \n1969. [9] S. Graham, S. Simeonov, T. Boubez, G. Daniels, D. Davis, Y. Nakamura, and R. Neyama. Building \nWeb Services with Java: Making sense of XML, SOAP, WSDL and UDDI. Sams; ISBN:0672321815, 2001. [10] S. \nHorwitz, J. Prins, and T. Reps. Integrating Non-Interfering Versions of Programs. Conf. Rec. Fifteenth \nACM Symposium on Principles of Programming Languages, pages 133 145, January 1988. [11] R. Khalaf, N. \nMukhi, and S. Weerawarana. Service-Oriented Composition in BPEL4WS. In Proceedings of the Twelfth International \nWorld Wide Web Conference, 2003. [12] J. Krinke. Static slicing of threaded programs. In Program analysis \nfor software tools and engineering (PASTE 98), pages 35 42. ACMSOFT, 1998. [13] P. Muth, D. Wodtke, J. \nWeissenfels, D. A. Kotz, and G. Weikum. From centralized work.ow speci.cation to distributed work.ow \nexecution. Journal of Intelligent Information Systems (JIIS), 10(2), 1998.  [14] R. Reiter. Scheduling \nParallel Computations. Journal of the ACM, 4(15):590 599, 1968. [15] V. Sarkar. Partitioning and Scheduling \nParallel Programs on Multiprocessors. Research Monographs in Parallel and Distributed Computing. MIT \nPress, Cambridge, MA, 1989. [16] V. Sarkar. Automatic Partitioning of a Program Dependence Graph into \nParallel Tasks. IBM Journal of Research and Development, 35(5/6), 1991. [17] V. Sarkar. A Concurrent \nExecution Semantics for Parallel Program Graphs and Program Dependence Graphs. Springer-Verlag Lecture \nNotes in Computer Science, 757:16 30, 1992. Proceedings of the Fifth Workshop on Languages and Compilers \nfor Parallel Computing, Yale University, August 1992. [18] A. Singh and S. Pande. Compiler optimizations \nfor Java aglets in distributed data intensive applications. In Proceedings of the ACM Symposium on Applied \nComputing, pages 87 92, 2002. [19] S. Singhai and K. McKinley. Loop fusion for data locality and parallelism. \nIn Proceedings of the Mid-Atlantic Student Workshop on Programming Languages and Systems, 1996. [20] \nJ. Subhlok, D. O Hallaron, T. Gross, P. Dinda, and J. Webb. Communication and memory requirements as \nthe basis for mapping task and data parallel programs. In Proceedings of Supercomputing, pages 330 339, \nWashington, DC, November 1994.  [21] J. Subhlok and G. Vondran. Optimal latency-throughput tradeo.s \nfor data parallel pipelines. In Proceedings of the Eighth Annual ACM Symposium on Parallel Algorithms \nand Architecture (SPAA), Padua, Italy, June 1996. [22] E. Tilevich and Y. Smaragdakis. J-Orchestra: Automatic \nJava application partitioning. In Proceedings of European Conference on Object Oriented Programming (ECOOP \n02), 2002. [23] T. Yang and A. Gerasoulis. DSC: Scheduling Parallel Tasks on an Unbounded Number of Processors. \nIEEE Transactions on Parallel and Distributed Systems, 5(9):951 967, 1994. [24] D. Zhou, S. Pande, and \nK. Schwan. Method partitioning -runtime customization of pervasive programs without design-time application \nknowledge. In Proceedings of the 23rd International Conference on Distributed Computing Systems (ICDCS \n03), 2003.  \n\t\t\t", "proc_id": "1028976", "abstract": "<p>Distributed enterprise applications today are increasingly being built from services available over the web. A unit of functionality in this framework is a web service, a software application that exposes a set of \"typed'' connections that can be accessed over the web using standard protocols. These units can then be composed into a &#60;i>composite&#60;/i> web service. BPEL (Business Process Execution Language) is a high-level distributed programming language for creating composite web services.</p> <p>Although a BPEL program invokes services distributed over several servers, the &#60;i>orchestration&#60;/i> of these services is typically under centralized control. Because performance and throughput are major concerns in enterprise applications, it is important to remove the inefficiencies introduced by the centralized control. In a distributed, or decentralized orchestration, the BPEL program is partitioned into independent sub-programs that interact with each other without any centralized control. Decentralization can increase parallelism and reduce the amount of network traffic required for an application.</p> <p>This paper presents a technique to partition a composite web service written as a single BPEL program into an equivalent set of decentralized processes. It gives a new code partitioning algorithm to partition a BPEL program represented as a program dependence graph, with the goal of minimizing communication costs and maximizing the &#60;i>throughput&#60;/i> of multiple concurrent instances of the input program. In contrast, much of the past work on dependence-based partitioning and scheduling seeks to minimize the &#60;i>completion time&#60;/i> of a single instance of a program running in isolation. The paper also gives a cost model to estimate the throughput of a given code partition.</p>", "authors": [{"name": "Mangala Gowri Nanda", "author_profile_id": "81100183505", "affiliation": "IBM India Research Laboratory", "person_id": "P186815", "email_address": "", "orcid_id": ""}, {"name": "Satish Chandra", "author_profile_id": "81100394237", "affiliation": "IBM India Research Laboratory", "person_id": "PP39040833", "email_address": "", "orcid_id": ""}, {"name": "Vivek Sarkar", "author_profile_id": "81100597290", "affiliation": "IBM T. J. Watson Research Center", "person_id": "PP14206121", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1028976.1028991", "year": "2004", "article_id": "1028991", "conference": "OOPSLA", "title": "Decentralizing execution of composite web services", "url": "http://dl.acm.org/citation.cfm?id=1028991"}