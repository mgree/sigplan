{"article_publication_date": "10-16-2006", "fulltext": "\n Adapting Virtual Machine Techniques for Seamless Aspect Support Christoph Bockisch* Matthew Arnold+ \nTom Dinkelaker* Mira Mezini* *Darmstadt University of Technology / +IBM T.J. Watson Research Center bockisch@informatik.tu-darmstadt.de \n/ marnold@us.ibm.com / {dinkelaker, mezini}@informatik.tu-darmstadt.de Abstract Current approaches to \ncompiling aspect-oriented programs are in\u00adef.cient. This inef.ciency has negative effects on the productiv\u00adity \nof the development process and is especially prohibitive for dynamic aspect deployment. In this work, \nwe present how well\u00adknown virtual machine techniques can be used with only slight modi.cations to support \nfast aspect deployment while retaining runtime performance. Our implementation accelerates dynamic as\u00adpect \ndeployment by several orders of magnitude relative to main\u00adstream aspect-oriented environments. We also \nprovide a detailed comparison of alternative implementations of execution environ\u00adments with support \nfor dynamic aspect deployment. Categories and Subject Descriptors D.3.4 [Programming Lan\u00adguages]: Processors \nrun-time environments General Terms Languages, Measurement, Performance Keywords aspect-oriented programming, \nvirtual machine sup\u00adport, aspect weaving, dynamic deployment, envelope-based weav\u00ading 1. Introduction \nCurrent approaches to compiling aspect-oriented programs are in\u00adef.cient. This inef.ciency has negative \neffects on the productiv\u00adity of the development process and is prohibitive for dynamic as\u00adpect deployment. \nElsewhere we have presented a new compilation technique for aspect-oriented programs [13] which improves \nthe compilation performance, however, at the cost of increased runtime overhead. In this paper, we show \nhow well-known virtual machine optimization techniques can be used with only slight modi.cations to support \nfast aspect compilation while retaining runtime perfor\u00admance. Furthermore, these optimizations can be \ncombined with es\u00adtablished implementations of dynamic features like class loading to provide excellent \nperformance for dynamic aspect deployment. With aspect-oriented programming, a new kind of modules, i.e., \naspects, is introduced. An aspect encapsulates a concern whose implementation would otherwise be scattered \nover several modules and tangled with the code of other concerns. There are several Permission to make \ndigital or hard copies of all or part of this work for personal or classroom use is granted without fee \nprovided that copies are not made or distributed for pro.t or commercial advantage and that copies bear \nthis notice and the full citation on the .rst page. To copy otherwise, to republish, to post on servers \nor to redistribute to lists, requires prior speci.c permission and/or a fee. OOPSLA 06 October 22 26, \n2006, Portland, Oregon, USA. Copyright c &#38;#169; 2006 ACM 1-59593-348-4/06/0010. . . $5.00 .avors \nof aspect-oriented programming [32]. The focus of this paper is on the pointcut-advice .avor [32], which \nis supported by the majority of aspect-oriented languages. A pointcut describes a set of points in the \nexecution of a program; the projections of these points onto places in the program s code are called \njoin point shadows [24, 33]. Advice are associated to pointcuts and specify functionality that is bound \nto join points selected by the pointcut. A more detailed discussion is provided in section 2.1. In addition \nto the workload of ordinary source code compil\u00aders, current compilers for aspect-oriented languages perform \naddi\u00adtional operations, which involve transformations, called weaving, throughout the entire program \ncode. 1. Join point shadows are searched for, and this search is expensive because it must involve the \ncomplete application code. Shadows are, generally, scattered throughout the code a natural conse\u00adquence \nof the crosscutting nature of the concern expressed in an aspect. For example, the shadows for a pointcut \nthat selects all calls to a public method are spread throughout the program. 2. Advice invocation logic \nis woven in at several different loca\u00adtions. E.g., when each call to a speci.c method must be advised, \nall call sites are affected. Hence, an advice invocation weaving action must be repeated several times. \n 3. More importantly, weaving an advice call at each individual join point shadow, generally, is complex: \nInserting instructions for advice invocation potentially entails updating other instruc\u00adtions and control \nstructures like the exception handler map.  Aspects can be activated either before starting the application \nor dynamically, what we call dynamic aspect deployment. When a new aspect is deployed at runtime, its \ncorresponding pointcut\u00adadvice pairs are introduced into a running system. Join point shad\u00adows of the \npointcuts must be searched for and the respective advice calls must be woven1. That is, the aspect weaving \noverhead affects the runtime behavior of the system. To improve performance of AOP compilers in previous \nwork [13] we introduced a new technique for weaving pointcut-advice, called envelope-based weaving. \nWith envelope-based weaving in\u00addirections are introduced between a method call or .eld access site and \nthe respective class members being accessed. The indirection takes the form of a method, called envelope, \none per each method or .eld of any class in the application. Instead of calling a method or accessing \na .eld directly the respective envelope is called. The envelope represents the original member and, in \nturn, carries out the actual method call or .eld access. 1 Aspect deployment may involve other operations, \ne.g., solving con.icts between aspects; these operations are outside the scope of this paper. Envelopes \nsimplify the weaving process in three ways: 1. Join point shadows are localized exclusively inside envelopes; \nhence, their search is better localized. 2. Weaving has to be executed at less shadows -only within \nthe envelopes and not e.g., at all call sites of a method. 3. Envelopes have a simple control .ow, which \nmakes the weaving action itself a cheap operation.  In a case study [13], we measured the effect of \nthese simpli.\u00adcations on the ef.ciency of the weaving process by comparing the compilation time of a \nprogram once with a simple aspect and once without aspects: Compiling XalanJ [52] with a simple aspect \nusing AspectJ s ajc compiler 1.2.1 [10] took up to 3 times longer than compiling the same application \nwithout the aspect using javac. The same experiment with envelope-based weaving only produced an overhead \nof 0.3 times. Yet, without special support from the execution environment, envelopes also introduce some \nruntime penalty. When using en\u00advelopes for all methods and .elds of application classes and with\u00adout \nvirtual machine support, we measured an overhead of 19.2% on average for the SPEC JVM98 benchmark applications \n(see section 7.1). Furthermore, the focus in [13] was on static compilation; im\u00adportant issues related \nto dynamic aspect deployment were left out of focus. To avoid the runtime performance degradation, in \nthis paper we propose to make the virtual machine (VM) envelope-aware so that it understands the concept \nof envelopes and can optimize accord\u00adingly. Current major VMs employ just-in-time compilation which is \nto dynamically compile bytecode at runtime into native machine code. In this step sophisticated optimizations \nare applied to the generated code. We show that existing dynamic optimization tech\u00adniques available \nin most modern VMs to improve performance in the presence of dynamic dispatch can seamlessly be adopted \nto (made aware of) envelopes. Making the VM envelope-aware has a number of advantages: 1. Eliminating \nperformance overhead of envelopes. The VM can ensure that envelopes are inlined for hot methods to avoid \nthe level of indirection and, thus, provide performance identical to a system not using envelopes. 2. \nEf.cient dynamic aspect deployment. Only envelopes are mod\u00adi.ed by the weaving, and we can use existing \nef.cient invalida\u00adtion mechanisms of the VM to replace envelopes after modi.\u00adcations. 3. Advising re.ective \noperations. Since Re.ection [27] is a service of a virtual machine, it is only possible to support advising \nre\u00ad.ective method calls or .eld accesses by modifying this service in the virtual machine2.  One might \nargue that adding AOP-speci.c optimizations to the Java virtual machine (JVM) also has disadvantages, \nsuch as in\u00adcreasing the complexity of the JVM, and increasing the depen\u00addencies between the JVM and AOP \nconstructs. However, aspect\u00adoriented programming is a new programming paradigm, just like object-oriented \nprogramming was a new paradigm as compared to structural programming. As such, it is natural to design \nand imple\u00adment VMs dedicated to aspect-oriented languages just as dedicated VMs for object-oriented languages \nexist. The performance is likely to be a key factor in whether aspects are adopted mainstream and treated \nmore as a language feature. 2 A weaver could also instrument re.ective operations to check at runtime \nwhether there is an advice for the operation to be executed re.ectively and to execute the advice if \nthis is the case. However, an aspect-aware re.ection mechanism provided at VM-level is more appropriate \n[51]. Applying targeted optimizations in the virtual machine is an effec\u00adtive way to achieve optimal \nperformance; the goal of this research is to advocate and evaluate this approach by investigating its \nper\u00adformance potential. The primary contributions of this paper are as follows: We describe how a VM \ncan be modi.ed to become envelope\u00adaware to obtain the compilation performance bene.ts of en\u00advelopes while \nminimizing the runtime performance drawbacks.  We describe how existing dynamic optimization technology \ncan be used to achieve ef.cient dynamic deployment.  We describe our design and implementation of the \naforemen\u00adtioned techniques in Jikes Research Virtual Machine [28].  We present experimental results \ndescribing the effectiveness of our approach and provide a comprehensive comparison be\u00adtween different \nimplementations of deployment.  The remainder of this paper is structured as follows. The fol\u00adlowing \nsection gives background information about aspect-oriented programming, envelopes, aspect weaving and \ndynamic optimiza\u00adtions in virtual machines. Section 3 provides an overview of our approach for making \na virtual machine envelope-aware. Sections 4 and 5 present alternative optimizations for reducing the \noverhead of envelopes and for ef.ciently invalidating compiled code at as\u00adpect deployment. Related aspect-oriented \nexecution environments are presented in section 6 and evaluated in section 7.1. Section 7.2 evaluates \nalternative optimization implementations. Section 8 con\u00adcludes the paper and discusses areas of future \nwork. 2. Background In this section, we will provide some background on aspect\u00adoriented programming, \nenvelopes, Java bytecode, conventional and envelope-based weaving, and dynamic optimizations in virtual \nma\u00adchines. Readers familiar with these concepts may want to skip the particular sub-sections. The following \nsub-section introduces aspect-oriented programming and basic terminology by means of a small example. \nIn section 2.2 we summarize our previous work [13] in describing the semantics and the terminology of \nenvelopes. We give a comparative discussion of conventional and envelope-based aspect weaving in section \n2.3. Finally, section 2.4 discusses estab\u00adlished optimization techniques. 2.1 AOP Background For an introduction \nto the pointcut-advice .avor of aspect-oriented programming consider an application using the Services \nclass (listing 1) to access data from a database. The application distin\u00adguishes two different trust \ncontexts from which database accesses may happen. Internal requests to data are trusted and originate \nfrom the method Facade.internal(). In contrast, external requests come through Facade.external() and \nare not trusted. To avoid security leaks, external requests should be validated before they may execute. \nListing 1 shows an excerpt of the Services class with two service methods that execute an SQL query on \nthe database. The query is passed as a string parameter. Using parameters that origi\u00adnate from possibly \nuntrusted sites in SQL statements bears the risk of an SQL command injection attack where unexpected \nparame\u00adter strings are used to alter the effect of the SQL query, e.g., in order to gain unauthorized \naccess to data. Such attacks can be pre\u00advented by checking unsafe SQL statements before sending them \nto the database. The sequence diagram in .gure 1 shows a possible execution that uses the Services class \nfrom listing 1. 1 class Services { 2 Database db; 3 4 5 6 7 8 9 10 11 12 13 void service1(String // \n... db.execQuery(sql); } void service2(String // ... db.execQuery(sql); } } sql) { sql) { Listing 1. \nExample Service class. Facade Services external()  internal() service1() execQuery() external() Figure \n1. Sequence diagram for an example execution using class Service. In listing 2, the checks for the example \n(listing 1) are mod\u00adularized into the aspect named QueryInjectionChecker pro\u00advided in the syntax of the \nAspectJ language [10]. First, the as\u00adpect de.nes when to check SQL statements by specifying the pointcut \nuntrustedDBCalls in lines 3 to 6. Pointcuts are ex\u00adpressions that refer to a set of join points which \nare points in the execution of a program. In the example, join points selected by untrustedDBCalls are \nthe method calls to the execQuery() method marked with index a in the sequence diagram (.gure 1). Second, \nthe aspect de.nes what should happen at these join points, in terms of advice which is a piece of code \nassociated with the pointcut (lines 8 to 11). The advice also speci.es whether the ac\u00adtions it de.nes \nshould be executed before, after or around selected join points. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 aspect \nQueryInjectionChecker { pointcut untrustedDBCalls(String sql): call(ResultSet Database.execQuery(String)) \n&#38;&#38; args(sql) &#38;&#38; cflow(void Facade.external(Request)); before(String sql): untrustedDBCalls(sql) \n{ if (isAttack(sql)) throw new SQLInjectionException(); } // ... } Listing 2. Aspect checking for SQL \ncommand injection. The pointcut given in the example consists of several sub\u00ad expressions. The call sub-expression \n(line 4) selects all call sites to execQuery(). But pointcuts can also select join points based on their \ndynamic context. In our example, this is used to select only re\u00adquests which are in in the control .ow \nof the external() method, by means of the cflow sub-expression in line 6. For illustration, the gray \nboxes in the sequence from .gure 1 mark the control .ow of the external() method in that sequence. Pointcuts \ncan also be used to reify the join point s context. This means that values from the context are passed \nas arguments to the associated advice. The args sub-expression in line 5 (listing 2) rei.es the argument \nof the method call. It is bound to the name sql by which the advice can access it to check the SQL query \nfor command injection. The predominant implementation approach for the pointcut\u00adadvice .avor of AOP builds \nupon the following idea: While join points generally depend on runtime execution properties, it is pos\u00adsible \nto locate a set of code locations, called join point shadows [24, 33], whose execution potentially yields \na join point. For ex\u00adample, the join points for calls to the method execQuery(), which are selected by \nthe pointcut in our example aspect, have join point shadows in lines 6 and 11 in the Service class in \nlisting 1. As can be seen in .gure 1, not all executions of joint point shadows (index a and b) really \nare join points but only those which also match the dynamic context speci.ed in the pointcut (index a). \nIn current approaches, the aspect weaver is the part of the com\u00adpiler that is responsible for inserting \naspects into the program. It .nds all join point shadows in an application for a given set of pointcuts; \na call to the advice functionality, which is mapped to a Java method, is inserted at these shadows [34]. \nIf necessary, the weaver also generates a check for the dynamic properties, which possibly skips the \nadvice invocation. In our example, the weaver generates such checks because of the cflow sub-expression \n(list\u00ading 2, line 6). The checks test whether execution is in the control .ow of the external() method. \nThe approach just sketched is pursued by AspectJ s [30] ajc [10] and abc [5] compilers, the CaesarJ compiler \n[7, 17], Steamloom [23, 14, 43], AspectWerkz [11, 15] and JAsCo [46] with the list being non-exhaustive. \n 2.2 Envelopes Background To support envelope-based weaving [13], the application has to be transformed \ninto the envelope-style. All call or access sites to mem\u00adbers in the application are replaced with an \ninvocation of a corre\u00adsponding envelope3. The envelope is a method that merely calls the original member \nit replaces. By introducing such an indirection, all direct accesses to any class member are localized \nin a single place. Envelopes, which are modeled as Java methods called envelope methods, can be inserted \ninto programs by using bytecode modi.\u00adcations. There are two steps in the transformation process: (a) \nen\u00advelopes are added for all members of a class, and (b) the bytecode of the program is changed to use \nthe envelopes instead of the mem\u00adbers they envelop. In our current work we support .eld accesses and \nmethod calls4, though envelopes for other join point models can be thought of. In our model, there are \nthree kinds of envelope methods (en\u00advelopes for short): Proxy envelopes for method calls (also proxy \nfor short), getter and setter envelopes, for .eld accesses (also, get\u00adter, respectively setter for short). \nWe call the latter two uniformly accessor envelopes (or accessor). A method for which a proxy is generated \nis called an implementation method of the proxy, a .eld for which an accessor is generated is called \nan enveloped .eld. The 3 In section 4.2 we will show that lazy introduction of envelopes yields better \nperformance, conceptionally, though, this indirection exists from the start of the application. 4 Though \nAspectJ distinguishes between method call and execution join points, we support both by means of method \ncall envelopes. body of an envelope method basically consists of a method call, respectively a direct \n.eld access. 1 2 3 4 5 6 7 8 9 10 class C { public C() {init();} private int field; public void init() \n{ int f = field; ... } } Listing 3. Original class. Listing 3 shows a simple Java class that is transformed \nto envelope-style as shown in listing 4. For each .eld declared in the class C (listing 3, line 4) a \ngetter and a setter method is created (listing 4, lines 6-7). The names of the generated methods encode \nthe fully quali.ed .eld name and the kind of access. To be able to access accessor envelopes in the same \nway as the .elds, they have the same visibility, and for static .elds accessors are also declared static5. \n1 2 3 4 5 6 7 8 9 10 11 12 13 14 class C{ public C () {this();} private C() {init ();} private int field; \nprivate int get_C_field() {return field;} private void set_C_field(int v) {field = v;} public void init \n() {init()}; private void init() { int f = get_C_field(); ... } } Listing 4. Class in the envelope-style. \nProxy envelopes are an indirection inserted between the actual call site and the actually executed callee. \nSince abstract methods, including methods declared in interfaces, do not have an implemen\u00adtation, they \ncan never be the actual target of a call and do not need an envelope. For all other kinds of methods \nwe create envelopes which call their implementation method, as shown in lines 2 and 9 of listing 4. A \nproxy envelope gets the same modi.ers, formal parameters, return type, and throws clause as its implementation \nmethod. The name is derived from the implementation method s name in listing 4 it is presented as the \nimplementation method s name with an additional charecter at the end. Finally, the implementation methods \nmust be transformed in order to ensure that envelopes are called instead of directly calling a method \nor accessing a .eld. Listing 3 shows the call to a method (line 2) and the access to a .eld (line 7) \nbefore the introduction of envelopes. Lines 3 and 11 in listing 4, illustrate how these calls are transformed \nto insert the envelopes into the call chain. The presented bytecode modi.cations can be performed with \nsimple operations, in one pass, and for each class individually, e.g., by using a post-processor as in \n[13]. In the implementation presented in this paper, they are performed at class loading-time. 5 Fields \ncan also be de.ned as constants in interfaces, but it is not valid in Java to add accessor methods to \ninterfaces. However, since compile-time constants will be inlined in the application s bytecode and access \nto inlined constants can not be detected at runtime, it does not make sense to support advising access \nto constants, anyway.  2.3 Conventional vs. Envelope-based Weaving Weaving into envelopes is far simpler \nthan weaving into normal methods. This is because envelopes have a control .ow structure that does not \nneed costly updates when weaving into. As well, there is no exception handling in envelopes. Hence, weaving \ninto envelopes does not require costly updates of the code structure. In contrast, other weaving techniques \ndirectly weave advice calls into the method at join point shadows of member accesses. A problem with \nthis approach is that weaving must take care not to destroy the control .ow structure of method s bytecode. \nIn the following, we present an example with several bytecode excerpts in order to show the differences \nbetween conventional and envelope\u00adbased weaving in detail. In listing 5, we show a class Timer with a \n.eld member startTime. The method setStartTime() (lines 5 11) sets the .eld startTime (line 3) to the \nvalue of parameter t, in case isValid(t) is true. Method isValid() simply checks if t is below the limit \n100 (line 17). If the time value is negative, an IllegalArgumentException is thrown (lines 15 16). Further, \nlisting 5 shows an aspect MyAspect (lines 21 to 24) that advises access to the startTime .eld. The aspect \nis woven into the class, once with the conventional and once with the envelope-based weaving approach, \nand the results are compared in the following. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 \n22 23 24 public class Timer { int startTime; void setStartTime(int t) { try { if(isValid(t)) {startTime \n= t;} } catch (IllegalArgumentException e) { MyApp.showErrorDialog(); } } static boolean isValid(int \nt) throws IllegalArgumentException { if (t< 0) throw new IllegalArgumentException(); return (t < 100); \n} } aspect MyAspect { before() : set(int Timer.startTime) { /* advice */ } } Listing 5. The Timer class. \nFigure 2 shows the bytecode and the exception table of the  method setStartTime(). A single instruction \nin the bytecodes can be addressed by its bytecode index (bc-index). The exception table speci.es how \nto handle exceptions occurring in speci.c code regions. Each row de.nes a division of bytecode for which \nexcep\u00adtions will be caught. The start and the end bytecodes of that division are determined by the indices \nin columns from and to. If an excep\u00adtion of the type speci.ed in column type occurs while executing the \nbytecode division, exception handling code is executed by con\u00adtinuing with the instruction at the index \nin column target. Nor\u00admal execution passes the instructions at bc-index 0 12 and 19. As can be derived \nform the exception table, an IllegalArgument-Exception is handled by the instructions from bc-index 15 \nto 16. 2.3.1 Conventional Weaving The box in .gure 2 that borders the instruction at bc-index 9 marks \nthe join point shadow that is matched by the pointcut of MyAspect int startTime; void setStartTime(int); \n0: iload_1 1: invokestatic #isValid(int) 4: ifeq +8 7: aload_0 8: iload_1   9: putfield #startTime \n12: goto +7  15: astore_2  16: invokestatic #showErrorDialog() 19: return Exception table:from to \ntarget type0 12 15 IllegalArgumentException Figure 2. Original bytecode of setStartTime(). speci.ed in \nline 22 in listing 5. In the case of conventional weaving, the box marks the location at which call to \nadvice is directly woven in. The results of weaving MyAspect with the conventional weav\u00ading approach \nare shown in .gure 3. All places in the bytecode ex\u00adcerpt that have been subject to bytecode manipulation \nare marked with gray boxes. Before the matched join point shadow, a call to the advice method is inserted, \nwhich has the size of three bytes. As a consequence, all following bytecodes (up from former bc-index \n9) are moved downward three bytes and their bc-indices are changed. Note that the branch instruction \nat bc-index 4, still points to the goto instruction which now is at bc-index 15. Because the goto was \nmoved, the relative offset of the jump had to be updated (gray box, bc-index 4). The same is true for \nthe bc-indices in the excep\u00adtion table. void setStartTime(int); 0: iload_1 1: invokestatic #isValid(int) \n4: ifeq 7: aload_0 8: iload_1 12: putfield #startTime 15: goto +7 18: astore_2 19: invokestatic #showErrorDialog() \n22: return  Exception table:from to target type0 15 18 IllegalArgumentException     Figure 3. \nBytecode of setStartTime() after conventional weav\u00ading. 2.3.2 Envelope-Based Weaving Figure 4 shows \nan excerpt of the modi.ed bytecode after inserting the envelopes. For illustration, at index A the structure \nof a plain envelope (here for setter set startTime()) is shown, i.e., an envelope without advice that \nonly consists of a member access block and a return block. In general, the member access block contains \ninstructions that either call a method or perform a .eld access. The return block terminates the envelope \ns execution and possibly returns the result. In our example, the member access block (bc-indices 0 2) \nonly writes the .eld and the return block (bc-index 5) simply returns without result.  Figure 4. Bytecode \nafter inserting envelopes. With envelope-based weaving, the call to advice happens in so-called advice \nblocks. In .gure 5 bc-index 0, such an advice block (index B) has been inserted into the envelope. Because \nthe inserted advice block has the length 3, the indices of the subsequent bytecodes of the member access \nand the return block have been increased by 3. Note that no existing instruction or exception table needs \nto be updated. void set_startTime(int); 0: invoke #advice0() B advice 3: 4: 5: aload_0iload_1putfield \n#startTime member access 8: return return Figure 5. Envelope with an advice. Due to the block structure \nof envelopes, weaving inside of them is very simple. Conventional approaches have to cope with sub\u00adstantially \nmore complexity, and as a result often manage this com\u00adplexity using bytecode toolkits [13]. The toolkit \nhelps to update structures of manipulated methods such as jump targets and excep\u00adtion handler tables; \nhowever, these updates have additional costs, and have a signi.cant effect on weaving performance. Furthermore, \nbytecode toolkits generally introduce overhead on their own due to high memory demands. 2.3.3 Weaving \nConditional Logic To realize dynamic pointcuts which only match, when a join point shadow is executed \nin a speci.ed dynamic context, a conditional advice block is used, which in addition to the advice call \ncontains a dynamic test that checks a condition. In .gure 6, a conditional advice block (index C) has \nbeen inserted into the sequence (bc-indices 0 4). If the test (bc-indices 0 1) fails the advice execution \nis skipped (bc-index 1) by jumping to the next block in the sequence (bc-index 7). void set_startTime(int); \n Figure 6. Envelope with a conditional advice. 2.3.4 Multiple Advice Weaving In the weaving process, \nseveral advice blocks may be inserted into the sequence of blocks in an envelope. When executing an envelope \nmethod, the blocks are executed in sequential order. After weaving an advice block, relative jump offsets \nin the blocks need never to be updated, because they only point to the instruction following the last \ninstruction of the current block. Note that, in .gure 7, even though another advice block (index D) has \nbeen woven into the envelope from .gure 6, the relative offset of the jump at bc-index 1 needs not to \nbe changed. void set_startTime(int); Figure 7. Envelope multiple advice.  2.3.5 Limitations of Envelope-Based \nWeaving Without VM Support The improved weaving performance due to envelopes does not come for free. \nEnvelopes change the program s structure, especially its call graph, affecting the non-functional behavior \nof the program. The additional indirections from envelopes degrade performance even if no aspect weaving \ntakes place. In our initial prototype [13], which worked completely at the bytecode level providing dynamic \ndeployment by using the Class Rede.nition feature of Java 5 [18], we observed a degradation of the application \ns performance of 19.2% in an experiment discussed in section 7.1. Also the general purpose Class Rede.nition \ndoes not provide optimal performance for dynamic deployment. In [13] we measured the time needed for \ndeploying an aspect to an application consisting of 11 classes. This deployment took 101 ms with our \nap\u00adproach which is, although faster than competing implementations of dynamic deployment, not satisfactory. \n Another drawback of the initial prototype is that, e.g., no en\u00advelopes can be generated for native methods \nand that re.ective .eld accesses circumvent the envelopes.  2.4 Dynamic Optimization In Modern VMs Modern \nvirtual machines (VM) [35, 44, 6, 26, 21] contain ad\u00advanced optimizing compilers, often called a just-in-time \n(JIT) com\u00adpiler, as well as an adaptive optimization system. For ef.cient startup time, most VMs begin \nexecuting the program by using an interpreter or an ef.cient non-optimizing compiler (i.e., Jikes RVM \ns baseline compiler). As the program executes, the applica\u00adtion is pro.led to identify the hot methods, \ni.e., those methods that contribute the most to the program s execution time. Pro.ling is usually performed \nusing invocation and loop counters, or by using a timer-based sampling mechanism. Methods that are identi.ed \nto be hot are then compiled with the optimizing compiler. Most JIT compilers in high-performing VMs contain \nmultiple optimization levels. Lower optimizations levels consume less compile time but produce lower \nquality code; op\u00adtimizations at high levels consume more compile time and per\u00adform aggressive optimizations \nto produce high-quality code. If a method continues to be identi.ed as hot, it may be recompiled mul\u00adtiple \ntimes at higher optimization levels, until the highest level is reached. One of the most important optimizations \napplied by a JIT com\u00adpiler is method inlining. Inlining is made more complex for object\u00adoriented languages \nwhich support dynamically dispatched (virtual) method calls, where the runtime type of the receiver object \ndeter\u00admines the method that is actually invoked. One technique for per\u00adforming inlining at potentially \npolymorphic call sites is guarded inlining [45, 9], where a runtime test is inserted to check whether \nit is safe to execute the inlined code. If not, a full virtual dispatch is executed. Inlining is complicated \neven further for dynamic languages like Java as classes may be dynamically loaded throughout the exe\u00adcution \nof the program. As a result, call sites that are currently monomorphic may become polymorphic in the \nfuture if class load\u00ading occurs. To achieve high performance in the presence of class loading, most JVMs \nperform speculative method inlining [19, 45, 9], as described below. 2.4.1 Speculative Method Inlining \nThe goal of speculative method inlining is to perform inlining based on the current class hierarchy, \nwith the knowledge that the inlining decisions can be reversed, or invalidated when necessary when the \nclass hierarchy changes. There are three primary steps when performing speculative inlining: 1. Dependency \ntracking. When inlining methods, the VM per\u00adforms bookkeeping to identify which methods need to be in\u00advalidated \nwhen classes are loaded. For example, if class load\u00ading causes a call site C to become polymorphic, any \ncompiled method that had inlining performed at call site C needs to be invalidated. 2. Recompilation \nof invalidated methods. When class loading oc\u00adcurs, the VM recompiles all affected methods (identi.ed \nby the dependency tracking from step 1 above) and performs new op\u00adtimizations based on the new class \nhierarchy. This ensures that future invocations of these methods will execute correctly.  3. Invalidation \nof methods that are currently executing on the stack. Although recompilation (from step 2 above) ensures \nthat future invocations will execute correct code, the VM must also invalidate methods that are currently \nexecuting on the stack. For example, consider a method main() that is invoked only once and loops inde.nitely. \nIf inlining was performed at call site C in main(), and that inlining needs to be invalidated, the optimized \ncode for main() needs to be updated while it is executing on the stack, to ensure that the remainder \nof execution executes the correct method at site C. Steps 1 and 2 are accomplished fairly easily in a \nVM. Depen\u00addency tracking to map inlined call sites to compiled methods is easily maintained using a lookup \ntable. Recompilation also is easy to perform because VMs already have advanced recompilation sup\u00adport \nfor the adaptive recompilation infrastructure. The main chal\u00adlenge to speculative optimization is step \n3: invalidating methods that are active on the stack. There are multiple VM technologies for invalidating \nspeculative optimizations in methods that are active on the stack. We brie.y present two that are relevant \nin the context of this paper: on-stack replacement and code patching. On-stack replacement. On-stack \nreplacement (OSR) [25, 20] al\u00adlows changing the compiled version of a method while it is active on the \ncall stack. It can be used for several purposes, including de\u00adoptimizing code for debugging [25], undoing \nspeculative optimiza\u00adtions in the presence of class loading [25, 20], and promoting long running methods \nto higher levels of optimization [20]. OSR can be performed lazily, one stack frame at a time, when the \nmethod re\u00adturns from the caller frame. The disadvantage of on-stack replace\u00adment is that it is fairly \ncomplex, and is not implemented by all vir\u00adtual machines. Code patching. For VMs that do not implement \nOSR, guarded inlining combined with code patching can be used for specula\u00adtive method inlining [45]. \nWith traditional guarded inlining (as dis\u00adcussed in section above) a conditional instruction is inserted \nin front of the inline location to check whether the inlined code can be executed. Code patching can \nbe used to gain the functionality of guarded inlining, but without the runtime overhead of executing \nthe conditional test. In a code patching approach, the conditional (or guard) is as\u00adsumed to be true \nby default, and is compiled into a no-op instruc\u00adtion. Thus, the inlined code executes unconditionally \nand there is no overhead of executing a conditional test. If the assumptions change and the inlined method \nbody is invalidated, the no-op instructions that guard the inlined code are dynamically replaced, or \npatched with a jump to a full dispatch to the method. Code patching is effective at removing the runtime \noverhead of the guard itself; however, the existence of the slow path (the full dispatch) can still interfere \nwith optimizations, resulting in poorer quality code than with OSR. Even though the guard starts out \nas a no-op, it represents a potential if-then-else structure in the method s control .ow (where the else \nclause is the full method dispatch). Instructions that follow the guarded code must be optimized assum\u00ading \nthat either path may have been taken, thus data .ow information from the inlined code cannot be propagated \noutside of the guarded region. To enable more effective optimization in the presence of guarded code, \ntechniques such as code splitting [45, 9] can be used to pre\u00advent the data .ow information from the slow \npath (full dispatch) from merging back into the main control .ow. While this approach increases method \nsize, and thus compile time, we show in sec\u00adtion 7.2 that it can be as effective as on-stack replacement \nin terms of the quality of the code produced. 3. An Envelope Aware VM We address the limitations of \nthe envelope-based weaving summa\u00adrized in 2.3.5 by making the virtual machine (VM) aware of the envelopes. \nWe used the Jikes RVM version 2.4.1 [28] for our ex\u00adtensions. The extended RVM is available for download \nat [2]. In this section, an overview of the VM modi.cations is given. Sub-section 3.1 presents how envelopes \nare generated in our mod\u00adi.ed virtual machine. Sub-section 3.2 outlines the optimizations we apply to \navoid the runtime overhead due to indirections intro\u00adduced by envelopes. Sections 4 and 5 elaborate on \nhow we imple\u00admented these optimizations in the Jikes RVM. Sub-section 3.3 fo\u00adcuses on language features \nthat need virtual machine level support for envelope-based weaving. 3.1 Constructing Envelopes Applications \nare transformed into the envelope style at class loading-time. This is easily possible because the transformation \ncan be executed in a single pass and separately for each class (cf. section 2.2 and [13]). When a class \nis loaded, proxies for all its methods and accessors for all its .elds are generated and added to the \nclass. Further, mappings between envelopes and their en\u00adveloped members are created at this point (later \nused by the virtual machine). Other than in the conceptional discussion in 2.2 we rename the original \nmethods and create proxy envelopes that have the original method name. This way, no method call sites \nneed to be changed. This is different from the initial prototype [13] where renaming was not feasible \nfor constructor methods; if renamed, the VM s bytecode veri.er would not recognize them anymore as construc\u00adtors \nand, thus, would regard them as illegal. Instead, the signature of methods and constructors was changed \nby adding a dummy ar\u00adgument which posed an additional overhead. By adjusting the con\u00adstructor handling \nin the virtual machine renaming methods and con\u00adstructors becomes possible in the VM integrated implementation \nof the envelope-based weaving. Renaming the implementation methods also has implications for re.ective \nmethod calls and the look-up for native methods which we will discuss in section 3.3. 3.2 Optimizations \nEnvelopes create two primary performance challenges to address: 1. Reduce the overhead of envelopes to \nensure high performance when no aspects are deployed. Section 4 discusses two ap\u00adproaches for reducing \nthe overhead of envelopes. The basic idea is that the JIT compiler can ensure that accessor envelopes \nand implementation methods are always inlined into their callers, thus eliminating the overhead introduced \nby the envelope s ex\u00adtra level of indirection. 2. Ensure that dynamic deployment is performed correctly \nand ef.ciently. A primary goal of this work is to support ef.cient dynamic aspect deployment, where the \nimplementation of en\u00advelopes may change at runtime when new advice are woven into them. To enable correct \ndynamic deployment, any specula\u00adtive optimizations that were applied must be invalidated when deployment \noccurs. The suite of techniques for speculative in\u00adlining, described in section 2.4.1 can be applied, \nenabling en\u00advelope methods to be invalidated and recompiled to include the dynamically woven advice. \nSection 5 discusses two approaches for invalidation to ensure correct dynamic weaving. 3.3 Support for \nSpecial Language Features and Join Point Context Rei.cation  Handling native methods. The implementation \nof native meth\u00adods (methods with the native modi.er) is not available as Java bytecode but as system \ndependent machine code in a dynamic li\u00adbrary. To execute such a method the virtual machine searches the \nloaded dynamic libraries for a function providing its implementa\u00adtion. The look-up is based on a naming \nconvention [29]: The func\u00adtion s name is derived from the Java method s signature. Since the introduction \nof envelopes renames original methods, this look-up mechanism had to be adapted accordingly. Handling \nre.ection. Re.ection is provided in Java by means of an API [27] that allows to introspect classes as \nwell as to select methods and .elds by their names and call respectively access them. Re.ection is affected \nby envelope-based weaving in two ways: First, methods are added to classes that should not be accessible \nvia re.ection; the programmer expects a class structure to be as it is de.ned in the source code. To \naddress this issue, Jikes re.ection were modi.ed so that accessors and implementation methods are hidden \nfrom the programmer introspecting the application.  Second, when the programmer deploys an aspect which \nadvises calls to certain methods or accesses to certain .elds, he/she expects that also re.ective calls \nor accesses are advised. This issue is implicitly resolved for re.ective method calls: As the original \nmethod names and signatures have been taken over by the proxies the proxy is automatically used instead \nof the implementation method. For .eld accesses some changes are necessary. A Field object from the Re.ection \nAPI provides methods for setting and getting the value of the encapsulated .eld. The implementation of \nthese methods was modi.ed to call the respective accessor envelopes instead of performing a memory access \nto write or read the .eld.  Handling join point context rei.cation. When an advice is exe\u00adcuted at a \njoin point, it can access values from the context of the join point. There are several such values supported \nby current aspect\u00adoriented languages: Advice executing before a method call can declare to access the \narguments of the method and the receiver object. Advice executed after a call can also access the value \nreturned by the called method.  Advice on .eld writes and reads can access the object whose .eld is \nwritten/read. For .eld write join points, also the .eld s new value is accessible.  For all kinds of \njoin points, the active object (this) can be accessed.  To make these values available, additional instructions \nthat re\u00adtrieve the required values and pass them to the advice call are in\u00adserted by the aspect weaver \nat join point shadows. This process is also called rei.cation of values from the join point s context, \nor context rei.cation for short. For an AspectJ compiler, all context values are available as local variables \nor in the operand stack at the join point shadow. With envelope-based weaving, the situation is slightly \ndifferent: The join point shadow is within the envelope rather than within the call site context which \na programmer intends to advise. The question is how this affects context rei.cation. Fortunately, all \nvalues but the active object are also available as local variables or in the operand stack of the envelope. \nIt is also possible to access the active object (this), which is a local variable of the caller; we outline \nan implementation for it in section 8. 4. Reducing the Overhead of Envelopes We explored two approaches \nto avoid performance degradation relative to an untransformed program due to introducing envelopes, each \nwith different trade-offs: 1. Eager envelopes. Envelopes are created and eagerly inserted at class loading-time \n(as described in section 3.1) and the opti\u00admizer is adviced to inline them. 2. Lazy envelopes. Envelopes \nare created at class loading-time, but are not inserted into the invocation chain until they are advised. \nWhen advice is woven into an envelope, all calls to advised methods are replaced with calls to their \ncorresponding envelopes.  In the following, each of these techniques is described in section 4.1, respectively \n4.2. Subsequently, their trade-offs are discussed. 4.1 Eager Envelopes With the eager approach in place, \nenvelopes are invoked by default, thus dynamic deployment is simple and requires very few changes to \nthe VM. Reducing the overhead of envelopes required a number of changes to the optimizing compiler s \ninliner to ensure that (a) envelopes are always inlined, and (b) other inlining decisions are made the \nsame as if envelopes had not been present. Furthermore, a number of changes to other VM subsystems were \ndone to account for non-obvious performance degradations caused by envelopes. These changes are discussed \nbelow. 4.1.1 Effects on the Baseline Compiler The baseline compiler provided by the Jikes RVM does not \nof\u00adfer the possibility to inline method calls. Consequently, we do not remove the indirection for baseline \ncompiled methods. However, methods that are hot will be promoted to a higher level of opti\u00admization where \nenvelopes are optimized accordingly. With the ex\u00adception of the envelope call, the baseline compiler \ngenerates the same machine code for implementation methods as it does for the original method in the \nabsence of envelopes. 4.1.2 Effects on the Optimizing Compiler When compiling bytecode, Jikes optimizing \ncompiler successively transforms the bytecode to different intermediate representations which eventually \nresults in machine code. At each such transforma\u00adtion step, different optimizations are applied. Initially, \nthe bytecode is transformed into high level intermediate representation (HIR for short) based on a virtual \ninstruction set. The objective of our modi.cations to the optimizing compiler was to ensure that the \nHIR of methods is not affected by envelopes, since, this would guarantee that all subsequent optimizations \nare carried out without any modi.cation. To achieve this goal, knowl\u00adedge on envelopes was added into \ndifferent modules of the optimiz\u00ading compiler, as elaborated in the following. Jikes optimizing JIT compiler \nuses an inline oracle to make decisions about inlining methods. The questions asked to the oracle are \nof the form should method callee be inlined into method caller? , and the answer is either no, yes,or \nyes with a guard. By construction, we ensure that implementation methods as well as accessor methods \nare always monomorphic. Hence, one would expect the inline oracle to always inline them into their caller. \nHowever, some of its heuristics prevent the oracle from al\u00adways making these decisions. Thus, we modi.ed \nthe inline oracle to always decide to inline implementation methods and accessor methods. To facilitate \ndynamic deployment, we additionally en\u00adhanced the oracle to answer yes with a guard when an envelope \n(proxy or accessor method) is inlined. Apart from this, we also had to make sure that the remainder of \ninlining decisions is the same as it would be without envelopes, to receive the same HIR. For this purpose, \nwe modi.ed three inlining heuristics: First, the oracle considers the size of the called method when \ndeciding whether or not to inline at a call site. When the oracle is asked if a call to a proxy method \nshould be inlined, it would make its decision based on the proxy method s bytecode size. Proxies interfere \nwith size estimates because the proxy itself is small, but it is guaranteed to have a (possibly large) \nimplemen\u00adtation method inlined into it. We modi.ed the inliner so that it does not inline a proxy unless \nit would have inlined the corre\u00adsponding implementation method.  Next, the oracle ensures that the inline \nsequence does not ex\u00adceed a .xed length to avoid excessive inlining and, thus, explo\u00adsion of code size. \nAn unmodi.ed oracle would count the length of the proxy methods in the inline sequence length. As a result, \nit would eventually reach the maximum inlining length faster as it would without envelopes. Knowing that \nproxy methods will be optimized away and do not increase the size of the compiled code, we adjusted the \ninline sequence length heuristics accord\u00adingly.  Finally, for values resulting from a .eld access, the \noptimizing compiler can apply special analyses on receiver types which, in turn, are used by the inline \noracle to eliminate guards. As .eld accesses are replaced by calls to accessor methods, we had to ensure \nthat the same analyses are performed for values returned by an accessor.  4.1.3 Effects on the Adaptive \nOptimization System Before optimization and inlining occur, envelopes execute as ex\u00adplicit method calls. \nAs the program executes, the adaptive optimiza\u00adtion system (AOS) will pro.le these calls as regular methods \nand try to optimize them. One might expect that an AOS in a VM would not be thrown off by the existence \nof many such calls, and would produce similar code as it does in their absence. Unfortunately, this was \nnot the case. Jikes RVM would eventually converge on the same performance, but the existence of a large \nnumber of enve\u00adlope calls pollutes the pro.le data, causing compilations to occur at different points, \nand delays optimization of some of the application methods. As a result, startup time is degraded and \nthe application runs longer before reaching steady-state performance. We addressed this performance issue \nwith a simple change. We modi.ed Jikes RVM s timer-based sampler so that if an implemen\u00adtation method \nis sampled at runtime, the stack is walked down one additional frame, thus crediting the sample to the \nproxy envelope instead. As a result, the proxy will be identi.ed as a hot method and compiled; our inlining \nmodi.cations discussed in the previous sub\u00adsection, will ensure that the (hot) implementation method \nis inlined into the proxy and optimized, as well. 4.2 Lazy Envelopes After observing the complexity of \nthe solution required to achieve high performance with eager envelopes, we also explored an alter\u00adnative \napproach of lazy envelopes. As mentioned at the beginning of the section, lazy envelopes are created \nat class loading-time, but are not invoked as long as they are not advised; instead, the imple\u00admentation \nmethod (or .eld access) is directly executed. Thus, if no aspects have been deployed, the executing program \nis essentially identical to the original program without envelopes. When advice is deployed at runtime, \nit is woven into the enve\u00adlope method that corresponds to the relevant method or .eld access. Since envelopes \nare no longer being called by default, all code that executes the advised method (or .eld access) must \nbe dynamically updated to invoke the newly advised envelope instead. To implement lazy envelopes, both \nthe baseline and optimizing compiler needed to be modi.ed to: 1. check whether .eld accesses and method \ncalls have advice, and if so, to invoke the corresponding envelope instead. In the op\u00adtimizing compiler, \nthis is performed early in the compilation process prior to inlining, during the construction of the \ninter\u00admediate representation. 2. record dependency information for all calls and .eld accesses, to identify \ncode to be updated when advice is deployed. 3. have an invalidation mechanism available for dynamically \nredi\u00adrecting calls and .eld accesses to calls to corresponding en\u00advelopes. 4.3 Eager vs. Lazy Envelopes \n With eager envelopes, the inliner handled the inlining of envelopes, which automatically provided the \nnecessary dependency tracking and invalidation mechanism. Since this functionality is already pro\u00advided \nby the virtual machine, this approach is easier to implement than lazy envelopes and, thus, may be integrated \ninto more virtual machines. Inlining and dependency tracking had to be performed manu\u00adally for lazy envelopes, \nand no modi.cations to the inlining heuris\u00adtics and the adaptive optimizing system are necessary. Because \nwith lazy envelopes, the code generated by the baseline compiler no longer contains unnecessary envelope \ncalls, the overhead at pro\u00adgram startup is minimized. In long running applications, however, where all \nhot methods are optimized, eager envelopes ultimately achieve the same result as lazy envelopes. 5. Invalidation \nof Speculative Optimizations So far, we have described how Jikes RVM generates envelopes, and how the \nJIT compiler s method inlining is modi.ed to gener\u00adate ef.cient code. This section presents changes to \nJikes RVM to enable dynamic weaving. When weaving occurs, the affected en\u00advelopes must be recompiled \nand all compiled code containing in\u00adlined copies of these envelopes must be invalidated. To track method \ninlining dependences, we used Jikes RVM s existing method dependency database that is used for speculative \ninlining. When an envelope is inlined into a method, a dependency is added to this database. When dynamic \ndeployment occurs, it calls Jikes RVM s existing invalidation routine passing the envelopes that have \nbeen invalidated. This call triggers the Jikes RVM s mech\u00adanism for invalidating all necessary compiled \nmethod bodies. The next invocation of these methods invokes the lazy compilation stub, triggering a baseline \ncompilation of the the method. After dynamic deployment has occurred, several compiled methods may have \nbeen invalidated, and need to be recompiled. The adaptive optimization system of the VM treats these \nmethods as it would at program startup, i.e., initially compiling them at a low level of optimization, \nand eventually promoting them to higher levels over time. This approach smooths out the performance im\u00adpact \nof dynamic deployment and avoids pauses that may occur if all affected methods were immediately optimized \naggressively. To invalidate methods that are currently active on the stack, we utilize two common techniques \nas discussed in section 2.4.1: on\u00adstack replacement, and guarded inlining. 5.1 On-Stack Replacement Jikes \nRVM contains an implementation of on-stack replacement, described by Fink-Qian [20]. The implementation \nin Jikes RVM is used for speculative inlining, and for promoting long-running methods from baseline compiled \ncode. When performing specu\u00adlative inlining, a patch point is placed at all inlined call sites that may \nbe invalidated. When invalidation occurs, the patch point is replaced with a jump to an OSR point, which \ntriggers on-stack re\u00adplacement. The key advantage of OSR is that if execution jumps to an OSR point, \nit never merges back into the body of the compiled code. Therefore, the data.ow information along this \npath does not merge back in and hinder optimizations based on forward data.ow. Using Jikes RVM s mechanism \nof OSR-based speculative in\u00adlining would work reasonably well for envelopes; however, given the expected \nfrequency of envelop calls in the code we optimize one step further. In our implementation, rather than \nplacing a patch point and OSR point at each inlined call site, we place them after all yieldpoints and \ncalls in the optimized method. When the method is invalidated, these points are all patched and OSR will \nbe triggered once the next yieldpoint is reached, or when execution returns from a call. To ensure correctness \nin this model, when invalidation occurs, the VM must wait for all threads to progress to the next yieldpoint \nto ensure that OSR was triggered if necessary. Threads that are further up the call chain will perform \nOSR when the stack frame is popped and the patch point after the call site is executed. A similar technique \nof waiting for threads to reach the next yieldpoint was used in [9] to allow removing additional redundant \nguards, thus creating larger regions of guard free code. In Jikes RVM, ensuring that all threads reach \na yieldpoint is particularly easy because all non-executing threads are guaranteed to be already stopped \nat yieldpoints. At invalidation time, the VM simply needs to stop and restart the currently executing \nthreads. With eager envelopes, envelopes are not inlined by the baseline compiler, so invalidation is \nneeded for the optimizing compiler only. With lazy envelopes, invalidation is needed in baseline code \nas well, so OSR points are placed in code compiled by both the baseline and optimizing compiler. 5.2 \nGuarded Inlining As an alternative to OSR, we experimented with using code patch\u00ading and guarded inlining \nto enable envelope invalidation. Insert\u00ading guards on all inlined envelopes is relatively easy using \nJikes RVM s infrastructure for speculative inlining. For eager envelopes, we modi.ed Jikes RVM s inlining \nroutine (described in section 4.1) to stipulate that envelope methods require a code patching guard when \ninlined. When using this approach to guard envelopes, two additional changes were necessary to Jikes \nRVM. First, Jikes RVM does not normally use guards in combination with static methods because static \nmethods can not be affected by dynamic class loading. As a result, we needed to modify the locations \nin the JIT compiler that assumed a guarded inline must have a receiver object. Second, Jikes RVM may \nperform guarded inlining of polymorphic methods; for these polymorphic calls, a dynamic type test is \nused as a guard rather than code patching. When inlining polymorphic envelopes into implementation methods, \nwe modi.ed Jikes RVM to insert a patch point prior to the dynamic type test, allowing us to invalidate \nthe implementation method when dynamic deployment occurs. However, as mentioned in the last section, \ncode patching com\u00adbined with guarded inlining is not suf.cient to eliminate all enve\u00adlope overhead. Even \nthough the guard itself has no runtime over\u00adhead, the existence of the failed guard path interferes with \na num\u00adber of optimizations. To address this problem, we extended Jikes RVM s implementation of code splitting \nand redundant guard elim\u00adination. 5.2.1 Splitting Jikes RVM performs a simple local code splitting pass, \nto help ex\u00adploit optimization opportunities created by guarded inlining. Using envelopes creates a larger \nnumber of guarded inlines, thus stressing this splitting infrastructure. We discovered a number of shortcom\u00adings \nin the Jikes RVM s existing simple heuristics. While it catches the most dominant case of back-to-back \nguarded inlines, it misses several common cases, such as nested inlining. We wrote a new splitting algorithm \nin Jikes RVM to perform slightly more aggressive splitting. It is similar to the algorithm for feedback-directed \nsplitting described in [8], but for simplicity does not use pro.le information. The algorithm maintains \na worklist of merge points. A merge point for our algorithm is de.ned as a merge in the control .ow graph \nwhere one or more incoming edges are known to be infrequent, and one or more are known not to be infrequent \n(or non-infrequent). It is desirable to eliminate these merge points so that the data.ow of the infrequent \npath does not pollute the regular path. The worklist is initialized with all of the control .ow merges \ncreated by guarded inlining (where one path is known to be in\u00adfrequent) and splitting is performed as \nfollows. A merge point is removed from the worklist and further processed; if the basic block at that \nmerge point is below a size threshold, the block is dupli\u00adcated. The infrequent paths are directed to \nthe duplicated block, while the remainder of the paths remain unchanged. If duplicating the block created \na new control .ow merge (between infrequent and non-infrequent paths), the new merge is added to the \nworklist. The algorithm continues until the worklist is empty, or a space bound is reached. The size \nthresholds for duplication can be varied to adjust the aggressiveness of the splitting. Similar to method \ninlining, more ag\u00adgressive splitting has the potential to produce more ef.cient code, but will consume \nmore compile time and compiled code space, thus the splitting thresholds may vary depending on the optimization \nlevel, with higher levels performing the most aggressive splitting. While this splitting algorithm was \ndesigned to improve the performance of envelopes, we discovered that it also improved the performance \nof the base Jikes RVM (independent of using envelopes). The steady-state performance of the mtrt benchmark \nwas improved by over 20%. We are in the process of contributing our splitting back to the Jikes RVM open \nsource code base. 5.2.2 Redundant Guard Removal A desirable side effect of performing splitting is that \nmany guards may now be redundant and able to be removed completely. Jikes RVM performs redundant branch \nelimination, based on global value numbering. However, similar to the optimized placement of OSR points \n(section 5.1), we enable additional optimization by enforcing that all threads will be allowed to progress \nto the next yieldpoint at the time of invalidation. By enforcing this property, guards can be eliminated \nmore aggressively; if invalidation did not occur by the time of the previous yieldpoint was executed, \nit is guaranteed not to occur until the next yieldpoint executes. To exploit this property, we wrote \na redundant guard removal optimization phase that focuses speci.cally on removing code patching inline \nguards. The algorithm removes redundant guards by exploiting the following property: if (1) the taken \nbranch of guard A dominates guard B, and (2) no yieldpoints or method calls can be executed between A \nand B, then guard B is redundant (i.e., guaranteed never to fail) and can be removed. A method call can \nexecute a yieldpoint, thus it is implied that no yieldpoints or calls can occur between A and B. This \noptimization is implemented in Jikes RVM as a linear pass by traversing the dominator tree, and propagating \nguard liveness . In this case, liveness means that the existing code is safely pro\u00adtected by an existing \nguard, so no further guards are needed. The true branch of a guard creates liveness, and liveness is \nkilled by yieldpoints, calls, or a control .ow merge that contains a non-live incoming edge. In addition \nto improving envelop performance, this optimization also provided modest performance improvements for \nthe base ver\u00adsion of Jikes RVM (without envelopes), improving mtrt by about 3%. This optimization is \nalso being contributed back to the open source code base. 6. Related Work This section presents several \nother currently available systems that support dynamic aspect deployment, and describes the similarities \nand differences from our approach. Section 7 compares the perfor\u00admance of these systems and the implementation \npresented in this paper. The systems discussed include AspectWerkz 2.0 [11], PROSE 1.3.0 [39], JAsCo \n0.8.7 [3], Steamloom 0.6 [43], and our previous prototype [13] offering limited support for dynamic envelope-based \nweaving and using standard Java 5 Class Rede.nition [18]. For a detailed discussion of the former four \nand other implementations also refer to [16]. 6.1 AspectWerkz AspectWerkz [11, 15] provides runtime weaving \ncapabilities for sets of join point shadows which are speci.ed before runtime. The weaving process [50] \nis divided into two distinct phases: Prepara\u00adtion and activation. In the preparation phase classes are \ntransformed such that in the activation phase advice calls can be inserted at pre\u00adpared join point shadows. \nPreparation can be performed either by a post-compiler or a special class loader. When preparing, AspectWerkz \nreplaces each join point shadow with a call to a wrapper, which is similar to introducing envelopes. \nIn the activation phase, advice calls are inserted into wrappers, as in our approach. Wrappers that changed \nin this phase are replaced by means of the Java 5 standard feature Class Rede.nition [18]. In contrast \nto our approach, the wrappers of AspectWerkz do not reduce the number of join point shadows where weaving \nhappens. Instead of redirecting the call sites to a single wrapper generated for each callee, as done \nwith envelopes, AspectWerkz merely gen\u00aderates one wrapper per member and per class in which it is called. \nFurthermore, the generation of wrappers in AspectWerkz does af\u00adfect the generated machine code, i.e., \nthe generated machine code with and without wrappers is not the same. 6.2 PROSE PROSE [39] has a two \nlayered architecture consisting of the dy\u00adnamic AOP engine which is basically an API and the execution \nmonitor. The AOP engine is used to dynamically deploy an aspect; when requested to do so, it determines \nthe set of join point shadows and passes it to the execution monitor. The latter monitors the exe\u00adcution \nand calls the AOP engine back any time a join point shadow from the set is executed; in response to the \ncallback, the AOP en\u00adgine calls the corresponding advice. There are two different implementations of \nthe execution mon\u00aditor. The .rst implementation [37, 38], which we will refer to as PROSE 1 in the following, \nuses the standard JVM tools interface (JVMTI) of Java 5 virtual machines; the JVM is instructed to gen\u00aderate \nevents at join point shadows which are in turn handled by PROSE s AOP engine. The second version, PROSE \n2 [36], imple\u00adments the execution monitor by means of a modi.ed Java Virtual Machine. 6.3 JAsCo JAsCo \n[46] also uses a registry for aspects which stores all active pointcut-advice bindings. At potential \njoin point shadows so called traps are called which notify the registry and let it execute advice if \nany is applicable. JAsCo implements two optimizations of this ap\u00adproach [47]. The .rst one is the Jutta \ncompiler which generates cus\u00adtom implementations of single trap methods which directly call ap\u00adplicable \nadvice, thereby circumventing the registry. Second, calls to traps are not inserted eagerly, but only \nwhen aspects are deployed. Class Rede.nition is used to replace a method without traps with a version \nincluding traps at runtime. The lazy insertion of traps is similar to the lazy introduction of envelopes, \nas discussed in this paper. Also, generating custom traps that contain woven code for speci.c join points \nbears similarities. However, unlike in our approach, the number of weaving locations is not reduced and \nthe use of a bytecode toolkit is still necessary JAsCo employs the Javassist toolkit [4]. 6.4 The Steamloom \nVM Steamloom [43, 23] is an extension to the Jikes RVM [28] with dedicated support for aspects and dynamic \naspect deployment. In contrast to other approaches, e.g., JAsCo or AspectWerkz, that only use bytecode \ntoolkits as an external means for weaving, Steamloom has integrated the bytecode toolkit BAT [12] into \nthe VM. Simi\u00adlar to our approach, join point shadow search and advice weaving makes use of the VM s internal \nrepresentation of loaded classes. Invalidating old versions of methods in which advice calls are wo\u00adven \nmakes use of standard features of the virtual machine; however, the VM s optimizations are not modi.ed \nin order to more naturally support dynamic weaving. Similar to our approach, Steamloom restricts the \nscope of join point shadow search for member accesses. To do so, for each mem\u00adber an index is stored \nthat points to all join point shadows at which the member is accessed. When searching for join point \nshadows, Steamloom only searches the corresponding index instead of the complete bytecode. However, Steamloom \ndoes not localize the weaving operations for member accesses as still each retrieved shadow must be manipulated \nindividually, and, hence, weaving per\u00adformance is still degraded. 6.5 Envelope-Based Weaving With Class \nRede.nition In [13], we have presented a dynamic envelope-based weaver that transforms Java bytecode \nclasses through a post-processor and uses Class Rede.nition [18] to exchange envelopes after weaving \nat runtime. The transformed bytecode conforms to the Java Virtual Machine Speci.cation [31] and, hence, \nmay run on any standard JVM. Because no VM-level support exists for envelopes in the proto\u00adtype, envelopes \nmust be generated in a different way than presented in this paper. This implies that neither the execution \nperformance nor the performance of invalidating methods after dynamic deploy\u00adment was specially optimized. \nAlso, the way envelopes are generated in the prototype prohib\u00adited generation of envelopes for native \nmethod. Finally, Re.ection was not specially supported and in some cases may not behave as intended after \nthe transformation. 7. Evaluation This section provides two kinds of evaluation of the envelope-aware \nJikes RVM. In section 7.1 the performance of the envelope-aware virtual machine using lazy envelopes \nand OSR is compared to other AOP implementations with support for dynamic deployment. In section 7.2, \nwe discuss the performance of the alternative imple\u00admentations we discussed in the paper, i.e., eager \nenvelopes and code patching. 7.1 Comparison of AOP Implementations We extended the SPEC JVM98 benchmark \nsuite [42] to deploy an aspect by providing a modi.ed benchmark harness [1]. The aspect deployed by the \nharness is presented in AspectJ syntax in listing 6. The aspect advises all calls to methods or constructors \nwithin any class in any sub-package of spec.benchmarks. The advice simply increments a counter. 1 aspect \nAspect { 2 static long counter; 3 before() : 4 call(* spec.benchmarks..*.*(..)) || 5 call(* spec.benchmarks..*.new(..)) \n{ 6 counter++; 7 } 8 } Listing 6. Aspect used by the modi.ed SPEC JVM98 benchmark suite to measure the \ndeployment. The modi.ed benchmark harness executes the SPEC JVM98 benchmark applications and aspect deployment \naccording to the following schema: 1. Perform n iterations of the application (initial-phase). 2. Deploy \nthe aspect (deploy). 3. Perform n iterations of the application (deployed-phase). 4. Undeploy the aspect \n(undeploy). 5. Perform n iterations of the application (undeployed-phase). 6. Start over at step 2 \nfor m times.  To abstract over the details of aspect deployment in differ\u00adent AOP environments, the \nextended benchmark harness de.nes an interface Deployer declaring the methods deploy() and undeploy(), \nwhich is implemented for each AOP environment. The number n of iterations in each phase and the number \nm of complete cycles are arbitrary. Their in.uence on the evaluation results will be discussed below. \nThe extended benchmark tells us different properties of an exe\u00adcution environment with support for dynamic \naspect deployment: It measures the time needed for the very .rst run in the ini\u00adtial phase (step 1). \nDuring the latter phase class loading and, if applicable to the AOP implementation, preparation of loaded \nclasses takes place. The median of the times for the n bench\u00admark iterations in the initial-phase tells \nus the general perfor\u00admance of the virtual machine when no aspects are deployed. We use the median of \nthe runs to measure the steady-state per\u00adformance, i.e., to ignore the startup performance.  The benchmark \nalso tells us how long it takes to deploy and undeploy an aspect (the time needed for steps 2 and 4). \nWe also measure the performance of the .rst benchmark run after deployment and after undeployment. This \nre.ects the need to re-compile methods after they have been modi.ed by deploy\u00ading or undeploying an aspect. \nFor the performance values of deployment, undeployment, and startup after deployment and undeployment \nwe use the median of the measured times of all m cycles. This is to rule out the initialization efforts \nfor the re\u00adspective systems.  The median of the performance .gures in the deployed-phase indicates the \nin.uence of advice calls on performance. The median of the undeployed-phase .gures indicates whether \nan aspect that was once deployed leaves any footprint after being undeployed. For the evaluation, the \nbenchmarks are run at the largest size (-s100), each in a separate VM instance. The number of cycles \n(m) was three and the number of iterations per phase (n)was .fteen which allowed all systems to reach \nsteady state. We ran the benchmarks with 512 MB of heap to rule out the different memory needs of the \nexecution environments. Tables 1 through 4 show the results of the aforementioned benchmarks for the \napproaches discussed in section 6 and the envelope-aware virtual machine presented in this paper. Since \nthe implementations extend different Java virtual machines, it is not possible to directly compare all \nbenchmark results. For each ap\u00adproach that is based on a different Java VM, we calculate the rel\u00adative \noverhead of executing a benchmark on that approach s ex\u00adtended VM as compared to executing the same benchmark \non the approach s unextended base VM. These relative overheads can be used to compare all approaches. \nThe versions of the benchmarked AOP implementations and their base virtual machines are as follows. AspectWerkz \n2.0, JAsCo 0.8.7 and the prototype of envelope-based weaving using Class Re\u00adde.nition (EBW-Unaware) extend \nthe HotSpot 1.5.0 VM, Steam\u00adloom 0.6 is based on Jikes RVM 2.3.1, and the approach described in this \npaper (EBW-Aware) is based on the Jikes RVM 2.4.1. JAsCo is executed using the suggested HotSwap 2 variant \nand having the inlinecompiler and the trapall .ags enabled. The former setting en\u00adables a new weaver \nwhich provides better performance, the latter setting allows to advise all methods, including private \nmethods, as the other AOP environments also do. The tables do not include results for PROSE implementa\u00adtions. \nFor PROSE1, the Java 5 based implementation, the severely degraded performance in the deployed phase \nprohibited careful benchmarking. It was not possible to run the benchmarks at their full size. Experiments \nwith smaller sizes revealed an overhead of up to 152,278%. The PROSE2 implementation supports the base\u00adline \ncompiler of Jikes RVM only and is for this reason not com\u00adpetitive. Figures are also missing for the \njavac benchmark on AspectWerkz; the latter crashed when it was started with enabling aspect deployment \nfor the join points in our benchmark aspect. For JAsCo, we lack .gures for undeployment and the undeployed\u00adphase \nbecause it crashed at undeployment for all benchmark appli\u00adcations. The goal of the work presented in \nthis paper was to provide a virtual machine which supports fast aspect deployment while pro\u00adviding good \nsteady-state performance. Table 1 shows how many milliseconds are spent for deploying and undeploying \nthe aspect of our benchmark in the above mentioned execution environments for aspect-oriented programs. \nFor every benchmark application our envelope-aware RVM provided the best performance with an aver\u00adage \nof 3 ms for both, deployment and undeployment. At most, it took 14 ms to deploy the benchmark aspect \nand 16 ms to undeploy it. While Steamloom performs well for some benchmark applica\u00adtions, the deployment \ntime goes as high as 4,977 ms and averages out at 941 ms. At undeployment, the measured times were consid\u00aderably \nbetter with an average of 154 ms, though still two orders of magnitude slower than our implementation. \nJAsCo needs 1,685 ms for deploying the aspect on average. With AspectWerkz the time for undeploying the \naspect even goes up to 20,230 ms for the jess application and is 195 ms, at least. Table 2 shows the \nsteady-state performance of the different exe\u00adcution environments. The .rst row in each segment shows \nthe time needed to execute the benchmark applications in steady-state on a virtual machine without support \nfor dynamic aspect deployment. For each AOP implementation the overheads in the initial, deployed and \nundeployed phase are shown. The rows below show the over\u00adhead of the AOP implementations extending this \nvirtual machine. The overhead is given as the factor by which the execution time is increased, i.e., \nan overhead of 1.5 means that execution is 50% slower. As it was the goal of this work, our implementation \nimplies no overhead to the steady-state performance of an application. JAsCo also provides a good steady-state \nperformance in the initial phase with only 1.7% overhead. For the other approches the overhead in the \ninitial phase reaches from 8.6% (Steamloom) to 121% (As\u00adpectWerkz). All approaches, except the one presented \nhere, perform slightly worse in the undeployed phase than in the intitial phase. In AspectWerkz the overhead \ndrops from 121% in the initial phase to 58.5%, which is still signi.cant. The overhead presented for \nthe deployed phase also contains the overhead for executing the additional advice functionality. Dif\u00adferent \napproaches have different schemes of invoking advice; e.g., some call a static method, while others look-up \na receiver object and call a virtual method on it. For this reason, we do not discuss the variations \nof the overhead in the deployed phase. AOP implementations may also imply a degradation of the startup \nperformance. As shown in table 3, the envelope-aware Jikes RVM on average performs startup 5.6% slower \nthan the unmodi\u00ad.ed Jikes RVM. This is mainly due to the mtrt benchmark where the overhead is 13.4%. \nCompared to the other benchmarked ap\u00adproaches, though, where the startup overhead reaches from 8.9% to \n215.9% on average, our implementation still provides a reasonable startup performance. When an aspect \nis dynamically deployed, affected methods are invalidated and re-compiled the next time they are invoked. \nAs a consequence, during the .rst run after deployment just-in\u00adtime compilation is performed, similar \nto the very .rst run. Ta\u00adble 4, shows the performance of the .rst run after deployment and undeployment \nas overhead compared to the steady-state of the deployed-or undeployed-phase. Our implementation performs \nfairly well with an average overhead of 48.9% after deployment and 46% after undeployment. The prototype \nof envelope-based weaving without virtual machine integration performs best. Steam\u00adloom, performing roughly \nthe same as our implementation after deployment, performs better after undeployment. AspectWerkz and \nJAsCo perform worse than our implementation. 7.2 Comparison of Alternative Optimization Techniques So \nfar we have shown that with virtual machine support the lazy envelopes approach performs very well compared \nto other AOP approaches with support for dynamic deployment of aspects. In the following, we compare \ncompare different approaches to VM integration we have experimented with and discussed in this pa\u00adper. \nFirst, we compare eager and lazy envelopes in terms of per\u00adformance. Further, we also compare the invalidation \nmechanisms using on-stack-replacement and code patching for envelopes. The second column of table 5, \nlabeled No Invalidation Mech, Steady , evaluates the performance of Jikes RVM when eager envelopes are \nused, but no invalidation mechanism is used; this methodology identi.es the overhead introduced by the \npresence of envelopes separate from the invalidation mechanisms. Our system using envelopes is compared \nto the unmodi.ed Jikes RVM, and the overhead is reported (higher numbers mean more overhead intro\u00adduced \nby envelopes). Performance was measured at steady-state, meaning that the program is run long enough \nfor the compilation activity to stabilize. The steady-state overhead of using envelopes was small, with \na max of 3.7% and averaging 0%. In some cases envelopes ac\u00adtually reduced execution time (in particular \ndb); this result is not expected, because the compiled code is generally identical in both systems after \ninlining occurs. However, as with any modi.cations to a virtual machine, anomalous results are possible \n[22]. In our system, the baseline compiled code does not have envelopes in\u00adlined, and the order of sampling \nand optimization may be changed slightly; these differences could affect code layout memory locality, \nwhich produces unpredictable results. Overall, these results demon\u00adstrate that method inlining is effective \nat reducing the performance penalty of envelopes. The third column of table 5, labeled OSR Steady shows \nthe percentage of runtime overhead (relative to the unmodi.ed Jikes RVM) that is present when eager envelopes \nare used and OSR points are inserted (as described in section 5.1) to ensure that invalidation is performed \ncorrectly if dynamic aspect deployment were to occur. There is very little additional overhead introduced \nby using OSR, demonstrating that it is an effective mechanism for enabling invalidation. The fourth column \nof table 5, labeled Guards Steady reports the steady-state performance of a system using eager envelopes \nto\u00adgether with guarded inlining, code patching and splitting to en\u00adable invalidation for dynamic weaving, \nas described in 5.2. The overhead is computed relative to the unmodi.ed Jikes RVM. With the exception \nof mpegaudio, guards performed roughly on par with that of OSR, demonstrating the effectiveness of the \nsplit\u00adting and redundant branch elimination algorithms being used. For mpegaudio, envelopes with guards \nresult in a 6.9% degradation relative to the original Jikes RVM. After investigating, we discov\u00adered \nthat a guard is hindering optimization in a tight loop in mpe\u00adgaudio; the loop contains a single basic \nblock that is executed fre\u00adquently, and the existence of a guard in that loop breaks up some of the optimizations \nperformed by Jikes RVM s optimizing com\u00adpiler. None of the optimizations are fundamentally blocked by \nthe guard, but the loop optimizations in Jikes RVM are not particularly aggressive, and the global optimizations \n(inter-block) are not as ag\u00adgressive as the intra-block optimizations. Despite eager envelopes minimal \nimpact on steady-state per\u00adformance, startup performance was more substantially affected. The fourth \ncolumn, labeled Startup shows the performance of eager envelopes with OSR for the .rst run of the benchmarks \nwith input size -s100. The startup overhead ranges from 2.7% to 18.3% with an average of 10.5% degradation \nrelative to the unmodi.ed Jikes RVM. The startup overhead is caused by the reduced performance of the \nbaseline compiled code. The .nal column in table 5 shows the overhead caused by eager envelopes in steady \nstate when the opti\u00admizing compiler is disabled, thus code is compiled with the base\u00adline compiler only. \nBecause envelopes execute with no optimiza\u00adtion in the baseline code when using eager envelopes, the \noverhead is quite high, ranging from 8.1% 122.4%. When an application begins executing, all code is \n.rst compiled by the baseline com\u00adpiler, thus short running programs, or startup scenarios will be degraded \nwhen the quality of the baseline code is reduced. This performance penalty is avoided with lazy envelopes, \nenabling the lower startup time overhead presented in section 7.1. 8. Conclusions and Future Work Envelope-based \nweaving is a technique that facilitates fast weav\u00ading of aspects, which is particularly valuable in systems \nthat allow dynamic aspect deployment. In this paper, we have shown that sub\u00adstantial improvement in dynamic \ndeployment performance is pos\u00adsible by making the virtual machine envelope-aware. Well estab\u00adlished dynamic \noptimization techniques can be used to avoid the compress jess db javac mpegaudio mtrt jack avg EBW-Aware \n(deploy) 1ms 4ms 0ms 14 ms 2ms 2ms 1ms 3ms EBW-Aware (undeploy) 0ms 2ms 0ms 16 ms 1ms 1ms 1ms 3ms EBW-Unaware \n(deploy) 63 ms 538 ms 19 ms 672 ms 109 ms 58 ms 149 ms 229 ms EBW-Unaware (undeploy) 126 ms 635 ms 103 \nms 1152 ms 236 ms 152 ms 380 ms 397 ms AspectWerkz (deploy) 349 ms 16833 ms 215 ms  869 ms 3784 ms 1470 \nms 3360 ms AspectWerkz (undeploy) 442 ms 20230 ms 195 ms  864 ms 4247 ms 1476 ms 3922 ms JAsCo (deploy) \n331 ms 3042 ms 251 ms 4772 ms 1228 ms 729 ms 1446 ms 1685 ms Steamloom (deploy) 102 ms 1211 ms 4ms 4977 \nms 55 ms 195 ms 45 ms 941 ms Steamloom (undeploy) 3ms 188 ms 2ms 805 ms 17 ms 53 ms 16 ms 154 ms Table \n1. Time for deploying and undeploying the benchmark aspect on SPEC JVM98 benchmarks. compress jess db \njavac mpegaudio mtrt jack avg Jikes 2.4.1 4919 ms 1633 ms 8925 ms 4025 ms 5268 ms 3247 ms 3511 ms EBW-Aware \n(initial) EBW-Aware (deployed) EBW-Aware (undeployed) 0.994 1.532 1.029 1.053 1.241 1.040 1.010 1.006 \n1.004 1.040 1.226 1.045 1.005 0.933 0.914 0.894 1.190 1.003 0.980 0.924 0.915 0.997 1.150 0.993 HotSpot \n5691 ms 1672 ms 9961 ms 3667 ms 5644 ms 1372 ms 2846 ms EBW-Unaware (initial) EBW-Unaware (deployed) \nEBW-Unaware (undeployed) 1.227 1.235 1.233 1.438 1.472 1.470 1.014 1.010 1.019 1.292 1.320 1.307 1.081 \n1.118 1.090 1.033 1.129 1.098 1.256 1.282 1.274 1.192 1.223 1.213 AspectWerkz (initial) AspectWerkz (deployed) \nAspectWerkz (undeployed) 1.384 1.544 1.052 1.805 1.869 1.714 1.125 1.124 1.125   1.034 1.094 1.044 \n6.854 7.855 3.453 1.059 1.145 1.124 2.210 2.272 1.585 JAsCo (initial) JAsCo (deployed) 1.001 1.278 1.003 \n1.231 1.016 1.011 1.046 1.293 0.998 1.103 1.019 4.360 1.036 1.124 1.017 1.628 Jikes 2.3.1 5261 ms 1850 \nms 7938 ms 4798 ms 5014 ms 3347 ms 3327 ms Steamloom (initial) Steamloom (deployed) Steamloom (undeployed) \n1.007 1.235 1.144 1.076 1.256 1.277 1.003 1.145 1.180 1.167 1.221 1.240 1.203 1.140 1.434 1.131 1.954 \n1.972 1.012 1.032 1.089 1.086 1.283 1.334 Table 2. Steady-state performance for the SPEC JVM98 benchmarks. \ncompress jess db javac mpegaudio mtrt jack avg Jikes 2.4.1 7488 ms 3365 ms 9549 ms 7342 ms 7259 ms 7074 \nms 4847 ms EBW-Aware 1.058 1.067 0.991 1.033 1.079 1.134 1.029 1.056 HotSpot 5689 ms 2006 ms 10036 ms \n4740 ms 5917 ms 1536 ms 3252 ms EBW-Unaware 1.237 1.536 1.031 1.476 1.117 1.268 1.350 1.288 AspectWerkz \n1.524 5.329 1.152  1.333 7.953 1.661 3.159 JAsCo 1.031 1.605 1.012 1.343 1.074 1.195 1.212 1.210 Jikes \n2.3.1 6547 ms 3378 ms 8554 ms 7217 ms 6756 ms 6197 ms 4103 ms Steamloom 1.052 1.089 0.998 1.119 1.121 \n1.140 1.101 1.089 Table 3. Startup performance for the SPEC JVM98 benchmarks. compress jess db javac \nmpegaudio mtrt jack avg EBW-Aware (startup-deploy) 1.191 1.492 1.034 1.689 1.242 2.734 1.221 1.515 EBW-Aware \n(startup-undeployment) 1.746 1.552 1.001 1.548 1.173 2.608 1.322 1.564 EBW-Unaware (startup-deploy) 1.011 \n1.259 1.000 1.359 1.043 1.160 1.125 1.137 EBW-Unaware (startup-undeployment) 1.018 1.304 1.012 1.447 \n1.081 1.143 1.191 1.171 AspectWerkz (startup-deploy) 1.051 6.514 1.015  1.160 1.403 1.526 2.111 AspectWerkz \n(startup-undeployment) 1.085 8.180 1.013  1.163 1.875 1.535 2.475 JAsCo (startup-deploy) 1.049 2.676 \n1.029 2.627 1.237 1.294 1.624 1.648 Steamloom (startup-deploy) 1.197 1.985 1.006 2.273 1.389 1.533 1.052 \n1.491 Steamloom (startup-undeployment) 1.222 1.465 0.975 1.437 1.099 1.518 1.006 1.246 Table 4. Startup \nperformance after deployment undeployment of the benchmark aspect. Table 5. Percent overhead of using \neager envelopes in Jikes RVM. (1.0 is 1%) No Invalidation Mech, Steady (%) OSR Steady (%) Guards Steady \n(%) Startup (%) Baseline Steady (%) compress 0.0 0.0 -0.1 15.2 122.4 jess 0.7 2.1 3.5 3.3 15.5 db -4.8 \n-4.8 -4.8 2.7 8.4 javac 3.7 4.2 5.3 18.3 17.8 mpegaudio -0.6 -0.2 6.9 13.3 32.7 mtrt -0.6 -0.1 -1.4 11.3 \n57.7 jack 1.8 3.0 1.7 8.4 8.1 average 0.0 0.6 1.6 10.5 37.5 performance penalty of envelopes and to \nef.ciently invalidate opti\u00admized code after dynamic aspect deployment. The integration of envelopes into \nthe VM represents an im\u00adportant conceptual contribution to the state-of-the-art of process\u00ading aspect-oriented \nlanguages. So far, only program elements that are identi.ed by the static or dynamic weaver as join point \nshad\u00adows have the potential to yield join points. With VM integrated en\u00advelopes, any expression in the \nprogram that yields execution events included in the join point model can potentially turn into a join \npoint at runtime, similar to the notion that all Java or Smalltalk methods are per-default virtual. With \nan execution environment where dynamic aspect deployment works almost instantly, as in the presented \napproach, using this new kind of .exibility will become more feasible and natural. Making the virtual \nmachine envelope-aware is also bene.\u00adcial for aspect-oriented approaches employing static compilation. \nWhen a mapping between envelopes and their enveloped members is created by the compiler and passed to \nthe virtual machine, pos\u00adsibly included in the bytecode, the optimizations presented in this paper can \nalso be applied to envelopes generated before the run\u00adtime. In this way, the bene.ts of improved weaving \nperformance can be combined with unaffected runtime performance. There are several areas for future work. \nFirst, access to the active object from the join point s context in the advice will be supported. In \nfact, we have a partial implementation, where we adopt an optimization from Jikes RVM s JIT compiler \nto provide meta data for accessing this in each compiled method. The second area of future work is to \nprovide more sophisticated concepts of aspect-oriented programming. For example, in the cur\u00adrent implementation \nof envelope-based weaving, only method calls and .eld accesses are supported as join points. AspectJ \nsupports additional kinds of join points, e.g., exception handlers. We will evaluate how the presented \nmechanisms can be extended to support other join point models. Further, some AOP approaches also de.ne \nconcepts such as thread-local or object-local dynamic aspect deployment [7, 23, 14, 49, 48, 41, 39, 40]. \nWe will investigate how the VM techniques presented in this paper and others can be be exploited to support \nsuch advanced concepts. Acknowledgments This work was supported by the AOSD-Europe Network of Excel\u00adlence, \nEuropean Union grant no. FP6-2003-IST-2-004349. References [1] Homepage of the aspect-oriented extension \nof the SPEC JVM98 Benchmarks suite. http://www.st.informatik.tu-darmstadt. de/DeployBench. [2] Homepage \nof the Envelope-Aware Jikes RVM. http://www.st. informatik.tu-darmstadt.de/EBW-aware. [3] Jasco homepage. \nhttp://ssel.vub.ac.be/jasco/. [4] Javassist homepage. http://www.csg.is.titech.ac.jp/ ~chiba/javassist/. \n [5] abc (AspectBench Compiler) homepage. http://aspectbench. org/. [6] A.-R. Adl-Tabatabai, J. Bharadwaj, \nD.-Y. Chen, A. Ghuloum, V. Menon, B. Murphy, M. Serrano, and T. Shpeisman. The StarJIT compiler: A dynamic \ncompiler for managed runtime environments. Intel Technology Journal, 7(1):19 31, Feb. 2003. [7] I. Aracic, \nV. Gasiunas, M. Mezini, and K. Ostermann. Overview of caesarj. Transactions on AOSD I, LNCS, 3880:135 \n 173, 2006. [8] M. Arnold, M. Hind, and B. G. Ryder. Online feedback-directed optimization of Java. ACM \nSIGPLAN Notices, 37(11):111 129, Nov. 2002. In Conference on Object-Oriented Programming, Systems, Languages \nand Applications (OOPSLA). [9] M. Arnold and B. G. Ryder. Thin guards: A simple and effective technique \nfor reducing the penalty of dynamic class loading. In 16th European Conference on Object-Oriented Programming \n(ECOOP), volume 2374 of LNCS, June 2002. [10] AspectJ homepage. http://www.eclipse.org/aspectj/. [11] \nAspectWerkz homepage. http://aspectwerkz.codehaus.org/. [12] BAT homepage. http://www.st.informatik.tu-darmstadt. \nde/BAT. [13] C. Bockisch, M. Haupt, M. Mezini, and R. Mitschke. Evenelope\u00adbased Weaving for Faster Aspect \nCompilers. In In Net.ObjectDays, 2005. [14] C. Bockisch, M. Haupt, M. Mezini, and K. Ostermann. Virtual \nMachine Support for Dynamic Join Points. In AOSD 2004 Proceedings. ACM Press, 2004. [15] J. Bon\u00b4er. AspectWerkz \n-Dynamic AOP for Java. http:// codehaus.org/~jboner/papers/aosd2004_aspectwerkz. pdf, 2003. [16] J. Brichau, \nM. Haupt, N. Leidenfrost, A. Rashid, L. Bergmans, T. Staijen, A. Char., C. Bockisch, I. Aracic, V. Gasiunas, \nK. Os\u00adtermann, L. Seinturier, R. Pawlak, M. S\u00a8udholt, J. Noy\u00b4e, D. Suv\u00b4ee, M. D Hondt, P. Ebraert, W. \nVanderperren, M. Pinto, L. Fuentes, E. Truyen, A. Moors, M. Bynens, W. Joosen, S. Katz, A. Coyler, H. \nHawkins, A. Clement, and O. Spinczyk. Report describing survey of aspect languages and models. Technical \nReport AOSD-Europe Deliverable D12, AOSD-Europe-VUB-01, Vrije Universiteit Brussel, 17 May 2005 2005. \n [17] CaesarJ homepage. http://caesarj.org/. [18] Api speci.cation for package java.lang.instrument. \nhttp://java. sun.com/j2se/1.5.0/docs/api/java/lang/instrument/ package-summary.html. [19] D. Detlefs \nand O. Agesen. Inlining of virtual methods. In 13th European Conference on Object-Oriented Programming \n(ECOOP), volume 1628 of LNCS, pages 258 278, June 1999. [20] S. J. Fink and F. Qian. Design, implementation \nand evaluation of adaptive recompilation with on-stack replacement. In International Symposium on Code \nGeneration and Optimization (CGO), pages 241 252, 2003. [21] N. Grcevski, A. Kilstra, K. Stoodley, M. \nStoodley, and V. Sundaresan. Java just-in-time compiler and virtual machine improvements for server and \nmiddleware applications. In 3rd Virtual Machine Research and Technology Symposium (VM), May 2004. [22] \nD. Gu, C. Verbrugge, and E. Gagnon. Code layout as a source of noise in JVM performance. In Component \nAnd Middleware Performance workshop, OOPSLA 2004, 2004. [23] M. Haupt, M. Mezini, C. Bockisch, T. Dinkelaker, \nM. Eichberg, and M. Krebs. An Execution Layer for Aspect-Oriented Programming Languages. In J. Vitek, \neditor, Proceedings of the First International Conference on Virtual Execution Environments (VEE 05), \npages 142 152. ACM Press, June 2005. [24] E. Hilsdale and J. Hugunin. Advice weaving in aspectj. In AOSD \n04: Proceedings of the 3rd international conference on Aspect-oriented software development, pages 26 \n35, New York, NY, USA, 2004. ACM Press. [25] U. H\u00a8 Debugging optimized olzle, C. Chambers, and D. Ungar. \ncode with dynamic deoptimization. In Proceedings of the ACM SIGPLAN 92 Conference on Programming Language \nDesign and Implementation (PLDI), pages 32 43, San Francisco, California, 17 19 June 1992. SIGPLAN Notices \n27(7), July 1992. [26] K. Ishizaki, M. Takeuchi, K. Kawachiya, T. Suganuma, O. Gohda, T. Inagaki, A. \nKoseki, K. Ogata, M. Kawahito, T. Yasue, T. Oga\u00adsawara, T. Onodera, H. Komatsu, and T. Nakatani. Effectiveness \nof cross-platform optimizations for a Java just-in-time compiler. ACM SIGPLAN Notices, 38(11):187 204, \nNov. 2003. [27] The Java Re.ection API. http://java.sun.com/j2se/1.4.2/ docs/api/index.html. [28] The \nJikes Research Virtual Machine. http://jikesrvm. sourceforge.net/. [29] Jni (java native interface) homepage. \nhttp://java.sun.com/ j2se/1.4.2/docs/guide/jni/. [30] G. Kiczales, E. Hilsdale, J. Hugunin, M. Kersten, \nJ. Palm, and W. G. Griswold. An Overview of AspectJ. In J. Lindskov Knudsen, editor, Proc. ECOOP 2001, \nvolume 2072 of LNCS, pages 327 353. Springer, 2001. [31] T. Lindholm and F. Yellin, editors. The Java(TM) \nVirtual Machine Speci.cation (2nd Edition). Addison-Wesley, 1999. [32] H. Masuhara and G. Kiczales. Modeling \ncrosscutting in aspect\u00adoriented mechanisms. In 17th European Conference on Object-Oriented Programming \n(ECOOP), pages 2 28, 2003. [33] H. Masuhara, G. Kiczales, and C. Dutchyn. Compilation semantics of aspect-oriented \nprograms. In Foundations Of Aspect-Oriented Languages (Workshop at AOSD 2002), April 2002. [34] H. Masuhara, \nG. Kiczales, and C. Dutchyn. A Compilation and Optimization Model for Aspect-Oriented Programs. In G. \nHedin, editor, Proc. CC 2003, volume 2622 of LNCS, pages 46 60. Springer, 2003. [35] M. Paleczny, C. \nVick, and C. Click. The Java Hotspot server compiler. In Java Virtual Machine Research and Technology \nSymposium (JVM), pages 1 12, Apr. 2001. [36] A. Popovici, G. Alonso, and T. Gross. Just-In-Time Aspects: \nEf.cient Dynamic Weaving for Java. In Proceedings of the 2nd International Conference on Aspect-Oriented \nSoftware Development, pages 100 109. ACM Press, 2003. [37] A. Popovici, T. Gross, and G. Alonso. Dynamic \nHomogenous AOP with PROSE. Technical report, Department of Computer Science, ETH Z\u00a8urich, Z\u00a8urich, Switzerland, \nMarch 2001. [38] A. Popovici, T. Gross, and G. Alonso. Dynamic Weaving for Aspect-Oriented Programming. \nIn AOSD 02: Proceedings of the 1st International Conference on Aspect-Oriented Software Development, \npages 141 147. ACM Press, 2002. [39] The PROSE Homepage. http://prose.ethz.ch/Wiki.jsp. [40] H. Rajan \nand K. Sullivan. Eos: instance-level aspects for integrated system design. In ESEC/FSE-11: Proceedings \nof the 9th European software engineering conference held jointly with 11th ACM SIGSOFT international \nsymposium on Foundations of software engineering, pages 297 306, New York, NY, USA, 2003. ACM Press. \n[41] K. Sakurai, H. Masuhara, N. Ubayashi, S. Matsuura, and S. Komiya. Association aspects. In AOSD, \npages 16 25, 2004. [42] SPEC JVM98 homepage. http://www.spec.org/osg/jvm98/. [43] The Steamloom Homepage. \nhttp://www.st.informatik. tu-darmstadt.de/static/pages/projects/AORTA/Steamloom. jsp. [44] T. Suganuma, \nT. Yasue, M. Kawahito, H. Komatsu, and T. Nakatani. A dynamic optimization framework for a Java just-in-time \ncompiler. ACM SIGPLAN Notices, 36(11):180 195, Nov. 2001. In Conference on Object-Oriented Programming, \nSystems, Languages and Applica\u00adtions (OOPSLA). [45] T. Suganuma, T. Yasue, and T. Nakatani. An empirical \nstudy of method in-lining for a Java just-in-time compiler. In Java Virtual Machine Research and Technology \nSymposium (JVM), pages 91 104, Aug. 2002. [46] D. Suv\u00b4ee, W. Vanderperren, and V. Jonckers. JAsCo: an \nAspect-Oriented Approach Tailored for Component Based Software Devel\u00adopment. In Proc. AOSD 2003, pages \n21 29, 2003. [47] W. Vanderperren and D. Suvee. Optimizing jasco dynamic aop through hotswap and jutta. \nIn Proceedings of the 2004 Dynamic Aspects Workshop, 2004. [48] W. Vanderperren, D. Suv\u00b4ee, M. A. Cibr\u00b4an, \nand B. D. Fraine. Stateful Aspects in JAsCo. http://ssel.vub.ac.be/jasco/media/ sc2005.pdf. [49] W. Vanderperren, \nD. Suv\u00b4ee, B. Verheecke, M. A. Cibr\u00b4an, and V. Jonckers. Adaptive programming in jasco. In AOSD 05: Proceedings \nof the 4th international conference on Aspect-oriented software development, pages 75 86, New York, NY, \nUSA, 2005. ACM Press. [50] A. Vasseur. Dynamic AOP and Runtime Weaving for Java -How does AspectWerkz \nAddress It? http://aspectwerkz.codehaus.org/ downloads/papers/aosd2004-daw-aspectwerkz.pdf, 2004. [51] \nA. Vasseur, J. Bon\u00b4er, and J. Dahlstedt. JRockit JVM Support for AOP. http://dev2dev.bea.com/pub/a/2005/08/jvm_aop_1.html. \n[52] Xalan-Java version 2.6.0. http://xml.apache.org/xalan-j/.  \n\t\t\t", "proc_id": "1167473", "abstract": "Current approaches to compiling aspect-oriented programs are inefficient. This inefficiency has negative effects on the productivity of the development process and is especially prohibitive for dynamic aspect deployment. In this work, we present how well-known virtual machine techniques can be used with only slight modifications to support fast aspect deployment while retaining runtime performance. Our implementation accelerates dynamic aspect deployment by several orders of magnitude relative to mainstream aspect-oriented environments. We also provide a detailed comparison of alternative implementations of execution environments with support for dynamic aspect deployment.", "authors": [{"name": "Christoph Bockisch", "author_profile_id": "81100623891", "affiliation": "Darmstadt University of Technology", "person_id": "PP14214980", "email_address": "", "orcid_id": ""}, {"name": "Matthew Arnold", "author_profile_id": "81100021720", "affiliation": "IBM T.J. Watson Research Center", "person_id": "PP14020675", "email_address": "", "orcid_id": ""}, {"name": "Tom Dinkelaker", "author_profile_id": "81100381772", "affiliation": "Darmstadt University of Technology", "person_id": "P728887", "email_address": "", "orcid_id": ""}, {"name": "Mira Mezini", "author_profile_id": "81100583946", "affiliation": "Darmstadt University of Technology", "person_id": "P201627", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1167473.1167483", "year": "2006", "article_id": "1167483", "conference": "OOPSLA", "title": "Adapting virtual machine techniques for seamless aspect support", "url": "http://dl.acm.org/citation.cfm?id=1167483"}