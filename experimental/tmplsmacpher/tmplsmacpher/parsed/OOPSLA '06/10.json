{"article_publication_date": "10-16-2006", "fulltext": "\n The DaCapo Benchmarks: Java Benchmarking Development and Analysis * Stephen M Blackburna\u00df, Robin Garner\u00df, \nChris Hoffmann., Asjad M Khan., Kathryn S McKinleyd , Rotem Bentzure, Amer Diwan., Daniel Feinberge, \nDaniel Frampton\u00df, Samuel Z Guyer., Martin Hirzel. , e Antony Hosking., Maria Jumpd, Han Leea, J Eliot \nB Moss., Aashish Phansalkard, Darko Stefanovi\u00b4c, Thomas VanDrunen., Daniel von Dincklage., Ben Wiedermannd \naIntel, \u00dfAustralian National University, .University of Massachusetts at Amherst, dUniversity of Texas \nat Austin, eUniversity of New Mexico, .University of Colorado, .Tufts, .IBM TJ Watson Research Center, \n.Purdue University, .Wheaton College Abstract Since benchmarks drive computer science research and industry \nproduct development, which ones we use and how we evaluate them are key questions for the community. \nDespite complex run\u00adtime tradeoffs due to dynamic compilation and garbage collection required for Java \nprograms, many evaluations still use methodolo\u00adgies developed for C, C++, and Fortran. SPEC, the dominant \npur\u00adveyor of benchmarks, compounded this problem by institutionaliz\u00ading these methodologies for their \nJava benchmark suite. This paper recommends benchmarking selection and evaluation methodolo\u00adgies, and \nintroduces the DaCapo benchmarks, a set of open source, client-side Java benchmarks. We demonstrate that \nthe complex in\u00adteractions of (1) architecture, (2) compiler, (3) virtual machine, (4) memory management, \nand (5) application require more extensive evaluation than C, C++, and Fortran which stress (4) much \nless, and do not require (3). We use and introduce new value, time-series, and statistical metrics for \nstatic and dynamic properties such as code complexity, code size, heap composition, and pointer muta\u00adtions. \nNo benchmark suite is de.nitive, but these metrics show that DaCapo improves over SPEC Java in a variety \nof ways, including more complex code, richer object behaviors, and more demanding memory system requirements. \nThis paper takes a step towards im\u00adproving methodologies for choosing and evaluating benchmarks to foster \ninnovation in system design and implementation for Java and other managed languages. Categories and Subject \nDescriptors C.4 [Measurement Techniques] General Terms Measurement, Performance Keywords methodology, \nbenchmark, DaCapo, Java, SPEC 1. Introduction When researchers explore new system features and optimizations, \nthey typically evaluate them with benchmarks. If the idea does not improve a set of interesting benchmarks, \nresearchers are unlikely to submit the idea for publication, or if they do, the community is unlikely \nto accept it. Thus, benchmarks set standards for innovation and can encourage or sti.e it. * This work \nis supported by NSF ITR CCR-0085792, NSF CCR-0311829, NSF CCF-CCR-0311829, NSF CISE infrastructure grant \nEIA-0303609, ARC DP0452011, DARPA F33615-03-C-4106, DARPA NBCH30390004, IBM, and Intel. Any opinions, \n.ndings and conclusions expressed herein are those of the authors and do not necessarily re.ect those \nof the sponsors. Permission to make digital or hard copies of all or part of this work for personal or \nclassroom use is granted without fee provided that copies are not made or distributed for pro.t or commercial \nadvantage and that copies bear this notice and the full citation on the .rst page. To copy otherwise, \nto republish, to post on servers or to redistribute to lists, requires prior speci.c permission and/or \na fee. OOPSLA 06 October 22 26, 2006, Portland, Oregon, USA. Copyright c . 2006 ACM 1-59593-348-4/06/0010. \n. . $5.00. For Java, industry and academia typically use the SPEC Java benchmarks (the SPECjvm98 benchmarks \nand SPECjbb2000 [37, 38]). When SPEC introduced these benchmarks, their evaluation rules and the community \ns evaluation metrics glossed over some of the key questions for Java benchmarking. For example, (1) SPEC \nreporting of the best execution time is taken from multiple it\u00aderations of the benchmark within a single \nexecution of the virtual machine, which will typically eliminate compile time. (2) In ad\u00addition to steady \nstate application performance, a key question for Java virtual machines (JVMs) is the tradeoff between \ncompile and application time, yet SPEC does not require this metric, and the community often does not \nreport it. (3) SPEC does not require re\u00adports on multiple heap sizes and thus does not explore the space\u00adtime \ntradeoff automatic memory management (garbage collection) must make. SPEC speci.es three possible heap \nsizes, all of which over-provision the heap. Some researchers and industry evaluations of course do vary \nand report these metrics, but many do not. This paper introduces the DaCapo benchmarks, a set of gen\u00aderal \npurpose, realistic, freely available Java applications. This pa\u00adper also recommends a number of methodologies \nfor choosing and evaluating Java benchmarks, virtual machines, and their memory management systems. Some \nof these methodologies are already in use. For example, Eeckhout et al. recommend that hardware ven\u00addors \nuse multiple JVMs for benchmarking because applications vary signi.cantly based on JVM [19]. We recommend \nand use this methodology on three commercial JVMs, con.rming none is a con\u00adsistent winner and benchmark \nvariation is large. We recommend here a deterministic methodology for evaluating compiler optimiza\u00adtions \nthat holds the compiler workload constant, as well as the stan\u00addard steady-state stable performance methodology. \nFor evaluating garbage collectors, we recommend multiple heap sizes and deter\u00administic compiler con.gurations. \nWe also suggest new and previ\u00adous methodologies for selecting benchmarks and comparing them. For example, \nwe recommend time-series data versus single values, including heap composition and pointer distances \nfor live objects as well as allocated objects. We also recommend principal component analysis [13, 18, \n19] to assess differences between benchmarks. We use these methodologies to evaluate and compare DaCapo \nand SPEC, .nding that DaCapo is more complex in terms of static and dynamic metrics. For example, DaCapo \nbenchmarks have much richer code complexity, class structures, and class hierarchies than SPEC according \nto the Chidamber and Kemerer metrics [12]. Furthermore, this static complexity produces a wider variety \nand more complex object behavior at runtime, as measured by data structure complexity, pointer source/target \nheap distances, live and allocated object characteristics, and heap composition. Principal component \nanalysis using code, object, and architecture behavior metrics differentiates all the benchmarks from \neach other. The main contributions of this paper are new, more realistic Java benchmarks, an evaluation \nmethodology for developing benchmark suites, and performance evaluation methodologies. Needless to say, \nthe DaCapo benchmarks are not de.nitive, and they may or may not be representative of workloads that \nvendors and clients care about most. Regardless, we believe this paper is a step towards a wider community \ndiscussion and eventual consensus on how to select, measure, and evaluate benchmarks, VMs, compilers, \nruntimes, and hardware for Java and other managed languages. 2. Related Work We build on prior methodologies \nand metrics, and go further to recommend how to use them to select benchmarks and for best practices \nin performance evaluation. 2.1 Java Benchmark Suites In addition to SPEC (discussed in Section 3), prior \nJava bench\u00admarks suites include Java Grande [26], Jolden [11, 34], and Ashes [17]. The Java Grande Benchmarks \ninclude programs with large demands for memory, bandwidth, or processing power [26]. They focus on array \nintensive programs that solve scienti.c com\u00adputing problems. The programs are sequential, parallel, and \ndis\u00adtributed. They also include microbenchmark tests for language and communication features, and some \ncross-language tests for com\u00adparing C and Java. DaCapo also focuses on large, realistic pro\u00adgrams, but \nnot on parallel or distributed programs. The DaCapo benchmarks are more general purpose, and include \nboth client and server side applications. The Jolden benchmarks are single-threaded Java programs rewritten \nfrom parallel C programs that use dynamic pointer data structures [11, 34]. These programs are small \nkernels (less than 600 lines of code) intended to explore pointer analysis and paralleliza\u00adtion, not \ncomplete systems. The Soot project distributes the Ashes benchmarks with their Java compiler infrastructure, \nand include the Jolden benchmarks, a few more realistic benchmarks such as their compiler, and some interactive \nbenchmarks [17]. The DaCapo benchmarks contain many more realistic programs, and are more ambitious in \nscope. 2.2 Benchmark Metrics and Characterization Dufour et al. recommend characterizing benchmarks \nwith architec\u00adture independent value metrics that summarize: (1) size and struc\u00adture of program, (2) \ndata structures, (3) polymorphism, (4) memory, and (5) concurrency into a single number [17]. We do not \nconsider concurrency metrics to limit the scope of our efforts. We use met\u00adrics from the .rst four categories \nand add metrics, such as .lter\u00ading for just the live objects, that better expose application behavior. \nOur focus is on continuous metrics, such as pointer distributions and heap composition graphs, rather \nthan single values. Dufour et al. show how to use these metrics to drive compiler optimization explorations, \nwhereas we show how to use these metrics to develop methodologies for performance and benchmark evaluation. \nPrior work studied some of the object properties we present here [4, 15, 21, 39], but not for the purposes \nof driving benchmark selection and evaluation methodologies. For example, Dieckmann and H\u00a8olzle [15] \nmeasure object allocation properties, and we add to their analysis live object properties and pointer \ndemographics. Stefanovi\u00b4c pioneered the use of heap composition graphs which we use here to show inherent \nobject lifetime behaviors [39].  2.3 Performance Evaluation Methodologies Eeckhout et al. study SPECjvm98 \nand other Java benchmarks using a number of virtual machines on one architecture, AMD s K7 [19]. Their \ncluster analysis shows that methodologies for designing new hardware should include multiple virtual \nmachines and benchmarks because each widely exercises different hardware aspects. One limitation of their \nwork is that they use a .xed heap size, which as we show masks the interaction of the memory manager \ns space\u00adtime tradeoff in addition to its in.uence on mutator locality. We add to Eeckhout et al. s good \npractices in methodology that the hardware designers should include multiple heap sizes and memory management \nstrategies. We con.rm Eeckhout et al. s .nding. We present results for three commercial JVMs on one architecture \nthat show a wide range of performance sensitivities. No one JVM is best across the suite with respect \nto compilation time and code quality, and there is a lot a variation. These results indicate there is \nplenty of room for improving current commercial JVMs. Many recent studies examine and characterize the \nbehavior of Java programs in simulation or on hardware [19, 21, 23, 28, 29, 30, 31, 32, 33]. This work \nfocuses on workload characterization, application behavior on hardware, and key differences with C pro\u00adgrams. \nFor example, Hauswirth et al. mine application behavior to understand performance [21]. The bulk of our \nevaluation fo\u00adcuses on benchmark properties that are independent of any particu\u00adlar hardware or virtual \nmachine implementation, whereas this prior work concentrates on how applications behave on certain hardware \nwith one or more virtual machines. We extend these results to sug\u00adgest that these characteristics can \nbe used to separate and evaluate the benchmarks in addition to the software and hardware running them. \nMuch of this Java performance analysis work either disables garbage collection [15, 35], which introduces \nunnecessary mem\u00adory fragmentation, or holds the heap size and/or garbage collector constant [19, 28], \nwhich may hide locality effects. A number of researchers examine garbage collection and its in\u00ad.uence \non application performance [3, 4, 20, 22, 28, 40]. For ex\u00adample, Kim and Hsu use multiple heap sizes \nand simulate different memory hierarchies with a whole heap mark-sweep algorithm, as\u00adsisted by occasional \ncompaction [28]. Kim and Hsu, and Rajan et al. [33] note that a mark-sweep collector has a higher miss \nrate than the application itself because the collector touches reachable data that may not be in the \nprogram s current working set. Blackburn et al. use the methodology we recommend here for studying the \nin.uence of copying, mark-sweep, and reference counting collec\u00adtors, and their generational variants \non three architectures [4]. They show a contiguously allocating generational copying collector de\u00adlivers \nbetter mutator cache performance and total performance than a whole-heap mark-sweep collector with a \nfree-list. A few studies explore heap size effects on performance [9, 10, 28], and as we show here, garbage \ncollectors are very sensitive to heap size, and in particular to tight heaps. Diwan et al. [16, 41], \nHicks et al. [22], and others [7, 8, 24] measure detailed, speci.c mechanism costs and architecture in.uences \n[16], but do not consider a variety of collection algorithms. Our work re.ects these results and method\u00adologies, \nbut makes additional recommendations.  3. Benchmark and Methodology Introduction This section describes \nSPEC Java and SPEC execution rules, how we collected DaCapo benchmarks, and our execution harness. 3.1 \nSPEC Java Benchmarks. We compare the DaCapo suite to SPECjvm98 [37] and a modi.ed version of SPECjbb2000 \n[38], and call them the SPEC Java bench\u00admarks, or SPEC for short. We exclude SPECjAppServer because it \nrequires multiple pieces of hardware and software to execute. The original SPECjbb2000 is a server-side \nJava application and reports its score as work done over a .xed time rather than elapsed time for a .xed \nwork load. Although throughput (measuring work done over a .xed time) is one important criteria for understanding \napplica\u00adtions such as transaction processing systems, most applications are not throughput oriented. \nSuper.cially, the difference between .x\u00ading the time and workload is minor, however a variable workload \nis methodologically problematic. First, throughput workloads force a repetitive loop into the benchmark, \nwhich in.uences JIT optimiza\u00adtion strategies and opportunities for parallelism, but is not represen\u00adtative \nof the wide range of non-repetitive workloads. Furthermore, variable workloads make performance hard \nto analyze and reason about. For example, the level and number of classes optimized and re-optimized \nat higher levels and the number of garbage collec\u00adtions vary with the workload, leading to complex cascading \neffects on overall performance. We therefore modify SPECjbb2000, creat\u00ading pseudojbb, which executes \na .xed workload (by default, 70,000 transactions execute against a single warehouse). SPEC benchmarking \nrules discourage special casing the vir\u00adtual machine, compiler, and/or architecture for a speci.c SPEC \nJava benchmark. They specify the largest input size (100), se\u00adquencing through the benchmarks, no harness \ncaching, and no pre\u00adcompilation of classes. The SPECjvm98 harness runs all the bench\u00admarks multiple times, \nand intersperses untimed and timed execu\u00adtions. Benchmarkers may run all the programs as many times as \nthey like, and then report the best and worst results using the same virtual machine and compiler con.gurations. \nSPEC indicates that reporting should specify the memory sizes: 48MB, 48 256MB, and greater than 256MB, \nbut does not require reporting all three. All these sizes over provision the heap. Excluding the virtual \nmachine, SPEC programs allocate up to 271MB, and have at most 8MB live in the heap at any time, except \nfor pseudojbb with 21MB live (see Section 7). Since 2000, none of the vendors has published results for \nthe smaller heaps. The SPEC committee is currently working on collecting a new set of Java benchmarks. \nThe SPEC committee consists of industrial representatives and a few academics. One of their main criteria \nis representativeness, which industry is much better to judge than academia. When SPEC releases new benchmark \nsets, they include a performance comparison point. They do not include or describe any measured metrics \non which they based their selection. This paper suggests methodologies for both selecting and evaluating \nJava Benchmarks, which are not being used or recommended in current industrial standards, SPEC or otherwise. \n 3.2 DaCapo Benchmarks We began the DaCapo benchmarking effort in mid 2003 as the re\u00adsult of an NSF \nreview panel in which the panel and the DaCapo research group agreed that the existing Java benchmarks \nwere lim\u00aditing our progress. What followed was a two-pronged effort to iden\u00adtify suitable benchmarks, \nand develop a suite of analyses to char\u00adacterize candidate benchmarks and evaluate them for inclusion. \nWe began with the following criteria. 1. Diverse real applications. We want applications that are widely \nused to provide a compelling focus for the community s innova\u00adtion and optimizations, as compared to \nsynthetic benchmarks. 2. Ease of use. We want the applications to be relatively easy to use and measure. \n We implemented these criteria as follows. 1. We chose only open source benchmarks and libraries. 2. \nWe chose diverse programs to maximize coverage of application domains and application behaviors. 3. \nWe focused on client-side benchmarks that are easy to measure in a completely standard way, with minimal \ndependences out\u00adside the scope of the host JVM. 4. We excluded GUI applications since they are dif.cult \nto bench\u00admark systematically. In the case of eclipse, we exercise a non-GUI subset. 5. We provide a \nrange of inputs. With the default input sizes, the programs are timely enough that it takes hours or \ndays to execute thousands of invocations of the suite, rather than weeks. With the exception of eclipse, \nwhich runs for around a minute, each benchmark executes for between 5 and 20 seconds on contemporary \nhardware and JVMs.  We considered other potential criteria, such as long running, GUI, and client-server \napplications. We settled on the above character\u00adistics because their focus is similar to the existing \nSPEC bench\u00admarks, while addressing some of our key concerns. Around 20 stu\u00addents and faculty at six institutions \nthen began an iterative process of identifying, preparing, and experimenting with candidate bench\u00admarks. \nRealizing the dif.culty of identifying a good benchmark suite, we made the DaCapo benchmark project open \nand transpar\u00adent, inviting feedback from the community [14]. As part of this process, we have released \nthree beta versions. We identi.ed a broad range of static and dynamic metrics, including some new ones, \nand developed a framework in Jikes RVM [1] for performing these detailed analyses. Sections 6, 7, and \n8 describe these metrics. We systematically analyzed each can\u00addidate to identify ones with non-trivial \nbehavior and to maximize the suite s coverage. We included most of the benchmarks we eval\u00aduated, excluding \nonly a few that were too trivial or whose license agreements were too restrictive, and one that extensively \nused ex\u00adceptions to avoid explicit control .ow. The Constituent Benchmarks We now brie.y describe each \nbenchmark in the .nal pre-release of the suite (beta-2006-08) that we use throughout the paper. More \ndetailed descriptions appear in Figures 4 through 14. The source code and the benchmark harness are available \non the DaCapo benchmark web site [14]. antlr A parser generator and translator generator. bloat A bytecode-level \noptimization and analysis tool for Java. chart A graph plotting toolkit and pdf renderer. eclipse An \nintegrated development environment (IDE). fop An output-independent print formatter. hsqldb An SQL relational \ndatabase engine written in Java. jython A python interpreter written in Java. luindex A text indexing \ntool. lusearch A text search tool. pmd A source code analyzer for Java. xalan An XSLT processor for transforming \nXML documents. The benchmark suite is packaged as a single jar .le containing a harness (licensed under \nthe Apache Public License [2]), all the benchmarks, the libraries they require, three input sizes, and \ninput data (e.g., luindex, lusearch and xalan all use the works of Shake\u00adspeare). We experimented with \ndifferent inputs and picked repre\u00adsentative ones. The Benchmark Harness We provide a harness to invoke \nthe benchmarks and perform a validity check that insures each bench\u00admark ran to completion correctly. \nThe validity check performs checksums on err and out streams during benchmark execution and on any generated \n.les after benchmark execution. The harness passes the benchmark if its checksums match pre-calculated \nvalues. The harness supports a range of options, including user-speci.ed hooks to call at the start \nand end of the benchmark and/or after the benchmark warm-up period, running multiple benchmarks, and \nprinting a brief summary of each benchmark and its origins. It also supports workload size (small, default, \nlarge), which iteration (.rst, second, or nth), or a performance-stable iteration for reporting ex\u00adecution \ntime. To .nd a performance-stable iteration, the harness takes a window size w (number of executions) \nand a convergence target v, and runs the benchmark repeatedly until either the coef\u00ad.cient of variation, \ns , of the last w runs drops below v, or reports \u00b5 failure if the number of runs exceeds a maximum m \n(where s is the standard deviation and \u00b5 is the arithmetic mean of the last w exe\u00adcution times). Once \nperformance stabilizes, the harness reports the execution time of the next iteration. The harness provides \ndefaults for w and v, which the user may override. (a) SPEC (b) DaCapo Figure 1. The Impact of Benchmarks \nand Architecture in Identifying Tradeoffs Between Collectors Figure 2. Gaming Your Results  4. Virtual \nMachine Experimental Methodologies We modi.ed Jikes RVM [1] version 2.4.4+ to report static met\u00adrics, \ndynamic metrics, and performance results. We use this virtual machine because it is open source and performs \nwell. Unless other\u00adwise speci.ed, we use a generational collector with a variable sized copying nursery \nand a mark-sweep older space because it is a high performance collector con.guration [4], and consequently \nis popu\u00adlar in commercial virtual machines. We call this collector GenMS. Our research group and others \nhave used and recommended the fol\u00adlowing methodologies to understand virtual machines, application behaviors, \nand their interactions. Mix. The mix methodology measures an iteration of an applica\u00adtion which mixes \nJIT compilation and recompilation work with application time. This measurement shows the tradeoff of \ntotal time with compilation and application execution time. SPEC s worst number may often, but is not \nguaranteed to, correspond to this measurement. Stable. To measure stable application performance, researchers \ntypically report a steady-state run in which there is no JIT com\u00adpilation and only the application and \nmemory management sys\u00adtem are executing. This measurement corresponds to the SPEC best performance number. \nIt measures .nal code quality, but does not guarantee the compiler behaved deterministically. Deterministic \nStable &#38; Mix. This methodology eliminates sam\u00adpling and recompilation as a source of non-determinism. \nIt modi.es the JIT compiler to perform replay compilation which applies a .xed compilation plan when \nit .rst compiles each method [25]. We .rst modify the compiler to record its sam\u00adpling information and \noptimization decisions for each method, execute the benchmark n times, and select the best plan. We then \nexecute the benchmark with the plan, measuring the .rst iteration (mix) or the second (stable) depending \non the experi\u00adment. The deterministic methodology produces a code base with opti\u00admized hot methods and \nbaseline compiled code. Compiling all the methods at the highest level with static pro.le information \nor us\u00ading all baseline code is also deterministic, but it does not provide a realistic code base. These \nmethodologies provide compiler and memory management researchers a way to control the virtual ma\u00adchine \nand compiler, holding parts of the system constant to tease apart the in.uence of proposed improvements. \nWe highly recom\u00admend these methodologies together with a clear speci.cation and justi.cation of which \nmethodology is appropriate and why. In Sec\u00adtion 5.1, we provide an example evaluation that uses the determin\u00adistic \nstable methodology which is appropriate because we compare garbage collection algorithms across a range \nof architectures and heap sizes, and thus want to minimize variation due to sampling and JIT compilation. \n 5. Benchmarking Methodology This section argues for using multiple architectures and multiple heap \nsizes related to each program s maximum live size to evaluate the performance of Java, memory management, \nand its virtual ma\u00adchines. In particular, we show a set of results in which the effects of locality and \nspace ef.ciency trade off, and therefore heap size and architecture choice substantially affect quantitative \nand quali\u00adtative conclusions. The point of this section is to demonstrate that because of the variety \nof implementation issues that Java programs encompass, measurements are sensitive to benchmarks, the \nunder\u00adlying architecture, the choice of heap size, and the virtual machine. Thus presenting results without \nvarying these parameters is at best unhelpful, and at worst, misleading. 5.1 How not to cook your books. \n This experiment explores the space-time tradeoff of two full heap collectors: SemiSpace and MarkSweep \nas implemented in Jikes RVM [1] with MMTk [4, 5] across four architectures. We exper\u00adimentally determine \nthe minimum heap size for each program us\u00ading MarkCompact in MMTk. (These heap sizes, which are speci.c \nto MMTk and Jikes RVM version 2.4.4+, can be seen in the top x-axes of Figures 2(a) (d).) The virtual \nmachine triggers collec\u00adtion when the application exhausts the heap space. The SemiSpace First iteration \nSecond iteration Third iteration Benchmark A B/A C/A Best A B/A C/A Best 2nd /1st A B/A C/A Best 3rd \n/1st SPEC 201 compress 7.2 0.76 0.75 0.75 3.3 1.51 1.65 1.00 0.75 3.2 1.02 1.69 1.00 0.65 202 jess 3.2 \n0.80 0.58 0.58 2.0 1.27 0.81 0.81 0.82 2.0 0.63 0.79 0.63 0.65 205 raytrace 2.6 0.86 0.54 0.54 1.5 0.99 \n0.82 0.82 0.66 1.5 0.64 0.82 0.64 0.58 209 db 8.0 1.07 1.00 1.00 6.9 1.14 1.14 1.00 0.91 6.8 1.05 1.14 \n1.00 0.88 213 javac 6.0 0.48 0.69 0.48 2.5 0.93 1.34 0.93 0.62 2.2 1.06 1.50 1.00 0.60 222 mpegaudio \n3.8 1.73 1.22 1.00 2.8 1.11 1.65 1.00 0.69 2.7 1.10 1.64 1.00 0.67 227 mtrt 2.9 0.72 0.51 0.51 1.4 1.53 \n0.94 0.94 0.74 1.5 0.61 0.81 0.61 0.56 228 jack 5.7 1.08 0.61 0.61 3.1 1.27 1.05 1.00 0.66 3.0 1.21 1.08 \n1.00 0.64 geomean 0.94 0.74 0.68 1.22 1.18 0.94 0.73 0.91 1.18 0.86 0.65 DaCapo antlr 6.0 0.53 0.68 0.53 \n3.4 0.69 0.94 0.69 0.66 3.2 0.69 0.97 0.69 0.64 bloat 12.0 0.98 1.03 0.98 9.7 1.28 1.20 1.00 0.93 9.1 \n1.24 1.28 1.00 0.88 chart 12.2 0.97 1.47 0.97 9.5 1.30 1.68 1.00 0.90 9.2 0.73 1.71 0.73 0.75 eclipse \n61.7 1.28 0.96 0.96 39.4 1.60 1.17 1.00 0.74 23.8 1.60 1.94 1.00 0.54 fop 7.1 0.40 0.40 0.40 4.8 0.33 \n0.36 0.33 0.63 5.1 0.31 0.35 0.31 0.66 hsqldb 12.0 0.82 0.47 0.47 7.7 0.67 0.66 0.66 0.66 7.3 0.86 0.68 \n0.68 0.67 luindex 15.5 1.04 0.94 0.94 9.8 1.41 1.41 1.00 0.80 9.1 1.55 1.49 1.00 0.79 lusearch 13.1 0.74 \n0.90 0.74 10.6 0.92 1.06 0.92 0.91 10.5 1.56 1.07 1.00 1.10 jython 16.5 0.52 0.68 0.52 8.3 0.92 2.87 \n0.92 1.09 7.9 0.92 0.83 0.83 0.59 pmd 10.4 1.04 0.95 0.95 7.5 1.50 1.15 1.00 0.89 6.9 1.18 1.27 1.00 \n0.76 xalan 8.3 0.87 0.90 0.87 5.3 1.53 1.20 1.00 0.86 5.0 1.24 1.26 1.00 0.76 geomean 0.84 0.85 0.76 \n1.10 1.25 0.86 0.83 1.08 1.17 0.84 0.74 Table 1. Cross JVM Comparisons collector must keep in reserve \nhalf the heap space to ensure that if the remaining half of the heap is all live, it can copy into it. \nThe MarkSweep collector uses segregated .ts free-lists. It collects when there is no element of the appropriate \nsize, and no completely free block that can be sized appropriately. Because it is more space ef.\u00adcient, \nit collects less often than the SemiSpace collector. However, SemiSpace s contiguous allocation offers \nbetter locality to contem\u00adporaneously allocated and used objects than MarkSweep. Figure 1 shows how this \ntradeoff plays out in practice for SPEC and DaCapo. Each graph normalizes performance as a function of \nheap size, with the heap size varying from 1 to 6 times the mini\u00admum heap in which the benchmark can \nrun. Each line shows the geometric mean of normalized performance using either a Mark-Sweep (MS) or SemiSpace \n(SS) garbage collector, executing on one of four architectures. Each line includes a symbol for the ar\u00adchitecture, \nun.lled for SS and .lled for MS. The .rst thing to note is that the MS and SS lines converge and cross \nover at large heap sizes, illustrating the point at which the locality/space ef.ciency break-even occurs. \nNote that the choice of architecture and bench\u00admark suite impacts this point. For SPEC on a 1.6GHz PPC, \nthe tradeoff is at 3.3 times the minimum heap size, while for DaCapo on a 3.0GHz Pentium 4, the tradeoff \nis at 5.5 times the minimum heap size. Note that while the 2.2GHz AMD and the 2.0 GHz Pen\u00adtium M are \nvery close on SPEC, the Pentium M is signi.cantly faster in smaller heaps on DaCapo. Since different \narchitectures have different strengths, it is important to have a good coverage in the benchmark suite \nand a variety of architectures. Such results can paint a rich picture, but depend on running a large \nnumber of benchmarks over many heap sizes across multiple architectures. The problem of using a subset \nof a benchmark suite, using a single architecture, or choosing few heap sizes is further illustrated \nin Figure 2. Figures 2(a) and (b) show how careful benchmark selection can tell whichever story you choose, \nwith 209 db and hsqldb performing best under the opposite collection regimens. Figures 2(c) and (d) show \nthat careful choice of architecture can paint a quite different picture. Figures 2(c) and (d) are typical \nof many benchmarks. The most obvious point to draw from all of these graphs is exploring multiple heap \nsizes should be required for Java and should start at the minimum in which the program can execute with \na well performing collector. Eliminating the left hand side in either of these graphs would lead to an \nentirely different interpretation of the data. This example shows the tradeoff between space ef.ciency \nand locality due to the garbage collector, and similar issues arise with almost any performance analysis \nof Java programs. For example, a compiler optimization to improve locality would clearly need a similarly \nthorough evaluation [4]. 5.2 Java Virtual Machine Impact This section explores the sensitivity of performance \nresults to JVMs. Table 1 presents execution times and differences for three leading commercial Java 1.5 \nJVMs running one, two, and three iterations of the SPEC Java and DaCapo benchmarks. We have made the \nJVMs anonymous ( A , B , &#38; C ) in deference to li\u00adcense agreements and because JVM identity is not \npertinent to the point. We use a 2GHz Intel Pentium M with 1GB of RAM and a 2MB L2 cache, and in each \ncase we run the JVM out of the box , with no special command line settings. Columns 2 4 show performance \nfor a single iteration, which will usually be most im\u00adpacted by compilation time. Columns 6 8 present \nperformance for a second iteration. Columns 11 13 show performance for a third iteration. The .rst to \nthe third iteration presumably includes pro\u00adgressively less compilation time and more application time. \nThe Speedup columns 10 and 15 show the average percentage speedup seen in the second and third iterations \nof the benchmark, relative to the .rst. Columns 2, 6 and 11 report the execution time for JVM A to which \nwe normalize the remaining execution results for JVMs B and C. The Best column reports the best normalized \ntime from all three JVMs, with the best time appearing in bold in each case. One interesting result is \nthat no JVM is uniformly best on all con.gurations. The results show there is a lot of room for over\u00adall \nJVM improvements. For DaCapo, potential improvements range from 14% (second iteration) to 25% (.rst iteration). \nEven for SPEC potential improvements range from 6% (second iteration) to 32% (.rst iteration). If a single \nJVM could achieve the best performance on DaCapo across the benchmarks, it would improve performance \nby a geometric mean of 24% on the .rst iteration, 14% on the second iteration, and 16% on the third interaction \n(the geomean row). Among the notable results are that JVM C slows down sig\u00adni.cantly for the second iteration \nof jython, and then performs best on the third iteration, a result we attribute to aggressive hotspot \ncompilation during the second iteration. The eclipse benchmark appears to take a long time to warm up, \nimproving considerably in both the second and third iterations. On average, SPEC bench\u00admarks speed up \nmuch more quickly than the DaCapo benchmarks, which is likely a re.ection on their smaller size and simplicity. \nWe demonstrate this point quantitatively in the next section. These results reinforce the importance \nof good methodology and the choice of benchmark suite, since we can draw dramatically divergent conclusions \nby simply selecting a particular iteration, virtual machine, heap size, architecture, or benchmark. \n 6. Code Complexity and Size This section shows static and dynamic software complexity met\u00adrics which \nare architecture and virtual machine independent. We present Chidamber and Kemerer s software complexity \nmet\u00adrics [12] and a number of virtual machine and architecture in\u00addependent dynamic metrics, such as, \nclasses loaded and byte\u00adcodes compiled. Finally, we present a few virtual machine depen\u00addent measures \nof dynamic behavior, such as, methods/bytecodes the compiler detects as frequently executed (hot), and \ninstruc\u00adtion cache misses. Although we measure these features with Jikes RVM, Eeckhout et al. [19] show \nthat for SPEC, virtual machines fairly consistently identify the same hot regions. Since the DaCapo benchmarks \nare more complex than SPEC, this trend may not hold as well for them, but we believe these metrics are \nnot overly in.u\u00adenced by our virtual machine. DaCapo and SPEC differ quite a bit; DaCapo programs are \nmore complex, object-oriented, and exercise the instruction cache more. 6.1 Code Complexity To measure \nthe complexity of the benchmark code, we use the Chi\u00addamber and Kemerer object-oriented programming (CK) \nmetrics [12] measured with the ckjm software package [36]. We apply the CK metrics to classes that the \napplication actually loads during exe\u00adcution. We exclude standard libraries from this analysis as they \nare heavily duplicated across the benchmarks (column two of Table 3 includes all loaded classes). The \naverage DaCapo program loads more than twice as many classes during execution as SPEC. The following \nexplains what the CK metrics reveal and the results for SPEC and DaCapo. WMC Weighted methods per class. \nSince ckjm uses a weight of 1, WMC is simply the total number of declared methods for the loaded classes. \nLarger numbers show that a program provides more behaviors, and we see SPEC has substantially lower WMC \nvalues than DaCapo, except for 213 javac, which as the table shows is the richest of the SPEC benchmarks \nand usually falls in the middle or top of the DaCapo s program range of software complexity. Unsurprisingly, \nfewer methods are declared (WMC in Table 2) than compiled (Table 3), but this difference is only dramatic \nfor eclipse. DIT Depth of Inheritance Tree. DIT provides for each class a measure of the inheritance \nlevels from the object hierarchy top. In Java where all classes inherit Object the minimum value of DIT \nis 1. Except for 213 javac and 202 jess, DaCapo programs typically have deeper inheritance trees. NOC \nNumber of Children. NOC is the number of immediate subclasses of the class. Table 2 shows that in SPEC, \nonly 213 javac has any interesting behavior, but hsqldb, luindex, lusearch and pmd in DaCapo also have \nno superclass structure. CBO Coupling between object classes. CBO represents the num\u00adber of classes coupled \nto a given class (efferent couplings). Method calls, .eld accesses, inheritance, arguments, return types, \nand exceptions all couple classes. The interactions be\u00adtween objects and classes is substantially more \ncomplex for Benchmark WMC DIT NOC CBO RFC LCOM SPEC 201 compress 154 19 0 55 426 780 202 jess 614 97 \n1 632 1846 2032 205 raytrace 330 33 3 117 743 1046 209 db 152 12 0 42 454 789 213 javac 1011 186 38 1175 \n3293 3753 222 mpegaudio 367 40 0 167 796 1350 227 mtrt 332 33 3 117 743 1046 228 jack 375 46 0 163 860 \n6911 pseudojbb 541 35 0 254 1419 2487 min max 152 1011 12 186 0 38 42 1175 426 3293 780 6911 geomean \n366 40 2 176 950 1701 DaCapo antlr 1253 84 8 674 3094 8444 bloat 2265 206 21 1661 6232 6521 chart 1627 \n101 16 648 3979 29169 eclipse 10763 830 164 7277 26209 218199 fop 1433 148 17 998 3867 13041 hsqldb 2419 \n73 3 766 4676 47371 jython 3023 225 60 1398 5725 97111 luindex 494 50 0 246 1372 2260 lusearch 618 55 \n0 297 1441 3419 pmd 2348 215 4 1199 5384 126893 xalan 2433 161 24 971 5682 37394 min max 494 10763 50 \n830 0 164 246 7277 1372 26209 2260 218199 geomean 1857 138 10 935 4420 22561 WMC Weighted methods/class \nCBO Object class coupling DIT Depth Inheritance Tree RFC Response for a Class NOC Number of Children \nLCOM Lack of method cohesion Table 2. CK Metrics for Loaded Classes (Excluding Libraries) DaCapo compared \nto SPEC. However, both 202 jess and 213 javac have relatively high CBO values. RFC Response for a Class. \nRFC measures the number of different methods that may execute when a method is invoked. Ideally, we would \n.nd for each method of the class, the methods that class will call, and repeat for each called method, \ncalculating the transitive closure of the method s call graph. Ckjm calcu\u00adlates a rough approximation \nto the response set by inspecting method calls within the class s method bodies. The RFC metric for DaCapo \nshows a factor of around .ve increase in complex\u00adity over SPEC. LCOM Lack of cohesion in methods. LCOM \ncounts methods in a class that are not related through the sharing of some of the class s .elds. The \noriginal de.nition of this metric (used in ckjm) considers all pairs of a class s methods, subtracting \nthe number of method pairs that share a .eld access from the number of method pairs that do not. Again, \nDaCapo is more complex, e.g., eclipse and pmd have LCOM metrics at least two orders of magnitude higher \nthan any SPEC benchmark. In summary, the CK metrics show that SPEC programs are not very object-oriented \nin absolute terms, and that the DaCapo benchmarks are signi.cantly richer and more complex than SPEC. \nFurthermore, DaCapo benchmarks extensively use object-oriented features to manage their complexity. \n6.2 Code Size and Instruction Cache Performance This section presents program size metrics. Column 2 \nof Table 3 shows the total number of classes loaded during the execution of each benchmark, including \nstandard libraries. Column 3 shows the total number of declared methods in the loaded classes (compare \nto column 2 of Table 2, which excludes standard libraries). Columns 4 and 5 show the number of methods \ncompiled (executed at least Benchmark Classes Loaded Methods Declared Methods &#38; Bytecodes Compiled \nI-Cache Misses All Optimized % Hot L1 I-cache ITLB Methods BC KB Methods BC KB Methods BC /ms norm /ms \nnorm SPEC 201 compress 157 1118 254 23.9 16 3.7 6.3 15.5 69 0.08 4 0.07 202 jess 293 1777 655 42.4 46 \n4.3 7.0 10.1 383 0.45 31 0.56 205 raytrace 177 1316 381 32.4 44 9.0 11.5 27.8 1826 2.12 191 3.42 209 \ndb 149 1108 249 23.7 11 1.5 4.4 6.3 34 0.04 2 0.04 213 javac 302 2261 978 89.0 141 25.4 14.4 28.5 6356 \n7.39 672 12.04 222 mpegaudio 200 1407 425 68.3 88 19.4 20.7 28.4 731 0.85 24 0.43 227 mtrt 178 1318 379 \n32.4 39 7.6 10.3 23.5 1940 2.25 45 0.81 228 jack 202 1392 488 53.6 33 5.4 6.8 10.1 3142 3.65 201 3.60 \npseudojbb 238 2622 824 69.7 174 25.7 21.1 36.9 5556 6.46 759 13.60 min max 149 302 1108 2622 249 978 \n23.7 89.0 11 174 1.5 25.7 4.4 21.1 6.3 36.9 34 6356 0.04 7.39 2 759 0.04 13.60 geomean 204 1523 464 43.6 \n46 7.8 10.0 18.0 860 1.00 56 1.00 DaCapo antlr 307 3517 1541 212.7 101 14.1 6.6 6.6 6198 7.20 597 10.70 \nbloat 471 5231 2023 169.1 100 9.5 4.9 5.6 6031 7.01 398 7.13 chart 706 8972 2299 204.1 113 20.8 4.9 10.2 \n11919 13.85 952 17.06 eclipse 1023 12450 3713 243.0 14 2.0 0.4 0.8 5053 5.87 702 12.58 fop 865 5761 2593 \n206.0 69 7.8 2.7 3.8 6603 7.68 532 9.53 hsqldb 355 5970 1411 130.2 122 18.9 8.6 14.5 4866 5.66 524 9.39 \nluindex 309 3118 940 74.3 168 29.3 17.9 39.4 1876 2.18 154 2.76 lusearch 295 2795 822 65.5 133 21.7 16.2 \n33.1 10183 11.84 1888 33.84 jython 886 9443 3242 462.5 297 28.5 9.2 6.2 2114 2.46 226 4.05 pmd 619 6163 \n2247 152.4 137 14.3 6.1 9.4 2819 3.28 223 4.00 xalan 552 6562 1747 126.2 194 36.0 11.1 28.5 3718 4.32 \n268 4.80 min max 295 1023 2795 12450 822 3713 65.5 462.5 14 297 2.0 36.0 0.4 17.9 0.8 39.4 1876 11919 \n2.18 13.85 154 1888 2.76 33.84 geomean 527 5768 1866 162.4 108 14.8 5.8 9.1 4792 5.57 455 8.16 Table \n3. Bytecodes Compiled and Instruction Cache Characteristics once) and the corresponding KB of bytecodes \n(BC KB) for each benchmark. We count bytecodes rather than machine code, as it is not virtual machine, \ncompiler, or ISA speci.c. The DaCapo bench\u00admarks average more than twice the number of classes, three \ntimes as many declared methods, four times as many compiled methods, and four times the volume of compiled \nbytecodes, re.ecting a sub\u00adstantially larger code base than SPEC. Columns 6 and 7 show how much code \nis optimized by the JVM s adaptive compiler over the course of two iterations of each benchmark (which \nEeckhout et al. s results indicate is probably representative of most hotspot .nding virtual machines \n[19]). Columns 8 and 9 show that the DaCapo benchmarks have a much lower proportion of methods which \nthe adaptive compiler regards as hot. Since the virtual machine selects these methods based on frequency \nthresholds, and these thresholds are tuned for SPEC, it may be that the compiler should be selecting \nwarm code. However, it may simply re.ect the complexity of the benchmarks. For example, eclipse has nearly \nfour thousand meth\u00adods compiled, of which only 14 are regarded as hot (0.4%). On the whole, this data \nshows that the DaCapo benchmarks are sub\u00adstantially larger than SPEC. Combined with their complexity, \nthey should present more challenging optimization problems. We also measure instruction cache misses \nper millisecond as another indicator of dynamic code complexity. We measure misses with the performance \ncounters on a 2.0 GHz Pentium M with a 32KB level 1 instruction cache and a 2MB shared level two cache, \neach of which are 8-way with 64 byte lines. We use Jikes RVM and only report misses during the mutator \nportion of the second iteration of the benchmarks (i.e., we exclude garbage collection). Columns 10 and \n11 show L1 instruction misses, .rst as misses per millisecond, and then normalized against the geometric \nmean of the SPEC benchmarks. Columns 12 and 13 show ITLB misses using the same metrics. We can see that \non average DaCapo has L1 I-cache misses nearly six times more frequently than SPEC, and ITLB misses about \neight times more frequently than SPEC. In particular, none of the DaCapo benchmarks have remarkably few \nmisses, whereas SPEC benchmarks 201 compress, 202 jess, and 209 db hardly ever miss the IL1. All DaCapo \nbenchmarks have misses at least twice that of the geometric mean of SPEC.  7. Objects and Their Memory \nBehavior This section presents object allocation, live object, lifetime, and lifetime time-series metrics. \nWe measure allocation demographics suggested byDieckmann and H\u00a8olzle[15].Wealso measurelifetime and live \nobject metrics, and show that they differ substantially from allocation behaviors. Since many garbage \ncollection algorithms are most concerned with live object behaviors, these demographics are more indicative \nfor designers of new collection mechanisms. Other features, such as the design of per-object metadata, \nalso depend on the demographics of live objects, rather than allocated objects. The data described in \nthis section and Section 8 is presented in Table 4, and in Figures 4(a) through 14(a), each of which \ncontains data for one of the DaCapo benchmarks, ordered alphabetically. In a companion technical report \n[6], we show these same graphs for SPEC Java. For all the metrics, DaCapo is more complex and varied \nin its behavior, but we must exclude SPEC here due to space limitations. Each .gure includes a brief \ndescription of the benchmark, key attributes, and metrics. It also plots time series and summaries for \n(a) object size demographics (Section 7.2), (b) heap composition (Section 7.3), and (c) pointer distances \n(Section 8). Together this data shows that the DaCapo suite has rich and diverse object lifetime behaviors. \nSince Jikes RVM is written in Java, the execution of the JIT compiler normally contributes to the heap, \nunlike most other JVMs, where the JIT is written in C. In these results, we exclude the JIT compiler \nand other VM objects by placing then into a separate, excluded heap. To compute the average and time-series \nobject data, we modify Jikes RVM to keep statistics about allocations and to compute statistics about \nlive objects at frequent snapshots, i.e., during full heap collections. Benchmark Heap Volume (MB) Heap \nObjects Mean Object Size 4MB Nursery Survival % Alloc Live Alloc/ Live Alloc Live Alloc/ Live Alloc Live \nSPEC 201 compress 105.4 6.3 16.8 3,942 270 14.6 28,031 24,425 6.6 202 jess 262.0 1.2 221.3 7,955,141 \n22,150 359.1 35 56 1.1 205 raytrace 133.5 3.8 35.1 6,397,943 153,555 41.7 22 26 3.6 209 db 74.6 8.5 8.8 \n3,218,642 291,681 11.0 24 31 14.6 213 javac 178.3 7.2 24.8 5,911,991 263,383 22.4 32 29 25.8 222 mpegaudio \n0.7 0.6 1.1 3,022 1,623 1.9 245 406 50.5 227 mtrt 140.5 7.2 19.5 6,689,424 307,043 21.8 22 25 6.6 228 \njack 270.7 0.9 292.7 9,393,097 11,949 786.1 30 81 2.8 pseudojbb 207.1 21.1 9.8 6,158,131 234,968 26.2 \n35 94 31.3 min max 0.7 270.7 0.6 21.1 1.1 292.7 3,022 9,393,097 270 307,043 1.9 786.1 22 28,031 25 24,425 \n1.1 50.5 geomean 86.5 3.8 23.0 1,180,850 35,886 32.9 77 110 8.7 DaCapo antlr 237.9 1.0 248.8 4,208,403 \n15,566 270.4 59 64 8.2 bloat 1,222.5 6.2 195.6 33,487,434 149,395 224.2 38 44 6.0 chart 742.8 9.5 77.9 \n26,661,848 190,184 140.2 29 53 6.3 eclipse 5,582.0 30.0 186.0 104,162,353 470,333 221.5 56 67 23.8 fop \n100.3 6.9 14.5 2,402,403 177,718 13.5 44 41 14.2 hsqldb 142.7 72.0 2.0 4,514,965 3,223,276 1.4 33 23 \n63.4 jython 1,183.4 0.1 8,104.0 25,940,819 2,788 9,304.5 48 55 1.6 luindex 201.4 1.0 201.7 7,202,623 \n18,566 387.9 29 56 23.7 lusearch 1,780.8 10.9 162.8 15,780,651 34,792 453.6 118 330 1.1 pmd 779.7 13.7 \n56.8 34,137,722 419,789 81.3 24 34 14.0 xalan 60,235.6 25.5 2,364.0 161,069,019 168,921 953.5 392 158 \n3.8 min max 100.3 60,235.6 0.1 72.0 2.0 8,104.0 2,402,403 161,069,019 2,788 3,223,276 1.4 9,304.5 24 \n392 23 330 1.1 63.4 geomean 907.5 6.2 147.6 18,112,439 103,890 174.3 53 62 8.4 Table 4. Key Object Demographic \nMetrics 7.1 Allocation and Live Object Behaviors Table 4 summarizes object allocation, maximum live objects, \nand their ratios in MB (megabytes) and objects. The table shows that DaCapo allocates substantially more \nobjects than the SPEC bench\u00admarks, by nearly a factor of 20 on average. The live objects and memory are \nmore comparable; but still DaCapo has on average three times the live size of SPEC. DaCapo has a much \nhigher ratio of allocation to maximum live size, with an average of 147 com\u00adpared to SPEC s 23 measured \nin MB. Two programs stand out; jython with a ratio of 8104, and xalan with a ratio of 2364. The DaCapo \nbenchmarks therefore put signi.cantly more pressure on the underlying memory management policies than \nSPEC. Nursery survival rate is a rough measure of how closely a pro\u00adgram follows the generational hypothesis \nwhich we measure with respect to a 4MB bounded nursery and report in the last column of Table 4. Note \nthat nursery survival needs to be viewed in the context of heap turnover (column seven of Table 4). A \nlow nurs\u00adery survival rate may suggest low total GC workload, for exam\u00adple, 222 mpegaudio and hsqldb \nin Table 4. A low nursery survival rate and a high heap turnover ratio instead suggests a substantial \nGC workload, for example, eclipse and luindex. SPEC and Da-Capo exhibit a wide range of nursery survival \nrates. Blackburn et al. show that even programs with high nursery survival rates and large turnover bene.t \nfrom generational collection with a copying bump\u00adpointer nursery space [4]. For example, 213 javac has \na nursery survival rate of 26% and performs better with generational collec\u00adtors. We con.rm this result \nfor all the DaCapo benchmarks, even on hsqldb with its 63% nursery survival rate and low turnover ratio. \nTable 4 also shows the average object size. The benchmark suites do not substantially differ with respect \nto this metric. A sig\u00adni.cant outlier is 201 compress, which compresses large arrays of data. Other outliers \ninclude 222 mpegaudio, lusearch and xalan, all of which also operate over large arrays.  7.2 Object \nSize Demographics This section improves the above methodology for measuring object size demographics. \nWe show that these demographics vary with time and when viewed from of perspective of allocated versus \nlive objects. Allocation-time size demographics inform the structure of the allocator. Live object size \ndemographics impact the design of per-object metadata and elements of the garbage collection algo\u00adrithm, \nas well as in.uencing the structure of the allocator. Fig\u00adures 4(a) through 14(a) each use four graphs \nto compare size demo\u00adgraphics for each DaCapo benchmark. The object size demograph\u00adics are measured both \nas a function of all allocations (top) and as a function of live objects seen at heap snapshots (bottom). \nIn each case, we show both a histogram (left) and a time-series (right). The allocation histogram plots \nthe number of objects on the y-axis in each object size (x-axis in log scale) that the program allocates. \nThe live histogram plots the average number of live objects of each size over the entire program. We \ncolor every .fth bar black to help the eye correlate between the allocation and live histograms. Consider \nantlr in Figure 4(a) and bloat in Figure 5(a). For antlr, the allocated versus live objects in a size \nclass show only modest differences in proportions. For bloat however, 12% of its allocated objects are \n38 bytes whereas essentially no live objects are 38 bytes, which indicates they are short lived. On the \nother hand, less than 1% of bloat s allocated objects are 52 bytes, but they make up 20% of live objects, \nindicating they are long lived. Figure 14(a) shows that for xalan there is an even more marked difference \nin allocated and live objects, where 50% of allocated objects are 12 bytes, but none stay live. In fact, \n65% of live objects are 2 Kbytes, whereas they make up only 2% of allocated objects. How well these large \nobjects are handled will thus in large part determine the performance of the collector on xalan. For \neach allocation histogram, we also present a time series graph in Figures 4(a) through 14(a). Each line \nin the time series graph represents an object size class from the histogram on the left. We color every \n.fth object size black, stack them, and place the smallest size classes at the bottom of the graphs. \nThe distance between the lines indicates the cumulative number of objects allo\u00adcated or live of the corresponding \nsize, as a function of time (in bytes of allocation by convention). Together, the histogram and time-series \ndata show marked dif\u00adferences between allocated and live object demographics. For ex\u00adample, the allocation \nhistograms for bloat, fop, and xalan (Fig\u00adures 5(a), 8(a), 14(a)) are similar, but the time series data \nshows many differences. The xalan program has eight allocation phases that are self-similar and mirrored \nin the live data, although in dif\u00adferent size class proportions. Whereas, in bloat allocation and live \nobjects show much less phase behavior, and phases are not self\u00adcorrelated. Comparing live and allocated \ntime-series for fop shows a different pattern. There is a steady increase in the live objects of each \nsize (and consequently, probably growing data structures), whereas fop allocates numerous sizes in a \nseveral distinct allocation phases. Thus, the allocation and live graphs are very different. This shows \nthat live and allocation time series analysis can reveal com\u00adplexity and opportunities that a scalar \nmetric will never capture.  7.3 Heap Composition Graphs Figures 4(b) through 14(b) each plot heap composition \nin lines of constant allocation as a function of time, measured in allocations (top) and pointer mutations \n(bottom). Like the live object time series graphs, these graphs expose the heap composition but show \nobject lifetime behaviors rather than object size. Since both graphs show live objects, their shapes \nare similar. The heap composition graphs group objects into cohorts based on allocation time. We choose \ncohort sizes as a power of two (2n) such that there are between 100 and 200 cohorts, shown as a line \nin each graph. The top line corresponds to the oldest cohort and indicates the total volume of live objects \nin the heap. The gaps between each of the lines re.ects the amount in each cohort, and when objects in \na cohort die, adjacent lines move closer together or if they all die, the lines merge. It is not uncommon \nfor programs to immediately allocate long lived data, indicated by a gap between the top line and the \nother cohorts; bloat, hsqldb, jython, and lusearch all show this behavior in Figures 5(b), 9(b), 10(b), \nand 12(b). Qualitatively, the complexity of the graphs in Figures 4(b) through 14(b) re.ect the object \nlifetime behaviors of each of the benchmarks. With the exception of jython and lusearch, the DaCapo benchmarks \nshow much richer lifetime behaviors than SPEC [6]; jython is an interpreter, which leads to a highly \nregular execution pattern. Although jython allocates more than any of the SPEC benchmarks, its behavior \nis highly regular. We experimented with a number of interpreted workloads and found very similar, highly \nregular behavior, suggesting that the interpreter rather than the interpreted program dominates. The \nprograms chart and xalan show distinct self-similar phases with respect to object lifetimes in Figures \n6(b) and and 14(b). The programs fop and hsqldb show regular, steady heap growth in Figures 8(b) and \n9(b). On the other hand, bloat, eclipse, luindex, and pmd show irregular, complex object lifetime patterns \nin Figures 5(b), 7(b), 11(b), and 13(b).  8. Reference Behavior in Pointer Distances Java programs primarily \nuse pointer-based data structures. This section provides statistics that describe the connectivity of \nthe data structures created and manipulated by the DaCapo benchmarks. We measure pointer distance between \nits source and target objects by the relative ages of the objects, for both static snapshots of the heap, \nand dynamically as pointers change. These properties in.uence aspects of memory performance, such as \ntemporal and spatial locality and the ef.cacy of generational garbage collectors. Figures 4(c) through \n14(c) show the relative distances between the sources and targets of pointers in the heap for each benchmark. \nPointer distance is measured by the difference between the target and source object positions within \n(a close approximation to) a per\u00adfectly compacted heap. We approximate a continuously perfectly compacted \nheap by tracking cohort sizes and the logical position of each object within each cohort during frequent \ngarbage collec\u00adtions. The youngest object has a heap position of 0 and the oldest has a heap position \nequal to the volume of live objects in the heap. Thus, positive values are old to young object pointers, \nand negative values are young to old. We include both a static snapshot measure of pointer distance, \nand a dynamic mutation measure. Snapshot pointer distance is established by examining all pointers in \nthe live object graph at a garbage collection measuring the state of the object graph. Mu\u00adtation distance \nis established by examining every pointer as it is created measuring the activity over the object graph. \nWe express these metrics as aggregate histograms for the execution of the en\u00adtire benchmark, and as a \ntime series to re.ect the changing shape of the histogram over time (measured in mutations). We .rst \nconsider snapshot pointer distance, the top histogram and time series in Figures 4(c) through 14(c). \nThe most striking feature of these graphs is the wide range of behaviors displayed by the benchmarks. \nSeveral programs show very irregular time\u00advarying behavior, e.g., antlr, chart, eclipse, and luindex; \nwhereas bloat hsqldb, and pmd are more stable, but still vary a bit; and xalan shows a very complex, \nbut exactly repeated pattern. The mutation pointer distance graphs have the same axes and are shown below \neach snapshot pointer distance .gure. These graphs are computed by tracking pointer distances at all \npointer stores (in a write barrier), rather than at static snapshots of the heap. These graphs show a \nwider range of irregularity and patterns than the heap snapshots. To illustrate the differences between \nthese metrics, consider bloat in Figure 5(c). Many pointers point from old to new objects (positive numbers \nin the snapshot graphs in the top half of Fig\u00adure: 5(c)), but almost all pointer mutations install new \nto old point\u00aders (negative numbers in the mutation graphs). The snapshot graphs indicate that around \n40% of pointers will point from old to new at any given snapshot (see the top-most line in the time series) \nand about 60% will point from new to old (the bottom-most line). On the other hand, the mutation graphs \nshow that for most of the execu\u00adtion of the benchmark, nearly 100% of pointer mutations are in the new \nto old direction. The divergence of the snapshot and mutation data, and the time-varying nature of each \nhighlight the limitations of single value summaries of benchmark behaviors. 9. Principal Components Analysis \n Previous sections demonstrate that DaCapo is more object oriented, more complex, and larger than SPEC, \nwhereas this section demon\u00adstrates that all the constituent programs differ from each other, us\u00ading principal \ncomponent analysis (PCA) [18]. This result indicates that we satisfy our goal of program diversity. It \nalso con.rms that DaCapo benchmarks differ from SPEC, which is unsurprising by now given the results \nfrom the preceding sections. PCA is a multivariate statistical technique that reduces a large N dimensional \nspace into a lower dimensional uncorrelated space. PCA generates a positive or negative weight (factor \nloading) associ\u00adated with each metric. These weights transform the original higher dimension space into \nP principal components using linear equa\u00adtions. We follow the PCA methodology from prior work [13, 19], \nMetric Rank PC1 PC2 PC3 PC4 Architecture Instruction mix ALU Intruction mix branches Instruction mix \n memory Branch mispreds/instruction for a PPM predictor Register dependence distance up to 16 Register \ndependence distance between 16 and 64 Register dependence distance above 64 -9 -10 1 8 -5 -15 6 -15 -4 \n13 8 -12 -1 3 7 16 -11 12 -5 6 13 -3 1 -13 14 12 -7 -15  Code Memory Instruction cache misses in miss/msec \nBytecode compiled in KB Methods compiled -14 -7 -2 14 11 7 1 8 9 11 -6 -9 Pointer distance mean Pointer \ndistance standard deviation Mutation distance mean Mutation distance standard deviation Incoming pointers \nper object standard deviation Outgoing pointers per object standard deviation 3 -4 16 -12 -13 -11 -16 \n9 10 2 5 6 10 14 3 15 -2 -4 -5 16 4 2 -8 -10 Table 5. Metrics Used for PC Analysis and Their PC Rankings \nbut use different constituent metrics. Table 5 shows these metrics which cover architecture, code, and \nmemory behavior. We include architecture metrics to expand over the code and memory met\u00adrics presented \nand explored in depth by previous sections, and to see which of these differentiate the benchmarks. Our \ncode met\u00adrics include the i-cache miss rate for each benchmark, the number of methods compiled and the \nvolume of bytecodes compiled. The memory metrics include static and dynamic pointer distances, and incoming \nand outgoing pointer distributions. Following prior work [27], our architecture metrics are micro\u00adarchitecture \nneutral, meaning that they capture key architectural characteristics such as instruction mix, branch \nprediction, and register dependencies, but do so independently of the underlying micro-architecture. \nWe gather these metrics using a modi.ed ver\u00adsion of Simics v. 3.0.11 [42]. We use our harness to measure \nstable performance in Sun s HotSpot JVM, v. 1.5.0 07-b03 running on a simulated Sun Ultra-5 10 with Solaris \n9. PCA computes four principal components (PC1, PC2, PC3, and PC4) which in our case account for 70% \nof the variance between benchmarks. PCA identi.es principal components in order of sig\u00adni.cance; PC1 \nis the most determinative component and PC4 is the least. Table 5 shows the relative ranks of each of \nthe metrics for PC1 PC4. The absolute value of the numbers in columns 2 5 in\u00addicates the rank signi.cance \nof the metric, while the sign indicates whether the contribution is negative or positive. We bold the \nten most signi.cant values overall; six of these contribute to PC1, four to PC2, three to PC3, and none \nto PC4. Memory instruction mix is the most signi.cant metric for PC1, and methods compiled is the next \nmost signi.cant. Note that the three most signi.cant contribu\u00adtors to PC1 cover each of the three metric \ncategories. Scatter plots in Figure 3 show how the benchmarks differ in two-dimensional space. Figure \n3 plots each program s PC1 value against its PC2 value in the top graph, and Figure 3 plots PC3 and PC4 \nin the bottom graph. Intuitively, the further the distance between two benchmarks, the further apart \nthey are with respect to the metrics. The benchmarks differ if they are apart in either graph. Since \nthe programs are well distributed in these graphs, the benchmarks differ. 10. Conclusion Benchmarks \nplay a strategic role in computer science research and development by creating a common ground for evaluating \nideas and products. The choice of benchmarks and benchmarking methodology can therefore have a signi.cant \nimpact on a research Figure 3. PCA Scatter Plots; PC1 &#38; PC2 (top), and PC3 &#38; PC4. .eld, potentially \naccelerating, retarding, or misdirecting energy and innovation. Prompted by concerns among ourselves \nand others about the state-of-the-art, we spent thousands of hours at eight separate institutions examining \nand addressing the problems of benchmarking Java applications. The magnitude of the effort surely explains \nwhy so few have developed benchmark suites. This paper makes two main contributions: 1) it describes \na range of methodologies for evaluating Java, including a number of new analyses, and 2) it presents \nthe DaCapo benchmark suite. We show that good methodology is essential to drawing meaningful conclu\u00adsions \nand highlight inadequacies prevalent in current methodology. A few of our speci.c methodology recommendations \nare: When selecting benchmarks for a suite, use PCA to quantify benchmark differences with metrics that \ninclude static and dy\u00adnamic code and data behavior.  When evaluating architectures use multiple JVMs. \nEvaluating new architecture features will also bene.t from multiple JVMs.  When evaluating JVM performance, \nuse multiple architectures with mix and stable methodologies, reporting .rst and/or sec\u00adond iterations \nas well as steady-state to explore the compile and runtime tradeoffs in the JVM.  When measuring memory \nperformance, use and report heap sizes proportional to the minimums.  When measuring GC and JIT compilation \nperformance use mix and stable methodologies, and use constant workload (rather than throughput) benchmarks. \n When measuring GC or compile time overheads use determin\u00adistic stable and mix methodologies.  This \npaper uses these methodologies to demonstrate that the Da-Capo benchmarks are larger, more complex and \nricher than the commonly used SPEC Java benchmarks. The DaCapo benchmarks are publicly available, evolving, \nand have and will remain open to public feedback [14].  Acknowledgments We thank Andrew Appel, Randy \nChow, Frans Kaashoek, and Bill Pugh who encouraged this project at our three year ITR review. We thank \nMark Wegman who initiated the public availability of Jikes RVM, and the developers of Jikes RVM. Fahad \nGilani wrote the original version of the measurement infrastructure for his ANU Masters Thesis.  References \n[1] B. Alpern, D. Attanasio, J. J. Barton, A. Cocchi, S. F. Hummel, D. Lieber, M. Mergen, T. Ngo, J. \nShepherd, and S. Smith. Implementing Jalape no in Java. In ACM Conference on Object Oriented Programming \nSystems, Languages, and Applications, Denver, CO, Nov. 1999. [2] Apache Software Foundation. Apache Software \nLicense, 2000. http://www.\u00adopensource.org/licenses/apachepl.php. [3] C. Attanasio, D. Bacon, A. Cocchi, \nand S. Smith. A comparative evaluation of parallel garbage collectors. In Proceedings of the Fourteenth \nWorkshop on Languages and Compilers for Parallel Computing, Cumberland Falls, KY, Aug. 2001. [4] S. M. \nBlackburn, P. Cheng, and K. S. McKinley. Myths and realities: The performance impact of garbage collection. \nIn Proceedings of the ACM Conference on Measurement &#38; Modeling Computer Systems, pages 25 36, NY, \nNY, June 2004. [5] S. M. Blackburn, P. Cheng, and K. S. McKinley. Oil and water? High performance garbage \ncollection in Java with MMTk. In Proceedings of the 26th International Conference on Software Engineering, \npages 137 146, Scotland, UK, May 2004. [6] S. M. Blackburn, R. Garner, C. Hoffman, A. M. Khan, K. S. \nMcKinley, R. Bentzur, A. Diwan, D. Feinberg, D. Frampton, S. Z. Guyer, M. Hirzel, A. Hosking, M. Jump, \nH. Lee, J. E. B. Moss, A. Phansalkar, D. Stefanovi\u00b4c, T. VanDrunen, D. von Dincklage, and B. Wiedermann. \nThe DaCapo Benchmarks: Java benchmarking development and analysis (extended version). Technical Report \nTR-CS-06-01, Dept. of Computer Science, Australian National University, 2006. http://www.dacapobench.org. \n[7] S. M. Blackburn and A. Hosking. Barriers: Friend or foe? In The International Symposium on Memory \nManagement, pages 143 151, Oct. 2004. [8] S. M. Blackburn and K. S. McKinley. In or out? Putting write \nbarriers in their place. In The International Symposium on Memory Management, pages 175 184, June 2002. \n[9] S. M. Blackburn, S. Singhai, M. Hertz, K. S. McKinley, and J. E. B. Moss. Pretenuring for Java. In \nACM Conference on Object Oriented Programming Systems, Languages, and Applications, pages 342 352, Tampa, \nFL, Oct. 2001. ACM. [10] T. Brecht, E. Arjomandi, C. Li, and H. Pham. Controlling garbage collection \nand heap growth to reduce the execution time of Java applications. In ACM Conference on Object Oriented \nProgramming Systems, Languages, and Applications, pages 353 366, Tampa, FL, 2001. [11] B. Cahoon and \nK. S. McKinley. Data .ow analysis for software prefetching linked data structures in java controller. \nIn The International Conference on Parallel Architectures and Compilation Techniques, pages 280 291, \nBarcelona, Spain, Sept. 2001. [12] S. R. Chidamber and C. F. Kemerer. A metrics suite for object-oriented \ndesign. IEEE Transactions on Software Engineering, 20(6):476 493, June 1994. [13] F. Chow, A. Wright, \nand K. Lai. Characterization of java workloads by principal components analysis and indert branches. \nIn Proceedings of the Workshop on Workload Characterization, pages 11 19, Dallas, TX, Nov. 1998. [14] \nDaCapo Project. The DaCapo Benchmarks, beta-2006-08, 2006. http://www.\u00addacapobench.org. [15] S. Dieckmann \nand U. H\u00a8olzle. A study of the allocation behavior of the SPECjvm98 Java benchmarks. In European Conference \non Object-Oriented Programming, June 1999. [16] A. Diwan, D. Tarditi, and J. E. B. Moss. Memory subsystem \nperformance of programs using copying garbage collection. In Conference Record of the Twenty-First ACM \nSymposium on Principles of Programming Languages, pages 1 14, Portland, OR, Jan. 1994. [17] B. Dufour, \nK. Driesen, L. Hendren, and C. Verbrugge. Dynamic metrics for Java. In ACM Conference on Object Oriented \nProgramming Systems, Languages, and Applications, pages 149 168, Anaheim, CA, Oct. 2003. [18] G. H. \nDunteman. Principal Components Analysis. Sage Publications, 1989. [19] L. Eeckhout, A. Georges, and \nK. De Bosschere. How Java programs interact with virtual machines at the microarchitecture level. In \nACM Conference on Object Oriented Programming Systems, Languages, and Applications, pages 169 186, Anaheim, \nCA, October 2003. [20] R. Fitzgerald and D. Tarditi. The case for pro.le-directed selection of garbage \ncollectors. In The International Symposium on Memory Management, pages 111 120, Minneapolis, MN, Oct. \n2000. [21] M. Hauswirth, A. Diwan, P. Sweeney, and M. Mozer. Automating vertical pro.ling. In ACM Conference \non Object Oriented Programming Systems, Languages, and Applications, pages 281 296, San Diego, CA, October \n2005. [22] M. W. Hicks, J. T. Moore, and S. Nettles. The measured cost of copying garbage collection \nmechanisms. In ACM International Conference on Functional Programming, pages 292 305, 1997. [23] U. H\u00a8olzle \nand D. Ungar. Do object-oriented languages need special hardware support? In European Conference on Object-Oriented \nProgramming, pages 283 302, London, UK, 1995. [24] A. L. Hosking, J. E. B. Moss, and D. Stefanovi\u00b4 c. \nA comparative performance evaluation of write barrier implementations. In ACM Conference on Object Oriented \nProgramming Systems, Languages, and Applications, pages 92 109, Vancouver, BC, Oct. 1992. [25] X. Huang, \nZ. Wang, S. M. Blackburn, K. S. McKinley, J. E. B. Moss, and P. Cheng. The garbage collection advantage: \nImproving mutator locality. In ACM Conference on Object Oriented Programming Systems, Languages, and \nApplications, pages 69 80, Vancouver, BC, 2004. [26] Java Grande Forum. The Java Grande Benchmark Suite, \n2006. http://www.\u00adepcc.ed.ac.uk/javagrande/. [27] A. Joshi, A. Phansalkar, L. Eeckhout, and L. John. \nMeasuring benchmark similarity using inherent program characteristics. IEEE Transactions on Computers, \n55(6):769 782, June 2006. [28] J. Kim and Y. Hsu. Memory system behavior of Java programs: methodology \nand analysis. In Proceedings of the ACM Conference on Measurement &#38; Modeling Computer Systems, pages \n264 274, Santa Clara, California, June 2000. [29] T. Li, L. John, V. Narayanan, A. Sivasubramaniam, J. \nSabarinathan, and A. Murthy. Using complete system simulation to characterize SPECjvm98 benchmarks. In \nProceedings of the 2000 ACM International Conference on Supercomputing, pages 22 33, Santa Fe, NM, 2000. \n[30] Y. Luo and L. John. Workload characterization of multithreaded Java servers. In IEEE International \nSymposium on Performance Analysis of Systems and Software, pages 128 136, 2001. [31] M. Marden, S. Lu, \nK. Lai, and M. Lipasti. Comparison of memory system behavior in Java and non-Java commercial workloads. \nIn Proceedings of the Workshop on Computer Architecture Evaluation using Commercial Workloads, Boston, \nMA, Feb. 2002. [32] R. Radhakrishnan, N. Vijaykrishnan, L. K., A. Sivasubramaniam, J. Rubio, and J. Sabarinathan. \nJava runtime systems: Characterization and architectural implications. IEEE Transactions on Computers, \n50(2):131 146, Feb. 2001. [33] A. Rajan, S. Hu, and J. Rubio. Cache performance in Java virtual machines: \nA study of constituent phases. In IEEE International Workshop on Workload Characterization, Nov. 2002. \n[34] A. Rogers, M. Carlisle, J. H. Reppy, and L. J. Hendren. Supporting dynamic data structures on distributed-memory \nmachines. ACM Transactions on Programming Languages and Systems, 17(2):233 263, Mar. 1995. [35] Y. Shuf, \nM. J. Serran, M. Gupta, and J. P. Singh. Characterizing the memory behavior of Java workloads: A structured \nview and opportunities for optimizations. In Proceedings of the ACM Conference on Measurement &#38; Modeling \nComputer Systems, pages 194 205, Cambridge, MA, June 2001. [36] D. D. Spinellis. ckjm Chidamber and Kemerer \nmetrics Software, v 1.6. Technical report, Athens University of Economics and Business, 2005. http://\u00adwww.spinellis.gr/sw/ckjm. \n[37] Standard Performance Evaluation Corporation. SPECjvm98 Documentation, release 1.03 edition, March \n1999. [38] Standard Performance Evaluation Corporation. SPECjbb2000 (Java Business Benchmark) Documentation, \nrelease 1.01 edition, 2001. [39] D. Stefanovic.\u00b4Properties of Age-Based Automatic Memory Reclamation \nAlgorithms. PhD thesis, Department of Computer Science, University of Massachusetts, Amherst, Massachusetts, \nDec. 1998. [40] D. Stefanovi\u00b4c, M. Hertz, S. M. Blackburn, K. McKinley, and J. Moss. Older\u00ad.rst garbage \ncollection in practice: Evaluation in a Java virtual machine. In Memory System Performance, Berlin, Germany, \nJune 2002. [41] D. Tarditi and A. Diwan. Measuring the cost of memory management. Lisp and Symbolic Computation, \n9(4), Dec. 1996. [42] Virtutec, Inc. Virtutech Simics, 2006. http://www.simics.net. Short Description \nA parser generator and translator gener\u00adator Long Description ANTLR parses one or more grammar .les and \ngenerate a parser and lexical analyizer for each. Threads Single threaded Repeats Two iterations, each \nparses 44 distinct grammar.les Version 2.7.2 Copyright Public Domain Author Terence Parr License Public \nDomain Benchmark Characteristics Total Allocation (MB) 237.9 (Obj) 4,208,403 Maximum Live (MB) 1.0 (Obj) \n15,566 Pointer Mutations (M) 3.91 Classes Loaded 126  Allocated (above) and Live (below) Object Size \nHistograms and Time-series (a)  Heap Composition Time-series, in Allocations (above) and Mutations (below) \n(b) (c) Snapshot (above) and Mutation (below) Pointer Distance Histograms and Time-series Figure 4. \nBenchmark Characteristics: antlr Short Description A Bytecode-level optimization and analysis tool for \nJava Long Description BLOAT analyzes and optimizes some of its own class.les Threads Single threaded \nRepeats Single iteration, transitively optimizes classes referenced by a single root class Version 1.0 \nCopyright Copyright (c) 1997-2001 Purdue Re\u00adsearch Foundation of Purdue University Author Nathaniel Nystrom \nand David Whitlock License BSD-style Benchmark Characteristics Total Allocation (MB) 1,222.5 (Obj) 33,487,434 \nMaximum Live (MB) 6.2 (Obj) 149,395 Pointer Mutations (M) 257.84 Classes Loaded 281  Allocated (above) \nand Live (below) Object Size Histograms and Time-series (a)  Heap Composition Time-series, in Allocations \n(above) and Mutations (below) (b) (c) Snapshot (above) and Mutation (below) Pointer Distance Histograms \nand Time-series Figure 5. Benchmark Characteristics: bloat Short Description A graph plotting toolkit \nand pdf renderer Long Description jfreechart plots a number of complex line graphs and renders them as \npdf via itext Threads Single threaded Repeats Single iteration plots 14 distinct graphs Version 0.9.21, \n1.0b Copyright (C)opyright 2000-2004, by Object Re\u00ad .nery Limited and Contributors; Copy\u00adright 2000, \n2001, 2002 by Bruno Lowagie Author David Gilbert, Bruno Lowagie and Paulo Soares License LGPL and MPL \nBenchmark Characteristics Total Allocation (MB) 742.8 (Obj) 26,661,848 Maximum Live (MB) 9.5 (Obj) 190,184 \nPointer Mutations (M) 19.64 Classes Loaded 219 Allocated (above) and Live (below) Object Size Histograms \nand Time-series (a)  Heap Composition Time-series, in Allocations (above) and Mutations (below) (b) \n(c) Snapshot (above) and Mutation (below) Pointer Distance Histograms and Time-series Figure 6. Benchmark \nCharacteristics: chart Short Description An integrated development environment Long Description Run a \nseries of eclipse jdt (non-gui) per\u00adformance tests Threads Workload is single threaded, but Eclipse uses \nmultiple threads internally Repeats Single iteration, performs multiple dis\u00adtinct Eclipse tasks Version \n3.1.2 Copyright Eclipse Foundation Author Eclipse Foundation License Eclipse Public License Benchmark \nCharacteristics Total Allocation (MB) 5,582.0 (Obj) 104,162,353 Maximum Live (MB) 30.0 (Obj) 470,333 \nPointer Mutations (M) 335.49 Classes Loaded 795  Allocated (above) and Live (below) Object Size Histograms \nand Time-series (a)  Heap Composition Time-series, in Allocations (above) and Mutations (below) (b) \n(c) Snapshot (above) and Mutation (below) Pointer Distance Histograms and Time-series Figure 7. Benchmark \nCharacteristics: eclipse Short Description An output-independant print formatter Long Description fop \ntakes an XSL-FO.le, parses it and formats it, generating an encrypted pdf .le Threads Single threaded \nRepeats Single iteration, renders a single XSL-FO.le Version 0.20.5 Copyright Copyright (C) 1999-2003 \nThe Apache Software Foundation Author Apache Software Foundation License Apache Public License Benchmark \nCharacteristics Total Allocation (MB) 100.3 (Obj) 2,402,403 Maximum Live (MB) 6.9 (Obj) 177,718 Pointer \nMutations (M) 3.85 Classes Loaded 231  Allocated (above) and Live (below) Object Size Histograms and \nTime-series (a)  Heap Composition Time-series, in Allocations (above) and Mutations (below) (b) (c) \nSnapshot (above) and Mutation (below) Pointer Distance Histograms and Time-series Figure 8. Benchmark \nCharacteristics: fop Short Description An SQL relational database engine writ\u00adten in Java Long Description \nHSQLDB executes a JDBC-like in\u00admemory benchmark, executing a num\u00adber of transactions against a model \nof a banking application Threads 20 client threads Repeats 40 transactions per client Version 1.8.0.4 \nCopyright Copyright (c) 2001-2002, The HSQL Development Group Author The HSQLDB Development Group License \nThe HSQLDB license. Benchmark Characteristics Total Allocation (MB) 142.7 (Obj) 4,514,965 Maximum Live \n(MB) 72.0 (Obj) 3,223,276 Pointer Mutations (M) 19.31 Classes Loaded 131 Allocated (above) and Live \n(below) Object Size Histograms and Time-series (a)  Heap Composition Time-series, in Allocations (above) \nand Mutations (below) (b) (c) Snapshot (above) and Mutation (below) Pointer Distance Histograms and Time-series \n Figure 9. Benchmark Characteristics: hsqldb Short Description A python interpreter written in Java Long \nDescription jython executes (interprets) the pybench benchmark or a small python program Threads Single \nthreaded Repeats Single iteration runs a single iteration of the pybench python benchmark Version 2.1 \nCopyright Copyright (c) Python Software Founda\u00adtion Author Jim Hugunin and Barry Warsaw License Jython \nSoftware License. Benchmark Characteristics Total Allocation (MB) 1,183.4 (Obj) 25,940,819 Maximum Live \n(MB) 0.1 (Obj) 2,788 Pointer Mutations (M) 82.96 Classes Loaded 251  Allocated (above) and Live (below) \nObject Size Histograms and Time-series (a)  Heap Composition Time-series, in Allocations (above) and \nMutations (below) (b) (c) Snapshot (above) and Mutation (below) Pointer Distance Histograms and Time-series \nShort Description A text indexing tool Long Description Indexes a set of documents, the works of Shakespeare \nand the King James Bible Threads Single threaded Repeats Single iteration indexes two multi-.le documents \nVersion 1.9.1 Copyright Copyright (C) The Apache Software Foundation Author Lucene Project Management \nCommit\u00adtee License Apache Public License Benchmark Characteristics Total Allocation (MB) 201.4 (Obj) \n7,202,623 Maximum Live (MB) 1.0 (Obj) 18,566 Pointer Mutations (M) 21.75 Classes Loaded 128  Allocated \n(above) and Live (below) Object Size Histograms and Time-series (a)  Heap Composition Time-series, in \nAllocations (above) and Mutations (below) (b) (c) Snapshot (above) and Mutation (below) Pointer Distance \nHistograms and Time-series Short Description A text search tool Long Description Text search of keywords \nover a corpus of data comprising the works of Shake\u00adspeare and the King James bible Threads 32 threads \nRepeats Each thread searches a large index for about 3500 distinct words Version 1.9.1 Copyright Apache \nSoftware Foundation, Apache license v2.0 Author Lucene Project Management Commit\u00adtee License Apache Public \nLicense Benchmark Characteristics Total Allocation (MB) 1,780.8 (Obj) 15,780,651 Maximum Live (MB) 10.9 \n(Obj) 34,792 Pointer Mutations (M) 64.77 Classes Loaded 118 Allocated (above) and Live (below) Object \nSize Histograms and Time-series (a)  Heap Composition Time-series, in Allocations (above) and Mutations \n(below) (b) (c) Snapshot (above) and Mutation (below) Pointer Distance Histograms and Time-series Short \nDescription A source code analyzer for Java Long Description pmd analyzes a list of Java classes for \na range of source code problems Threads Single threaded Repeats Single iteration checks a single large \nsource.le against 18 coding rules Version 1.8 Copyright Copyright (c) 2003, InfoEther, LLC Author Tom \nCopeland License BSD-style Benchmark Characteristics Total Allocation (MB) 779.7 (Obj) 34,137,722 Maximum \nLive (MB) 13.7 (Obj) 419,789 Pointer Mutations (M) 105.52 Classes Loaded 325  Allocated (above) and \nLive (below) Object Size Histograms and Time-series (a)  Heap Composition Time-series, in Allocations \n(above) and Mutations (below) (b) (c) Snapshot (above) and Mutation (below) Pointer Distance Histograms \nand Time-series Short Description An XSLT processor for transforming XML documents Long Description \nXalan transforms an XML document (either a test case or the works of Shake\u00adspeare) and transforms the \ndocument into html Threads Single threaded Repeats 8 iterations, each transforms a single large XML document \n(the works of Shakespeare) Version 2.4.1 Copyright Copyright (C) 1999-2003 The Apache Software Foundation \nAuthor Apache Software Foundation License Apache Software License Benchmark Characteristics Total Allocation \n(MB) 60,235.6 (Obj) 161,069,019 Maximum Live (MB) 25.5 (Obj) 168,921 Pointer Mutations (M) 278.20 Classes \nLoaded 244  Allocated (above) and Live (below) Object Size Histograms and Time-series (a) Heap Composition \nTime-series, in Allocations (above) and Mutations (below) (b) (c) Snapshot (above) and Mutation (below) \nPointer Distance Histograms and Time-series \n\t\t\t", "proc_id": "1167473", "abstract": "Since benchmarks drive computer science research and industry product development, which ones we use and how we evaluate them are key questions for the community. Despite complex runtime tradeoffs due to dynamic compilation and garbage collection required for Java programs, many evaluations still use methodologies developed for C, C++, and Fortran. SPEC, the dominant purveyor of benchmarks, compounded this problem by institutionalizing these methodologies for their Java benchmark suite. This paper recommends benchmarking selection and evaluation methodologies, and introduces the DaCapo benchmarks, a set of open source, client-side Java benchmarks. We demonstrate that the complex interactions of (1) architecture, (2) compiler, (3) virtual machine, (4) memory management, and (5) application require more extensive evaluation than C, C++, and Fortran which stress (4) much less, and do not require (3). We use and introduce new value, time-series, and statistical metrics for static and dynamic properties such as code complexity, code size, heap composition, and pointer mutations. No benchmark suite is definitive, but these metrics show that DaCapo improves over SPEC Java in a variety of ways, including more complex code, richer object behaviors, and more demanding memory system requirements. This paper takes a step towards improving methodologies for choosing and evaluating benchmarks to foster innovation in system design and implementation for Java and other managed languages.", "authors": [{"name": "Stephen M. Blackburn", "author_profile_id": "81100547435", "affiliation": "Intel and Australian National University", "person_id": "P268028", "email_address": "", "orcid_id": ""}, {"name": "Robin Garner", "author_profile_id": "81319492234", "affiliation": "Australian National University", "person_id": "PP18005322", "email_address": "", "orcid_id": ""}, {"name": "Chris Hoffmann", "author_profile_id": "81100074601", "affiliation": "University of Massachusetts at Amherst", "person_id": "PP18005793", "email_address": "", "orcid_id": ""}, {"name": "Asjad M. Khang", "author_profile_id": "81100345181", "affiliation": "University of Massachusetts at Amherst", "person_id": "P813665", "email_address": "", "orcid_id": ""}, {"name": "Kathryn S. McKinley", "author_profile_id": "81100402805", "affiliation": "University of Texas at Austin", "person_id": "P157900", "email_address": "", "orcid_id": ""}, {"name": "Rotem Bentzur", "author_profile_id": "81363592288", "affiliation": "University of New Mexico", "person_id": "P813678", "email_address": "", "orcid_id": ""}, {"name": "Amer Diwan", "author_profile_id": "81100202872", "affiliation": "University of Colorado", "person_id": "PP15025608", "email_address": "", "orcid_id": ""}, {"name": "Daniel Feinberg", "author_profile_id": "81363594512", "affiliation": "University of New Mexico", "person_id": "P813667", "email_address": "", "orcid_id": ""}, {"name": "Daniel Frampton", "author_profile_id": "81314488699", "affiliation": "Australian National University", "person_id": "P789538", "email_address": "", "orcid_id": ""}, {"name": "Samuel Z. Guyer", "author_profile_id": "81332502517", "affiliation": "Tufts", "person_id": "PP42049875", "email_address": "", "orcid_id": ""}, {"name": "Martin Hirzel", "author_profile_id": "81100572340", "affiliation": "IBM TJ Watson Research Center", "person_id": "PP18002227", "email_address": "", "orcid_id": ""}, {"name": "Antony Hosking", "author_profile_id": "81100554713", "affiliation": "Purdue University", "person_id": "PP14192483", "email_address": "", "orcid_id": ""}, {"name": "Maria Jump", "author_profile_id": "81100432998", "affiliation": "University of Texas at Austin", "person_id": "P699720", "email_address": "", "orcid_id": ""}, {"name": "Han Lee", "author_profile_id": "81416601459", "affiliation": "Intel", "person_id": "PP18007154", "email_address": "", "orcid_id": ""}, {"name": "J. Eliot B. Moss", "author_profile_id": "81406593781", "affiliation": "University of Massachusetts at Amherst", "person_id": "PP43115329", "email_address": "", "orcid_id": ""}, {"name": "Aashish Phansalkar", "author_profile_id": "81309509198", "affiliation": "University of Texas at Austin", "person_id": "PP18010150", "email_address": "", "orcid_id": ""}, {"name": "Darko Stefanovi&#263;", "author_profile_id": "81100613136", "affiliation": "University of New Mexico", "person_id": "P813669", "email_address": "", "orcid_id": ""}, {"name": "Thomas VanDrunen", "author_profile_id": "81100316503", "affiliation": "Wheaton College", "person_id": "PP114029007", "email_address": "", "orcid_id": ""}, {"name": "Daniel von Dincklage", "author_profile_id": "81100459374", "affiliation": "University of Colorado", "person_id": "PP114029008", "email_address": "", "orcid_id": ""}, {"name": "Ben Wiedermann", "author_profile_id": "81322510080", "affiliation": "University of Texas at Austin", "person_id": "PP18011988", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1167473.1167488", "year": "2006", "article_id": "1167488", "conference": "OOPSLA", "title": "The DaCapo benchmarks: java benchmarking development and analysis", "url": "http://dl.acm.org/citation.cfm?id=1167488"}