{"article_publication_date": "10-16-2006", "fulltext": "\n Method-Speci.c Dynamic Compilation using Logistic Regression John Cavazos Michael F.P. O Boyle Member \nof HiPEAC Institute for Computing Systems Architecture (ICSA), School of Informatics University of Edinburgh, \nUnited Kingdom {jcavazos,mob}@inf.ed.ac.uk Abstract or method within a program. However, just as the \nbest set of optimizations varies from program to program [2, 18], we show Determining the best set of \noptimizations to apply to a program that the best set of optimizations varies within a program, i.e., \nit is has been a long standing problem for compiler writers. To reduce method-speci.c. We would like \na scheme that selects the best set of the complexity of this task, existing approaches typically apply \nthe optimizations for individual portions of a program, rather than the same set of optimizations to \nall procedures within a program, with\u00adsame set for a whole program. out regard to their particular structure. \nThis paper develops a new This paper develops a new method-speci.c technique that auto\u00admethod-speci.c \napproach that automatically selects the best opti\u00admatically selects the best set of optimizations for \ndifferent sections mizations on a per method basis within a dynamic compiler. Our of a program. We develop \nthis technique within the Jikes RVM en\u00adapproach uses the machine learning technique of logistic regression \nvironment and automatically determine the best optimizations on a to automatically derive a predictive \nmodel that determines which per method basis. Rather than developing a hand-crafted technique optimizations \nto apply based on the features of a method. This tech\u00adto achieve this, we make use of a basic machine \nlearning technique nique is implemented in the Jikes RVM Java JIT compiler. Using known as logistic regression \n[4] to automatically determine what this approach we reduce the average total execution time of the optimizations \nare best for each method. This is achieved by training SPECjvm98 benchmarks by 29%. When the same heuristic \nis ap\u00adthe technique of.ine on a set of training data which then automati\u00adplied to the DaCapo+ benchmark \nsuite, we obtain an average 33% cally learns an optimizing heuristic. reduction over the default level \nO2 setting. Machine learning based techniques have recently received con-Categories and Subject Descriptors \nD.3 [Software]: Program-siderable attention as a means of rapidly developing optimization ming languages; \nD.3.4 [Programming languages]: Processors heuristics [20, 7]. Unfortunately, they have been largely constrained \nCompilers, Optimization; I.2.6 [Arti.cial intelligence]: Learning to tuning an individual optimization \noften with disappointing re-Induction sults. For example, although Stephenson et al. [20] were able to \nconstruct a register allocation heuristic automatically using ma- General Terms Performance, Experimentation, \nLanguages chine learning, the heuristic was only able to achieve a modest im-Keywords Compiler optimization, \nMachine learning, Logistic provement over the existing hand-tuned register allocator heuristic. Regression, \nJava, Jikes RVM To the best of our knowledge this is the .rst paper to automat\u00ad ically learn an overall \ncompilation strategy for individual portions 1. Introduction of a program. This means that our scheme \nlearns which optimiza\u00adtions to apply rather than tuning local heuristics and it does this in Selecting \nthe best set of optimizations for a program has been the a dynamic compilation setting. Furthermore, \nwe show signi.cant focus of optimizing compilation research for decades. It has been performance improvement \nover an existing well-engineered compi\u00adthe long term goal of compiler writers, to develop a sequence \nof lation system. Using our approach, for the maximum optimization optimization phases which analyze \nand, where appropriate, trans-O2 setting within Jikes RVM, we reduce the total runtime by 29% form the \nprogram so that execution time is reduced. Determining on average for the SPECjvm98 benchmarks suite. \nThis increases the best set of optimizations and their ordering, however, is noto\u00adto 33% on a set of \nbenchmarks including DaCapo, SPECjbb2000, riously dif.cult. In fact, Cooper et al. [9, 2] have shown \nthat the and ipsixql. This result is particularly impressive, when consider\u00adbest ordering is program \ndependent and that any sequence that is ing that recent work on developing by hand predictive models \nto best for one program is unlikely to be the best for another. What select optimizations [24] achieves \nperformance improvements of we would like is a mechanism that automatically selects the best less than \n3% on average. optimization sequence for a particular program. The paper is organized as follows. Section \n2 provides motiva-In optimizing compilers, it is standard practice to apply the same tion as to why method-speci.c \noptimizations outperform current set of optimizations phases in the same order on each procedure approaches. \nThis is followed in section 3 by a description of how standard logistic regression can learn whether \nor not to apply a particular optimization for a particular method. Section 4 then de\u00adscribes the experimental \ninfrastructure and methodology which is Permission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee provided that copies are not made or distributed \n followed in section 5 by a presentation of the results. Section 6 pro\u00adfor pro.t or commercial advantage \nand that copies bear this notice and the full citation vides an analysis of why logistic regression works. \nSection 7 gives on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \na brief overview of related work and is followed in Section 8 by to lists, requires prior speci.c permission \nand/or a fee. some concluding remarks. OOPSLA 06 October 22 26, 2006, Portland, Oregon, USA. Copyright \nc &#38;#169; 2006 ACM 1-59593-348-4/06/0010. . . $5.00   $ % # $ $ % # $ $ % # $ Figure 1. The \nresults of applying the best setting for each method relative to the default setting for each of the \ndifferent optimization levels. The best setting for each method was found through exhaustive exploration \nfor optimization levels O0 and O1. For optimization level O2, we took the best setting for each method \nfrom 1000 random settings evaluated. Total execution time = dynamic compilation time + running time. \n2. Motivation This section shows that selecting the best set of optimizations on a per method basis has \nthe potential to signi.cantly improve the total running time of dynamically compiled programs. Potential \nWe conducted an initial experiment to determine the ex\u00adtent to which different optimizations affect the \ntotal execution time of an application compiled and executed by the Jikes RVM Java JIT compiler. For \neach method within each SPECjvm98 benchmark, we applied many different optimization settings and recorded \nthe setting that gave the best total execution time on a per method ba\u00adsis. As Jikes RVM has multiple \noptimization levels, we performed this experiment for the O0, O1, and O2 optimization levels and re\u00adport \nthe results here. Due to the way that Jikes RVM is constructed, it is not possible to arbitrarily change \nthe order in which optimizations are applied. Instead, we con.ne our search to just enabling and disabling \nopti\u00admizations. For optimization levels O0 and O1, which apply 4 and 9 optimizations by default, we tried \nall possible enumerations of these optimizations, i.e., 24 = 16,29 = 512, respectively. As level O2 applies \n20 optimizations an exhaustive enumeration of the 220 settings was not possible, instead we randomly \nenumerated 1000 different settings and recorded their performance. Table 1 shows the optimizations that \nare applied by default for these three differ\u00adent optimization levels. Once the best settings were found \nfor each method, we then dy\u00adnamically compiled and ran each program using the best optimiza\u00adtion settings \nfor each method. Figure 1 presents results of selecting the best set of optimizations at each optimization \nlevel on a per method basis for each of the SPECjvm98 benchmarks. As compi\u00adlation time is dynamic and \npart of the overall execution time we show two performance results: running time and total time. Run\u00adning \ntime indicates the benchmark running times without compila\u00adtion time while total time indicates running \ntimes with compilation. The results are plotted relative to the existing default heuristic which is to \napply all optimizations grouped at that optimization level (see Table 1). In the case of optimization \nlevel O0, there is on average a 4% reduction in total time available and this reduces to 2% for O1. However, \nin the case of the highest optimization level O2, there is a signi.cant performance improvement available. \nSelecting the right optimization for each method gives an average 19% reduction in total execution time. \nOne size does not .t all Looking at the results, one may conclude that the default settings are poor \nand that there potentially exists another setting that is not method-speci.c and would perform well across \nall other benchmarks. To test this we enumerated all the different possible combina\u00adtions of the 4 optimizations \nwhich are used by default for optimiza\u00adtion level O0 to try to .nd one .xed setting which if applied \nto all methods would provide a performance improvement over the cur\u00adrent default.1 Of the 16 different \noptimization settings we enumer\u00adated, we did not .nd any single con.guration that out-performed the current \ndefault. The current default is in fact the optimal .xed setting for the SPECjvm98 benchmarks. Now, in \nthe previous section we were able to outperform op\u00adtimization O0 for several of the SPECjvm98 benchmarks \ngiving an average reduction in total execution time of 4%. Thus, an op\u00adtimization setting tuned for each \nmethod is better than the opti\u00admal .xed setting across the whole program. In other words, for the 1 Optimization \nlevel O0 is an important optimization since it is the .rst and most often used optimization setting during \nadaptive compilation. SPECjvm98 benchmarks a method-speci.c strategy should outper\u00adform any program-speci.c \nstrategy. Conclusion In our experiments we did not .nd a single overall optimization setting that signi.cantly \nimproves the performance of the Jikes RVM JIT optimizing compiler. However, selecting a different optimization \nsetting for each method shows the potential to deliver signi.cant gains. The challenge therefore is to \ndevelop a predictive model that can analyze each method and select the optimization set that will reduce \ntotal running time. As we are considering dynamic JIT compilation, the overhead of this heuristic must \nbe small otherwise it will outweigh any bene.ts. One approach is to handcraft a heuristic based on experimenta\u00adtion \nand analysis. This is undesirable for two reasons. Firstly, it will be an arduous task and Jikes RVM \nspeci.c. Secondly, if the plat\u00adform were to change, the entire tuning of the heuristic would have to \nbe repeated. Instead we use an machine-learning based approach which automatically learns a good heuristic. \nThis has the advantage of being a generic technique that easily ports across platforms.  3. Approach \nThis section gives a detailed overview of how logistic regression based machine learning is used to determine \na good optimization heuristic for each of the optimization levels O0, O1, O2, and adap\u00adtive within Jikes \nRVM. The .rst section outlines the different sub\u00adactivities that take place when learning and deploying \na heuristic This is followed by sections describing how we generate training data, how we extract features \nfrom methods, and how these fea\u00adtures and training data allow us to learn a heuristic that determines \nwhether or not to apply a set of optimizations. This is followed by an example showing how our learned \nheuristic is used in practice. Figure 2 outlines our scheme. 3.1 Overview There are two distinct phases, \ntraining and deployment. Training occurs once, off-line, at the factory and is equivalent to the time \nspent by compiler writers designing and implementing their opti\u00admization heuristics. Deployment is the \nact of applying the heuristic at dynamic compilation time. As any learned heuristic incurs dy\u00adnamic compilation \noverhead, it is imperative that it be as cheap as possible to evaluate otherwise its runtime bene.t will \nbe out\u00adweighed by its cost. Within the training phase, we .rst generate appropriate training data based \non whether we are trying to improve the O0, O1, or O2 optimization levels within Jikes RVM. The training \ndata is randomly generated by applying different optimization settings to each method within each training \nprogram and recording their performance on a per method basis using .ne-grain timers. We also derive \na short description (called a feature vector) of each method so that we can later build a function that \ntakes the feature vector as input and provides the set of optimizations that should be applied as output. \nThis predictive model once learned is installed into the Jikes RVM compiler and used at runtime as an \noptimization heuristic. The next sections describe these stages in more detail 3.2 Generating training \ndata Our aim is to develop a function, f, which, given a new method described as feature vector x, outputs \na vector c of 1s and 0s whose elements determine whether or not to apply the corresponding optimization: \nf (x)=c 1. Training at the factory (a) Generate training data results for O0,O1, or O2 i. Instrument \neach method with timing calls ii. For each method randomly select a set of optimizations and apply iii. \nRecord dynamic compilation, running, and total execu\u00adtion time per method (b) Generate method features \n i. For each method calculate each element of the feature vector ii. Record feature vector for each method \n (c) Learn a model that classi.es features  i. For each method select those optimization settings within \n1% of best ii. Generate a table where each recorded feature vector is associated with its best optimization \nsettings. iii. For each optimization setting c determine a probabilistic function f that states whether \na feature vector x should have this optimization set or not. iv. Output this set of functions as the \nlearned heuristic 2. Deployment (a) Install learned heuristic into Jikes RVM (b) For each method dynamically \ncompiled  i. From bytecodes generate a feature vector ii. Use heuristic to determine which optimizations \nto apply and apply them Figure 2. Overall technique Before we can do this, we need to know how different \nopti\u00admization settings affect performance. We therefore try many differ\u00adent optimization settings to \nsee how each setting affects the perfor\u00admance of each method. For the problem of optimizing Java methods, \nwe can search the entire space of optimizations if the number of optimizations is not prohibitively large. \nIn the case of optimization levels O0 and O1, there are only 4 and 9 optimizations turned on by default \nallowing exhaustive enumeration. We measure the time to compile a method and instrumented each method \nwith a .ne-grain timer to record the amount of time spent executing that method. For optimization level \nO2, we could not exhaustive enumerate all possible optimization combinations (O2 has 20 optimizations), \nso for this level we choose to evaluate a set of 1000 randomly generated settings. The opti\u00admizations \nwe control at each optimization level with our logistic regressors are described in Table 1. The best \nsettings were recorded in a vector c for each method. Thesizeof c corresponds to the number of optimizations \navailable at each level, i.e., size 4 for O0, 9 for O1, and 20 for O2.  3.3 Feature Extraction Once \nwe have examples of good optimization settings for different methods we would like correlate the settings \nwith the characteris\u00adtics of a method. To do this we need to describe each method in a suf.ciently succinct \nform, to enable standard machine learning techniques to be employed. Determining the properties of a \nmethod that predict an optimiza\u00adtion improvement is a dif.cult task. As we are operating in a dy\u00ad  Meaning \n  REORDER CODE Reorders basic blocks  Local copy propagation  STATIC SPLITTING Basic block static \nsplitting BRANCH OPTS MED More aggressive branch optimizations   Optimization Level O2  LOOP UNROLL \nLoop unrolling LOAD ELIM Load elimination  REDUNDANT BRANCH Redundant branch elimination      \nTable 1. This table describes all the optimizations investigated. These optimizations are all on by default \nfor each of the differ\u00adent optimization levels. The optimizations at level O1 include all optimizations \non at level O0 and optimizations at level O2 include all optimizations on at levels O1 and O0. namic \ncompilation environment we chose features that are simple to calculate and which we thought were relevant. \nComputing these features requires a single pass over the bytecode of the method. We calculate features \nafter inlining has been performed. Table 2 shows the 26 features used to describe each method. The values \nof each feature will be an entry in the 26-element feature vector x associated with each method. The \n.rst 2 entries are integer values de.ning the size of the code and data of the method. The next 6 are \nsimple boolean properties (represented using 0 or 1) of the method. The remaining features are simply \nthe percentage of bytecodes belonging to a particular category. (e.g., 30% loads, 22% .oating point, \n5% yield points, etc.). As an example, the feature vector for the method compress.Compressor.cl block() \nis the following vec\u00adtor. [108,25,0,0,0,0,1,0,0.2,0.0,0.0,0.0,0.0,0.0 0.12,0.0,0.08,0.0,0.0,0.0,0.2,0.32,0.08,0.0] \nIn other words, there are 108 bytes in this method, 25 bytes allocated for locals, etc. We make no claim \nthat this is the best set of features to describe a method. It is possible that an entirely different \nset of features would give better performance. However, our logisitic regressors are able to learn automatically \nwhich features are most important to each of the different optimiza\u00adtions. And, since calculating all \nthe features is very cheap (typically less than 1% of a method s compile time) there is no need to .lter \nout unimportant features. The logistic regressor does this automat\u00adically. Researchers have devised different \nfeatures [1, 7, 6, 15, 20] that worked well on other optimization problems. For instance, Mon-  Feature \nMeaning bytecodes Number of bytecodes in the method locals space Number of words allocated for lo\u00adcals \n  synch Method is synchronized exceptions Method has exception handling code leaf Method is a leaf \n(contains no calls)  .nal Method is declared .nal private Method is declared private static Method \nis declared static   Category Fraction of bytecodes that ... aload, astore are Array Loads and Stores \n   jsr are a JSR switch are a SWITCH put/get are a PUT or GET invoke are an INVOKE new are a NEW arraylength \nare an ArrayLength   multi newarray are a Multi Newarray  Table 2. Features of a Method. To reduce \nthe length of the table several (different) features have been placed in logical groups. sifort et al. \n[15] focus on loop structure as features in predicting whether to unroll or not. In future work, we will \nexplore using other features such as those presented by Georges et al. [11] in combina\u00adtion with the \nfeatures we used in this study.  3.4 Learning a classi.er using logistic regression We are now at the \nstage where we have, for each method, a feature vector x and a vector c which corresponds to the best \nsettings for that particular method. Given a feature vector of a new method x we wish to develop a function, \nf, that returns a good set of optimizations c to apply to it. Classi.cation Our task can now be phrased \nas a classi.cation problem: given a method s feature vector x should we apply a particular optimization \nor not? Given an n = 26 dimensional space of features, we wish to .nd a curve or hyperplane that separates \nthose points where each optimization is turned on from those where it is off. To illustrate this, see \nthe simple 2D example shown in Figure 3 where each point corresponds to a feature vector and we wish \nto .nd a line that separates them into classes. Here the line b + xT w = 0 forms the decision boundary, \non the one side examples are classi.ed as 1s, and on the other, 0s. The parameter b simply shifts the \ndecision boundary by a constant amount. The orientation of the decision boundary is determined by w which \nrepresents the normal to the hyperplane. However, in practice there does not exist a clean separation \nbe\u00adtween points. Instead, we wish to associate with each point a prob\u00adability of whether to apply an \noptimization. The simplest technique that achieves this is logistic regression [4] and is a probabilistic \n0 00 0 0 0 1 0 w 1 1 1 1 1 1 Figure 3. The linear separator decision boundary (solid line). For two \ndimensional data, the decision boundary is a line. extension to linear regression. This provides us with \na con.dence measure as to how good our classi.cation is. Logistic Regression We wish to determine a \nfunction f that gives the probability that a particular method should have a set of optimizations enabled. \nWe have a set of up to to 20 optimizations to consider, c,but to ease presentation we initially consider \nthe case of determining the probability that just one optimization c is enabled, i.e., c = 1. Formally \nthis is stated as: p(c =1|x)= f (x;w) (1) where p(c = 1|x) is the probability that, given a feature vector \nx, optimization c is enabled or turned on and f is some function parameterised by the weights w. Since \nthe function f represents a probability, it must be bounded between 0 and 1. One of the standard choices \nof function is the sigmoid function, s(y)=1/(1 +exp(-y)). When the argument of the sigmoid func\u00adtion \nis positive, the probability that the input point y belongs to class 1 is above 0.5. The greater the \nargument value, y, is, the higher is the probability that it is classi.ed as having the optimization \nsetting c enabled. Similarly, large negative values of x will imply that c is disabled. Logistic regression \nbased classi.cation is described as: T p(c =1|x)=s(b +xw) (2) where b is a constant scalar, and w is \na vector of weights. Selecting the weights w allows the selection of the separating boundary orientation \nand a mechanism to state how con.dent we are in this boundary classi.cation. The larger the weights, \nthe more con.dent we are in the classi.cation.  Training Given that the sigmoid function is a good one \nto classify the data, we now have to derive or learn it. So, given the training data set D, gathered \nduring the earlier exhaustive or random search, how can we adjust or learn the weights to obtain a good \nclassi.cation? As\u00adsuming that each of the P data points has been drawn independently the probability \nthat the data belongs to a particular class c is given by a standard formula [4]: PP j ()j 1-cp(D)= . \np(cj|x j)= .(p(c =1|x j))c1 - p(c =1|x j)j=1 j=1 (3) where x j is the jth ( j . 1,...,P) feature vector \nselected from j the training data and c is its corresponding best optimization setting for that feature \nvector. If p(D)= 1 then we have a perfect classi.cation and the data is clearly separated. In practice \ndue to the noisy data we will not achieve a perfect classi.cation, but instead wish to get the best possible \nclassi.cation. If we adjust the weights to maximise p(D)this will give the best decision hyperplane. \nEach of these probabilities p is a function of the weight vector w, so we wish to choose w in order to \nmaximise p(D).As the values of p are small, it is common [4] to work with the L =logp(D) to avoid rounding \nerrors: P () L = . cj log p(c =1|x j)+(1 - cj )log1 - p(c =1|x j)(4) j=1 and try to maximize it instead. \n(Maximizing L = log(p(D))is equivalent to maximizing p(D)). In the logistic regression model, we wish \nto therefore .nd the weights w to maximise: P () TT L(w,b)= . cj logs(b +wx j)+(1 -cj )log1 - s(b +wx \nj)j=1 (5) Unfortunately, this can not be achieved analytically and is nor\u00admally achieved by using an \niterative solver based on gradient as\u00adcent, based on the partial derivatives of L.2 The gradient is given \nby the following equation: P .wL = .(cj - s(x j;w))x j (6) j=1 The derivative with respect to the biases \nis as follows: P dL j - s(x = .(cj;w)) (7) db j=1 In other words, select a value of w and update it in \nthe direction of ascent. If .w is the partial derivative with respect to the vector w then we update \nour values of w and b as follows: new w=w +..wL (8) dL bnew =b +. (9) db where .,the learning rate is \na small scalar chosen small enough to ensure convergence of the method. This is repeated until there \nis no further change and we have reached the maximum. At the end of this iterative process we have a \nset of weights and offset that gives the best classi.cation on a given set of training inputs. It can \nthen be used as a function which determines for a set of input features the probability of whether an \noptimization should be turned on or off. 3.5 Deployment The .nal step involves installing the heuristic \nfunction in the compiler and using it during dynamic compilation. Each method that is compiled by the \noptimizing compiler is considered a pos\u00adsible candidate for all optimizations. We compute features for \nthe method. If the heuristic function says we should optimize a method with a particular optimization, \nwe do so. As an illustration, when applying the logistic regressor to the feature vector of compress.Compressor.cl \nblock(), it returns a value of [1,0,1,1,0,0,0,1,1,1,1,1,1,1,1,0,1,1,1,0] denoting which of the 20 optimizations \nto apply with the asso\u00adciated probabilities 2 as s'(x)=s(x)(1 - s(x))), partial derivatives are easy \nto calculate   jess Java expert system shell db Builds and operates on an in-memory database javac \n Java source to bytecode compiler in JDK 1.0.2 mpegaudio Decodes an MPEG-3 audio .le raytrace A raytracer \nworking on a scene with a dinosaur jack A Java parser generator with lexical analysis [0.7,0.3,0.8,0.7,0.4,0.3,0.1,0.9,0.7,0.6, \n0.6,0.7,0.9,0.6,0.6,0.2,0.6,0.7,0.7,0.3]. This means that with a 70% probability BRANCH OPTS LOW should \nbe applied and that with a 30% probability of CONSTANT PROP should be applied. In other words apply branch \noptimizations, but do not apply local constant propagation for this method.   4. Infrastructure + Methodology \nHere we describe the platform and benchmarks used as well as the methodology employed in our experiments. \n4.1 Platform We implement our learned heuristic in the Jikes Research Virtual Machine [3] version 2.3.3 \nfor an Intel x86 architecture. The Intel processor is a 2.6GHz Pentium-4 based Red Hat Linux workstation \nwith 500M RAM and a 512KB L1 cache. We used the FastAdap\u00adtiveGenMS con.guration of Jikes RVM, indicating \nthat the core virtual machine was compiled by the optimizing compiler, that an adaptive optimization \nsystem is included in the virtual machine, and the generational mark-sweep garbage collector was used. \n 4.2 Benchmarks We examine two suites of benchmarks. The .rst is the SPECjvm98 benchmarks [19] which \nwere run with the largest data set size (called 100). These benchmarks are described in Table 3. The \nsecond set of programs consists of 5 programs from the DaCapo benchmark suite [5], ipsixql, and SPECjbb2000 \nand are described in Table 4. The DaCapo benchmark suite is a collection of programs that have been used \nfor various different Java perfor\u00admance studies aggregated into one benchmark suite. We ran the DaCapo \nbenchmarks under its default setting. We also included a program called ipsixql that performs XML queries \nand a modi.ed version of SPECjbb2000 (hence, it is referred to as pseudojbb) that performs a .xed number \nof transactions. We refer to these 7 bench\u00admarks collectively as DaCapo+. 4.3 Optimization Levels We \nran experiments under each of the three different optimization levels, that is, O0, O1, and O2 as well \nas the adaptive compila\u00adtion scenario. When running under a particular optimization level we compile \nall methods using only the optimizations available at that level. The logistic regressor chooses which \nsubset of these op\u00adtimizations to apply to each method being compiled. Although we speci.cally train \nonly for the O0, O1, and O2 levels, we also evaluated the adaptive scenario which uses each of these \nthree levels. Under the adaptive scenario, all dynamically loaded methods are .rst compiled by the non-optimizing \nbaseline compiler that converts bytecodes straight to machine code without performing any optimizations. \nThe resultant code is slow, but the compilation times are fast. The adaptive optimization system then \n  Program Description antlr Parses one or more grammar .les and generates a parser and lexical analyzer \nfor each fop Takes an XSL-FO .le, parses it and formats it, generating a PDF .le  ipsixql Performs queries \nagainst a persistent XML docu\u00ad ment pseudojbb SPECjbb2000 modi.ed to perform a .xed amount of work Table \n4. Characteristics of the DaCapo+ benchmarks. uses online pro.ling to discover the subset of methods \nwhere a sig\u00adni.cant amount of the program s running time is being spent. These hot methods are then recompiled \nusing the optimizing compiler. These methods are .rst compiled at optimization level O0, but if they \ncontinue to be important they are recompiled at level O1 and .nally at level O2 if warranted. The individual \noptimization levels makeup the adaptive compiler and it is therefore important to tune these individual \nlevels properly. When using logistic regression un\u00adder the adaptive scenario, we used a logistic regressor \ntrained for each of the three different optimization levels. 4.4 Measurement As well as the different \ncompiler scenarios we also considered two different optimization goals namely: total time and running \ntime. Total time is a combination of running and compilation time. As compilation is part of the total \nexecution time for dynamic compilers then optimizing for total time will try to minimize their combined \ncost. However, when the program is likely to run for a considerable length of time, it may be preferable \nfor the user to reduce the running time at the expense of potentially greater compilation time. We therefore \ninclude running time for our benchmarks which is the execution time of the program without compilation \ntime. Each benchmark was run multiple times and the minimum execution time is reported. 3 4.5 Evaluation \nMethodology As is standard practice, we learn over one suite of benchmarks, commonly referred to in the \nmachine learning literature as the training suite. We then test the performance of our tuned heuristic \nover another unseen suite of benchmarks, that we have not tuned for, referred to as the test suite. These \nis achieved in two separate experiments. Leave one out cross-validation We .rst use a standard approach \nto evaluate a machine learning technique called leave-one-out cross-validation on the SPECjvm98. Given \nthe set of n = 7 bench\u00admark programs, in training for benchmark i we train (develop a heuristic) using \nthe training set from the n - 1 = 6 other bench\u00admarks, and we apply the heuristic to the test set, the \nith benchmark. So if we wish to test our technique on the compress bench\u00admark, we .rst train using the \nresults from all programs except compress. This way we never cheat in evaluating our heuristic on a program \nby having prior knowledge of that program. 3 For total time, we ran each benchmark once and repeated \nthis at least 5 times. For running time, we ran SPECjvm98 benchmarks 26 times remov\u00ading the .rst run \nwhich includes the compilation costs of the program. For DaCapo+, which are longer running programs, \nwe ran each program at least 5 times. Testing on DaCapo+ To provide a different evaluation, we trained \nour heuristic on all the SPECjvm98 benchmarks and ap\u00adplied it to an entirely new benchmark suite, DaCapo+,ofwhich \nthe heuristic has no prior knowledge. The training set and the test suite DaCapo+ are entirely distinct. \n 4.6 Training For training, we exhaustively generated all optimization con.gura\u00adtions for optimization \nlevels O0 and O1. Given optimization level O0 consists to 4 optimizations and optimization level O1 consists \nof another 5 additional optimizations, this gives us 16 and 512 pos\u00adsible settings, respectively. Optimization \nlevels O0 and O1 are sub\u00adsets of O2, so we were able to use the exhaustively enumerated data for training \na logistic regressor for O2. We randomly enumerated an additional 1000 optimization con.gurations to \ntrain the O2 logis\u00adtic regressor. For each optimization con.guration we recorded total execution times \non a per method basis. These timings were used for training the logistic regressors where we select the \noptimization setting which gave the smallest total execution time. Selection threshold Each logistic \nregression model returns the probability whether an optimization should be applied. We there\u00adfore have \nto make a decision as to the probability threshold be\u00adyond which we will apply the optimization. If p \n= 0.5 then the optimization can equally be applied or not -there is no conclusive decision. In our experiments, \nwe make the conservative assumption that p > 0.6 before we apply an optimization. Some investigation \nshowed the value 0.6 to be a reasonable value to use. Of course it is also possible to learn the ideal \nthreshold, but this is beyond the scope of the paper.  5. Results In this section, we apply our trained \nlogistic regressors to our benchmark suites and compare their performance against using the default heuristic. \nThe default heuristic is to apply all 4 optimiza\u00adtions at level O0, all 9 optimizations at level O1, \nand all 20 opti\u00admizations at level O2. For each of the three different optimization levels available \nin the Jikes RVM compiler(O0, O1, and O2), we have a speci.cally trained regressor that replaces the \ndefault heuris\u00adtic. We .rst discuss our results applying our logistic regressors to optimize SPECjvm98, \nthen discuss our results optimizing Da-Capo+. We note here that all timings include computation of fea\u00adtures \nfor each method which is a simple linear pass as well as the computation involved in applying our learned \nheuristic. The cost of computing features and applying our heuristic function is typically less than \n1% of total compilation costs. 5.1 SPECjvm98 We now discuss the performance of our logistic regressors \non the SPECjvm98 benchmarks relative to the default settings. The results are presented in Figures 4(a) \nthrough 4(d). Opt Level O0: We .rst applied logistic regression to optimiza\u00adtion level O0. Under this \nscenario, we reduce average total time down by 4%. Most notably we reduce the total time of mpegaudio \nby 23%. It is worth noting that these results are similar to the results in Figure 1 where we show the \nresults of applying the best setting found for each method. This provides evidence that our logistic \nre\u00adgressor has learned the correct .ag settings from this data set and is also an indication that the \nfeatures used make our data linearly sep\u00adarable. Thus, because we are able to selectively apply optimizations \nto each method, we are inhibiting optimizations when they degrade performance and only applying them \nwhen they are bene.cial. We also get a signi.cant reduction in running time of 5% on average. Again, \nwe signi.cantly reduce running time of mpegaudio by 26%. Along with mpegaudio, we are able to reduce \nrunning time of 3 other programs. Opt Level O1: For optimization level O1, our learned models give us \nbetter total time and running time on average, 3% and 2% respectively. Again, if we look at the results \nwhen applying the best .ag found for each method, we see there is only a modest improvement we can achieve \non total time or running time over the default. Opt Level O2: For optimization level O2, we achieve signi.\u00adcant \nimprovements over the default. Our logistic regressor learned that a substantial number of optimizations \nat this level do not im\u00adpact performance and therefore should not be applied.4 Thus, it is possible to \nsigni.cantly reduce compilation time and total time over the default. This comes at no change in running \ntime on av\u00aderage. Thus, we are able to reduce total time of this scenario sub\u00adstantially (by 29% on average) \nwith no change in running time on average. Adaptive: The standard model of execution used by today s \nmodern Java JIT compiler is an adaptive scenario. Under the adap\u00adtive scenario, Jikes RVM uses multi-level \nselective optimization, that is, multiple optimization levels are used and the most impor\u00adtant methods \nare optimized multiple times, each time with at suc\u00adcessively higher optimization level. When a method \nis optimized at a particular level, the logistic regression trained for that level can be used in place \nof the default heuristic. Under the adaptive scenario, our learned models reduces total time on average \nby 1%. This comes from improvements in total time for compress (3%), jess (3%), and javac (2%). We were \nalso able to reduce the running time of most SPECjvm98 benchmarks, up to 5% for compress and javac. This \nleads to an average de\u00adcrease of 2% over the default. There are several reasons why we are unable to \nimprove performance under the adaptive scenario for these benchmarks. First, under the adaptive scenario, \nthe optimiz\u00ading compilers is only used for a small proportion of all methods compiled. Because most methods \nin this scenario are compiled with the baseline compiler we do not bene.t from reductions in compile \ntime. Second, the adaptive setting in Jikes RVM has been highly tuned for the performance of SPECjvm98 \nbenchmarks. Therefore, there is little room for improvement with regards to running time on top of the \nextensive hand-tuning that has already been done. 5.2 DaCapo+ Next, we applied the logistic regressors \nthat were trained using SPECjvm98 training data to the DaCapo benchmark suite and two additional benchmarks, \nipsixql and pseudojbb. The results are presented in Figures 5(a) through 5(d). For optimization level \nO0, we obtain average decreases of 3% for running time and 2% for total time. We get substantial decreases \nin running time for fop (9%), jython (5%), and antlr (8%) and for these benchmarks we also achieve some \nbene.t in total time. For optimization level O1, we get an even larger decrease in running time of 13% \non average with a smaller decrease in total time of 2% on average. At this level, we can decrease the \nrunning time of jython signi.cantly (11%) and we get a large decrease in running time for antlr at 63%. \nWe do get a slow down for two programs, fop and pmd, however because of a large decrease from antlr our \naverage is still signi.cantly better than the default. Again, we see substantially improved performace \nwith optimization level O2 using our regressors. We achieve a large decrease in average total time of \n33% and an even more dramatic decrease in average running time of 56%. Clearly many optimizations performed \nby default degrade per\u00adformance. By selectively applying optimizations at level O2 (and 4 We show this \nin more detail in Section 6  !\" #!  !\" #!              (a) Logistic Regressor for \nOpt Level O0 on SPECjvm98 (b) Logistic Regressor for Opt Level O1 on SPECjvm98 !\" #!        \n      (c) Logistic Regressor for Opt Level O2 on SPECjvm98 (d) Logistic Regressor for adaptive on \nSPECjvm98 Figure 4. Performance of logistic regressors for the SPECjvm98 benchmarks. Table 1 lists the \noptimizations controlled by the regressor at each optimization level. The results are relative to the \ndefault setting, that is, a 1.0 indicates performance equal to the default setting and below 1.0 is performance \nbetter than the default. to a lesser extent at level O1), we can improve running and total time by 4%. \nWe get smaller decreases in running time leading to a time signi.cantly. Note for optimization levels \nO1 and O2, we can 1% decrease on average. decrease running time of DaCapo+ benchmarks much more than \nSPECjvm98 benchmarks. We believe this is due to the Jikes RVM 6. Discussion compiler being highly tuned \ntoward the SPECjvm98 benchmarks. In effect, the optimization heuristics in the Jikes RVM compiler This \nsection discusses the optimizations that are applied by the have been specialized to the SPECjvm98 benchmarks. \nIn contrast, logistic regressors for the different optimization levels. Tables 5, using logistic regression \nallows us to construct heuristics that are 6, and 7 show the percentages that each optimization was ap\u00admore \ngeneral and that can signi.cantly improve performance on plied at the different optimization levels for \nthe hot methods of unseen benchmarks. the SPECjvm98 benchmarks.5 To calculate these percentages we Finally, \nwe apply our learned models under the adaptive sce-counted the number of times each optimization was \napplied at a nario to our DaCapo+ suite. For most benchmarks we get a decrease speci.c level when compiling \nthe hot methods of a benchmark in total time. For fop and ps, we improve total time by at least 10% and \nfor ps we get a 5% improvement. On average, we decrease total 5 Here, we de.ne the hot methods of a benchmark \nto be those methods that run long enough to trigger recompilation in adaptive mode.  # $ % # # $ \n% #  (a) Logistic Regressor for Opt Level O0 on DaCapo+ (b) Logistic Regressor for Opt Level O1 on \nDaCapo+ # $ % # \" #    ! \"  !             (c) Logistic Regressor for Opt Level O2 on \nDaCapo+ (d) Logistic Regressor for adaptive on DaCapo+ Figure 5. Performance of logistic regressors for \nthe DaCapo+ benchmarks. Table 1 lists the optimizations controlled by the regressor at each optimization \nlevel. The results are relative to the default setting, that is, a 1.0 indicates performance equal to \nthe default setting and below 1.0 is performance better than the default. and divided by the total number \nof hot methods compiled for that not very important at optimization level O0 (with the exception benchmark. \nof javac). However, at optimization level O1 this optimization be- The tables show that the importance \nof each optimization differs comes very important to most of the benchmarks. This is due to the for the \ndifferent benchmarks. For instance, Table 5 shows that enabling effect BRANCH OPTS LOW has on other optimizations \nCONSTANT PROP is applied to 81% of raytrace s hot methods at this level. and is therefore important to \nthat benchmark while it is only applied Tables 6 and 7 show that many optimizations are not important \nto 57% of jack s hot methods and is therefore not as important for all benchmarks. These optimizations \ncan perhaps be removed or for that benchmark. On the other hand, CSE is more important turned off by \ndefault with little or no effect on overall performance. to jack (applied 84%) than raytrace (applied \n66%). This indicates The tables also show which optimizations are consistently impor\u00adthat certain optimizations \nare more important for some benchmarks tant for all benchmarks. This can give an indication to JVM de\u00adand \nnot for others further motivating the need for method-speci.c signers which optimizations should be given \nhighest priority when optimization con.gurations. developing new JIT compilers. Tables 5 and 6 show that \nthe importance of an optimization depends on the other optimizations it might enable or disable at that \noptimization level. For example, BRANCH OPTS LOW is        jess 0 CONSTANT PROP 40 25 81 75 95 \n70 57 CSE 20 75 66 88 81 78 84 REORDER CODE 0 16 59 75 81 44 65 Table 5. The percent of level O0 optimizations \napplied to the hot methods of each benchmark.     compress javac 75 73 CONSTANT PROP 50 80 29 CSE \n100 91 100 REORDER CODE 50 4 67 COPY PROP 100 100 100 100 100 100     0 87 34 0 31 20 100 90 69 \n100 36 95 100 Table 6. The percent of level O1 optimizations applied to the hot methods of each benchmark. \n Optimizations compress jess raytrace db javac mpegaudio jack BRANCH OPTS LOW 100 78 88 100 96 98 91 \nCONSTANT PROP 0 11 25 50 24 49 60 CSE 50 2 35 17 26 70 9 REORDER CODE 100 27 70 83 7 92 97 100 100 100 \n100 100     25 96 54 69 98 38 3 50 0 49 26 17     69 14 100 23 100 100 100 100 100 25 25 3 6 \n33 41 2 30 Table 7. The percent of level O2 optimizations applied to the hot methods of each benchmark. \n7. Related Work There have been a number of paper aimed at using machine learn\u00ading to tune individual \noptimization heuristics. Moss et al. [16] published one of the .rst papers showing that machine learning \ncould successfully construct effective and ef.\u00adcient compiler heuristics. They used supervised learning \ntechniques to construct a heuristic function for instruction scheduling. The au\u00adtomatically constructed \nheuristic was able .nd schedules that per\u00adformed as well as a highly-tuned hand-crafted instruction sched\u00aduler. \nCalder et al. [6] used supervised learning techniques, namely decision trees and neural networks, to \ninduce static branch predic\u00adtion heuristics. Our learning methodologies are similar, but there are important \ndifferences. First, they began with a rich set of hand\u00adcrafted heuristics from which to derive their \nfeatures. In contrast, we had no pre-existing heuristics from which to draw features. Sec\u00adond, their \ntechnique made it inherently easy to determine a label for their training instances. The optimal choice \nfor predicting a branch was easily obtained by instrumenting their benchmarks to observe each branch \ns most likely direction. We obtained our labels using method timings as we discuss in Section 4.6. Also, \nbecause our timing measurements are imprecise and we do not take interaction effects of different methods \ninto account, it is impossible to deter\u00admine the optimal choice of whether or not to apply an optimization. \nStephenson et al. [20] used genetic programming (GP) to tune heuristic priority functions for three compiler \noptimizations: hyper\u00adblock selection, register allocation, and data prefetching within the Trimaran s \nIMPACT compiler. For two optimizations, hyperblock selection and data prefetching, they achieved signi.cant \nimprove\u00adments. However, these two pre-existing heuristics were not well implemented and most of the improvements \ncame from producing 400 random heuristics and choosing the best heuristic from this group. The authors \neven note that turning off data prefetching com\u00adpletely is preferable and reduces many of their signi.cant \ngains. For the third optimization, register allocation, iterating the GP im\u00adproved over the initial population. \nHowever, for this optimization they were only able to achieve on average a 2% increase over the manually \ntuned heuristic. Stephenson et al. [21] use machine\u00adlearning to characterize the best unroll loop factor \nfor a given loop nest, and improve overall by 1% over the ORC compiler heuris\u00adtic with SWP enabled. Both \nof these approaches are successful in automatically generating compiler heuristics for a single optimiza\u00adtion. \n Cavazos et al. [7] describe an idea of using supervised learning to control whether or not to apply \ninstruction scheduling. They induced heuristics that used features of a basic block to predict whether \nscheduling would bene.t that block or not. Using the induced heuristic, they were able to reduce scheduling \neffort by as much as 75% while still retaining about 92% effectiveness of scheduling all blocks. However, \nthey were unable to reduce the total execution time for the SPECjvm98 benchmark suite. Monsifrot et al. \n[15] use a classi.er based on decision tree learning to determine which loops to unroll. They looked \nat the performance of compiling Fortran programs from the SPEC 95 benchmark suite and some computational \nkernels using g77 for two different architectures, an UltraSPARC and an IA64. They showed an average \nimprovement over g77 s hand-tuned unroll heuristic of 3.1% and 2.7% on the IA64 and UltraSPARC, respectively. \nLagoudakis et al. [13] describe an idea of using features to choose between algorithms for two different \nproblems, order statis\u00adtics selection and sorting. The order statistics selection problem consists of \nan array of n (unordered) numbers and some integer in\u00addex i,1 <= i <= n. The problem involves selecting \nthe number that would rank i-th in the array if the numbers were sorted in ascending order. The authors \nused reinforcement learning to choose between two well-known algorithms: Deterministic Select and Heap \nSelect. The learned algorithm outperformed both these algorithms at the task of order statistics selection. \nThe second problem they look at is the sorting problem, that is, the problem of rearranging an array \nof n (unordered) numbers in ascending order. Again, the authors used reinforcement learning to choose \nbetween two algorithms: Quick\u00adsort and Insertion Sort. The learned algorithm again was able to outperform \nboth of these well-known algorithms. Rather than optimizing a single heuristic, others have looked at \nsearching [22, 10, 9, 8, 12, 17, 14] for the best set or sequence of optimizations for a particular program. \nCooper et al. [9] propose a number of algorithms to solve the compilation phase ordering prob\u00adlem. Their \ntechnique searches for the best phase order of a partic\u00adular program. Such an approach gives impressive \nperformance im\u00adprovements but has to be performed each time a new application is compiled. While this \nis acceptable in embedded environments, it is not suitable for dynamic compilation. Kulkarni et al. [12] \nintroduce techniques to allow exhaustive enumeration of all distinct function instances that would be \npro\u00adduced from the different phase-orderings of 15 optimizations. This exhaustive enumeration allowed \nthem to construct probabilities of enabling/disabling interactions between the different optimization \npasses. Using these probabilities, they constructed a probabilistic batch compiler that dynamically determined \nwhich optimization should be applied next depending on which one had the highest probability of being \nenabled. This method however does not con\u00adsider the bene.ts each optimization can potentially provide \nwhen applied. In contrast, we train our logistic regression on the best optimizations found for each \nmethod, and therefore our technique learns which optimizations are bene.cial to apply to unseen methods \nwith similar characteristics. However, the techniques pre\u00adsented in this work would allow a larger exploration \nof the opti\u00admization space than we attempted. By exploring a larger part of the search space, we would \nlikely improve the data used for training our logistic regressors. Pan et al. [17] partitioned a program \ninto tuning sections and then developed fast techniques to .nd the best combination of optimizations \nfor each of these tuning section. They are able to reduce the time to .nd good optimization settings \nfrom hours to minutes. This technique, although useful in static compilers (especially those targetting \nembedded processors), is not applicable to dynamic compilers where large optimization times can easily \noutweight any bene.ts gained from the optimizations. However, these techniques could also be bene.cial \nduring the training data generation stage of our logistic regressor technique. Speci.cally, the technique \nto test different optimization settings on a tuning section during a single run of the program would \nallow us to increase the number of optimization settings we evaluate. This would also improve the quality \nof the training data we used for our logistic regressors. Agakov et al. [1] show that the iterative compilation \nsearch space can be reduced by learning from other programs. They con\u00adstruct a set of probabilities, \ncalled search distributions, for a set of training programs and use features to choose with nearest neigh\u00adbor \nwhich search distribution to use for a new program. However, this approach is to select optimization \ncon.gurations for a whole program and still requires multiple runs of the program to achieve signi.cant \nimprovement. In the area of predictive modelling, Zhao et al. use manually constructed cost/bene.t models \nto predict whether to apply PRE or LICM[24]. They achieve 1% to 2% improvement over always applying an \noptimization, but at a cost of greatly increasing compi\u00adlation time (by up to 68%). Because it is expensive \nto apply, their predictive models would not be bene.cial in a dynamic compila\u00adtion setting. Also, their \nmodels appear to be quite complicated and have to be manually constructed. Our models, on the other hand, \nare simple and automatically constructed using machine learning. Yotov et al. [23] describe a model-based \napproach for optimiz\u00ading BLAS libraries. They show that using a model-based approach to evaluate the \nperformance of an optimization can be as effective as empirical evaluation. Again, their models are complicated \nand require manual tuning. In contrast, our regressor models are auto\u00admatically constructed and have \nthe potential to outperform hand\u00adtuned models. Lau et al. [14] present an online framework, called performance \nauditing, that allows the evaluation of the effectiveness of opti\u00admization decisions. The framework allows \nfor online empirical op\u00adtimization, which improves the ability of a dynamic compiler to increase the \nperformance of optimizations while preventing per\u00adformance degradations. Instead of using models to predict \nan opti\u00admization s performance they compile different versions of the same method with different optimization \nsettings and then run each of these different versions evaluating their performance empirically on the \nreal machine. Combining these techniques with the tech\u00adniques presented in this paper would be interesting \nfuture work. For example, one could use machine learning to create a small set of optimization settings, \nwhich could be explored online using per\u00adformance auditing. Georges et al. [11] present a technique for \nmeasuring processor\u00adlevel information gathered through performance counters and link\u00ading that information \nto speci.c methods in a Java program. They study method-level phase behavior of Java applications to \niden\u00adtify methods that exhibit similar or dissimilar behavior within the phases. They characterize methods \nusing a number of performance counter events such as cache miss rates, TLB miss rates, branch misprediction \nrates, etc. This information can allow developers to identify performance bottlenecks and to gather insights \non how a Java application interacts with the VM. For our logistic regres\u00adsion techniques, we trained \nusing only static features of a method which are cheaper to collect than dynamic performance counter \nin\u00adformation. However, in future work, we would like to investigate whether dynamic features (perhaps \nin combination with static fea\u00adtures) could improve the predictions of our machine learning tech\u00adniques. \n8. Conclusions This paper has shown that method-speci.c optimization settings can give signi.cant performance \nimprovements within the Jikes RVM JIT compiler. It has also demonstrated that a simple machine learning \ntechnique can automatically derive a predictive model that gives signi.cant performance improvements \nover existing schemes. We show total execution time reductions of 25% and 51% on the SPECjvm98 and DaCapo+ \nbenchmark suites respectively. To our knowledge this is the .rst paper to demonstrate that a predictive \nmodel trained with machine learning can be successfully used as a method-speci.c optimization strategy \nwithin a dynamic compiler. Future work will investigate learning the best ordering of trans\u00adformations \non a per method basis and applying this approach to the adaptive compilation setting. We would also like \nto experi\u00adment with different kinds of features, such as dynamic performance counter features, and perhaps \ninvestigate the use of online empir\u00adical evaluation in order to evaluate a few optimization settings \nfor each method that a learned model predicts as being promising. 9. Acknowledgements Thanks to Dries \nBuytaert for providing a patch allowing us to in\u00adstrument methods to collect .ne grain timing information. \nAlso, thanks to Felix Agakov and Marc Toussaint for suggesting we use logistic regression for this problem. \nFinally, thanks to the anony\u00admous reviewers for their helpful comments on drafts of this paper. References \n[1] F. Agakov, E. Bonilla, J. Cavazos, B. Franke, G. Fursin, M. F. P. O Boyle, J. Thomson, M. Toussiant, \nand C. K. I. Williams. Using machine learning to focus iterative optimization. In Fourth Annual IEEE/ACM \nInterational Conference on Code Generation and Optimization, pages 295 305, New York City, NY, March \n2006. [2] L. Almagor, K. Cooper, A. Grosul, T. Harvey, S. Reeves, D. Subra\u00admanian, L. Torczon, and T. \nWaterman. Finding effective compilation sequences. In Proceedings of the Conference on Languages, Com\u00adpilers, \nand Tools for Embedded Systems (LCTES), pages 231 239, 2004. [3] B. Alpern, C. R. Attanasio, J. J. Barton, \nM. G. Burke, P.Cheng, J.-D. Choi, A. Cocchi, S. J. Fink, D. Grove, M. Hind, S. F. Hummel, D. Lieber, \nV. Litvinov, M. F. Mergen, T. Ngo, J. R. Russell, V. Sarkar, M. J. Serrano, J. C. Shepherd, S. E. Smith, \nV. C. Sreedhar, H. Srinivasan, and J. Whaley. The Jalape no virtual machine. IBM Systems Journal, 39(1):211 \n238, Feb. 2000. [4] C. Bishop. Neural Networks for Pattern Recognition. Oxford University Press, 2005. \n[5] S.M.Blackburn, R.Garner, C. Hoffman, A. M. Khan, K.S. McKinley, R. Bentzur, A. Diwan, D. Feinberg, \nD. Frampton, S. Z. Guyer, M. Hirzel, A. Hosking, M. Jump, H. Lee, J. E. B. Moss, A. Phansalkar, D. Stefanovi\u00b4c, \nT. VanDrunen, D. von Dincklage, and B. Wiedermann. The DaCapo benchmarks: Java benchmarking development \nand analysis. In OOPSLA 06: Proceedings of the 21st annual ACM SIGPLAN conference on Object-Oriented \nPrograming, Systems, Languages, and Applications, New York, NY, USA, Oct. 2006. ACM Press. [6] B. Calder, \nD. Grunwald, M. Jones, D. Lindsay, J. Martin, M. Mozer, and B. Zoren. Evidence-based static branch prediction \nusing machine learning. ACM Transactions on Programming Languages and Systems, 19(1):188 222, January \n1997. [7] J. Cavazos and J. E. B. Moss. Inducing heuristics to decide whether to schedule. In Proceedings \nof the ACM SIGPLAN 04 Conference on Programming Language Design and Implementation, pages 183 194, Washington, \nD.C., June 2004. ACM Press. [8] K. D. Cooper, A. Grosul, T. Harvey, S. Reeves, D. Subramanian, L. Torczon, \n, and T. Waterman. Acme: adaptive compilation made ef.cient. In Proceedings of the Conference on Languages, \nCompilers, and Tools for Embedded Systems (LCTES), 2005. [9] K. D. Cooper, A. Grosul, T. Harvey, S. Reeves, \nD. Subramanian, L. Torczon, , and T. Waterman. Searching for compilation sequences. Tech. report, Rice \nUniversity, 2005. [10] B. Franke, M. O Boyle, J. Thomson, and G. Fursin. Probabilistic source-level optimisation \nof embedded programs. In Proceedings of the Conference on Languages, Compilers, and Tools for Embedded \nSystems (LCTES), 2005. [11] A. Georges, D. Buytaert, L. Eeckhout, and K. D. Bosschere. Method\u00adlevel phase \nbehavior in java workloads. In OOPSLA 04: Proceedings of the 19th annual ACM SIGPLAN conference on Object-oriented \nprogramming, systems, languages, and applications, pages 270 287, New York, NY, USA, 2004. ACM Press. \n[12] P. A. Kulkarni, D. B. Whalley, G. S. Tyson, and J. W. Davidson. Exhaustive optimization phase order \nspace exploration. In Fourth Annual IEEE/ACM Interational Conference on Code Generation and Optimization, \npages 306 318, New York City, NY, March 2006. [13] M. G. Lagoudakis and M. L. Littman. Algorithm selection \nusing reinforcement learning. In Proceedings of the 17th International Conference on Machine Learning, \npages 511 518, Stanford, CA, June 2000. Morgan Kaufmann. [14] J. Lau, M. Arnold, M. Hind, and B. Calder. \nOnline performance auditing: using hot optimizations without getting burned. In Proceedings of the ACM \nSIGPLAN 06 Conference on Programming Language Design and Implementation, pages 239 251, Washington, D.C., \nJune 2006. ACM Press. [15] A. Monsifrot and F. Bodin. A machine learning approach to automatic production \nof compiler heuristics. In Tenth International Conference on Arti.cial Intelligence: Methodology, Systems, \nApplications (AIMSA), pages 41 50, Varna, Bulgaria, September 2002. Springer Verlag. [16] J. E. B. Moss, \nP. E. Utgoff, J. Cavazos, D. Precup, D. Stefanovi\u00b4c, C. Brodley, and D. Scheeff. Learning to schedule \nstraight-line code. In Proceedings of Neural Information Processing Systems 1997 (NIPS*97), Denver CO, \nDec. 1997. [17] Z. Pan and R. Eigenmann. Fast automatic procedure-level perfor\u00admance tuning. In IEEE \nPACT, Seattle, WA, September 2006. IEEE Computer Society. [18] R. Pinkers, P. Knijnenburg, M. Haneda, \nand H. Wijshoff. Statistical selection of compiler options. In Proceedings of the IEEE Interna\u00adtional \nSymposium on Modeling, Analysis, and Simulation of Com\u00adputer and Telecommunication Systems (MASCOTS), \npages 494 501, 2004. [19] Standard Performance Evaluation Corporation (SPEC), Fairfax, VA. SPEC JVM98 \nBenchmarks, 1998. [20] M. Stephenson, S. Amarasinghe, M. Martin, and U.-M. O Reilly. Meta optimization: \nImproving compiler heuristics with machine learning. In Proceedings of the ACM SIGPLAN 03 Conference \non Programming Language Design and Implementation, pages 77 90, San Diego, Ca, June 2003. ACM Press. \n[21] M. Stephenson and S. P. Amarasinghe. Predicting unroll factors using supervised classi.cation. In \nThird Annual IEEE/ACM Interational Conference on Code Generation and Optimization, pages 123 134, 2005. \n[22] S. Triantafyllis, M. Vachharajani, N. Vachharajani, and D. August. Compiler optimization-space exploration. \nIn First Annual IEEE/ACM Interational Conference on Code Generation and Optimization, pages 204 215, \n2003. [23] K. Yotov, X. Li, G. Ren, M. Cibulskis, G. DeJong, M. Garzaran, D. Padua, K. Pingali, P. Stodghill, \nand P. Wu. A comparison of empirical and model-driven optimization. In Proceedings of the ACM SIGPLAN \n03 Conference on Programming Language Design and Implementation, pages 63 76, San Diego, Ca, June 2003. \nACM Press. [24] M. Zhao, B. R. Childers, and M. L. Soffa. A model-based framework: an approach for pro.t-driven \noptimization. In Third Annual IEEE/ACM Interational Conference on Code Generation and Optimization, pages \n317 327, 2005.  \n\t\t\t", "proc_id": "1167473", "abstract": "Determining the best set of optimizations to apply to a program has been a long standing problem for compiler writers. To reduce the complexity of this task, existing approaches typically apply the same set of optimizations to all procedures within a program, without regard to their particular structure. This paper develops a new method-specific approach that automatically selects the best optimizations on a per method basis within a dynamic compiler. Our approach uses the machine learning technique of logistic regression to automatically derive a predictive model that determines which optimizations to apply based on the features of a method. This technique is implemented in the Jikes RVM Java JIT compiler. Using this approach we reduce the average total execution time of the SPECjvm98 benchmarks by 29%. When the same heuristic is applied to the DaCapo+ benchmark suite, we obtain an average 33% reduction over the default level O2 setting.", "authors": [{"name": "John Cavazos", "author_profile_id": "81100096445", "affiliation": "University of Edinburgh, United Kingdom", "person_id": "PP18002688", "email_address": "", "orcid_id": ""}, {"name": "Michael F. P. O'Boyle", "author_profile_id": "81452607145", "affiliation": "University of Edinburgh, United Kingdom", "person_id": "P198272", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1167473.1167492", "year": "2006", "article_id": "1167492", "conference": "OOPSLA", "title": "Method-specific dynamic compilation using logistic regression", "url": "http://dl.acm.org/citation.cfm?id=1167492"}