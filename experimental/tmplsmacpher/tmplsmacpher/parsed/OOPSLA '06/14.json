{"article_publication_date": "10-16-2006", "fulltext": "\n Replay Compilation: Improving Debuggability of a Just-in-Time Compiler Kazunori Ogata Tamiya Onodera \nKiyokuni Kawachiya Hideaki Komatsu Toshio Nakatani IBM Research, Tokyo Research Laboratory 1623-14 Shimo-tsuruma, \nYamato-shi, Kanagawa 242-8502, Japan ogatak@jp.ibm.com Abstract The performance of Java has been tremendously \nimproved by the advance of Just-in-Time (JIT) compilation technologies. However, debugging such a dynamic \ncompiler is much harder than a static compiler. Recompiling the problematic method to produce a diagnostic \noutput does not necessarily work as expected, because the compilation of a method depends on runtime \ninformation at the time of compilation. In this paper, we propose a new approach, called replay JIT compilation, \nwhich can reproduce the same compilation remotely by using two compilers, the state-saving compiler and \nthe replaying compiler. The state-saving compiler is used in a normal run, and, while compiling a method, \nrecords into a log all of the input for the compiler. The replaying compiler is then used in a debugging \nrun with the system dump, to recompile a method with the options for diagnostic output. We reduced the \noverhead to save the input by using the system dump and by categorizing the input based on how its value \nchanges. In our experiment, the increase of the compilation time for saving the input was only 1%, and \nthe size of the additional memory needed for saving the input was only 10% of the compiler-generated \ncode. Categories and Subject Descriptors D.2.5 [Software Engineering]: Testing and Debugging debugging \naids; D.3.4 [Programming Languages]: Processors compilers, debuggers. General Terms Reliability. Keywords \nReplay compilation, deterministic replay, problem determination, Java, JIT compiler, dynamic optimization, \ndebuggability. 1. Introduction Over the last decade, the performance of Java has been tremendously improved. \nUndoubtedly, advances in dynamic compilation technologies have significantly contributed to these improvements. \nJava JIT compilers perform increasingly more advanced, and thus more complicated, optimizations [9,15,20], \nand can even generate more efficient code than static compilers by taking advantage of runtime profiles. \nPermission to make digital or hard copies of all or part of this work for personal or classroom use is \ngranted without fee provided that copies are not made or distributed for profit or commercial advantage \nand that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, \nto post on servers or to redistribute to lists, requires prior specific permission and/or a fee. OOPSLA \n06 October 22 26, 2006, Portland, Oregon, USA. Copyright &#38;#169; 2006 ACM 1-59593-348-4/06/0010 $5.00. \nHowever, a dynamic compiler is much harder to debug than a static compiler. Assume that an application \ncrashed in a production environment, and analysis suggests that the code generated for a certain method \nmay be causing the crash. What will then be the next step? If the application was developed with a static \ncompiler, we can simply recompile the method with an option to produce diagnostic output. The diagnostic \noutput contains all the details of what the compiler does, including what optimizations are applied and \nhow each optimization transforms the code. This greatly helps a compiler writer analyze a bug in the \ncompiler, and the compiler writer can often recognize the bug without re-executing the compiler using \na debugger. Without a diagnostic output, it is very difficult to associate each generated machine instruction \nwith the source code. We could do the same thing when the application is written in Java. More precisely, \nwe could again JIT compile the problematic method by rerunning the application with an option specified \nto produce diagnostic output. However, this does not necessarily work, because the method may not be \ncompiled in exactly the same way. The reason is that the compilation of a method depends not only on \nthe method's bytecode but also on the runtime information at the time of compilation, such as the resolution \nstatus [6] of classes referenced in the method, the class hierarchy, and the runtime profile. This runtime \ninformation is not necessarily the same from run to run because the Java application is multi-threaded, \nand non-determinism in execution is unavoidable. We actually observed that the combination of applied \noptimizations had changed in at least one out of ten executions for each of the Java programs we evaluated \nbecause of changes in the execution order of threads and the runtime profiles. A straightforward solution \nwould be to run an application with the diagnostic option specified even in a production environment. \nHowever, this significantly increases the compilation time and thus the execution time of the application. \nIn addition, forcing the compiler to always generate the diagnostic output would require prohibitively \nlarge amounts of disk space. For example, the diagnostic output for a single execution of a SPECjvm98 \nbenchmark [19] can be in the hundreds of megabytes. In this paper, we propose a new approach for debugging \na JIT compiler, replay JIT compilation, which allows methods to be recompiled in exactly the same way \nas in a previous run. Our approach uses two compilers, the state-saving compiler and the replaying compiler. \nThe state-saving compiler is used in a normal run, and, while compiling a method, records into a log \nall of the runtime information referenced during the compilation. The log is in the main memory, and \nautomatically included in the system dump when the application crashes. The replaying compiler is then \nused in a debugging run with the system dump, to recompile a method with the options for diagnostic output. \nThis technique is a kind of trace-and-replay technique, but we successfully minimized its overhead by \nsaving in memory (not on hard disk) only the additional trace information that will otherwise not appear \nin the system dump. As a result, the system dump will always include all the information required for \nreplaying the JIT compiler to recreate the failure. It is worth noting that using a system dump to reproduce \na problem that crashed a mission-critical application is much better than trying to reproduce the problem \nby recreating the environment in which the application crashed at a remote site. Such an application \ntends to be very complicated to install, configure, and deploy, and may demand substantial hardware resources. \nThus, it would be laborious to set up the same environment at a remote site to reproduce the observed \nproblem. In addition, it may be impossible to obtain the data to run the application if the data includes \nhighly confidential or sensitive information such as credit card numbers. Without the same data, the \napplication will run differently and the problem may not be reproduced. We implemented our prototype \nbased on the J9 Java VM [7] and the TestaRossa (TR) JIT compiler [21,23] for AIX. This prototype also \nimplements confidence-based filtering to save only the logs that are likely to be needed for debugging. \nOur experiment showed that the time overhead for saving the input is only 1%, and the space overhead \nfor saving the input is only 10% of that of the compiled code. To our knowledge, this is the first report \ndescribing how to successfully replay the JIT compilation offline. This paper makes the following contributions: \n Replay JIT compilation: We reduced the overhead for saving the input for the JIT compiler by using a \nsystem dump so that it can always be enabled in a production environment.  Confidence-based filtering: \nWe can reduce the size of the input to save by considering the likelihood that a method may cause an \nerror in the JIT compiler.  The rest of this paper is organized as follows. Section 2 summarizes related \nwork. Section 3 explains the behavior of a dynamic compiler. Section 4 discusses our approach to replaying \nthe JIT compilation using a system dump. Section 5 describes the implementation of our prototype. Section \n6 shows how small the overhead of the replay JIT compilation is in terms of the size of the saved input \nand the time to save the input. Section 7 offers concluding remarks. 2. Related Work Trace-and-replay \nis a common technique for cyclic debugging of multi-threaded programs. There are two approaches for trace-and\u00adreplay, \nthe ordering-based and content-based [17] approaches. The ordering-based approach is to record and replay \nthe order of synchronization events [12], such as locking and message passing. For this approach, many \ntechniques [2,4,12,14] have been developed and discussed to trace and replay the program execution with \nsmall overhead. For a Java JIT compiler, the compiler itself operates deterministically, but the compilation \nresults may change because the input for the compiler may change non-deterministically during the execution \nof the Java program. The input for the JIT compiler is runtime information from the Java VM, and that \ndata is changed by many of the Java operations, such as object allocation, access to a field variable, \nor method invocation. The changes made by other threads immediately change the input for the compiler. \nThus, it is impractical to use the ordering-based approach for a Java VM because recording the order \nof all of those operations is needed to reproduce the input for the JIT compiler. The content-based approach \nis to save and restore the values of the input. Recap [16] records the input for a program. However, \nit is estimated to generate 1 Mbyte of trace per second, even on a slow VAX-11/780 machine, and a faster \nmachine could generate an unacceptably huge amount, such as 1 Gbyte per second. The jRapture system [22] \nrecords the parameters and the return values of a Java API that interacts with the underlying system. \nHowever, their prototype was three to ten times slower than normal execution. The idea of the content-based \napproach is simple and easy to adopt, but it tends to cause prohibitively large overhead in size and \nspeed for use in practical systems. Using a system dump, we successfully minimized the overhead of recording \nthe information required for replaying the JIT compiler. The first error data capture (FEDC) concept \n[11] also aims to improve the debuggability of a mainframe system used in a production environment. It \nuses special hardware, firmware, and software that continuously record information about each component \nin the system by using a separate computer. When an error occurs, support personnel can analyze the recorded \ninformation to debug the error. This is an effective approach for problem determination without replaying \nthe system, but it needs special support by hardware and firmware. Non-determinism in execution can also \ncause problems in performance analysis. For example, we cannot discriminate between the causes of performance \nimprovements because of the non-determinism. OOR [8] and PEP [3] solved this problem by using advice \nfiles produced by the JIT compiler in the previous best run. Those files record the compilation level \nand the results of profilers, and they are used by the complier in a performance measurement run. Their \nmotivation is very close to ours, but they did not describe the details of their implementation. 3. \nBehavior of a Dynamic Compiler The user invokes a static compiler by specifying one or more source files \nand zero or more options as command line arguments. Since these are all of the inputs for the static \ncompiler, the user can reproduce the same compilation simply by supplying the same set of the command \nline arguments. In addition, the user can obtain the details of the compilation by adding an option for \nthe compiler to generate diagnostic output. In contrast, the user does not directly invoke a dynamic \ncompiler. The dynamic compiler is not a standalone tool, but a component of a virtual machine. The user \ninvokes the virtual machine to run an application, and the virtual machine invokes the dynamic compiler \nwhile executing the application. The virtual machine determines at runtime which methods to compile with \nwhat set of options. In addition, as we will soon explain, the dynamic compiler fully exploits information \navailable at runtime to generate highly efficient code. Reproducing the compilation is much harder for \na dynamic compiler for the following reason. The inputs for the dynamic compiler are all provided as \ndata structures in the virtual machine, which will be lost when the virtual machine terminates. For example, \nthe runtime information mentioned above is kept as runtime data structures. Furthermore, this is true \neven for the bytecode of the method to be compiled. The dynamic compiler does not directly obtain the \nbytecode from an external file such as the .classfile in java, but does so from runtime data structures. \nNote that dynamic bytecode generation is becoming popular in the recent versions of Java [5], and there \nmay be no corresponding external files for some compiled methods. 3.1 Runtime Information as Inputs While \ncompiling a method, the dynamic compiler exploits runtime information which can be categorized into three \ntypes, the system configuration data, the virtual machine states, and the runtime profiles. An example \nof system configuration data is whether the underlying system is uniprocessor or multiprocessor. The \ndynamic compiler generates faster code for synchronization for a uniprocessor system. Another example \nis the processor architecture of the underlying system. The dynamic compiler exploits the information \nto generate instructions only available in a specific processor architecture. Note that, with the static \ncompiler, the user can specify system configuration data as command line options. This implies that the \nuser must prepare different executables for different configurations, and pick up the right executable \nbased on the actual execution platform. A common example of the virtual machine states is the resolution \nstatus for external references [6]. The bytecode of a method contains external references to classes, \nfields, and methods. When the virtual machine loads a class, all of the external references are symbolic. \nDuring the execution, references will undergo resolution and become direct references. The dynamic compiler \ngenerates faster code for direct references, and code to force the resolution for symbolic references. \nAnother example is the hierarchy of classes loaded into a virtual machine. The dynamic compiler analyzes \nthe hierarchy to devirtualize method invocations [9]. Devirtualization is one of the most important optimizations \nfor object-oriented programs. The dynamic compilation system is considered to be best positioned for \nthe profile-guided optimizations since it can be made transparent to collect runtime profiles. The runtime \nprofile of a method could be based on block profiling, edge profiling, or path profiling. The dynamic \ncompiler generates optimized code in favor of frequently executed basic blocks, edges, or paths [1,25,27]. \nThe runtime profile may even include value profiles, or distributions of values of arguments and variables. \nThe dynamic compiler then creates specialized versions of a method based on values frequently observed. \n 3.2 Difficulty in Reproducing the Compilation As we mentioned earlier, the user invokes a virtual machine \nto run an application, which in turn invokes a dynamic compiler. However, it is almost impossible to \nreproduce the compilation of a method simply by re-running the application. The reason is that the modern \nvirtual machine runs an application in a non\u00addeterministic manner. It creates system threads for compilation \nand garbage collection, and runs an application in a multi\u00adthreaded environment. Thus, different runs \nof an application may result in different methods being compiled. Even if a method is compiled in two \nruns of an application, the compiled code may not be the same because the inputs for the dynamic compiler \nmay not be identical. For example, the system configuration data will be different if the virtual machine \nruns the application on different execution platforms. The virtual machine states and runtime profiles \nmay be different because of the non-determinism in the way the virtual machine runs the application. \nTo reproduce the compilation of a method, we need to be able to do the following things. First, we need \nto be able to invoke a dynamic compiler as if it were a stand-alone tool, since different runs of an \napplication may result in different methods being compiled. Second, we need to be able to reproduce the \ninputs including the runtime information exploited at the compilation. This can normally be done by saving \nthe values of those inputs, and restoring them for the reproducing compilation. 3.3 Variable and Fixed \nInputs We can categorize the inputs for a dynamic compiler into two types, variable inputs and fixed \ninputs. If the inputs may change after a compilation of a method, we call them variable inputs. Otherwise, \nwe call them fixed inputs. For example, the resolution statuses of the external references are variable \ninputs. Also, the class hierarchy is variable input since new classes may be loaded after the compilation. \nOn the other hand, the system configuration data is fixed input. Also, the bytecode of the method can \nbe a fixed input if the virtual machine prohibits the rewriting of the bytecode. Table 1 of Section 4.2 \nshows a more detailed summary of inputs for the dynamic compiler, including whether they are variable \nor fixed. Among the four types of input (the target method, the system configuration, the virtual machine \nstates, and the runtime profiles), the first two types are fixed inputs and the others are variable inputs. \nNote that, while the values of the variable inputs must be saved at the time of compilation, those of \nthe fixed inputs can be saved at the arbitrary time.  4. Overview of Replay Compilation We use the \ncontent-based trace-and-replay technique to reproduce the compilation by a dynamic compiler. Concretely, \nwe create two versions of a dynamic compiler, the state-saving compiler and the replaying compiler. In \nour approach, the virtual machine invokes the state-saving compiler to dynamically compile methods. This \ncompiler saves all of the inputs for each compilation into a log. Later, the user invokes the replaying \ncompiler by specifying a method to be replayed. The replaying compiler reproduces the compilation of \nthe method by restoring all of the inputs for the compilation from the corresponding log. Since the virtual \nmachine invokes the state-saving compiler while running an application, the overhead of the compiler \nmust be sufficiently small both in terms of time and space. As we will show, we employ a cascade of techniques \nto reduce the overhead. Note that the replay compilation is meant to support the debugging of a dynamic \ncompiler. Assume that a problem occurred while a Java application is running. The replay compilation \nis not a tool for analyzing the problem in general. Instead, it should be used when the analysis of the \nproblem suggests that an execution of a compiled method caused the problem, and that the dynamic compiler \nfailed to generate the code correctly. 4.1 System Dump Based Approach We assume that the virtual machine \nis configured to generate a system dump at crashes and user interrupts. As a core file in UNIX systems \n[24], a system dump contains the memory image of an OS process. Exploiting this assumption, the state-saving \ncompiler creates the logs for compilations in the main memory, not explicitly writing them into a file. \nThe logs are then automatically saved into a system dump when the operating system creates one. Avoiding \nexpensive I/O during compilation significantly helps reduce the time overhead of the state-saving compiler. \nNote that this system-dump-based approach has a significant advantage. It does not require the virtual \nmachine to be invoked to rerun an application. It suffices to invoke the replaying compiler with the \nsystem dump. Furthermore, the platform where the replaying compiler is invoked does not have to be identical \nto the platform where the system dump was generated. Thus, one of the scenarios which are only made possible \nby our approach is as follows. The customer invokes the virtual machine in a production environment which \nruns a mission critical application, invoking the state-saving compiler. The virtual machine crashes, \nand a system dump is generated. The customer sends the system dump to the support personnel at a different \nsite. They invoke the replaying compiler in their environment to fix a problem in the compiler. Developers \nof a dynamic compiler can also benefit from our approach. Assume that a test case is highly multi-threaded \nand thus runs in a very non-deterministic manner. Obviously, it is hard to reproduce an error in such \na test case by rerunning it. With replay compilation, developers do not have to run the test case repeatedly. \nThey only have to invoke the replaying compiler.  4.2 Log Structure In general, the dynamic compiler \ngets the inputs for a compilation, whether variable or fixed, by accessing data structures and calling \nfunctions. For an input by data structure access, the state-saving compiler records a pair of the address \nand the value into a log. Later, the replaying compiler retrieves the value from the log, by using the \naddress as the key. For an input in a function call, the state-saving compiler records into a log the \nreturn value together with a list of function identifier and zero or more parameter values. The replaying \ncompiler retrieves the return value from the log by using the list as a key. The system-dump-based approach \nmakes an optimization possible for a fixed input by data structure access. As mentioned in Section 3.3, \nwhile the values for variable inputs must be saved at the time of the compilation, the values for fixed \ninputs can be saved at the arbitrary time. In particular, the values for fixed inputs can be saved at \nthe time of creating the system dump. Thus, the state\u00adsaving compiler does not have to save anything \nfor fixed inputs during compilation, simply relying on the system dump that will  Table 1. Types of \ninput used by the Java JIT compiler and the values to be saved in a log Type of input Input for the JIT \ncompiler Value to be saved into a log How the JIT compiler uses the input Target method (fixed input) \nBytecode, literals, and the metadata for external references Address of the data structure associated \nwith the target method The JIT compiler reads these inputs as source code. System configuration (fixed \ninput) Configuration of the hardware and the software Model, cache size, number, and specifications of \nthe processors in the machine, and the type and version of the operating system The JIT compiler can \ngenerate code that can run faster in a specific environment than generic code. Command line options and \nthe environment variables Address of the data structure that holds the parsed command line options and \nthe environment variables Those options may change the compilation process for all or particular methods. \nVirtual machine states (variable input) Set of classes that are referred to and that have been initialized \nA flag for each class indicating if the class has been initialized When the class has already been initialized, \nthe JIT compiler can generate faster code, since the generated code need not handle the initialization. \nAddress of the compiled code Addresses of the compiled code invoked from the method being compiled The \nJIT compiler can generate code that directly calls the compiled code of the callee method if it is already \ncompiled. Saved results of the JIT optimizations Addresses of the classes that hold the results of the \ninter-procedural analysis The JIT compiler can reuse the saved results of inter\u00adprocedural analysis to \nreduce compilation time. Class hierarchy of the loaded classes A set of the parameters and the return \nvalue of each function call for devirtualization The JIT compiler can use the class hierarchy analysis \nto devirtualize the method invocation of virtual and interface methods. Resolution statuses A bitmap \nindicating which of the external references have been resolved For each resolved reference, the JIT compiler \ncan generate faster code, since the generated code need not handle the resolution. The JIT compiler may \nbe able to inline the callee method when the reference to it has been resolved. Runtime profiles (variable \ninput) Runtime profiler output The values of the runtime profiles The JIT compiler will apply more aggressive \noptimizations to the frequently executed path, or can generate code that is specialized for the frequently \nappearing values. Optimization level A value of the optimization level that is determined based on the \nruntime profile The JIT compiler selects the set of optimizations to apply based on the optimization \nlevel that was determined based on the profiler output. macros, one for the state-saving compiler and \nthe other for the replaying compiler. The definitions for the state-saving compiler will store the inputs \ninto a log as well as return the inputs, while the definitions for the replaying compiler will retrieve \nthe inputs from a log. In this way, we can derive the two compilers from a single source. Figure 2 illustrates \nthe details. Figure 2 (a) shows the code of the conventional compiler. Receiving the name of the target \nmethod, the function compileobtains the number of available processors, a data structure md associated \nwith the target method, the addresses of the bytecode, and a bitmap that holds the resolution status. \nWe assume that the number of the available processors and the bytecode are the fixed inputs, while the \nothers are the variable inputs. Figure 2 (b) presents the macro version for replay compilation. We wrap \nwith a macro every piece of the code which obtains an input. We use different macros, depending on whether \ninputs are variable or fixed and whether inputs are obtained by accessing data structure or by calling \nfunctions. Figure 2 (c) lists the definitions of the macros for the state-saving compiler. Because of \nthe optimization mentioned in Section 4.2, the getFixedInputByDataAcc macro is simply defined to do the \nsame operation as in Figure 2 (a), and not to save anything into a log. Figure 2 (d) shows the definitions \nfor the replaying compiler. The getFixedInputByDataAcc macro is defined to get the input from the system \ndump, not from the log. As we will explain in Section 4.4, we may need to adjust the address appropriately. \nThe replay compilation assumes that both of the state-saving and the replaying compilers apply the same \nset of optimizations for the same inputs. Thus, the versions of the source code for the state-saving \nand the replaying compilers must be synchronized. Deriving the two compilers from a single source greatly \nsimplifies this task of synchronization.  4.4 Replaying the Compilation While the virtual machine invokes \nthe state-saving compiler, the user invokes the replaying compiler by specifying one or more compilations \nto be reproduced. This is usually done by specifying method signatures or addresses of compiled code. \nNote that, like a static compiler, the replaying compiler independently reproduces the specified compilations. \nThere is no restriction on the replay order, and we can even replay all of the compilations in the reverse \nof the order in which the sate-saving compiler did. As in Figure 1, it is common that the replaying compiler \nand the virtual machine invoking the state-saving compiler run on different platforms. Using the replaying \ncompiler, the support  (a) an example of the code in a base compiler (c) the definitions of the macros \n(state-saving compiler) (b) the code modified for the replay compilation (d) the definitions of the macros \n(replaying compiler) Figure 2. An example of the code to get the input for the compiler personnel do \nnot have to create exactly the same platform as the state-saving compiler used. This significantly reduces \nthe cost for the support division. When it is invoked, the replay compiler first loads the system dump \ninto its address space. It then attempts to reproduce a compilation by retrieving the inputs for the \ncompilation. A tricky part of the replaying compiler is that if an input retrieved from the log is an \naddress, it is the address in the process for the state\u00adsaving compiler (state-saving process). The replaying \ncompiler cannot use the address to retrieve the value of a fixed input, since the system dump is not \nnecessarily loaded at the same address as the sate-saving process. This is the reason why we need the \nadjustment in the getFixedInputByDataAcc macro. We may be able to avoid this adjustment as follows. Assume \nthat we can know the address range in the system dump where fixed inputs reside. If the replaying compiler \ncan reserve the address range at start-up, it can load the data from the system dump into the address \nrange, restoring the fixed inputs at the same addressees. However, it depends on the underlying operating \nsystem and the details of how the process for the replaying compiler is initialized.  4.5 Further Reducing \nthe Size of Logs The overhead of the state-saving compiler must be small enough both in terms of time \nand space, since the virtual machine invokes it while running an application. Exploiting the system-dump\u00adbased \napproach, the state-saving compiler allocates logs in the main memory, and skips saving fixed inputs \nby data structure access. The former significant help contributes to reducing the time overhead, while \nthe latter contributes to reducing both the time and space overhead. Here we show three techniques to \nfurther reduce the space overhead. The first technique is to compress the logs in memory. This is simple \nyet very effective in reducing the log size. The second technique is to exploit default values. The state\u00adsaving \ncompiler skips saving into a log an input when the value equals the default value, while the replaying \ncompiler interprets the value of an input as default when it can find in the log no value corresponding \nto the input. Default values should be defined as frequently observed values, and different default values \ncan be defined for different functions and data structure types. The third technique is to skip saving \nall of the inputs for the compilation of a method if we have high confidence in the method, or if we \ncan assume that the method is very likely to be compiled correctly. We call this technique confidence-based \nfiltering. We define the confidence of a method as follows. Increase the confidence of a method, if \nthe method is in heavily used libraries such as the Java core classes  Decrease the confidence of a \nmethod, if the control flow of the method is complex.  The rationale behind the first criterion is that \nmethods in heavily used libraries are frequently compiled and executed, and thus the paths to compile \nthem are already well tested. The second criterion is simply based on our rule of thumb. We often observed \nthat a problem in the compiler appeared when the compiler processed a complex control flow. We can approximate \nthe complexity of the control flow of a method as the number of basic blocks. The time spent on compiling \nthe method is also a good approximation. 4.6 Discussion By definition, the replaying compiler uses exactly \nthe same set of the options for compiling a method as the state-saving compiler used, except an additional \noption for diagnostic output. However, in some cases the support personnel may want to replay a compilation \nwith a slightly different set of options in order to narrow down the cause of a problem. At a first glance, \nthis would be impossible since the replaying compiler may now need to obtain the inputs which the state-saving \ncompiler did not use and thus did not save. However, we can exploit the mechanism of default values described \nin Section 4.5. That is, we simply let the replaying compiler pick up the default value for an input \nwhen it fails to find in the log a value corresponding to the input. The Java virtual machine may unload \nclasses and delete data structures associated with them from the main memory. As a result, the system \ndump will not include the data structures for unloaded classes, such as bytecode for methods. This means \nthat the replaying compiler is unable to replay the compilation of a method in an unloaded class. We \ncould argue that this is not a serious issue, based on the following observation. A class is unloaded \nonly when there is no reference to the class in the virtual machine [13]. That is, there is no instance \nof the class, and no method of the class currently being executed. Assume that a system crash occurred \nand that it was actually caused by incorrectly compiled code for a method. We believe that such a method \nis very likely to have a stack frame, actively being executed. We could also modify the state-saving \ncompiler to store the data structures of a class into an external file when the class is unloaded. We \ncould do so by generating a system dump at the time of unloading if we could unload classes as a batch \nprocess. The state-saving compiler also timestamps the logs for compilations so that the replaying compiler \ncan properly associate the logs and the data structures for unloaded classes.  5. Implementation This \nsection describes our prototypes of a state-saving compiler and a replaying compiler. We implemented \nthe prototype based on the J9 Java VM [7] and the TestaRossa (TR) JIT compiler [21,23] for AIX. 5.1 State-Saving \nCompiler Our state-saving compiler allocates a memory area as the log for each compilation of a method. \nThis compiler compresses each log using zlib library [28]. The log works as if it were a cache, so that \nthe compiler can avoid saving duplicated input that happens to be constant during the compilation. That \nis, even if the state-saving compiler tries to get a variable input multiple times, it actually gets \nthe value only on the first access, and subsequent accesses get the value saved in the log. The state-saving \ncompiler associates each log with the address of the JIT-compiled code. This makes it possible to identify \nthe log for a particular compilation, even if the method is compiled multiple times for different optimization \nlevels and there are multiple compiled code blocks for the method. Our prototype does not support replay \ncompilation for unloaded classes. To defer dealing with this complex issue, we simply disabled class \nunloading in our experiments.  5.2 Replaying Compiler Our prototype restores a system dump into the \nsame address as the state-saving process to avoid the overhead by the adjustment of pointer variables, \nas described in Section 4.4. Our state-saving compiler creates a special data structure, called an anchor \nstructure, so that the replaying compiler can find the important data from a system dump. The data structure \nhas markers in its header and trailer, and contains its size, the version number of the state-saving \ncompiler, and a pointer to the list of logs. The state-saving compiler manages all logs as a linked list, \nand stores the pointer to the head into the anchor. The anchor structure also saves the fixed input that \ncan be determined when the Java VM is initialized, such as the system configuration. The replaying compiler \nscans the markers in the restored system dump, which is a block of unstructured binary data, for finding \nthe anchor structure. When the compiler finds a marker, it verifies the size and the version of the state-saving \ncompiler. It then finds the pointer to the head of the log list, and scans the list for the log corresponding \nto the compilation to be replayed. The anchor structure has another pointer to the address of the log \nfor currently being compiled (called the current log). Since an incomplete log may crash the replaying \ncompiler, the current log Table 2. Configurations of the tested machinesTable 4. Comparison in size: \ndiagnostic output vs. log Machine-1 Machine-2 Machine-3 CPU POWER3, single processor POWER4, 4-way SMP \nPOWER3, 2-way SMP RAM 768 Mbytes 8 Gbytes 768 Mbytes OS AIX 5.2L AIX 5.2L AIX 4.3.3 Table 3. Evaluated \nprograms Program Description mtrt, jess, compress, db, Each of the programs included in the mpegaudio, \njack, javac benchmarks suite SPECjvm98 [19]. SPECjbb The SPECjbb2000 [18] benchmark. xml parser The operation \nof parsing a sample XML file using the XML parser for Java [26]. The sample file is included in the package. \nThe execution performance was measured by the elapsed time for parsing the sample file. jigsaw The operation \nto start the Jigsaw HTTP server release 2.2.5a [10], and load the default top page using a Web browser. \nThe execution speed was not measured because this is an I/O bound program. should not be accessible \nin the list of \"complete\" logs. While the JIT is compiling a method, the pointer holds the address of \nthe current log, and clears it when the compilation has finished successfully. Using this pointer variable, \nthe replaying compiler can tell if the system crashed during a JIT compilation.  6. Experimental Results \nUsing the prototypes of the state-saving and the replaying compilers described in Section 5, we measured \nsize and time overhead for saving the input into logs. Various machine configurations and programs were \nused for the evaluation, as shown in Table 2 and Table 3. For all measurement, we used the exploitation \nof default values described in Section 4.5. The confidence-based filtering is not uses, unless otherwise \nis specified. This prototype forces the Java VM to always create a system dump when it terminates after \nexecuting the specified Java program (because none of the tested program crashes the Java VM). 6.1 Replayed \nCompilation Our prototype successfully reproduced the compilation for all of the methods that were compiled \nfor these programs. We executed the state-saving and the replaying compilers in the same machine in each \nof the three tested machines. We verified that the replaying compiler reproduced the same compilation \nby generating the JIT compiled code at the same address as that of the state-saving compiler and by comparing \nthe generated code with the code saved in the system dump. In addition, we also verified that the replaying \ncompiler successfully reproduced the compilation from the system dump generated by a different machine. \nThe replaying compiler succeeded in replaying all six possible combinations of the machines listed in \nTable 2 to execute the state-saving compiler with the replaying compiler. Program Diagnostic output [MB] \nLog [MB] (not compressed or filtered) mtrt 378 0.054 jess 243 0.077 compress 80 0.011 db 83 0.019 mpegaudio \n200 0.045 jack 360 0.075 javac 588 0.258 SPECjbb 1033 0.222 xml parser 101 0.030 jigsaw 136 0.056 Geo. \nmean 227 0.056  6.2 The Size of a Log Table 4 shows the size of the logs and the size of the diagnostic \noutput for each program. We used neither compression nor the confidence-based filtering for this measurement. \nThis result shows that the size of the diagnostic output is too large to save in memory, and it is also \ntoo large to save on disk because the overhead to save so much data on disk during the execution of the \napplication program will slow down the program. The replay compilation technique reduced the size of \nthe trace information so much that saving input can always be enabled even in a production environment. \nFigure 3 shows how the total size of the log changes due to compression and filtering. The sizes are \nrelative to the total size of the compiled code. The left two bars for each program show the results \nwhen no compression is used, and the right two bars show the results with compression using the zlib \nlibrary. The bars labeled `no filter' (first and third) show the results when the compiler does not use \nthe confidence-based filtering. The bars labeled `filter system classes' show the results when the compiler \nuses filtering of the system classes (the classes in the java.lang, java.util, java.math, and java.iopackages). \nThe reduction in the log size by filtering the system classes was 22.7% and 25.4% without and with compression, \nrespectively. (All percentages are geometric means.) Compression reduced the total size of the logs by \napproximately half, and the reduction made by the combination of compression and filtering was 62.9%. \nAs a result, the geometric mean of the size of the logs was reduced to less than 10% of the compiled \ncode, which was our initial target as being acceptable for many users. Table 5 shows the number of logs \nwhen filtering the methods of the system classes is used or not used. It also shows the reduction in \nthe numbers and the sizes of the logs by filtering. The reduction in size is the summary of Figure 3 \nwhen zlib is not used. The reduction in the number of methods was 38%, but the reduction in the size \nof the logs was only 23%. We think this is because many of the filtered methods are smaller than average \nmethods. Table 6. Comparison in size: system dump vs. Java heap Relative size to compiled code mtrtjesscompressdbmpegaudiojackjavacSPECjbbxml \nparserjigsawGeoMean Figure 4. Increase in compilation time without and with zlib compression Java heap, \nthe Java stack, the native heap, the native stacks of all threads, and the JIT compiled code blocks, \nthough the largest part is the Java heap. Note that the JIT compiler usually does not directly use the \nvalues in Java objects for compiling a method, but mtrtjesscompressdbmpegaudiojackjavacSPECjbbxml parserjigsaw \nGeoMean No zlib, no filter zlib, no filter No zlib, filter system classes zlib, filter system classes \nFigure 3. Reduction in the size of log with zlib and filter Table 5. Reduction in the number of logs \n Program No filter Filter system classes Reduction in size by filtering (no zlib) mtrt 153 113 (-26%) \n-16% jess 153 104 (-32%) -12% compress 39 22 (-44%) -25% db 59 22 (-63%) -38% mpegaudio 161 141 (-12%) \n-9% jack 201 141 (-30%) -16% javac 604 514 (-15%) -7% SPECjbb 502 345 (-31%) -20% xml parser 78 44 (-44%) \n-25% jigsaw 160 64 (-60%) -49% Geo. mean 152 94 (-38%) -23% The total size of the log was less than \n0.1% of the total memory uses the values collected by the runtime profiler. Thus, a large usage of the \nJava VM process for these programs when both portion of this large system dump is not needed by the replaying \nfiltering and compression are used. This is because the size of the compiler. Although a large system \ndump requires a large free Java heap is much larger. Thus, for the measured programs, space on disk, \nthis is considered to be acceptable in many filtering and compression may not be needed for some users \nproduction environments because the system dump is normally because the total size is still less than \nabout 0.3% of the total used for debugging problems that occurred in those environments. memory usage. \nHowever, the number of compiled methods will increase in large commercial applications, such as Web application \nservers, and thus the ratio of the size of the compiled 6.3 Compilation Time Figure 4 shows how much \nthe compilation time is increased over the base compiler when zlib is used. Each bar labeled `no zlib' \ncompiled code is used to directly benefit the user by improving the execution performance of the programs. \nHowever, the log is elapsed time of the compilations that performed the same used only for improving \ndebuggability, while it reduces the optimizations in all executions for the base, `no zlib', and `zlib'. \navailable memory for the user programs regardless of whether or not a problem occurs. The increase in \ncompilation time was up to 2.0% in both cases. The geometric mean of the increase was about 1.0% and \n1.1% for Table 6 shows the sizes of the system dumps and the heap sizes of the Java VM that created \nthe system dumps. A system dump contains all of the data areas in the process memory, such as the `no \nzlib' and `zlib', respectively. This very small increase in compilation time indicates that the time \nto save the input to the logs was negligible. The increase in compilation time in the replaying compiler \nover the base was 9.4%, as a geometric mean, when the diagnostic output was not generated. When the diagnostic \noutput was generated, most of the compilation time is used for writing hundreds of megabytes of text \nto disk, regardless of whether or not the replay compilation technique is used. 6.4 Execution Speed \nThe operations to save the input into logs is the only additional overhead in the state-saving compiler \nagainst the base compiler because, for the same inputs, the state-saving compiler applies the same compilation \nand generates the same compiled code as the base compiler. There is no additional overhead in the compiled \ncode. Since the increase in compilation time was small, the slowdown of execution speed was also small. \nThe geometric mean of the slowdown was only 1%.  7. Conclusion We have proposed a new approach, called \nreplay JIT compilation, to reproduce the same JIT compilation offline and remotely by using two compilers, \nthe state-saving compiler and the replaying compiler. The state-saving compiler is used in a normal run, \nand, while compiling a method, records into a log all of the runtime information referenced during the \ncompilation. The log is in the main memory, and automatically included in the system dump when the application \ncrashes. The replaying compiler is then used in a debugging run with the system dump, to recompile a \nmethod with the options for diagnostic output. We developed our prototype based on the J9 Java VM and \nthe TestaRossa (TR) JIT compiler for AIX and showed that the prototype successfully reproduces the same \ncompilations done by the state-saving compiler. We also developed confidence-based filtering to save \nonly the logs that are likely to be needed for debugging. The time overhead of running the state-saving \ncompiler was only 1% and the size overhead for saving states was only 10% of the compiled code. To our \nknowledge, this is the first report describing how to successfully replay the JIT compilation offline. \n  Acknowledgements We would like to thank Akihiko Togami of IBM Japan and Trent Gray-Donald of IBM Canada \nfor helpful discussions on the initial idea and implementation of the replay JIT compilation. We also \nthank Toshimasa Shimizu of IBM Japan for pointing out the difficulties in debugging the JIT compiled \ncode in mission-critical environments, by which we were motivated. We also thank Derek Inglis of IBM \nCanada for pointing out the possible problem on Java class unloading. We also thank Shannon Jacobs of \nIBM Japan HRS and the members of the Systems Group in the IBM Tokyo Research Laboratory for helpful discussions \nand comments on an earlier version of this paper. References [1] M. Arnold and B. G. Ryder. A Framework \nfor Reducing the Cost of Instrumented Code. In Proceedings of the ACM SIGPLAN Conference on Programming \nLanguage Design and Implementation (PLDI '01), pp. 168-179. June, 2001. [2] D. F. Bacon. Hardware-Assisted \nReplay of Multiprocessor Programs. In Proceedings of 1991 ACM/ONR Workshop on Parallel and Distributed \nDebugging (PADD '91), pp. 194\u00ad 205. May, 1991. [3] M. D. Bond and K. S. McKinley. Continuous Path and \nEdge Profiling. In Proceedings of the 38th Annual IEEE/ACM International Symposium on Microarchitecture \n(MICRO '05), pp. 130-140. November, 2005. [4] J. D. Choi and H. Srinivasan. Deterministic Replay of Java \nMultithreaded Applications. In Proceedings of the SIGMETRICS Symposium on Parallel and Distributed Tools \n(SPDT '98), pp. 48-59. August, 1998. [5] Dynamic Proxy Classes, available at http://java.sun.com/j2se/1.4.2/docs/guide/reflection/proxy.html \n[6] J. Gosling, B. Joy, and G. Steele. Java Language Specification, available at http://java.sun.com/docs/books/jls/index.html \n[7] N. Grcevski, A. Kielstra, K. Stoodley, M. Stoodley, and V. Sundaresan. Java Just-in-Time Compiler \nand Virtual Machine Improvements for Server and Middleware Applications. In Proceedings of the Third \nVirtual Machine Research and Technology Symposium (VM '04), pp. 151-162. May, 2004. [8] X. Huang, S. \nM. Blackburn, K. S. McKinley, J. E. B. Moss, Z. Wang, and P. Cheng. The Garbage Collection Advantage: \nImproving Program Locality. In Proceedings of ACM SIGPLAN Conference on Object-Oriented Programming, \nSystems, Languages, and Applications (OOPSLA '04), pp. 69-80. October, 2004. [9] K. Ishizaki, M. Kawahito, \nT. Yasue, H. Komatsu, and T. Nakatani. A Study of Devirtualization Techniques for a Java Just-In-Time \nCompiler. In Proceedings of ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, \nand Applications (OOPSLA 2000), pp. 294-310. October, 2000. [10] Jigsaw - W3C's Server, available at \nhttp://www.w3.org/Jigsaw/ [11] S. Koerner, R. Bawidamann, W. Fischer, U. Helmich, D. Koldt, B. K. Tolan, \nand P. Wojciak. The z990 first error data capture concept. IBM Journal of Research and Development, Vol. \n48(3/4), pp. 557-568. May, 2004. [12] T. J. LeBlanc, J. M. Mellor-Crummey. Debugging parallel programs \nwith Instant Replay. IEEE Transactions on Computers, Vol. C-36(4), pp. 471-482. April, 1987. [13] T. \nLindholm and F. Yellin. The Java Virtual Machine Specification, available at http://java.sun.com/docs/books/vmspec/index.html \n[14] B. P. Miller and J. D. Choi. A Mechanism for Efficient Debugging of Parallel Programs. In Proceedings \nof the ACM SIGPLAN 1988 Conference on Programming Language Design and Implementation (PLDI '88), pp. \n135-144. June, 1988. [15] M. Paleczny, C. Vick, and C. Click. The Java HotSpot Server Compiler. In Proceedings \nof the Java Virtual Machine Research and Technology Symposium (JVM '01), pp. 1-12. April, 2001. [16] \nD. Z. Pan and M. A. Linton. Supporting Reverse Execution of Parallel Programs. In Proceedings of the \n1988 ACM SIGPLAN and SIGOPS Workshop on Parallel and Distributed Debugging (PADD '88), pp. 124-129. May, \n1988. [17] M. Ronsse, K. D. Bosschere, and J. C. Kergommeaux. Execution replay and debugging. In Proceedings \nof the Fourth International Workshop on Automated and Algorithmic Debugging (AADEBUG 2000). August, 2000. \n[18] Standard Performance Evaluation Corporation. SPEC JBB2000, available at http://www.spec.org/osg/jbb2000/ \n[19] Standard Performance Evaluation Corporation. SPEC JVM98, available at http://www.spec.org/osg/jvm98/ \n[20] T. Suganuma, T. Ogasawara, M. Takeuchi, T. Yasue, M. Kawahito, K. Ishizaki, H. Komatsu, and T. Nakatani. \nOverview of the IBM Java Just-In-Time Compiler. IBM Systems Journal, Java Performance Issue, Vol. 39(1), \npp. 175-193. February, 2000. [21] L. Stepanian, A. D. Brown, A, Kielstra, G. Koblents, and K. Stoodly. \nInlining Java Native Calls At Runtime. In Proceedings of ACM/Usenix International Conference On Virtual \nExecution Environments (VEE '05), pp. 121-131. June, 2005. [22] J. Steven, P. Chandra, B. Fleck, and \nA. Podgurski. jRapture: A Capture/Replay Tool for Observation-Based Testing. In Proceedings of International \nSymposium on Software Testing and Analysis (ISSTA 2000), pp. 158-167. August, 2000. [23] V. Sundaresan, \nD. Maier, P. Ramarao, and M. Stoodley. Experiences with Multi-threading and Dynamic Class Loading in \na Java Just-In-Time Compiler. In Proceedings of the International Symposium on Code Generation and Optimization \n(CGO '06), pp. 87-97. March, 2006. [24] UNIX System Certification, available at http://www.opengroup.org/certification/unix-home.html \n[25] J. Whaley. A Portable Sampling-Based Profiler for Java Virtual Machines. In Proceedings of ACM 2000 \nJava Grande Conference, pp. 78-87. June 2000. [26] XML Parser for Java, available at http://www.alphaworks.ibm.com/tech/xml4j \n[27] T. Yasue, T. Suganuma, H. Komatsu, and T. Nakatani. An Efficient Online Path Profiling Framework \nfor Java Just-In-Time Compilers. In Proceedings of the Twelfth International Conference on Parallel Architectures \nand Compilation Techniques (PACT-2003), pp. 148-158. September, 2003. [28] zlib, available at http://www.gzip.org/zlib/ \n \n\t\t\t", "proc_id": "1167473", "abstract": "The performance of Java has been tremendously improved by the advance of Just-in-Time (JIT) compilation technologies. However, debugging such a dynamic compiler is much harder than a static compiler. Recompiling the problematic method to produce a diagnostic output does not necessarily work as expected, because the compilation of a method depends on runtime information at the time of compilation.In this paper, we propose a new approach, called <i>replay JIT compilation</i>, which can reproduce the same compilation remotely by using two compilers, the <i>state-saving compiler</i> and the <i>replaying compiler</i>. The state-saving compiler is used in a normal run, and, while compiling a method, records into a <i>log</i> all of the input for the compiler. The replaying compiler is then used in a debugging run with the system dump, to recompile a method with the options for diagnostic output. We reduced the overhead to save the input by using the system dump and by categorizing the input based on how its value changes. In our experiment, the increase of the compilation time for saving the input was only 1%, and the size of the additional memory needed for saving the input was only 10% of the compiler-generated code.", "authors": [{"name": "Kazunori Ogata", "author_profile_id": "81100139883", "affiliation": "IBM Research, Kanagawa, Japan", "person_id": "PP30024941", "email_address": "", "orcid_id": ""}, {"name": "Tamiya Onodera", "author_profile_id": "81100474003", "affiliation": "IBM Research, Kanagawa, Japan", "person_id": "PP31043832", "email_address": "", "orcid_id": ""}, {"name": "Kiyokuni Kawachiya", "author_profile_id": "81100038759", "affiliation": "IBM Research, Kanagawa, Japan", "person_id": "P162073", "email_address": "", "orcid_id": ""}, {"name": "Hideaki Komatsu", "author_profile_id": "81100557247", "affiliation": "IBM Research, Kanagawa, Japan", "person_id": "PP39048455", "email_address": "", "orcid_id": ""}, {"name": "Toshio Nakatani", "author_profile_id": "81100311827", "affiliation": "IBM Research, Kanagawa, Japan", "person_id": "PP14113792", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1167473.1167493", "year": "2006", "article_id": "1167493", "conference": "OOPSLA", "title": "Replay compilation: improving debuggability of a just-in-time compiler", "url": "http://dl.acm.org/citation.cfm?id=1167493"}