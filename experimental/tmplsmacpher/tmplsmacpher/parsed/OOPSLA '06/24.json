{"article_publication_date": "10-16-2006", "fulltext": "\n Understanding the Shape of Java Software Gareth Baxter, Marcus Frean, James Noble, Mark Rickerby, Hayden \nSmith, Matt Visser School of Mathematics, Statistics, and Computer Science Victoria University of Wellington \nWellington, New Zealand .rstname.lastname@mcs.vuw.ac.nz Abstract Large amounts of Java software have \nbeen written since the lan\u00adguage s escape into unsuspecting software ecology more than ten years ago. \nSurprisingly little is known about the structure of Java programs in the wild: about the way methods \nare grouped into classes and then into packages, the way packages relate to each other, or the way inheritance \nand composition are used to put these programs together. We present the results of the .rst in-depth \nstudy of the structure of Java programs. We have collected a number of Java programs and measured their \nkey structural attributes. We have found evidence that some relationships follow power-laws, while others \ndo not. We have also observed variations that seem related to some characteristic of the application \nitself. This study provides important information for researchers who can investigate how and why the \nstructural relationships we .nd may have originated, what they portend, and how they can be managed. \nCategories and Subject Descriptors D.2.8 [SOFTWARE ENGI-NEERING]: Metrics Product metrics; D.1.5 [PROGRAMMING \nTECHNIQUES]: Object-oriented Programming General Terms Design, Measurement Keywords Power-law distributions, \nobject-oriented design, Java 1. Introduction Much of software engineering has focused on how software \ncould or should be written, but there is little understanding of what actual software really looks like. \nWe have development methodologies, design principles and heuristics, but even for a well-de.ned subset \nof software, such as that written in the Java programming language, we cannot answer simple questions \nsuch as How many methods does the typical class have? or even Is there such a thing as a typical class \n? What we would really like to know about software is Is it good? that is, does it have quality attributes \nsuch as high modi.\u00adability, high reusability, high testability, or low maintenance costs. We believe \ncurrent methodologies lead to good software, but with\u00adout knowing what good software looks like, we cannot \nknow that Permission to make digital or hard copies of all or part of this work for personal or classroom \nuse is granted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. OOPSLA \n06 October 22 26, 2006, Portland, Oregon, USA. Copyright c . 2006 ACM 1-59593-348-4/06/0010. . . $5.00 \nHayden Melton, Ewan Tempero Department of Computer Science University of Auckland Auckland, New Zealand \n .rstname@cs.auckland.ac.nz the methodologies are actually working. We are left with circular arguments \nof the form The methodologies are good because the software is good, and the software is good because \nthe methodolo\u00adgies are good. Understanding the shape of existing software is a crucial .rst step to understanding \nwhat good software looks like. Just as biologists classify species in terms of shape and structure and \necologists study the links and interactions between them, we have been collecting a body of software \nand analysing its abstract form. We remove semantics and focus on the network of connec\u00adtions where information \n.ows between components. Just as biol\u00adogists (and other scientists) seek to understand the characteristics \nof the population under study, so too would we like to know such basic features as the distributions \nof the software structures we .nd. Of speci.c interest are recent claims that many important rela\u00adtionships \nbetween software artifacts follow a power-law distribu\u00adtion (e.g. [34]). If this were true, it would \nhave important implica\u00adtions on the kinds of empirical studies that are possible. One issue is the fact \nthat a power-law distribution may not have a .nite mean and variance. If this is the case, the central \nlimit theorem does not apply, and so the sample mean and variance (which will always be .nite, because \nthe sample size is .nite) cannot be used as estimators of the population mean and variance. This would \nmean that basing any conclusions on sample means and variances without fully un\u00adderstanding the distribution \nwould be questionable at best. In this paper, we extend past similar studies in two ways. First, we examine \na much larger sample than previous studies. We have analysed a corpus of Java software consisting of \n56 applications of varying sizes, and measured a number of different attributes of these applications. \nSecond, we consider distributions other than those following a power-law. We .nd evidence that supports \nclaims by others of the existence of power-law relationships, however we also .nd evidence that some \ndistributions do not appear to obey a power-law. Furthermore, whether or not a relationship follows a \npower-law appears to depend on an identi.able characteristic of the relationship, namely, whether or \nnot the programmer is inherently aware of the size of the relationship at the time the software is being \nwritten. We also see variations between applications. We speculate that this may be due to some characteristic \nin the application s design, that is, some property of the design is re.ected in the distribution of \nsome measurements. The rest of the paper is organised as follows. In Section 2, we discuss the motivation \nfor our study. Section 3 describes in detail the salient features of our study, namely the corpus we \nuse and the metrics we gather. In Section 4 we give the analysis of our results, and in Section 5 we \ngive our interpretation of this analysis. Section 6 discusses the most relevant related work, and we \ngive our conclusions in Section 7. 2. Motivation and Background Software systems are now large, complex, \nand ubiquitous, however surprisingly little is known about the internal structures of practi\u00adcal software \nsystems. A large amount of research has studied how software ought to be written, how it should be structured. \nMany rules, methodologies, notations, patterns and standards for design\u00ading and programming such large \nsystems [9, 17, 24] have been produced. Psychological models have been constructed of the pro\u00adgramming \nprocess [6, 33]. Quantitative models of software have been designed to predict the effort required to \nproduce a system, measure the development rates of software over time (process met\u00adrics) or measure the \nvolume of software in a system and its quality (product metrics) see e.g. [7, 15, 27]. But we know very \nlittle about the large-scale structures of software that exists in the real world. With the methodologies, \nnotations, and other advice that has been developed, we should be able to say something about the soft\u00adware \nthat results if such advice is followed. However the conditional is key until recently there was very \nlittle work done in determin\u00ading even if the advice that has been offered is actually been taken. There \nis some evidence that common advice is not being followed. For example, a number of people have advised \nagainst creating cy\u00adcles of dependencies in software, but recent evidence suggests that not only do programmers \nregularly introduce cycles, but they are often very large [20]. One consequence of much of the advice \noffered with respect to object-oriented design is what we call the Lego Hypothesis,which says that software \ncan be put together like Lego, out of lots of small interchangeable components [26, 29]. Software constructed \naccording to this theory should show certain kinds of structure: components should be small and should \nonly refer to a small num\u00adber of closely related components. In fact, we don t know whether or not this \nis true, because we lack models describing the kinds of large structures that exist in real programs. \nThere are no quantitative, testable, predictive theories about the internal structures of large scale \nsystems, or how those structures evolve as programs are constructed [8, 13]. While design patterns, rules, \nmetrics and so on, can give guidance regarding developing program structure, they cannot predict the \nanswers to questions about the large-scale structure that will result, such as: in a program of a given \nsize, how many classes or methods will exist? How large will they be? How many instances of a each class \nwill be created? How many other objects will refer to any given object? We need answers to these kinds \nof questions in order to be able understand how large scale software is actually organised, built, and \nmaintained in practise. Recently there has been an interest in looking for power-law relationships in \nsoftware. A distribution of the number of occur\u00adrences Nk of an event of size k is a power-law if it \nis proportional to k raised to some power s. A common method used to detect pos\u00adsible power-laws is to \nrank the event sizes by how often they occur, and then plot N vs. the rank on logarithmic scales. A distribution \nfollowing a power-law will appear as a line with slope s. Studies of computer programs have considered \nboth static [30, 31, 34] and dynamic [23, 25, 26] relationships, in different forms of software as diverse \nas LISP, visual languages, the Linux kernel, and Java applets [3, 22, 23, 25, 26, 28, 31], and the design \nof Java programs [5, 30, 31]. The conclusions from these studies is that power-laws appear to be quite \ncommon. Our work follows from Wheeldon and Counsell, who examined a number of inter-class relationships \nin Java source code, namely Inheritance, Interface, Aggregation, Parameter Type, and Return Type in three \nJava systems: the core Java class libraries, Apache Ant, and Tomcat [34]. We attempted to reproduce the \nWheeldon and Counsell study, and found examples of their metrics that, for frequency (log scale) 103 \n102 101 100 count (log scale) Figure 1. A distribution that does not appear to obey a power-law. Open \ncircles are data, solid line is best .t power law distribution. some applications, did not appear to \nobey a power-law. One exam\u00adple is shown in Figure 1 (which appears again, with full explana\u00adtion, as \nFigure 3). This .gure shows a plot organised as described above it is a log-log plot of frequency of \noccurrence of different values of a particular metric. The data in this .gure seems to have a distinct \ncurve to it. Had we plotted this on a normal scale, we would see something like a power-law curve, except \ntruncated at the high end. This .gure casts some doubt as to whether the distri\u00adbution shown is a power-law. \nOur experience raised two questions. The .rst is, do the rela\u00adtionships others have studied really obey \na power-law? While the evidence provided is compelling to the naked eye, there is little analytical support. \nIn this paper, we will provide such an analysis to support our claims. The second question is, are the \nstudies rep\u00adresentative of software in general. This is not a question that can be answered easily due \nto the scale involved, however, our study involves a much larger corpus than other studies, and so provides \nbetter support for our claims. 3. Method 3.1 Gathering the Corpus The corpus consists of 56 applications \nwhose source code is avail\u00adable from the web. Many of the applications were chosen because they have \nbeen used in other studies (e.g., [10, 12, 26]), although comparison to these other studies isn t possible \nas version num\u00adbers were not always provided. Also, we weren t always able to acquire all applications \nused in those other studies. Further appli\u00adcations were then added to the corpus based on software that \nwe were familiar with (e.g. Azureus, ArgoUML, Eclipse, NetBeans). Finally we identi.ed popular (widely \ndown-loaded) and actively developed open-source Java applications from various web-sites, including: \ndeveloperWorks1, SourceForge2, Freshmeat3,Java.net4, Open Source Software In Java5 and The Apache Software \nFoun\u00addation6. Figure 2 gives an indication of the distribution of the size 1 http://www-128.ibm.com/developerworks/views/java/ \ndownloads.jsp 2 http://sourceforge.net/ 3 http://freshmeat.net/ 4 http://community.java.net/projects/ \n5 http://java-source.net/ 6 http://apache.org/  Figure 2. Distribution of application size in Corpus. \nof the applications, measured in terms of the number of top-level classes. Appendix B gives more details \nof contents of the corpus we used. 3.2 Metrics There are a number of variables that must be taken into \naccount when carrying out this kind of research. In the interests of allowing others to reproduce and \nextend our results, we discuss our choices in detail. Any Java program makes some use of the Standard \nAPI, and so there is a question of how much the Standard API is counted when doing the analysis. For \nexample, when counting the num\u00adber of methods per class, should the number of methods in the java.lang.String \nclass be counted, or should the number of methods that use String as a parameter or return type be counted? \nThis type is so heavily used that measuring its use seems likely to distort the results, and so it would \nseem reasonable to not con\u00adsider it. However there are also less frequently used types, such as java.util.jar.Pack200, \nthat seem less likely to distort the re\u00adsults and so maybe should be counted. It is not clear where to \ndraw the line. In this analysis we have chosen to consider only the human ed\u00aditable aspect of an application \ns construction, that is, the source code that is under the control of the application developers. For \nthis reason, when metrics have been computed, we have considered only those classes declared in the source \n.les of the application. Uses of the Standard API (and indeed any other API used but not constructed \nfor the application) are not considered. In the descrip\u00adtions below, the phrase in the source will reinforce \nthis choice. Note that in the case where the application is the JDK/JRE, it is the Standard API being \nanalysed. All the metrics have been computed from the byte code repre\u00adsentation of top-level classes, \nthat is, classes that are not contained within the body of another class or interface [11, chapter 8]. \nRela\u00adtionships relating to inner classes are merged with their containing class. To restrict the analysis \nto only those classes in the applica\u00adtion s source code, names discovered in the byte code were .ltered \naccording to package names of packages in the source code. Note that this means our analysis is limited \nto those applications that use a package structure. We used two methods to carry out the analysis. One \nmethod ap\u00adplied to the byte code directly, using the Byte Code Engineering Library (BCEL)7. The other \napplied javap, a Java byte code dis\u00adassembler that outputs representations of classes in a plain text \nfor\u00ad 7 http://jakarta.apache.org/bcel mat. From this, we were able to extract information about the struc\u00adture \nof .elds, methods, and opcode instructions, which we used to build a meta model of each application as \na nested collection of the basic types package , class , method , and .eld . These col\u00adlections gave \nus a simple source for calculating metrics we were interested in. When byte code is generated, some information \n(par\u00adticularly type information) is thrown away. This means some of our results will not match a similar \nanalysis done directly on the source code. We discuss this point in more detail when we present the met\u00adrics. \nMany of the metrics we use come from Wheeldon and Coun\u00adsell, as indicated in the list below, and we use \ntheir naming scheme where possible [34, Figures 8-10]. Due to the dif.culty in interpret\u00ading their descriptions \n[34, Figure 1] we give more detailed de.ni\u00adtions here, with a more formal treatment in Appendix A. We \nwill use the abbreviations given below. Where the abbreviation does not match the Wheeldon and Counsell \nnames, we indicate the phrase on which they are based. Our de.nitions assume that there is only one top-level \n[11] type declaration per source .le (.java .le). That is, we explicitly rule out the following situation, \nwhere two classes are declared in the same .le (or compilation unit). // A.java containing two class \ndeclarations public class A { ... } class B {... } The main reason for making this assumption is that \nit simpli.es the de.nitions. However, compiling the .le A.java above will yield two .les, A.class and \nB.class. Since there is no requirement that a class be declared to be public, even when it is the only \nclass in a compilation unit, there is no way to tell from looking at B.class that it was generated from \nthe same source .le as A.class. In the following description, we occasionally need to distin\u00adguish between \nwhen a name refers to a class and when it refers to an interface. When no distinction is necessary, we \nwill say the name refers to a type. Number of Methods nM (WC) For a given type, the number of all methods \nof all access types (that is, public, protected, private, package private) declared (that is, not inherited) \nin the type. Number of Fields nF (WC) For a given type, the number of .elds of all access types declared \nin the type. Number of Constructors nC (WC) For a given class, the number of constructors of all access \ntypes declared in the class. Note that since the measurements are taken from the byte code, this is guaranteed \nto be at least 1. If no constructor is speci\u00ad .ed, the Java compiler automatically generates a default \npublic nullary constructor that is included in the byte code. Subclasses SP Subclass as Provider (WC) \nFor a given class, the number of top-level classes that specify that class in their extends clause. Implemented \nInterfaces IC Interface as Client (WC) For a given class, the number of top-level interfaces in the \nsource that are speci.ed in its implements clause. For a given interface, the number of top-level interfaces \nin the source that are speci.ed in its extends clause. Interface Implementations IP Interface as Provider \n(WC) For a given interface, the number of top-level classes in the source for which that interface appears \nin their implements clause. Note that when an inner class implements a given inter\u00adface, it is the top-level \nclass that contains it that is counted. References to class as a member AP Aggregate as Provider (WC) \nFor a given type, the number of top-level types (including itself) in the source that have a .eld of \nthat type. Members of class type AC Aggregate as Client (WC) For a given type, the size of the set \nof types of .elds for that type. References to class as a parameter PP Parameter as Provider (WC) For \na given type, the number of top-level types in the source that declare a method with a parameter of that \ntype. Parameter-type class references PC Parameter as Client (WC) For a given type, the size of the \nset of types used as parameters in methods for that type. References to class as return type RP Return \nas Provider (WC) For a given type, the number of top-level classes in the source that declare a method \nwith that type as the return type. Methods returning classes RC Return as Client (WC) For a given type, \nthe size of the set of types used as return types for methods in that type. Depends on DO For a given \ntype, the number of top-level types in the source that it needs in order to compile. The intent is to \ncount all top-level types from the source whose names appear in the source for the type. There are some \nrare situations (when only methods from parent classes are called on the object) where the types of local \nvariables are not recorded in the byte code. Our experience is that this happens suf.ciently rarely to \nhave no effect on the results. Depends On inverse DOinv For a given type, the number of type implementations \nin which it appears in their source. Public Method Count PubMC The number of methods in a type with public \naccess type. Package Size PkgSize The number of types contained direction in a package (and not contained \nin sub-packages). Method size MS The number of byte code instructions for a method. Note that this is \nnot the number of bytes needed to represent the method.  4. Results We have applied the 17 metrics \ndescribed in the previous section to 56 applications from our corpus. This has yielded more data than \ncan be conveniently shown here, so instead we have done some preliminary analysis based on various assumptions \nas to what the distribution of the data is, and present the results of analysis. 4.1 Analysis The raw \ndata consists of a number for each element (method, top\u00adlevel class, package) in each application. The \n.rst step was to group all values by application, count the number of occurrences of each value and record \nthat in order of value. The primary goal of our analysis was then to determine whether the resulting \ndistribution obeyed a power-law. Some of the distributions derived from our analysis of soft\u00adware structure \nlook like straight lines when plotted with logarith\u00admic scales on both axes. This is the hallmark of \na power-law dis\u00adtribution, which is interesting because of its scale-free properties, which we will describe \nbelow. Any other distribution will not be exactly a straight line in such a plot. Not all the plots look \nexactly straight. Some have a sort of curve to them. We can respond by either saying that we do not care, \nas they are nearly straight, at least for part of the range, or we can say that they really are not power-laws \nat all, and are characterised by some other distribution. Secondly, even if it really is a power-law, \nbecause the data is noisy and because there is a .nite sample size and a .nite range of sizes , a power-law \ncurve won t exactly .t the data, especially at large values of the metric. This also means that some \nalternative distributions might be made to .t the data just as well we might not be able to discriminate, \neven for the plots that look pretty straight. Our approach is to take the data, and do rigorous best-.ts \nto several different distributions, and see .rst whether it is reasonable to .t a power-law, second whether \na power-law is more reasonable than the others, third whether the data can be divided into two or more \ngroups according to which distribution .ts best . 4.1.1 Power-Law In general a power-law distribution \nhas the form [21]: ppowlaw(x). x -a , (1) where ais a positive constant and we assume xto be non-negative. \nIn our case, xis the value of the metric as de.ned in the previous section. If a<1there must be a .nite \nmaximum value of x,in order for the distribution to be normalisable. If a>1, normalis\u00adability requires \nthat the minimum value of xnot be equal to zero. For a= 2the mean of the distribution is in.nite (assuming \nthere is no upper cutoff in x). When a>2the mean is proportional to the small-xcutoff. For a= 3the variance \nis also in.nite. One consequence of this fact is that the central limit theorem doesn t hold for such \ndistributions, so the mean and variance of a sample (which will always be .nite) cannot be used as estimators \nfor the population mean and variance. A distribution is said to be scale free if [21]: p(bx)=g(b)p(x), \n(2) where gdoes not depend on x. This means the relative probability of occurrence of events of two different \nsizes (bxand x) depends only on the ratio b, and not on the scale x. One of the reasons for the interest \nin power-laws is that they possess this scale-free prop\u00aderty. If we can show that the distributions we \nsee in our analysis of software obey a power-law, we can say that there is no character\u00adistic size (where \nsize might mean in-degree, for example) to the components. A scale-free distribution such as a power-law \nwould contradict the Lego Hypothesis. While an idealised power-law distribution might be strictly scale-free, \nfor the distributions we encounter in real systems this can only be approximately true. The data in our \nstudies only occurs at discrete, integer values of x. This imposes a small-size cutoff on x the smallest \nvalue of xwe measure is 1.There is also a large-size cutoff of x, as the programs in the corpus are of \n.nite size. Nevertheless, we are still interested in power-laws. The scale\u00adfree property (2) may still \nhold over a limited range. We can never say for certain that a distribution is a power-law because we \nare always dealing with measured data that involve some noise, and also .nite size effects but we might \nbe able to say that it is ap\u00adproximately a power-law, well characterised by a power-law over a large \nrange, or more likely to be a power-law than something else. 3 10 frequency (log scale)  4.1.2 Other \nCandidates Given our experience with plots such as that shown in Figure 1, we are interested in distributions \nthat are close to power-laws, but resemble the curves we have seen. Two other distributions which have \nsome credibility as natural distributions are: Log-normal distribution. Power-laws and log-normals look \nthe same at low values of x (i.e., at the high frequency end), but the tail is fatter for a power-law. \nFor continuous x a log-normal probability density function is de.ned as: 2 10 1 10 1 -(ln x - \u00b5)2 plognorm(x)= \nv exp, (3) xs 2p 2s2 while for discrete values of x, the normalisation will be more complicated, and \nthe distribution is of absolute probability, not probability density. Note that our data is not ranked, \nso it is usually, but not nec\u00adessarily monotonically decreasing with x: sometimes the smallest value \nof x does not have the highest frequency. Log-normal distri\u00adbutions can reproduce this pattern, but to \n.t a power-law we must treat this turnover as a statistical anomaly. 0 10 count (log scale) Figure 3. \nAC distribution and .tted curves for Eclipse. Open cir\u00adcles are data, solid line is best-.t power-law, \ndashed line is best-.t log-normal and dotted line is best-.t stretched exponential. 3 10 Stretched exponential. \nThis is known to occur in natural dis\u00adtributions [18] (it is the same as the two-parameter Weibull dis\u00adtribution \n[32] which is used to model electrical component failure probabilities): ()c-1 {()c} cx x pstrexp(x)= \nexp - . (4) x0 x0 x0 Again, this is the continuous x version of the distribution. The form is the same \nin the discrete case, but the normalisation is different. A stretched exponential looks just like a power-law \nfor small values of x, but has a sort of exponential behaviour for large x. frequency (log scale) 2 10 \n1 10 0 10 Both of these (depending on the choice of parameters) are slightly curved on a log-log plot, \nso they are likely to be good .ts to the data we have that is not exactly straight. Neither has the long \ntail characteristic of a power-law, so the curves drop off sharply at the right hand side of a log-log \nplot. The distinguishing features of power-laws are therefore straight\u00ad count (log scale) Figure 4. AP \ndistribution and .tted curves for NetBeans. introducing a weight to each square in the sum: ness in the \nlog-log domain, and not dropping off as fast as the k Q = wi[hi - f(a, \u00df, xi)]2 . (6) others for large \nvalues of x. This is sometimes called a fat tail or long tail , in contrast with the truncated tail evident \nin Figure 1. One potential problem is that the data is poorest in this tail region our best statistics \nwill be at the non-tail end. 4.1.3 Weighted Least Squares Fits Fitting a distribution to data means \nchoosing the parameters of the distribution so that it is closest to the data. One way to do this is \nto minimise the sums of the squares of the differences between the data values and the distribution values. \nSuppose the data takes value hi at xi,where i runs from 1 to k, the number of data points. If the value \nof the distribution at xi is given by f(a, \u00df, xi),where a and \u00df are the parameters of the distribution, \nwe want to choose a and \u00df so that the residual: k Q =[hi - f(a, \u00df, xi)]2 (5) i=1 is as small as possible. \nWeighted least squares .tting is where we use this method but allow for different uncertainties in different \ndata points by i=1 wi should re.ect how much uncertainty there is in the value of a data point. We set \nwi =1/hi. Thus k 1 Q =[hi - f(a, \u00df, xi)]2 . (7) hi i=1 4.1.4 Uncertainty and Con.dence Intervals If \nf is the true distribution, we would have E[hi]= f(a, \u00df, xi) where E[z] denotes the expected value of \nz. Expanding each term in (7) and neglecting higher terms we .nd E[Q] ~ k - 1 (8) and Var[Q]= E[(Q - \nE[Q])2] ~ k - 2 . (9) We have assumed hi is binomially sampled from a distribution with mean f/N,where \nN is the sample size, N =i hi. This gives us a way to estimate how good our .t is. We have effectively \na distribution for Q, based on our assumption that the 3 10 3  frequency (log scale) frequency (log \nscale) 2 10 2 1 1 10 0 10 count (log scale) Figure 5. PC distribution and .tted curves for Eclipse. data \nfollows the candidate distribution f. We can then choose a Con.dence Interval (CI) for Q, and if the \nvalue for Q that we actually .nd from our .tting procedure actually falls within this range, we can take \nthis as evidence for our assumption about f. For example, if the distribution is really the one we have \n.tted, we would expect Q to be within 1.64s of E[Q],where s = Var[Q], 90% of the time. E[Q] \u00b1 1.64s is \ncalled a 90% con.dence interval (CI), and if the minimum value of the residual Q that we do get falls \nwithin this range, we say that the distribution .ts the data at the 90% CI. (This is not the same as \nsaying that we are 90% sure the distribution is right. ) 4.1.5 Fitting the data in the current study, \nthe minimisation of equation (7) was done nu\u00admerically, with f(a, \u00df, xi) replaced by each of the three \ndistribu\u00adtions (1), (3) and (4) in turn. The raw data is in the form of frequen\u00adcies occurring at integral \nvalues of x. Note that the normalisation of these distributions at discrete values differs from the normalisation \nof a continuous distribution, and it is important to take this into ac\u00adcount. This normalisation depends \nof course on the parameter val\u00adues. The log normal and stretched exponential distributions each have \ntwo parameters, while the power-law distribution is de.ned by a single parameter. A second parameter \ncould be introduced by allowing the constant of normalisation to vary (in a log-log plot, a power-law \nappears as a straight line, with slope given by the single parameter, a, also known as the exponent . \nThe offset of the line is given by the normalisation constant, so .tting an offset param\u00adeter is equivalent \nto .tting the normalisation constant). We found that the .t was very similar when the .t was done with \nonly a sin\u00adgle parameter (calculating the normalisation explicitly), returning very similar exponent \nvalues and residuals. The aim of this exercise is mainly to establish the plausibility of the different \ndistributions .tting the data, therefore we do not give uncertainties in the .tted parameters, or speculate \non the interpre\u00adtation of, for example, different .tted power-law exponents. Table 1 shows a small excerpt \nfrom the results of the .t pro\u00adcess. This shows the estimated parameters for each of the three dis\u00adtributions \nusing the full datasets: a pow is for power-law, m log and s log are for log-normal, and a str and b \nstr are for the stretched exponential. The next three columns show the residuals for each of the .tted \ncurves, tot cnt is the sum of the frequencies, and the last column is the number of data points. Recall \nthat the expected value for the residuals is k - 1 and the variance is k - 2. This means, for the .rst \nrow of Table 1 (the AC metric) the 90% con.dence interval would be 25 \u00b1 8.03 (1.64 \u00d7 0 10 count (log \nscale) Figure 6. nF distribution and .tted curves for JRE. v 24), and so we can conclude that the log-normal \ndistribution .ts the data at the 90% CI, but the other two distributions do not. Figure 3 shows an example \nof a plotted dataset with .tted curves (and is the same as Figure 1). This .gure is a log-log plot of \nthe number of types (y-axis) having a given number of .elds (x-axis), that is, the AC metric, for Eclipse. \nThe best-.t for a power-law is shown as a solid line, the best .t for the log-normal is shown as a dashed \ncurve, and the best .t for the stretched exponential is a dotted curve. In this case, there is a pronounced \ncurve in the data, and in fact the log-normal has a much better .t than the power-law. Figures 4-15 show \na representative sample of .tted curves for different metrics and different applications. The parameters \nand residuals for these curves are shown in Table 2.  4.1.6 Summarising the results For each metric \nof each program in the corpus, the .ts were done .rst to the whole set of available data, then the number \nof points was reduced by removing 5, 10, 15, or 20 percent of the data points (or cuts ) from both ends \n that is, using only the middle 90, 80, 70, or 60 percent of the non-zero data points. The residuals \nfor each .t were then compared for the three distributions. We checked whether each .t was consistent \nwith the data at 95%, 90%, 80% and 60% con.dence intervals, and then the power-law .t was compared to \nthe best (residual closest to the expected value) of the other two .ts. Each metric for each program \ncould then be classi.ed at each CI with .ags as follows: a Power-law residual is within the CI and both \nother residuals outside CI. b Power-law residual within CI and one or both of the other residuals within \nCI. c Log normal and/or stretched exponential residual within CI, but power-law residual outside CI. \nd None of the residuals within CI. x No data. Roughly speaking, this order (ignoring x)represents decreasing \nsupport for the distribution of the data being a power-law. While b does not rule out a power-law, the \nfact that it .ts one of the other candidate distributions indicates more doubt than a indicates. Since \nwe chose our other candidate distributions to be close to power-law, a d suggests that not only do we \nnot have a power-law, but we do not even have something close. Metric a pow m log s log a str b str \nQpow Qlog Qstr tot cnt k AC 1.72 0.62 0.83 0.64 1.03 163.54 32.63 52.79 668 26 AP 3.03 0.52 8.40 1.94 \n0.95 12.06 410.05 19.61 326 23 DO 1.11 1.56 0.78 0.28 0.63 1004.79 187.74 575.22 1251 97 DOinv 1.12 -3.20 \n3.80 0.33 0.54 1045.67 684.07 650.27 1251 634 IC 2.12 -0.23 0.85 1.09 0.90 3.91 8.92 12.72 89 14 IP 3.29 \n0.72 7.79 2.04 0.97 8.18 240.15 2.88 157 9 MS 0.91 2.38 1.20 0.08 0.57 7304.28 1354.52 5545.82 9859 1854 \nPC 1.65 0.68 0.85 0.61 1.05 254.13 22.11 61.78 1105 18 PP 1.83 -0.30 1.20 0.69 0.89 8.27 14.55 19.22 \n127 113 PubMC 1.36 0.95 1.14 0.42 0.98 199.13 70.02 110.66 1005 306 RC 1.55 -1.07 1.90 0.52 0.76 994.56 \n510.30 426.12 1240 38 RP 2.44 -0.30 0.75 1.51 0.86 25.34 41.39 50.76 263 31 SP 1.41 -2.95 8.80 0.57 0.80 \n37.45 47.79 36.81 133 94 nC 3.07 -0.06 0.50 1.72 0.99 37.32 13.01 32.34 1153 10 nF 1.40 0.72 1.24 0.45 \n1.03 80.27 36.37 43.36 668 146 nM 1.21 1.22 1.15 0.33 0.93 292.00 113.29 205.25 1170 320 pkgSize 0.92 \n2.80 2.85 0.00 1.19 13.73 12.53 13.51 72 128 Table 1. The estimated parameters for the three distributions \nfor arguuml-0.18.1 for the full dataset. Application Metric a pow m log s log a str b str Qpow Qlog Qstr \nk eclipse AC 1.82 0.80 0.82 0.58 1.03 3685.96 44.52 701.82 41 eclipse PC 1.57 1.24 0.79 0.44 0.76 10613.55 \n214.25 3470.69 118 eclipse IC 1.91 -0.06 1.12 0.70 1.01 65.96 33.83 64.41 117 eclipse MS 1.11 2.53 1.22 \n0.18 0.38 286047.39 13143.66 91823.29 4172 5jre nF 1.47 0.90 1.29 0.42 1.03 933.12 113.72 229.11 427 \njre nM 1.26 1.54 1.25 0.31 0.92 2374.36 218.22 1084.17 257 jre nC 2.74 -0.06 0.70 1.16 1.00 340.55 68.30 \n243.58 14 jre SP 1.84 -0.03 1.10 0.72 1.01 91.50 46.63 61.28 353 jre IC 1.86 0.10 0.99 0.73 1.02 74.64 \n37.64 40.27 451 netbeans AP 2.13 -0.15 0.95 0.87 0.96 44.61 105.89 184.63 508 netbeans IP 3.14 -0.02 \n0.50 1.63 1.00 109.44 11.08 45.33 7 netbeans PP 1.85 -0.25 1.35 0.58 0.93 93.60 204.40 308.58 618 tomcat \nMS 0.89 2.45 1.25 0.00 0.51 10318.07 4334.30 7020.94 1634 tomcat (5% cut) MS 1.68 1.65 1.75 0.29 0.96 \n320.63 569.26 285.78 562 openof.ce IP 3.74 0.43 9.15 2.26 0.95 133.66 19713.79 21.95 8 compiere RC 1.20 \n1.20 0.62 0.30 0.37 3113.16 457.41 923.42 18 Table 2. Fitted parameters for applications and metrics \nshown in plots. 4 10 3 10 3 10  frequency (log scale) 2 10 frequency (log scale) 2 10 1 10 1 10 0 0 \n10 10count (log scale) count (log scale) Figure 7. nM distribution and .tted curves for JRE. Figure \n8. nC distribution and .tted curves for JRE. AC AP DO DOinv IC IP MS PC PP PubMC RC RP SP nC nF nM pkgSize \njeppers b x b d x x b b d b b d x b b b b .tjava b b c b x x c b b b b b b d b b d junit b c b b c x \nd b b b b b c b b b b jgraph b b b b b d d b b b b b d b b b b jparse b b c d b d d d b b d b b b b c \nb jaga joggplayer .tlibraryfor.tnesse javacc lucene rssowl sablecc jag antlr jrefactory hsqldb jedit \naxion galleon james colt aglets jhotdraw ganttproject jetty ireport jext pmd aoi jung megamek jfreechart \npoi b b c b d b b c c b b c c d b c d c c c b b b c c c c c b b c b b b b b b b b b b b b b b b b b b \nb b b b b b b d b c b c c d b c d b c c b b d c c c d c d d c c c c d b b b d d b d c b d d c c b b b \nd c d c d c c d c c c d b x d b d d b b b c b b b b b b d b b b b b b b b b b b x x d d x d x x b d x \nd d c d d b b b b d x d d c d b b d a d d d d d d d d d d d d d d d d d d d d d d d d d d b c b d d c \nd c b d d c d c c c c c c c c b d c d d c d b b b b b b d b b d b b b b b b b b b b b b d b b b b b c \nb b b b b d b b b b b b b b b b b b b b b b d c b b c c c b b d c d c d c c c d c d d c c c d d b c d \nc d c d b d b b b b b b b b b b b b b b b b b b b b b b b b b c b d b b b b b b b b b b b b b b b b b \nb b b b b b b b b c b b c b c d c b d a a a d d d c c c c c d c c c b c d b b b b b b b b b b b b b c \nb b b c b b b b b c b b c c c b b b b b c b b b b b b b b b b b c b b b b c b b b d b b b b b b b b b \nb b b b b b b b b b b b b b b b b b b jmeter glass.sh jchempaint jasperreports scala drjava ant sandmark \ntomcat hibernate sequoiaerp d a c b b b c c c c c b b b b b b b b b b b c c c d c d c c c c d c d c d \nc d c d c d c b b b c b c b b b b b b b d c d c c b b c d d d d d d d d d d d d d c c c c d c d d d c \nb b b b b d b b b b b c b d b d d c c b c c c c d d d d d d d d d b b b b b b b b b b b b b b b b d b \nb b c b c b d c c d d d d d d c b d b b b c c c c c c c d b d d c c b c c b b b b b b b b b b b columba \nargouml compiere derby azureus geronimo jtopen openof.ce jboss jre netbeans eclipse c c c c c d d c c \nc d c b a b b b b b b b b a a c d d c c d d d d d d d c d d d d c d c d d d d b b b b b a b b b c b b \nc c d c d c d d c d c d d d d d d d d d d d d d d c d c d d d c d d d d b b b b b d b b c b a a c c d \nc c d c d c d c c c d d d c d d c d d d d b d b b c d b b c b c b b b c b b b b b b c b c d c c d c d \nd d d d d d c c b c c c c d c c c c c c d c c d c d c c c c b b b b c b b b d b c c Table 3. Quality \nof .t at Con.dence Interval 80% for full dataset: a good .t only to power-law, b good .ts to more than \none curve, c good .t only to other curves, d no good .ts. Applications are ordered by increasing size \n(number of classes). 103  3 10 102 101 frequency (log scale) frequency (log scale) 2 10 1 10 0 10 100 \n 0 1 2 0123 101010101010 10 count (log scale) count (log scale) Figure 9. SP distribution and .tted \ncurves for JRE. Figure 12. PP distribution and .tted curves for NetBeans. 3 10  2 10 1 10 frequency \n(log scale) frequency (log scale) frequency (log scale) 2 10 1 10 0 0 10 10 0 10count (log scale) count \n(log scale) Figure 10. IC distribution and .tted curves for JRE. Figure 13. IP distribution and .tted \ncurves for Openof.ce. 3 10 3 10 frequency (log scale) 2 10 2 10 1 10 1 10 0 0 1010 0 10 count (log \nscale)  Figure 11. IP distribution and .tted curves for NetBeans. Figure 14. IC distribution and .tted \ncurves for Eclipse. frequency (log scale) 2 10 2 1 frequency (log scale) 10 1 0 10 01 10100 10 count \n(log scale) 1 2 3 101010Figure 15. RC distribution and .tted curves for Compiere. count (log scale) Figure \n17. MS distribution and .tted curves for Tomcat after a 5% cut. 3 10 frequency (log scale) 4 10 2 10 \n1 10 frequency (log scale) 3 10 2 10 100 100 101 102 103 count (log scale) Figure 16. MS distribution \nand .tted curves for Tomcat.  Table 3 shows these results for the 80% CI and using complete datasets \n(0% cuts). In this table, the applications are ordered in increasing size, as measured by number of classes. \nThe four groups are: applications with fewer than 200 classes, applications with fewer than 500 classes, \napplications with fewer than 1000 classes, and those with more than 1000 classes. To aid comprehension, \nwe use different typefaces for the entries. For the moment, we will just note patterns and trends, and \n1 10 100 100 101 102 103 count (log scale) Figure 18. MS distribution and .tted curves for Eclipse. \n 3 10 leave interpretation and discussion to the next section. The .rst thing to note (other than the \nsheer size), is that, while all values are represented, b (multiple distributions have good .ts) is quite \nprominent. The next point is that a (good .t only to power-law) is relatively rare. Looking at individual \nmetrics for the larger applications (last category), we note that AC, PC, and RC tend to have c and d, \nindicating lack of support for them having a power-law distribution, whereas their opposites, AP, PP, \nand RP, as well as SP, tend to have a and b. In almost all cases, however, there are exceptions for individual \napplications. IC and IP show the opposite trend, with frequency (log scale) 2 10 1 10 IC having mainly \na and b and IP having mainly c and d. 10 0 123 101010 It must be kept in mind that Table 3 represents \nonly 5% of the count (log scale) results of the curve .tting (which itself represents a summarisation \nFigure 19. MS distribution and .tted curves for Eclipse after a 5% of the original data) there are the \nother CIs and cuts. What the results show for the other cuts and CIs is what one would expect. cut. \nAs the cut size increases, meaning the highest and lowest frequency data (where most of the variation \noccurs) is removed, we get better .ts for all three distributions (that is, tending toward b). Similarly, \nas the CI is increased, it also becomes easier to get a good .t. We chose to show the 80% CI as it seemed \nthe most representa\u00adtive. The 60% CI is not that different from what is shown in Table 3, and all of \nthe differences are what one would expect more d s (no good .ts) at 60% than at 80% or tending toward \nb when going from 60% to 80%. To .nish this section, we show a few more .tted curves. In this case, Figures \n16-19, we show various MS distributions. These are interesting as they have many more data points than \nthe others, being based on methods not types. We also show the effect of applying a 5% cut.   5. Discussion \n5.1 Interpretation Recall that several of our metrics measure 5 inter-type relationships Inheritance \n(SP), Aggregation (AC and AP), Parameter (PC and PP), Return (RC and RP), and Interface (IC and IP). \nThe C variant of the metric for a relationship measures the client end and P the provider end. Or, if \nthe code were represented as a directed graph with types as vertices and the different relationships \nas edges, then C would be the out-degree and P the in-degree for each relationship of each vertex. We \nnote that out-degree is impacted by decisions made with respect to the type represented by the vertex, \nwhereas in-degree is the result of decisions made with respect to other types. In the previous section, \nwe noted that AC, PC, and RC distribu\u00adtions tended not to have good .ts to a power-law, but AP, PP, RP, \nand SP did. From the comments above, this suggests out-degree distributions are not power-laws but in-degree \nare. The distribu\u00adtions we are seeing for the C metrics tend to be truncated at the high-value (low-frequency) \nend. A person changing the code for a class is inherently aware of its outward dependencies (e.g. the \nnumber of types it uses or the number of interfaces it implements), but they are not inherently aware \nof the number of classes that sub\u00adtype it or call methods on it. They therefore have less control over \nthe latter than they do over the former. Furthermore, we believe there is a tendency is to avoid (consciously \nor subconsciously) big things , whether due to dif.culty of management (e.g., methods with many parameters) \nor simply through training ( Don t write big classes! ). This suggests that C relationships are more \nlikely than P relationships to have truncated curves. We can generalise this to hypothesise that any \nmetric that measures something that the programmer is inherently aware of will tend to have a truncated \ncurve, that is, not be a power-law. The nF, nM, and PubMC, distributions are explained by our hypothesis. \nThey are all aspects of a type description that the developer is inherently aware of, and all tend not \nto have support for power-laws. Unfortunately our hypothesis does not explain the IC and IP distributions. \nWe believe that the main cause of the poor .ts for the IP distributions is the small datasets (no more \nthan 11 data points, and see for example Figure 13). This, however, does not explain IC (e.g., Figures \n10 and 14). nC also suffers from having small datasets, which might explain the results we see. DO and \nDOinv are related DO is the client end, and DOinv the provider . However in this case there is not a \nstrong distinction between the two, both being c and d. The DO relationship is effectively including \nall of AC, PC, RC, and IC, as well as types used for local variables. This would mean that the behaviour \nof IC noted above would oppose the behaviour of the others, which may explain the results. We do know \nthat types used for local variables (or rather, not used in the published interface) do account for signi.cant \ndependency structures [19]. MS, with few exceptions (all small applications), does not .t any distribution \nat the 80 CI. However, at 90 CI and above, there are good .ts to all of them. Our hypothesis would suggest \nthis should be a truncated curve (the size of the method being a decision made as it is written) but \nit would seem that there is too much noise to be sure. There is another important point to make. There \nis quite notice\u00adable variation on the degree of .t between different applications. This raises an interesting \nquestion: if a given relationship (metric) does follow a particular distribution, why do we not see this \ndistri\u00adbution for all applications, how is it that this variation exists? Two answers spring to mind. \nThe .rst is that different applica\u00adtions come from different domains, and it is possible that different \ndomains have different distributions. For example, NetBeans and Openof.ce often have different values \n(usually c vs d or a vs d). NetBeans is an IDE, whereas Openof.ce is an of.ce suite, and in fact is really \nseveral applications wrapped as one. We picked these two because they were both originally Sun products. \nThat said, Compiere is ERP and seems somewhat different in nature than, for example, Openof.ce, and yet \nthe distributions seem mainly similar. Another answer is that there is another thing that is potentially \nquite different (and much harder to see) between the applications their design. If we are seeing different \ndistributions due to different designs, if we could understand how aspects of the design related to the \nkind of distribution exhibited, there is the potential for developing a quantitative measure for design \nquality. Having such a measure could have tremendous impact on how software is developed in the future. \nOf course before this can happen, we must understand (presum\u00ading such a relationship exists) which distribution \ncorresponds to a good design and which does not. It is not obvious that, for example, the power-law distribution \nis found in good designs it could just as easily be the opposite! Our results do not provide much ad\u00advice \neither way. This does, however, suggest an extremely interest\u00ading avenue for future research. 5.2 Threats \nto Validity The most likely threat to the validity of our conclusions is the corpus we used. It consists \nentirely of open-source applications of small to medium size. Some applications originated from com\u00admercial \norganisations, but it is not obvious that the IBM and Sun\u00addonated code is typical of closed-source code. \nOther studies have suggested there is little difference between open-source and closed\u00adsource software \n[19], but we cannot say whether or not this is true here. While we cannot claim that our corpus represents \na random sample of Java software, our situation is no different than corpora used in applied linguistics. \nHunston describes a number of ways corpora may be reasonably used [14]. Our corpus is what she de\u00adscribes \nas a reference corpus, which are often used as base-line for further studies. Thus, a random sample is \nnot necessary in order to produce an valid result. Our results hold for what is in our corpus: whether \nor not they hold for other collections will in itself be of interest. So we cannot say for sure how representative \nour corpus is of Java software in general, or even open-source software in particu\u00adlar. Nevertheless, \nthe commonality we have seen across all of the applications we analyse gives us con.dence that our conclusions \nwill hold generally. A similar issue is that our corpus consists only of Java applica\u00adtions. It is possible \nwe may see different distributions when looking at other languages such as C# or C++. While there appears \nnoth\u00ading obviously different between Java and languages such as C# or C++ with respect to our study, \nthey do share the property of having static type checking, so while we may see no differences for such \nlanguages, we may see differences in languages, such as Smalltalk, that do not have static type checking. \nA property of the software we have studied that we have not ad\u00addressed in our study is the manner in \nwhich the software was cre\u00adated. Our hypothesis is based on the lack of global view a developer has of \nthe application being developed. Recently, there has been a signi.cant increase in the use of sophisticated \nIntegrated Develop\u00adment Environments (IDE) such as Eclipse, and one characteristic of these IDEs is that \nthey provide a better view of the source code than has been available in the past. The use of such IDEs \nmay af\u00adfect the shape of the distributions we have been investigating. We believe most of the code in \nour corpus was written before the ad\u00advent of such IDEs, but some of the variation we see may be due to \nhow the code was written. Again Smalltalk may show differences as it has always had an IDE. As noted \nearlier, because we measure from byte code, there is some information from the source code not available \nto us. The circumstances for which this is the case seem to be such that this will be rare.  6. Related \nWork As with many other things, Knuth was one of the .rst to carry out empirical studies to understand \nwhat code that is actually written looks like [16]. He presented a static analysis of over 400 FOR-TRAN \nprogrammes and dynamic analysis of about 25 programs. His main motivation was compiler design, with the \nconcern that compilers may not optimise for the typical case as no-one knew what the typical case was. \nHis analysis was at the statement level, counting such things as the number of occurrences of an IF state\u00adment, \nor the number of executions of a given statement. Collberg et al. have carried out a study of 1132 Java \nprograms [4]. These were gathered by searching for jar .les with Google and removing any that were invalid. \nTheir main goal was the devel\u00adopment of tools for protection of software from piracy, tampering, and \nreverse engineering. Like Knuth, they argued that their tools could bene.t by knowing the typical and \nextreme values of various aspects of software. Consequently, their interest is in the low-level details \nof the code with a view toward future tool support or lan\u00adguage design. Although their interest is in \nlow-level details, Collberg et al. do gather a number of similar statistics to ours, such as number of \nclasses per package, number of .elds per class, number of methods per class, size of the constant pool, \nand so on. However comparison with their results is problematic, as they appear to include all classes \nreferred to in an application, whereas we only consider classes that appear in the application source. \nGil and Maman analysed a corpus of 14 Java applications for the presence of micro patterns, patterns \nat the code level that represent low-level design choices [10]. They found that 3 out of 4 classes matched \none of the 27 micro patterns in their catalogue, and just over half of the classes are catalogued by \njust 5 patterns. This is a form of structural analysis, however it focuses on individual classes, rather \nthan at the application level as we have done. As already mentioned, Wheeldon and Counsell have performed \na similar analysis to ours. They looked at JDK 1.4.2, Ant 1.5.3, and Tomcat 4.0. They computed the 12 \nmetrics as noted in section 3 and concluded that what they were seeing were power-laws. There are some \ndifferences between their work and ours. Most notably is how the metrics were computed. Wheeldon and \nCounsell used a custom doclet to extract the relevant information, which limited them to just the information \navailable from the Javadoc comments. Also, they were not speci.c as to what choices they made for the \nvariables discussed in section 3. We believe the inconsistency between Wheeldon and Counsell s conclusions \nand ours is due to our more extensive corpus. Our orig\u00adinal intention was to reproduce their study and, \nwe thought, results. The truncated-curve distribution only really became apparent in the repetition across \nmultiple applications. In fact, their .gure 2(b) appears to have something of a curve to it. Our work \ndoes, however, add signi.cant evidence to support their hypothesis that there are regularities that are \ncommon across all non-trivial Java programs. 7. Conclusion We have studied the hypothesis that the distribution \nof a number of metrics on object-oriented software obey a power-law. We did so over a larger sample size \nthan has been considered by past similar studies, and applied analysis techniques to characterise how \nclosely each distribution obeyed a power-law. We have presented our method and analysis in what we hope \nis suf.cient detail to allow our studies to be reproduced with con.dence. What we found was that while \nthere were distributions for which there was good evidence for a power-law, there are a number for which \nthere was little evidence that a power-law exists. This is in contrast with what earlier studies have \nsuggested. We hypothe\u00adsise that any metric that measures a relationship that the program\u00admer is inherently \naware of will tend to have a truncated curve, that is, not be a power-law. Of particular interest is \nthe fact that some applications fre\u00adquently differed for some metrics from the other applications, in\u00addicating \nthat some attribute of the application s code can affect the resulting distribution. This .nding has \npotentially tremendous im\u00adplications. If the distribution does depend on either design quality or domain, \nthen knowing the distribution of a good design would provide a much sounder foundation for developing \nsoftware than currently exists. As open-source applications make extensive use of version control and \nbug-tracking systems, we believe the data necessary for such studies as correlations between distribution \nand prevalence of defects will be possible. There remains much work to be done. Further studies are needed \nto determine how representative our .ndings are. This means expanding the studies to other (especially \nlarger) applica\u00adtions, to applications developed in other environments, such as closed-source, to other \ndomains (for example, real-time software is not represented in our corpus at the moment), and to other \nlan\u00adguages. We need to be able to explain why we see some distributions in some applications for some \nmetrics and not others. For example, we need models that explain how these distributions arise. In the \ncase of power-law distributions, there is no theory to explain why we should see such scale-free structures \nin software. Two main hypothetical mechanisms have been put forward [1] to account for the origin of \nscale-free network structure in other domains: growth with preferential attachment [2], in which existing \nnodes link to new nodes with probability proportional to the number of links they already have, and hierarchical \ngrowth [33] in which networks grow in an explicitly self-similar fashion. Additionally arguments from \noptimal design have been proposed [30, 28]. It is still far from clear, however, what (if any) fundamental \ntheory might account for the ubiquity of the phenomenon in software. Ultimately, we need to understand \nthe relationship between large-scale structures found in software, and quality attributes such as understandability, \nmodi.ability, testability, and reusability. We believe this study is an important step toward that goal. \n  Acknowledgements We would like to thank the anonymous referees for their comments and suggestions \nfor improving this paper. References [1] A. Barabasi. Linked: the New Science of Networks. Perseus Press, \nNew York, 2002. [2] A. L. Barabasi and R. Albert. Emergence of scaling in random networks. Science, 286:509 \n512, 1999. [3] D. W. Clark and C. C. Green. An empirical study of list structure in lisp. Commun. ACM, \n20(2):78 87, 1977. [4] C. Collberg, G. Myles, and M. Stepp. An empirical study of Java bytecode programs. \nTechnical Report TR04-11, Department of Computer Science, Univeristy of Arizona, 2004. [5] S. Dieckmann \nand U. H\u00a8olzle. A study of the allocation behavior of the SPECjvm98 Java benchmarks. In 13th European \nConference on Object-Oriented Programming, pages 92 115, 1999. [6] K. Ehrlich and E. Soloway. Human factors \nin computer systems, chapter An empirical investigation of the tacit plan knowledge in programming, pages \n113 133. Ablex Publishing Corp., Norwood, NJ, USA, 1984. [7] N. E. Fenton and S. L. P.eeger. Software \nMetrics: A Rigorous &#38; Practical Approach. PWS Publishing Company, second edition, 1997. [8] J. Frederick \nP. Brooks. Three great challenges for half-century-old computer science. J. ACM, 50(1):25 26, 2003. [9] \nE. Gamma, R. Helm, R. Johnson, and J. Vlissides. Design Patterns: Elements of Reusable Object-Oriented \nSoftware. Addison-Wesley Professional Computing Series. Addison-Wesley, 1995. [10] J. Y. Gil and I. Maman. \nMicro patterns in Java code. In OOPSLA 05: Proceedings of the 20th annual ACM SIGPLAN conference on Object \noriented programming systems languages and applications, pages 97 116, New York, NY, USA, 2005. ACM Press. \n[11] J. Gosling, B. Joy, G. Steele, and G. Bracha. The Java(tm) Language Speci.cation. Addison-Wesley, \n2000. [12] C. Grothoff, J. Palsberg, and J. Vitek. Encapsulating objects with con.ned types. In OOPSLA \n01: Proceedings of the 16th ACM SIGPLAN conference on Object oriented programming, systems, languages, \nand applications, pages 241 255, New York, NY, USA, 2001. ACM Press. [13] J. Heering. Quanti.cation of \nstructural information: on a question raised by Brooks. SIGSOFT Softw. Eng. Notes, 28(3):6 6, 2003. [14] \nS. Hunston. Corpora in Applied Linguistics. Cambridge University Press, 2002. [15] C. Jones. Programming \nproductivity. McGraw-Hill, Inc., New York, NY, USA, 1986. [16] D. E. Knuth. An empirical study of FORTRAN \nprograms. Software Practice and Experience, 1(2):105 133, 1971. [17] P. Kruchten. The Rational Uni.ed \nProcess: An Introduction, Second Edition. Addison-Wesley, United States of America, 2000. [18] J. Laherrere \nand D. Sornette. Stretched exponential distributions in nature and economy: fat tails with characteristic \nscales. The European Physical Journal B, 2:525, 1998. [19] H. Melton and E. Tempero. An empirical study \nof cycles among classes in Java. Technical Report UoA-SE-2006-1, Department of Computer Science, University \nof Auckland, 2006. [20] H. Melton and E. Tempero. Identifying refactoring opportunities by identifying \ndependency cycles. In V. Estivill-Castro and G. Dobbie, editors, Twenty-Ninth Australasian Computer Science \nConference, Hobart, Tasmania, Australia, Jan. 2006. Proceedings published as Conferences in Research \nand Practice in Information Technology, Vol. 48 . [21] M. E. J. Newman. Power laws, Pareto distributions \nand Zipf s law. Contemporary Physics, 46(5):323 351, Sept. 2005. [22] J. Noble and R. Biddle. Visualising \n1,051 visual programs module choice and layout in the nord modular patch language. In CRPITS 01: Australian \nsymposium on Information visualisation, pages 121 127, Darlinghurst, Australia, Australia, 2001. Australian \nComputer Society, Inc. [23] J. Noble and R. Biddle. Software Visualization, chapter Visual Program Visualisation. \nKluwer, 2003. [24] Object Management Group. Uni.ed Modeling Language (UML) 1.5 speci.cation, 2004. [25] \nA. Potanin, J. Noble, and R. Biddle. Checking ownership and con.nement. Concurrency -Practice and Experience, \n16(7):671 687, 2004. [26] A. Potanin, J. Noble, M. Frean, and R. Biddle. Scale-free geometry in OO programs. \nCommun. ACM, 48(5):99 103, 2005. [27] S. Purao and V. Vaishnavi. Product metrics for object-oriented \nsystems. ACM Comput. Surv., 35(2):191 221, 2003. [28] R. V. Sol\u00b4e, R. Ferrer-Cancho, J. M. Montoya, and \nS. Valverde. Selection, tinkering, and emergence in complex networks. Complex., 8(1):20 33, 2002. [29] \nC. Szyperski. Component Software: Beyond Object-Oriented Programming. Addison-Wesley, 1998. [30] S. Valverde, \nR. Ferrer-Cancho, and R. V. Sol\u00b4e. Scale-free networks from optimal design. Europhysics Letters, 60(4):512 \n517, Nov. 2002. [31] S. Valverde and R. V. Sol\u00b4e. Hierarchical small-worlds in software architecture. \nUnder review, IEEE Transactions in Software Engi\u00adneering. An earlier versino is available as Sante Fe \nInstitute Working Paper 03-07-044, 2005. [32] W. Weibull. A statistical distribution function of wide \napplicability. ASME Journal Of Applied Mechanics, pages 293 297, Sept. 1951. [33] G. M. Weinberg. The \nPsychology of Computer Programming. John Wiley &#38; Sons, Inc., New York, NY, USA, 1985. [34] R. Wheeldon \nand S. Counsell. Power law distributions in class relationships. In Third IEEE International Workshop \non Source Code Analysis and Manipulation (SCAM03), 2003.  Appendix A: Formal de.nitions for Metrics \n This appendix contains more formal de.nitions of the metrics we compute as computed from the byte code \n(the .class .les). As mentioned in Section 3, the de.nitions assume one top-level[11] type declaration \nper source .le (.java .le). Let S =the set of source .les in the application under consideration. Under \nour assumption, every top-level class C is declared in a source .le C.java that in turns generates a \n.le C.class (which is what is used to compute the metrics). We express many of the metrics in terms of \nsource .les because it makes some de.nitions easier to explain. We de.ne the following notation: For \ntop-level classes A and B, A DEPENDS ON B if B s name appears in the constant pool of A.class.  For \na type T ,and .le u, T IS DECLARED IN u is true if and only if there is a declaration for T in u. Note \nthat u is not necessarily T.java, it could be the equivalent of B in the example above, and T could also \nbe a inner type declaration.  T ={T |T IS DECLARED IN u, u .S}. Note that this set is not just the top-level \ntypes, but also includes inner types.  METHODS(T ) =the set of methods declared in T (appear in the \n.class .les).  FIELDS(T ) =the set of .elds declared in T (appear in the .class .les).  ISCLASS(T )istrue \niff T is a class.  ISINTERFACE(T )istrue iff T is an interface.  ISCONSTRUCTOR(c)is trueiff c is a \nconstructor.  For C, D .T where (ISCLASS(C). ISCLASS(D). ISINTERFACE(C). ISINTERFACE(D))is true, C \nEXTENDS D if D appears in C s extends clause in its declaration.  For C, I .T where (ISCLASS(C). ISINTERFACE(I))is \ntrue, C IMPLEMENTS I if I appears in C s implements clause  The following de.nitions apply only to top-level \ntype declarations. We will use the conventions that C and D refer to classes, I refers to an interface, \nT refers any type, u refers to a source .le, m refers to a method, and f refers to a .eld. Number of \nMethods nM(C)=|METHODS(C)| Number of Fields nF(C)=|FIELDS(C)| Number of Constructors nC(C)=|{m :m . METHODS(C), \nISCONSTRUCTOR(m)}| Subclasses SP(C)=|{u :u .S, .D, D IS DECLARED IN u, D EXTENDS C}| Implemented Interfaces \nIP(I)=|{u :u .S, .D, D IS DECLARED IN u, D IMPLEMENTS I}| Interface Implementations IC(T )= |{u : u .S, \n.I, ISINTERFACE(I),I IS DECLARED IN u, T IMPLEMENTS I}| if ISCLASS(T ) |{u :u .S, .I, ISINTERFACE(I),I \nIS DECLARED IN u, T EXTENDS I}| if ISINTERFACE(T ) References to class as a member AP(C)=|{u :u .S, .D, \nD IS DECLARED IN u, C IS FIELDTYPE OF D}| Members of class type AC(C)=|{T :T .T ,T IS FIELDTYPE OF C}| \nReferences to class as a parameter PP(C)=|{u :u .S, .D, D IS DECLARED IN u, C IS PARAMETERTYPE OF D}| \nParameter-type class references PC(C)=|{T :T .T , .m . METHODS(C),T IS PARAMETERTYPE OF m}| References \nto class as return type RP(C)=|{u :u .S, .D, D IS DECLARED IN u, .m . METHODS(D),C IS RETURNTYPE OF m}| \nMethods returning classes RC(C)=|{T :T .T , .m . METHODS(C),T IS RETURNTYPE OF m}| Depends On DO(C)=|{u \n:u .S, .D, D IS DECLARED IN u, C DEPENDS ON D}| Inverse of Depends On DOinv(C)=|{u :u .S, .D, D IS DECLARED \nIN u, D DEPENDS ON C}| Public Method Count PubMC(C)=|{m :m . METHODS(C), ISPUBLIC(m)| Package Size PkgSize(p)=number \nof top-level classes in p. Method size MS(m)=number of byte code instructions in m. Note that this is \nnot the number of bytes needed to represent the method. Appendix B: Corpus details This appendix provides \nthe details of the part of the corpus used in this study. We use the standard naming scheme for each \napplication, which typically includes some kind of version identi.cation. The domain comes from our assessment \nbased on the application documentation. We identify where we acquired the source code. The column O/C \nrefers to whether the application can be considered open or closed source (all applications used here \nare open source). The column V identi.es where we have multiple versions (we only used the latest version \nin this study). Finally, any notes that seem relevant are provided. Application #    aglets-2.0.2 \nant-1.6.5 antlr-2.7.5 aoi-2.2 argouml-0.18.1 axion-1.0-M2 azureus-2.3.0.4 colt-1.2.0 columba-1.0 compiere-251e \nderby-10.1.1.0 drjava\u00ad20050814 eclipse-SDK\u00ad3.1-win32 .tjava-1.1 .tlibraryfor.tnesse\u00ad20050923 galleon-1.8.0 \nganttproject\u00ad 1.11.1 geronimo-1.0-M5 glass.sh-9.0\u00adb15 hibernate-3.1\u00adrc2 hsqldb-1.8.0.2 ireport-0.5.2 \njag-5.0.1 jaga-1.0.b james-2.2.0 jasperreports\u00ad 1.1.0 javacc-3.2 jboss-4.0.3-SP1 280 700 209 415 1251 \n237 1650 269 1180 1372 1386 668 11413 37 124 243 310 1719 582 902 217 347 208 100 259 633 125 4143 Domain \nFramework for developing mo\u00adbile agents Java build tool Parser genera\u00adtor 3D modelling and rendering \nUML draw\u00ading/critic SQL database P2P .lesharing High per\u00adformance collections library Email client ERP \nand CRM SQL database IDE IDE Automated testing Automated testing TiVo media server Gantt chart drawing \nJ2EE server J2EE server Persistence ob\u00adject mapper SQL database Visual report design for JasperReports \nJ2EE applica\u00adtion generator API for genetic algorithms Enterprise mail server Reporting tool Parser genera\u00adtor \nJ2EE server Origin Sourceforge Apache antlr.org Sourceforge tigris.org tigris.org Sourceforge hoschek.home.cern. \nch Sourceforge Sourceforge Apache Jakarta Sourceforge www.eclipse.org fit.c2.com Sourceforge Sourceforge \nSourceforge Apache dev.java.net Sourceforge Sourceforge Sourceforge Sourceforge jaga.org Apache Sourceforge \ndev.java.net Sourceforge O/C O O O O O O O O O O O O O O O O O O O O O O O O O O O O V N Y N N Y N \nY Y N N N N Y N N N N N N N N N N N N N N N Notes Donated by IBM Donated by IBM Donated by IBM Continued \non next page Table 4 continued from previous page Application #Classes Domain Origin O/C V Notes jchempaint\u00ad \n612 Editor for Sourceforge O N 2.0.12 2D molecular structures jedit-4.2 234 Text editor Sourceforge O \nN jeppers\u00ad 20 Spreadsheet ed- Sourceforge O N 20050607 itor jetty-5.1.8 327 HTTP Sourceforge O N Server/servlet \ncontainer jext-5.0 353 IDE Sourceforge O N jfreechart\u00ad 469 Chart drawing Sourceforge O N 1.0.0-rc1 jgraph-5.7.4.3 \n50 Graph drawing Sourceforge O Y jhotdraw-6.0.1 300 Graph drawing Sourceforge O Y jmeter-2.1.1 560 Java \nperfor- Apache O Y mance testing joggplayer\u00ad 114 MP3 player joggplayer. O N 1.1.4s webarts.bc.ca jparse-0.96 \n69 Java compiler www.ittc.ku.edu/ O N front-end JParse jre-1.4.2.04 7257 JRE sun.com O N jrefactory\u00ad \n211 Refactoring Sourceforge O N 2.9.19 tool for Java jtopen-4.9 2857 Java toolbox Sourceforge O N Donated \nby IBM for iSeries and AS/400 servers jung-1.7.1 454 Graph drawing Sourceforge O Y junit-3.8.1 48 Unit \ntesting Sourceforge O N framework lucene-1.4.3 170 Text indexing Apache O Y megamek\u00ad 455 Game Sourceforge \nO N 2005.10.11 netbeans-4.1 8406 IDE netbeans.org O Y Donated By Sun openof.ce\u00ad 2925 Of.ce suite openof.ce.org \nO N Donated By Sun 2.0.0 pmd-3.3 375 Java code anal- Sourceforge O N yser poi-2.5.1 480 API to access \nApache O N Microsoft for\u00ad mat .les rssowl-1.2 189 RSS Reader Sourceforge O N sablecc-3.1 199 Compiler/ \nSourceforge O N Interpreter generating framework sandmark-3.4 837 Software www.cs.arizona. O N watermark\u00ad \nedu/sandmark ing/security scala-1.4.0.3 654 Multi\u00ad scala.epfl.ch O N paradigm programming language sequoiaerp\u00ad \n936 ERP and CRM Sourceforge O N 0.8.2-RC1-all\u00ad platforms tomcat-5.0.28 892 Servlet con- Apache O N tainer \n  \n\t\t\t", "proc_id": "1167473", "abstract": "Large amounts of Java software have been written since the language's escape into unsuspecting software ecology more than ten years ago. Surprisingly little is known about the structure of Java programs in the wild: about the way methods are grouped into classes and then into packages, the way packages relate to each other, or the way inheritance and composition are used to put these programs together. We present the results of the first in-depth study of the structure of Java programs. We have collected a number of Java programs and measured their key structural attributes. We have found evidence that some relationships follow <i>power-laws</i>, while others do not. We have also observed variations that seem related to some characteristic of the application itself. This study provides important information for researchers who can investigate how and why the structural relationships we find may have originated, what they portend, and how they can be managed.", "authors": [{"name": "Gareth Baxter", "author_profile_id": "81319488085", "affiliation": "Victoria University of Wellington, Wellington, New Zealand", "person_id": "P813670", "email_address": "", "orcid_id": ""}, {"name": "Marcus Frean", "author_profile_id": "81100509975", "affiliation": "Victoria University of Wellington, Wellington, New Zealand", "person_id": "PP18001980", "email_address": "", "orcid_id": ""}, {"name": "James Noble", "author_profile_id": "81100588708", "affiliation": "Victoria University of Wellington, Wellington, New Zealand", "person_id": "PP15036977", "email_address": "", "orcid_id": ""}, {"name": "Mark Rickerby", "author_profile_id": "81319500110", "affiliation": "Victoria University of Wellington, Wellington, New Zealand", "person_id": "P813674", "email_address": "", "orcid_id": ""}, {"name": "Hayden Smith", "author_profile_id": "81319501469", "affiliation": "Victoria University of Wellington, Wellington, New Zealand", "person_id": "P813671", "email_address": "", "orcid_id": ""}, {"name": "Matt Visser", "author_profile_id": "81319503083", "affiliation": "Victoria University of Wellington, Wellington, New Zealand", "person_id": "P813675", "email_address": "", "orcid_id": ""}, {"name": "Hayden Melton", "author_profile_id": "81331499236", "affiliation": "University of Auckland, Auckland, New Zealand", "person_id": "P799443", "email_address": "", "orcid_id": ""}, {"name": "Ewan Tempero", "author_profile_id": "81100213613", "affiliation": "University of Auckland, Auckland, New Zealand", "person_id": "PP14083915", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1167473.1167507", "year": "2006", "article_id": "1167507", "conference": "OOPSLA", "title": "Understanding the shape of Java software", "url": "http://dl.acm.org/citation.cfm?id=1167507"}