{"article_publication_date": "01-01-1976", "fulltext": "\n Code Generation for Expressions with. Common Subexpressions EXTENDED ABSTRA CT A. V. Aho and S. C. Johnson \nBell Laboratories, Murray Hill, New Jersey 07974 J. D. Ullman* Princeton University Princeton, New Jersey \n08540 1. Introduction Easy as the task may seem, many compilers generate rather inefficient code. Some \nof the difficulty of generating good code may arise from the lack of realistic models for programming \nlanguage and machine semantics. In this paper we show that the computational complexity of gen- erating \nefficient code in realistic situations may also be a major cause of difficulty in the design of good \ncompilers. We consider the problem of generat- ing optimal code for a set of expressions. If the set \nof expressions has no common sub- expressions, then a number of efficient op- timal code generation algorithms \nare known for wide classes of machines [SU, AJ, BL]. In the presence of common subex-pressions, however, \nBruno and Sethi have shown that the problem of producing op- timal code for a set of expre~ssions is \nNP- complete, even on a single register machine [BS, S1]. However, Bruno and Sethi's proof of NP-completeness \nuses rather complex expressions, so it leaves some hope of being able to find efficient algorithms for \ngenerat- ing optimal code for restricted classes of ex- pressions with common subexpressions. Unfortunately, \nwe show in this paper that the problem of optimal code generation * Work partially supported by NSF grant \nDCR74-15255. remains NP-complete even for expressions in which no shared term is a subexpression of any \nother shared term. We also show that the optimal code generation problem is NP-complete for these expressions \non two-address machines, even when the number of registers is unlimited. Faced with these negative results, \nwe consider both heuristic and exact solutions for generating code. First, we investigate the worst case \nperformance of a collection of fast heuristics for single and multiregister machines. One seemingly reasonable \nheuristic is shown to produce code that is in the worst case three times as long as op-timal; other heuristics \nare given which have a worst case of 3/2 for one-register machines. Then, we present for the single regis- \nter machine an algorithm which generates optimal code and whose time complexity is linear in the size \nof an expression and ex-ponential only in the amount of sharing. Since the number of common subexpres-sions \nin expressions tends to be limited in practical situations, this approach appears at- tractive. Finally, \nafter discussing code gen- eration for commutative machines, we con- clude with a list of open problems. \n2. Background and Definitions 2.1 Dags For the purposes of this paper we can assume we have a compiler \nof the form .~ Front'~ Dag ~ Code End I Ma~ !Generator[ The front end translates a source program into \na sequence of straight-line intermediate code segments, called basic blocks, along with information that \nidentifies the flow of control among basic blocks. Within a basic block the flow of control is sequential. \nEach basic block consists of a se-quence of assignment statements of the form a~-bopc where a, b, and \nc are distinct variables and op is any binary operator for which there exists a corresponding machine \noperation. Since we shall concentrate on generating code for basic blocks in this paper, the flow of \ncontrol information will not be men-tioned further. The dag maker transforms the basic blocks into a \ndirected acyclic graph (dag, for short) that represents the computations of the expressions in the block. \n(See [AU] or [CS] for algorithms to construct a dag from a basic block.) We shall not consider the use \nof algebraic identities to transform dags to make them easier to compute; this has been discussed in \n[Brl, [Fa]. Fig. 1 shows a basic block and its corresponding dag. We call node 2 a right child of node \n3, and a left child of node 4. For symmetry, we call node 4 a right parent of node 2, and node 3 a/eft \nparent of node 2. We say node x uses a node y if y is ei- ther a left or right child of x. We say x left \nuses (resp. right uses) y if y is a left (resp. right) child of x. u 1 *'--c --d u2*-b +u 1 u3 \"-a * \nu 2 U4 ~-'U2 * U 1 U 5 ~ U 3 + U 4 Basic Block Dag Fig. 1 Basic Block and its Dag A node with no children \nis called a leaf. A node with no parents is called a root. Nodes that are not leaves are often referred \nto as interior nodes. We assume for simplicity that all operations are binary. We ignore con-straints \nthat may be introduced into the dag because of side effects. For example, sup- pose two nodes of a dag \nrepresent the operation of indirect assignment through a pointer. If the two pointers could point to \nthe same datum, and the source code specified an order for the two assignments, then an edge in the dag \nconnecting the two nodes in the proper order must be intro-duced. 2.2 The Machine Model We assume the \ncode generator is to produce code for a multiregister two-address machine. The instructions of the machine \nare of the form (1) r~ .-- r i op ~/ /* op-instruction */ (2) r i \"-r i op m /* op-instruction */ (3) \nr~ .-- r/ /* register copy */ (4) r i ~-m /* load */  (5) m~---~ /* store */ Here r i and \u00a7. are any \nof N >/ 1 registers, and m is any memory location; op stands for any machine operation. When a machine \nhas only one register (i.e., N = 1), then there are only type (2), type (4), and type (5) instructions. \nA machine program is a sequence of in- structions. The length of a program is the number of instructions \nit contains. Definition: The optimal code generation prob- lem (OCG) is to produce from a dag a shor- \ntest machine program that evaluates and stores all roots of the dag. When discussing single and multireg-ister \nmachines, we assume the leaves of the dag are labeled by memory locations and the interior nodes (non-leaves) \nby machine operations. We also assume for conveni-ence that a dag has no root that is a leaf. Example \n1: The dag of Figure 1 can be evaluated on a two-register machine and stored in memory location m by \nthe se-quence of instructions: r 1 ~c /*loadc*/ r I ,--- r 1 -d /* evaluate node 1 */ r 2 \"-- b /* load \nb */ r 2 ,--- r 2 + r 1 /* evaluate 2 */ t ~----r 1 /* store 1 */ r t ~--- a /* load a */ r I ,-- r 1 \n* r 2 /* evaluate 4 */ r 2 ~ r 2 * t /* evaluate 3 */ r I ~--- r 1 + r 2 /* evaluate 5 */ m ~ q /* store \nroot */ [] In Section 6, we shall discuss code 3. Why OCG is Hard Bruno and Sethi have shown that OCG \nis NP-complete even for one-register machines. Their proof technique was to po- lynomially transform \nthe satisfiability prob- lem with three literals per clause (see [AHU], e.g.) to OCG. Their technique, \nhowever, resulted in rather complex dags. We begin by showing that OCG is NP-complete for one-register \nmachines even on a rather simple class of dags. A node both of whose children are leaves is called a \nlevel-one node. A shared node in a dag is a node with more than one parent. A level-one dag is a dag \nin which every shared node is a level-one node. A leafdag is a dag in which every shared node is a leaf \n[C]. Several code generation algorithms for one-register machines make use of the no-tion of a left chain, \nthat is, a sequence of in- terior nodes n t , n 2 ..... n k such that n i is the left child of hi+ 1 \nfor 1 ~< i < k. For example, in the dag of Figure 1, 3-5 and 2-4 are the only nontrivial left chains. \nThe first (lowest) node on a left chain is called its tail, and the last (highest) node its head. Definition: \nThe feedback node set problem (FNS) is: Given a directed graph G, find a smallest set of nodes F (a feedback \nnode set) such that removing F from G eliminates all cycles. FNS is a well-known NP-eomplete problem \n(see [AHU], e.g.). Theorem 1: OCG for level-one dags on a one-register machine is NP-complete. Proof.\" \nWe show how to polynornially transform an instance of FNS to OCG. Let generation for commutative machines; \ni.e., machines in which for every type (1) and (2) op-instruction above, thege is also an in- struction \nof the form (1') rj \"- ~ op ri (2') r i .-'- m op r i Even for noncommutative machines, OCG is a very \ndifficult problem; the next section discusses why. G be the directed graph in the feedback node set problem. \nFrom G construct a dag D as follows. For each node n in G of out- degree d ~> 0 create a corresponding \nleft chain of d+ 1 interior nodes n 0, n I ..... n d in D; no is the tail and n d the head of the chain. \nMake no a level-one node by giving it left and right children labeled by memory locations. The remaining \nedges of D are deter-mined as follows. Suppose the out-edges of n are directed to nodes m I , m 2 ..... \nm d in G. Make the tails of the left chains 21 corresponding to m I , m 2 ..... m d right children of \ntl I , I12 ..... rid, respectively, in D. Fig. 2(a) shows a directed graph G and Fig. 2(b) the dag resulting \nfrom G using the construction above. (a) Directed Graph G 2, (b) Resulting Dag D Fig. 2 Graph and Corresponding \nDag We must now show that we can con-struct a minimal feedback node set F for G from an optimal program \nP for D and con-versely. It can be shown that an optimal program for a one-register machine does not \nstore and load any uniquely left-used interi- or nodes of a dag (interior nodes with exact- ly one left \nuse and no right uses). Thus ex-cept for the loads of leaves, the only loads in P are loads of some level-one \nnodes. It can be seen that these level-one nodes identify a feedback node set F in G. Con-versely, given \na minimal feedback node set F of G we can construct an optimal program for D by first evaluating and \nstoring the tails in D of the left chains corresponding to the nodes in F. For example, {d} is a minimal \nfeed-back node set for the directed graph G of Fig. 2(a). The optimal program P corresponding to this \nfeedback node set first computes node d o of D in Fig. 2(b). Then P evaluates c 0, c I , b 0, b I , b \n2, a o, a I , d I . To evaluate d 1, P needs to load do; this is the only level-one node loaded by P. \n[] To appreciate the difficulty of generat- ing optimal code for dags, let us assume we are generating \ncode for an infinite register machine, a machine in which the number of registers is unbounded. To eliminate \nthe problem of deciding what to store or load, let us further assume the leaves of the dag are labeled \nby register names rather than by memory locations; similarly, let us assume we need not store the roots. \nThe relevant instructions of the infinite register machine then become ri ,--- ri op ri ri ,--- fi Even \nin this highly simplified environment, the optimal code generation problem is NP-complete. Theorem 2: \nOCG for leaf dags on an infinite register machine is NP-complete. Prool? Similar to that of Theorem 1. \n[] Thus, even if the problems of code selection and storage of intermediate values are made trivial, \njust finding an optimal evaluation order for the nodes of a dag is an NP-complete problem. On the other \nhand, if we perturb the infinite register machine architecture by permitting arbitrary three-address \ninstructions of the form r i ~---- rj op r k, then we can generate optimal code for arbitrary dags in \nlinear time. We simply evaluate the dag bottom up, level by level, assigning a distinct register to each \nnode. The three main problems in code gen- eration are what instructions to use, in what order to do \nthe computations, and what values to keep in registers. The results of this section indicate that, for \ntwo-address machines, just deciding the order in which instructions are to be executed is an NP-complete \nproblem. 22  4. Heuristic Techniques Since even simple versions of the op- timal code generation problem \nare NP-complete, it is not surprising that in the past code generation algorithms for dags usually have \nmade several restricting assumptions. One approach has been to ignore sharing by representing a set of \nexpressions as a forest of trees. For this case a number of optimal code generation algorithms have been \ndeveloped [SU], [A J], [BL], [Was]. Another approach has been to avoid the problem of finding an optimal \nevaluation order by tak- ing some fixed order for the nodes of a dag and then concentrating on optimal \nutiliza- tion of registers ([Bea], [HKMW], [WJWHG], [Fr], e.g.). Optimal code cannot be guaranteed, however, \nwithout consider- ing sharing and retaining the freedom to reorder code that is inherent in the source \nprogram. When faced with an NP-complete problem, there are two standard approaches: (1) develop and analyze \nheuristics, and (2) look for useful special cases that  have polynomial time algorithms. We shall \nconsider both approaches here. For the analysis of heuristics we use the worst case measure, a time-honored \nway of measuring the goodness of a heuristic which may be applied to various populations of data. For \nour purposes, we define the worst case of an algorithm to be the supremum over all dags of the ratio \nof the length of the code produced by that algorithm to the length of the optimal code for the dag. 4.1 \nTwo Methods of Accounting for Costs There is a usual way .of charging the interior nodes of a dag for \nthe cost of their evaluation. Charge a node one unit of cost for each of: (1) performing its operation, \n(2) storing its value, and (3) loading its left child or copying its left child from another register. \nWe call a program semi-intelligent if (1) it performs no useless instructions, i.e., instructions which \ncan be deleted without changing the value of the program, (2) it never moves (via loads or register-to-register \ntransfers) a value into a register without subsequently left using that value, and (3) it never stores \nthe same value more than once.  It is easy to check that a semi-intelligent program will have each of \nits in- structions assigned to some one node by the above scheme. Formally, we may show the following. \nTheorem 3: Let P* be an optimal program for some dag D, and let P be any semi-intelligent program for \nD. Then the ratio of the length of P to that of P* is at most 3. Proof.\" Using the above costing scheme, \nevery interior node of D is assigned at least one instruction, but never more than three instructions \n(a load, an operation, and a store). [] Obviously there are an infinite number of programs to evaluate \nany dag. We shall restrict ourselves to semi-intelligent programs. Doing so serves only to rule out blatantly \ninetiicient programs. There is little loss of generality since we can construct from any program P an \nequivalent semi-intelligent program in time proportion- al to the length of P. There is a second cost \naccounting scheme which we find quite useful. This scheme gives the same overall cost as the scheme above \nbut the cost units are appor- tioned differently amongst the nodes. We now charge each node for every \ninstruction that affects its value, i. e., for the operation that computes its value, and any loads, \nstores, or register-to-register copies of that value. We call this cost accounting scheme the use-cost. \nIf n is a node of a dag, let i(n) and r(n) be the number of times n is used as a left and right child, \nrespectively, of some other node. In the dag of Figure 1, for ex- ample, /(1) --0 and r(1) z2. Also, \n/(2) --r(2) = 1. Lemma 1: The following costs are upper and lower bounds on the use-cost of a node n \nwith respect to any semi-intelligent program on a one-register machine. 23 case lower upper (1) n is \na leaf /(n) /(n) n is not a leaf and: (2) /(n) = 0 2 2 (3) /(n) > 1 /(n)+l /(n)+2  (4) /(n) = 1 1 3 \nand r(n) = 0 (5) /(17) = 1 2 3 and r(n) > 0 Prool? Let us consider case (3) as an exam-ple. Surely an \nop-instruction evaluating n is necessary. Since n is used more than once and there is only one register, \none store in-struction is also necessary. Every time n is left used, its value in the register is des- \ntroyed, and it. must be reloaded the next time it is left used. Thus, /(n) -1 load in- structions are \nnecessary, and I(n) load in-structions are sufficient by condition (2) in the definition of semi-intelligent. \nThere-fore, a total of /(n)+ 1 or /(n)+2 instruc-tions is needed. [] Lemma 2: For a multiregister machine, \nLem- ma 1 holds with the lower bound set to 1 in cases (2) and (5) and to/(n) in case (3). Prooj? Omitted. \n[] We observe from Lemma 1 that it is only interior nodes with one left parent and no right parent that \ncould give us a worst case ratio of 3 for the one-register machine. Interestingly, it is quite easy to \nhandle such cases. If node n is uniquely left-used, we can alwaysarrange to have n evaluated im-mediately \nbefore its parent. Thus it is un-necessary to load or store n, and it achieves a use-cost of 1. Therefore, \nwe can state the following. Theorem 4: Any. algorithm for a one-register machine which generates semi-intelligent \nprograms that avoid storing uniquely left-used nodes has a worst case no greater than 3/2. Note that \nTheorem 4 fails to hold in the multiregister case, since /(n) = 1, r(n) > 0 is another case that can \nyield a worst case ratio of 3, and some other cases yield a ratio of 2. We must also point out that the \nuse-cost can, in some cases, underestimate the cost of an optimal program on a one-register machine by \na factor of 3/2. Fig. 3, for ex-ample, shows a dag whose lower bound use-cost is 6+2p. This lower bound, \nhow-ever, is not achievable since any program evaluating this dag must store and subse-quently load each \nof the +-nodes. The cost of an optimal program for the dag is 6 + 3p. 'p nodes Fig. 3 Underestimated \nDag Thus, if we found an algorithm with a worst case ratio less than 3/2, the proof of that worst case \nbound must use a more so-phisticated cost analysis than the above.  4.2 Heuristics for One Register \nMachines An evaluation order for a dag is any to- pological sort of the interior nodes of the dag. For \na one-register machine, from an evaluation order we can easily construct a program that is as short as \nany other which computes the dag in that order. Thus an al-gorithm that produces an evaluation order \nfor a dag is in effect a code generation algo- rithm for a single register machine. In this section we \nanalyze the performance of some 24 simple heuristics for creating evaluation ord- ers for single register \nmachines. Definition: The Bottom-up Greedy Algorithm (BUG) creates an evaluation order for a dag by repeatedly \nlisting the nodes of a longest left chain that can be currently evaluated. To evaluate a node, both its \nchildren must have been previously evaluated. Note that it is permissible for the right child of a node \nin a chain to be an unevaluated node which is lower on the same chain. Example 3.\" Return to the dag \nof Figure 1. The only left chain that carl be listed initial- ly is 1. Then the chain 2-4 can be listed, \nand finally the chain 3-5, giving an evalua-tion order of 12435. In this case the code is: r I *--c r \n1 ~-- r I --d t *\"-r 1 r 1 *---b r I *---r 1 + t u *--'r 1 r I *\"- r I * t v *---r 1 r I ,---a r I *--- \nr I * u r I ~ r 1 -I- v m ,---r I By Lemma 1, this program is optimal. [] Theorem 5.\" BUG has a worst \ncase ratio of 3. Proof\" Figure 4 shows how the worst case of 3 can be approached arbitrarily closely. \nAt the right we show p nodes c 1, c 2 ..... Cp, called \"controllers,\" whose initial evaluation is necessary \nfor optimal code. If we evalu-ate the controllers first and store them, at a cost of 3p, we can go up \neach of the p left chains with a cost of p + 2 per chain, for a total of p 2 + 5p. However, BUG c~3uld \nselect c 1, then the bottom node of each chain, then c 2, then the next node of each chain, then c 3, \nand so on, using three instructions per inte- rior node. The worst case ratio for this ex-ample is3(p \n+ 1)/(p + 5). [] p Fig. 4 BUG Buggerer Definition: The Top-down Greedy Algorithm (TDG) ([AU], p. 866; \nsee also [Wai]) works by listing left chains in the reverse order of their evaluation. Repeatedly select \na node n all of whose parents, if any, have already been listed. Then list n and as many nodes of the \nleft chain with head n as may be list- ed. Note that a node may be listed only if all of its parents \nhave already been listed, as we are generating the evaluation sequence in reverse. Also, once an unlistable \nnode is encountered, we do not proceed further down the chain. After sequencing all interi- or nodes, \nreverse the list to get the evalua- tion order. Example 4: In the dag of Figure 1, we would select the \nroot 5 first and find we may list it and its left child 3. Then we could select 4, since its parent, \n5, has already been listed. We could proceed to 2, the left child of 4, since its parents, namely 3 and \n4, have been listed. Finally we list 1. Reversing the list gives 12435 again, so the same code as for \nBUG is produced in this case. [] TDG and BUG, however, are quite different in their worst case performance. \nTDG produces optimal code for Fig. 2 for which BUG produced the worst case code. 25 Theorem 6: TDG has \na worst case ratio of 3/2. Proof.\" To see that it is no worse than 3/2, note that if n is uniquely left-used \nby m, then n must be listed immediately after m, and therefore n is evaluated immediately be- fore m. \nThus case (4) of Lemma 1 is always handled correctly; the lower bound of one instruction charged to n \nis attained. All oth- er cases in Lemma 1 have a ratio of 3/2 or less. For the proof that 3/2 can be \nap-proached from below, consider the grid of Figure 5. The optimal sequence of evalua- tion goes up (the \nslanted) left chains, start- ing at the bottom right, storing each value with the exception of those \non the leftmost chain. Roughly two instructions per node are used in this evaluation sequence. On the \nother hand, TDG could list nodes row by row, from the right, taking three instruc- tions per node. [] \nFig. 5 Grid It is also .worth noting that TDG al-ways handles the case r(n) -~ O, I(n) > 1 correctly, \nsince n will be listed immediately after the last of its left parents to be listed. Thus the generated \ncode will have only I(n) --1 loads of n. In truth there is no magic about \"top down\" vs. \"bottom up\" \nalgorithms; by ex- ercising care in the selection of chains it is easy to construct a modification of \nBUG that has the same performance as TDG. Another 3/2 worst case algorithm can be obtained using depth \nfirst search. Definition: The Depth-first Search Algorithm (DFS) performs a depth-first search (see [AHU], \ne.g.) of the rag, preferring to move to the right child rather than the left when there is a choice. \nNodes are then evaluated in order of last visit. Theorem 7: The worst case ratio for DFS is 3/2. 4.3 \nHeuristics for Multlregister Machines The Top-down Greedy Algorithm can be generalized to the case of \nan N register machine. For a multiregister machine, how- ever, it is no longer sufficient to specify \nonly an evaluation order; we must also specify in which register a computation is to be done. The following \nprocedure lists the interior nodes of a dag in reverse evaluation order. The register assigned to a node \nis the regis- ter in which that node is to be computed. Stores and loads of registers are performed as \nneeded. procedure TDG(n, k);/* n is a node, k is the number of registers available */ if k >/ 1 and n \nis an interior node all of whose parents have been listed then begin list n and assign it register k; \nTDG(right child of n, k-1); TDG(left child of n, k)  end; /* main program */ while not all interior \nnodes have been listed do select an interior node n, all of whose parents have been listed, and perform \nTDG(n, N) Although TDG performs well in many cases, the worst case performance of TDG approaches 3 as \nNgets large. Theorem 8: The worst case ratio for the TDG Algorithm with N registers is no less than 3N/(N \n+ 1). 26 ProoJ? The grid of Figure 5 provides the essence of the proof. [] Demers [D] has considered \na generali- zation of DFS to multiregister machines in which a depth-first search is used to obtain an \nordering of the nodes, and then Beladay's algorithm [Bel] is used to allocate registers. 5. An Optimal \nAlgorithm for the One Regis- ter Machine We define s(n), the sharing of a node n in a dag, to be s(n) \n= /(n) + 1 ill(n) > 1 s(n) =2 if/(n) = 1 and r(n) > 0 s(n) = 0 otherwise The sharing, s, for a dag is \nthe sum of the s(n) over all interior nodes n in the dag. We shall now present an algorithm for the one \nregister case that is optimal and is of time complexity O(p2S), where p is the number of nodes in the \ndag and s is the amount of sharing. To introduce this algo- rithm, we note that any dag can be parti-tioned \nin various ways into left chains. Given a program for a dag, we can create a partition by looking for \nmaximal sequences of one or more consecutive operations unin- terrupted by loads. Each such consecutive \nsequence forms a left chain, and the set of left chains so formed partitions the interior nodes of a \ndag. We can obtain a partial converse of the above. We say that a partition of the interior vertices \nof a dag D is legal if the fol- lowing holds: Form a graph G whose nodes correspond to the left chains \nof D and with an edge from c I to c 2 whenever there is a path in D from some node of c l to some node \nof c 2. Then there must be no cycles in G. We may thus state the following. Lemma 3: There is an evaluation \norder pro-ducing a given partition of a dag into left chains if and only if that partition is legal. \nMore importantly, we can relate the cost of evaluation sequences to the costs of the heads of the left \nchains. Theorem 9: Let D be a dag. Then there is a constant c o with the following property: Suppose \nP is any semi-intelligent program evaluating D. Let the partition induced by P have k I chains whose \nheads are left used at least once. Let k 2 of these be uniquely left-used. Then the length of P is c \nD + k 1 + k 2. Conversely, if there is a program for D of cost c, then we can find a legal partition \ninto left chains with parame-ters k I and k 2 such that c = c D + k 1 + k 2 . ProoJ? c D is the sum over \nall interior nodes of D of the lower bound on cost given in Lemma 1. k 1 accounts for excess loads of \nleft-used nodes, that is, the /th load of a node left used / times. Note that a node will be loaded / \nrather than /-1 times if and only if it is the head ofachain, k 2 ac- counts for stores of uniquely left-used \nnodes. [] We intend to reduce the problem of finding an optimal program to that of finding a set of heads \nof chains. Clearly any node with /(n) =0 must be the head of a chain. A node with r(n) = 0 and /(n) = \n1 can always be attached to its left parent in a chain, as claimed in the follow- ing lemma. Lemma 4.\" \nIf zr is a legal partition of dag D, and there is a uniquely left-used node n which heads a chain, then \nthe partition formed from zr by removing n from its current chain and attaching it to the chain of its \nparent is also legal. Prooj? If there is a path to n in D, it must go through the unique parent of n. \n[] We thus see that the question of whether or not a node is the head of a chain in an optimal program \nis only un-resolved for those interior nodes with more than one parent, at least one of which is a left \nparent. We may thus try all subsets of these nodes, selecting those which are not the head of a chain. \nFor each selected node with more than one left parent, we must also select the left parent to whose chain \nit is attached. The number of these selections can be shown not tO exceed 2 s, where s is the sharing. \nWe may thus test each selec-tion, in order of lowest cost, until we find one that is legal. The test \nfor legality is seen to be O(p) on a p node dag. Thus: Theorem 10: There is an O(p2 s) algorithm for \nobtaining an optimal program for a dag. 6. Commutative Machines A commutative machine is one in which \nfor each instruction of the form r i ~-r i op r/ or r i ~--r iop m there is also an instruction of the \nform r i `-- ri op r i or r i `-- m op r r That is, the register used to hold the result can be the same \nas as either the left or the right register operand. This arrangement al- lows us to think of the order \nof the children of any node as being permutable, as far as code generation is concerned, although the \noperator itself may not be commutative. For one-register commutative machines the analog of a left-chain \nis a worm. A worm is any path in a dag, exclud- ing the leaves. We define a partition of a dag D into \nworms to be legal if the graph whose nodes are the worms of D, with an edge from w I to w 2 if and only \nif some node of worm w 1 has a path in Dtosome node of worm w 2, is acyclic. The following is an analog \nto Theorem 9. Theorem 11: Let D be a dag. Then there is a constant c D with the following property. Suppose \nD has a legal partition into worms such that there are k I worms whose heads are used (either left-or \nright-used), k 2 of which are uniquely used. Then there is a program for D on a commutative machine of \ncost c D -~ k I + k 2. Conversely, if there is a program for D with cost c, then we can find a legal \nworm partition with parameters k I and k 2 as above, such that c = c D + k 1 + k 2 . We can generalize \nthe top-down greedy algorithm to commutative machines by listing worms in reverse evaluation order. If \nwe always list a uniquely used child im-mediately after its parent, then we can show that 3/2 is a worst \ncase ratio for this algo-rithm for the commutative one-register machine. We can also produce an analog \nof the optimal algorithm of Section 5 which is po- lynomial in the size of the dag and ex-ponential only \nin the sharing. The follow-ing lemma is needed; it is not as strong a result as could be proved about \nworm parti-tions. Lemma 5: Let W be a legal worm partition of a dag. Suppose node n I is the head of \na worm w = n l, n 2 ..... n k in which n 1, n 2 ..... n k are all uniquely used. Then there is another \nworm partition W' of D with a worm of which n 1, n 2 ..... n k is a proper tail (that is, the new worm \nincludes at least the parent of n 1) such that the cost of the program induced by W' is no greater than \nthe cost of the program induced by W. ProoJ? Fig. 6 shows a fragment of a dag in which node m is on some \nworm; perhaps n, m's other child, is on the same worm. Fig. 6 Worm Construction Remove edge m--. n from \nthat worm if it is there, and add the edge m---,n I. This may make node n a head, but n I will no longer \nbe a head. It is easy to show that the cost of the partition is not increased, and the fact that n 1 \n, n 2 ..... n k are unique- ly used makes a proof of legality for the new partition easy. [] Theorem \n12: There is an O(n6 s) algorithm for finding an optimal program on a com-mutative machine for an n-node \ndag with sharing s. Prooj? The first step is to take the dag D and divide it into trees, an idea discussed \nby [Wai]. For each root or shared node n, we do the following. Find the maximal subtree with n as root \nwhich includes no other shared nodes, except as leaves. An example 28 is shown in Fig. 7. (a) Dag (b) \nTrees  Fig. 7 Division of a Dag~into Trees Note that each multiply-used node ap- pears as a leaf in \nat most one tree per use. If it is used by two nodes iri the same tree, divide the shared node into as \nmany leaves as necessary so that no leaf is used more than once. Note also that no non-shared node appears \nin more than one tree. The algorithm begins by determining which edges of tile various trees belong to \nworms. Later we shall combine the trees to allow worms to cross tree boundaries. Ima-gine worms proceeding \nupward from all leaves in a given subtree. The fact that worms do not actually reach the leaves can be \naccounted for later by removing edges from worms at the bottom. At each interior node n we must decide \nwhich, if any, of the worms reaching children of n include the node n. If either or both of the children \nof n are part of worms that begin at tree leaves of D or interior nodes of D that are not shared (recall \nsome leaves of a tree here may actually be shared nodes of D), then Lemma 5 assures us that we may allow \nany such worm to continue up to n. The only uncertainty occurs when both worms reach-ing the children \nof n originate at shared nodes of D. Then we must try all three possibilities -that either worm or neither \nincorporates n. In the last case, n begins a new worm, a situation which could be necessary to achieve \na legal worm partition. We see from the above that the only worms where there is uncertainty regarding \nhow far up to proceed are those which ori-ginate al~ shared nodes of D. If tree T i has k i leaves which \nare shared nodes of D, then there are at most 3 ki outcomes for the \"contests\" regarding which worm proceeds \nupwards. Moreover, for each shared node n that is a leaf of T i, n's worm in T/ may or may not connect \nwith that worm which in-cludes n in the tree of which n is the root. (Recall the edges from leaves to \ntheir parents are not really parts of worms.) Thus, we may either include or exclude the edges from node \nn to its parent in T i from the set of edges of D comprising worms. Thus for tree T/ there are at most \n6 ki subsets of edges which could possibly be the edges used in an optimal worm partition of D. Not all \nsubsets of edges need be con- sistent. For example, some node could be connected to two or more of its \nparents by selected edges. The number of possible sets of selected edges is no more than 116 k~ which \nis no greater than 6 s, where s is the total sharing of D. Each such'set of selected 29 edges can be \nchecked for consistency and legality in O(n) time, where n is the number of nodes of D. If legal, the \ncost of the partition can easily be computed in O(n) time by inspecting the worm heads. We can therefore \nrun through all the candi- date worm partitions in O(n6 s) time and select that one with the least cost. \n[] 7. Summary and Open Questions We have analyzed several simple heuristics for generating code for a \none-register machine, showing them to have a worst case of 3/2 or, in one case, 3. We have also shown \nfor the one-register machine, both in the commutative and non- commutative cases, that there are optimal \ncode generation algorithms which are linear in the size of the dag and exponential only in the sharing. \nAdditionally, we have shown that even some very simple code generation problems are NP-complete. We feel \nthat this work only scratches the surface of what can be learned about the important area of code generation \nalgo- rithms. We therefore propose the following questions as potentially fruitful areas for fu- ture \nresearch. 1. Is there an optimal algorithm for multiregister machines which is polynomial in the number \nof nodes and registers, and exponential only in the amount of sharing? 2. How closely can the optimal \ncode generation problem be approximated by a polynomial time heuristic on (a) single re-gister, (b) multiregister, \nand (c) infinite re-gister machines? In particular, can we, for all e > 0, develop polynomial time algo-rithms \nwith a worst case ratio of 1 + e? What about the same problems for commu- tative machines? 3. On some \nmachines certain opera-tions such as multiplication require an even-odd register pair. How do machine \nanomalies such as these affect the computa- tional complexity of code generation? Is optimal code generation \npolynomial, even for trees? 4. How difficult is it to generate code for a tree in which some of the \nleaves are labeled'by registers rather than memory 1o-  cations? The leaves whose values are in re- \ngisters cause a register to be freed when they are used. Sethi [$2] has shown that we can without loss \nof generality evaluate any subtree containing a leaf in a register, pro- vided we can do so with no stores, \nreplacing that subtree by a leaf in a register. The problem of what to, do when no such reduc- tions \nare possible appears NP-complete. Is it? Acknowledgements The authors wish to thank Brenda Baker, Brian \nKernighan, Doug Mcllroy, and Elliot Pinson for their helpful comments on the manuscript. References \n[AHU] A. V. Aho, J. E. Hopcroft and J. D. Ullman, The Design and Analysis of Computer Algorithms, Addison \nWesley, 1974. [AJ] A. V. Aho and S. C. Johnson, \"Optimal Code Generation for Expression Trees,\" Proc. \nSeventh Annual ACM Symposium on Theory of Computing, May 1975, pp. 207-217. [AU] A. V. Aho and J. D. \nUllman, The Theory of Parsing, Translation and Compiling, Vol. II, Compiling, Prentice Hall, 1973. [Bea] \nJ. C. Beatty, \"A Register Assignment Algorithm for Generation of Highly Optim- ised Object Code,\" IBM \nJ. Res. Dev. 18,1 (January 1974), 20-39. [Bel] L. A. Belady, \"A Study of Replace-ment Algorithms for \nVirtual Storage Com- puters,\" IBMSyst. J., 5:2 (1966) 78-101. [Br] M. A. Breuer, \"Generation of Optimal \nCode for Expressions via Factorization,\" Comm. ACM 12,6 (June 1969), 333-340. [BL] J. L. Bruno and T. \nLassagne, \"The Generation of Optimal Code for Stack Machines,\" J. ACM 22,3 (July 1975), 382- 397. [BS] \nJ. L. Bruno and R. Sethi, \"Register Al- location for a One-Register Machine,\" TR-157, Computer Science \nDept., Penn State Univ., University Park, Pa., Oct., 1974. [C] S. Chen, \"On the Sethi-Ullman Algo- 30 \n port #11, Bell Laboratories, Holmdel, N. J., May, 1973. [CS] J. Cocke and J. T. Schwartz, Program-mhlg \nLanguages and their Compilers (second edition) Courant Institute, NYU, New York, 1970. [D] A. Demers, \nprivate communication. [Fa] R. J. Fateman, \"Optimal Code for Serial and Parallel Computation,\" Comm. \nACM 12,12 (December 1969), 694-695. [Fr] R. A. Freiburghouse, \"Register Alloca- tion Via Usage Counts,\" \nComm. ACM 17,11 (November 1974), 638-642. [HKMW] L. P. Horowitz, R. M. Karp, R. E. Miller and S. Winograd, \n\"Index Register Al- location,\" J. ACM 13,1 (January 1966), 43- 61. [SI] R. Sethi, \"Complete Register \nAllocation Problems,\" SIAM J. Comput#tg 4,3 (Sep-tember 1975), 226-248. [$2] R. Sethi, private communication. \n[SU] R. Sethi and J. D. Ullman, \"The Gen- eration of Optimal Code for Arithmetic Ex- pressions,\" J. ACM \n17,4 (October 1970), 715-728. [Wai] W. M. Waite, \"Optimization,\" In Com-piler Construction: An Advanced \nCourse, F. L. Bauer and J. Eickel, eds., Springer-Verlag, 1974, pp. 549-602. [Was] S. G. Wasilew, \"A \nCompiler Writing System with Optimization Capabilities for Complex Order Structures,\" Ph.D. Thesis, Northwestern \nUniv., Evanston, Ill., 1971. [WJWHG] W. A. Wulf, R. K. Johnsson, C. B. Weinstock, S. O. Hobbs and C. \nM. Geschke, The Design of an Optimizing Com-piler, Elsevier, 1975.  \n\t\t\t", "proc_id": "800168", "abstract": "<p>Easy as the task may seem, many compilers generate rather inefficient code. Some of the difficulty of generating good code may arise from the lack of realistic models for programming language and machine semantics. In this paper we show that the computational complexity of generating efficient code in realistic situations may also be a major cause of difficulty in the design of good compilers.</p> <p>We consider the problem of generating optimal code for a set of expressions. If the set of expressions has no common sub-expressions, then a number of efficient optimal code generation algorithms are known for wide classes of machines [SU, AJ, BL].</p>", "authors": [{"name": "A. V. Aho", "author_profile_id": "81100024612", "affiliation": "", "person_id": "PP43126072", "email_address": "", "orcid_id": ""}, {"name": "S. C. Johnson", "author_profile_id": "81332506933", "affiliation": "", "person_id": "PP43126474", "email_address": "", "orcid_id": ""}, {"name": "J. D. Ullman", "author_profile_id": "81100314798", "affiliation": "", "person_id": "PP43144686", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/800168.811537", "year": "1976", "article_id": "811537", "conference": "POPL", "title": "Code generation for expressions with common subexpressions (Extended Abstract)", "url": "http://dl.acm.org/citation.cfm?id=811537"}