{"article_publication_date": "01-01-1976", "fulltext": "\n BINDING TIME OPTIMIZATION IN PROGRAMMING LANGUAGES: SOME THOUGHTS TOWARD THE DESIGN OF AN IDEAL LANGUAGE* \n Neil D. Jones &#38; Steven S. Muchnick Department of Computer Science The University of Kansas Lawrence, \nKansas 66045 ABSTRACT A new approach to the design of a programming language and its processor is proposed \nand some of the techniques necessary to realize the design are investigated. The language would have \na precisely specified syntax and semantics, with both designed to provide the programmer maximal expressive \npower and to be as easily understood as possible. The semantics would be based on extremely late binding \ntimes 9 which provide great power to the programmer and are consistent with ease of understanding of \nthe execution process. It would be the responsibility of the processor to implement each program in the \nmost efficient manner consistent with its being correctly executed. Implications of this design philosophy \nand some of the techniques to be used are discussed in greater detail, focusing particularly on data \ntypes and storage allocation. I. Introduction &#38; Overview In this paper we propose a new approach \nto the design o~ a programming language and its processor, and investigate some of the techniques necessitated \nby the design. In the past, when a programming language has been designed, an implicit set of binding \ntime assumptions has been a fixed part of its semantics. For example, FORTRAN has static storage binding; \nthe design of ALGOL 60 implies stacked allocation (except for own variables); PASCAL uses static and \nheap storage; ALGOL 68 and PL/I use all three of these schemes with explicit allocation and deallocation; \nand LISP, SNOBOL, and APL provide implicit heap allocation. A similar range of fixed binding strategies \nis provided for data types, name binding, program text binding, and parameter access methods in these \nand other languages. In only a few cases does any current language allow more than one or two approaches \nfor any of them, and in each case where a choice is allowed it is always an explicit choice made by the \nprogrammer, rather than by the processor. The choice of binding time discipline has major consequences \nfor both the runtime efficiency of programs and the convenience of the language in expressing algorithms. \nThus FORTRAN with its extremely early binding times sacrifices user convenience to runtime efficiency, \nwhile SNOBOL, at the other end of the binding spectrum, is known for its ease of programming complex \ntasks, but certainly not for its execution speed. The usual approach to implementing a language is to \ndevise a method of executing statements which is sufficiently general to allow for bindings This is \na preliminary report of work in progress. occurring as late as they are allowed to by the semantics \nof the language. Thus the runtime support package for a PL/I program is likely to contain code for heap \nstorage management and a SNOBOL processor is likely to maintain a runtime symbol table for each program \nit executes to allow for symbolic indirect addressing, regardless of whether these features are used \nin a particular program or not. Another unfortunate consequence of the current design philosophy is \nthat if a user anticipates eventually running the program he is designing on a production basis, he is \nimpelled to code from the beginning in a language chosen for its runtime efficiency rather than its \nease of use. The alternate approach of designing and debugging in a more convenient language and later \ntranslating to a more efficient one is highly impractical because it forces the programmer to be fully \ncompetent in a variety of languages with extremely diverse syntax and semantics and to be able to translate \namong them. This probably accounts in part for the continued use of FORTRAN and COBOL in spite of the \nnow longterm existence of much more powerful and convenient languages. Our approach to solving these \nproblems is as follows: i. We propose that a single powerful and convenient language be designed which \nallows for extremely late binding times. This language should have simple, precisely defined syntax and \nsemantics which are free of ad hoc restrictions imposed in the name of implementation efficiency (such \nas compulsory declarations, data types, fixed dimensions or even numbers of dimensions, explicit storage \nmanagement, etc.). It should provide for the easiest possible understanding of the semantics of programs \nwritten in it (in contrast to-such powerful languages as PL/I). Dijkstra, in [ 6 ], suggests a similar \napproach 77 based on defining an abstract machine to model the execution process of a language: \"By \ndisregarding all efficiency requirements and tailoring the machine to the language we can obtain a much \nsimpler organization, so simple in fact, that it can very well be used as a means of language definition.\" \n 2. We also propose that a new type of pro- cessor be designed for this language. The pro- cessor would \nhave as its major new functions (i) the task of determining the attributes of each variable, such as \ndata types, name binding, storage requirements, and so on, and (2) the implementation of each program \nin the most efficient manner (i.e. earliest binding times) consistent with its being correctly executed. \nThe implementations resulting will range from complete interpretation to the production of highly efficient \n compiled code, depending upon the usage of language features in each program. As Dijkstra suggests in \n [6], I assume that the structure of many a machine is such that it is desirable that the translator \nthoroughly analyses this program and tries to detect all kinds of 'special cases' of our general concepts \nfor example whether an array has a regular form (rectangular, triangular, etc.), whether an array is \nperhaps homogeneous, whether a variable is always simple and never defined in the form of a procedure, \nwhether a pro- cedure is used recursively or not, etc. In short, the translator will search for 'un- \nused generality' with the aim of gaining something. These analyses are no child's play and furthermore, \nas the analysis is carried out statically, the translator must always remain on the safe side. In addition \nthe processor may, of course, do more traditional optimizations such as code movement, strength reduction, \nand so on, as described for example in [i] and [5]. 3. \"But we can hardly speak of 'good use of a computer' \nwhen the translator spends a con- siderable amount of time and trouble in trying to come to discoveries \nthat the programmer could have told it as well!\" [ 6 ] Thus the processor will also accept declarations \nwhich specify in advance the attributes of any or all variables (but they will not be required!). This \nwould simplify the task of attribute determination, enhance the capability of error detection for attribute \nincompatibilities, and aid the gener- ation of efficient machine code. The resulting processor will \nbe capable of producing highly efficient object programs when applied to FORTRAN-like source programs, \nand yet will be capable of executing arbitrary programs in a powerful and very high-level language. It \nwill, as a result of preliminary program analysis, decide which additional modules are needed to complete \nprocessing and to execute the resulting object program. The runtime support actually necessary for any \nparticular program might range from only input/output routines at one extreme to the complete processor \nat the other (for the situation in which the program creates and executes new program text at runtime). \n Such a system will provide a new form of pro- gram development by stepwise refinement. The user could \ndevelop and debug his program using the full power of the highly interpretive general processor (which \ncould easily produce excellent diagnostics), and then once his program is running correctly, he could \ninterrogate the processor concerning his usage of features which necessitate late bindings and systematically \nmodify his program to eliminate them. The advantages of such an approach are described in [18], and \nshould be even more evident in the case where the source language is fixed, rather than progressing \nfrom SETL to BALM to PL/I as Tenenbaum does in [18]. It is our belief that the development of such a \nlanguage and processor is one small step along the path to constructing an artificially intelligent programming \nassistant, as proposed by Winograd in [19]. In a recent paper entitled \"Does APL Really Need Run-time \nChecking?\" [ 2 ], Bauer and Saal discuss the overhead incurred by APL interpreters for implementing \nthe variability of data types and shapes by runtime checking. They conclude that 80% of the rank, length, \ndomain and value checks required by a naive interpreter could be eliminated by static analysis before \nexecution. This result gives us confidence that the design advocated here will not totally sacrifice \nproduction efficiency to programmer convenience. In the remaining sections of this paper we shall \nfirst discuss the kinds of information to be obtained from binding time analysis and how they are useful \nin constructing an appropriately efficient implementation of each program processed and then focus \nin on a particular subset of the binding time information, namely the analysis of data types and storage \nmanagement. In an appendix we discuss the TEMPO language which serves as the basis for our current \nwork in this area. TEMPO was developed as a tool for teaching concepts of programming language semantics \n and their effects on implementation efficiency. It is far too limited to fulfill the requirements for \n an ideal language discussed earlier in this section, but it does provide an excellent testing ground \n for this approach because of its extremely late binding times. It will be described more fully in \nour forthcoming paper [9]. II. Binding Time Analysis The main innovation of our approach to language \n and processor design is the consideration of binding time as a parameter which may vary from one program \n to another, and even among the parts of a single program. We now discuss several aspects of the spectra \nof binding times and the kinds of informa- tion we would like to obtain from binding time analysis. \n i. Data types -to determine for each occurrence of a variable what range of types its value may assume \nduring execution. These would include both atomic types (integer, string, etc.) and compound types such \nas structures and arrays. One use of this information is to determine at (pre-execution) analysis time \nwhether runtime type tags and tests for type correctness will be re- quired during execution and, if \nthey will, where the tests can be most efficiently performed. The need for such tests should, we feel, \nbe 78 determined by the processor and optionally be re- ported to the programmer. The need for such \ntests and tags and the techniques for avoiding them are clearly irrelevant to the task of implementing \nany given algorithm and so should not be the responsi- bility of the programmer. Clearly only two binding \n times are relevant to data type checking--pre- execution analysis time and execution time. 2. Storase \nmana$ement -to determine for each variable whether it requires dynamic storage allocation and, if so, \nby what discipline (stack or heap). This dimension of binding variability has significant effects on \nruntime efficiency and is one of the major differences among current languages. Storage binding disciplines \nrange from that of APL, SNOBOL, and LISP, in which every assignment may cause reallocation, through programmer-specified \nexplicit allocation and de- allocation with heap management (PL/I, ALGOL 68, PASCAL); allocation at block \nentry and deallocation at block exit, allowing much simpler stack manage- ment (ALGOL 60, PL/I); to static \nallocation determinable at analysis time (FORTRAN, COBOL, PL/I, PASCAL). Clearly the simplest, most \nnatural, and most powerful method from the programmer's viewpoint is the totally dynamic approach. We \nfeel that it is worth considerable effort to learn how to select the most appropriate storage allocation \nmethod for an arbitrary program, rather than forcing the programmer to explicitly manage storage as is \nnecessary, for example, in PL/I. 3. Name bindin$ -to determine for each use of a variable (or procedure \nor label) name whether it is bound in a fixed way within the enclosing program text, or whether it is \na global variable or formal parameter. In the latter cases it is important to determine to which variables \nor expressions it may be dynamically bound, and how this may vary during execution. In the case of procedures \nit is conceptually natural to allow global references to be bound dynamically to their calling contexts \n(as in APL and SNOBOL), and to bind similarly variables which occur in actual arguments of procedures \n (as in ALGOL 60). The main alternative to this approach is to bind names in some static manner. A consequence \nof this is that calls to library procedures must pass all variables which may be referenced in the form \nof (often cumbersome) argument lists. 4. Parameter access methods -to determine for each formal parameter \nwhether it may be stored into, fetched, or both and the most appropriate method of accessing it (e;g. \ncall by text substitution, reference, value, result, value-result, or name). As for hame binding, the \nmost natural method for the programmer, namely text substitution, is particularly inefficient. Program \nanalysis will attempt to determine for each parameter a more efficient method which yields the same behavior. \n 5. Program text creation -to determine whether the program may read or otherwise dynami- cally create \nprogram text during execution. This facility adds considerable power to some of the interpretive languages \nused in artificial intell- igence research (e.g. LISP, PLANNER, CONNIVER, and POPLER) [ 3] and has been \navailable in some  compiled languages as well, such as MAD [15]. However, it is clearly quite expensive \nsince it requires the ability to use the full language processor during execution. We next discuss each \nof the above topics briefly with respect to issues of efficiency, implementation consequences, and analysis \nstrategies: i. Data types -the need for runtime type tests imposes overhead in both space and time-- \nspace for the \"type tags\" which record the current type of a variable, and time to set the tag when- \never the value of the variable changes and to test it at each use. To date this overhead has been obligatory \nin type-variable languages such as LISP, APL, SNOBOL, and ALGOL 60 (due to the fact that formal parameter \ntypes need not be specified). Analysis will eliminate the need for a great many of these tests by verifying \ncompatibility of assignments and uses. Techniques for this type of analysis based on program flow are \ndescribed in detail in Section III of this paper. 2. Storase manasement -program flow analysis (as will \nbe described for data types in Section III) seems quite appropriate to determining program points before \nwhich storage for a given variable must have been allocated and those points beyond which a variable's \nvalue may no longer be referenced (thus permitting deallocation), as well as providing some information \nabout the amount and organization of the storage to be allocated for the variable. Given the substantial \noverhead of runtime storage allocation activity, it is clearly desirable to allocate variables which \nassume only atomic values (integer, boolean, etc.) statically before execution or on a stack at entry \nto a program unit if re- cursion allows multiple activations of the program unit. The situation is more \ncomplex for variables whose size and/or shape may vary during execution. The first priority for such \nvariables is to determine whether allocation must be done statement- by-statement, or whether their storage \nrequirements remain constant over larger segments of the program. If we assume that, as in TEMPO, array \nbounds need not be declared and that any component of an array may itself be an array, then two distinct \nproblems arise. The first is that the number of \"levels\" of an array-valued variable may increase without \nbound. It appears that flow analysis techniques may make it possible to decide whether this can happen \nand to determine the various shapes the variable may assume by methods similar to those used to test \ninfiniteness of context-free languages, The second problem is that the dimensions or number of components \nat one level of an array may vary (in TEMPO any assignment may add a new compo- nent to a structured \nvariable). This problem is more difficult since flow analysis provides no natural means to determine \n(e.g.) the range of values of I in a statement such as \"A[I] := i;\" Hence a conservative analysis scheme \nwould treat every variable which appears with subscripts as unboundedly extensible, which is clearly \nundesir- able. This difficulty could be alleviated by (i) optional specifications of array bounds (when \narrays are not in fact extensible), (2) by allowing subranges of the integers as subscripts, as is done \nin PASCAL, or (3) in a well-structured language possibly by more sophisticated analysis techniques based \non the suggestions of Klerer and May [12]. Another storage allocation problem relates to procedures. \nSince these are typically entered and exited in a last-in, first-out manner, it appears natural to assume \na stack allocation scheme for their local variables. This is certainly applicable to those variables \nwhich are not assigned on one call and fetched during a later one. It appears that flow analysis may \nagain be a suitable tool by which to determine which values must be retained from one activation to the \nnext and that the \"spaghetti stack\" of Bobrow and Wegbreit [4] may provide a suitable and relatively \nefficient implementation method for this, as well as providing a basis for a variety of less traditional \ncontrol disciplines, such as coroutines and backtracking. 3. Name bindin$ -the only relevant Binding \ntimes for names are pre-execution (processor) time and during execution. Name binding during execution \n(which is necessary to support symbolic indirect addressing as is found in SNOBOL and dynamic contexts \nfor global variables) involves some sort of runtime symbol time and overhead to search the table whenever \nan unbound variable is referenced. This may significantly slow program execution. Name binding may be \ndetermined by (i) forming a graph which indicates calls from each procedure to the others, (2) associating \nwith each node lists of variables which are scoped, listed as formal parameters, or used but neither \nscoped nor listed as parameters, respectively, (3) associating actual arguments with formal parameters \nfor each call, and (4) propagating the information through the graph. The process is simple enough that \nwe shall not illustrate it here. This algorithm assumes that all called procedures are accessible to \nthe processor when the analysis is performed, which seems highly desirable, since otherwise each variable \nwhose scope includes a call to an in- accessible procedure must be regarded as a possible nonlocal reference \nfrom within that procedure. Of course, this might be alleviated by associating with a procedure library \na table of nonlocal references for each procedure and allowing the processor access to these tables. \n 4. Parameter access methods -clearly call by text substitution is to be avoided if possible since it \nrequires runtime statement analysis. Further, call by name, which is similar in effect, can be prohibitively \nexpensive. Analysis would attempt to determine the circum- stances under which call by text substitution \nor name could be replaced by value-result, reference, or other methods without affecting the meaning \nof the program. Aside from complications due to name binding, the main problem in this area is that a \ncalled procedure may change the Values of nonlocal variables and parameters which appear in actual arguments \nwhich are themselves expressions. Flow analysis seems to be an appropriate tool to determine whether \nsuch assignments can occur. However, we have not as yet undertaken a serious investigation of this area. \n S. Prosram text creation -we envision the language proposed in Section I as allowing (as TEMPO does) \nstatements such as \"call X\", where X is a variable whose value is a procedure. The chief question is \nwhether X may have values stored into it which are constructed dynamically during execution. This would \nrequire a complete runtime language processor and would probably render pointless the other kinds of \nbinding-time optimi- zations discussed here. Note that procedure-valued variables do not in themselves \ncause difficulties as long as they are only assigned and not operated on in other ways. Thus a TEMPO \nprogram segment such as the following X := 'Y := input;'; if Z = 0 then X := 'output := Y;'; call X; \n can be handled by compiling all of the procedure values before execution. Determining whether procedure \ntext can be input or constructed during execution appears to be amenable to flow analysis techniques. \nOne implementation technique that might be used to ameliorate the effects of dynamic procedure creation \nis the facility in some LISP systems to run compiled and interpreted routines together. III. Data Type \nAnalysis Data types are one variety of a larger category of information about variables which we call \nattr/butes. The set of attributes for TEMPO can be naturally represented by a context-free language given \nby the following grammar*: type \u00f7 int I str I und i gen I shape shape ~ (atsym , \u00a3$~6t) I (? atsym> \natsym \u00f7 [type][def][ref][sz(num)] I atsym V \u00a3\u00a36t where num represents a nonnegatlve integer or the symbol \n~. An attribute symbol may indicate, for a particular variable at a particular point in a program, what \nits data type is (the simple types are integer, string, and undefined, respectively; gen (i.e. \"general\") \nis the union of all types; and a shape type represents a structure with a known or unknown (respectively) \nnumber of compo- nents); whether it has a value (def); whether it will be used later in the program (ref); \nand how much storage it occupies (sz). The notation atsym I V ... V atsym n denotes the situation in \nwhich a variable may have any of the attributes atsyml, ..., atsym n. Such information propagates both \nforward and backward in a program; e.g. if the statement \"A := B + 4;\" appears in a program, it tells \nus that A must be numeric after this statement and B must be numeric before it. To determine the attributes \nof each variable at each point of the program, we exploit the for- ward and backward propagation and \nthe type re- strictions of the operators to construct an algorithm which begins with all attributes at \nall points assumed general and then iteratively inserts and propagates information until the process \ncon- verges. Tenenbaum ~8] has used a related approach for type determination in the SETL language \n(where extensive analysis is required merely to determine the meaning of an operator, due to the use \nof the same operator symbols for many different purposes). See the Appendix for an explanation of the \nuse of brackets and the \u00a3J~6t operator. Our approach appears to be both more general and less complicated, \nthough without further refinement it results in larger systems of equations than Tenenbaum's. Other related \napproaches are those of Kennedy [i0], Kildall [ii], and Graham and Wegman [ 7 ], though none of these \nspecifically consider applications to data type determination. The last authors present a general model \nof \"information propagation problems\" which could provide an appropriate basis for formalizing our methods. \n A. Simple Types We now present a complete type analysis pro- cedure for simple (i.e. unstructured) \ntypes for TEMPO programs with only assignment, conditional, and branch statements and the block structure \nbrackets begin, end used only for grouping, not to introduce new variables except at the whole pro- gram \nlevel (and lacking structured variables). This analysis will be useful in minimizing runtime storage \nallocation activity and the use of type tags as follows. The result of the analysis will be a pair of \nfunctions F,B: P x I~ \u00f7 atsym, where 19 denotes the set of identifiers occurring in the program, P is \nthe set of \"program points\" (or Junctures between sequentially executable state- ments), and atsym is \nthe set of attribute symbols defined above. To explain their use, we must first define for a statement \nS and variable X the sets UCs(X ) (use constraints) and SCs(X) (~tore _con- straints): UCs(X) = {t 6 \natsym I there is an assignment of values to the variables in S such that S may be executed correctly \n with X having type t} SCs(X) = {t E atsym I execution of S may store a value of type t into X}. For \nexample, if S is the statement \"A := B + length A;\" then UCs(A) = {str}, UCs(B) = {int}, SCs(A) = {]n~}, \nand SCs(B) = ~, Now let I be a program point just prior to a statement S which uses the variable X. \nWe use F( ) to monitor program execution as follows. If F(I,X) -UCs(X) # ~, then i. at point I, a \ntest of a runtime type tag associated with X for membership in UCs(X) is necessary; if not present, signal \na run- time type error and take appropriate action at the programmer's option (the action could be skipping \nthe statement, aborting execution, or attempting error correction). ii. store a runtime type tag for \nX whenever X is assigned by a statement from which control may pass to I without reassigning X. If F(I,X) \nis sufficiently large t0 contain all types which the value of X may possibly have at program point I, \nthen this method will signal all runtime type errors and avoid attempts to execute statements with type \nerrors. Clearly letting F(I,X) = gen would provide complete type-error testing~ but at the expense of \nmuch unnecessary type tag manipulation. Our technique will con- struct the smallest possible sets F(I,X) \nwhich allow complete type-error testing consistent with the conservative assumption that all possible \nexecution paths through the program may occur in actual computations. Note that such a minimal set must \nexist (assuming F(I,X) is finite), since if functions F( ) and F'( ) both provide complete type testing \nas described above, then F't( ) given by F\"(I,X) = F(I,X) N F'(I,X) will also. This scheme can be improved \nin two ways: first, a more explicit method needs to be provided for deciding which assignments must be \naccompanied by corresponding type tag assignments; second, if it can be determined at each assignment \nof X that certain types are incompatible with all future uses, then the type compatibility test can be \nperformed at the point of assignment, obviating the need for later testing and possibly making the type \ntag itself unnecessary. Both of these improvements may be realized by using B: F x [Q \u00f7 atsym to further \nmonitor execution as follows. Assume program point J immediately follows statement S, which assigns a \nvalue to variable X. If SCs(X) -B(J,X) # ~, then after performing S, test the type actually stored into \nX against B(J,X); if not present, signal a runtime type error; if present, set X's type tag accordingly. \n We clearly wish B(J,X) to contain all types which X may be assumed to have in subsequent state- ments \nwhich use the current value of X. It is also clearly less important than F(I,X), since use errors will \nbe detected at the point of use if not sooner; however, it does allow earlier error de- tection and the \nuse of fewer type tags. The properties of B( ) are eontravariant to those of F( ): a larger B( ) will \nresult in fewer runtime tests; and if both B( ) and B'( ) detect type violations which would result in \nerrors in later uses, then B( ) U B'( ) has the same property. Hence there is a maximal value for B(). \n Note finally that the F( ) and B( ) functions need not be stored in any explicit form during ex- ecution. \n B. Formulation of the Equations First we convert the program into flowchart form for conceptual convenience \nand insert a program point between any two statements such that control may pass directly from one to \nthe other, at the entry to the flowchart, and at each exit. Let the resulting graph contain the subgraph \nin Figure i, where I,J are program points and S is ? S Figure .1 a statement. Information about types \nwill in general propagate forward from F(I,X) to F(J,X) and backward from B(J,X) to B(I,X). In particular, \nif S does not change X, then we have F(J,X) = F(I,X); and if S assigns a new value to X, then F(J,X) \nwill be modified as well. These effects of program flow are naturally expressed as a system of set- theoretic \nequations with each F(I,X) and B(I,X) as variables. In the following (with I,J, S, and X as above) we \nwrite FINFO(I,J,S,X) to denote the new forward information about X derived from F(I,X), B(J,X), and \nSC(S,X). Similarly BINFO(J,S,X) denotes the new backward information derived from B(J,X) and UC(S,X). \n Now suppose a statement S is immediately pre- ceded by program points Ii, ..., I m and succeeded by \nJl, \"''' Jn as shown in Figure 2. Then the %3 S \u00a3...% Figure 2  equations are as follows: F( ) \nequations for X i. F(I,X) = und if I is the initial pro- gram point, m  2. F(Jj,X) = U FINFO(Ii,J \nj,S,X) for each pro- i=l gram point Jj as shown mn Figure 2; B( ) equations for X n  i. B(Ii,X) = \nU BINFO(Jj,S,X) for each program j=l point I i as shown in Figure 2,  2. B(Jj ,X) = gen if Jj is a \nter- minal program point; where I. FINFO(I,J,S,X) = F(I,X) if X is not assigned in S; 2. FINFO(I,J,S,X) \n= B(J,X) f] SC(S,X) if X is assigned in S (except for case 3, below);  3. FINFO(I,J,S,X) = B(J,X) q \nF(I,Y) if S is  \"X := Y;\"; and i. BINFO(J,S,X) = B(J,X) if X does not appear in S; 2. BINFO(J,S,X) \n= gen if X is assigned but not used in S;  3. BINFO(J,S,X) = UC(S,X) if X is both assigned and used \nin S;  4. BINFO(J,S,X) = B(J,X) N B(J,Y) if S is \"Y := X;\";  5. BINFO(J,S,X) = B(J,X) N UC(S,X) if \nX is used but not assigned in S, except that S is not as in 4.  The equation n  B(li,X) = U BINFO(Jj,S,X) \nj=l can in most cases be replaced by the more informa- tive n  B(li,X),= q BINFO(Jj,S,X), j=l which \nwill frequently result in a smaller F( ) solution and hence fewer runtime tests. The reason is that \nnormally any constraints which X must satisfy along paths leading backward from Jl, -.., Jn must all \nbe satisfied at If, ..., I m. However, it is possible that S is, directly or indirectly, a branch on \ntype for X and the Jl path will be taken only if X is an integer, the J2 path only if X is a string, \netc. In this (exceptional) case the only recourse is to use the union. The example given below illustrates \nthis problem. Finally we show how to determine UC(S,X) and SC(S,X) from the form of S and X: Store \nconstraints I. SC(S,X) = t if S is \"X := constant;\" and t is the type of \"constant\" 2. SC(S,X) = gen \nif S is \"X := input;\"  3. SC(S,X) = range type of \"op\" if S is \"X := exPl op exP2;\"  4. SC(S,X) is \nnot used for S of the form \"X := Y;\" so we leave it undefined.  Use constraints i. UC(S,X) = UC(exp,X) \nif S is \"Y := exp;\", \"output := exp;\" or \"if exp\" 2. UC(exp,X) = gen if x does not appear in exp or \nexp is \"X\"  3. UC(exPl op exP2,X) = UC(exPl,X) N UC(exP2,X) if exPl # \"X\" and exP2 # \"X\"  4. UC(X op \nexp,X) = UC(exp,X) N leftoperand- types(op)  5. UC(exp op X,X) = UC(exp,X) q rightoperand- types(op) \n 6. UC(X p Y,X) = gen for p any of =, #, <, ~, >, and >  7. UC(X p exp,X) = type of exp if exp is not \na variable, for p as in item 6.  C. Solution of the System of Equations The system of equations for \nF( ) just defined will be of the form X 1 = fi(Xi,...,X_) P X2 f2 (Xi ' ,Xp) Xp = fpiXl ..... Xp) \n or equivalently~= f(~), where each X~ denotes one of the F(I,X) and ~is the vector ~Xi,...,Xp). For \nfixed values of B(J,Y) it is clear that X 1 = Xi' , ..., X = X ' implies fi(Xl,...,X.) fi(Xl' ..... Xp') \n~or i p= 1,2 ..... p, i.e. that f( ) is componentwise monotonic on the partial ordering of set inclusion. \nIt follows that a unique minimal solution to the set of equations exists and can be obtained by computing \n lim fi(~ .....\u00a2) = U fi(\u00a2 .....~) . i~ i=O  This is the desired solution for F(), since as noted before, \nit should be as small as possible in order to minimize runtime type checking. For B( ) the contravaiance \nmentioned above requires that it be as large as possible. The system~f equations for B( ) has the same \nabstract form: Y = ~(Y) and again Yi g ~i' for i = 1,...,p implies bi(Y) g bi(Y'). Thus bi(gen, .... \ngen) m bi+l(gen .... ,gen) for i = 0,i.... and thus Y = iQO b (gen ..... gen) will be a maximal solution \ntoy = b(~). This solution may then be substituted into the original ?  F( ) equations to begin their \nsolution process. D. An Example of Data Type Analysis (Simple Types) As an example of our methods we \nshall perform the data type analysis on the TEMPO program begin scope SUM,ST,X,Y; SUM := 0; ST := \n'';  M: X := input; Y := input; if X > 0 then begin ST := ST catenate Y; goto 'L'; end; SUM := \nSUM+ Y;  L: if X ~ 0 then goto 'M'; output := SUM; output := ST; end We first translate the program \ninto the flowchart shown in Figure 3 and then construct the systems of equations which we write in Figure \n4 in an abbreviated form. We abbreviate F(I,X) = B(I,X) N int, for example, by writing B I ~ int at the \nintersection of the X column and the F I row, and B(I,X) = B(J,X) N B(J,Y) by Bj N Bj(Y) at the intersection \nof the X column and the B I row. Note that this program contains an implicit branch on type, as discussed \nabove--the if statement following point 4 differentiates (if the program and its input are compatible) \nthe type of the Y input according to the value of X. Solving the equations in Figure 4 by the methods \ndescribed in Section Ill.C, we obtain the type information shown in Figure 5. Comparing this information \nwith the values of UC( ) and SC(), we conclude that: i. No type tag is required for SUM, ST, or X; a \nsingle tag with possible values int and str is needed for Y.~ 2. No type checking is needed for SUM \nor ST.  3. At point 3, a check should be made to ensure that the value input for X is of type int. \n4. At point 4, a check should be made to ensure that the value input for Y is of type int or str and \nthe type tag should be set accordingly.  5. At points 5 and 7, the type tag for Y should be checked \nto ensure that it is of type sir or int, respectively. 6. No other type checking or tags are needed. \n  Inspection of the program will show that this is SUM := 0 ST := vT + IX := input Y := input if \nX > 0 NO ES SUM :=~SUM + Y ST := ST i~na~ Y ~ > YES if X ~ 0 output := SUM output := ST Figure3 \n indeed the minimal type checking sufficient to ensure the correct execution of the program. Note that \nthe efficiency of the solution process can be improved somewhat by first compress- ing chains such as \nF(8,X) = F(7,X) = F(4,X) = F(3,X). We leave a general attack on the efficiency question to a later paper. \n SUM ST B 0 gen B 1 B I B 2 gen B 2 B 3 U B 9 B 3 U B 9 B 3 B 4 B 4 B 4 B 5 U B 7 B 5 U B 7 B 5 B 6 str \nB 6 B 9 U BiO B 9 U Bi0 B 7 int B 8 B 8 B 9 U Bi0 B 9 U Bi0 B 9 B 3 B 3 BiO Bii N gen Bii Bii Bi2 Bi2 \nq gen Bi2 gen gen B( ) Equations B I B 2 gen B 4 (B 5 N int) U (B 7 q B 6 (B 9 N int) U (Bi0 q B \n8 (B 9 N int) U (Bi0 q B 3 Bii BI2 gen F( ) Equations SUM ST F 0 und und F 1 B I N int F o F 2 F I \nB I q str F 3 F 2 U F 9 F 2 U F 9 F 4 F 3 F 3 F 5 F 4 F 4 F 6 F 5 B 6 N str F 7 F 4 F 4 F 8 B 8 q int \nF 7 F 9 F 6 U F 8 F 6 U F 8 FiO F 6 U F 8 F 6 U F 8 Fii FiO FI0 Fi2 Fll Fii X und F 0 F I B 3 q gen \nF 3 F 4 F 5 F 4 F 7 F 6 U F 8 F 6 U F 8 FiO Fii Figure 4  B I B 2 B 3 U B 9 gen i nt) B 5 U B 7 B \n6 q str i nt) B 9 U BI0 B 8 N int i nt) B 9 U Bi0 gen Bii Bi2 gen Y und F 0 F I F 2 U F 9 B 4 q gen \nF 4 F 5 F 4 F 7 F 6 U F 8 F 6 U F 8 FIO Fii  B( ) Function F( ) Function SUM ST x Y SUM ST gen gen gen \ngen ~nt und d und B 0 F 0 gen gen gen gen int und d und B I F 1 gen gen gen gen int str d und B 2 F \n2 gen gen int gen int str t und v int v str B 3 F 3 gen gen int int v str int str nt int v str B 4 \nF 4 gen str int str int str t int v str B 5 F 5 gen gen int gen int str nt int v str B 6 F 6 int gen \nint int int str nt int v str B7 F 7 gen gen int gen int str nt int v str B 8 F 8 gen gen int gen int \nstr nt int v str B 9 F 9 gen gen gen gen int str nt int v str BiO FiO gen gen gen gen int str nt int \nv str Bii Fii gen gen gen gen int str nt int v str Bi2 Fi2 Figure 5 2. B(Jj,X) = ~ if Jj is a terminal \n IV. Extensions and Other Applications program point; In this section we shall discuss several ex-where \ntensions to the techniques introduced in the pre- i. FINFO(I,J,S,X) = F(I,X) if X is not ceding section \nand point out some further appli- assigned in S; cations to storage management. 2. FINFO(I,J,S,X) = def \nif x is assigned in S; and A. Definitions and References i. BINFO(J,S,X) = B(J,X) if X is neither \n The attribute symbol def, as applied to a used nor assigned in S; variable X at a program point I, \nindicates that X 2. BINFO(J,S,X) = ref if X is used in S; has received a value along some path leading \nto 3. BINFO(J,S,X) = ~ if X is assigned but I and ref indicates that X will be used at some not used \nin S. later point in the program. Thus points at which X has both these attributes are exactly those \nThe solution process for these systems of where it must have an associated storage area equations is \nidentical to the earlier ones and the to hold its value. convergence analysis there applies as well. \nAs an The set-theoretic equation technique of example, we analyze the TEMPO program Section III is quite \nappropriate for determining these attributes. The equations are as follows: begin scope A,B,C; A := \ninput ;  F( ) equations for X L: B := input; i. F(I,X) = $ if I is the initial pro- C :=A+B; gram \npoints m if C > 0 then goto 'L'; 2. F(Jj,X) = U FINFO(Ii,Jj,S,X) for each pro- if B > 0 then i=l \ngram point  Jj as in begin Figure 2;  output := C; B( ) equatipns for X goto 'M'; n  i. B(Ii,X) \n= U BINFO(Jj,S,X) for each pro- end j=l gram point J. output := B;  as shown in J Figure 2, M: end \n The flowchart corresponding to this program is shown B( ) Function F( ) Function  in Figure 6 and \nthe corresponding equations and their solutions in Figures 7 and 8, respectively. A C A B C  B 0 F \n0 'ef ~ $ def @ @ B I F I B 2 F 2 'ef reF $ def def def A := input B 3 F 3 ef reF ref def def def \n'ef $ $ def def def B 4 F 4 @ r f ref def def def B 5 F 5 $ reF def def def B 6 F 6 ~, ) B := input \n$ @ ref def def def B 7 F 7 def def def B 8 F 8 $ $ $ def def def B 9 F 9 C :=A+B Figure 8 Now \nby forming the union of F( ) and B( ) to determine the variables which have both attributes YES if C \n> 0  at each point we determine that A requires storage at points i, 2, 3, and 4; B at 2, 3, 5, and \n6; and C at 3, 5, and 7.  The results of this analysis can be used to determine the minimal spans over \nwhich each ifB> 0 variable requires allocation and hence minimize the storage space used by a program. \nAlternatively, it can be used to determine simply the latest point at which each variable can be allocated \n storage and the earliest point at which it can be permanently deallocated. output := B output := C \nFigure 6  B( ) Equations F( ) E( uations A B C A B B 0 B I B I F 0 $ B I B 2 B 2 F I def F 0 F 0 B 2 \nref ref F 2 F I U F 4 def F I U F 4 B 3 B 4 U B 5 B 4 U B 5 ref F 3 F 2 F 2 def B 4 B 2 $ B 2 F 4 F 3 \nF 3 F 3 B 5 B 6 U B 7 ref B 6 U B 7 F 5 F 3 F 3 F 3 B 6 B 8 ref B 8 F 6 F 5 F 5 F 5 B 7 B 9 B 9 ref F \n7 F 5 F 5 F 5 B 8 $ $ @ F 8 F 6 F 6 F 6 B 9 F 9 F 7 F 7 F 7 Figure 7 86 B. Procedure Text Creation \n As noted in Section II, creation of procedure text during execution can preclude most types of optimization \ndiscussed here. Thus it is extremely important to determine whether this kind of activity can occur in \na given program. To perform this type of analysis we must first identify each call statement in the \nprogram and examine the string expression appearing in it. If the string expression is a constant it \ncauses no difficulty (i.e. it is susceptible of compilation). If it is a variable, further analysis is \nrequired. If it is an expression, then procedure text must be created during execution. If it is determined \nthat all procedures are given by string constants, they may all be compiled. If at least one procedure \nis given by a variable but none are given by expressions, then we must perform the data type analysis \nof Section III and can then perform a further analysis to determine whether program text is in fact constructed \ndynamically or merely assigned to variables. Let X be a variable whose attributes include $tr at program \npoint I. Then we define attributes predef and constr to mean that (i) at least one path leading to I \nmay assign X a string value (without performing operations on it) and, respectively, (2) at least one \npath leading to I may assign a computed or inputted string value to X (directly or indirectly). These \nattributes will only propagate forward; the equations are given by i. F(I,X) =und if I is the initial \npro- gram point, m 2. F(Jj,X) = @ FINFO(Ii,Jj,S,X) for each pro- i=l gram point Jj in Figure 2;  \nwhere i. FINFO(I,J,S,X) = F(I,X) if X is not assigned in S; 2. FINFO(I,J,S,X) = predef if s is of the \nform \"X := stringconstant;\";  3. FINFO(I,J,S,X) = c0nstr if X is assigned input or a string expression \n(other than a constant or a variable) in S;  4. FINFO(I,J,S,X) = F(I,Y) if S is of the form \"X := Y;\"; \n 5. FINFO(I,J,S,X) = und if x is assigned an expression or constant not of type string in S.   The \noperator @ is defined by the table und predef constr und und predef constr predef predef predef constr \nconstr constr constr constr and by the property of associativity, a @ (b @ c) = (a @ b) @ c. For example, \nwe analyze program text creation for the TEMPO program begin scope A,B,C,D; A := input; B := 'output \n:= A;'; if A = 0 then B := 'output := C;'; C := B; call C; D := 'begin scope C;' catenate B catenate \n'end';  call D; end The flowchart is given in Figure 9. Note that data type analysis shows that all \nvariables except A may be of type stY. The resulting forward equations for program text creation analysis \nare given by Figure 10(a) and their solution is in Figure 10(b). ? A := input B := 'output := A;' \n if A ~ 0 B := 'output := C;' C:=B( call C D := 'begin scope C;' catenate B catenate 'end' call \nD &#38; Figure 9  87 F( ) Equations F( ) Function C D B C F 0 und und und F 0 und und und F I F o \nF o F o F I und und und F 2 predef F 1 F 1 F 2 predef und und F 3 F 2 F 2 F 2 F 3 predef und und F 4 \nF 2 F 2 F 2 F 4 predef und und F 5 predef F 4 F 4 F 5 predef und und F 6 F 3 @ F 5 F3(B) F5(B) F 3 ~ \nF 5 F 6 predef predef und F 7 F 6 F 6 F 6 F 7 predef predef und F 8 F 7 F 7 constr F 8 predef predef \nconstr F 9 F 8 F 8 F 8 F 9 predef predef constr (a) (b) Figure i0 the flowchart of Figure ii (with the \ncontaining Thus we determine that the procedure texts which loop provided). On performing the data \ntype may be assigned to B and C can be compiled, while analysis, we find that B is of type und V int \nat that assigned to D must be constructed and pro- point i and hence is a candidate for retention. \n cessed during execution. The points where a variable receives the predef attribute in the form- ulation \nof the equations supply all the string constants which are candidates for compilation. Note that assignment \nto a global variable in ?  a called procedure or the appearance of a possibly YES procedure-valued \nvariable as an actual argument ~ifA= i to a procedure complicates this analysis consider- ?  ably since \nsuch an assignment may cause an other- wise predefinable variable value in the calling B := i procedure \nto become constructible. For example, if the statement \"B := 'output := C;';\" were \"B := 'B := input;';\" \ninstead, then the statement B := B*2~ call C would cause B to receive a value unknown before executions \nso that any later use of B must be non-predefinable. C. Retention of Local Variable Values X := B \nThe local variables which need to be re- tained from one invocation of a procedure to the next are exactly \nthose variables which may be used along some execution path before they receive a value (in the current \ninvocation), since Figure 11 any other variable will get a new value in the current invocation before \nit is first used. The need for retention can be determined D. Lengths of Strings  by placing the body \nof a procedure in a loop and then performing the data type propagation for To optimize storage allocation \nfor string- it. Any variable which.has und among its valued variables we must first determine the range \nresulting F( ) types at a use of it is a candidate of lengths which each may have. Then any string for \nretention. For example, for the TEMPO variable whose length has a constant bound may be procedure allocated \nstorage before execution and those whose lengths are possibiy unbounded can be selec- POWTWO := 'parameters \nA,X; ted for reallocation at appropriate points during execution. begin scope B;  Flow analysis is \nagain an appropriate technique if A = i then B := i; for determining bounds on string lengths. Only forward \npropagation is necessary and the form of B := B*2;  the equations is as follows: X := B; end';  which \nis called by call POWTWO(A,X), we obtain 88  F( ) equations for X i. F(I,X) = 0 if I is the initial \npro- gram point, 2. F(Jj ,X) = max FINFO(Ii,Jj,S,X ) i=l,...,m for each pro- gram J point 3 as shown \nin Figure 2;  where i. FINFO(I,J,S,X) = F(I,X) if X is not assigned in S; 2. FINFO(I,J,S,X) = 0 if \nS assigns to X a value of a type other than str; 3. FINFO(I,J,S,X) = LB(S) if X is assigned a string \nvalue by S (except for case 4, b~low);  4. FINFO(I,J,S,X) = F(I,Y) if S is \"X := Y;\".  The function \nLB(S) (the length ~ound imposed by S), for S a statement of the form \"X := strexp;\", is defined as follows: \n i. LB(X := strexp) = LB(strexp); 2. LB(string) = length of string, for any string constant; 3. nB(input) \n= ~; 4. LB(strexPl catenate strexP2 ) = LB(strexPl) + LB(strexP2) ; 5. LB(strexp substr arithexPl , \narithexP2 ) = arithexp 2 if arithexP2 is a constant; i LB(strexp)-arithexPl+l if arithexPl is a constant; \nLB (strexp) otherwise For example, LB('DOG' catenate 'LEOPARD' substr 3,A+B) = LB('DOG') + LB('LEOPARD' \nsubstr 3,A+B) = 3 + LB('LEOPARD') -3 + 1 = 3 + 7 -3 + 1 = 8. To illustrate the analysis process, we \ninspect the following TEMPO program for a Post tag pro- blem derived from Minsky [16]: begin scope X,TAG; \n X := 'i0010';  M: TAG := X substr i,i; X := X substr 4,length X -3; if TAG = '0' then begin X := \nX catenate '00'; goto 'L'; end X := X catenate 'ii0';  L: if length X ~ 3 then goto 'M'; end The \nflowchart is shown in Figure 12 and the F( ) equations and their solution in Figure 13. Note that if \nwe substitute \"X := input;\" for \"X := 'i0010';\" then we get F(I,X) = ~ for I = 1,2,...,9, which suggests \nthat we must do all6cation for X during execution. Of course we 89 X := i0010' P ~TAG := X substr \ni,i X := X substr 4,length x -3 ! TAG=  X := X catenate '00' X := X catenate 'ii0' --if length X \n>~ YES ~O -- Figure 12  F( ) Equations F( ) Function TAG TAG X F 0 0 0 F 0 0 0 F 1 0 5 F 1 0 5 F 2 \n1 max(Fi,F 9) F 2 1 5 F 3 F 2 F 2 -3 F 3 1 2 F 4 F 3 F 3 F 4 1 2 F 5 F 3 F 3 F 5 1 2 F 6 F 4 F 4 + 2 \nF 6 1 4 F 7 F 5 F 5 + 3 F 7 1 5 F 8 max (F 6 , F 7 ) max(F6,F 7 ) F 8 1 5 F 9 max (F6,F 7 ) max(F6,F \n7) F 9 1 5 Figure 13  must do so, since the value read into X during execution may be arbitrarily long. \nBut, in cases such as this program we can restrict the allocation activity for X by refining the analysis. \nIn par- ticular, if we set F(i,X) = length X in the F( ) equations, where length X denotes the unknown \nlength of the X read in, the solution shows that F(I,X) < length X for I = 1,2,...,9. Thus, in this \nmore re- fined analysis, we can determine that allocation for X need be performed only once--when its \nvalue is read in. We leave to a later paper a discussion of the complications introduced by allowing \nmultiple symbolic constants of the form length X in the equations. String length analysis introduces \na new phenomenon--since the set of attributes is no longer finite, the solution to the equations may \nbe infinite. For example if the constant 'ii0' is replaced by '1101' in the example above, then F(2,X) \nmust satisfy F(2,X) = max(F(2,X),5)+l, so that F(2,X) = co is the only possible solution. E. Structures \nand Arrays  As mentioned in Section II~ structures and arrays of either the highly general variety found \nin TEMPO or of more limited varieties pose signifi- cant complications for the techniques discussed here. \nRather than attempt a full explication of the difficulties here we shall only point them out and outline \nthe directions of our future work in this area. The first problem is that in analyzing an assignment \nof the form \"A[I] := exp;\" there is no way to determine (in general) the range of values of I by data \nflow analysis techniques, or for that matter any method short of executing the program. Thus we must \nassume that this assignment may modify any first-level component of A and this implies that we must union \nthe possible types of components of A--even though the program might never assign values of different \ntypes to the same component. Thus any array whose components are possibly in- homogeneous requires runtime \ntype checking at each ac ces s. Second, unless all subscripts of A in every assignment to it are constants \n(or can be deter- mined to have bounded value), we must assume that the number of components of A at \neach level is unboundedly extensible. Third, the number of levels of subscripting in a structure may \nalso be unbounded, and flow analysis alone will fail to determine that it is bounded in cases where it \nis. For example, consider the trivial TEMPO program begin scope I,A; I :=i; A :=I;  L: A := (I,A>; \n I := I+l; if I < i0 goto 'L';  end Our present techniques can determine that A has two components \nat each level, but not that it never has more than ten levels. It would, in fact, find the type of A \nimmediately following execution of \"A := <I,A>;\" to be <int,int> v <int,<int,int)) v <int,<int,(int, \nint))) v ... Solutions to all of these problems can be provided by optional declarations, but the philo- \nsophical approach outlined in Section I suggests that we first attempt to obtain as much information \nfrom program analysis as possible before exercising this alternative. Some of the improvements which \ncan be provided easily by optional declarations are the following: i. The use of named selectors or \nsubrange types (as in PASCAL [8]) as subscripts provides bounds for the value of a sub- script and hence \nmarks structures which are not extensible as to the number of components at each level. 2. Declaration \nof a maximum (or constant) number of levels for a structure clearly solves the third problem above. \n Note that the use of named selectors or subrange types introduces the need for checking the validity \nof each reference to a component, while no sub- script checking is needed in full TEMPOsince all possible \ncomponents are at least implicitly present at all times. Though this introduces overhead, it is generally \ndesirable since it provides one kind of execution error detection. V. Conclusions and Future Research \nin this Area We have presented a proposal for a new pro- gramming language whose semantics would be \nbased on very late binding times and minimal use of declarations, and whose processor would auto- matically \nselect the most efficient binding times consistent with the semantics of each program. We have also presented \nsome program flow analysis techniques which appear to be appropriate for several aspects of binding time \nselection. Clearly much remains to be done. First, it remains to be determined just how far these methods \ncan be extended without requiring the user to supply a mass of declaratory material. Second, we lack \nevidence to determine whether the approach is in fact feasible--that is, whether the algorithms can be \nmade satisfactorily efficient and whether they will produce an acceptable degree of optimization for \nmost programs. These questions must be prime concerns for future work in this direction. Some particular \nsubjects for future work include the following: i. How much useful information about data types and \nstorage requirements for largely unconstrained structures, such as those found in TEMPO, can be provided \nby flow analysis techniques? 2. How much improvement can be made in the analysis of structures if they \nare con- strained to have declared bounds on their depth of subscripting and/or extensibility?  3. What \ncomplications are introduced into the analysis of program text creation by global variables and actual \narguments whose values may be procedures and may themselves be modified in procedures?  4. What are \nappropriate methods for solving the systems of equations with the symbolic constants length X discussed \nin Section IV.D?  5. How much useful information about the dimensions of structures can be obtained \n  from observations of the following sort: \"if A always appears in a form such as for I := 1 by 1 to \ni0 do A[I] := expression; endfor in a given program, then A has exactly i0 components\"? As noted previously, \nKlerer and May [12] sketch some possibili- ties for analysis of this sort.  6. Is flow analysis or some \nother method appropriate for elucidating constraints on parameter access?  7. What effects do dynamic \ngoto's, as found in TEMPO, have on the analyses?  8. In contrast to the preceding question, what improvements \ncan be obtained in the efficiency or completeness of the analysis methods by restriction to well-structured \nprograms?  9. What are appropriate organizations for the processor and runtime support package?  i0. \nHow can support for user-defined data types (as in PASCAL) or for data abstract- ions (as in CLU [14] \nand ALPHARD [20]) be integrated into a language of the sort discussed here and what effect do they have \non the analysis methods and imple- mentation efficiency? Some of the concerns about implicit abstractions \ndiscussed by Wulf in [20] are particularly cogent as motivations for combining our approach with such \nhigh-level abstractions. ii. What are appropriate semantic models to express formally the optimizations \ndis- cussed here? Methods based on VDL, Pratt's H-graphs [17], and a model we have under development \nourselves all seem likely candidates. References [I] Allen, F. E., Control Flow Analysis, Proc. of \nSymp. on Compiler Optimization, SIGPLAN Notices, Vol. 5, No. 7, July 1970, pp. 1-19. [2] Bauer, Alan \nM. &#38; Harry J. Saal, Does APL Really Need Run-time Checking?, Software-- Practice and Exp~rience~ \nVol. 4, !1974, pp. 129-138. [3] Bobrow, Daniel G. &#38; B. Raphael, New Pro- gramming Languages for \nArtificial Intelligence Research, Computing Surveys, Vol~ 6, No. 3, September 1974, pp. 155-174. [4] \nBobrow, Daniel G. &#38; Ben Wegbreit, A Model and Stack Implementation of Multiple Environments, Comm. \nof the ACM, Vol. 16, No. i0, October 1973, pp. 591-603. [5] Coeke, John &#38; Jacob T. Schwartz, Programming \nLanguages and Their Compilers, Second Revised Edition, Courant Inst. of Math. Sciences, New York Univ., \n1970. [6] Dijkstra, Edsger W., On the Design of Machine Independent Programming Languages, in Richard \nGoodman (ed.), Annual Review in Automatic Progr~nming, Vol. 3, pp. 27-42, 1963. [7] Graham, Susan L. \n&#38; Mark Wegman, A Fast and Usually Linear Algorithm for Global Flow Analysis, Conf. Record 2nd ACMSymp. \non Princ. of Prog. Lang., January 1975, pp. 22-34. [8] Jensen, K. &#38; Niklaus Wirth, PASCAL User Manual \nand Report, Springer-Verlag, Berlin, 1974. [9] Jones, Neil D. &#38; Steven S. Muchnick, TEMPO: A Unified \nTreatment of Binding Time and Para- meter Passing Concepts, in preparation. [i0] Kennedy, Kenneth W., \nNode Listings Applied to Data Flow Analysis, Conf. Record 2ndACM Symp. on Princ. of Prog. Lang., January \n1975, pp. 10-21.  [ii] Kildall, G. A., A Unified Approach to Global Program Optimization, Proc. ACM \nSymp. on P~nc. of Prog. Lang., October 1973, pp. 194-206. [12] Klerer, Melvin &#38; Jack May, Automatic \nDimension- ing, Comm. of the ACM, Vol. i0, No. 3, March 1967, pp. 165-166.  [13] Leavenworth, B. M., \nSyntax Macros and Extended Translation, Comm. of the ACM, Vol. 9, No. ii, November 1966, pp. 790-793. \n [14] Liskov, Barbara &#38; Stephen Zilles, Programming with Abstract Data Types, SIGPLAN Notices, Vol. \n9, No. 4, April 1974, pp. 50-59. [15] The MAD Manual, Computing Center, The Univer- sity of Michigan, \nAnn Arbor, 1967. [16] Minsky, Marvin, Co~utation: Finite and Infinite Machines, Prentice-Hall, 1967, \npp. 267-268. [17] Pratt, T. W., A Theory of Programming Languages: Part I, UTEX-CCSN-41, Dept. of Computer \nSciences, Univ. of Texas at Austin, July 1975. [18] Tenenbaum, Aaron M., Type Determination in Very \nHigh Level Languages, Report No. NSO-3, Courant Inst. of Math. Sciences, New York Univ., 1974. [19] \nWinograd, Terry, Breaking the Complexity Barrier Again, Proe. ACM SIGPLAN-SIGIR Inter- face Meeting, \nSIGPLAN Notices, Vol. 10, No. i, Jan. 1975, pp. 13-22. [20] Wulf, William A., ALPHARD: Towards a Language \nto Support Structured Programs, Dept. of Computer Science, Carnegie-Mellon Univ., Pittsburgh, Pa., April \n1974. Appendix. The TEMPO Programming Language TEMPO is ~ pedagogic language designed as a tool for \nteaching some of the semantic and pragmatic aspects of programming languages. Its syntax, sem- antics, \nand methods of implementation are described completely and precisely in our forthcoming exposi- tory \npaper [9]. One major design geal of the langL~age is to provide a context in which the effects of design \nfeatures on implementation efficiency can be clearly observed. Since we have concentrated on semantic \nissues, the syntax is deliberately as simple as possible, omitting such (desirable) structural features \nas if...then...else, while, for, and case statements. However, such structures may easily be added by \npurely syntactic methods, such as syntax macros [13], as we show in the version of the language named \nTEMPO/SP [9]. The syntax of TEMPO is presented in extended Backus-Naur Form in Figure Ai. We use {,} \nfor grouping; [,] for optional items; + and * with their usual meanings for regular expressions; and \na new form A B f~6\u00a3 to stand for A {B A}*. The +, , and \u00a3~66t operators all take precedence over concatenation \nwhich takes precedence over alter- nation. Underscoring is used to indicate brackets as terminal symbols--i.e. \n[,] are extended BNF symbols, while !,~ indicate that ordinary brackets appear in the terminal string. \n The semantics of TEMPO is defined by specifying an abstract machine which executes programs written \nin the language. The full definition of the abstract machine is too extensive to include here, so we \ncontent ourselves with some remarks and examples of the semantics: i. The value of a variable may be \nan integer, a character string, the undefined value (denoted ), or a structures which is a finite ordered \ntree with objects of the other types at the leaves. The components of a structure are accessed via a \nsub- scriptlnotation , so that e.g. X[I][J] indicates the J tn subtree of the I th subtree of the current \nvalue of X. A linear notation is used to write structures, so that X = <i,<2,3>,<('AB',4>,'CAT'>> corresponds \nto the tree X 1 AT' 'AB' 4  and X[i] = i, X[2] = <2,3>, X[3][2] = 'CAT', and X[4] = *. Variables are \nallowed to change their sizes, shapes and types during execution of a pro- gram. 2. Block structures \nis provided to limit the scope of name references (by the scope declaration), but has no effect on allocation \nof storage or data types. The scope of a global reference depends on the dynamic calling context as in \nAPL, rather than the static block structure as in ALGOL 60. 3. An expression may be an arithmetic expression, \na string expression or a structure whose components are expressions.  4. An assignment statement with \na variable on the left and an expression on the right is performed by  i) evaluating the expression \nby the precedence rules inherent in the grammar and then ii) binding the variable name (or component \nif subscripted) appearing on the left to the value computed in step (i). An assignment statement with \ninput on the right causes the next value in the input stream to be read and one with output on the left \ncauses the value of its right side to be printed. Note that the size, shape and type of the value assigned \nto a variable may change arbitrarily from one assignment to the next. 5. The if statement is executed \nas usual except that no else clause is allowed. It is re- quired that the expressions compared in an \nif statement be of the same type if they are not structured or that both be structures. Notice that only \nequality and inequality comparisons are meaningful for structures.  6. No distinction is made among \ncharacter strings which are data, labels, or program text. Thus \"goto strexp;\" is executed by first evaluating \nthe string expression to a string constant. If this string constant labels some statement in the current \nblock, control is transferred to that statement. Otherwise, we trace back through the dynamic chain of \ncontaining blocks until the label is found. If it is not found, execution of the program aborts.  Similarly \n\"call strexp(argl .... ,argn)\" is ex- ecuted by i) evaluating the string expression to an executable \nprocedure body (if the result is not executable this will be discovered only during the attempt to execute \nthe procedure), ii) textually substituting each actual argument for each occurrence of the corresponding \nformal parameter in the procedure body resulting from step (i), and then iii) transferring control to \nthe first ~ statement of the resulting text. Note that this makes it possible to create or input new \nprogram text during the execution of a program and then execute it. Execution of a return statement \nor the end of a block causes control to exit from the con- taining procedure or block and the variables \nscoped within it to become inaccessible. Note that the above describes the semantics of TEMPO and not \nan implementation, so that a processor is not bound in any way to use text sub- stitution when a more \nefficient parameter passing method is equivalent to it, or to carry runtime type tags for variables whose \ntypes are known to be static, and so on.  92 program \u00f7 block block -~ begin [scope] statement + end \nscope \u00f7 scope ident , f~66\u00a3 ;  statement * [label] {assign I goto I if I call I block I re~n;} assign-~ \n{var I output} := {exp I input} ; goto \u00f7 goto strexp ; if -~ if logexp then statement logexp + exp { \n< I ! I = I # I > I >_ } exp call -~ call strexp [args] ; args \u00f7 ( exp , list ) exp \u00f7 atomexp + < exp \n, list > atomexp \u00f7 arithexp I strexp arithexp \u00f7 term { + I - } f~66\u00a3  term + factor { * I / } f~6t factor \n\u00f7 var I number I ( arithexp ) I length strexp strexp \u00f7 strterm catenate ~\u00a3 strterm \u00f7 strfactor [subst~ng \narithexp , arithexp] strfactor \u00f7 var I string I (strexp) string \u00f7 ' { '' I nonquote }* ' nonquote \u00f7 letter \nI digit I keyw\u00b0rd I I , I : I ; I + I -I * I / I  < liI> I> I= I~ I (I) lil!l <I > keyword \u00f7 begin \nI end I scope I return I input I output I goto I if l then I call I length I catenate I substring I parameters \n label \u00f7 ident : * var \u00f7 ident subscript subscript \u00f7 ! arithexp ] ident -~ letter {letter I digit}* \nletter \u00f7 A I B I ... I Z  number -~ digit + digit \u00f7 0 I i I 2 I 3 I 4 I 5 I 6 I 7 I 8 I 9 procedure \n-~ [formalpars] statement formalpars \u00f7 parameters ident , ~\u00a3 ; data \u00f7 constant , ~\u00a3 constant \u00f7 number \nI string I ( data > Figure Ai. Syntax of TEMPO A number of extensions to TEMPO have been defined with \nthe purpose of making implementations more efficient for all programs written in the language at the \nexpense of decreasing the power of the language. The extensions cover all of the five dimensions of binding \ntime discussed in Section II of the paper. For example, in the area of storage allocation, there are \nthree variants: i) TEMPO/ALLOCATE which requires the pro- granmner to explicitly allocate and deallocate \nstorage. The syntax is modified by adding two more alternatives to \"statement\"~ namely allocate and \ndeallocate and defining allocate \u00f7 allocate {ident [shape]} , ~\u00a3 ; deallocate \u00f7 deallocate ident , \n~\u00a3 ; share \u00f7 [ part , f~66\u00a3 ] part \u00f7 arex I shape I arex shape  and making appropriate changes to the \nsemantics. The size and shape of the value of a variable is now constrained to be unchanging between \nallocation and the corresponding deallocation. ii) TEMPO/STACK which causes allocation at block entry \nand deallocation implicitly at block exit. In this version \"scope\" in the definition of \"block\" is replaced \nby \"decl\" and decl \u00f7 declare ~tacked {ident [shape]} , ~t ;  is added to the syntax. The semantics \nis modified to be identical to doing explicit allocate's (as in TEMPO/ALLOCATE) of all variables whose \nscope is the current block at entry to it, explicit deallocats's at all exits from the block, and no \n reallocation within it. iii) TEMPO/STATIC which requires storage allocation for the entire program \nto be done before its execution begins. The syntax is mod- ified as for TEMPO/STACK except that \"decl\" \nand \"shape\" are now decl \u00f7 declare static {ident [shape]} , f~g6t ; shape \u00f7 [ {number I shape I number \nshape} ,  and the semantics is appropriately modified. Each of these versions constrains programs more \nstringently than the one before it, but also results in a more efficient implementation. A similar spectrum \nof restricted versions is defined for each of the other four dimensions. We refer the reader to our forthcoming \npaper [9] for further information on the restricted versions and the language in general. The research \ndirections of this paper resulted from the observation that the need for the more powerful versions is \nusually present implicitly in the structure of a program written in a less powerful version. Hence a \nsufficiently intelligent processor could choose the most efficient binding times compatible with the \nstructure of a program rather than constrain- ing the progranmner to use the less powerful versions. \n  \n\t\t\t", "proc_id": "800168", "abstract": "<p>A new approach to the design of a programming language and its processor is proposed and some of the techniques necessary to realize the design are investigated. The language would have a precisely specified syntax and semantics, with both designed to provide the programmer maximal expressive power and to be as easily understood as possible. The semantics would be based on extremely late binding times, which provide great power to the programmer and are consistent with ease of understanding of the execution process. It would be the responsibility of the processor to implement each program in the most efficient manner consistent with its being correctly executed. Implications of this design philosophy and some of the techniques to be used are discussed in greater detail, focusing particularly on data types and storage allocation.</p>", "authors": [{"name": "Neil D. Jones", "author_profile_id": "81452616043", "affiliation": "", "person_id": "PP95034660", "email_address": "", "orcid_id": ""}, {"name": "Steven S. Muchnick", "author_profile_id": "81332517217", "affiliation": "", "person_id": "PP31023637", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/800168.811542", "year": "1976", "article_id": "811542", "conference": "POPL", "title": "Binding time optimization in programming languages: Some thoughts toward the design of an ideal language", "url": "http://dl.acm.org/citation.cfm?id=811542"}