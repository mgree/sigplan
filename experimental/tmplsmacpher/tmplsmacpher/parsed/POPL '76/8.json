{"article_publication_date": "01-01-1976", "fulltext": "\n A Lazy Evaluator Peter Henderson University of Newcastle upon Tyne ,lames H. Morris, Jr. Xerox Palo \nAlto Research Center Abstract: A different way to execute pure LISP programs is presented. It delays \nthe evaluation of parameters and list structures without ever having to perform more evaluation steps \nthan the usual method. Although the central idea can be found in earlier work this paper is of interest \nsince it treats a rather well-known language and works out an algorithm which avoids ' full substitution. \nA partial correctness proof using Scott-Strachey semantics is sketched in a later section. I. Introduction \nThis paper studies a non-standard method of performing the mechanical evaluation of expressions in a \npurely applicative language;i.e, one without assignment. The intuitive ideas behind this method are two: \n-Perform an evaluation step only when it is necessary. -Never perform the same step twice. It is somewhat \nsurprising that these objectives can be approached through the use of rather simple data structures and \nalgorithms. The following example should serve to acquaint the reader with the basic idea. integer procedure \ng(x,y); g := if x = 0 then 1 else y*y An ALGOL-60 programmer who wanted to enhance this procedure's speed \nby choosing whether to declare y a call-by-name or a call-by-value parameter would feel most uncomfortable. \nThe value of y is going to be used either twice or not at all, depending on the value of x. The lazy \nevaluation technique overcomes this dilemma because the evaluation of g(E,F) will proceed as follows: \n(1) Substitute pointers, ct and /3, to the expressions E and F for the formal parameters x and y.  (2) \nEvaluate (i.e. reduce to a numeral) the contents of a. (3) If the result is 0 return 1. (4) Otherwise, \nevaluate the contents of ,fl and replace them with the resulting numeral. (5) Evaluate the contents \nof/3 again (this takes little time since already a numeral) and multiply the results.  Thus a lazy evaluator \nwill perform the work to e~'aluate the second parameter either once or not at all. This e\u00d7ample illustrates \nthe call-by-need mechanism of Wadsworth [7] and and the delay rule of Vuillemin [6]. Here we shall carry \nthis strategy one small, but important step further as suggested in [6]: list structures are evaluated \nincrementally. In LISP parlance, an argument of CONS is not evaluated until and unless it is selected \nand examined by some later operation. Thus the statement car[cons[x;y]] = x is always true, even if the \nevaluation of x or y never terminates. This extension is pragmatically important bccause it allows the \npossibility for significantly different styles of programming as the following examples illustrate. Example \nI. Infinite Lists The function defined by integers[i] = cons[i; integers[i+l]] is quite useful under \na lazy evahlation regime, integers[0] denotes the infinite list (0 1 2 ... ) and the expression car[cdr[integers[0]]] \nwill evaluate to 1 via the following intermediate steps: car[cdr[cons[O; integers[0+l]]]] car[integers[0+l]] \ncar[cons[O+l; integers[O+l+l]]] 0+1 1 In a similar way, the list defined by L = cons[l; cons[2; L]] \nis useful and computable. Example 2. A leaf comparator The following functions solve a problem posed \nby Carl Hewitt [2] to illustrate the need for co-routines. EqLeaves[x;y] = EqList[Flatten[x];Flatten[y]] \n95 Flatten[x] = if atom[x] then cons[x;NIL] else Append[Flatten[car[x]]; Flatten[cdr[x]]] Append[x;y] \n= if null[x] then y else cons[car[x];A ppend[cdr[x];y]] EqList[x;y] = if null[x] then null[y] else if \nnull[y] Ihen false else if eq[car[x];car[y]] then EqList[cd r[ x];cdr[y]] else false In other words, \nEqLeaves tests two S-expressions to see if their atoms are identical, independent of structure. This \nobvious solution uses Flatten to eliminate the structure, then uses EqList to compare the atoms. It would \nbe an unnecessarily slow method under normal circumstances because applied to a pair of expressions like \n( A Huge1 ) and ( B Huge2 ) it would go to all the work of Flattening Hugel and Huge2 even though the \nanswer is false because of the first atoms in each structure differ. If a lazy evaluator is used, however, \nthere is no need to change the solution to one involving co- routines because the same computational \neffect will be achieved automatically. Suppose location ~r 0 holds the expression to be evaluated. The \ncomputation will follow this pattern: Wo: EqLeaves[(A Hugel);(B Huge2)] First ~r 0 is updated with the \ndefinition of EqLeaves with actual parameters substituted for formals. no: EqList[Flatten[(A Hugel)];Flatten[(B \nHuge2)]] Now pointers, 71/1 and 7r2, to the parameters are substituted into the definition of EqList \nwithout any evaluation of the parameters. 7to: if null[Trt] then null[Tt2] else if null[~r2] then false \nelse if eq[car[~rl];car[~2] ] then EqList[cdr[Trl];Cdr[n2] ] else false 7rl: Flatten[(A Hugel)] ~2: Flatten[(B \nHuge2)] The primitive null now forces the lazy evaluator to go to work on the contents of ~r 1. ~rl: \nif atom[(A Hugel)] then cons[(A Hugel); NIL] else Append[Flatten[car[(A Hugel)]]; Flatten[cdr[(A Hugel)]]] \n 7rl: Append[Flatten[car[(A Hugel)]]; Flatten[cdr[(A Hugel)]]] ~rl: if nullDr3] then ~t. else cons[car[It \n3];A ppend[cdr[~3];~4] ] ~r3: Flatten[car[(A Hugel)]] 7r4: Flatten[cdr[(A Hugel)]] Again the primitive \nnull forces evaluation steps on ~r 3. ~r3: if atom[~t5] then consists;NIL ] else A Plfend[ Flatten[cdr[~ \n5]];Flatten[cdr[It 5]]] ~r5: car[(A Hugel)] The primitive atom forces the evaluation of ~r s. ~r5: A \n7r3: cons[~rs;NIL ] since atom[A] is true ~rl: cons[car[n3]; Append[cdr[lr3]; ~r4] ] since null[cons \n...] is false 7to: if null[T2] then false if eq[carDrl];car[~r2] ] then EqList[cdr[Trl];Cdr[~r2] ] else \nfalse If we choose to view Flatten as a co-routine, at this point we would say that it has produced \nits first ,~u,~, ~a,t,,3j -A, and its context has been saved in ~r 4 for later reacti vation. Now the \ncontents of ~t 2 is evaluated in the same way until we have ~r2: cons[car[T6]; Append[cdr[~6];~rT]] 7r6: \ncons[B;NIL] 7r7: Flatten[(B Hugel)] ~r0: if eq[car[~rl];car[Tr2] ] then EqList[cdr[nl];Cdr[~r2] ] else \nfalse The primitive eq forces ~1: cons[A; Append[cdr[zt3]; ,/r4] ] ~2: cons[B; Append[cdr[~r6]; lr7] \n] Finally the test is made and the computation terminates with n0: false Notice that Hugel and Huge2 \ndid not enter into any of the foregoing computation and that the work done to evaluate the subexpressions \nin ~r I and ~r 2 for the benefit of the null primitive is not repeated when the eq primitive examines \nthe parameters. Generalizing from this example we can see that a large class of co-routine applications \ncan be subsumed by this technique. A producer co-routine becomes a function that produces a long (possibly \ninfinite) list and a consumer co-routine becomes the receiver of such a list. Also, notions such as streams \nand the dynamic lists of POP-2 are subsumed. The purpose of these programming constructs is to allow \none to describe a sequence of values with a single sub-program, yet have them computed on a hand-to-mouth \nbasis. This assumption is built into a lazy evaluator at the most basic level so there is no need to \ncall for it explicitly. One the other hand. one might ask how to force a more conventional evaluation \nto occur. Suppose one wishes to cause the evaluation of f[s] to proceed conventionally, computing the \nS-expression s before invoking f. One could 96 say, instead of f[s], Force[f,s] where Force[f,s] = if \nFinite[s] then f[s] else don't care and Finite[s] = if atom[s] then true else if Finite[car[s]] then \nFinite[cdr[s]] else don't care The function Finite simply explores the complete S-expression, forcing \nevery part of it to be evaluated. If it ever terminates, Force invokes the function on the now-evaluated \nargument. Example 3: Prime Numbers (due to P. Quarendon) primeswrt[x;I] produces a new list from I by \nremoving all multiples of x primes[I] produces a new list from I by removing any element which is a multiple \nof a predecessor. primeswrt[x;I] = if car[I] rood x=O then primeswrt[x;cdr[I]] else cons[car[I];primeswr \nt[x;cd r[I]]] l)rimes[I] = cons[car[I];primes[primeswrt[car[I];cdr[I]]]] then primes[integers[2]] is \nthe infinite list of prime numbers. Ii. A lazy evaluator for Hyper-Pure LISP In this section we shall \ndescribe a language and its implementation in order to crystallize the notion of lazy evaluation. Hyper-Pure \nLISP is a variant of LISP 1.0 which remains true to the principles of the ~.-calculus [1]. Specifically, \nFUNA~G binding is the only possibility. The syntax of expressions in this language is as follows: (expression> \n::= <variable> I (QUOTE (atom>) I (CONS <expression> <expression>) I (CAR (expression>) I (CDR (expression>) \nI (ATOM <expression>) I (EQ (expression> <expression~.) I (IF (expression> <expression> <expression>) \nI ((expression> (expression>) I (LAMBDA (variable> (expression>) I (LABEL <variable> <expression>) I \n(FUNARG (expression> <alist>) <alist> ::= <empty> I (variable> (expression> <alist> (atom> ::= <any string \nof capital letters> <variable> ::= <any atom except CONS, CAR, CDR, ATOM, EQ, IF, LAMBDA, QUOTE, LABEL, \nor FUNARG) The variations from the syntax of LISP are the replacement of COND by IF, the restriction \nof QUOTE to atoms, the restriction of LAMBDA-defined functions to one argument, and the elevation of \nthe FUNARG construct from an internal bookkeeping device. The first three restrictions are inessential \nand the FUNARG phrase is an extension. Intuitively \"(FUNARG el x e2 y e3)\" means \"el where x = e2 and \ny = e3\". It will greatly simplify the discussion if we assume that the computer memory is of a very accommodating, \nif unrealistic, sort: each cell is capable of holding any of the forms listed as <expression>s where \naddresses are used for any component of type expression. In other words, if 90,. 91, etc. are addresses \nand ~0'~]' etc. are variables, a single memory cell is capable of holding (and discriminating among) \nitems like \u00a20 ' (CONS 91 92) , (90 ~71) , (LAMBDA ~1 ao)' and (FUNARG ~r 0 \u00a21 91 \u00a22 92)\" Naturally, any \nreal implementation would represent such variable-sized memory cells with linked lists; but the extra \npointers would only complicate this discussion. The state of a computation is described by a partial \nmemory function, g, which has the following consistency property: If a particular address, 7r, occurs \nas a component anywhere in the memory (i.e. in #'s range), then #(9) is defined. Thus the addresses for \nwhich # is undefined are the free cells and no non-free cell points at a free one. A computation is started \nby loading the memory with the expression in the obvious way, performing transformations for a while, \nand then examining the root cell of the expression. As the first step in describing the lazy evaluator \nwe describe a set of reduction rules which transform the memory, #, to produce a new memory #'. Each \nrule changes just a few cells. To describe such an altered function we introduce some notation: #[ e \n/ 90 ] = ~.u. if 9=~t 0 then e else #(~) In other words the new function differs from the old at just \none argument, 90 where its value has become E. Furthermore g[ tl/~tl' E:2hr2 ] = (#[ el/Irl 1)[ e2/~t \n2 ] In other words the rightmost pair represents the last change to the memory. There are seven transformation \nrules. [C] g(90) = (CAR 91) and #(?r l) = (CONS ~r 2 93) ;' = ~[ #(92) / ~o ] In programming terms, the \ncontents of 92 are copied into 770. When convenient we shall dispense with mention of extra references \nby convening that p(Tr0) = (CAR (CONS 712 93)) implicitly asserts the existence of a 9 t for which the \nabove is true. The rule for CDR is similar. p.(90) = (CDR (CONS 92 93) ) ~ kt'= #[#(93) / ~t0] [A] #(90) \n= (ATOM(QUOTE a)) /z' = #[(QUOTE \"F) / ~t 0 ] ~(9o) = (ATOM(CONS ~r 92) ) /~ = #[(QUOTE NIL) / 90 ] \nI.e. \"atom-hood\" is simply the property of being QUOTEd. 97 [E] /Z(no) = (EQ(QUOTE al)(QUOTE a2) ) /tt'= \n/z[if % = a 2 then(QUOTE T)else(QUOTE NIL) /%] [I] /t(n0) = (IF (QUOTE T) 7r 1 n2) ~ #'= /t[#(nl) / \nno] p.(n0) = (IF (QUOTE NIL) n 1 n2) ~ /.t'= /t[/t0r2) / no] Thus IF is just an alternate form of the \nusual CONDprimitive. The following rule is a sort of incremental substitution operation which allows \nFUNARG expressions to bubble down through the memory. When the specific list of variable-address pairs \nin a FUNARG expression is not relevant we use 0 to denote it. IF] (a) #(~o) = (FUNARG n' ~1 nl \"'\" ~n \nlrn) and ttt(n') = ~i for some l<i<n (choose the smallest i) #' =/uE#(~r i) / % ] (b) p.(~ro) = (FUNARG \n(QUOTE a) 0) p' = ~[(QUOTE a) / n 0 ]  (c) #(N0) = (FUNARG (n t n2) 0)  =O #'= ~[(~l' ~2') / %' (FUNARG \n71 t 0) / ~l\" (FUNARG 172 0) / n 2' ] where w l' and n 2' are distinct free cells in p (d) If a is one \nof CONS, CAR, CDR, ATOM, EQ, or IF then #(no) = (FUNARG (an l ... ~m) 0) p?= p.[(a n 1' ,.. rim') / \nn 0, , (FUNARG n I 0) / hi' ..... (FUNARG n m 0) / nrn' ] where hi', ~l',-\", rim' are distinct free \ncells in p  Rules Fc and Fd are the only ones which use additional storage. Although it appears here \nthat the list of pairs 0 is being duplicated, in a real implementation which represents a single, variable-sized \ncell with multiple linked cells, it would not be; only a pointer to the list would be duplicated. Notice \nthat rule F does not tell what to do when a LAMBDA or LABEL construct is encountered. The following rules \ngive an answer for some cases. The first one shows how variables are bound to values in FUNARG lists, \nand the second shows how recursion is implemented. [G] #(n0) = ( (FUNARG (LAMBDA ~ hi) 0) n2) /z'=#[(FUNARG \nn I ~ n 2 0) / ~r0] The following rule creates a circular structure. [L] Given p.(n) = (FUNARG (LABEL \n~ ~o) O) p' = ~[(FUNARG % ~ ~ 0) / u ] What can be said of these rules? The reader may wonder whether \nthey \"work\". For example, one of the authors (Morris) thought that rule G might suffer from the usual \ncapture of free variables problem, and was surprised to find it was not the case. This issue is taken \nup in the next section. Presently we shall continue with the description of the evaluator. Note that \nat most one rule can be applicable to any particular \"location, so the only choices left open to an evaluator \nare where to perform the next reduction and when to halt. The procedure for a lazy evaluator is defined \nrecursively as follows: Eval(n,#) : n is the location to be reduced /.t is the memory Eval returns a \nnew memory if p(n) = (QUOTE a) then return p if p(n) = (CONS ...) then return p if p(n) = (FUNARG (LAMBDA \n...) ...) then return/1. [cH if #(n) = (CAR ~') then let #1 = Eval(u',/z) if #l(n') = (CONS ~I ~'2 ) \nthen let #2 = Eval(nl,/~l) return p2[/z2(~t) / ~r] else there is an error, CAR applied to non-pair [C2] \nif B(n) = (CDR n') then let #1 = Eval(n',g) if pl(n') = (CONS It l 172) then let p. 2 = Eval(~r2,gl) \nreturn /t2[/.t2(n2) I n] else there is an error, CDR applied to non-pair [A] if/z(n) = (ATOM n') then \nlet #1 = Eval(u'jz) it #l(n') = (QUOTE a) then return #l[ (QUOTE T) / hi; if #l(n') = (CONS ...) then \nreturn #i[(QUOTE NIL) / n]; else there is an error [E] if/u(u) = (EQ ~t ~2) then let Pl = Eval(n2,Eval(~l,P)) \nif #i(nl)=(QUOTE al) A /s.l(n2)= (QUOTE then if aj = a 2 then return #j[(QUOTE T) a2/)n] else return \n#i[(QUOTE NIL) / u] else there is an error 98 [l] if pot) = (IF ~'o ~'l ~2) then let /t I = Eval0r0,~) \nif btl(~r0) = (QUOTE T) then let #2 = Eval0rl'~l) return #2[P.2(~1) / ~ ] if ~(~o) = (QUOTE NIL) then \nlet /.t 2 = Eval0r2,/tl) return /z2[/t2(~r2) / ~ ] else there is an error [Fa] if p.(Tr) = (FUNARG ~0 \n~1 ~'1 \"\"~n ~n ) and /z(lr0) = ~0 then if i is the smallest such that \u00a2o = ~i then let #1 = Eval0ri,#) \nreturn #l[/tl(~ri) / Ir ] else there is no such i and there is an unbound variable error [Fb] if #(~) \n= (FUNARG (QUOTE a) ...) then return p[ (QUOTE a) / ~r] [L] if #(~r) = (FUNARG (LABEL ~ ~') O) then \nreturn EvaI0r,#[(FUNARG ~r' ,~ ~t 0) / rt ]) [FC] if P(~)e Or 0 ~l) ...) =thn (FUNARG apply rule Fc \nto yield/,t' return Eval(~,#') [Fd] if #(~r) = (FUNARG (a ... ~n) \"\") then apply rule Fd to yield/.t' \nreturn Eval0r,#')  [G] if bt(~r) = (~r 0 ~rl) then let #1 = Eval(~o~) if/~l(~r0) = (FUNARG (LAMBDA ...)...) \nthen apply rule G to location ~r to get #2 return Eval(~,#2 ) else there is an error else there is \nan error The most subtle aspect of this algorithm centers about rules C, I, and Fa. For example, in C1, \nthe first component of the pair is evaluated in situ before being copied into 7r. This policy maximizes \nthe number of paths through the data structure which \"see\" the change. A conventional evaluator differs \nfrom a lazy evaluator in that the parameters of CONS are evaluated as soon as they are encountered and \nthe actual parameter of. a function call are evaluated before they are bound. In terms of the foregoing \ndefinition of Eval this means that the second case is changed to if /z(~)e=th.(CONS ~l qr2 ) return Eval(Tr \n2 ,Eval0r\u00c2 ,/t)) and the case [G] is changed to if/tOO = (If 0 ~I) then let #1 = Eval(~l'EVal(~o,~)) \nif #1(~o) = (FUNARG (LAMBDA ...)...) then apply rule G to location ~i and #1 to get #2 return Eval0r,~2) \nelse there is an error These two changes make certain other invocations of Eval unnecessary. Specifically, \nthe case C1 becomes if #(~) = (CAR ~') then let #I = EvaI(~',p,) if /z](~') = (CONS ~1 ~2 ) then return \n#lfP.l(Ttl) / ,//-] else there is an error, CAR applied to non-pair Case C2 changes analogously, and \ncase Fa becomes if#(~) --(FUNARG ~o ~1 ~1 \"\"~n ~n ) and #(~o) = ~0 then if i is the smallest such that \n~o = ~i then relurn #[~(~i) / ~ ] else there is no such i and there is an unbound variable error All \nthe other cases remain the same. We believe that the lazy evaluator never performs more reduction steps \nthan the conventional one but shall not attempt to prove it here. Example 4. To illustrate the lazy evaluator \nwe shall perform a full evaluation of the expression from example 1. To get things started it is necessary \nto embed the expression to be evaluated in a FUNARG expression with all empty alist. Also, we shall assume \nthat PLUS is a primitive operator with the same general characteristics as EQ and that numerals are understood \nto be QUOTEd atoms. Not all addresses are explicit; each pair of parentheses indicates the presence of \nan unnamed address. a:(FUNARG ((LAMBDA INTS (CAR(CDR(INTS 0)))) (LABEL INTEGERS (LAMBDA I (CONS I(INTEGERS \n(PLUS I 1))))))) Apply Fc to ct a: ((FUNARG (LAMBDA INTS (CAR(CDR(INTS 0))))) n: (FUNARG (LABEL INTEGERS \n(LAMBDA I (CONS I(INTEGERS (PLUS I 1)))))) Apply G to a a: (FUNARG (CAR(CDR(INTS 0))) INTS n) Apply \nFd to a a: (CAR \"y) 77: (FUNARG (CDR tINTS 0)) INTS n) Stack a, Apply Fd to y ~: (CDR ~) 99 /5: (FUNARG \n(INTS 0) INTS/3) Stack v,Apply Fc to 8 /5: (t ~o) E: (FUNARG INTS INTS fl) ~0: (FUNARG 0 INTEGERS/3) \nStack 6, E, Apply L to B /3: (FUNARG (LAMBDA i (CONS I(INTEGERS (PLUS i I)))) INTEGERS/3) Return to \n~, Apply Fa e: (FUNARG (LAMBOA i (CONS I (INTEGERS(PLUS I 1)))) INTEGERS fl) Return to /5, Apply G i5: \n(FUNARG (CONS ! (iNTEGERS(PLUS i 1))) irp li~ fEGERS fl) Apply Fd to ~5 /5: (CONS ~ 7) ~: (FUNARG I I \n~ INTEGERS fl) 7: (FUNARG (INTEGERS (PLUS I 1)) I \u00a2p INTEGERS/3) Return to T to note CDR, Stack T again, \nApply Fc to r/ 7: (~ s) t: (FUNARG INTEGERS I \u00a2p INTEGERS 13) s: (FUNARG (PLUS I 1) I \u00a2p INTEGERS/3) \n Stack O, Apply Fa to t t: (FUNARG (LAMBDA I (CONS ! (INTEGERS (.PLUS I 1)))) INTEGERS/3) Return to 7, \nApply G ~7: (FUNARG (CONS ] (INTEGERS (PLUS I 1))) is INTEGERS/3) Apply Fd to n 7: (CONS ~ X) x: (FUNARG \nI I INTEGERS/3) h: (FUNARG (INTEGERS (PLUS i 1)) I s iNTEGERS fl) Return to T, Apply C T: (CONS ~ h) \nReturn to a to note CAR, Stack a again, Stack x, Apply Fd to s s: (PLUS # ~,) /z:(FUNARG i I rp INTEGERS \nfl) v:(FUNARG 1 i \u00a2p iNTEGERS fl) Stack s, Stack #, Apply Fb to \u00a2p \u00a2p: 0 Return to# , Apply Fa to# #: \n0 Return to s to note PLUS, Stack s, Apply Fb to v v: 1 Return to s, Apply P s: 1 Return to x, Apply \nFa K: 1 Return to a, Apply C a: 1 !11. Semantic Considerations There are several questions one might \nask about the foregoing transformation rules and evaluator. (1) Is the final answer independent of the \norder in which the rules are applied; i.e. does the system have the Church-Rosser property? (2) Are \nthere enough rules to allow an answer to be computed in all cases we consider legal? (3) is the evaluator \ncomplete in the sense that will compute an answer whenever any application of the rules will do so? \n(4) is the evaluator optimal in the sense that it performs the minimum needed steps to compute an answer? \n We believe that the answer to the first three is yes, and know that the fourth is not true. However, \nthey are not very meaningful questions unless we have an independent definition of what an \"answer\" is. \nTo get one we shall first define the meaning of an expression in terms of Scott-Strachey semantics [4] \nand then define an answer as a certain finite amount of information about that meaning. An analogous \napproach for arithmetic would be as follows: 100 (1) Given the class of arithmetic expressions involving \nnumerals and no variables (e.g. \"4\", \"4+5\", \"5-(6+2)\"), consider the domain of integers (i.e ..... -2,-1,0,1,2,...). \n (2) Define a semantic function, V, mapping expressions into domain elements. V(\"9\") : 9 and V(\"4+5\") \n: 9 (3) In this case the definition of an answer is obvious: Any computer should reduce its input expression \nto the (possibly signed) numeral which has the same value as the original expression.  The central idea \nis that the computer does not find the value for the expression, but only reduces the input to a more \ncomprehensible form which has the same value. When, as in the case of arithmetic, there is a unique, \nfinitely representable canonical form for any value the distinction between a value and the output expression \nis not interesting. On the other hand, here we are dealing with values that can be infinite list structures \nand functions so the distinction between a value and an answer is real. The definition of a semantics \nfor the h- calculus has already been carried out by Wadsworth and others, see [5,7]. Our approach follows \ntheirs but extends it somewhat to introduce the notion of a semantic memory. This approach allows us \nmore easily to make the connection between the semantics and the evaluator, In particular, it allows \nus to deal with the sharing and sometimes circular data structures more directly. The Domains A -the \nprimitive domain of atoms, a,a',a 1, etc. denote atoms. C -the primitive domain of variables. ~ denotes \na variable. There is no reason why atoms cannot be used as variables, but things seem clearer if they \nare kept distinct. R the primitive domain of references (addresses). It denotes a reference.  E : C \n+ A + RXR + R + R + R + RXR + RXRXR + R\u00d7R + CXR + CXR + R\u00d7(C\u00d7R)* the domain of expressions corresponding \nto the twelve possibilities listed in section II. In other words (CAR 77) is a member of the fourth part \nof the disjoint union, e denotes an expression. M : R ~ E -the domain of memories, each cell of which \nis capable of holding an expression. # denotes a memory. V = A + ( V X V ) + ( V ~ V ) -the domain of \nvalues. A value can be an atom, a pair of values, or a function from a value to a value. This domain \nis the one which requires Scott's theory as it contains its own function space. Furthermore we use Reynolds's \n[3] version of the disjoint union operator so that _L, (QUOTE ..L), (CONS _L _L), and (LAMBDA X _L) all \nhave distinct values, with _L being the bottom of the whole lattice. N : C ~ V -the domain of environments, \np denotes an environment. S : R ~ ( N ~ V ) -a semantic memory, mapping references and environments \ninto values, o denotes a semantic memory. A semantic memory has addresses just  like a conventional \none but its cells can hold genuinely infinite objects. Roughly speaking the semantic object that a cell \nw holds is what one gets by tracing out, in the conventional memory, the structure of pointers emanating \nfrom that cell. Tile Semantic Function V In the following, to reduce parentheses, we shall adopt the \nconvention that fa,B~ means ((f(a))(fl))(~);i.e. f is applied to a, the result is applied to fl, etc. \nV maps conventional memories into semantic memories;i.e. V:M~S it is defined recursively by Vp~p = U(V#)(#~)p \nwhere /.k~Ep = if E E C then p~ else if e = (QUOTE a) then a if ~ = (CONS ~1 7t2) then <a~lp , a~2p> \nelse if c = (CAR ~') then (an'p) 1 else if ~ = (CDR ~') then (of'p) 2 else if ~ = (ATOM ~\") then [ if \na~'p E A then \"T\" else if aTt'p C V X V then \"NIL\" else .L ] else ife = (EQ~t 1 1t2) then [ if O~lp \nE A A a~2p E A then if a~lp = a~2p then \"T\" else \"NIL\" else _L ] else ife : (IF~0w! ~2) then [ if art \n0 p = \"T\" then a~tlP else if art 0 p = \"NIL\" then a~t2p else .L  ] else if e = (~0 ~1) then cr~Op (aztlp) \nelse if E = (LAMBDA ~ ~') then ~x. aTt'#[ x/~ ] else if ~ = (LABEL ~ ~t') then Y {hx. aT'p[ x/~ ]} else \nif ~ = (FUNARG w0 ~I ~1 \"'\" ~n ~n) then a~ 0 P[ altnPl~n,...,a~tPl~ l ]  else .1. This definition depends \nupon several informal semantic operations: <,> forms pairs, a subscript selects components,etc. The most \nnoteworthy are the last four: The application of a~0p to awlp is the function application which required \nScott's construction to justify since both values reside in the same domain. Note that the h is also \nan informal notion and that the bracket notation is used to define the changed environment p[ x/~ ]. \nY is the minimal fixed point operator. It happens that mapping a FUNARG construction into its semantics \ninvolves a complete reversal of the list of bindings. I01 It is instructive to compare this function \nwith the Eval function of section II. The major difference is that V is quite happy to deal with completed \ninfinite objects like functions. The operation of applying a function to its argument is taken as primitive \nhere, while it was done very slowly and incompletely by Eval. This particular way of describing the function \nisolates the application of the memory function to a single place, namely the /tw in V's definition; \nU uses only the semantic memory (7. Now, given the definition of Eval, it should be clear what an answer \nis. Suppose we load the memory with an expression so that the root of the expression occurs in w0, and \nwe start the lazy evaluator on that location. If it ever halts (ignoring the possibility of error stops) \nwe know that w 0 will contain an expression with one of the three forms (QUOTE a), (CONS ..), or (FUNARG(LAMBDA...)...). \nThus the answer tells which of the three components of V the value lies in; and, if it is an atom, what \natom it is. In the other cases, nothing more is revealed, or computed. This fact indicates how we should \ndefine the correctness of an evaluator: If I'itw0_L = _L then Eval(w0,/u. ) does not halt. If I'jZwo.L \n= a then Eval(w0,g)(w0) = (QUOTE a). If Vgwo_L is a pair then Eval(w0,#)(w0)=(CONS ...). If V/uw0_L is \na function then Eval(~0,#)(w0)=(FUNARG(LAMBDA..)...). We use the empty environment, _L, in these statements \nbecause it is assumed that the expression to be reduced does not contain free variables at the outermost \nlevel. Therefore, an environment function is not needed initially. Soundness For the present we shall \ncontent ourselves with sketching a partial correctness proof;i.e, that the last three clauses hold if \nEval halts. This can be done by showing that each of the seven transformation rules leaves the semantic \nmemory unchanged (except at newly allocated cells). Then if the evaluator starts with memory /t at location \n770 and halts with memory #' we know that V/tw0.L = V/t'Wo..L , and the last three c!auses are immediate \nsince the Eval function halts only on the three forms in question. This method of proof also proves a \nqualified \"yes\" to the first question in this section; it doesn't matter in what order the transformations \nare api~lied. The argument goes as follows: On semantic grounds we know that (QUOTE a), (CONS ...) and \n(FUNARG(LAMBDA ...)...) all denote distinct objects, and that atoms that look different are different. \nIf we then show that the transformations cannot change the semantic value in a cell, we know that all \nsequences of transformations which produces one of those configurations in a cell must produce the same \none. The proofs for the various rules are very similar. We shall give two. Rule C: Suppose /t(n0) = (CAR \nWl) and /t(nl) = (CONS w 2 w3) and rule C is applied. We shall prove that V/t = V#\" where /t' = /t [ \n#(~2) / w 0 ]. First we define the truncations of V by V i =_L for i_<0 v~+~/twp: u(r,/t)(/t~)p so V \n= lim i V i Now a straight-forward computation shows that Vi/.t = ~wp.if w=w 0 then U(Vi_i/t)(CAR(CONS \nw 2 w3))p else U(Vi_l/t)(/t~)p = hwp.if ~=~r 0 then {U(Vi_2/t)(CONS w 2 7t3)p} 1 else U(Vi_l/t)(g~)p \n = hwp.if w=w 0 then U(Vi_:)(/t~2) p else U(Vi_t/t)(/t~)p and V:'=hwp. if w=~ 0 then U(I'i_l/t')(#w2) \np else U(Vj_lg')(/t~)p Now it is easy to show that (Vi)(3j)[ Vi/t C I~' ] and vice versa. Thus lim i \nVi/t = limjlK~'. Rule L: Here the proof is somewhat different since it involves comparing a circular \ndata structure with a minimal fixed-point. Suppose #(no) =(FUNARG (LABEL ~ ~t) ~2 ~2 \"'\" ~n Wn) and \nIC = #[(FUNARO w I ~ ~0 ~2 w2 \"'\" ~n qln)/q/0 ] First, we claim without proof that V# and V#' are the \nminimal solutions to the following equations, respectively: V# = hwp. if lz=Iz o then Y{Xx. V#wtP [ V#~tnPl~n...Vl~Tt2Pl~p \nx/~]} else U( V#)(#~)p Vg' = hwp. if w=w 0 then V#'w]p[ V#'WnO/~n...V/t'~2P/~2, V#'woP/~ ] else U(V/t') \n(/tw)p It is simple to show that V# satisfies the equation for V/t' so V#' C V/t. The proof in the other \ndirection is more difficult. The difficulty seems to be that V# involves a loop, represented by the Y, \nwithin a loop, represented by the definition of V, while V/t' involves just one loop. To overcome this \nproblem we will \"cut\" the loop represented by Y at the same time we cut the loop represented by V. First, \nwe claim (based on continuity) that the following equations are equivalent to the ones above for defining \nV/t. Vi/t = ..L for i<0 hwp. if 7t=It 0 then Yi{hx.Vi_lgWlp [ Vi_lp.W~/,~n.. .Vi_I/tw2p/{2, x/{]} else \nU(Vi_l/t)(/tw)p Yi = _L for i<0 Yi = hf'f(Yi-lf) V = lira i V i Now we prove Vi/t C V/t' by induction \non i. It is vacuously 102 true for i ~ 0, so assume it for all k<i. Then, by the Acknowledgement induction \nhypothesis I~# C_ X~rp. if ~r:lr 0 then Yi F else U( Vil')(#~r )p where F = {hx. Vg'~rtp [ VIl'TrrvO/~n...V#'~r2P/~2, \nx/,~]} The proof will be complete if we can show ViF C_ V#'~r0P This fact can also be shown by and induction \non i. Assume the result for all k<i,then Yi F : V/l'Trl.p[ V~'~rnP/~n...Vtt'Tr2Pl~2, Yi_iF/~ ]   C \nvW~!p[ vW~,.p/~,...vW~2P(~2, v~'~op/~ ] by the induction nypothesis : y~'~ IV. Remarks The two objectives \npresented at the beginning of the paper can now be g ve ~ more substance. First, the general question \nof when an evaluation step is necessary needs to be answered. Initially, some external consideration \nmust indicate that a particular location's value must be pursued; e.g. the user would like that value \nto be printed. Then, the \"need to be ewduated\" propagates itself to the descendants of that location \naccording to rules peculiar to the semantics of the language. These rules were straightforward for the \nlanguage studied here. Sometimes they are less so. For example, changing the semantics of IF so that \n(IF p x x) : x even when p is undefined would require simu!taneous evaluation of all three parts of an \nIF clause. In any case, it appears that performing \"outer-most\" reductions, i.e. those closest to the \ninitial source of the need, is a good heuristic. The reason is that these reductions may make some parts \nof the structure which are farther away unnecessary. Second, the notion of \"same step\" needs clarification. \nHere, all that has been achieved through the use of pointers is that the advantages of evaluating an \nexpression earlier have been I~LdlIICU. II t,C c^plc~tU, \"3+7\" arose twice in an evaluation from entirely \ndifferent places there is no simple way to avoid its recomputation. Finally, a comment on the usefulness \nof Scott-Strachey semantics. We can hardly claim that the lazy evaluator is \"right\" because it is correct \nwith respect to the semantics defined in section II1. It is obvious that the semantics can be adjusted \nto fit any mechanical evaluation method one chooses. One the other hand the use of a semantic model is \na great aid in studying the implications of various evaluation rules without gettting involved in too \nmany details. Section Ill of this paper was primarily the work of the second author with significant \nhelp from Howard Sturgis of Xerox PARC.  References [1] Curry,H.B. and Feys,R., Combinatory Logic, vol \n!. North- Holland, 1958. [2] Hewitt, Carl, et. al., Behavioral semantics of non-recursive control structures, \nProceedings, Colloque sur la Programmation, Springer-Verlag Lecture Notes in Computer Science, No. 19, \n1974. [3] Reynolds, J. R., Notes on a lattice-theoretic approach to the theory of computation, Lecture \nnotes, Syracuse Unversity, 1971. [4] Scott, D. and Strachey, C., Toward a mathematical semantics for \ncomputer languages, Proc. of the Symposium on Computers and Automata, Polytechnic Institute of Brooklyn, \nand PRG-6 Oxford University Computing Laboratory, 1971 [5] Stoy, J., The Scott-Strachey approach to the \nmathematical semantics of programming languages, Course notes at M.I.T. Project MAC, 1973. [6] Vuillemin, \nJ., Correct and optimal implementations of recursion in a simple programming language, Journal of Computer \nand System Sciences, vol. 9, No. 3, December 1974. [7] Wadsworth, Christopher, Semantics and Pragmatics \nof the Lambda-calculus, PhD. thesis, Oxford, 1971 103 \n\t\t\t", "proc_id": "800168", "abstract": "<p>A different way to execute pure LISP programs is presented. It delays the evaluation of parameters and list structures without ever having to perform more evaluation steps than the usual method. Although the central idea can be found in earlier work this paper is of interest since it treats a rather well-known language and works out an algorithm which avoids full substitution. A partial correctness proof using Scott-Strachey semantics is sketched in a later section.</p>", "authors": [{"name": "Peter Henderson", "author_profile_id": "81100434048", "affiliation": "", "person_id": "PP14152823", "email_address": "", "orcid_id": ""}, {"name": "James H. Morris", "author_profile_id": "81332516966", "affiliation": "", "person_id": "PP31028920", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/800168.811543", "year": "1976", "article_id": "811543", "conference": "POPL", "title": "A lazy evaluator", "url": "http://dl.acm.org/citation.cfm?id=811543"}