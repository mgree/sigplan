{"article_publication_date": "01-01-1976", "fulltext": "\n AUTOMATIC GENERATION OF EFFICIENT EVALUATORS FOR ATTRIBUTE GRAMMARS Ken Kennedy Scott K. Warren Department \nof Mathematical Sciences Rice University Houston, Texas 77001 1. INTRODUCTION The translation process \nmay be divided into a syntactic phase and a semantic phase. Context-free grammars can be used to describe \nthe set of syntactically correct source texts in a formal yet intuitively appealing way, and many techniques \nare now known for automatically constructing parsers from given CF grammars. Knuth's attribute grammars \noffer the prospect of similarly automating the implementation of the semantic phase. An attribute grammar \nis an ordinary CF grammar extended to specify the \"meaning\" of each string in the language. Each gram- \nmar symbol has an associated set of \"attri- butes\", and each production rule is provided with corresponding \nsemantic rules expressing the relationships between the attributes of symbols in the production. To find \nthe meaning of a string, first we find its parse tree and then we determine the values of all the attributes \nof symbols in the tree. When an attribute grammar is used to specify the translation activity of a com- \npiler, the attributes will be such things as data types of expressions, symbol tables for use in translating \nidentifiers, and machine code generated for statements. Using an attribute grammar to specify the trans- \nlation performed by a compiler offers these advantages: the semantics is given in a descriptive rather \nthan algorithmic nota- tion, independent of any parsing scheme; the description is modular, given on \na production-by-production basis, so that it is easier to understand and modify; and the notation naturally \nexpresses the idea of context-dependence as well as the relation of a whole to its parts. Work supported \nby Grant DCR73-03365 from the NSF, Division of Computer Research. Despite these advantages, attribute \ngrammars have not been widely used because of the difficulty of obtaining implemen- tations efficient \nenough for practical use. The problem is one of converting a non- procedural semantic description into \nan efficient procedure for performing the translation. Attribute grammars leave the order of evaluation \nof semantic functions largely unspecified, requiring only that a function's arguments be computed before \nthe function is evaluated. A suitable eval- uation order must take into account both the data dependencies \nof the semantics and the shape of the particular parse tree. Until now, automatic evaluation has been \npossible only through the use of a non- deterministic algorithm IF] or by severely restricting the class \nof acceptable attri- bute grammars [Bi,Ji]. In this paper we present a method of constructing, for a \ngiven attribute grammar, a recursive procedure which performs the specified semantic evaluation. These \n\"treewalk evaluators\" are produced by ana- lysing the data dependencies of the attri- bute grammar to \nfind an acceptable eval- uation strategy. A wide class of attribute grammars can be handled, and the \nresulting evaluators are efficient in that they make no use of nondeterminism or searching through the \nparse tree and may be generated as directly executing machine code. Our construction should therefore \nbe useful in a practical compiler-writing system. Section 2 provides an introduction to the basic concepts \nassociated with attribute grammars. Sections 3, 4, and 5 describe treewalk evaluators and how they are \nbuilt; here we characterize the class of attribute grammars for which the construction will work. In \nSection 6 we compare our method to previously known techniques for imple- menting attribute grammars. \nFinally, Section 7 contains a summary and discussion of possible improvements to our scheme. 2. ATTRIBUTE \nGRAMMARS  A variety of techniques have been used to introduce the idea of semantics into context-free \ngrammars, such as indexed gram- mars [A], property grammars [SL], programmed grammars JR], two-level \ngrammars [vW], and scattered-context grammars [GH]. These efforts were concerned primarily with lim- \niting the set of terminal strings generated by a grammar, and only secondarily with specifying the meanings \nof those strings. Irons [I] introduced the idea of syntax- directed translation, essentially allowing \na single synthesized attribute to be associ- ated with each nonterminal. Generalizations of this idea \nto allow multiple synthesized attributes were studied extensively by Lewis and Stearns [LS] and by Aho \nand Ullman [AUl, AU2]. Knuth [Knl,Kn2] extended this form- alism in an important way by allowing attributes \nto depend on the context sur- rounding a grammar symbol as well as on its constituents. Knuth's \"attribute \ngrammars\" have attracted widespread interest [Bi,B2, Cu,D,Ji,J2,NA, Wil,Wi2] as a means of specifying \nsyntax-directed trinslations because of their naturalness, \"declarative\" nature and expressive power. \nIn the re- mainder of this section we define attribute grammars and related concepts and illustrate \ntheir use. Many of our definitions are taken directly from [Bi]. An attribute grammar is an ordinary \ncontext-free grammar augmented with attributes and semantic functions as de- scribed below. Grammar: \nA reduced context-free grammar G = ~N' VT' P, S). We write V for V N u V T. A production pep is written \nas , P: X 0 + X 1 X 2 ... Xnp  where np a i, X0~VN, and Xk~V for k > 0. We write p[k] to mean X k \nfor k=0,1,...np. We assume that the grammar is standardized if necessary with a 0-th production 0:S+S' \nso that the start symbol S occurs in no other production. A parse tree of G is a finite ordered tree \nwhose nodes are labelled with symbols from V, such that: for each interior node t there is a production \npeP such that t is labelled with the symbol p[0], t has np sons, and the k-th son of t is labelled with \nthe symbol p[k]. We say that p applies at t, or equivalently that t is a type-p node. Attributes: For \neach X~V, there are finite disjoint sets I(X) and S(X) of inherited and synthesized attributes respectively. \nFor X=S, the start symbol, and for X~V~, we require that I(X) = ~. We write A(X) -for I(X) u S(X). The \nattributes of a grammar symbol identify the various components of its \"meaning\". A production p:X0+Xl...Xnp \n has the attribute occurrence (a,k) if acA(X k) for k=0,1,...np. Attribute occur- rences are to be understood \nas variables which are used in writing the semantics for a production. Inherited attributes trans- mit \ninformation down the parse tree toward the leaves, while synthesized attributes transmit information \nup the parse tree to- ward the root. The start symbol may not have inherited attributes because it can \nhave no ancestor. Terminal symbols may have no inherited attributes because they have no associated semantics. \nThe values of a terminal symbol's synthesized attri- butes are given initially; in a compiler, this is \nthe job of the lexical scanner. The term \"attribute\" is used ambiguously to mean some aEA(X), as in \"an \nattribute of a nonterminal\"; to mean some occurrence (a,k), as in \"an attribute of a production\"; or \nto mean a value attached to the parse tree, as in \"an attribute of a node\". It should always be clear \nfrom the context which sense is intended. Semantic functions: For each production pep there is a set \nof semantic functions as follows: for every synthesized occurrence (a,k) with k=0, and for every inherited \noccurrence (a,k) with k=l,2,...np, there is a semantic function f~a,k) mapping certain other attribute \noccurrences of p into a value for (a,k). The semantic functions specify the meanings of parse trees locally, \nin terms of only a node and its immediate descendents. We do not consider the language in which the functions \nare written; we assume only that we can identify the attri- bute occurrences referenced in a function \nand that the function can be translated into machine code to do the evaluation. A semantic tree of an \nattribute grammar is a parse tree in which each node labelled with XeV is a structured variable whose \nfield selectors are the elements of A(X). In order to determine the meaning of a string, we parse it \nand build a semantic tree, and then we fill in the fields of each node in accordance with the semantic \nfunc- tions. The latter process is called evalu- atin~ the semantic tree. An evaluator for an attribute \ngrammar is a program which accepts a semantic tree and evaluates it. In this paper we refer to the time \nwhen an evaluator operates as run-time; thus we think of our construction method as a com- piler which \nproduces evaluators as object programs. The significance of the attribute grammar formalism is specified \nby a defining evaluator which provides a con- ceptual model for evaluating semantic trees. Evaluation \nbegins with all fields of the semantic tree undefined. At each step, some attribute of a node is chosen \nwhose 33  semantic function can be evaluated; that is, all of whose argument occurrences are al- ready \ndefined. The chosen semantic function is executed and the corresponding field of the tree node is defined \nby setting it to the computed value. The process continues until all attributes in the tree have been \ndefined. The defining evaluator is non- deterministic, since at each step any attri- bute can be chosen \nwhich is ready. Example. Figure 1 gives an attribute grammar specifying the translation of binary con- \nstants into the numeric values they repre- sent [F]. Observe that the notation Xk.a stands for the attribute \noccurrence (a,k); if the nonterminal X k occurs only once in a production then the subscript k is omitted. \nTo represent the desired translation we have invented the val attribute for the start symbol N. val is \nalso an attribute of the bit-lists (L) and the individual bits (B). However, in positional notation \nthe value of a l-bit depends on how many places to the left it appears; the attribute pos is introduced \nto express this. For instance, the semantics for production 7 says that the value of a l-bit is the power \nof two corresponding to the bit's position. On the other hand, rule 6 says that the value of a 0-bit \nis always zero. In this attri- V N = {N,S,L,B} V T = {+,-,0,I} I (N) i~ S(N) = {val} I (S) = S(S) = \n{neg} I(L) = {pos} S(L) = {val} I(S) = {pos} S(B) = {val} Production Semantics i: N-~SL L.pos \u00f7 0 N,val \n\u00f7 i_/_ S.neg then -L.val else L.val fi 2: S + + S.neg \u00f7 FALSE 3: S -* - S.neg \u00f7 TRUE 4: L-~ B B.pos \n\u00f7 L.pos L.val \u00f7 B.val 5: L 0 -~ L I B Ll.pos \u00f7 Lo.pos+l B.pos \u00f7 Lo.pos Lo.val \u00f7 Ll.val+B.val 6: B + \n0 B.val + 0 7: B + 1 B.val \u00f7 2+B.pos bute grammar val is a synthesized attribute carrying information \nup the tree toward the start symbol, while pos is an inherited attribute carrying information down \nthe tree from above. Figure 2 shows the semantic tree for the string -101. The fields have been filled \nin as prescribed by the semantic rules. The effect of our attribute grammar in this case is to specify \nthat the trans- lation of -101 is -5. It is a matter of taste whether the translation of a string is \ntaken to be all the attribute values in its semantic tree or just those of the root. 3. TREEWALK EVALUATORS \n As the name suggests, a treewalk eval- uator is a recursive routine which traverses the semantic tree \nperforming attribute eval- uations. As defined by Knuth [Kn3], a tree traversal is \"a method of examining \nthe nodes of a tree systematically so that each node of the tree is visited exactly once\", but we have \nin mind something a bit more general;\" in particular, we allow multiple visits to the same node, and \ndifferent nodes may be visited different numbers of times. More specifically, a treewalk evaluator is \na recursive routine taking a tree node as a parameter; in execution it is said to be \"visiting\" its \nparameter node. While at the node, the evaluator may take actions of two N L po:;: I po.' (> ( )  \nFigure i. Figure 2. 34 kinds. It may execute semantic functions of the production applying at the node, \nthereby evaluating some attributes of the tree; or it may call itself recursively with one of its sons \nas a parameter. It may per- form any combination of such actions before returning. Our treewalk evaluators \nwill have a particularly simple control structure: upon reaching a node, the evaluator will assess the \nsituation and choose one of a fixed set of instruction sequences, which it will then execute before returning. \nEach of these instruction sequences, or plans, will be a straight-line sequence of actions of the two \nkinds mentioned above; there is no branching within a plan (except possibly within sem- antic functions, \nwhich we consider atomic). Within a plan no tests are necessary to determine whether an attribute can \nbe eval- uated, since all such analysis has been performed during evaluator construction. To select the \nproper plan for execution at a node, our evaluators will have two types of information available. First, \nthe eval- uator will receive a control parameter from above when it is called; this current-input- set \nparameter summarizes the activity that has taken place above since the last visit, and is used to allow \nevaluation strategy at a node to depend on the context in which it occurs. Second, there is the situation \nat the node itself, represented by the value of a fla@ field present in each node; the flags are used \nby the evaluator to remember information between visits to a node. In operation, the evaluator moves \nfrom node to node performing attribute evalu- ations until evaluation is complete. Initially, the evaluator \nis called with the root of the semantic tree as its argument. In general, a recursive call is made when \nsome new inherited attributes of a son have been made available; the effect of the call is to accomplish \nas much more evaluation of the subtree as the new attributes permit. This may involve deeper recursive \ncalls. During a visit, the evaluator may make mul- tiple visits to a subtree of the node, each time performing \na partial evaluation. Thus a visit may contain multiple passes over the subtrees of the visited node. \nBecause of prior dependency analysis, our evaluators always \"know\" where to go next; they do not have \nto search around looking for an attri- bute which can be evaluated. The evaluator will eventually terminate \nby returning from its initial call, at which time all attri- butes in the semantic tree will have been \nevaluated. Definition of a treewalk evaluator. We now make precise the idea of a tree- walk evaluator \nand its operation, as well as some subsidiary concepts. A treewalk evaluator is a 6-tuple (Q' q0' E, \nI, GOTO, PLAN), where Q is a finite set of quiescent states; q0: P + Q assigns to each production p \nan initial state q0(p); E is a finite set of entry states; I is a finite set of current-input-sets, \ncontaining a distinguished element denoted ~; GOTO: Q. \u00d7 I + E is a partial function assigning an entry \nstate to certain quiescent-state/current-input-set pairs; PLAN: E + {plans} is a total function assigning \na \"plan\" to each entry state. A plan is a pair (SlS2...Sn, q) where SlS2...s n is a sequence of \"instruc- \ntions\" and q ~ Q. An instruction is either (a) a semantic function fP(a,k)'  or (b) a symbol of \nthe form VISIT(k,I) where k is an integer and I ~ I.  A semantic tree with root r is evaluated as follows: \n i. Each node of the tree is flagged with its initial state, given by q0(p) where p is the production \napplying. 2. The algorithm EVALUATE(r,~) is - performed. Al~orithm EVALUATE(t,I) : 0. t is a tree \nnode and I c I. i. Let qbe the flag of t. 2. Let e = GOTO(q,I). 3. Execute PLAN(e) at t. end To execute \nplan (SlS2...Sn, q) at node t: i. For i = 1,2,...n, execute s i at t. 2. Set the flag of t to q. To \nexecute instruction fP at node t: (a,k) Evaluate the semantic function and set the tree field corresponding \nto attri- bute occurrence (a,k) to the resulting value. To execute the instruction VISIT(k,I) at node \nt: i. Let t' be the k-th son of t. 2. Perform the algorithm EVALUATE(t',I). Comments: It should be \nclear that the eval- uation process does not make any use of the structure of members of Q, E, or I; \ntherefore these elements may be encoded as integers at run-time. Similarly, since the functions q0' GOTO, \nand PLAN have finite domains, they may be implemented by suitable forms of table look-up. For this reason \nwe use the term \"GOTO-table\" to refer to the GOTO function considered as a set of pairs; we say ((q,I),e) \nis a \"GOTO-table entry\" if GOTO(q,I) = e. Finally, each plan may be compiled into machine code and directly \nexecuted at run-time. Overview of the construction. Our approach differs from previous work [B1,Jl] \nin that we \"compile\" an attri- bute grammar into a tailor-made evaluation algorithm rather than propose \na general- purpose algorithm a priori. We do so by a method which involves the simulation, in a limited \nsense, of events which could take place during evaluation. In this way we are able to do all necessary \ndependency analysis in advance, thereby eliminating the overhead of run-time analysis and avoiding blind \nsearch through the tree. We make the assumption that the evaluator's action at a node can be independent \nof the structures of the node's subtrees. This assumption is motivated by the conception of grammar nonterminals \nas abstractions with definite interpretations of their own; the idea of locally-defined semantics sug- \ngests that it should be possible to under- stand the semantics of a construct without any further knowledge \nof the abstractions it is built of. We believe this is a reasonable assumption and we expect that most \npractical attribute grammars will sat- isfy it; however, in [Wa] it is shown how to extend our method \nto treat any non- circular attributegrammar. The evaluator construction itself is presented in two parts. \nPlannin~ is the process of building an instruction sequence to be used in a certain situation. A par- \ntial simulation of a visit is conducted which maintains the set of attributes known to be available at \neach step. This is enough information to allow dependency ana- lysis to decide which semantic function \n should be evaluated next. By considering the possible dependencies between syn- thesized and inherited \nattributes of each son, the method is also able to decide when a recursive visit to a sub-node should \nbe made. For a given initial situation, the simulation produces a plan which may be immediately executed \nupon encountering that situation. Makin@ a complete set of plans is an iterative process which ensures \nthat every possible situation during evaluation has been planned for. Recall that when EVALUATE is called \nto visit a node, it first consults the GOTO-table to find this visit's entry state, and then executes \nthe plan corres- ponding to that state. Thus the evaluator construction must ensure two things: the GOTO-table \nmust have an entry for every com- bination of quiescent-state flag and current- input-set parameter which \ncan be encountered, and PLAN must associate a plan with every entry state. To start things off, a GOTO- \ntable entry is made corresponding to the initial visit to the root of the semantic tree, and a plan is \nmade for the resulting entry state. By examining the plans made so far we can determine what GOTO-table \nentries a particular VISIT instruction may cause to be looked up. If not already pre- sent this must \nbe added to the GOTO-table; and if the entry state does not already have an associated plan, a new one \nmust be made for it. This process is repeated until no new GOTO-table entries can be found. At this point \nthe evaluator contains provisions for every situation that may arise. Figure 3 shows an example attribute \ngrammar and a treewalk evaluator for it. 4. PLANNING Our intention is to avoid considering dependencies \nat run-time by doing this work during evaluator construction. We make use of incomplete representations \nof run-time situations and actions containing only the information necessary to see that dependency constraints \nare obeyed. We imagine that the evaluator has just arrived at a node and found it in a given initial \nsituation. We then repeatedly select an action permitted by dependency constraints, schedule it for execution \nnext, and determine what the new situation will be after it is executed. When no next action is permitted, \nplanning stops; this is equivalent to scheduling a return to the evaluator's caller. Definition: An \nevaluation state is a pair (p,A) where peP and A is a set of attribute occurrences for p. An evaluation \nstate represents the situation at a node at some instant during evaluation. If a node is in state (p,A) \nthen production  36 Production Semantic functions 0: S \u00f7 A S.r\u00f7A.z, A.c\u00f71 I: A + a B A.z\u00f7B.y, B.a\u00f7B.y, \nB.b\u00f7B.x 2: A \u00f7 a a B A.z\u00f7B.x, B.a\u00f7B.y, B.b\u00f7A.c 3: B + b B.x\u00f7B.a, B.y\u00f7B.b 4: B \u00f7 b b B.x\u00f7B.a, B.y\u00f72 \n (Q' q0' E, I, GOTO, PLAN), where Q = {ql, q2 ..... q14} and q0(0)=ql, q0il)=q3, q0(2)=q5, q0(3)=q7 \n' q0(4)=ql I E = {el, e2 .... ,e11} Z = {~, ii, i2, i3, i4} GOTO = PLAN = ii i2 i3 i4 e SlS2...s n ql \nel q2 -- el A.c\u00f71; S.r\u00f7A.z VISIT(I,il); q3 -- e2 ...... e2 B.a\u00f7A.c; VISIT(2,i2); q4 -- B.b\u00f7B.x; A.z\u00f7B.y \nVISIT(2,i4); q5 -- e3 .... ~ -- e3 B.b\u00f7A.c; VISIT(3, i3); q6 -- B.a\u00f7B.y; A.z\u00f7B.x VISIT(3,i4); q7 -- -- \ne4 e6 -- e4 B. x\u00f7B. a q8 -- ...... e5 e5 B.y\u00f7B.b q9 -- ...... e7 e6 B. y\u00f7B. b qlO -- e7 B. x\u00f7B. a qll \n-- -- e8 elO -- e8 B.x\u00f7B.a; B.y\u00f72 q12 -- ...... e9 e9 skip q13 -- ...... ell elO B.y\u00f72 q14 -- el 1 B.x\u00f7B.a \n q2 q4 q6 q8 qlO q9 qlO q12 q14 q13 ,q14 Figure 3. 37  p applies at the node and A is the set of attributes \nwhich are known to be available. An evaluation state is the minimum amount of information about a node's \nsituation with which we can carry out dependency ana- lysis: we must know what production applies \nin order to know what semantic functions are involved, and we must know which attri- butes are available \nin order to decide whether a function can be evaluated yet. The elements of a treewalk evaluator's \nsets Q and E will be evaluation states. If a node is flagged with qEQ, this will mean that q=(p,A) described \nthe node's situation at the end of the last visit to it. If upon arriving at a node, GOTO-table look-up \nyields the entry state e~E, this will mean that e=(p,A') describes the node's situation after taking \ninto account any new inherited attributes made available since the last visit. Requiring that the eval- \nuator's action at a node be determined only by the evaluation state there (as found by GOTO-table look-up) \nintroduces our assumption that the structure of a node's subtrees is irrelevant, since this infor- mation \nis not encoded in the evaluation states. If we are to pick a next instruction based on the current (imagined) \nevaluation state and to find the new evaluation state resulting from its execution, we must be able to \ncharacterize each instruction in terms of the attribute occurrences it ref- erences and those it defines. \nWe discuss this matter next. Dependency characterization of instructions. For planning purposes, an \ninstruction is characterized completely by the depen- dency relations between the attributes it uses \nand the attributes it computes. For a semantic function this is specified by the attribute occurrence \nwhich it defines and the set of occurrences which it references. We denote the set of occurrences used \nin fP by D~a,k ) For a VISIT instruction (a,k)  the question is more complicated. The sub- trees of \na node may be viewed as semantic functions of a sort; given some inherited attributes, a son may be visited \nto yield some synthesized attributes. Unlike the genuine semantic functions, a subtree may be capable \nof \"partial evaluation\"; that is, not every output has to depend on every input. It may be possible to \nvisit a sub- tree with only some of its \"inputs\" avail- able and return with some of its \"outputs\" computed. \nA subtree may be visited several times before its evaluation is complete. The dependency graph of a \nproduction pcP, denoted DGp, is a convenient repre- nodes of DGp are the attribute occurrences of p; \nthere is a directed arc from (a',k') P to (a,k) if (a',k') ~ D(a , i.e. if ,k) (a',k') is used in the \nevaluation of (a,k). The arcs of such a dependency graph may be thought of as data flow paths. Figure \n4 shows two ways of drawing the dependency graph for a rule from our example grammar; the more elaborate \nform is intended to suggest a portion of a parse tree and will be used in the remainder of the paper \nto depict data flow up and down the tree. If we have a parse tree of an attri- bute grammar, we can \nconstruct a dependency graph which represents all data flow paths in the tree. This graph is the result \nof \"pasting together\" copies of the DG's for P  productions occurring in the tree, as illus- trated \nin Figure 5. If a subtree is detached from the rest of the parse tree, some data flow paths through the \nroot of the subtree will be interrupted. Data flows into the subtree through inherited attri- butes of'the \nroot, and out of the subtree through synthesized attributes of the root. For this reason we refer to \nthese attributes as input attributes and output attributes of the subtree. In a production, the input \noccurrences are those of the form (i,0) where i\u00a2I(X0), and the output occur- rences are those of the \nform (s,0) where seS(X0). The dependency graph of a subtree may imply that certain input attributes \nmust be made available before an output attribute can be evaluated. These relation- ships are called \nthe input-output depen- dencies of the subtree. (val,O) (pos,O) (val,1) (pos,1) (val,2) (pos,2) L \n0 caI I pos / L I s  sentation of the constraints imposed locally by the semantic rules for p. The \nFigure 4. 38 I Fi@ure 5.  We wish to associate with each nonter- minal X of the grammar an \"i/o graph\" \nIO x which will describe the input-output depen- dencies of subtrees with root X. The graph iOXk will \nthen serve as the dependency characterization of VISIT(k,I) instructions. In effect, the i/o graphs \nare input-output assertions specifying the behavior of the EVALUATE routine; we are guaranteed that after \nexecution of a VISIT(k,I), an output attribute of the k-th son will be available provided that it depends \non (has arcs in IOxk from) only input attributes that were available at the time of the call. Unfortunately, \ndifferent subtrees with the same root symbol may nevertheless exhibit different input-output dependencies, \nso we cannot hope to characterize a non- terminal precisely by means of a single graph. Instead, our \nIOx's will present a sort of worst-case picture of potential input-output dependencies. In order to compute \nthe graphs IO x for each XeVN, we first show the apparent input-output depen- dencies for a node of type \np are related to the dependency graph for production p and the i/o graphs of symbols occurring in p's \nright side. This allows us to establish a recursive definition of the IOx'S, which ity test for attribute \ngrammars.  Recall that for each pcP there is a directed graph DGp, the dependency graph for production \np. Suppose that for k=l,2,...np we have a directed graph G k whose nodes are a subset of A(Xk). Then \nlet DGp[Gi,G2,...Gnp] be the directed graph obtained from DGp by adding an arc from attribute occurrence \n(a,k) to occurrence (a',k) whenever there is an arc from a to a' in G k. Figure 6 illustrates this idea. \nDefinition: The augmented dependency graph DG~ for a production P:X0+XlX2. .Xnp is the graph DGp[IOxi,IOx2 \n.... IOXnp], where IOxk is taken to be the empty graph if X k E V T. The graph DG~ is the dependency \ngraph for DGp= a b a b x y x y DGp[Gi,G2 ]= lalblxlYl x0 S X 2 Ial bl  leads in turn to an iterative \nalgorithm for finding them. What follows is adapted from Knuth's [Knl] original, erroneous circular- \n Fi@ure 6. 39 production p extended to include potential dependency chains through subtrees. It shows \nboth the dependency constraints assoc- iated with the semantic functions of p and those associated \nwith the use of the EVAL- UATE routine in VISIT instructions. Now let X 0 be the left side of pro- \nduction p. Any path in DG~ from an input of X 0 to an output of X 0 represents an apparent input-output \ndependency of X 0 and hence must be represented by an arc in IOx0. Moreover, the IOx's should be the \nsmallest graphs satisfying this requirement, since there is no other way for an input-output dependency \nto appear except through a chain of instruction dependencies. We may there- fore make the following \nrecursive definition. Definition: The set of i/o graphs is a set {IO x} of directed graphs indexed \nby V N satisfying (a) the nodes of the graph IO x are the attributes A(X), and  (b) there is an arc \nfrom i to s in IO x  iff there is a path from (i,0) to (s,0) in the graph DG~ for some production \np whose left side is X. The following method may now be used to compute the set of i/o graphs. Initially \nlet each IO x have nodes A(X) and no arcs. Then repeat until no more arcs can be added to any IOx: \nif there is a production p with left side X such that DGp has a path from (i,0) to (s,0) but IO x has \nno arc from i to s, add the missing arc to IO x. The algo- rithm must terminate since there are only \na finite number of arcs possible at all. It is clear that the resulting graphs satisfy the definition \nabove; (a) is trivial, the \"if\" part of (b) follows from the algorithm's termination condition, and the \n\"only if\" part of (b) is true because only such arcs are ever added to any IO x. The arcs in IO x reflect \nall the actual input-output dependencies that could exist in a subtree with root X, so we can safely \nuse them during planning to predict the effect of visiting a subtree: all promised outputs can in fact \nbe produced. However, not every arc necessarily represents a pos- sible input-outpu t dependency; the \nIOx'S may be overpessimistic, asserting i/o depen- dencies that no subtree could actually exhibit. The \nexample computation of Figure 7 illustrates this. The reason is that an IO x attempts to represent in \na single graph all the i/o relationships that can be exhibited by any subtree with a given root symbol, \neven though different such subtrees may exhibit different input-output dependencies. Two arcs in an IO \nx may be \"contributed\" by two different sub~rees so that no actual subtree could exhibit both dependencies. \nLater in the computation of the i/o graphs, some DG D may have a path from (i,0) to (s,0) which ihcludes \nboth of these arcs; the re- sult will be to add an arc from i to s in the i/o graph of p's left side \nnonterminal even though no such dependency could ever be found in a semantic tree. The information loss \ndue to merging all a nonterminal's i/o relationships into a single graph may prevent evaluator construc- \ntion by making it seem that evaluation is deadlocked in certain situations when it is not. For a wide \nclass of attribute grammars this does not happen. Definition: An attribute grammar is absolutely noncircular \nif no graph DGp contains a directed cycle. An absolutely noncircular grammar contains no circular dependencies \nin its semantics even assuming that the IOx'S must be obeyed. A grammar can fail to be absolutely non- \ncircular either because it is actually circular or because the IOx'S are too pessi- mistic. The Plannin@ \nAlgorithm. A plan for a given entry state is an instruction sequence which will accomplish all attribute \nevaluation in the subtree which is permitted by the dependency con- straints. The algorithm described \nbelow makes use of the IOx's discussed above, and assumes that the attribute grammar is absolutely noncircular. \n Definition: The semantic function fP (a,k) is ready to evaluate in the current evaluation state (p,A) \nif (a,k) ~ A but D p (a,k) = A. Definition: The yield of the k-th subtree in the current evaluation \nstate (p,A) is the following set of attribute occurrences: {(a,k) l (a,k) ~ A, a ~ S(Xk), and for every \ni with an arc from i to a ia IOXk, (i,k) \u00a2 A}  40 0 H -r-I X x o 0 I\" .Cl 0 o N 0 ! r~ 0 >0 0-r.I xO \n~0 o r~ H ~0 ~0 0 x 0 ,el ,1, r-I l~OJ r--I ~JO O,Cl 0 ~o u~ -,-i i-i ~-,-i o C; I:I -,~ I:::: 0 \n0 4J r~ OJ U~ 0 -~.1 0 0 0 X ,.Q ,.Q ~ r~ II II ,-I r~ N x + r~ r-I .Q II 00 o r~ o I=I II 41  That \nis, a visit to the k-th son will yield those outputs of X k which were not already available and which \ndepend (in IOXk) on only available inputs. Algorithm Make a plan for entry state (p,A) : 1. [Initialize] \nLet S be the empty sequence of instructions, and let A' = A.  2. [Repeat] The current evaluation state \nis (p,A').  3. [Evaluate?] If some semantic func-  tion f~a,k) is ready to evaluate, then append \nthe instruction fP (a,k) to S, add (a,k) to A', and go to Step 2. 4. [Descend?] If there is a k-th \nsub- tree whose yield Y is nonempty then: append the instruction VISIT(k,I) to S, where I = { (a,0) \nI a ~ I(X k) and (a,k) ~ A'}; let A' = A' u Y; and go to Step 2. 5. [Done] Let q be the evaluation \nstate (p,A'). The algorithm is complete and the resulting plan is (S,q).  end The heuristic used is \nto evaluate all pos- sible semantic functions before visiting any subtrees. This makes as many inputs \nas possible available so that a lot of evalu- ation of the visited subtree can be done, which tends to \nreduce the number of visits which must be made to each node. We do not consider the question of which \nsubtree it is best to visit if more than one have non- empty yields. Note that although semantic functions \nare evaluated only once, a plan may contain several visits to the same subtree. We note that the current \nevaluation state (p,A') at a step in the simulation does not correspond precisely to the situ- ation \nthat will exist at that point in the execution of the plan at run-time. In any particular execution at \na node, there may be some attrfbutes available which are not in A'; these would be outputs of subtrees \nwhich were not guaranteed to be in the yield. Since a particular subtree may not exhibit all the dependencies \nin its IOx, sometimes a VISIT instruction will result in evalu- ating some \"extra\" attributes. The only \neffect of this'is that some later VISIT which does promise the attributes will not have as much to do; \nthe selection of a plan at the subnode will avoid duplicate work. 5. MAKING A COMPLETE SET OF PLANS \n When a VISIT(k,I) instruction is exe- cuted, the recursive call to EVALUATE will result in the look-up \nGOTO(q,I)=e, where q is the state flag found on the k-th son. The process of constructing a treewalk \nevaluator involves finding all (q,I) pairs that will ever be used to access the GOTO- table and making \nall the relevant entries in GOTO and PLAN. We first introduce a mechanism to keep track of the order \nin which plans may be executed at a node, then show how it may be used to determine whether a given VISIT \ninstruction can cause the look-up of a GOTO-table entry not yet in- cluded in our partially constructed \nevalu- ator. Finally we present the evaluator construction in abstract form and sketch how it could be \nimplemented more efficiently. The history graph. Definition: A history graph is a directed graph whose \nnodes are labelled with evaluation states, such that (a) the nodes are partitioned into two disjoint \nsets called the q-nodes and the e-nodes;  (b) an arc from a q-node to an e-node is called an arrival \narc, and is labelled with a current-input-set;  (c) an arc from an e-node to a q-node is called a visit \narc, and is labelled with a sequence of in- structions;  (d) there are no other kinds of arcs.   A \npath through the history graph represents a sequence of events which could happen during the evaluation \nof a node. An arrival arc represents the arrival of the evaluator at the node, while a visit arc represents \nthe execution of a plan at the node. Thus SlS2\"''Sn~~  depicts what happens at the k-th son when a \nVISIT(k,I) is executed. The son is initially flagged with quiescent state q. The evalu- ator arrives \nbringing the current-input-set I and finds that the corresponding entry state is e. It then executes \nthe plan for e, which consists of the instructions Sl, s2, ... s n followed by setting the son's flag \nto quiescent state q'. The history graph of an evaluator contains the same in- formation as its formal \ndefinition in terms of GOTO and PLAN. Each arrival arc q I e 42  corresponds to the GOTO-table entry \nGOTO(q,I)=e, and each visit arc SlS2...sn e ~ c' corresponds to the PLAN entry PLAN(e)=(SlS2...Sn,q'). \n A history graph is not in general a connected graph; it will have one connected component for each production \nin the gram- mar. The component corresponding to pep will contain all nodes with labels of the form (p,A), \nand represent all possible sequences of events at a node where p applies. The history graph is useful \nbecause it makes explicit the order in which things can happen at a node, even across the boun- daries \nof single plans. If we take any path through the history graph, we can get a corresponding instruction \nsequence by concatenating, in order, the labels of the included visit arcs. Instruction occurrence i \nI precedes occurrence i 2 if there is a path whose corresponding instruction sequence includes i I and \ni2, and i I is to the left of i 2 in that sequence. Note that this is only a partial order. New GOTO-table \nentries. We are concerned with whether a given VISIT(k,I) instruction can cause the look- up of aGOTO-table \nentry not yet present in the evaluator being constructed. The entry looked up will depend on the state \nq in which it finds the k-th son and on I, the current-input-set. From q and I we can com- pute what \nthe looked-up entry state should be as follows: GOTO(q,I) = (p,A') where A' = A u I and q = (p,A)  \nThat is, the corresponding entry state is just the old quiescent state augmented with any newly available \ninput occurrences. We see that the GOTO-table is merely a device to avoid the explicit use of attribute \nsets and set union at run-time by saving the answers to those computations that will be needed. Definition: \nThe GOTO-table entry ((q,I),e) is required b~ an occurrence of a VISIT(k,I) instruction if the k-th \n subtree could be flagged with state q at that instruction, and e is the entry state corresponding \nto q and I.  This is the definition we need to find new GOTO-table entries; any entry required by \nan existing VISIT instruction must be added if not already present. We still have to show how to determine \nwhat states a subtree might be flagged with at a given point in the execution of a plan. At any point \nin a plan, a subtree must be flagged with the state left by the most recent visit to it, or with its \ninitial state if it hasn't been visited yet. At an instruction in the history graph, a \"most recent\" \nvisit to the k-th subtree is a VISIT(k,I) which precedes the instruction such that there are no other \nVISIT(k,I') instructions intervening. There may be sev- eral most recent visits, since they may lie \non different paths to the point in question; there may also be no most recent visit at all. If there \nis a maximal path to the instruction such that there is no VISIT(k,I) at all, we say that the \"initial \nvisit\" is most recent. Note that it is possible for both the \"initial visit\" and some VISIT instructions \nto be most recent at the same point. The state that the k-th son is flagged with after execution of \na VISIT(k,I) in- struction is uniquely determined by I and the production which applies at the son. The \nreason for this lies in the way plans are constructed: a plan performs all attri- bute evaluations permitted \nby the dependency constraints, and the amount of evaluation that can be done is limited only by the availability \nof inputs to the subtree. Thus the set of attributes available at the end of a visit is determined only \nby the avail- able inputs and not by the details of pre- vious visits. Definition: The quiescent state \ncorresponding to production p and current-input-set I, denoted Q(p,I), is q, where (SlS2...Sn, q) \nis the result of making a plan for the entry state (p,I).  Q(p,I) is the evaluation state in which \nall attributes of a type-p node are available which can be computed with input set I. To sum up, we \nsaw that in order to find the GOTO-table entries required by a VISIT(k,I) instruction we needed to know \nwhat states the k-th son might be flagged with at that point. We observed that it could be in any state \nthat a \"most recent\" visit could have left it in. The most-recent visits can be found with the help of \nthe history graph, and we can compute what states they might leave the son flagged with. Specifically, \nat a given point the k-th son can be flagged with state q if: (a) q = Q(p,I) where p[0] = X k and some \nVISIT(k,I) is \"most recent\", or (b) q = q0(p) where p[0!, = X k and the \"initial visit\" is most recent\". \n Evaluator construction. We now present \u00a3he treewalk evaluator construction algorithm. The main data \nstructure is the history graph discussed above. Initially the history graph consists of one q-node per \nproduction, labelled with 43  the initial state for that production, and no arcs. The functions GOTO \nand PLAN, considered as sets, are empty. A first GOTO-table entry is made corresponding to the evaluator's \nvisit to the root of the semantic tree; since the start symbol has no inherited attributes, the current-input- \nset is ~. After that, an iterative process of making plans and finding new GOTO-table entries is carried \nout until no further additions to the evaluator can be made. Definition: The initial state for peP, \n denoted q0(p), is the state (p,A) where A = { (a,k) I XkeV T and a~A(Xk)}.  In an initial state, only \nthose attributes are available which are given in advance, the (synthesized) attributes of terminals. \n A196rithm Construct a treewalk evaluator: i. [Initialize] Let the history graph consist of a q-node \nfor each pep labelled with q0(p). Let GOTO and PLAN be empty. 2. [First GOTO-table entry] Add an arc \nlabelled ~ from the q-node labelled q0(0) to a new e-node with the same label. Let GOTO(q0(0)) = q0(0). \n 3. [New GOTO-table entry] If some in- struction VISIT(k,I) requires a GOTO- table entry ((q,I),e) not \nalready present, then:  3.1. Add an arc labelled I from the q-node labelled q to the e-node labelled \ne (add the nodes if necessary). 3.2. Let GOTO(q,I) = e. 3.3. Go to Step 3.  4. [New plan] If there \nis an e-node labelled e with no arc leaving it, then:  4.1. Let (SlS2...Sn,q) be the result of making \na plan for e. Add an arc labelled SlS2...sn from the e-node labelled e to the q-node labelled q (add \nthe q-node if necessary). 4.2. Let PLAN(e) = (SlS2...Sn,q). 4.3. Go to Step 3. 5. [Done] Let Q be \nthe set of all evalu- ation states labelling q-nodes. Let E be the set of all evaluation states labelling \ne-nodes. Let I be the set of all labels of arrival arcs. The constructed evaluator is  (Q' q0' E, I, \nGOTO, PLAN). end The algorithm must terminate because each step in the loop adds to the history graph, \nyet the potential size of the history graph is finite. There are a finite number of evaluation states, \nand hence of nodes in the graph (no two q-nodes or two e-nodes have the same label). Each e-node has \nonly one out-arc, and for each q-node only a finite number of distinct labels (subsets of a production's \nset of attribute occur- rences) are available for out-arcs. Implementin9 the construction. The construction \nalgorithm above is straightforward, but somewhat extravagant if programmed directly. The test in Step \n3, if taken literally, involves recomputing all GOTO-table entries required by each VISIT instruction \nof the history graph on every pass through the loop. In the re- mainder of this section we describe an \nop- timized version of the construction which involves less work. Our purpose is not to propose a \"best\" \nalgorithm, but merely to indicate that reasonable implementations of the construction can be made. Our \napproach is to avoid recomputation by introducing some bookkeeping variables and updating them when necessary. \nWe assoc- iate with each node n a set STATES(n) con- taining information about what states each subtree \ncould be flagged with at that node. Each element of STATES(n) is a pair (k,q) where k is the number of \na son and q is a state the k-th son could be flagged with. We will call these elements infrastates. When \nthe history graph is initialized, each initial q-node has its STATES set initial- ized to show that each \nson must be flagged with an initial state; that is, if qn is an initial q-node labelled with state q=q0(p), \nthen STATES(qn) = { (k,q0(p')) l isksnp and p'[0]=X k }  initially. Moreover, we associate with each \ne-node en a set NEW(en) of infrastates. Elements of a NEW set are infrastates that have been discovered \nbut not yet used to generate new GOTO-table entries. During evaluator con- struction the only VISITs \nthat may require a nonexistent GOTO-table entry are VISITs belonging to an e-node with a nonempty NEW \nset;thus these sets help organize the work and avoid repeated examination of the en- tire history graph. \n The NEW and STATES sets must be kept up to date when the history graph is added to. A newly created \nnode is given an empty STATES set; a newly created e-node also gets an empty NEW set. When an arc is \nadded from a q-node qn to an e-node en, thus completing a new path to en and per- haps making a new VISIT \ninstruction \"most 44  recent\", any previously known infrastates will be saved in NEW(en) for later \nproces- sing. Our first sub-algorithm adds a seg- ment to the history graph and performs the needed \nupdate. Al@orithm Make the GOTO entry ((q,I),e): i. Add the entry ((q,I),e) to the GOTO-table. 2. \nLet qn be the q-node labelled q and en the e-node labelled e. Add an arc labelled I from qn to en, creat- \ning the nodes if necessary. Let NEW(en) = NEW(en) u (STATES(qn) -  STATES(en) ).  3. Let (SlS2...Sn,q') \nbe the result of  making a plan for e. Add an arc labelled SlS2...s n from en to the  q-node labelled \nq'; add the q-node if necessary. end The central activity of this version of the construction is \"considering\" \nan e-node whose NEW set is nonempty. Each VISIT instruction of the node's plan is ex- amined in sequence \nto see if assuming a son is flagged with a NEW state will result in \"requiring\" a new GOTO-table entry. \nAddi- tions to the history graph may be made, as well as new infrastates to be propagated along: a new \npossible flag for a son at the beginning of a plan may imply a new possi- bility at its end. New possibilities \nat a q-node are not \"buffered\" in a NEW set there but are passed immediately to suc- cessor e-nodes for \nholding. Algorithm Consider the e-node en: 1. Let CURRENT = NEW(en). Let STATES (en) = STATES (en) \nu NEW(en) . Let NEW(en) = ~.  2. Let e be the label of en. For each VISIT(k,I) in PLAN(e) in order: \n  2.1. For each state q such that (k,q) ~ CURRENT, make the GOTO entry ((q,I),e) if it is not already \npresent. 2.2. Delete all pairs (k,q) from CURRENT. Add the pairs { (k,Q(p,I)) I p[0]=Xk}. 3. Let q' \nbe such that PLAN(e) = (SlS2...Sn,q'). Let qn' be the q-node labelled q'. Then let DIFF = CURRENT -STATES(qn'), \nand STATES(qn') = STATES(qn') u DIFF. 4. If DIFF ~ ~, then for each e-node en' with an arc from qn' \nto en', let NEW(en') = NEW(en') u (DIFF -STATES(en')) .  end Steps 3 and 4 of this algorithm propa- \ngate only the \"interesting\" --i.e. new -- infrastates along for further processing later. There are, \nof course, many ways to improve the algorithm. The actual evaluator construction algo- rithm, version \ntwo, follows. Algorithm Construct a treewalk evaluator: i. [Initialize] Let the history graph consist \nof a q-node for each pEP labelled with q0(p). Let STATES(qn) be empty for each q-node qn. Let GOTO and \nPLAN be empty. 2. [First segment] Make the GOTO entry ((q0(0) ,~) 'q0 (0)) .  3. [New segment] If NEW(en) \n= ~ for every e-node en, go to Step 4. Otherwise: let en be an e-node with NEW(en) ~ @; consider en; \nand go to Step 3.  4. [Done] Let Q be the set of all evalu- ation states labelling q-nodes. Let E be \nthe set of all evaluation states labelling e-nodes. Let I be the set of all labels of arrival arcs. Then \nthe constructed evaluator is  (Q' q0' E, I, GOTO, PLAN).  end 6. COMPARISON WITH OTHER METHODS  \nThree authors have studied the problem of attribute grammar evaluation. The first, Fang, implemented \na general system based on parallel processes. Bochmann proposed eval- uating certain attribute grammars \nin a fixed number of left-to-right passes over the tree, and Jazayeri suggested extending this idea to \na fixed number of alternately left- right, then right-left passes. Fang's system [F] was a more or less \ndirect imple- mentation of the defining evaluator; it could evaluate any attribute grammar, but it was \nnot a practical method of evaluation. As Fang himself noted, \"a nondeterministic approach such as this \nis bound to be inher- ently less efficient than deterministic approaches\" Bochmann was the first to \ndescribe such an approach [B1]. He began by considering the possibility of evaluating an attribute grammar \nin a single left to right pass through the semantic tree. A recursive 45 routine, evaluate-subtree, \ntraversed the tree in depth-first left to right order performing evaluation of attributes. To ensure \nthat all the semantic functions of a node could be executed it was necessary to compute all of the node's \ninherited attri- butes before visiting it. Evaluate-subtree thus performed the following algorithm: for \neach subtree in left to right order, first evaluate all its inherited attributes, then call evaluate-subtree \nrecursively on it to finish its evaluation; finally, compute the current node's synthesized attributes \nand return. Not every attribute grammar can be evaluated this way. Bochmann stated the condition that \nthe semantic functions' dependencies must satisfy: no inherited attribute of a son can depend on any \nof its  synthesized attributes or on any attributes of sons to its right. This represented a very strong \ncondition on the attribute grammar. To increase the number of attribute grammars that could be evaluated, \nBochmann proposed to allow the evaluation to occur in several left to right passes. On each pass, all \nattributes evaluated on previous passes could be used. He gave an algorithm to determine whether an attribute \ngrammar could be evaluated in this way and to find the attributes which could be evaluated on each pass. \nThe iterative algorithm assumed initially that all remaining attributes could be evaluated on the next \npass. The one-pass dependency condition was checked; if any attribute failed the check (could not be \nevaluated) it was eliminated from the current pass and the condition checked again. If the condition \nwas satisfied, all attributes not eliminated were assigned to the current pass, and the process was re- \npeated. The algorithm terminated when all attributes had been assigned to a pass or when it was found \nthat the next pass could not evaluate any of the remaining attri- butes. In the latter case the attribute \ngrammar could not be evaluated in a fixed number of left to right passes. Jazayeri [Jl] suggested a \nmodification of Bochmann's method. Observing with Fang that \"the left-to-right bias is not all- pervasive\" \nin programming language seman- tics, he described an Alternating Semantic Evaluator that made every other \npass from right to left. Jazayeri showed that cer- tain left-recursive situations could be evaluated \nin a single right-to-left pass even though no fixed number of left-to- right passes was sufficient. However, \nthis proposal did not remedy the difficulty with evaluation in passes, as we shall see below. The major \ndifference between this work and that of Bochmann and Jazayeri is in the methods used to handle partial \neval- uation of tree nodes. The difference is a consequence of choosing an evaluation order by examining \ndependencies rather than fix- ing the order in advance (e.g. left to  right). In what follows we will \ntry to pro- vide some insight into how the two methods really work and how the resulting evaluators \n differ. Partial evaluation occurs when only some of a node's attributes are evaluated during a visit, \nso that several visits are required to complete the node's evaluation. In syntax-directed translation \ninvolving only synthesized attributes, information flows only up toward the root of the tree. Attributes \nof a node can depend only on attributes of its subtrees, and consequently any node can be completely \nevaluated in one visit. The situation is similar if only inherited attributes are involved. When both \nkinds of attributes are present, though, there can be information flow both into and out of a subtree, \nand the semantics may cause these inputs and outputs to interact. An input to a subtree may depend on \none of its outputs, requiring at least two visits to the node: a first one to make the output available, \nand then, when the input has been computed, a second visit to make use of it. Partial evaluation can \nbe much more compli- cated than this, since the interaction be- tween inputs and outputs of a subtree \nmay depend on the context in which it occurs, and the inputs needed to produce an output may vary with \nthe details of the subtree's structure. Evaluation in passes, the strategy used by Bochmann and Jazayeri, \nhandles this problem by reducing it to a sequence of one- pass evaluations. The reductionstrategy is \na natural consequence of starting out with a traversal order given in advance. The attri- bute grammar \nis checked for evaluability in the given order, and if it fails it is sim- plified by deleting the offending \nattribute and its semantics. If repeated simplifica- tion yields a one-pass grammar, then the first pass \nhas been constructed. All pass-i attributes are now treated as constants, and the original grammar so \nreduced is submitted again to this process. If successful, this construction results in the decomposition \nof the original attribute grammar in such a way that each sub-grammar is evaluatable in a single pass \nprovided that attributes of pre- vious passes are treated as constants. The resulting evaluator visits \neach node several times, but in a highly con- strained fashion. Each node of the tree is visited once \nbefore any is visited a second time; each gets a second visit before any gets a third; and so on in lockstep. \nEval- uation is done uniformly throughout the tree; all occurrences of X.a must be evaluated \"at the \nsame time\", on one sweep through the tree. Evaluation in passes runs into trouble when \"nested passes\" \nare required, as in the example of Figure 8. The attribute grammar shown there is more easily understood \nif one knows that, for B nodes, the first visit brings 8.a and yields B.x, while the second visit brings \nB.b and yields B.y. Any sem- antic tree of this grammar can be evaluated 46 by making two visits to \neach B node. The catch is in the semantics for rule 2: because of the definition Bl.a \u00f7 Bo.b + 1 the \nfirst visit to the son (B I) cannot be made\" u~l during the second visit to the father (B0), since B0.b \n~ not available until then. Neither-Bochmann's nor Jaza- yeri's evaluators can handle this grammar, because \nthere is no attribute of 8 all of whose occurrences can always be evaluated in the first pass. In our \nconstruction we treat partial evaluation directly instead of reducing the attribute grammar to eliminate \nit. Just as evaluation in passes is a natural outgrowth of a priori traversal order, our direct approach \nnaturally arises from the idea of choosing a traversal order by exam- ining dependencies. We view the \nsubtrees of a node as semantic functions of a sort; unlike ordinary semantic functions they may be partially \nevaluated, yielding some of their outputs when some of their inputs are available. We use the i/o graphs \nof non- terminals to schedule visits to subtrees just as we use the dependency sets of sem- antic functions \nto schedule their executions. A natural consequence of this is that a plan may contain more than one \ninstruction to visit the same subtree; that is, a visit to a node may result in several visits to a subnode. \nThis is just what is required to deal with \"nested passes\", and our construc- tion has no trouble with \nthe example above. In short, the method of evaluation in passes treats partial evalu ation by reducing \nthe attribute grammar to a sequence of simpler ones that can each be evaluated in a single pass. Our \nmethod, on the other hand, determines the conditions for partial evaluation in a set of i/o graphs, then \nproceeds with planning as if the subtrees were special semantic functions. The result is that our evaluators \nare able to move around in the tree in a more general way and can therefore handle a larger class of \nattribute grammars.  Ir=13 J a = 1 B b = 3 x = 2 y = 12 a = 4 b b = 6 x = 5 y = ]i b b b parse trge \nB a = 7 b dep. graph b = 9 x = 8 Production Semant los i:A}B B.a+l, B.b.~B.x+l, A.r~B.y+l y = i0 2 :]90->B1 \nb B0.x+B0.a+l, B l.b~B l.x+l, Bl.a~ B0.b+l, B 0.y+B l.y+l b 3 : B-~b B. x~-B. a+i, B.y\u00f7B.b+l semantic \ntree after evaluation Fi@ure 8. 47 7. SUMMARY AND CONCLUSIONS  We are interested in the automatic \nimplementation of attribute grammars be- cause of their suitability for specifying the semantic actions \nof compilers. The translation from source language to target language may be described by an attribute \ngrammar in a natural, modular, and nonpro- cedural way just as context-free grammars are natural for \nspecification of syntactic analysis. We present a method for con- verting the nonprocedural semantic \ndescrip- tion into an efficient procedure for per- forming the translation, which should be useful in \na practical compiler-writing system. Three main ideas are the basis for the method proposed in this \npaper. The first is the choice of generalized tree traversal algorithms as a framework for constructing \nevaluators. A treewalk evaluator is a recursive routine receiving as a parameter a tree node to visit; \nwhile at a node it may evaluate semantic functions or call it- self recursively to visit sons. Such a \nsequential evaluator avoids the expense of simulating parallelism on today's computers. The second idea \nis using dependency analysis to choose evaluation orders. The body of a treewalk evaluator is constructed \nby sched- uling attribute evaluations in an order which will satisfy the dependency con- straints when \nexecuted, enabling the eval- uator to do its work without considering data dependencies at run time. \nEliminating the overhead of a run-time scheduler greatly improves the efficiency of evaluation. The power \nof this approach is enhanced by the third main idea: the use of i/o graphs of nonterminals to schedule \nvisits to sub- trees. The sons of a node are treated as special semantic functions which may be partially \nevaluated to yield certain output attributes when only some of the son's in- put attributes areavailable. \nWith the relationship between each nonterminal's inputs and outputs being given by its i/o graph, the \nprocess of dependency analysis is able to determine at which points recur- sive calls should be made. \nThis built-in knowledge of dependency constraints allows the evaluator to always go directly to the next \npoint of evaluation, so that nondeter- minism or searching through the semantic tree is necessary. The \nprinciple of our construction is to make a plan of action for every situa- tion thatcan arise during \nevaluation. The notion of evaluation state is intro- duced to model the relevant aspects of the situation \nat a node. The planning algo- rithm performs dependency analysis in order to find a sequence of instructions \nthat may be executed upon encountering a given situ- ation or entry=state. Evaluators are con- structed \nby an iterative process which re- peatedly discovers new situations which can arise and makes plans to \ndeal with them. At the same time a GOTO-table is built up which the completed evaluator will use to \nselect plans for execution. The main data structure during construction is the his- tory graph, which \ngives all possible evalu- ation sequences at a node in the form of a flowchart. The resulting evaluators \nmay easily be generated directly in machine code. The construction given here will work for attribute \ngrammars which are absolutely noncircular; for a grammar in this class, an evaluator's action at a node \nneed not depend on the structure of the node's sub- trees. The relaxation of this restriction to include \n\"look-down\" into subtrees is discussed in [Wa] and will be the subject of a future paper. REFERENCES \n [A] Aho,A.V. Indexed grammars --an exten- sion of context-free grammars. IEEE Conf. Record of 8th Annual \nSymp. on Switching and Automata Theory. Austin, Texas, October, 1967. [AUi] Aho, A.V. and Ullman, J.D. \nProperties of syntax-directed translations. J. Computer Systems Sci. v.3, pp.319- 334 (1965).  [AU2] \nAho, A.V. and Ullman, J.D. The Theory of Parsin@, Translation, and Compilin@, v.2, Prentice-Hall, Englewood \nCliffs, N.J. (1973).  [Bi] Bochmann, G.V. Semantics evaluated from left to right. Publ. No. 135, Departemente \nd'Informatique, Universite de Montreal (June 1973). [B2] Bochmann, G.V. Semantic equivalence of syntactically \nrelated attribute grammars. Publ. No. 148, Departemente d'Informatique, Universite de Montreal (November \n1973).  [cu] Culik, K. Attributed grammars and languages. Publ. No. 3, Departemente d'Informatique, \nUniversite de Montreal (May 1969).  [D] Dreisbach, T.A. A declarative semantic definition of PL360. \nUCLA-ENG-7289, Computer Science Department, UCLA (October 1972).  [F] Fang, I. FOLDS, a declarative \nformal language definition system. STAN-72- 329, Comp. Sci. Dept., Stanford Univ. (December 1972). \n [GH] Greibach, S. and Hopcroft, J. Scattered context grammars. Scientific Report, System Development \nCorp., Santa Monica, California (1967). [I] Irons, E.T. Towards more versatile mechanical translators. \nProc. Symp. on Applied Math., v.15, pp.41-50 (1963). [Jl] Jazayeri, M. On attribute grammars and the \nsemantic specification of program- ming languages. Ph.D. Thesis, Comp. and Inf. Sci. Dept., Case Western \nReserve University (October 1974). [J2] Jazayeri, M. Live variable analysis, attribute grammars, and \nprogram opti- mization, draft. Dept. of Comp. Sci., Univ. of N. Carolina, Chapel Hill, N.C. (March 1975). \n [Knl] Knuth, D.E. Semantics of context-free languages. Math. Systems Theory J., v.2, pp.127-145 (1968). \n [Kn2] Knuth, D.E. Semantics of context-free languages: Correction. Math. Systems Theory J., v.5, p.95 \n(1971). [Kn3] Knuth, D.E. The Art of Computer Pro- 9ramming, v.l, Addison-Wesley Publ., Menlo Park, \nCalifornia (1968). [LS] Lewis, P.M. and Stearns, R.E. Syntax directed transductions. JACM v.15, no.3, \npp.654-688 (July 1968). [NA] Neel, D. and Amirchahy, M. Removal of invariant statements from nested \nloops in a single effective compiler pass. SIGPLAN Notices, v.10, no.3, pp.87-96 (March 1975). [R] Rosenkrantz, \nD.J. Programmed grammars and classes of formal languages. JACM v.16, no.l, pp.107-131 (January 1969). \n[SL] Stearns, R.E. and Lewis, P.M. Property grammars and table machines. Infor- mation and Control v.14, \npp.524-549 (1969) [vW] van Wijngaarden, A. et. al. Report on the Algorithmic Language ALGOL 68. Numerische \nMathematik v.14, pp. 79-218 (1969). [Wa] Warren, S.K. The efficient evaluation of attribute grammars. \nM.A. Thesis, Dept. of Math. Sci., Rice Univerity, Houston, Texas (April 1975). [Wil] Wilner, W.T. A \ndeclarative semantic definition. Ph.D. Thesis, Comp. Sci. Dept., Stanford University (1971). [Wi2] Wilner, \nW.T. Formal semantic ~efin- ition using synthesized and inherited attributes. Formal Semantics of Pro- \n@ramming Languages (Rustin, ed.), Prentice-Hall (1972).  \n\t\t\t", "proc_id": "800168", "abstract": "<p>The translation process may be divided into a syntactic phase and a semantic phase. Context-free grammars can be used to describe the set of syntactically correct source texts in a formal yet intuitively appealing way, and many techniques are now known for automatically constructing parsers from given CF grammars. Knuth's attribute grammars offer the prospect of similarly automating the implementation of the semantic phase.</p> <p>An attribute grammar is an ordinary CF grammar extended to specify the &#8220;meaning&#8221; of each string in the language. Each grammar symbol has an associated set of &#8220;attributes:&#8221;, and each production rule is provided with corresponding semantic rules expressing the relationships between the attributes of symbols in the production. To find the meaning of a string, first we find its parse tree and then we determine the values of all the attributes of symbols in the tree.</p>", "authors": [{"name": "Ken Kennedy", "author_profile_id": "81100453545", "affiliation": "", "person_id": "PP40027435", "email_address": "", "orcid_id": ""}, {"name": "Scott K. Warren", "author_profile_id": "81100610839", "affiliation": "", "person_id": "P261236", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/800168.811538", "year": "1976", "article_id": "811538", "conference": "POPL", "title": "Automatic generation of efficient evaluators for attribute grammars", "url": "http://dl.acm.org/citation.cfm?id=811538"}