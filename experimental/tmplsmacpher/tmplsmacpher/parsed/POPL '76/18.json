{"article_publication_date": "01-01-1976", "fulltext": "\n VERIFYING FORMAL SPECIFICATIONS OF SYNCHRONOUS PROCESSES* Patricia P. Griffiths + and Charles J. Prenner \n\"H\" Center for Research in Computing Technology Harvard University Cambridge, Massachusetts 02138 ABSTRACT: \nSYNVER is an automatic programming system for the synthesis of solutions to problems of synchronization \namong concurrent processes from specifications written in a high level assertion language (SAL). The \ncorrectness of the solutions constructed by SYNVER follows from the soundness of the synthesizer itself \nand from a verification phase which is applied to the specifications. This verification phase is the \nmain topic of this paper. To provide context for the verification the paper includes a discussion of \nsynchronization problems and a brief overview of both the SYNVER system and the SAL specification language. \nA formal definition of the correctness of a SAL specification is then presented along with algorithms \nwhich may be used to determine if a given specification is correct. I. INTRODUCTION This paper describes \nsome of our recent work in the area of automatic programming. While this area has been defined in many \nways, we view it as a collection of tools, methods, and techniques for fast, inexpensive production of \nreliable software. Automatic programming attempts to put more of the burden of software production and \nreliability upon the machine or software production system, and less upon the programmer or user. These \nmotivations are similar to those that led to the development of high level programming languages as a \nreplacement for assembly languages. Then, as now, the desire is to produce software which does as much \nwork as possible for the programmer. We are specifically concerned with developing approaches and techniques \nthat advance toward the long-range goal of constructing general purpose automatic programming systems. \nHere, we begin work on some prerequisites of this goal through the construction of such a system for \na specific problem domain. The value of this effort is twofold. In addition to examining outlines and \ndirections for eventual more general systems, we have constructed an operational system for a non-trivial \nproblem domain. Thus, we present an existence proof that many of the problems can be simplified and clarified \nby a Judicious choice of problem domain and by careful system design. The problem domain chosen is the \nsynchronization of systems of concurrent processes. Conventional approaches have made the description \nand solution of such problems quite difficult. Our system, SYNVER, overcomes these difficulties by approaching \nsynchronization at a level close to a human conceptual model of such problems. The design and construction \nof the SYNVER system [Grif74,Grif75a] involves three areas of automatic programming: very high level \nlanguages, program synthesis, and program verification. Traditionally, the main efforts in automatic \nprogramming have been in the area of verification. Automatic or semi-automatic theorem-provers formulate \nand prove theorems about logical formulae which are assertions or verification conditions about programs. \nFrom the resulting proofs, programs can be extracted to satisfy the. theorems proven. Thus, synthesis \nof programs was originally based on verification. SYNVER approaches synthesis and verification as separate \nissues. A very high level specification language (SAL) is formulated in which synchronization problems \ncan be described in a high-level, formal, yet human-oriented way. (SAL is an extension of the extenslble \nlanguage ECL [Weg70][Weg74], using data, operator, and control extension facilities [Pren72]. SYNVER \nis itself written in ECL as well.) It is This research was supported in part by the Advanced Research \nProjects Agency under contract F19628-74-C-0083 and by IBM under an IBM fellowship grant. + Current \nAddress: IBM Research Laboratory, San Jose, California 95193 Current Address: Computer Science Division, \nUniversity of California at Berkeley, Berkeley, California 94720 192 this specification language together \nwith problem-domain-specific knowledge which provides sufficient information to facilitate the synthesis \nand verification of solutions to the described problems. The synthesis and verification can be done independently; \nsynthesis, therefore, is freed from its dependence on verification and from the limitations of theorem \nprovers. In this paper we will be concerned primarily with the verification phase of SYNVER. However \nwe must first provide both context and motivation for the verification techniques. In Sections 2 and \n3 we discuss the nature of synchronization problems and previous techniques for their description and \nsolution. Sections 4 and 5 give an overview of SYNVER and a brief discussion of the salient points of \nthe specification language SAL. Sections 6 and 7 define specification verification. Section 8 discusses \nthe algorithms used by the verification phase. Section 9 discusses extensions to these techniques. \n2. SYNCHRONIZATION PROBLEMS The basic unit of discourse in synchronization problems is the process. \nWulf et al. [Wulf74] describe a process as the smallest entity that can be scheduled for independent \nexecution. One can visualize a process as a program plus a data environment and a control environment \ntogether with enough status information (e.g., a program counter, a set of interrupts enabled or disabled, \netc.) so that the process can be executed. A processor is the active agent; it executes a process. Throughout \nthis paper it is assumed that there exist a fixed number of processors available for the simultaneous \nevaluation of processes. The processors are multiplexed over the processes in order to insure that each \nprocess receives some share of processor time. However, no assumptions are made about the allocation \nof processor time. Because the multiplexing is transparent to the processes, in any given process an \narbitrary amount of time may pass between the execution of one instruction and the next. Consequently, \nwithout some mechanism for coordination, communication between processes, such as sharing a resource, \nwould be quite unreliable. The possibility of interference between systems of concurrent processes characterizes \nwhat we call synchronization problems. Such systems are characterized by two or more processes, running \nconcurrently on the same or different machines which share some data or resources. These processes are \nmainly asynchronous; usually they are operating independently on their own computations. Occasionally, \nhowever, they need to access shared data or resources or to make contact with each other. We can describe \nseveral typical kinds of synchronization problems: i) \"Mutual exclusion problems. These problems occur \nwhen processes want exclusive access to a resource. Examples of such resources are line printers or disk \nfiles needed for output. The concept of processes mutually excluding each other from resource access \nextends to program code as well. It is occasionally the case that processes have sections of code, called \ncritical sections, which contain operations on shared data which are to be performed without interference \nfrom other processes. That is, these critical sections must be evaluated indivisibly with respect to \nother processes. Consequently, no more than one process at a time should be evaluating its critical section. \n 2) Shared access. Mutual exclusion problems generalize to requests for multiple, yet still restricted \naccess. In this type of problem, resource access is not restricted to one process at a time. Instead, \nseveral processes of special types or in limited numbers may be permitted simultaneous access. An example \nof such a case is a file which can be accessed by any number of processes which access it on a read-only \nbasis. However, processes which write on the file are still excluded. 3) Cooperation problems. Cooperation \ntype synchronization problems occur when a process requires simultaneous access to two or more resources. \nThese resources cannot in general be acquired one by one and kept until they are all accumulated because \nif the number of resources is limited, they may be exhausted before any process has filled its requirements. \nThus all processes may be waiting for additional resources; none may be able to proceed. This situation \nis known as total deadlock [Holt71a, Holt 71b]. One way of avoiding this situation is for a process to \nacquire all its needed resources simultaneously, or else wait, retaining none of them. Our knowledge \nof synchronization problems is quite informal. We know that processes generally acquire and release resources \none by one, or if they acquire several at a time, they generally later release the same number. In a \nsystem of synchronous processes, processes generally come to some section of code which they must not \nenter before making some arrangements with other processes. Presumably this code uses scarce resources, \nand the arrangements are in the form of obtaining exclusive access, waiting for sufficient resources, \nand so on. At the end of this section of code, processes generally notify other processes that the restricted \naccess has been lifted. Before entering this critical section of code, a process either obtains access \nand proceeds, or waits until access can be obtained. A process in a synchronous system never interrupts \nanother process and causes it to wait. It is characteristic of synchronization problems that if a process \ncan either wait or continue upon attempting to enter a critical section, then that waiting is transparent \nto the process. Once inside the critical section, the state of the world from the process's viewpoint \nis the same whether it was delayed and then awakened or whether it proceeded directly. We call this \ncommutatlvlty the delay invisibility principle. 3. SOLVING SYNCHRONIZATION PROBLEMS Many primitive \noperations have been invented to implement solutions to synchronization problems. One of the original \nsynchronization primitives is DiJkstra's semaphore operations [DiJ68], which we will consider as a representative \nexample. Simply, a semaphore counts the scarce resource usage. Its initial value is the initial number \nof available resources. When its value is zero, no resources are available, and processes requesting \nresources have to wait. The P operation on a semaphore requests the resource, the V operation releases \nit. Semaphore operations are ideal for describing very simple synchronization problems. They are not, \nhowever, easily adaptable to situations of shared data access where, for example, one type of process \nhas priority over another [Cour71]. Nor are they easy to use for cooperation type synchronization problems \n[Par75]. Many of the more complex synchronization problems are of these latter two types. Semaphores \nand other similar types of synchronization primitives do not provide a natural model for human conceptualization \nof these kinds of problems. It is our view that the human conceptual model of synchronization problems \nis centralized. A person tends to think not about each of the individual processes, but about what happens \nin the system of processes as a whole. The human problem conception focuses on how the scarce resource \n(be it critical section code, llne printer, disk buffer, etc.) may be accessed. This encourages thinking \nabout synchronization problems in terms of \"states of the entire system\" with respect to resource access. \n Specification of synchronization problems in terms of their solutions with semaphore operations (or \nother low-level primitives) is inconsistent with such a centralized, state-oriented representation. Semaphores \ndistribute the knowledge about synchronization among the various processes, while the human mind conceives \nof this knowledg e in a centralized way. In addition, semaphores are very implementation-orlented. The \ngap between the conceptual model in terms of states and the solution in terms of P and V operations in \neach process is substantial. The problem solutions bear little resemblance to the problem description \nin English. Hence, in coding solutions to these problems, there is a difficult transformation from the \nhuman problem conception to the resultant code. These implementation-oriented solutions are also difficult \nto change. If an incremental change is made in the problem description (such as giving one type of process \npriority but keeping the rest of the problem the same), there is often a vast change in the P and V solution \nto the problem. This does not reflect our intuition that the translation between mental conception and \nthe machineable problem specification and solution should be a continuous function. In addition, P and \nV solutions are cast at too low a level for a programmer to visualize all the possible interactions between \nprocesses. He has difficulty envisioning all the possible orderings of events. This makes it difficult \nto write programs to solve synchronization problems, and further, to prove that such a system of synchronous \nprocesses is correct. The verification techniques we present are related to those of Robinson and Holt \n[Rob74] and Levitt [Lev72]. Their techniques in turn are based on work by Manna [Man69] [Man70] and Aschcroft \nand Manna [Ash70]. Manna formalizes properties of non-deterministlc programs executed by a single processor. \nHe introduces the choice point which corresponds to the set of instructions from which the processor \nmay select at that point. Correctness of non-deterministic programs is related to the set of all possible \nexecutions of the program on a given input. Manna and Aschcroft formalize properties of parallel programs \nin a similar manner. A parallel program can be transformed into an equivalent non-determlnlstic program \nwhich, when executed, chooses one particular interleaving of instructions of the parallel programs from \nthe set of all possible interleavlngs. Because of the proliferation of choice points, and thus the number \nof assertions as well, the concept of protected bodies of code (those containing no choice points and \ntherefore executed indivisibly with respect to other processes) is introduced. Levitt extends the Asehcroft \nand Manna techniques. He retains the notion of protected bodies of code, and in addition to the choice \nnode he introduces a split node to split a control path previously executed by one processor into two \nor more paths, each executed by its own processor. As with the techniques for sequential programs [Floyd67], \nLevitt divides control paths into fragments of code bounded by assertions and proves each piece separately. \nThus, the synchronization may be proven correct in the same way as and at the same time as the asynchronous \ncode. Robinson and Holt present a formal specification approach in which the code involved with synchronization \nis separated from the asynchronous programs. High level abstract entities (sates) control access to critical \nsections of code. Proving the synchronization correct in this context means proving that these gates \nprotect critical sections in the desired manner with respect to global invarlants. However, the verification \ntechnique is not easily mechanized. In addition, no implementation of gates is suggested. 194 4. OVERVIEW \nOF THE SYNVER SYSTEM These difficulties have led us to propose an automatic programming approach to \nsolving synchronization problems. Using this approach, a programmer can present his problem description \nin the centralized state-oriented way in which he conceives of it. He is provided with a very high level \nspecification language SAL (Synchronization Assertion Language) which provides a natural medium for describing \nsynchronization problems. Thus problem specifications can be easily read, modified, and communicated \nto others. This is not necessarily the case for semaphore solutions of synchronization problems. Because \nSAL is a high level human-oriented medium for the description of synchronization problems, the choice \nof primitives in which to implement the specifications is separable from the problem itself. This enables \nsuch problems to be stated more clearly and frees the solutions from implementation-specific restrictions. \nThe linguistic power of SAL allows incremental changes in the problem to be reflected by incremental, \nlocal changes to the SAL problem description. In addition, the specifications are formal enough to permit \nsynthesis and verification of programs which solve the specified problem. A SAL specification is in \nthe form of an abstract program specification for each type of process in the system. This abstract program \nspecification contains a skeleton of program code, including calls upon one or more synch functions. \nThese synch functions, the code for which will be synthesized by SYNVER, provide the only means by which \nprocesses may communicate. Synch functions may be either monitor procedures [Hoare74]; calls on Prenner's \ncontrol interpreter [Pren73], or procedures utilizing Dijkstra's P and V operations [Steele75]. Both \nSAL and the verification technique are independent of the choice of synchronization primitives used, \ni.e., SYNVER can synthesize synch functions which utilize any of these various sets of lower level primitives. \nSynch function calls are indivisible with respect to other synch function calls, i.e., all such calls \nare ordered linearly in time. The code for the synch functions is completely independent of the code \nfor the asynchronous computations of the processes. We believe that this separability is a desirable \nfeature for communicating processes. It avoids \"collusion\" (unwitting or otherwise) arising from processes \nknowing too much about each other's internal computations [Byrn74]. This kind of separability and minimization \nof knowledge follows naturally from the principles of structured programming, where procedural encapsulation \nis intended to minimize the assumptions one component makes about another [Zilles73]. If we consider \nthe system of processes to be in one of several distinct global states, and synch functions are the only \nmeans of communication between processes, then only execution of synch functions may cause a change of \nstate. For synchronization problems there is usually only a small number of legitimate states that the \nset of communicating processes can be in. These global states are most often related to which processes \nare executing their critical sections. SAL provides facilities for describing these global states and \nfor specifying the transitions between these states made by the synch functions. The former is achieved \nusing SAL state definition statements. The latter is achieved by placing assertions before and after \neach synch function call. Invariant assertions (I-assertions), which are placed before calls to synch \nfunctions, consist of a union of global states. These states are the ones which are permissible when \nthe process is executing in the code section preceding the call. Result assertions (R-assertions), which \nare placed after synch function calls, specify the effects of the call upon the global state of the system \nand upon other processes, i.e., R-assertions indicate the impact of a contact between processes upon \nthe entire system. However, the global state resulting from an R-assertlon must be consistent with the \nfollowing I-assertion as well as I-assertions for all other code sections executed concurrently. Hence, \nI-assertions restrict the impact of other R-assertions during the execution of asynchronous code. When \na SAL synchronization problem description, including definition of global states and abstract program \nskeletons for the process types, is input to the SYNVER system, SYNVER will synthesize code for the synch \nfunctions and then apply a verification technique to the spec&#38;fications to prove them correct. The \ncorrectness of the code generated for the synch functions follows from the correctness of the synthesizer. \nThis may be achieved using standard techniques and will not be discussed further. Here, we are concerned \nwith the verification techniques which are applied to the specifications. Verifying that a SAL specification \nis correct means proving that the invariant and result assertions are all mutually consistent; if (i) \nthe system of processes is initially in a state which is consistent with the I-assertions for the initial \ncode section for all processes, and (ii) there exists no sequence of synch function calls such that the \nsystem is put into a statewhich is inconsistent with the I-assertlons of the code sections being evaluated \nconcurrently, then no I-assertion will ever be violated. Since the I-assertions, in essence, describe \nthe synchronization desired, the specifications have been shown to be correct. 195 5. THE SAL SPECIFICATION \nLANGUAGE The first portion of a SAL problem specification is a set of state definitions of the form: \n <name> IS <conjunction of Boolean relations on state variables> for example, NULL IS NREAD = 0 AND \nNWRITE = O. The rest of the SAL specification consists of one or more process type definitions of the \nform: <process name> DOES <asynchronous code interspersed with synch function calls> Synch function \ncalls occur at the points of contact with other processes and are intended to provide the appropriate \nsynchronization between processes. SAL assertions must be provided before and after each synch function \ncall to describe the actions or the impact that the user expects of these synch functions. SAL provides \ntwo types of assertions: invariant assertions (l-assertions) and result assertions (R-assertions), which \nact as pre- and post-conditions, respectively. 1-assertions are specified to hold for entire sections \nof asynchronous code (the section preceding the call), and they indicate what global states may hold \nupon the initiation of the call. R-assertions follow each call and indicate the effect on the entire \nset of processes of having performed that call. The effect of a call may be to change the global state \nof the system and to make processes wait or to cause previously waiting processes to continue. When \none proves entire (sequential) programs correct, an output assertion is associated with the halting point \nof the program. It is often characteristic of synchronization problems that there is no such halt and \ncorresponding output assertion. Consequently, SAL assertions do not behave as input-output assertions \nfor a synch function call. We emphasize that SAL assertions are not like Hoare assertions [Hoare69] \nin this respect. An R-asser\u00a3ion describes the state of the system after the evaluation of the associated \nsynch function call, regardless of whether or not the calling process continues execution. To facilitate \nthe presentation of SAL syntax as well as the verification technique, we will introduce an example, the \n\"first readers and writers problem\" [Cour71]. There is a table of data, access to which is shared by \nall processes. Some processes are writers who require exclusive access to the table. Others are readers \naccessing the table on a read-only basis. Any number of readers may access the table at a time. Readers \nmay not access the table while a writer has access, and vice versa. Neither type of process has priority \nwhen the table becomes available. We require two integer state variables: NREAD, the number of readers \naccessing the table, NWRITE, the number of writers accessing the table. There are three states of the \nsystem: NULL IS NREAD = 0 AND NWRITE = 0 WRITING IS NREAD = 0 AND NWRITE = i READING IS NREAD GT 0 AND \nNWRITE = 0 There are two types of processes, READERS and WRITERS. READERS perform the synch functions \nSTARTREAD followed by ENDREAD, while WRITERS perform the synch functions STARTWRITE then ENDWRITE. We \ncan now discuss the syntax of invariant and result assertions. Invariant assertions immediately precede \neach synch function call and describe the state of the entire system of processes throughout the evaluation \nof the preceding code section, up to the start of the evaluation of the synch function call. The syntax \nof an invariant assertion is: ASSERT(Si .... , Sn) where Sj's are names of states already defined by \nthe SAL specification. The system may be in only one state at a time. Hence the state definitions are \ndisjoint; exactly one of the Boolean expressions defining the states is true at a time. The meaning of \nan invarlant assertion, then, is that throughout the asynchronous code preceding the synch function call \nthe state of the system is a member of the specified set of states. In our example (see Appendix) the \ninvariant assertion ASSERT(READING) holds throughout the reader's critical section, while the invariant \nassertion for the non-crltlcal sections is ASSERT(NULL, WRITING, READING), i.e., all the states. The \nsyntax of result assertions is much richer than that of invariant assertions. We will give only a brief \ndescription of the syntax here. The reader is referred to [Grif75a, Grif75b] for greater detail and a \nmore complete description of SAL. Result assertions are placed immediately after every synch function \ncall, They specify the transitions between states of the system to be effected by the preceding synch \nfunction call, Result assertions consist of a series of cpnditlonals separated by commas. The basic format \nof a result assertion is: ASSERT(cond-i, cond-2, ..., cond-n) where each cond-J is a conditional whose \nsyntax is of the form: possible current state + next state AND control actions We call the possible \ncurrent state on the left hand side of \u00f7 the initial state of the conditional. Correspondingly, we call \nthe next state on the right hand side the final state. The semantics of an individual conditional in \na result assertion are that if the system state is the initial state, the synch function should change \nthe system to the final state, performing the control actions specified on the right hand side of the \nconditional. The conditionals of a result assertion, ASSERT(cond-l,...,cond-n), are to be considered \nin order of appearance, as in a LISP conditional. If the state of the system is not the initial state \nof cond-l, then consider cond-2, etc. The union of the initial states of the conditionals of a result \nassertion is equal to the set of states listed in the preceding invariant assertion. The control conJuncts \nWAIT and PROCEED on the right hand side of a result assertion conditional are possible control actions \nwhich may be specified by a result assertion. They indicate whether or not control is to be transferred \nback to the process performing the synch function call immediately after the call terminates. The conjunct \nPROCEED may be omitted; if WAIT is not specified, PROCEED is assumed. In the last conditional of a result \nassertion, SAL permits the ELSE operator to replace the initial state and then \"\u00f7\" as syntactic sugar \nfor all the remaining states of the preceding invariant assertion not already explicitly listed in any \nother conditional. In the case that there is only one conditional in the result assertion, ELSE may be \nreplaced by ALWAYS for readability. Boolean expressions involving the variables of the invariant states \nmay appear on the left hand side of ~. An example is: READING AND (NREAD = i) \u00f7 NULL . We now present \na brief discussion of several other control conjuncts that may appear on the right hand side of a result \nassertion conditional. We define sets of processes, called Waitsets, which contain those processes which \nare required to WAIT. A process may belong to at most one waitset at a time. There are three control \nconjuncts on the right hand side of result assertion conditionals which describe waitsets and operations \nupon them. WAIT IN <waitset name> enters the process performing the synch function call as a member \nof the set <waitset name> and prevents it from continuing execution. Corresponding to this is the conjunct \nSTARTUP. STARTUP [ALL] <process type> OUTOF <waitset name> specifies that (all) processes(es) of type \n<process type> are to be removed from the waitset <waitset name> and scheduled for execution. Execution \nwill begin with the next statement in the waiting process after the synch function call. If the IN <waitset \nname> or OUTOF <waitset name> are omitted, the <waitset name> is assumed to be the same as the <process \ntype> name. The third control conjunct related to waitsets is REVIVE [ALL] <process type> OUTOF <waitset \nname> . REVIVE removes (all) process(es) of type <process type> from the waitset <waitset name> and \nallows them to redo their synch function. Note that REVIVE does not necessarily cause a process to continue \nexecution beyond the synch function call, as the process may again be blocked. The purpose of REVIVE \nis to make a clear distinction between the actions of the process performing the wakeup and the actions \nof the process being reawakened. If STARTUP is used to wake up a waiting process, the reawakened process \nwill begin execution at the statement following its synch function call, thereby having no opportunity \nto make any changes in the system state or perform any control actions. Hence, any such actions must \nbe performed by the process performing the STARTUP. In general, the use of STARTUP in this fashion would \nrequire distribution of a copy of a reawakened process' result assertion into every conditional where \na process of that type might be reawakened. REVIVE achieves an equivalent effect in a more modular fashion, \nin that it in effect generates a call on the revived process's synch function as a subroutine. Thus, \nknowledge about the actions of a synch function is localized, and the entire specification becomes easier \nto write and more concise. 197 The REVIVEd result assertion may itself contain REVIVE conjuncts and \nnesting of REVIVEs may continue to arbitrary depth. In this case, or when a sequence of REVIVEs occurs \non the right hand side of a conditional, a chain of interim states is created. Each successive REVIVE \ntakes as its initial state the state resulting from the previous REVIVE. For some states, several actions \nmight be taken with different priorities. The conditional specifying this uses the TRY conjunct, or priority \nset. The priority set begins with the keyword TRY and consists of one TRY clause followed by an arbitrary \nnumber of NEXT clauses. Each of the TRY or NEXT clauses will be called a branch of the prio rity set. \nEach bran ch is considered in turn. The branches are listed in order of priority from highest to lowest. \n The format of a priority set is: lhs \u00f7 right hand side THEN TRY IF actions-i POSSIBLE NEXT IF actions-2 \nPOSSIBLE NEXT IF actions-n POSSIBLE END-TRY . Each branch consists of a set of control conjuncts at \nleast one of which is enclosed by the words IF ... POSSIBLE. The IF ... POSSIBLE brackets enclose the \ncontrol operations which may not necessarily be possible. The highest priority branch found to be possible \nis the one chosen. An example of the use of the priority set would be in the second readers and writers \nproblem [Cour71], where when the table becomes available, we grant the use of it to a waiting writer, \nif any. If there are no waiting writers, then all waiting readers are granted the table. Thus writers \nhave priority over readers. It is sometimes the case that several alternatives for a result assertion \nconditional are desired with equal priority. An example is the first readers and writers, problem [CourTl] \ndescribed above. When the table becomes available, either one waiting writer or all waiting readers may \nbe granted access, and we explicitly do not wish to favor either process type. The SAL conjunct which \npermits one to specify this is an OR conjunct. The format of an OR conjunct is: EITHER aetions-i OR \nactions-2 OR ... OR actions-n END-OR . Ordinarily all branches of an OR conjunct are assumed to be possible. \nWhen an OR conjunct is contained in a branch of a priority set, however, this is no longer true. Within \na priority set, all the branches of an OR conjunct must be considered and rejected before the next branch \nof the priority set can be considered. 6. INFORMAL DESCRIPTION OF THE VERIFICATION The verification \nthat a given specification is correct is by induction. We will explain the proof technique using the \nfirst readers and writers problem as an example. The SAL specification contains a llst of the states \nof the system. It also provides a process type description with the order of synch function calls for \neach type of process which will be in the system of concurrent processes. Each synch function call is \npreceded by an invariant assertion and followed by a result assertion. Since a synch function is in essence \ndefined by these assertions, if a synch function is used more than once, it must have the same assertions \nassociated with it. For each process type definition, the verifier constructs an abstract program. For \neach synch function call, the abstract program contains four items of information: (i) A code index \nnumber representing the asynchronous code preceding the synch function call.  (2) The set of states \nspecified by the invariant assertion.  (3) The name of the synch function.  (4) The result assertion \nin an internal form.  We will use the names ci, ai, fi, and r i to refer to the above items, respectively. \nThe verifier gives the code following the last synch function call in each process description the same \nindex as the code preceding the first synch function call in that description. The purpose of this is \nto help simulate the effect of more than one process of each type running concurrently. The first readers \nand writers problem has the following abstract programs. The complete SAL specification appears in the \nAppendix. Reader : c I al: (NULL, WRITING, READING) STARTREAD 198 r I /* result assertion for STARTREAD \n c 2 a2: (READING) ENDREAD r 2 /* result assertion for ENDREAD c I Writer: c 3 a3: (NULL, WRITING, \nREADING) STARTWRITE r 3 /* result assertion for STARTWRITE c 4 a4: (WRITING) ENDWRITE r 4 /* result \nassertion for ENDWRITE c 3 One may visualize the effect of synch function call as causing a transition \nfrom a code section c i to zero or more result code sections depending upon whether the process PROCEEDs \nor WAITs and whether any other processes are REVIVEd or caused to STARTUP. Similarly these synch functions \nmake transitions from one state of the system to one of a set of several possible result states depending \nupon the initial and final states of the conditionals of the corresponding result assertions. We can \nrepresent these transitions among states and among code sections as a transition table with transitions \nbetween ordered pairs of the form: (state, set of code sections) The transition table for the first \nreaders and writers problem is given below. If the system is currently performing the synch function \nfi and the state of the system is s4; then the resulting transition is one J of the ones listed for \nthe ordered pair (sj, ci). There may be several transitions possible depending on which branches of a \npriority set or OR conjunct are used, as well as the truth value of the Boolean expressions which may \noccur on the left hand side of a conditional. .Synch Function Current Pair Set of Result Pairs STARTREAD \n(NULL, Cl) (READING, c2) (READING, Cl) (READING, c2) (WRITING, Cl) empty ENDREAD (READING, c2) (READING, \nCl) (NULL, Cl) (WRITING, (Cl, c4)) STARTWRITE (NULL, c3) (WRITING, e4) (WRITING, c3) empty (READING, \nc3) empty ENDWRITE (WRITING, c4) (WRITING, (c3, c4)) (READING, (c2, c3)) (NULL, c 3) 199 A transition \ntable like this may be constructed mechanically from the result assertions corresponding to each synch \nfunction call. The transition table specifies how each synch function call changes the state of the system \nand what code sections control will enter as a result of each call. If waiting processes become unblocked \nas a result of a STARTUP or REVIVE specified by a call, then several code sections may be entered simultaneously. \nThis is the case for ENDREAD when the system is in state READING and NREAD=i; then not only does the \nreader process continue (code c I is entered), but a writer is REVIVEd. Thus the entry in the transition \ntable is (WRITING, (Cl, c4) ) . In a later section we will provide an algorithm for constructing the \ntransition table. We will refer to the set of result pairs in the transition table which correspond \nto a given current ordered pair as the entry-set. Any individual ordered pair in the entry set will be \ncalled an entry. The first element of an entry will be referred to as entry.state, while the second will \nbe referred to as entry.code-set. In general when control is simultaneously in code sections Cl,...,Cn, \na state S of the system is allowable if and only if S is a member of the INTERSECTION (al,...an) , where \nthe ai's , iSiSn, are the sets of states listed in the invariant assertions corresponding to the code \nsections c i. If this is not true, then an invariant assertion has been violated; i.e., control is in \nsome cj, but the state of the system S is not a member of aj. It is required, therefore, that for each \nentry in each entry-set of the transition table, entry.state must be an allowable state for entry.code-set. \nThis requirement can easily be checked mechanically. This will guarantee that the performance of a synch \nfunction call does not immediately change the state of the system to a state which is not allowable. \n In order to show that no result assertions violate any invariant assertions, we must prove the following: \nFor any situation where a synch function is called, the resultant state of the system does not violate \nthe invariant assertion of any code section in which control resides. These code sections include the \nsections resulting from the transition as well as sections evaluated concurrently with the synch function \ncall. Thus, the resultant state of the system must be allowable for the set of simultaneous code sections. \n This is not the only criterion which must be satisfied however. Implicit in the assertions are bounds \non the number of processes which may be executing a code section concurrently. We define a function B \non the set of states such that B(S) returns a set of code sections which may be executed by no more than \none process at a time while the system is in state S. For the first readers and writers problem, the \nfunction B is as follows: B(WRITING) = c 4 , i.e., only one writer may access the table at a time. \n We refer to the state of the system plus the set of code sections being evaluated as a system descriptor, \nwhich we will write (S, SCS), where S is the state and SCS is the set of simultaneous code sections. \nWe define a system descriptor (S, SCS) to be legal if and only if: (i) S is an allowable state for SCS. \nThis also implies that the intersection of the ai's corresponding to the c's in SCS i s non-empty. (2) \nIf any c i in B(S) is also in SCS, then no more than one process is evaluating the code section c i . \n In the next section we will present a better representation for system descriptors and describe the \ninduction which will prove that a specification is correct. 7. FORMAL DESCRIPTION OF THE VERIFICATION \n We now present the formal description of the induction proof necessary to prove that a SAL specification \nis correct. The proof technique is similar to computation induction [Man71]. This induction proves that \nif the system does not violate any invarlant assertions initially, then there is no sequence of subsequent \nsynch function calls which will violate any invariant assertions. In this section we describe the induction \nformally and indicate how it also proves that a specification is deadlock-free, i,e., total deadlock \nas defined by Holt [Holt71a] [Holt71b], where all processes are blocked. We do not detect, except by \ninspection in individual cases, whether a given process becomes deadlocked. We first make a number of \ndefinitions. Let C be the set of all code sections c i in a specification. Let NC denote the cardinality \nof C, the total number of code sections. Let c denote any arbitrary member of C. Let S be the set'of \nall states of the system. Let s denote any arbitrary member of S. Let M:C\u00f72 S be a function defined \non any code section, whose range is an element of the power set of S, such that 200 M(ci) : a i M returns \nthe set of states specified by the invariant assertion associated with the code section c i. Let N denote \nthe set of natural numbers {0,i,...}. Let F denote the set of functions f such that f:C+N. Then f(c) \nis a non-negative integer for any f and any c. f(c) will be used to count the number of processes evaluating \nc. Let DELTA be the set SxF. Let <s,f> denote an arbitrary member of DELTA. We call an element of DELTA \n a system descriptor. As noted in the previous section, there are certain code sections which may be \nevaluated by no more than one process at a time. These restrictions depend on the state of the system. \nWe define a function B:S + 2 C such that for a given s, B(s) is the set of all code sections which \nno more than one process at a time may evaluate when the system is in state s. We define a set D which \nis a subset of DELTA such that <s,f> is in D if and only if i) s is in INTERSECTION(M(c)) f(c)>0 2) \nc in B(s) IMPLIES f(c) s i This set D is a representation of all the legal system descriptors. The \nBase Step The base step of the induction can now be described. Let INITIAL be the subset of DELTA which \nconsists of all the possible initial system descriptors (i.e., the configurations before any synch function \ncalls are made). Then the base step of the induction is simply to show that INITIAL is a subset of D. \n The Induction Step For each c in C we define a function gamma c which describes the actions made by \nthe synch function call following code section c. For every c in C, gamma : D \u00f7 2 DELTA c where gamm \nc (<s,f>) is empty if and only if f(c)=0. So gamma c is totally defined on D. As explained in the previous \nsection, a synch function call for a given state s may result in different actions depending on which \nbranch of a TRY or EITHER clause was performed, etc. So gamma c (<s,f>) returns an element of 2 DELTA. \nThis set is always finite, however, because the number of conditionals in a result assertion and the \nnumber of branches of TRY and EITHER clauses are always finite. Now we can define a function GAMMA \nGAMMA:D + 2 DELTA such that GAMMA(<s,f>) : UNlON(gammac(<S,f>)) c in C For a given <s,f>, GAMMA(<s,f>) \nis the set of all possible system descriptors which might result from the occurrence of any synch function \ncall that could be performed at that point in the computation. The induction step is to show that <s,f> \nin D IMPLIES GAMMA(<s,f>) is a subset of D. We define <s,f> to be a descendant of <s,f> 0 if <s,f> = \n<s,f> 0 or <s,f> is in GAMMA(<s,f>') for some descendant <s,f>' of <s,f> 0. If the base step and the \ninduction step are proven, then we may infer that if <s,f> is in INITIAL then every descendant <s,f>' \nof <s,f> is in D. Hence, from any initial configuration of the system, there is no sequence of synch \nfunction calls which causes any invariant assertion to be violated. 201 D is constructed from the invariant \nassertions of the specification together with information about how many processes may be executing a \ncode section at a time, i.e., the definition of the function B. The function GAMMA is constructed from \nthe individual gammac's which are constructed from the result assertions. 8. THE IMPLEMENTATION It \nremains now to show how the sets INITIAL and D can be obtained, and how to construct each function gammac, \nthe function M, and the function B. INITIAL can be obtained from the set of states S and the abstract \nprograms for each process type in the system. At system initialization, the initial state of the system \nis the null or default state where no resources are in use. Usually this state is named NULL. Then for \neach process type p some number Np of processes of type p are started up. By inspection of the abstract \nprogram for processes of type p, it can be determined which code section Cp is the first code section \nfor processes of type p. Let Z denote the set for all such c . P Then INITIAL = {<NULL,f>If(Cp)=N p \nfor all Cp in Z f(c)=0 for c in C-Z}. For the moment, let us assume that the function B is supplied \nby the user as additional information. Later we will indicate how the SYNVER verification system can \nhelp with the construction of B. The definition of the function M is clear. For each code section ci, \nM(c i) returns ai, the set of states specified by the invariant assertion which holds for c i. From \nthese sets and functions D can be constructed. Actually D is almost always an infinite set, even with \nthe restrictions on f made by B, because for c's not contained in B(s) for any s, f(c) can be any non-negative \ninteger. Thus there are an infinite set of such f's for each unrestricted c. Consequently the set D cannot \nbe enumerated in a finite number of steps. If D were finite, then we could take each element <s,f> in \nD and check if GAMMA(<s,f>) is a subset of D. Then the induction step of the verification would be proven \nand would terminate in a finite number of steps. By the construction of the set D and the definition \nof each gammac, it is clear that the relevant values of any f(c) are 0, i, and 2 or more: i) f(c)=0 \n--This corresponds to no processes evaluating code section c. 2) f(c)=l --This corresponds to exactly \none process evaluating code section e. 3) f(c) > 1 --This corresponds to two or more processes evaluating \ncode section c. This justifies the definition of three equivalence classes of the values of f(c); for \neach c in C we define the relation EQUIV c as follows: <s,f> EQUIV <s',f'> if and only if s=s' and G(f(c)) \n= G(f'(c)) c where G:N + N is a function such that G(O) : o G(1) = 1 G(n) = 2 for n >- 2. We can \nnow define the equivalence relation EQUIV such that <s,f> EQUIV <s',f'> if and only if for all c in \nC, <s,f> EQUIV c <s',f'> With this equivalence relation, D is partitioned into a finite number of elements. \n Thus the induction step of the verification can be proven by examining only the elements of the parti- \ntion of D induced by EQUIV. There are at most 3*NC*NS such elements of the partition where NC is the \ncardi- nality of C and NS is the cardinality of S. This is a finite set, and hence the induction step \nof the veri- fication can be proven in a finite number of steps. With this technique deadlock can also \neasily be detected. By the definition of D, there are no elements <s,f> of D such that f is identically \nzero. This is because <s,f> in D implies s in INTERSECTION(M(c)), which is empty if f(c)=0 for every \nc. Consequently, if every descendant of any element f (c) >0 of D is also in D, the solution is free \nfrom total deadlock. In order to compute GAMMA(<s,f>) for any <s,f> in D, we need to compute gammac(<s,f>) \nfor each c in C. gamma c is determined from the transition table which is constructed from the result \nassertions of the specification. We now present the algorithm for constructing the transition table. \nThen we will describe how to construct each gamma c from the table. The Transition Table The transition \ntable describes for each code section c in C and for each state s in S, how the synch function call following \ncode section c affects both the state of the system and the set of simultaneous code sections. The transition \ntable is constructed from the result assertions of the SAL specification together with the abstract program \nproduced for each process type. An abstract program is constructed for each process type from the SAL \nspecification. Every piece of code of the form: <asynchronous code> ASSERT(---); synch function call; \nASSERT(---); is abstracted to a code index ci, a set of states for the invariant assertion ai, a synch \nfunction fi and a result assertion r i. The entries in the transition table are formed from examination \nof the conditionals of each result assertion. One row in the table is constructed for each (s,ci) pair, \nwhere s ranges over all the states in the corresponding a i. For every state s in ai, the result column \nof the transition table represents all the actions specified by all the conditionals whose initial state \nis s in the result assertion r i. Sometimes there are several such conditionals in ri, each with different \nBoolean expressions. Thus there may be multiple entries for each (s,ci) line in the table. In addition, \neach branch of a TRY or EITHER clause causes another entry to the (s,ci) line in the table. For example, \n sl \u00f7 s2 THEN TRY B1 NEXT B2 NEXT Bn END-TRY creates n entries in the transition table, one for each \nof the branches Bj. EITHER constructs are treated similarly. Each individual result pair in the entry-set \nconsists of two parts: (I) A state which the system may be in after the completion of the synch function \ncall. (2) A list of code sections where control will be as a result of the synch function call. This \nis empty when the process performing the synch function call must WALT, and no other processes are REVIVEd \nor caused to STARTUP.  Although the system is actually in only one state at a time, it may not be decidable \nwhen constructing the transition table which state this is. For example, for code section Ci if the result \nassertion is: ASSERT(Si and K=i \u00f7 $2, Si and K=2 \u00f7 $3) , the (SI,Ci) part of the transition table has \ntwo entries, ($2, C2) and ($3, C2). Actually the system will be either in state $2 or in state $3, depending \nupon the value of K when the synch function call is made, Statically, however, the only information that \ncan be ascertained is the entries ($2, C2) and ($3, C2). So, if for a given c i the result assertion \nr i has multiple conditionals for a given state s, then the (s,cl) line of the table consists of one \nentry for each conditional with initial state s. Let us now consider how to construct an individual \nentry for a given c i and s, and a given set of branches of TRY and EITHER clauses. This narrows the \nset of conjuncts to be considered on the rhs of a conditional to be rhs state, STARTUP, REVIVE, WALT, \nand PROCEED. Initially, entry.code-set and entry.state are both empty. If the conditional PROCEEDs (i.e., \ndoes not WAIT), then the next code index in the list of abstract program code (described above) for that \nprocess should be included in entry.code-set. Let us refer to this index as cj. In most cases J=i+l. \n If the conditional WAITs, entry.code-set remains unchanged. If there are no REVIVE conJuncts on the \nright hand side of the conditional, then entry.state is the rhs state of the conditional. For example, \nif the conditional is s + s AND PROCEED, n the entry is (Sn, cj), where cj is the next code section \nindex in the abstract program. If one or more STARTUP conjuncts appear on the right hand side of the \nconditional, one code index for each STARTUP conjunct is added to entry.code-set. This is the code index \nwhich follows the synch function call which caused a process of that type to WAIT. The remaining conjunct \nto consider is REVIVE. This affects both parts of a transition table entry. Recall that a REVIVEd process \nredoes the synch function which caused it to WAIT. The REVIVEd process takes as its initial state the \nfinal state of the conditional specifying REVIVE. Consequently, the state of the system upon completion \nof the synch function call may not necessarily be that final state. In addition, if the REVIVEd process \neventually PROCEEDs, the code section following the synch function call (which caused the REVIVEd process \nto wait) should be added to entry.code-set. To construct an entry for (s,c i) when one of the conditional's \nconjuncts is a REVIVE, one first constructs the entry for that conditional without considering any REVIVEs. \nThis results in a set of one or more result pair entries. Then, for each such entry E, apply the following \nprocedure to the synch function of the REVIVEd process: Let c~ denote the code section which corresponds \nto the synch function of the REVIVEd process. Replace E by the new-entries obtained by: for each entry \nE' from the result pairs correspondiffg to (E.state,cJ) (i) Let new-entry.state be E'.state  (2) Let \nnew-entry.code-set be UNION(E.code-set,E'.code-set)  This results in some new set of (s,ci) entries. \nIf there is another REVIVE in the (s,ci) conditional above, then this procedure is applied to each entry \nE in this new set of (s,ci) entries for the synch function which caused this next REVIVEd process to \nWAIT. In general, a REVIVEd process may in turn REVIVE another process, and so on. This creates a transitive \nclosure problem of the chain of REVIVE references to (s,c i) entries. If this chain of references has \na minimal element (one which does not make a further REVIVE reference) then the corresponding transition \ntable entries can be completed. Otherwise the chain of references is strongly connected. Tarjan [Tarj72] \ndescribes an algorithm which detects strong-connectivity. If the chain of REVIVEs is strongly connected, \nthen this means a process eventually tries to REVIVE itself or a process identical to itself. In these \ncases another method must be found for completing the transition table, after which this verification \ntechnique may be applied. Once the transition table entries have been completed by performing the transitive \nclosure of all chains of REVIVEs, we can construct the functions gamma c. The construction of the transition \ntable from the result assertions is completely mechanizable with the exception of completing the transition \ntable in the case of a strongly connected chain of REVIVEs. Constructin$ the Function GAMMA Once the \ntransition table is completed, it is easy to construct each gammac, and therefore the function GAMMA \nas well. The transition table contains entries for each c in C and each s in M(c). The transition table \nmay have several entries for each such c and s, and each entry has two parts, entry.code-set and entry:state. \nThen for each c, and each <s,f> in D, gamma c (<s,f>) is empty if f(c) = 0. Otherwise (f(c)#0), gammac(<S,f>) \n= {<E.state,fE> I for all entries E in the transition table for (s,c)} where fE is the function in F \nsuch that (i) fE(c) = f(c)-l, and  (2) for each c i in E.code-set fE(ci) = f(c i) + i.  204 EXAMPLE: \nTHE FIRST READERS AND WRITERS PROBLEM S = {NULL,WRITING,READING} C = {Cl, c2, c3, c 4} Synch Function \nSTARTREAD ENDREAD STARTWRITE ENDWRITE Current Pair (NULL, c I) (READING, Cl) (WRITING, Cl) (READING, \nc2) (NULL, c3) (WRITING, c3) (READING, c3) (WRITING, c4) Set of Result Pairs (READING, c 2) READING, \nc2) empty (READING, Cl) (NULL, c I) (WRITING, (Cl, c4) (WRITING, c4) empty empty (WRITING, (c3, c4) (READING, \n(c2, c3) (NULL, c 3) The function B supplied to the verifier is as follows: B(READING AND NREAD=i) \n= {c 2} B(WRITING) = {c 4} Elements of the Partition of D f(c I) f(c 2) f(c 3) f(c 4) dl: <NULL, \nGEI 0 0 0 > d2: <NULL, 0 0 GE i 0 > d3: <NULL, GE i 0 GE i 0 > d4: <READING. 0 i 0 0 > d5: <READING. \n0 1 GE i 0 > d6: <READING. GEI 1 0 0 > d7: <READING GE 1 1 GE 1 0 > d8: <READING 0 GE 2 0 0 > d9: <READING \nGE 1 GE 2 0 0 > dl0: <READING 0 GE 2 GE 1 0 > dll: <READING GE 1 GE 2 GE 1 0 > d12: <WRITING 0 0 0 i \n> d13: <WRITING GE i 0 0 i > d14: <WRITING, 0 0 GEi I > d15: <WRITING, GE 1 0 GEi I > Notice that we \nhave collapsed this partition for c I and c3, because the classes =i and GE 2 behave equivalently. Base \nStep INITIAL = {<NULL, GE 1 0 0 0 > <NULL, 0 0 GE i 0 > <NULL, GE i 0 GE 1 0 >} Clearly INITIAL is \na subset of D, Induction Step GAMMA(d3) : {d5, dT, d13, d15} GAMMA(d7) = {d3, d6, dT, dl0, dll, d15} \nGAMMA(dii) = {d7, d9, all} GAMMA(dl5) = {d3, d7, all, d13, dl4, dl5} . We have taken advantage of an \noptimization here which is described in the following section. Therefore it was not necessary to apply \nGAMMA to a representative element of each equivalence class in the partition of D. 9. EXTENSIONS The \nverification technique presented above is easily mechanized as it consists of iterating through an enumeration \nof a finite set, the partition of D, and applying the function GAMMA to each representative element of \nthat partition. In addition, the function B may be generalized. In most synchronization problems either \nan unbounded number of processes are permitted to evaluate a critical section simultaneously or at most \none process may be evaluating it. A problem could be formulated, however, in which at most some bounded \nnumber (greater than i), say N, processes are permitted to evaluate a critical section simultaneously. \nThen B would have to be generalized such that c in B(s) I~LIES f(c)~N. The verification technique can \n accommodate this generalization of B without difficulty. Since N is finite, the number of equivalence \nclasses in the partition of D is also finite, and thus the verification may be accomplished as above. \n This verification technique can be optimized to avoid having to apply GAMMA to every representative \nelement of the partition of D. Let us define a \"less than\" relation, <, on functions such that f' < \nf if and only if there exists a c.l such that f'(ci) = 0, f(ci) > 0 and f(c) = f'(c) for circ. Then \nclearly f' < f implies GAMMA(<s,f'>) is a subset of GAMMA(<s,f>) since UNION(gammac(<S,f'>)) is a subset \nof UNION(gammac(<S,f>)) . c in C c in C Thus for any two representative elements of the partition <s,f> \nand <s,f'>, if f'<f, then only <s,f> need be considered. i0. CONCLUSION The verification technique \npresented here is based on the existence of synch functions to achieve synchronization among concurrent \nprocesses. Synch functions are separate from the asynchronous code of a program. This simplifies the \ncorrectness proofs of both the synchronization and the asynchronous code because the proofs can be done \nindependently. Consequently, the combinatoric explosion of program paths to be proven as in the Levitt-Manna-Aschcroft \napproach is reduced. This is achieved without making assumptions that some bodies of code are protected. \nFurthermore, our technique is easily mechanizable and is suitable for incorporation in an automatic programming \nsystem. In addition, we have several implementations for synch functions, including Prenner~s control \ninterpreter, Hoare's monitors, and DiJkstra's P and V operations. This flexibility is achieved because \nboth SAL and the verification technique we use are at a sufficiently high level so as to be independent \nof the implementation. APPENDIX The following is the SAL specification of the first readers and writers \nproblem [Cour71]: NULL IS NREAD = 0 AND NWRITE = 0 WRITING IS NREAD = 0 AND NWRITE = 1 READING IS NREAD \nGT 0 AND NWRITE = 0 READER DOES BEGIN ASSERT(NULL,WRITING,READING); STARTREAD; ASSERT(<READING,NULL> \n\u00f7 <READING,READING>, ELSE WAIT); <code for reading> ASSERT(READING); ENDREAD; ASSERT(READING AND NREAD \nGT 1 + READING, if not last reader, Just continue READING AND NREAD = 1 + NULL THEN TRY IF REVIVE(WRITER) \nPOSSIBLE END-TRY) ; if last reader, revive waiting writer if any END; WRITER DOES BEGIN ASSERT(NULL,WRITING,READING); \nSTARTWRITE; ASSERT(NULL+WRITING, ELSE WALT); if table free then go, else wait <code for writing> ASSERT(WRITING); \nENDWRITE; ASSERT (ALWAYS NULL THEN TRY EITHER IF REVIVE(WRITER) POSSIBLE OR IF REVIVE(ALL READER) POSSIBLE \nEND-OR END-TRY); exit, letting writer or all readers go if any END; REFERENCES [Ash70] Ashcroft, E. \nand Manna, Z., Formalization of Properties of Parallel Programs. Machine Intellisence Vol. 6, 1970. [Ash75] \nAshcroft, E. A, Proving Assertions about Parallel Programs, J. Comp. and Sys. Sci. i0, pp 110-135 (1975). \n[Byrn74] Byrn, W. H., \"Sequential Processes, Deadlocks, and Semaphore Primatives,\" Ph.D. Thesis, Harvard \nUniversity, August 1974. [Cour71] Courtois, P. J. et al., \"Concurrent Control with 'Readers I and ~Writers',\" \nComm. ACM Vol. 14, No. i0 (October 1971), pp. 667-668. [DiJ68] Dijkstra, E. W. \"Cooperating Sequential \nProcesses,\" in Programming Languages, (F. Genuys, ed.), Academic Press, New York, 1968, pp, 43-1%2. \n[Floyd67] Floyd, R. W. \"Assigning Meanings to Programs,\" in Mathematical Aspects of Computer Science \n(J. T, Schwartz, ed.) Vol. 19, American Math. Soc., Providence, Rhode Island, 1967. [Grif74] Griffiths, \nP., SYNVER: A System for the Automatic Synthesis and Verification of Synchronous Processes, Proceedings \nACM74, Nov. 1974. [Grif75a] Griffiths, P., SYNVER: An Automatic System for the Synthesis and Verification \nof Synchronous Processes, Ph.D. Thesis, Harvard University, June 1975. 207 [Grif75b] Griffiths, P., \nSAL: A Very High Level Specification Language, Proceedings International Symposium on Proving and Improving \nPrograms, July 1975. [Hoare69] Hoare, C. A. R., An Axiomatic Basis for Computer Programming, Comm. ACM \nVol. 12, No. i0 (October 1969). [Hoare74] Hoare, C. A. R., Monitors: An Operating System Structuring \nConcept., Comm. ACM Vol. 17, No. i0 (October 1974), pp. 549-557. [Holt71a] Holt, R. C., On Deadlock in \nComputer Systems, Ph.D. Thesis, Cornell University, January 1971. [Holt71b] Holt, R. C., Some Deadlock \nProperties of Computer Systems, Symp. on Opg. Sys. Principles, Palo Alto, October 1971. [Lev72] Levitt, \nKarl N., The Application of Program-Proving Techniques to the Verification of Synchronizatio n Processes, \nFall Joint Computer Conference, pp. 33-47, 1972. [Man69] Manna, Z., The Correctness of Programs, Journal \nof Computer and System Sciences, 3, 119-127, 1969. [Man70] Manna, Z., The Correctness of Nondeterministic \nPrograms, Artificial Intelligence I, pp. 1-25, 1970. [Man71] Manna, Z., Ness, S., and Vuillemin, J., \nInductive Methods for Proving Properties of Programs, Computer Science Department, Stanford University, \n1971. [Naur66] Naur, P., Proof of Algorithms by General Snapshots, BIT 6,4, pp. 310-316, 1966. [Par75] \nParnas, D. L., On a Solution to the Cigarette Smoker's Problem (without conditional statements), Comm. \nACM Vol. 18, No. 3 (March 1975), pp. 181-183. [Pren72] Prenner, C. J., Multi-path Control Structures \nfor Programming Languages, Ph.D. Thesis, Harvard University, May 1972. [Pren73] Prenner, C. J., Extensible \nControl Structures, SIGPLAN Notices, Vol. 8, No. 9, pp. 129-132, September 1973. [Rob74] Robinson, L. \nand Holt, R. C., Formal Specifications for Solutions to Synchronization Problems, Computer Science Group, \nSRI, Menlo Park, California. [Steele75] Steele, G. L., Generation of Optimized Semaphore Synchronization \nCode, Senior Thesis, Harvard University, May 1975. [Tarj72] Tarjan, R., Depth-first Search and Linear \nGraph Algorithms, SIAM J. Comput. Vol. i, No. 2, June 1972. [Weg70] Wegbreit, B., Studies in Extensible \nProgramming Languages, Ph.D. Thesis, Harvard University, May 1970. [Weg74] Wegbreit, B. et al., ECL \nProgra=~erVs Manual, Ctr. for Research in Comp. Tech., Harvard University, TR 23-74. [Wulf74] Wulf, \nW. et al., HYDRA: The Kernal of a Multiprocessor Operating System, Comm. ACM Vol. 17, No. 6 (June 1974) \npp. 337-344. [Zilles73]'Zilles, S., Procedural Encapsulation: A Linguistic Protection Technique, SIGPLAN \nNotices, Vol'. 8, No. 9 (September 1973), pp. 142-146. 208 \n\t\t\t", "proc_id": "800168", "abstract": "<p>SYNVER is an automatic programming system for the synthesis of solutions to problems of synchronization among concurrent processes from specifications written in a high level assertion language (SAL). The correctness of the solutions constructed by SYNVER follows from the soundness of the synthesizer itself and from a verification phase which is applied to the specifications. This verification phase is the main topic of this paper. To provide context for the verification the paper includes a discussion of synchronization problems and a brief overview of both the SYNVER system and the SAL specification language. A formal definition of the correctness of a SAL specification is then presented along with algorithms which may be used to determine if a given specification is correct.</p>", "authors": [{"name": "Patricia P. Griffiths", "author_profile_id": "81100069652", "affiliation": "", "person_id": "PP14034639", "email_address": "", "orcid_id": ""}, {"name": "Charles J. Prenner", "author_profile_id": "81100522090", "affiliation": "", "person_id": "PP31045999", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/800168.811553", "year": "1976", "article_id": "811553", "conference": "POPL", "title": "Verifying formal specifications of synchronous processes", "url": "http://dl.acm.org/citation.cfm?id=811553"}