{"article_publication_date": "01-01-1976", "fulltext": "\n GOAL-DIRECTED PROGRAM TRANSFORMATION BEN WEGBREIT XEROX PALO ALTO RESEARCH CENTER 3333 COYOTE HILL \nROAD PALO ALTO, CALIFORNIA 94314 ABSTRACT Program development often proceeds by transforming simple, \nclear programs into complex, involuted, but more efficient ones. This paper examines ways this process \ncan be rendered more systematic. We show how analysis of program performance, partial evaluation of functions, \nand abstraction of recursive function definitions from recurring subgoals can be combined to yield many \nglobal transformations in a methodical fashion. Examples are drawn from compiler optimization, list processing, \nvery high level languages, and APL execution. KEY WORDS AND PHRASES: program transformation, program \nanalysis, partial evaluation, optimizing transformations, compiler optimization, analysis of programs, \nexecution analysis, simplification, generalization, evaluation in context, very high level language, \nlist processing, Lisp, APL. CR CATEGORIES: 4.12, 4.22, 4.6. 5.24, 5.25 1. INTRODUCTION Optimizing transformations \nprovide a means for converting a clear, well-articulated but inefficient program into one with equivalent \nresults but better performance characteristics. Certain optimizing transformations have been used for \nsome years in the compilers for algebraic languages. A 1972 survey by Cooke. and Allen [3] lists and \nclassifies approximately twenty such transformations. Languages in which the built-in operations act \non composite objects such as arrays (e.g., APL [16]), sets (e.g., SETL [21]), or relations (e.g., VERS \n[12]) give rise to the need for additional classes of optimizing transformations. There are three reasons \nfor this. (1) The substantial gulf which separates language from underlying machine makes it possible \nfor the programmer inadvertently to write simple programs for which unoptimized execution yields poor \nperformance. (2) The pervasive use of composite objects often makes such programs the most natural expression: \neasiest to write, debug, and modify. (3) ;vluch of the appeal of these languages is due precisely to \nthe decoupling of expression from implementation. For these reasons, the potential payoff from an appropriate \nset of optimizing transformations for these languages is substantial. This paper shows how the process \nof program transformation can be rendered systematic. Our thesis, in brief, is that program transformation \ncan be made goal-directed. It is possible to analyze programs so as to obtain expressions for their execution \nperformance and the complexity of their output. Based on discrepancies between these. performance goals \nare established. These goals are used to direct the process of program transformation--which is carried \nout by local simplification, partial evaluation of recursive functions, abstraction of new recursive \nfunction definitions from recurring subgoals, and generalization of expressions as required to obtain \ncompatible subgoals. Thus, high-level goals or i.ntentions are used to guide and give coherence to the \noperations of local activities. 153 We expand on this brief description below. Our intention in this \npaper is three-fold. (1) We show how this approach provides a framework in which many of the optimizing \ncompiler transformations can be placed and systematized. In this framework, related transformations not \ncommonly implemented in optimizing compilers are seen to have a natural place. (2) We show how many of \nthe optimizing transformations for very high level languages [2] can be obtained in a straightforward \nway from the program. Thus, these techniques may find a role in the future processors of such very high \nlevel languages. (3) We hope to make a small step in identifying and explicating techniques by which \nthose software-engineers who must be concerned with efficiency can carry out their design in a systematic \nfashion. Considerable attention has been given in recent years to the systematic development of well-structured \nprograms, formalizing the practices of outstanding programmers. Comparable attention should, perhaps, \nbe paid to the fundamental techniques which underlie the systematic development of high-performance programs. \n  2. BASIC IDEAS One means for transforming a program into an equivalent one with better performance \nrelies on program analysis to single out the appropriate portions of the program to be rewritten. By \nprogram analysis, we mean the derivation of closed-form algebraic expressions which describe execution \nbehavior These expressions specify the program's computation cost (e.g., execution time, amount of storage \nused, number of I/O requests) and . the program's output characteristics (e.g., size of the result, textual \nsource of allocated storage comprising the result, probability of the result satisfying a given predicate) \nas a function of input characteristics [23]. Using program analysis techniques, our transformation approach \nproceeds as follows: (1) Obtain some idea of the minimum computation cost required to produce the input/output \nmapping being realized by the program. (2) Analyze the program to determine its comPutation cost, and \nrelate components of the cost to specific segments of the program text. (3) Find those program segments \nwhose computation costs are not accounted for in the estimates of minimum cost. These segments are potential \nsources of computational waste and are therefore designated as targets for simplification. (4) If the \ntargets so designated contain multiple possibilities for simplification, focus attention on the program \nsegments having the greatest analyzed cost: Insofar as the program can be. transformed to realize a significant \nperformance improvement, it must be by simplifying these segments The central idea we wish to present \nis that this approach can be rendered systematic and thus has a place in programming methodology as well \nas serving as a basis for mechanical program transformation. 2.1. NOTATION Several of our example programs \nwill be written in a syntactic variant of Lisp, using the following notation: The empty list is denoted \nby nil.  Cons(x,y) constructs a list in which the first element is x and y is the list of all elements \nexcept the first.  H, read as \"Head\", is a prefix operator which extracts thefirst element from a non-empty \nlist; to avoid parenthesis clutter, the argument to H is not parenthesized, e.g. H Cons(x,y) = x.  \n T, read as \"Tail\", is a prefix operator which extracts all elements except the first from a non-empty \nlist; T Cons(x,y) = y.  154  Null(x) is defined as x=nil. Transformation:  Conditional expressions \nare written as  if Pl then e 1 else if P2 then e 2 else e n Function definitions are written as <=' \nfname(parameter I ..... parameterk) de fining form  2.2. EXAMPLE--CONCATENATION OF THREE LISTS Given \nthe following definition of Append, A(x,y) <= if Null(x) then y else Cons(Hx, A(Tx,y)) consider appending \nx to y to z by the expression A(A(x,y), z). Analysis: Program analysis shows that the execution cost \nis proportional to 2'N+ly I where Ix] is the length of the list x. It is useful to differentiate between \nthe inner and outer calls On Append. Let these be denoted by A1, and A 2 and let the associated calls \non Cons be Cons 1 and Cons 2. Then program analysis shows that Al(A2(x,y), z) executes ]x] calls on Cons \n2 and N+[jl calls on Cons 1. Analysis of the output of Al(A2(x,y), z) shows that its length is [x]+[yl+]z \n]. Differentiating the source of the Cons-cells which comprise the output, analysis shows that Ixl+]y[ \ncalls come from Cons 1 and that ]z] calls come from the input z. Comparing the execution costs (internal \nwork) to the output (accountable work), it is seen that the Ix] calls on Cons 2 represent wasted effort. \nThese calls are being executed internally but cannot be accounted for in the output. The task of transformation \nis to rewrite the program so as to remove the calls on Cons 2. We refer to Cons 2 as the target for simplification. \nTo remove a Cons, we can apply the local simplification rules H Cons(a,#) -~ a T Cons(aft) -~ fl To obtain \nan opportunity to apply these, we expand the definition of Append to the point where H and T appearing \nin the program can be applied to Cons 2. Start with: (2.1) Al(A2(x,y ), z) Expand A2(x,y ), i.e., replace \nthe call by an instantiated body, resulting in: Al(if Null(x) then y else Cons2(Hx, A2(Tx,y)), z) Distribute \nthe conditional, i.e., bring the conditional expression from the argument position to outside the call \non A 1, so that A 1 is applied to the result of each conditional clause. This yields: if Null(x) then \nAl(Y,Z ) else Al(COns2(Hx, A2(Wx,y)), z) Expand the second call on A 1 and then simplify. We call this \npartial evaluation, and the steps in this case are as follows: Consider Al(COns2(Hx, A2(Tx,y)), z). Let \n~ denote Cons2(Hx, A2(Tx,y)), so we are considering Al(~,z ). Expanding A 1 results in if Null(v) then \nz else COnSl(H~, Al(TY,z)). Since ~ is not Null, it follows that Al(Y,z ) = COnSl(HY, Al(T-r,z)). Further, \nwe can apply the local simplification rules for Cons to \"r, so that H~r -~ Hx and T~ -~ A2(Tx,y ). Thus, \nwe have removed one instance of Cons2--a step toward our goal. We have Al(~,,z ) = COnSl(HX , Ai(A2(Tx,y \n), z)). Thus, (2.1) has been partially evaluated to yield: (2.2) if Null(x) then Al(Y,Z ) else COnSl(HX, \nAi(A2(Tx,y), z)) 155 Because one Cons 2 is absent, the evaluation of (2.2) requires, in the case of \na non-Null x, one fewer Cons 2 than the evaluation of (2.1). Thus, (2.2) is a slightly improved way of \ncomputing (2.1). Whenever we are confronted with evaluating an expression of the form (2.1), it is desirable \nto use (2.2) in its place. Observe that the form (2.1) appears as a subexpression of (2.2) with the substitution \nof Tx for x, i.e., as Ai(A2(Tx,y ), z). We have identified a subproblem, Ai(A2(Tx,y ), z) which is identical \nin form to a more global problem, Al(A2(x,y ), z). We call this identification subgoal abstraction and \nuse it to define a new recursive function in which the performance improvement of (2.2) is systematically \nachieved. We introduce F(x,y,z) to stand for Al(A2(x,y ), z). Then (2.1) and (2.2) become (2.1') F(x,y,z) \n(2.2') if Null(x) then Al(Y,Z ) else COnSl(HX, F(Tx,y,z))  Ignoring subscripts, we have F(x,y,z) <: \nif Null(x) then A(y,z) else Cons(Hx, F(Tx,y,z)) as an executable definition of F. Since the above derivation \npreserves correctness, it follows that A(A(x;y), z) : F(x,y,z). Analysis shows that the execution cost \nfor F is linear in N+lyl and, in particular, the number of Cons executed is N+]yl. Thus, the goal has \nbeen attained. 2.3. FURTHER EXAMPLES Some additional examples will illustrate that this approach can \nyield interesting results. We show the problem, analysis, and final program--omitting derivations. 2.3.1 \nThe following program compares two arrays B[l:nJ and C[l:nJ and sets the Boolean. variable same to true \nif they are pairwise equal j+0; same+true; while jtn do (j~j+l; same + same A B[j]=C[j]) return same \n Analysis shows that this requires n steps and that the result is a Boolean scalar. It is therefore possible \nthat the scalar could be computed in fewer steps. The entire loop is the target for simplification. The \nA operator has the local simplification rule x A y -~ if x then y else false which avoids computing y \nwhen the value of x determines the result. We write the above loop as a recursive function, and apply \nour transformation method using the local simplification rule. We obtain a new function which, when put \ninto iterative form, is j~-O; same+true; while jCn A same do (j+j+l; same + B[j]=C[j]); return same; \n This requires n steps in the worst case and 1 step in the best case. If the probability that B[j]=C[j] \nis fl, program analysis shows that the average number of steps is (I-fin)/(1-#). Observe that this is \nbounded from above by min(n, 1/(1-fl)). 2.3.2 The following APLl\"one-liner '' has the value 1 if n is \nprime and 0 otherwise: 2=+/O=(m)ln. (Reading from right to left, this may be rendered as: Consider n \nand the sequence 1,2,...,n; obtain the remainders after dividing the elements of the sequence into n; \nfind the elements with 0 remainder; count their occurrences; and test the count to see if 156 it is 2. \nIf so, then n is prime.) Analysis shows that this constructs three temporary arrays each of length n, \nbut that the output is a single scalar. Transformation takes place in three stages each of which eliminates \none unneeded array. The final result is 2=H(1,n) where HCk,n) <= if k>n then 0 else if (n mod k)=O then \nl+H(k+l,n) else H(k+l,n) which constructs no temporary arrays: It simply counts the number of times \nthe remainder of n divided by k is 0, for k from 1 to n. 3. TECHNIQUES The preceding derivations employ \nthree techniques: program analysis, partial evaluation, and subgoal abstraction. These activities may \nbe roughly characterized as: analyzing the program's resource expenditure and output to find appropriate \ntargets for simplification; rewriting portions of the program to realize a performance improvement on \nthe first execution of the program's loops; recognizing subproblems and using these to form recursive \nprograms such that the performance improvement is attained on every execution of the program's loops. \nWe now examine these in greater detail. 3.1. PROGRAM ANALYSIS Program analysis obtains closed-form expressions \nwhich describe execution behavior as a function of input characteristics, e.g., worst-case execution \ntime as a function of the input-length. A system which automatically carries out program analysis for \nsimple programs is discussed in [23]. Techniques, implementation issues, and limitations are discussed \nthere. In the interest of brevity, we confine our discussion here to outlining the sorts of analysis \nwhich can be produced in this way. Analysis is carried out for three cases--best, worst, and average \n(under some assumptions about input distributions). It is useful to write the closed-form expressions \ndescribing execution cost in a partially factored form, separating out the dependence on input characteristics \nfrom the dependence on machine implementation. For example, the execution time of Append is written as \nCo+cl'n where n is the length of the first argument to Append and Co,C 1 are implementation constants. \nThe implementation constants are written as linear arithmetic expressions of the form rl'Pl + ... + rk'Pk \nwhere the ri's are rational numbers and the Pk'S denote costs of executing primitive operations. For \nthe case of Append, c o = [ncall + 2\"vref + null c I = car + cdr + Cons + 3\"vref + fncall The lower \ncase spelling of a primitive operation stands for that operation; fncall denotes the action required \nto invoke a non-primitive operation; vre[ denotes access to a variable. When analysis is used for the \npurpose of directing program transformations, it is useful to distinguish the source of a primitive operation \nbased on the defining function in which it appears. Thus, cons A denotes those Cons-cells created by \nthe execution of the Cons operation textually inside Append (c.f., the definition of Append, Section \n2.2). To do this, we distinguish the symbolic \"costs\" of elementary operations based on their textual \nposition: a Cons textually inside function A is treated, for the purpose of analysis, as if it were distinct \nfrom a Cons textually inside function B. In the case where a defined function, F, appears more than once \nin the expression to be transformed, it is useful, for 157 the purpose of analysis, to treat each occurrence \nof F in the expression as if it were a distinct function. Suppose F is recursive and the expression is \nF(G(x), F(y, z)). This is treated as if it were Fi(G(x),. F2(Y , z)). All calls to F from F 1 (and from \nfunctions called by F1) are treated as calls on F 1. Similarly, F 2 is treated as calling F 2 recursively. \nA function call \"within\" the inner F is therefore labeled fncall F . Thus, we can distinguish the cost \nof executing the inner F from the cost of executing the outer F (once its arguments have been evaluated). \nThis representation of execution was chosen for its generality. From it, we can obtain the answer to \nspecific questions by assigning appropriate values to the primitive operation symbols. For example, to \nobtain total execution time\" on a given machine, let each primitive symbol stand for the execution time \nof the corresPonding operation. To obtain the total number of Cons cells created, let cons i (for all \ni) be 1 and let the other elementary operation symbols be 0. In analogous fashion, we may obtain the \nnumber of [ncall's (function calls) out of a specific function, or the computation time spent within \na specific group of functions. Several other sorts of analysis are also of interest. One class is output \nanalysis, e.g., determining the length of a function's output if an array or list, or its range if a \nscalar. In the case of list length, it is useful to distinguish the textual source of Cons-cells which \noccur in the output, e.g. an output list might be of length Ixl, but this is more usefully expressed \nas l'cons R + (N-1)'cons A. Such expressions are obtained by computing the output list length as a polynomial, \nin a way similar to the execution cost analysis. Another class of analysis is concerned with internal \noperations, e.g., the probability of a Boolean-valued procedure returning the value true; such analysis \nis required as auxiliary data in computing computation cost. These are obtained, and the results are \nexpressed using methods similar to those for execution cost. The results of analysis are employed in \ntwo ways. The first method exploits discrepancies between the complexity of a program's output and the \ncomputation cost of obtaining that output. Such discrepancies represent wasted work and the corresponding \nprogram components are therefore targets for simplification. When this singles out a specific portion \nof the program, it provides a sharp criterion. In some cases, this may not be specific enough; to further \nnarrow the target, a second method is employed. This consists of hypothesizing plausible goals on the \nbasis of current performance and using the results of analysis to determine the program components causing \ndiscrepancies between these goals and the computation cost. For example, if a program runs in n 2 steps, \na plausible goal is n steps and the target for simplification is narrowed to those sub-regions responsible \nfor the n 2 behavior. If the goal of n steps is attained, then a second, more stringent, goal will be \ntried, i.e., fewer steps in the best case. Thus, at each step, the plausible goal is taken to be the \nnext significant performance level. This selects as the target for simplification those program components \nwhich must be simplified if that performance level can be attained by our transformation method. We stop \nwhen the computation cost is commensurate with the complexity of the generated output, or when transformations \nfail to attain a goal.  3.2. PARTIAL EVALUATION Partial evaluation consists of rewriting portions of \na function to exploit knowledge of its arguments. In the simplest case, a partial evaluator takes a function \nP of n formal parameters Xl,...,x n along with values al,...,a k for the first k actual arguments and \nconstructs a new function P' such that P'(bk+l,...,bn) = P(al,,...,a k, bk+ 1 ..... b.n ) for all sets \nof bj. That is, P is a variant of P, specialized to the case where the first k parameters are known 158 \n constants. To the extent that P' is a simplified version of P, its computation cost is smaller. Here \nwe employ an extension of this idea: Rather than knowing the actual values of certain formal parameters, \nwe know the function which constructs them. Let Q1,...,Qk be defined functions (of one argument, for \nsimplicity). Our partial evaluator will typically take an expression such as P(Qi(Yl) ..... Qk(Yk), Yk+l,...,yn) \nalong with the definitions of Q1 ..... Qk and construct a new function P' such that P'(bl,...,b k, bk+ \n1 ..... bn) = P(Ql(bl) ..... Qk(bk), bk+ 1 ..... bn). That is, P' is a variant of P, specialized to the \ncase where the first k parameters are known to be computed by Q1 ..... Qk\" To the extent that P' combines \nthe computations of the first k arguments with each other and with the execution of P, P' runs faster \nthan the sequential execution of Q1 ..... Qk followed by P. It is. useful to distinguish four facets \nof partial evaluation: expanding function definitions, distributing conditionals, simplifying, and evaluating \nin context. Expansion replaces a function call by a suitably instantiated copy of the function definition. \nIf P has formal parameters Xl,...,x n, then the complete expansion of P(Ql(Yl),...,Qk(Yk), Yk+l,...,yn) \nis obtained as follows: (i) Let R i be the result of instantiating the body of Qi with argument Yi' for \ni=l,...,k. (ii) In the body of P, substitute R i for x i  (i:l ..... k) and yj for xj (j=k+l ..... \nn). It is undesirable to carry out an expansion of all defined functions (even to one level), since \nthis blows up the program size and makes difficult the recognition of recurring subexpressions needed \nfor subgoal abstraction. Instead, we adopt the following method: defined functions are expanded only \nif they can be reduced to constants or so far as necessary to expose the target for simplification to \nlocal simplification rules. This resolves into three expansion criteria: (El) The function call which \ncontains the target is expanded. (E2) The surrounding function is expanded as necessary to obtain surrounding \ncontext for the local simplification rule. (E3) If all the arguments (and free variables) of a function \ncall are constant then the function call is expanded. Thus, to eliminate a Cons by the local simplification \nrule H Cons(a,fl) -, a, we expand the function call which contains that Cons (criterion El) and also \nexpand the surrounding function call so as to obtain an H operation which may be applied to the Cons \n(criterion E2). Typically, a function expanded in this way contains conditional expressions, e.g., controlling \nrecursion. Suppose such an expanded function occurs as the argument to an outer function, say F. We then \nhave an expression such as F(a, if Pl then e 1 else if P2 then e 2 else en, ~) where a, the conditional \nexpression, and v are the first, second, and third arguments to F. Distributing the conditional consists \nof bringing the conditional tests out of the argument position to the surrounding scope. This yields \na conditional expression in which F is applied to the result of each conditional clause: if Pl then F(a,el,v) \n else if P2 then F(a,e2,T) ... else F(a,en,V) 159 This creates new, specialized function calls-- \nF(a,el,~ ), F(a,e2n, ) ..... F(a,enn, ). Because they are specialized, their further partial evaluation \nmay lead to simplifications To illustrate the steps of partial evaluation, we return to the example of \nSection 2.2 and consider Al(A2(x,y), z). Since the Cons in A 2 is chosen as the target for simplification, \nA 2 is selected for expansion by criterion El. From the definition of A2, we obtain: Al(if Null(x) then \ny else Cons2(Hx, A2(Tx,y)), z)  Distributing the conditionals, we obtain if Null(x) then Al(Y,Z ) else \nAl(COns2(Hx, A2(Tx,y)), z)  The target for simplification, Cons2, is now in an argument position. Expansion \ncriterion E2 selects the surrounding function, A1, for expansion. Thus, Al(COns2(Hx, A2(Tx,y)), z) is \nexpanded into if Null(Cons2(Hx, A2(Tx,y)) ) then z else COnSl(H Cons2(Hx, A2(Tx,y)) , AI(T Cons2(nx, \nA2(Tx,y)), z)) which is ripe for simplification. In regard to simplification, we take the following \noperational point of view. Let la] be the cost 2 of computing expression a. Then expression 8 is simpler \nthan expression a if 16 I < lal for some assi.gnment of values to variables and 161 _< la I for all assignments \nof values to variables. By this criteria, the following are simplifications: Pl A P2 -~ if Pl then P2 \nelse false H Cons(el,e2) -~ e 1 Null(Cons(el,e2) ) -~ false if false then e I else e 2 -~ e 2 if p then \ne else e -~ e Applying these sorts of local simplification rules to the above expression results in \nCOnSl(SX, Ai(A2(Tx,y ), z)) Thus, the partial evaluation of Al(A2(x,y), z) yields if Null(x) then Al(Y,Z \n) else COnSl(HX , -Ai(A2(Tx,y ), z)). We denote this as Al(A2(x,y), z) = if Null(x) then Al(Y,Z ) else \nCOnSl(HX, Al(A2(Tx,y),z))  That is, a = 6 means that a can be partially evaluated to yield 6 and, thus, \nif the computation a terminates then the computation 6 terminates and yields the same answer. An additional \nfacet of partial evaluation, not illustrated by the above example, is evaluation in context. This consists \nof using information derived from conditional expressions to assist in local simplification. Evaluation \nin context arises when an expression embedded within a conditional is selected for expansion--all the \npredicates on test branches leading to that expression are known to be true. Consider, for example, the \nexpression b=M(b,y) where M computes the maximum of the set {b}Uy, as follows: M(b,y) <= if Null(y) then \nb else if b<Hy then M(Hy,Ty) else M(b,Ty) Expanding M(b,y) in b=M(b,y) and simplifying yields if Null(y) \nthen true else if b<Hy then b=M(Hy,Ty) else b=M(b,Ty) where b:b has been simplified to true. Next, we \nexpand the expression b:M(Hy,Ty). In so doing, we can use results of the tests leading to this point, \nso we know: -Null(y) A b<Hy. We call these tests context conditions for the expansion of b=M(Hy, Ty). \nExpanding this in context yields 160 if Null(y) then false else if Hy<HTy then b=M(HTy,TTy) else b=M(Hy,TTy) \n This follows because value of the first conditional expression, b=Hy, can be simplified to false in \nthe context b<Hy. To express the use of context conditions in partial evaluation, we extend the above \nnotation and write a {in context p) : 6 Similarly, we extend the definition of simpler to include context \nconditions: 6 is simpler than a in context p if 161<lal for some assignment of values to variables which \nsatisfies p and [61_<Hfor all assignments of values to variables which satisfy p. 3.3. SUBGOAL ABSTRACTION \nSubgoal abstraction consists of identifying subproblems which are identical in form to more global problems \nand using this identification to construct the definitions of recursive functions. Suppose that a --if \np then e else r(#) when a and fl are expressions and r is an expression involving ft. Suppose that there \nis some substitution 3 e which carries a into fl, i.e., ae=fl. We say that a is the goal, # is the subgoal, \nand fl is a substitution instance of a. Rewriting the above, a = if p then e else r(ae). Let f be the \nset of variables in a. Let F(f) be defined by (3.1) F(~) <= if p then e else r(F(fe)) It follows that \na = F(f). That is, if the computation a terminates then the computation F(~) terminates and their values \nare equal. The reason for introducing such a definition is to obtain a performance improvement. Hence, \nwe construct such a definition F and use it to compute a only when F is computationally simpler than \na. If [a[ _> [if p then e else r(fl)[ for all assignments of values to variables, then [a[ _> [F(~)[; \nif also [a[ > [if p then e else r(fl][ for some assignment of values to variables, then [a[ > IF[t)[ \nfor that assignment; thus, F(~) is simpler than a. In the general ease, comparing the costs of a with \n\"if p then e else r(fl)\" is carried out by analyzing the computational costs of the latter expression. \nSince some of its constituents have been previously analyzed and since the analysis technique reuses \nthe analysis of constituents when dealing with a larger expression in which they are contained, such \nanalysis is generally easier than the original analysis of a. In a commonly occurring case, a cost comparison \ncan be carried out more directly. To explain this, it is necessary to first clarify the relation between \npartial evaluation and computation cost. If a = 6, then 6 must terminate whenever a terminates, but there \nis no assurance that 6's cost is less than \u00a3s. In obtaining 6 from a, two counterposing phenomena are \nat work: (1) Local Simplification tends to make 5 simpler than a. Thus if a contains \"if p' then e' else \ne'\" and this is simplified to e', then 5 will be simpler by the cost of p' plus the cost of an if. (2) \nDuplicating actual arguments when expanding functions tends to make 6 more costly than a when the arguments \nare complex expressions and must be executed more than once. Thus, if a contains F(Cons(G(x)+l, x)) and \nif F is expanded and simplified to \"if P(G(x)) then G(x]+2 else Cons(x, G(x))\" then 6 will be more costly \nsince it executes G(x) twice in the expanded body of F rather than once as the argument to F. 4 Because \nthe combined effect of these two phenomena may be complex, the relation of a to 5 is determined, in the \nmost general case, by analyzing 6. Often, however, the second phenomenon does not occur. After local \nsimplification, the arguments to an expanded function appear at most once on each execution path through \nthe expanded portion of the function body. In 161 such cases, the cost of 6 differs from the cost of \na only insofar as local simplifications may have taken place. If there have, in fact, been any simplifications, \nthen 6 is necessarily simpler than a. If a new function F is defined as specified in (3.1), then F is \nknown to be a better way of computing the same result than a computes. It is important to appreciate \nthat the goal for subgoal abstraction need not be the original top-level problem. In general, the goal \na will be some subexpression which arises in the course of partial evaluation, and the subgoal fl will \nbe the matching subexpression. Let p be the context condition for the evaluation of a. The criteria for \nsubgoal abstraction are: (SA1) a {in context p} : Zx(fl) (SA2) there is a substitution e such that ae=fl \n(SA3) A(fl) is simpler than a in context p (SA4) pO is true at the points in Zx where fl appears The \nlast of these criteria may require further explanation: In order for a subgoal fl to be identical in \nform to a goal a, the context conditions used in partially evaluating a must be true at each appearance \nof /~ in A. We refer to this as cheching the context conditions. An example will illustrate the importance \nthese considerations. Let M be the maximum of (c)Uy defined M(c,y) <: if Null(y) then c else if c<Hy \nthen M(Hy,Ty) else M(c,Ty) Evaluation in context shows that b=M(c,y) {in context b<c} : if Null(y) then \nfalse else if c<Hy then b=M(Hy,Ty) else b:M(c,Ty) We match b=M(Hy,Ty) against b=M(c,y) with the substitution \nO={Hy/c, Ty/y). Checking the context condition b<c under the substitution e requires showing that b<Hy \nat the expression b=M(Hy,Ty) which is true since b<c A c<Hy implies b<Hy. Similarly, we match b=M(c,Ty) \nagainst b=M(c,y) with the substitution {Ty/y}; the context condition, b<c, is easy to check since it \nis unchanged by this substitution. Thus, subgoal abstraction can be carried out. We let F(b,c,y) stand \nfor b=M(c,y) {in context b<c} meaning that F is defined only when its first argument is less than its \nsecond. We have F(b,c,y) <: if Null(y) then false else if c<Hy then F(b,Hy,Ty) else F(b,c,Ty) Simplifying \nthis, F(b,c,y) = false since the only way F can terminate is by returning false. Thus b=M(c,y) {in context \nb<c} = false. This may be read as b<c ~ b#maximum({c}Uy). Stated as a theorem, this is unremarkable. \nHowever, in program optimization, one does not have explicit statements of desirable theorems as input. \nThat the transformation method obtains this directly from the expression and definitions is of interest. \n3.4. GENERALIZATION A somewhat subtle point in subgoal abstraction is the way in which argument positions \nare generalized. We first consider the generalization of constant arguments. Consider an expression of \nthe form F(0, G(x,y)) and suppose that the target for simplification is inside G, so that G is to be \nexpanded followed by a partial evaluation of F. It would not be desirable to uniformly replace all constants \nby new individual variables--here, replacing 0 by some new zi--since F might be simplified in its partial \nevaluation in the case that its first argument is known to be 0. An extreme case would be where F(0, \nG(x,y)) is partially evaluated to a constant. On the 162 other hand, there are cases in which matching \nis blocked by the presence of different constants in the same argument position of a goal expression \nand a subgoal. For example, partial evaluation of F(0, G(x,y)) might lead to an expression in which \nF(Hx, G(Tx,Ty)) appears. Hx cannot be matched against 0, so subgoal abstraction is inhibited. When a \nmatch fails because of the presence of a constant in the goal, generalization is employed: Argument positions \nwhich are constant in the goal and different in the subgoal are replaced by new individual variables. \nThen the generalized goal expression is partially evaluated. If the result is similar to the previous \nresult, the match will succeed. For example, suppose F(O, G(x,y)) is generalized to F(z 1, G(x,y)) and \nthat partial evaluation of this leads to an expression in which F(Zl+HX, G(Tx,Ty)) appears. The match \nis successful with the substitution {Zl+HX/Zl, Tx/x, Ty/y}. A similar situation is caused by the multiple \nappearance of an individual variable in a goal expression. Consider, for example, E(I(k,n), k), where \nthe variable k appears twice. It would be undesirable to adopt the uniform policy of generalizing this \nto E(I(Zl,n ), z2) and then attempting to optimize this, for it might be the case that E(I(Zl,n ), z2) \nhas a simple partial evaluation and subgoal abstraction for Zl=Z 2 but not otherwise. It would be equally \nundesirable to accept only subgoal matches of the form E(I(el,e2), el) since it might be the case that \nno such subgoals occur. Suppose, for example, that the first \"near match\" in the partial evaluation of \nE(I(k,n), k) was the subexpression E(I(k+l,n), k) and that all subsequent near matches had the form E(I(k+j,n), \nk) for j=2,3 .... We adopt the same solution here as for constants: The matching process constructs a \nlist of substitutions. If a match fails because the substitutions for a variable are incompatible, i.e., \nel/x and e2/x where elte2, then the conflicting appearances of the variable in the goal expression are \ngeneralized to distinct individual variables. In summary, the strategy for generalization is to delay \nso doing until required by subgoal abstraction, to generalize as dictated by the match, and then to determine \nthe effect of this generalization by repeating the partial evaluation. An example will show the importance \nof generalization and how generalization interacts with evaluation in context. Consider, for example, \nthe Fibonacci function. F(n) <= if n:0 V n=l then 1 else F(n-1)+F(n-2)  Analysis shows that F(n) takes \nexponential time. We start with the right hand side of the definition. if n=0 V n:l then 1 else F(n-1)+F(n-2) \nAnalysis shows that the cost of F(n-1) is the largest component, so it is selected for partial evaluation. \nExpansion uses the context condition n#0 A n~l. After distributing conditionals and simplifying, the \nresult is if n=0 V n=l then 1 else if n=2 then 2 else 2\"F(n-2)+F(n-3) Taking F(n-1)+F(n-2) as the goal \nand 2\"F(n- 2)+F(n-3) as the subgoal, we attempt to match. This fails, since the constant 2 does not match \nthe implicit constant 1 in l'F(n-l). We generalize the goal to k'F(n-1)+F(n-2). Partially evaluating \nk'F(n- 1)+F(n-2) {in context n~0 A n#l} we get if n=2 then k+l else (k+l)'F(n-2)+k'F(n-3) Again, the \nmatch fails due to a constant argument in the goal and again we generalize. Taking k'F(n-l) + j'F(n-2) \n{in context n#O A n#l} as the goal and partially evaluating, we get if n=2 then k+j else (k+j)'F(n-2)+k'F(n-3) \nThe match is now. successful with the substitution {k+j/k, k/j, n-l/n}. Under this substitution, the \ncontext conditions are true at the calls on F, so we can carry out subgoal 163 abstraction. Let G(k,j,n) \nstand for k'F(n- 1)+j'F(n-2) {in context n\u00a20 A n\u00a21). Then define G(k,j,n) <= if n=2 then k+j else G(k+j, \nk, n-l) Thus, we have that F(n) can be computed as if n=0 V n=l then 1 else G(1,1,n) which executes in \nlinear time. 4. FURTHER EXAMPLES In this section, we present three examples to illustrate particular \npoints of interest. Each example is labeled with the points it illustrates. We confine our exposition \nto a statement of the origina.1 program, the key points of the processing, and the result. Omitted steps \nare either straightforward, or repetitions of points illustrated elsewhere. 4.1. TREATMENT OF WHILE LOOPS, \nSPECIAL- PURPOSE LOCAL SIMPLIFICATION RULES, EVALUATION IN CONTEXT, CHECKING CONTEXT CONDITIONS IN SUBGOAL \nABSTRACTION Consider the while loop (4.1) while P do if Q then R else S where the value of Q is unaffected \nby R and S, i.e., the test is taken in the same direction on the i+l st iteration as on the i th. Let \n~ be a vector of the variables appearing in P,Q,R, or S. It turns out that processing is simplified if \nsuch iterative programs are converted to functional form, viz. F(~), where (4.2) F(~) <= if -P~ then \nelse if Q~ then FR~ else FSf provided that P and Q have no side effects. Let the number of times the \nloop is executed be n. The contribution of Q to the computation cost is [Q['n. Suppose analysis shows \nthis is large so that Q becomes the target for simplification. Since the value of Q is unchanged by S \nand R; we have two special-purpose local simplification rules. (4.3) QR~ -, Q~ and QS~ -~ Q~ Transformation \nproceeds as follows: Start with (4.2) and expand the functions which contain the target for simplification--the \ninner calls on F.  F(~) = if -P~ then else if Q~ then (if ~PR~ then R~ else if QR~ then FRR~ else FSR~) \nelse (if -PS~ then S~ else if QS~ then FRS~ else FSS~) We now proceed to evaluate in context and apply \nlocal simplification rules. Using (4.3), QR~=true in the context Q~=true while QS~=false in the context \nQ~=false. Thus F(~) : if -P~ then else if Q~ then (if -PR~ then R~ else FRR~) else (if ~PS~ then S~ else \nFSS~) Comparing this to (4.2), we have FR~ {in context Q~} = if -PR~ then R~ else FRR~ FS~ {in context \n-Q~} = if -PSi then S~ else FSS~ We can match FRR~ against FR~ with the substitution e={R~/~}. For the \nmatch to succeed, it is also necessary to check that the 164 context conditions of the goal, FRL are \nalso true for the subgoal, FRR~. Here, this requires checking that the context condition, Q~, is true \nafter the substitution 0 at the point where FRR~ is invoked. Since (Q~)e=QRL this is manifestly true. \nMatching FSSf against FS~ is similar. Letting Fr(~) and Fs(~) stand for FR~ and FS~ respectively, we \nhave the following definitions: Fr(\u00a2 ) <= if -PC then ~ else FrR~ Fs(\u00a2 ) <= if -P~ then ~ else FsS ~ \nSubstituting these into (4.2) and converting to iterative form, the final result is: if P then {if Q \nthen (R; while P do R) else (S; while P do S)} The contribution of Q to the computation cost has been \nreduced from n'lQ [ to IQ[, so the goal has been attained. The transformation of (4.1) to this form is \ngenerally termed loop unswitching [3] in compiler optimization. That this is a straightforward application \nof the general transformation method is of interest. The utility of carrying out the derivation in functional \nform should be apparent. 4.2. SUCCESSIVE TRANSFORMATIONS WITH INCREASINGLY STRINGENT PERFORMANCE GOALS, \nTHEOREM PROVING TO ENABLE A LOCAL SIMPLIFICATION RULE We use the following definitions for set membership, \nM, and set union, U, where sets are represented as non-repeating lists: M(b,y) <= if Null(y) then false \nelse if b=Hy then true else M(b,Ty) U(x,y) <= if Null(x) then y else if M(Hx,y) then U(Tx,y) else Cons(Hx, \nU(Tx,y)) Consider the expression M(b, U(x,y)), i.e., bexUy in more conventional notation. Analysis shows \nthat this has a best case time proportional to Ixl, and a worst-case time of Ix[ly[, using Ixl Cons-cells. \nThe result is a Boolean. Thus, the elimination of the Cons-cells is taken as the goal; in particular, \nthe Cons in U is the target for simplification. We start with M(b, U(x,y)), expand the function call \nU which contains the target 'for simplification, distribute the conditional, and partially evaluate the \nthird invocation of M--the only one which simplifies. The result is if Null(x) then M(b,y) else if M(Hx,y) \nthen M(b, U(Tx,y)) else if b=Hx then true else M(b, U(Tx,y)) We have found a subgoal: M(b, U(Tx,y)) in \nits two occurrences can be matched against the original expression M(b, U(x,y)). This allows definition \nof a function F(b,x,y) to stand for M(b, U(x,y)) F(b,x,y) (= if Null(x) then M(b,y) else if M(Hx,y) then \nF(b,Tx,y) else if b=Hx then true else F(b,Tx,y) Analysis shows this has a best-case time of Ixl and a \nworst-case time of IxHyl but now uses no Cons-cells. Our first goal has been attained. This can be carried \none important step further. The worst-case factor of lYl is due to the expression M(Hx,y): This becomes \nthe next target for simplification. We have the local simplification rule if p then e else e -, e This \ncould be employed to eliminate M(Hx,y) if we could exchange the order of the second and third clauses \nof the conditional. Such an exchange is legal so long as the value of the program is not affected, i.e., \nif Pl then e I else if P2 then e 2 else e 3 if P2 then e 2 else if Pl then e I else e 3 provided that \nif Pl is true then P2 terminates and Pl A P2 ~ el=e2 That is, when both predicates apply the values produced \nare identical. To eliminate M(Hx,y), we must prove M(Hx,y) A b=Hx D F(b,Tx,y)=true This is a simple theorem \nand, in fact, has been proved using the program verifier described in [6]. Thus, we have F(b,x,y) <= \nif Null(x) then M(b,y) else if b=Hx then true else F(b,Tx,y) which has constant time in the best case \nand time ]x[+[y[ in the worst case, thus attaining the second goal. Note that the final program is equivalent \nto \"if box then true else bey\", as expected. 4.3. GENERALIZATION, DECOMPOSITION OF COMPLEX EXPRESSIONS \n This example has its origin in the processing of APL. We use several APL operators which may be unfamiliar \nto the reader. For ease of exposition, we restrict usage to scalar and vector arguments and assume that \nall vectors are conformable as required; thus, their definitions are simplified: m the vector 1,2,...,n \nthe concatenation of x with yx,y +\\y the partial sums of y, i.e., the vector (Yl' Yl+Y2'\"\"Yl+Y2+'\"+Yn \n)' ~y the negation of y; the i-th element of the result is 1 if Yi is 0 and 0 if Yi is not 0. xly the \nrood operation. If x is a scalar and y a vector, the result is a vector of elements (Yi mod x). The other \nbinary operations on scalars are extended in the same way. A/y the and-reduction of y, i.e., the logical-and \nof all elements of y. To simplify the discussion and carry it out in the same framework as the other \nexamples, we treat APL vectors as if they were lists. Thus Ln is treated as I(n,1) where ! is the function \ndefined: I(n,k) <= if k>n then nil else Cons(k, I(n,k+l)) The storage expenditure for m is therefore \nn Cons-cells, which is isomorphic--under our representation--to a vector of n elements as expended in \nan actual APL implementation. Suppose y is an array of l's and O's. The following expression, suggested \nby Alan Perlis, tests whether all sequences of l's are of even length: A/N21(-y,0)/+\\y,0. This may be \nread, from right to left as: consider the vector y concatenated with a 0; form a vector of partial sums; \nselect from that vector all elements whose corresponding element in the vector (y,0) is 0; take the remainders \nof that vector when divided by 2; construct a vector whose i-th element is 1 or 0 as the i-th remainder \nis x/y the compression of y by x, i.e., 0 or 1; form the logical-and of all elements. selects those elements \nYi such That logical-and will be true if and only if all that x.=l and forms a new vector sequences of \nl's in y are of even length. This of the 1 selected elements. constructs seven temporary arrays and makes \neight passes over y and the temporaries. In our Lisp notation, this is written as (4.4) R(N(E(C(N(A(y,Cons(0,nil))), \nS(0, A(y,Cons(0,nil))))))) where A(x,y) <= if Null(x) then y else Cons(Hx, A(Tx,y)) S(k,y] (= if Null(y) \nthen nil else Cons(k+Hy, S(k+Hy,Ty)) N(y) <: if Null(y) then nil else Cons(Not(Hy), N(Ty)) C(x,y) <= \nif Null(x) then nil else if Hx=0 then C(Tx,Ty) else Cons(Hy, C(Tx,Ty)) E(x) <: if Null(x] then nil else \nCons(Hx mod 2, E(Tx)) R(x) <= if Null(x) then true else if Hx:0 then false else R(Tx) Optimization proceeds \nfrom the inside out: transforming argument expressions, substituting the transformed arguments in place \nof the original ones, and using these in transforming enclosing operations. We begin with S(0, A(y,Cons(0,nil))), \nwhich corresponds to +\\y,0. Analysis shows that this computation takes 2(lyl+1) Cons-cells, that the \nlength of the output is ly]+l, and that the cells constituting the output come from S. The executions \nof Cons in A are being wasted. These become the target for simplification. Taking S(0, A(y, Cons(0,nil))), \nexpanding A and partially evaluating S, results . in an expression containing S(Hy, A(Ty, Cons(0,ni]))). \nThe first occurrence of 0 in the goal must correspond to Hy in the subgoal, so the match fails. Generalization \nof the first argument position to a new individual variable k is required. Observe that the second occurrence \nof 0 in the goal matches a 0 in the subgoal, so this is unaffected by generalization. After generalization, \nthe new expression under consideration is S(k, A(y, Cons(0,nil))). Expanding A, partially evaluating \nS, and abstracting on a subgoal, results in S(k, A(y, Cons(0,nil))) = F(k,y) where F is defined: F(k,y) \n<= if Null(i) then Cons(k,nil) else Cons(k+Hy, F(k+Hy,Ty)) This requires lyl+l calls of Cons to yield \na result of lyl+l new cells so the goal has been attained. Next, F(0,y) is substituted for S(0, A(y, \nCons(0,nil))) in (4.4) and the decomposition process is repeated. The second expression for optimization \nis N(A(y, Cons(0,nil))). Again, analysis shows that the executions of Cons in A are being lost. Transformation \nyields G(y), where G(y) <= if Null(y) then Cons(1,nil) else Cons(Not(Hy), G(Ty]) Next, the expression \n\" C(G(y), F(0,y)) is considered. Analysis Shows that the executions of Cons in both G and F are being \nlost. Transformation yields H(0,y), where H(k,y) <= if Null(y) then Cons(k,nil) else if Hy#0 then H(k+Hy \nTy) else Cons(k+Hy, H(k+Hy, Ty)) Successive steps consider E(H[0,y)), then N of that result, and then \nR of that result. In all, the optimization process is carried out six times. The final program is J(0,y), \nwhere J(k,y) <: if Null(y) A k rood 2~0 then false else if Null(y) then true else if Hy#0 then J(k+Hy, \nTy) else if k mod 2~0 then false else J(k,Ty) which, constructs no temporary arrays and makes, at most, \na single pass over y. This may be directly transformed to the iterative program: k~-0; while ~Null(y) \ndo {if Hy#0 then (k~-k+Hy; y~Ty) else if k mod 2#0 then return false else y~Ty}; if k mod 2#0 then return \nfalse else return true the program expansion steps during partial evaluation seems to be a natural and \nuseful technique. Another contribution is the use of context conditions in partial evaluation to establish \nenabling conditions for local simplifications and, associated with this, the checking of context conditions \nin subgoal abstraction. A third contribution is the treatment of generalization in subgoal abstraction. \nDelaying generalization until required and then generalizing as dictated by the match appears to be a \npromising approach. 5. CONCLUSION 5. I. RELATION TO OTHER WORK Mechanical program analysis is discussed \nin [23]. An interactive system which provides assistance to the analyst-user in estimating program efficiency \nis discussed in [10]. A number of partial evaluators have been implemented for various purposes [8,17,20]. \nA good survey of partial evaluators and their applications may be found in [5]. Program transformations \nwhich preserve correctness with respect to given assertions are discussed in [13]. The notion of loop \nexpansion followed by - abstraction to obtain a computational advantage is discussed in [15], in the \ncontext of generating efficient code for machines with parallel operation capabilities. More recently, \n[7] and [18] have. employed the idea that a recursive function call can be formed when, in the course \nof working on a problem, a subgoal is generated that is identical in form to the top-level goal. The \nuse of transformations--\"beating\" and \"drag-along\"--to optimize the execution of APL programs is discussed \nin [1]. Further studies in the optimized interpretation of APL expressions are presented in [4]. The \nmajor contribution of this work is in the use of program analysis to direct the transformation process. \nUsing analysis and performance goals to select a target for simplification, and then using this to direct \n 5.2. PROSPECTS While the techniques we have presented can yield some interesting results, it would \nbe a mistake to overestimate their capabilities. They are limited in effect to the transformation of \none program to a better one. Cases in which the input/output mapping can be better realized by a radically \ndifferent algorithm are beyond the scope of this method. For example, we can see no way to transform \na definition of bubble-sort to a version of quick- sort. Where change of algorithm is required, program \nsynthesis [18] from input/output specifications appears to be a more natural way to proceed--particularly, \nif such synthesis were guided by considerations derived from mechanical program analysis [23]. Even within \nthe province of these techniques, there are notable lacuna which invite further investigation. As an \nexample, consider a variation on Example 2.3.2; an APL expression which counts the number of primes less \nthan n is: +/2=+/[1]0=(m)o.lm. (Read this from right to left as: Consider the n by n matrix obtained \nby considering the remainders of all pairs of elements from the arrays (1,2,...,n) and (1,2,...,n); test \nfor equality of the remainders with 0 and form a new matrix of the test results; sum the columns of the \nresulting matrix; test for equality of the sums with 2; count the number of columns whose 168 sum is \nexactly 2.) This constructs two n by n temporary matrices and two vectors of length n. The result is \na scalar. Analysis and subsequent transformation yields M(n,1) where M(n,k) <: if k>n then 0 else if \nH(1,n,k,2) then l+M(n,k+l) else M(n,k+l) H(j,n,k,s) <: if j>n then s:0 else if k mod j=0 then H(j+l,n,k,s-1) \nelse H(j+l,n,k,s)  This achieves a considerable storage economy since it constructs no temporary matrices \nor vectors. However, two defects remain. First, the function H(j,n,k,s) does not terminate until j>n, \nwhich requires time n. Inspection shows that if s is ever negative then H must be false. Thus, inserting \na leading test, \"if s<0 then false\", would leave the input/output mapping unchanged but typically lead \nto a performance improvement. Second, the test j>n can be sharpened to j>k, since k mod jr0 for j>k. \nHowever, we can find no entirely satisfactory set of transformations that would lead to these changes: \nIn the interest of brevity, we explain APL only to the extent required for understanding the examples. \nThe original definition by Iverson is given in [16]; a description of the APL system may be found in \n[19]. Our notation for APL expressions differs from the APL system's in that we use lower case :letters \nfor variable names. There should be no confusion between lal to denote the cost of computing the expression \na and !x I to denote the length of the list x. Context and the argument type will indicate which is intended. \nThe following usages are standard in formal logic. A substitution is a set of the form {ei/~ili=l ..... \nn} where the ~i are non-repeating variables and the e i are expressions. Let 0 be a substitution and \na an expression. Then aO is the expression obtained from a by simultaneously replacing each occurrence \nof v i by e i. It should be pointed out that there are evaluation techniques [22] which defer evaluation \nof arguments until they are needed and store the result so an argument is evaluated at most once. However, \nsuch techniques require that the argument be used in the body exactly as it appears as an actual parameter. \nIn partial evaluation, we wish to carry out simplifications, e.g., H(Cons(G(x)+l, x))-I -~ G(x), so that \nsuch techniques are not directly applicable. More recent studies [14] show promise of being extendable \nto such situations, but additional research seems required to clarify the relation between deferred evaluation, \nlocal simplification, and function expansion. ACKNOWLEDGMENTS My interest in the technique of partial \nevaluation coupled with subgoal abstraction was stimulated by enjoyable discussions with Rod Burstall \nand John Darlington; these discussions raised several of the questions answered in this work. Examples \n2.3.1 and 4.1 are due essentially to Edsger Dijkstra who presented [11] the versions before and after \ntransformation as being \"hard to compare\"; the challenge afforded by these examples was a motivating \nforce in exploring the techniques presented here. Several of the APL examples were provided by Alan Perlis; \nour ongoing discussions as to the relative merits of transformations vs. his \"ladder\" evaluation techniques \nfor APL have been provocative and fruitful. REFERENCES 1. Abrams, P. An APL Machine. SLAC-14, Stanford \nLinear Accelerator Center, Feb. 1970. 2. ACM Sigplan Symp. on Very High Level Languages. Sigplan Notices, \n9, 4, April 1974. 3. Allen, F.E. and Cocke, J. A catalogue of optimizingtransformations. In R. Rustin \n(Ed.) Design and Optimization of Compilers, Prentice-Hall, 1972, 1-30. 4. Battarel, G. et al. Optimized \ninterpretation of APL statements. In P. Gjerlov, H.J. Helms, and J. Nielsen (Eds.) APL Congress 73, North-Holland, \n1973, 49-57. 5. Beckman, L. et al. A partial evaluator and its use as a programming tool. Dept. of Computer \nSciences, Uppsala University, Sweden, Nov. 1974. 6. Boyer, R.S. and Moore, J.S. Proving theorems about \nLisp functions. JACM, 22, 1 (Jan. 1975), 129-144. 7. Burstall, R.M. and Darlington, J. Some transformations \nfor developing recursive programs. Int. Conf. on Reliable Software, IEEE Computer Society, April 1975, \n465-  472. 8. Chang, C.L. and Lee, R. Symbolic Logic and Mechanical Theorem Proving. Academic Press, \nNew York, 1973. 9. Cheatham, T.E. and Wegbreit, B. A laboratory for the study of automating programming. \nAFIPS Conf. Proc., Vol. 70 (Spring 1972), 11-21.  10. Cohen, J. and Zuckerman, C. Two languages for \nestimating program efficiency. CACM, 17, 6 (June 1974), 301-308. 11. Dijkstra, E.W. Notes on structured \nprogramming. In O.J. Dahl, E.W. Dijkstra, and C.A.R. Hoare (Eds.) Structured Programming, Academic Press, \n1972, 1-82. 12. Earley, J. High level operations in automatic programming, in [2].  13. Gerhart, S.L. \nCorrectness preserving program transformations. Second ACM Symposium on Principles of Programming Languages, \nJan. 1975, 54-66. 14. Henderson, P. and Morris, J.H. A lazy evaluator. CSL, Xerox Palo Alto Research \nCenter, August 1975. 15. Holt, A.W. et al. Final report for the information system theory project. Applied \nData Research, Inc., New York, Feb. 1968. 16. Iverson, K.E. A Programming Language. John Wiley and Sons, \nInc., 1962. 17. Lombardi, L.A. and Raphael, B. Lisp as the language for an incremental computer. In \nE.C. Berkeley and D.G. Bobrow (Eds.) 18. Manna, Z. and Waldinger, R. Knowledge  The Programming Language \nLISP: Its Operation and Applications, MIT Press, Cambridge, 1964, 204-219. and reasoning in program \nsynthesis. Stanford Research Inst., Menlo Park, CA, Nov. 1974. 19. Pakin, S. APL/360 Reference Manual. \nScience Research Associates, Inc. 1968. 20. Sanderwatl, E.A. Programming tool for management of predicate-calculus-oriented \ndata bases. Proc. Second Int. Joint Conf. on Artificial Intelligence, British Computer Society, 1971. \n 21. Schwartz, J.T. Automatic and semiautomatic optimization of SETL, in [2]. 22. Vuillemin, J. Correct \nand optimal implementations of recursion in a simple programming language. JCSS, 9, 3 (Dec. 1974), 332-353. \n 23. Wegbreit, B. Mechanical program analysis. CACM, 18, 9 (Sept. 1975), 528-539.  170  \n\t\t\t", "proc_id": "800168", "abstract": "<p>Program development often proceeds by transforming simple, clear programs into complex, involuted, but more efficient ones. This paper examines ways this process can be rendered more systematic. We show how analysis of program performance, partial evaluation of functions, and abstraction of recursive function definitions from recurring subgoals can be combined to yield many global transformations in a methodical fashion. Examples are drawn from compiler optimization, list processing, very high level languages, and APL execution.</p>", "authors": [{"name": "Ben Wegbreit", "author_profile_id": "81100646452", "affiliation": "", "person_id": "PP31051406", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/800168.811549", "year": "1976", "article_id": "811549", "conference": "POPL", "title": "Goal-directed program transformation", "url": "http://dl.acm.org/citation.cfm?id=811549"}