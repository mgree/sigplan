{"article_publication_date": "01-01-1976", "fulltext": "\n MATHEMATICAL SEMANTICS and DATA FLOW PROGRAMMING Paul R. Kosinski Project MAC Massachusetts Institute \nof Technology Cambridge, Massachusetts 02139 and IBM Thomas J. Watson Research Center Yorktown Heights, \nNew York 10598 Abstract A Data Flow program [1,2] is a flow- chart like network of operators which \ncompute concurrently, dependent only on the availability of the data which flow along the paths. Each \noperator has only a local effect, transforming input data to output data. Although operators may ex- \nhibit memory and thus not be functional from an input to an output, all operators are functions from \ninput sequences to output sequences. This plus the strong locality of effect allows mathematization of \nsemantics more readily than traditional programmzng languages which are burdened with omnipresent storage \nand occasional GQTO's. This paper proves the semantic behavior of some elementary Data Flow pro- grams \nand proves that certain optimization transformations preserve other behaviors. Background In the past \nseveral years, the math- ematical specification of programming language semantics has been much investi- \ngated. There have been two main lines of attack on this problem, the axiomatic ap- proach taken by Hoare \n[3], and the func- tional approach taken by Scott [4] and Strachey. In the axiomatic approach, each \nprimitive operation in the programming language has assigned to ~t one or more 1 axioms which formally \nspecify the effect that the operation has upon the state of the abstract machine when that operation \nis executed. That is, the axioms describe the mathematical relationship between the \"before\" state and \nthe \"after\" state. A sequence of operations have an effect which is the composition of the individual \nrelations for the component operations. Thus, given a program together with a putative set of axioms, \none can determine by theorem proving (manual or automatic) whether the program does indeed satisfy its \naxiomatization. Alternatively, one can derive a theorem which describes the program's behavior. In the \nfunctional approach, each primitive operation is assumed to compute a particular function. Thus, a sequence \nof operations computes the function which is the composition of the component op- erations' functions. \nIf the operations are performed repeatedly, as in a WHILE loop, the composite function is not so easily \ndetermined (in the axiomatic ap- proach, an inductive proof is needed). Setting up the functional equation \ncorres- ponding to the loop, one gets F(X) = IF Test(X) THEN F(Body(X)) ELSE X where Test is the predicate \nof the WHILE, Body is the function computed by the body of the loop, and F is the function com- puted \nby the loop as a whole. This is a recursive definition, but it is hard to solve because the unknown \nis the function F. Such equations can be solved in certain circumstances by means of the Y or fixed \npoint operator. Scott's contribution has been to show that there exist lattices called reflexive domains \nin which the Y operator can always apply to give the unique minimal fixedpoint solution of such equations, \nand that such domains adequately characterize programming languages. This approach can be applied to \nap- plicative languages with relative ease since such languages are based on the idea of functions and \nfunction composition. Unfortunately, applicative languages are seldom used for programming, even LISP \nhas nonapplicative operators such as the GO, SETQ and RPLACD. The effect of such operators is to make \nthe functional char- acterization of the program depart consid- erably from the syntactic structure of \nthe program. This occurs for two reasons. First, since some operators, such as assignment (eg. SETQ or \nworse, RPLACD) , change the state of the whole abstract machine, the function corresponding to such \nan operator must transform states into states. Then, in order to be com- posable, all operators must \ntransform states whereas the program is written as if most operators transform variables. Second, control \nflow operators (of which LISP's GO is a mild example) can cause both the conditional and the loop structure \nof the program to become arbitrarily com- plicated. Structured Programming, with insistence on .a limited \ndisciplined set of control operators (IF-THEN-ELSE and DO-WHILE) prevents the second problem from occurring, \nthat is, one recursive equation corresponds to one loop. The first problem remains however, since most \nexisting languages have state transforming assignment operators. Data Flow Semantics DFPL, a Data Flow \nProgramming Language [2], has the basic mathematical simplicity of applicative languages without most \nof their drawbacks. Operators in DFPL func- tionally transform their inputs to their outputs without \never affecting the state of the rest of the program. Since there is no control flow, there is no GOTO; \nin spite of this, loops may be programmed as well as recursion. Most significant though, is the fact \nthat unlike ordinary applicative languages, programs may exhibit memory behavior, that is, the current \noutput may depend on past inputs as well as the current input. Memory in DFPL is not primitive but \n is programmed like other nonprimitive operators. Thus its effects are local like those of other operators \nand it does not permeate the semantics of programs. A DFPL program is a directed graph whose nodes are \noperators and whose arcs are data paths. Data in DFPL are pure values, either simple like numbers or \ncom- pound like arrays or records. There are no addresses in DFPL, although certain opera- tors may be \nprogrammed to interpret input values in a manner reminiscent of addresses. An operator \"fires\" when its \nrequired inputs are available on its incoming paths. After a variable amount of time, it sends its outputs \non its outgoing paths. It is not necessary that all inputs be present before an operator fires~ it depends \non the partic- ular operator. Similarly, not all outputs may be produced by a given firing. Synchro- \nnous operators fire only when all their inputs are present, and produce their outputs all at once, \nthey are analagous to subroutines. Some operators produce a time sequence of output values from one \ninput 176 value or conversely, they are analogous to coroutines. The operators in a DFPL pro- gram \nthus operate in parallel with one another subject only to the availability of data on the paths. An \noperator may either be primitive or defined. An operator is defined as a network of other operators which \nare connected by data paths such that certain paths are connected on one end only. These paths are the \nparameters of the defined operator. A defined operator operates as if its node were replaced by the network \nwhich defines it and the parameter paths spliced to the paths which were connected to that node. Thus, \nrecursive operators may be defined. Sufficient synchronization signals are passed with the data on the \npaths so that operators do not fire prematurely, and so that the operation of the program as a whole \nis independent of the timings of the component operators (at least in basic DFPL, full DFPL allows timing \ndependent programs in order to cope with the real world, but it is not yet possible to mathematize the \nsemantics). Fortunately , the synchronization mechanism is implicit in the mathematization presented \nhere. There are six primitive operators in DFPL shown in Figure i. Of these, three are simple in their \nbehavior: the Constant, the Fork and the Primitive computational function (Pcf). This latter is really \na whole class of opegators in- cluding the usual arithmetic, logical and aggregate operators (eg. construct \nand select). These three operators all have the property that they demand all their inputs to fire, whereupon \nthey produce all their outputs (the constant is a degenerate case having no inputs). Furthermore, each \nfiring is independent of any past history, that is, the operator is a function from current input to \ncurrent output. The functional equations for the operators in Figure 1 are thus: X = C for the Primitive \nconstant, X = Fx(u,V,W) &#38; Y = Fy(U,V,W) for the Pcf F, X = U &#38; Y = U &#38; Z = U for the Fork. \n The next most complicated operators are the Switch operators, also shown in Figure i. These two operators \nalso have the property that each firing is indepen- dent of previous firings, but not all inputs/outputs \nare demanded/produced upon each firing. The Outbound Switch, for example, demands C and U as inputs for \neach firing, but only one of X, Y and Z receives output. Which one is determined by the value received \non input C. The output value is just the value of U. The Inbound Switch operates conversely, only one \nof the inputs X, Y and Z is accepted upon firing (C is demanded), and its value is always sent out \non U. Since these operators sometimes do not accept/produce inputs/outputs, we can not describe their \nfunctional behavior by such simple equations as before (not producing an output is not the same as producing \na null output). But we can describe their behavior if we view them as functions from sequences of inputs \nto sequences of outputs. Now the functional equations for both kinds of Switches are (one origin indexing \nis assumed): U* = Inswitch(C*,X*,Y*,Z*) and X* = Outswitchx(C*,U*) &#38; Y* = Outswitchy(C*,U*) &#38; \n Z* = Outswitchz(C*,U*) where X k = U 3. if C 3.=1 &#38; k=#{i~jlCi=l} YM = Uj if Cj=2 &#38; k=#{i~jlCi=2} \n Z k = Uj if Cj=3 &#38; k=#{i~j I Ci=3} The notation #{i~jICi= q} means the number of times the length \nj prefix of C* takes on the value q. 177 Thus, roughly speaking, the Inbound Switch merges two or more \nsequences into one sequence the same length as the control sequence. Conversely, the Outbound Switch \nsplits a data sequence into two or more sequences dependent on the values in the control sequence. In \nall cases, the order of the input sequence(s) is preserved in the output sequence(s). The most complicated \nprimitive opera- tor is the Loop, shown in Figure 1 also. The Loop provides the DFPL analog of the standard, \nleading test, WHILE loop of ordinary programming languages. The Loop operator also has the property that \nit does not accept~produce all of its inputs/ outputs each time it fires. Its firing, however, is a two \nstage process that introduces a \"phase shift\" of one unit in mapping input sequences to output sequences, \nthus allowing construction of iterative loops and even an analog to memory or storage in conventional \n languages. The four paths connecting to the Loop in Figure 1 can be characterized as fol- lows. X is \nthe initialization value, Y is the current iteration value, Z is the feedback value Which becomes current \non the next iteration, and C is the control value which tells the Loop whether to stop or take another \niteration. Although other Loops can be imagined, such as one having a final output value, they can all \nbe programmed from this minimal Loop plus the primitives above. The precise func- tional equation for \nthis Loop is: Y* = Loop(X*,C*,Z*) where Y1 = Xl Yk = Zk-i if Ck_l=l &#38; k>l Yk = Xj+l if Ck_l=0 &#38; \nk>l &#38; j=# {i<k I Ci=0} Viewed over \"time\" (the columns), the Loop operates as follows (the value \ncarried on a path appears under its name if appropriate). X 1 ...... X 2 .... \"\" Y1 Y2 Y3 \"\" Y4 Y6 \n .. =X 1 =Z 1 =Z 2 .. =X 2 =Z 4  . Z 1 Z 2 Z 3 .. Z 4 Z 5  . C 1 C 2 C 3 .. C 4 C 5 .. =i =i =0 .. \n=I =i  Now the first three operators can be recast as functions from sequences of inputs to sequences \nof outputs: X 1 = C for the Primitive constant X i = Fx(Ui,Vi,Wi) &#38; Yi = Fy(Ui'Vi'Wi) \u00a5i for the \nPcf F, X. = U. &#38; Y. = U. &#38; Z. = U. \u00a5i 1 1 1 1 1 1 for the Fork. A synchronous operator S \nis defined as one whose function is such that Y. = S(X~) 3 3 where X~ = first j elements of X*, ie. \n 3 there is one output for each input but that output may depend on past inputs also. This property \nof synchronous operators allows us to avoid the tedium of using a separate index for the sequence of \nvalues on each data path. All paths in a subnetwork of synchronous operators may share the same sequence \nindex since that subnetwork behaves like a single synchronous operator. In general, any operator constructed \nentirely out of synchronous operators is itself synchronous and the Fork and all Pcf oper- ators are \nsynchronous. All primitive operators are causal in the sense that an output cannot be affected by future \ninputs, that is, once an output is produced, it cannot be changed. More precisely, if Y* = F(X*) &#38; \nX~ = F(X~) &#38; YZ* = F(X~) &#38; j~i then \u00a3~k. Optimization One can prove that natural adaptations \nof optimization transformations [5] preserve the functionality of certain DFPL programs. 178 For example, \nin Figure 2 we see the appli- cation of common subexpression elimination. The Before and After program \ncompute the same function for any operator F. Referring to the \"Before\" operator definition in Figure \n2, we see that X'2 &#38; Yi \u00a5i: X i = Xi' = l = Y!i = Y'' &#38; i Z I. = Z!1 = Z?l by the definition \nof the Fork operator. Hence, X* = X'* = X\"*, Y* = Y'* = Y\"* and Z* = Z'* = Z\"*, so V* = Fv(X*,Y*,Z*), \nW* = FW(X*,Y*,Z*) , V'* = Fv(X*,Y*,Z*) and W'* = Fw(X*,Y*,Z*). Therefore, V* = V'* and W* = W'*. By similar \nreasoning, in the \"After\" operator definition of Figure 2, V* = V'* = Fv(X*,Y*,Z*) and W* = W'* = Fw(X*,Y*,Z*). \nThus, the two operators are equivalent for any operator F. Since Forks have such simple function- al \nproperties, we will henceforth omit them as explicit operators in our proofs and just label all paths \nconnected to a Fork with the same symbol. In Figure 3, we see the application of \"hoisting\", that is, \nmoving a compu- tation out of a conditional expression. The operator F is moved to the front of the conditional, \nand the operator G is moved to the rear. For this optimization to apply, it is sufficient for F and G \nto be simple functions of their inputs (eg. Pcf's), that is Vi: H i = F(Ai,B i) &#38; Z!l = G(Li)\" \nTo prove this optimizationi we shall assume that D and E are synchro- nous operators and that A, B, C \nand M are mutually synchronized input paths so that we can use the same index for all of them. If these \nassumptions were not valid, the network would hang up. The proof consists of three parts, first show \nthat R = R' &#38; S = S', second show V = V' &#38; W = W' using the obvious result that U = U' &#38; \nT = T', and third show that Z = Z'. We will prove the first part in fair detail: the second part is \nobvious and the third is just like the first. I. Hj = F(Aj,Bj) by assumption. 2. R~ = Hj if Cj=l &#38; \nk=#{iSjlCi=l} by definition of Outswitch.  3. R~ = F(Aj,Bj) if C3 .=i &#38; k=#{iSj ICi=l} by 1 and \n2 above.  4. Pk = Bj if Cj=i &#38; k=#{iSj[Ci=l}  N k = Aj if Cj=i &#38; k=#{i~j l Ci=l} both by definition \nof Outswitch.  5. R k = F(Aj,Bj) if Cj=i &#38; k=#{i JICi=l} by assumption for F and 4 above.  6. R \nk = R{ QED.  v Similarly, we can prove S k = S k, thus concluding the first part of the proof that \nhoisting preserves the semantics. The first and third parts of this proof stand as separate theorems \nin themselves. They would not often be used however, because unbalanced Switch operators (ie. an Inswitch \nwithout an Outswitch or vice- versa) would rarely be used in programs. Memor~ The most interesting \nkind of DFPL operator is one which behaves like a mem- ory cell. A trivial kind of memory cell, which \nserves as the building block for fancier ones, is shown in Figure 4. It is just a holding station, that \nis, the out- put is what the input was on the previous firing. More precisely, it can be shown to satisfy \nthe following equations: Y1 = Q &#38; Yi = Xi-i \u00a5i>i . The proof is straightforward: i. W 1 = Q by \ndefinition of the Primitive constant. 2. Y1 = W1 by definition of the Loop.  3. Yk = Xk-i if Zk_l=l \nby definition of the Loop operator.  4. Z k = True(X k) = 1 Yk by definition of the Pcf True.  5. Y1 \n= Q by 1 and 2 above.  6. Yk = Xk-i Yk>l by 3 and 4 above, QED.  1 79 A fancier memory cell is shown \nin Figure 5. When a 0 value is presented on the control path C, the current contents is read out on path \nY, when a 1 value is pre- sented on C and a data value is presented on the input path X, the cell is \nupdated to contain that new value. The cell has an initial contents of Q. Viewed over time, the Mem operator \nbehaves as follows: C1 C2 C3 C4 c5 c6 c7 =0 =0 =i =0 =i =i =0 Y1 Y2 --Y3 .... Y4 =Q =Q .. =R ..... \nT .... x 1 .. X 2 x 3 .. .... =R .. ~S =T .. The precise formulation of this behavior may be proved \nto be: Y. = Q if vi~j: C.=0 ] z Yj = X k if Cj+k=0 &#38; k=#[i< j+klCi=l} The proof of this follows: \n 1. A 1 = Q by definition of Hold.  2. A\u00a3 = BZ_ 1 V\u00a3>i by definition of Hold.  3. A~ = Yj if C~=0 &#38; \nj=#{i~\u00a31Ci=0 } by definition of Outswitch.  4. A\u00a3 = Wk if C\u00a3=l &#38; k=#{iS\u00a31Ci=l } by definition of \nOutswitch.  5. B Z = Yj if C~=0 &#38; j=#{i~\u00a31Ci=0 } by definition of Inswitch.  6. B\u00a3 = Z k if CZ=i \n&#38; k=#{i~\u00a31Ci=l } by definition of Inswitch.  7. Z k = X k by definition of ! operator.  8. B\u00a3 \n= B\u00a3_ 1 if C\u00a3=0 by ~,\u00b0 3 and 5 above. 9. A\u00a3+ 1 = A\u00a3 if C\u00a3=0 by 8 and 2 above. i0. Yj = Q if Vi~j: \nCi=0 by induction on i, 3 and 9 above. ii. Yj = Bj+ k if Cj+k=0 &#38; j=#{i<j+kICi=0} by 5 above. 12. \nBj+ k = Bj+k_ m if \u00a50<i_~m: Cj+k_i=0 by induction on 8 above.  13. Yj = Bj+k_ m if [~0<iSm: Cj+k_i=0 \n] &#38; j=#{i~j+klCi=0} by ii and 12 above.  14. Bj+k_ m = X k if Cj+k~m=l &#38; k=#{i~j+klCi=l} by \n6 and 7 above. = X k if C.. =i &#38; [V0 i m. 15 Yj 3 +K-m . Cj+k_i=0] &#38; j=#{iSj+klCi=0} &#38; \nk=#{i~j+k~Ci=l } by 13 and 14 above. 16. Yj = X k if Cj+k=0 &#38; j=#{i~j+klCi=0} &#38; k=#{i~j+klCi=l} \nfrom 15 above, by simplifying the if condition, making use of the fact that m is arbitrary in the range \n1 to j+k-l.  17. Yj = X k if Cj+k=0 &#38; k=#{i<j+klCi=l} from 16 above, since Cj+k=0 &#38; k=#{iSj+kICi=l} \nimplies that  k=#{i<j+klCi=l} &#38; j=#{i~j+klCi=0 }. Steps i0 and 17 above are the desired results \nfor the behavior of the memory cell. More complicated memories may be programmed by substituting other \noperators for the Fork and ! operators in Figure 4. For example, by replacing the Fork by a Dequeue operator, \nand the i by an Enqueue, a queue memory results. To program a random access memory, another input path, \nto carry the \"address\", must be added, as well as replacing the operators. Latticework To make our \ndomains and codomains of data value sequences into lattices, we have to define a partial order on them. \nFollowing G. Kahn [6], we say that a sequence A* is \"bigger\" than a sequence B* if and only if B* is \na prefix of A*. The set of sequences (including countably infinite ones) form a lattice under this partial \norder. The \"bottom\" of this lattice is the empty sequence. This lattice does not encompass the Scott \nnotion of value approximation, that requires further investigation. The operator obtained by connecting \nthe output of the Hold operator to a two way Fork, connecting one Fork output back to the Hold input, \nand making the second 180 Fork output the output parameter of the defined operator, is the Repeating \ncon- stant operator. It is characterized by the equation: X i = Q ~i. That it satisfies this equation \ncan be proved inductively as above. Another way of proving it is to use the lattice fixed-point approach. \nTo do this, we note that our earlier notion of causality exactly corresponds to mono- tonicity in the \nlattice. We make the fur- ther assumption of continuity, which cor- responds to the reasonable assumption \nthat an operator will produce output after a finite sequence of inputs or not at all. Then, referring \nto our previous description of the Hold operator as a function which transforms any input sequence to \nan output sequence which is the initial constant prefixed to that input sequence, we see that the minimal \nfixed-point of this func- tion is the infinite sequence of that constant value. A more detailed proof \nof an almost identical situation appears in G. Kahn [6]. DFPL currently does not allow operator valued \ndata and thus does not require the existence of Scotts reflexive domains. In spite of this, DFPL allows \niterative and recursive programs, both in the prac- tical and mathematical senses. It is hoped that DFPL \ncan be extended to allow operator valued data in the near future, and that this extension can be mathematized \nwith Scott's techniques. Conclusions We have shown that it is possible to develop a mathematical semantics \nof DFPL in terms of functions from sequences of inputs to sequences of outputs. This mathematization \nis not complicated by the omnipresence of memory, because memory is local like all other operators, nor \nby the presence of control flow, which leads to \"continuations\". The necessity for dealing with input \nand output sequences is not all bad: many programs (such as database sys- tems) are inherently non-terminating, \nand cannot be reasonable viewed as simple func- tions from an input to an output. As it currently stands, \nDFPL has a primitive operator which is indeterminate, or timing dependent, in its operation. It would \nbe extremely desirable if it could be characterized as a mathematical function also. To do this would \nprobably require redefining the functions to take datum/time pairs as values, thus compli- cating the \nentire system of axioms, theo- rems and proofs. The theorems of particular interest in this new system \nwould be those which show that certain defined operators, although indeterminate in their internal operation, \nare completely determinate when considered as atomic operators. Then, those parts of a program which \nhave to be in- determinate in order to deal with the out- side world could be so, whereas other parts \nof the program could be determinate and thus simpler to analyze. This duality of determinate operators \nand indeterminate operators suggests the need for a convenient transformation be- tween them. If DFPL \nprograms are viewed as an algebra, then morphisms between such algebras might be defined. In fact, the \nprocess of compiling one DFPL program into another (with simpler operators) can be analyzed as a particular \nmorphism. Hopefully, the approaches set forth in this paper will yield a practical applicability of \nmathematical semantics to more realistic programs than heretofore possible. References i. J.B. Dennis, \n\"First Version of a Data Flow Procedure Language\". MIT Project MAC, Computation Structures Group, Memo \n93 (1973). 181 2. P.R. Kosinski, \"A Data Flow Programming 5. F.E. Allen and J. Cocke, \"A Catalog of \n Language\", IBM Research Report RC 4264 Optimizing Transformations\", IBM Research (March 1973). Report \nRC 3548 (September 1971).  3. C.A.R. Hoare, \"An Axiomatic Basis for 6. G. Kahn, \"A Preliminary Theory \nfor Computer Programming\", Comm ACM 12, Parallel Programs\", IRIA Laboratory pp 576-583 (October 1969). \nReport 6 (January 1973).  4. D. Scott, \"Outline of a Mathematical Theory of Computation\", Proceedings \nof the Fourth Annual Princeton Conference o__n Information Sciences and Systems, pp 169-176 (1970). \n U V W x$ SY PRIMITIVE CONSTANT PRIMITIVE COMPUTATIONAL FUNCTION U J Y FORK LOOP  U X Y Z X Y Z OUTBOUND \nSWITCH INBOUND SWITCH FIGURE 1 182 ff--I ! I , I o ! c. E J J L~ LI../ :ZD, L.9 I...I- \u00b0I i--Jr ILl I \nI I___I L___I r--IN I i 1 I i I I I L_% I ' I I i- I I I --I- b_ ---- II I d I ~D I,.I_ F-4----ILl i,-- \nL__I L.. _ __I 183 :: ~ J,, mL ,, J I I F- Z I I..LI I I'- I..I-- I I w ~ Z O~ I- W I- f Z I---  \nj I-..~ >-I z 0 Z:) I-- 0 \u00d7 i ILl I m i F c~ o 184 \n\t\t\t", "proc_id": "800168", "abstract": "<p>A Data Flow program [1,2] is a flow-chart like network of operators which compute concurrently, dependent only on the availability of the data which flow along the paths. Each operator has only a local effect, transforming input data to output data. Although operators may exhibit memory and thus not be functional from <underline>an</underline> input to <underline>an</underline> output, all operators are functions from input sequences to output sequences. This plus the strong locality of effect allows mathematization of semantics more readily than traditional programming languages which are burdened with omnipresent storage and occasional GOTO's. This paper proves the semantic behavior of some elementary Data Flow programs and proves that certain optimization transformations preserve other behaviors.</p>", "authors": [{"name": "Paul R. Kosinski", "author_profile_id": "81100626960", "affiliation": "", "person_id": "PP40029187", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/800168.811551", "year": "1976", "article_id": "811551", "conference": "POPL", "title": "Mathematical semantics and data flow programming", "url": "http://dl.acm.org/citation.cfm?id=811551"}