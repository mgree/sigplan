{"article_publication_date": "01-01-1976", "fulltext": "\n PROGRAM IMPROVEMENT BY SOURCE TO SOURCE TRANSFORMATION David B. Loveman Massachusetts Computer Associates, \nInc. 26 Princess Street Wakefield, Massachusetts 01880 Key Words Optimization, source-to-source transfor- \nmation, program improvement, compilation, pro- gram manipulation. Abstract We treat a program as an \nobject of manip- ulation, determine items of program constancy, and simplify the program based on the \nconstancy. Some motivation for program manipulation is pre- sented, along with two examples of \"higher \nlevel optimization\" written in an Algol-like language. A collection of program transformations and a \nmodel of the compilation process in terms of source-to- source transformations are presented. Finally \na description of the application of these ideas to an existing programming language is given. Introduction \n The class of programs which have severe time and/or space requirements is quite important. These requirements \nmay arise from constraints caused by the run-time computer, performance re- quirements of the program \nor economic considera- tions resulting from repetitive execution of the program. 'Optimization\" is the \nname given to the application of a set of rules for manipulating var- ious representations of a program. \nThese manipu- lations exploit local or global invariances within the program in order to improve the \nprogram rela- tive to some measure. Before discussing optimization, we should discuss some ideas on \nthe nature of the program- ming process. The programming process can be divided somewhat arbitrarily \ninto a number of con- ceptual phases: construction, verification, opti- mization, compilation, execution, \nmaintenance, documentation, etc. In an idealized world one constructs a program using computer aids such \nas a text editor, file system, and syntax checker. The intent during program construction is to des- \ncribe data objects of interest, primary operations on these objects, and an algorithm to solve some problem \nusing these objects and operations. At the construction phase in the programming pro- cess, programming \nstyle should be aimed primar- ily at clarity of expression and ease of verifica- tion and only secondary \nattention should be paid to efficiency. Unfortunately, programming styles which lead to logically clear \nprograms tend to result in programs which, when compiled, are not efficient enough to meet their operational \nrequirements. Structured programming techniques, such as the isolation of implementation details of operations \nfrom the application of the operations, tend to re- sult in highly modular programs. Naive compila- tion \nof highly modular programs results in great in- efficiency as a consequence of the interconnection of \nthe modules. It is necessary to decide which modules'should be expanded as inline code, which should \nbe specialized to their particular calling environments, and which should be left as general modules. \nThe code resulting from modules which are expanded inline will usually allow for consid- erable optimization \nwhen viewed in the context of the program in which the expansion occurs. Con- ventional optimization \ntechniques, unfortunately, will only perform a little of this optimization. More powerful techniques \nwhich address them- selves to program structure, loops and condition- als in particular, are needed. \n These optimization techniques may be triggered in two ways: by facts known about the particular data \nobjects being manipulated at the point of module expansion, or by interactions with the program text \nthat is the context of the point where the module is expanded. Examples of these two different ways of \ntriggering optimization will be given in a later section. In general, we view the process of opti- mization \nas one which either deduces, or accepts as given, statements of invariance in a program. Statements of \ninvariance include the fixing of the specification of the internal representation of data objects, and \nthe implementation of the basic op- erations on the data objects, as well as such \"standard\" constancy \ninformation as \"N has the value of 4\", \"SIN represents the library sine rou- tine\", and \"This algorithm \nwill be executed only on a CDC 7600\" Given certain statements of in- variance, certain program transformations \nmay be shown to preserve program correctness while at the same time \"improving\" the program relative \nto some measure. Obviously, fault can be found with the simplistic view of the programming process \npre- sented above. Algorithms to manipulate data ob- jects are not designed without some notion of how \nto implement these objects. Even at program con- struction some constancy information such as \"this program \nwill be compiled by IBM FORTRAN Level H\" may be known and the statement of the algorithm may be affected. \nAlso, while doing op- timization, it may be discovered that a chosen representation does not allow the \noperational re- quirements to be met, thus requiring the construc- tion, verification, and optimization \nof a new rep- resentation. Our intent in dividing the program- ming process into construction, verification, \nand optimization for algorithm and representation is to stress two separations: data objects and opera- \n tions on them which are natural to a problem area from the underlying representation of the objects \n and operations; and the problem of constructing a correct algorithm from the problem of making an \nalgorithm meet certain time and/or space require- ments. Higher Level Optimization In optimization \nwe treat the algorithm as an object of manipulation, determine items of al- gorithm constancy either \nfrom the algorithm itself or by statement from the outside, and simplify the algorithm based on the constancy. \nWe include within optimization such standard techniques as constant propagation, constant computation, \ncom- mon subexpression elimination, removal of invari- ant code from loops, strength reduction, elimina- \ntion of redundant code, etc. Our primary interest, however, is in \"high level optimizations\" such as \ncase splitting based on known possible values of a variable, loop unrolling, loop fusion, interaction \nbetween loops and conditional statements, etc. We are interested in those optimizations which can be \ndescribed as transformations on a tree-structured representation of a program, that is, optimization \nwhich can be considered as \"source-to-source\". We thus may describe an optimization as a mapping of \none particular tree structure to another. The vital issue is that this mapping must preserve program \nvalidity. In some cases, for example loop unrolling, the transforma- tion always preserves validity; \nin others certain enabling conditions must be true before the trans- formation can be performed. For \nexample, two syntactically identical computations are conven- tional common subexpressions only if \ncorrespond- ing variables referenced by them can be shown to have the same values. A second consideration, \nthe raison d'Etre of optimization, is that the transformation must lead to an improvement (of some \nform) in the pro- gram. A complication is that the transformation itself may not improve the program, \nbut may change it in such a way that later transformations will result in a gain. This is analag(sus \nto the intermediate expression swell issue in algebraic symbol manipulation. For example, in certain \n cases computation done prior to an if ... then... else can be duplicated on the then and else branches. \nThis results in no savings of time and an increase in space, but it may be that taking advantage of \nthe knowledge that the if predicate is true on the then branch and false on the else branch will lead \nto considerable savings. One can imagine an optimizer searching blindly through a program tree, trying \nto decide where to apply transformations. Such a process would be extremely wasteful, however. It ap- \npears that the application of certain transforma- tions immediately suggests other transformations to \napply. For example, having discovered that a variable is assigned a constant value immediately suggests \nthat we try to propagate that constant, and then see whether the assignment is to a dead variable. Having \npropagated the constant value, we want to see if constant computations can be done and whether we can \nassert that other vari- ables are constant. We thus view an \"optimization\" as con- taining seven parts: \n The name of the optimization, for identification purposes, A pattern to be found in the program tree \nstruc-ture, A pattern predicate which evaluates to true if the optimization preserves program validity, \n  A win predicate which evaluates to true if the optimization, or a succeeding optimization, will improve \nthe program,  A sequence of pattern-replacement rules which make the transformation,  A set of predicates \nasserted to be true as a re- sult of applying this optimization, and  A sequence of names of optimizations \nwhich should be tried as a result of applying this op- timization.  A serious problem involves the \npropagation of information from its point of origin to where it is needed to evaluate predicates in an \noptimiza- tion rule. Classical optimizers such as IBM's FORTRAN H have used the technique of \"interval \nanalysis\" as described in Schaefer [Optimization]. An improved form of global analysis called \"p- graph \nanalysis\" has been developed to Massachu- setts Computer Associates and used in the ILLIAC-IV FORTRAN \ncompiler and in the FORTRAN Laundry, a program developed for FORTRAN source language optimization (Loveman \n[Optimization], Shapiro and Saint [Algorithms]). Wegbreit [Ex- traction] has extended the p-graph concepts \nso that they apply to any property set which satisfies an appropriate set of axioms. Recent work by Karr \n[Information], [Inequalities], [Affine] has studied the problems of gathering and propagating information \nabout programs. We are interested in techniques for manip- ulating programs which contain loop structures. \n Clearly, most programs will have looping struc- tures explicitly written by the programmer. Al- though \nthese are of interest, of greater interest are the loops which exist implicitly within the program \nas a result of high level programming lan- guage features. A programmer could conceivably perform transformations \nsuch as loop fusion on loops which he has written. There is no way, however, that he can touch the \nimplicit loops in the program. By analogy, a programmer can, if he is careful, perform common subexpression \nelimin- ation on code he has written. There is no way, however, that he can eliminate the common code \n resulting from similar array references, the com- piler must accomplish this. High level programming \nlanguage features which provide implicit looping may exist in two forms: They may be built into the \nlanguage as standard features, such as the Primary Operators and condition expressions in cryptological \nlan- guages (Loveman et.al. [Development 1975], [Development 1974]), the vector and array opera- tions \nin APL (Abrams [APL]), or they may be pro- vided by extension in an extensible language such 141 as \nECL (Wegbreit [ECL]). The first case can clearly be viewed as a special case of the sec- ond, where \nthe \"extensions\" are made at lan- guage definition time. A later section describes two examples of \nhigher level optimization and shows the type of success that may be expected. Description of Program \nTransformation Rules As indicated earlier, a program transforma- tion rule says: \"find some pattern \nin the program for which some conditions are true. If an evalua- tion function indicates that this transformation \nre- sults in a gain, relative to the appropriate mea- sure, then perform a set of replacements and make \na set of assertions. Having executed the rule, there is a set of rules which may have become applicable, \nsee if they apply\". A possible syn- tax for such a rule might be: name: i_n q pattern where pattern\\predicate \n when win\\ predicate then pat I = replacement I, .... pat n = replacement n assert predicate I .... \n, predicate k name I, ..., namem; As an example of a conventional optimization, the rules for constant \nfolding might be written: constant\\assert: i__n \"x := y\" where constant(y) assert constant(x), value(x) \n= value(y) dead\\var, constant\\propagate; constant\\ propagate: i_n \" \"x\" where constant (x) then \"x\" \n= value(x) constant\\ computation, constant\\assert; constant\\computation: i_n \"x op y\" where constant(x) \nand constant(y) then \"x op y\" = value(x) op value(y) constant\\ computation, constantkassert; dead~var: \ni_n \"x := y\" where dead(x) then \"x := y\"= null; The rules in the above examples of con- stant folding \ndo not have \"win\\predicates\" be- cause there is always an improvement by apply- ing them. Other rules \nare not always beneficial. For example, in general, splitting a loop into two loops introduces an overhead \nof extra control structure. However, on a GDC 7600, if the new loops will fit into the instruction stack \nof 12 words, there will be a gain. This rule might be written: loop\\ split: in-\"loop A;B repeat;\" \nwhere can\\split(loop) and code\\length(loop) > 12 when code\\length('qoop A repeat\") <_ 12 o_/ code\\length(\"loop \nB repeat\") < 12 then \"loop A;B repeat;\" = \"loop A repeat; loop B repeat;\" ; Ifwe were to actually \nstate formally all the details of the rules presented later, the pre- sentation would be quite tedious, \nexcessively long, and difficult to follow. Some predicates, and functions, such as \"can\\split\" and \"code\\ \nlength\" above, are decidedly non-trlvial, their existence can be assumed without much dlfflculty, however. \nWe are most interested in the pattern and replacements. Rather than give the detailed replacements, in \nmost cases we have just given a generalized before and after example in the hope that the reader can \ndeduce the form of the trans- formations. The details of the transformations are presented in Loveman \n[Transformation]. The idea of program transformation is ap- plicable to most high level languages; the \ndetails of transformations, however, are obviously lan- guage dependent. Rather than pick an existing \nlanguage, warts and all, we have designed an ad hoc Algol-like language as the target of our transformation \neffort. Knuth [GO TO] states a dream that a consensus may develop by 1984 for a really good programming \nlanguage, which he named Utopia 84. We modestly name our language Penultima 75. Samples written in this \nlanguage will be seen in the next section. Penultima 75 is designed so that reasonably high level well \nstruc- tured algorithm specifications can readily be written and so that suitable low level constructs \nare provided to serve both as targets for devolu- tion of features and to make the job of code gen- erator \nproduction easier. We shall not give here a formal definition of the language. This can be found in \nLoveman [Transformation]. We shall give, however, exam- ples of the more important language features: \n Variables a x2 waldo Constants 12 2. 347 true 'abcd' (1,3,8,12) Operators arithmetic + * / \u00f7 t - \nre lationa 1 < <-= _~ > / logica 1 an__d or not 1 xor implies is assignment := :=: ~- Expressions \nconventional a+b*7 x<_ 9 and b/6 embedded x+ (y:=y+l) *5 assignment s ubs cripting a[12] b[x := x \n+ i, y] function invocation f(x, 12) sum of first (a+b) integers relational x=31sl7 3< a<b+4 cond \nit io na I if x > 2 then a+b else a-b endif block 2 *begin: real j, i initially i; loop fo__/ j \n:= i ~ 1 t..~n: i := i'j; repeat; i en_~ Statements assignment v:=a + 7 * b; x:=find smallest value \nin array(A); conditional i_f 3<x<10 then x: = 12;endif; i fa =x I Y I z then b:=true; else b:=false; \nendif; conditional (continued) de clara tion a s s ertion procedure invocation procedure return \nlabel go to situation occurrence block loop iteration loop value list loop multiple iteration coindex \niteration Dahl loop i_f pl then sl; elsif p2 then s2; else s3; endif; declare x:real initially 0, a \n:integer array[0: n], s :static label array[1:3] constantly (~ 1, \u00a3 2 ,\u00a3 3), matrix[m, n] :mode [i, \nj] represented b~ array[l :m*nJ selected b~ (i-l) *n+j, p:procedure (\u00a3 :inter, r:intege r shared; x:integer) \n:sl;s2; end; assert x > 0; assume a < x <_ b; suppose i<j implies all,j]=0; sort first(n) elements \nof array(a); call waldo (a, b'c+7, lim ~- 12); gosub subrl; returnj label abc; label 12; g_qt_~ abc; \ng9_. t__q 12; t_pq s[2]; done; normal termination; error('symbol table full'); be~: sl; s2; end; !qq~z: \nA repeat; loop fo__[ i := b ~ s to f: A repeat; loop fo._[ i := jl,j2 ..... jn: A repeat; loop fo_[ \ni := bl ~sl to fl, b:2 ~s2 _~_f2: A repea t; io~ fo_[ i := b i b~si t oq fi # j := bj .b..y..-sj t._o \nfj: A repeat; loop : A while p: B repeat; concurrent loop loop conc for i:=b ~ s t_o f: A repeat; \nsimultaneous Ioo~ si___m fo! i:=b b~ s t_o_ f: loop A repeat; synchronous loop sync fo__fl i:=b ~ s t._~ \nf: loop A re pea t; Zahn situation until sitl o_/...or sitn: stmtl; strut m; whereupon: sitl: actionl; \n sitn: actionn; end; Examples of Higher Level Optimization In order to give examples of the power of \nhigher level optimization, let us assume we are using the programming language defined above, which allows \ndata type and operator definitions. Assume we have defined the data type \"matrix\" in the natural way \nand have defined the operator \"mult\" to be matrix multiply, a procedure which will multiply an \u00a3 by \nm and an m by n ma- trix, giving an \u00a3 by n result matrix: declare mult:procedure (x:matrix[~, m], y:matrix[m, \nn], z :matrix[\u00a3 , n] ) : loop fo__! i := 1 t_.q \u00a3: loop for j := 1 to n: ffii,j] := 0; loop fo_./ k \n:= 1 to m:  z [i, j] :=x[iT~] *y[k, j] +z [i, j]; repeat; [gPg.9_t; repeat; end; Let us look at a \nparticular call on mult, namely a multiplication of a by b giving c, where a happens to be a diagonal \nmatrix, i.e., diagonal(x:matrix) =-i / j implies x[i,j] = 0; declare a:matrix[10,10], b:matrix[10,20], \n c:matrix[10,20] ; assume a is diagonal; mult (T, b, Directly expanding mult in line gives with con- \n stant propagation: loop fo__/ i := 1 t._~ i0: loop for j := 1 to 20:  ~ii,j] :: 0; loop for k := \n1 to I0: ~i,j] := a~,k]*b[k,j]+c[i,j]; repeat; re pea t; repeat; By performing a case splitting based \non the knowledge \"a is diagonal\" the assignment statement in the innermost loop becomes: c[i,j] :=(i_f \ni/k then 0 else a[i,k])*b[k,j]+c[i,j]; By expanding the scope of the if, we get: tional compilation \ntechniques would result in loop fo_./ i := 1 to lO: loop to__! j := 1 t_o 20: eli,i] := 0; loop to__! \nk := 1 t_o i0: if i / k then -c [~]--: = 0*b[k, j]+c[i, j] ; else eli,j] := a[i,k]*b[k,j]+c[i,j]; endif; \n repeat; repeat; repeat; The then clause simplifies to c[i,j] := c[i,j] which is redundant and may be \neliminated. In the else clause, i = k , thus references to k may be replaced by references to i . The \npro- gram is now: loop to_! i := 1 t_o i0: loop fo_[ j := 1 t_o_ 20: eli,j] := 0; loop to._/ k := 1 \nt_o i0: if i = k then c[i,j] := a[i,i]*b[t,j]+c[i,j]; e ndi f; repeat; repeat; repeat; The purpose \nof the i_f statement within the inner- most loop is to select the one number in the range of k which \nis equal to the current value of i , which ranges from 1 to i0. For some values of i , this one number \nmight not exist; if it exists, it is in fact the current value of i o The program may thus be written \nusing the loop elim- ination transformation as: loop fo__r i := 1 t_oq i0: loop for j := 1 to 20:  \nc~,j] :=0Y if max(l,1) < i < rain(10,10) then --eli,j] := a~i,i]~b[i,j]+c[i,j]; endif; repeat; repegt; \n max(l, i) and rain(10, i0) clearly may be evaluated at compile time, and the predicate of the if state- \nment, 1 < i < i0 is clearly true for all i , since the if statement is in the range of a loop in which \n 1 < i < 10 . Thus the if statement may be re- placed by its then branch giving: loop fo.__r i := 1 \nt_~ i0: loop for j := 1 to 20:  ~,fl ::0Y c[i,j] := a[i,i]*b[i,j]+c[i,j]; repeat; repeat; At this point \nconstant propagation, ex- pression simplification, and dead variable elim- ination (after propagating \nthe zero, the assign- ment c[i,j] := 0 is redundant since that genera- tion of c[i,j] is dead) gives \nthe simplified code: loop fo_./i := i to i0: loop for j := 1 to 20: ~,j] := a[i,i]*b[i,j]; repeat; repeat; \nWith the program in this form, conven- good object code. This example of optimization depended on assertions \nknown true at the begin- ning and on assertions derived from the program. Clearly, the more that is known \nabout data and program regularlties, the better job an optimizer can do. Such initial assertions can \ncome from two sources: from the programmer by direct statement; or derived from the program as a re- \nsult of global compilation, compiling the routine in the presence of the call of the routine in a ma \nin program. As a second example, suppose we have in addition to the above, extended the definition \nof \"*\" as a binary infix operator for matrix multiply, \"+\" as a binary infix operator for matrix addition, \n and \".-\" as a binary infix operator for matrix .- assignment. The meaning of \"*\" is the procedure mult, \nthe meaning of \"+\" is: declare plus :procedure (x:matrix[%, m], y:matrix[%, m], z :matrix[%, m] ) : \nloop fo__r i := I t_~ %: loop to_! j := 1 to m: z[i,j] := x'~,j] + y[i,j]; repeat; repeat; end; and \nthe meaning of \"~-\" is: declare assign:procedur e (z :matrix[%, m], x:matrix[%, m]) : loop to_! i := \n1 t__o %: Io0 p for j := 1 to m: ~,j] := x~,j]; repeat; re pea t; end;~ Suppose we have the following: \n declare a:matrix[10,10], b:matrix[10,10], --c:matrix[10,10], d:matrix[10,10]; d := (a * b) +c ; Naive \ncompilation of the assignment statement will give : declare tl:matrix[10,10], t2:matrix[10,10]; mult(a, \nb, tl); plus(tl, c, t2); assign(d, t2); Note the need for 200 words of temporary storage. Expanding \nplus, mult, and assign in line gives: loop to__/ i := 1 t_o_ i0: loop fo__r j := 1 to_ I0: tl[i,j] \n:= 0; loop to._/ k := i to 10: tl[i,j] := ~,k]*b[k,j]+tl[l,j]; repeat; repeat; repeat; loop fo_._r i \n:= 1 t oq i0: loop to__/ j := 1 t_Q_ i0: t2[i,j] := tl[i,j] + c[i,j]; [9_P_9_~_t; repeat; loop fo__r \ni := 1 t_.o i0: loop to! j := 1 to i0: d[i,j] ~- t2~,j]; repeat; repea t; The outer loops can clearly \nbe fused since the in- dex sets are the same and there are no data depen- dencies which forbid it, giving: \n loop fo__r i := 1 t_o i0: loop fo__r j := 1 to i0: tl[i,j] := ~; loop for k := I to 10: tT'[i,j] := \na'-~,k]*b[k,j]+t1[i,j]; := tl[i,j] + c[i,j]; d[i,j] := t2[i,j]; repeat; We now note that each instance \nof tl[i,j] and t2[i,j] is dead after the < i, j > iteration of the two outer loops. I.e., tl and t2 have \nbeen re- duced to scalars. Subsumption (replacing a vari- able which is used only once by its definition) \nand initializing tl to be c[i,j] gives us: loop fo__/ i := 1 t_~ i0: loop for j := 1 to i0:  tT:= c[i,jU \nIoop fo_./ k := 1 to i0: tl ~. a[i,k]Tb[k,j]+tl;  4- tl; repeat; repeat; We could have eliminated tl \ncompletely by ac- cumulating in d[i,j] , but we did not for two rea- sons. First on many machines an \narray reference is more expensive than a scalar reference. Secondly, a compiler's code generator is \nmore likely to assign a scalar which is dead at the end of each loop iteration to a high speed register \n than assign an array element reference to a high speed register. Typical Program Transformation Rules \nWe have somewhat arbitrarily divided the program transformations into five classes, for ease of exposition: \nsimplifications, optimizations, evolutions ~ devolutions, and manipulations. Since the program transformation \nrules are still being developed, we cannot give a complete set of such rules, if indeed such a set exists. \nWe shall des- cribe informally, however, a collection of approx- imately 40 transformation rules. These \ninclude fairly trivial program simplifications, conventional optimizations, sophisticated loop parallelism \nde- tection evolutions, straightforward devolution or replacement of a construct by its lower level meaning, \nand manipulation rules. These rules in- clude all those referred to in the previous exam- ples. Simplification \nrefers to a generalization of the idea of constant computation and involves cleaning up local patches \nof program in fairly ob- vious ways. We shall list some of the more ob- vious simplifications :  constant \ncomputation including evaluation of procedures with constant arguments (and no side effects).  loop \ncollapse - elimination of a vacuous loop.  ~rune conditional -if true then A else B endif; = A if false \nthen A else B endif; : B reorder conditional - i_f p then else A endif; : if not p then A endif;  \nassignment elimination by equality: If a = b , the assignment a := b; can be eliminated. A special case \nof this eliminates the assignment  a := a;  Optimization is the term we use to des- cribe those program \ntransformations, usually of a low level, which are already well understood in some sense. These transformations \ninclude all the classical ones and perhaps a few new ones:  common subexpression elimination.  code \nmotion from program areas of high frequen- cy to those of lower frequency.  strength reduction.  \ndead variable elimination.  constant propagation.  subsumption - following an assignment m:=expr; \nand before modification of any part of expr, a use of m may be replaced by expr (only if expr has no \nside effects). This is especially valu- able if there is only one use of m , or if the expr is a simple \nvariable name, in which case the transformation is called scalar propagation. The idea of subsumption \nmay be used with em- bedded assignment to assist register assign- ment. For example r:=a+b;...; z:=r; \ncan be replaced by z:=(r:=a+b);  go to chasing.  array temporary elimination - for example a[i,j] \n:= b'c;...; a[j,i] := a[i,j]*2; can be re- placed by t := b'c; all,j] := t;...; a[j,i]:=t*2; count up \nto zero - a counting up loop can be shifted so that the last loop value is zero, thus simplifying the \nloop termination test. This re- quires modifying all uses of the index within the loop and insertion \nof an index adjustment assignment following the loop. If the loop final value is a constant, the index \noffset is a constant and its addition can be done via ad- dress arithmetic. If the index is dead following \nthe loop, the index adjustment assignment is not needed.  loop fo__/i:=l ~ 1 t._o n: loop fo! i:=l-n \n~ 1 t_o 0: .. i ... = ... i+n ... repeat; repeat; i := n+l; move while to end - any loop must have \na con- ditional jump. Thus if a loop terminates with an unconditional jump, the loop can be rotated \n to move the conditional jump to the end, thus eliminating the unconditional jump. 9.Pq t_9_ ~ ; loop: \n= loop: A B while p: \u00a3 : A B while p repeat; repeat;  The term evolution is used to describe those \nprogram transformations which discover higher level language constructs lurking among lower level constructs. \nPrimarily we are con- cerned with discovering higher level parallel loop- ing constructs. Lamport's work \non the parallel execution of loops (Lamport [Parallel], [Hyper- plane], [Loops], [Coordinate], Presberg \nand 1ohnson [Paralyzer]) is of great importance here. Lamport defines three types of parallel loops: \nloop conc .... concurrent loops in which individual processors are assumed to operate asynchronously \non different iterations of the loop, loop sim .... simultaneous loops in which individual processors \noperate in lock-step fashion, similar to the ILLIAC IV, and loop sync .... synchronized loops in which \nthe individual processors are assumed not to be in lock-step but to obey a synchroniza- tion condition \nwhich guarantees preservation of generation-use orderings. Two transformations have been developed to \ndetect parallelism in con- ventional iterated loops (Lamport [Loops]):  the coordinate method transforms \ncertain well behaved sets of nested loop fo!'s into a loop sim.  the hyperplane method transforms certain \nwell behaved sets of nested loop fo!'s into a loop conc. Obviously any loop conc is also a loop s im. \n  It has been shown that any loop sim produced by the coordinate method is also a loop syn c. strip \nmining refers to the transformation of a given parallel loop into the iterative execution of a parallel \nloop with smaller index set. This transformation is necessary for any parallel machine with a fixed number \nof processors. For example, on the ILLIAC IV which has 64 pro- cessors: loop fo__[i := 1 ~ 1 t_o 256: \na(i) := b(i) + c(i); repeat;  = loop conc for i := i b~ 1 t_p_ 256: a(i) := b(i) + c(i); repeat;  \n= loop fo__!j := 0 ~ 1 t_.o 3: loop sire for i := 64\"j ~ 1 t.~ 64+64\"j: a(i) := b(i) + c(i); re pea f; \nrepeat;  Observe that strip mining can work equally well for a strip of width one, i.e., for a machine \nwith only one processor. It has been shown that any loop conc or loop sync can be strip mined. Thus in \nplace of the second transformation in the exam- ple above, we could have strip mined in reverse order \nwith a width of one to get the counting back- ward loop : loop fo__r i := 256 ~ -i t_p_ i: a(i) := b(i) \n+ c(i); repeat; The transformations to parallel loops require con- siderable data dependency analysis, \nexactly the analysis necessary for loop splitting, loop fusion, etc. We shall see the role of parallel \nloop detec- tion in easing the explaination of these transfor- mations later. Another evolution transformation \nwe need  is: coindex detection which, for a given loop, de- termines variables, other than those given \nex- plicitly, which also serve as loop indices. The coindices are written in the fo__! clause as explicit \nparallel indices. For example: j := 4; ioop fo__/ i:=l ~_Z 1 t_o 7: ioop fo__[ i:=l b~ 1 t_po 7 A = \n~ j:=4 ~ 3 t..o 22: j :=j +3; A repea t; repea t; Devolution describes the replacement of a higher level \nlanguage feature by its meaning in terms of lower level language features. Typically we perform transformations \non given language con- structs, then replace the constructs by their mean- ing and perform further transformations \nat the lower level. coindex elimination loop fo_./ i:=bi ~ si to fi j := bj; # j:=bj~ sj t_po fj: until \ndone: A = loop fo__/ i:=bi b~ si t_ofi repeat; A j := j + sj; ij (bj -fj) *sign (sj)>O then done;endif; \nrepeat; end;  multi-step iteration elimination  loop declare fo__[ i:=il ~ sl to fl, b:procedure:A \nend] i2 ~ s2 to f2: loop fo___[ i:=il b~ sl t_pofh A = b; repeat; repeat; loop fo__./ i:=i2 b_ys2 t of \n2: b; repeat;  value-list iteration elimination  loop declare fo._./ i:=vl ,v2 ,v3 ,v4: b:procedure:A \nend; A = i:= vl; b; repeat; i:= v2; b; i: = v3; b;  i: = v4; b;  alternatively = declare b:procedure \n:A enid\" declare a :array[1:4] init ia fly. (vl, v2, v3, v4); loop for j:=l ~ i t_.o 4: ~.'= a[j]; b; \nrepeat; single step iteration reduction loop fo__r i:=b ~ s t_o_f: si:=s; A : fi:=f; repeat; loop fo__ \nr i:=bi ~ si t__q f\u00a3 A repeat; single step iteration elimination loop fo.__r i:=b by_ s t_o f: i:=b; \n A : si:=s; repeat; fi:=f; g_q t pq \u00a3 ; ioo p: A i := i+si; : while (i-fi) *sign (si)< 0 repeat;  \nDahl loop elimination loop: g_Q_t._o\u00a3; A m:B while p: ~ ~ :A B if p then ~ to m; endif; repeat; procedure \ninvocation elimination Suppose f has been declared a procedure of n arguments pl,...,pn of modes ml,... \n,ran respectively, and with return value pr of mode mr. For example: declare f:procedure (pl :ml ..... \npn:mn;pr:mr) :A end; A reference to f can be replaced by a block which computes the appropriate value: \n f(a 1 .... an) ~ begin declare pl:ml initially al, pn:mn initially an, pr:mr; A pr end  Manipulation \nis the mapping of one ver- sion of a computation into another version which is better in the sense that \nit is smaller or faster, or will enable other beneficial transformations to apply to the computation. \nSome of the manipula- tions that have been developed are given here: back expansion of conditional allows \na compu- tation to be tailored by the information that p is true on the then branch and false on the \nelse branch. If the absorbed computation is an assignment, the variable assigned to may well be dead \non one of the branches. A i_f p then = if p then TC A ; TC els e FC else A ; FC e ndif; endif; forward \nexpansion of conditional i_f p then if p then TC ~ TC ; A else FC else FC ; A endif; endi.f; A loop \nunrolling in general reduces the number of tests and jumps executed and increases the number of instructions \nexposed for parallel ex-ecution. It often aids register allocation by ameliorating the seam matching \nprpblem. loop loop A = A repeat; A repeat; loop while p: = until done: A loop while p: A if p then \n done; endif; A repeat; end; loop fo__r i:=b ~ s t_o_ f: until done: A = loop to_/ i:=b b~ s t_o f: \n repeat; A i:=i+s; if i> f then done; endif; A repeat; end; Note that if the number of iterations \nof the loop is divisible by the unrolling factor then the mid-loop test for termination is not needed. \nThe assign- ment i := i + s; can often be eliminated by sub- sumption. loop first case loop fo__r i:=b \n~ s to f: i := b; A if i < f then repeat; A loop fo__r i:=i+s b_ys t_o_f: A re pea t; endif; If it can \nbe shown that the loop will have at least one iteration, i.e., that i < f initially, then the enclosing \nif statement can be removed. The assignment i := b; can often be eliminated by subsumption. loop last \ncase loop to!i: = b~s Lo f: if i < f then A = ioop for i:=bby s!Q_f-s: repeat; A repea t; A endif; \n loop middle case loop fo_r i: = b b~ s t_o_ f: 10op to_! i:=b ~ s t__o m: A = A repe@t; repeat; loop \nto__/ i:=i ~ s t_o_ f: A repeat; This manipulation is particularly useful when the loop body is an appropriate \nconditional statement, for example if i > m+l then... If the predicate is i = m+l , performing a loop \nfirst case manipulation on the second resulting loop is often valuable. case split Assume there is an \nassertion of the form p implies q where p is a predicate about array subscripts and q an assertion about \narray values of the form ar = value where ar is an array reference (whose subscripts are restricted by \np). For example i /j implies all,j] = 0 . If there is an expression which contains an array reference \nref which \"matches\" at, then replace ref with i.f.p then value else ref endif. If there is an assignment \nref := x where ref \"matches\" at, and x matches value the assignment can be replaced with if not p then \nref := x; endif; . For example, ~2_P__q~9_ i a[k /~] j implies a[i,j] = O; then := 0 , will become if \nk = % then a[k,~] := O; endi__j; loop elimination A predicate within an inner loop may be used to select \nan item out of the intersection of  the index sets of the inner loop and outer loop. When detected, \nthis condition can be greatly sim- pltfied. Io~ to_! i:=j b~ 1 t_pq k: loop to_! i:=j ~ 1 t_o_ k: A \n= A loop fo__r\u00a3:=m by 1 t_on: i_fmax(j,m) < i< i_f i = ~ min(k,n) the___n then - S ~ :=i; endif; 8 \nrepea t; endif; B B  repeat; repeat; Typically the max and min expressions evaluate to constants and \nthe assignment ~ := i is eliminated by subsumption. loop compression Often a predicate within a loop \nserves to restrict the index set of the loop. In this case, the restriction can be incorporated into \nthe index set itself: suppose b <_ \u00a3 <_ f; loop fo__r i:=b ~ 1 tO_f: loop fo__r i:=~ b~ I t..._@f: if \ni > ~ then : A A repeat; endif; repeat; or suppose b <_ \u00a3 <_ f; loop to__! i:=b b~ 1 to_ f: loop to! \ni:=b b~ 1 t_po \u00a3; i_f i~ ~ then : A A repeat; endif; repeat; order reduction If an array a is used in \na loop on i and all references to a have i in, for example, the first subscript position, then if a is \ndead fol- lowing the loop, the dimensionality of a may be reduced. (As stated here, this is a special \ncase of a more general rule.) For example : loop for i := .. o: loop for i := ...: ... a7i, j] .... \n.. ~5] ...  ... all,3] ... ... a[3] ... ... all,j+7] ... ... a[j+7] ... repeat; re pea t; index set \nshift It may be that the set of values assumed by an index variable, if modified, will be more compatible \nwith additional transformations. loop to__! i:=b ~ s t_~. f: loop to__/ i:=b+c ~ s to f+c: ... i ... \n: ... i-c .o. repeat; repeat; loop fusion Adjacent loops over the same index set, if data dependency \nconsiderations allow, can be fused into a single loop. The data dependency analysis can be represented \nby first evolving con- ventional loops into sire loops, fusing, and strip mining to get back to a conventional \nloop. One of the index sets may require index set shifting to make it identical to the other. ioop to_! \ni:=b ~_X s to f: !PqOl! sire for l:=bb_xs t_of: A : A repeat; ~9_P_9_~; loop to_! j:=b ~ s t_o f: loop \nsi__m to__/j:=bbzs t_of: B B repeat; re~t; ~o~ io~ to__/ i:=b ~ s t_9_f: sire for i:=b ~ s t_o_ f: \nA A j := i; j :=i; ~ B B repeat; repeat; loop splittin9 The loop splitting manipulation reverses a loop \nfusion and depends on similar analysis: evolve to sire loops, split, strip mine. loop to_! i:=b ~ s \nto_ f: ~ sire for i:=bb_zs tot: A = A B B repeat; red; ~oop io~ to__/ i:=b ~ s t_o f: sire for i:=b \n~ s t_o f: A A ~ .~.p_9_~._t; repeat; Io~ fo__r i:=b b~ s t_o_ f: loop B sire for i:=b ~ s to f: ~P.9_9_~; \n B re pea t; iOO p reordering The loop reordering manipulation depends on finding a tightly nested \nloop set, evolving to a si__.m loop, and strip mining in an appropriate way: loop fo__r i := bi ~ si \nt_o fi: loop to__/ j := bj ~sj t_.ofj: A repeat; repeat; =loop sire for i,j:=(bi ~ si t_oq fi)\u00d7(bj ~ \nsj t pq fj): A [epeat; :loop fo_i j := bj ~ sj t_o fj: loop to__/ i := bi ~ sl t_o -fi: A repeat; repeat; \nloop variable elimination suppose t is invariant in loop and di is the change in i as a result of the \nloop; loop for i := t+b ~ s t__o_ f: A repeat; :loop fo__f t := t+b ~ s to f: i := t; A repeat t := \nt - di; The Compilation M.o.del As Hoare [Hints] has commented, a solu-tion to the problem of producing \nefficient object code for programs written in a high level language is to have designed the language \nso that \"a simple straightforward 'non-pessimizing' compiler will produce straightforward object programs \nof acceptable compactness and efficiency ... the language [should be] sufficiently expressive [so] that \nmost ... optimizations can be made in the language itself ... that a general machine-inde- pendent optimizer \ncan simply translate an ineffi- cient program into a more efficient one with guar- anteed identical \neffects, and expressed in the same source language\". This is our approach. Our model is that a straightforward \nparser, perhaps automatically generated, will parse the character string representation of a program \ninto a more highly structured, easier to manipulate, in- ternal form. It is this internal form that is \nin fact transformed by our source-to-source transforma- tions. At any point in the transformation process \nwe can inspect the results by use of an unparser or transcriber which will re-create the character string \nrepresentation of a program. We believe that by applying an appropriate set of transforma- tions to a \ngiven source program written in a suit- ably rich multilevel source language we can transform the given \nprogram in such a way that quite naive code generation techniques will result in very good code. It is \nour hope that this reduc- tion in scope of the code generation problem may in fact make possible some \nreal progress toward the automatic, or at least semi-automatic, pro- duction of code generators. The \npoint here is that a considerable amount of the work done by con- ventional code generation can be viewed \nas source-to-source transformation where the win predicate depends on the target machine. I.e., the transformation \nmay always be a valid one, but may only be useful for a particular target machine. We have found, however, \nthat most exist- ing languages do not allow \"non-pessimizing\" compilation. It is an assumption inherent \nin source-to-source transformation that there is some subset of the language for which the compiler knows \nhow to generate efficient code. For exam- ple, part of the benefit of constant propagation will be lost \nif the compiler does not generate im- mediate instructions when one argument of a com- putation is a \nconstant. In our work with existing languages, then, we have found it necessary to make suitable language \nextensions so that the re- sults of our transformations can be expressed. It is intended that the algorithm \nwriter, however, will confine himself to the original language. The language presented earlier in this \npaper has been designed so that it contains both appropriate high level features for the algorithm writer \nsuch as iteration loops, Dahl loops0 and Zahn situation statements and appropriate low level features \nsuch as labels and gotos. Let us reconsider the previous example of matrix multiply, which we had transformed \nto loop fo__r i := 1 t_.g. I0: loop fo_/ j := 1 to 20: c[i,j] := aT, i] * b[i,j]; repea t; We shall \ncontinue this example to show how source-to-source transformations (given a rich enough source) can \nassist in the remaining com- pilation. The underlying representation of an m by n matrix is a one dimensional \narray of m*n items, where selection of the [i,j] item of the  matrix selects the [ (i-1)*n+j ] item \nof the under- lying array. Letting ua, ub and uc be the underlying arrays for a, b, and c respectively, \nwe have: loop to__/ i := i t q i0: loop fo__r j := 1 t_o_ 20: uc[ (i-l)*20+j ] := ua[ (i-1)*lO+i ] * \nub[ (i-1)*lO+j ] ; repea.t; repeat; Doing expression simplification and com- mon subexpression elimination \nwithin the inner loop body, the inner loop body becomes tl := 20\"i + j - 20; t2 := ll*i - i0; uc[tl] \n:= ua[t2] * ub[tl];  Part of the tl computation and all of the t2 com- putation are invariant in the \ninner loop. Perform- ing code motion we get: loop to__/ i := 1 t_p_ i0: t3 := 20\"i - 20; t2 := ll*i- \ni0; loop to_/ j := i t_o20; tl := t3 + j; uc[tl] := ua[t2] * ub[tl]; [9_Rf_9_t; repeat; Analyzing both \nloops for coindices, the computa- tion becomes: loop fo_r i := 1 t_po I0 # t3 := 0 ~20 t_~ 180 # t2 \n:= i ~ ii to I00: loop to__/ j := i to_20 tl := t3+1 by 1 to t3+20: uc[tZ] := ua[t2~ * ~[tl]; repea \nt; repeat; The i and j indices are now dead and can be eliminated: loop to._/ t3 := 0 ~ 20 t_g 180 # \nt2 := i ~ ii t_o i00: loop for tl := t3+l ~ 1 to t3+20: ~[tl] := ua[t2] * u~tl]; repea t; repeat; Devolution \nremoves the one remaining coindex. Notice that the index sets of t3 and t2 contain the same number of \nelements, thus range testing for t3 is not needed: t3 := O; loop fo_/ t2 := 1 ~ ll t_o lO0: loop for \ntl := t3+l b~ 1 to t3+20: ~c[tl] := ua[t2J * u~tl]; red; t3 := t3 + 20; f.~Pg~_t;  Using loop variable \nelimination, we can replace tl by t3. Using index set shifting we can have the t2 loop final value be \nzero, thus easing code select: t3 :=0; loop fo__[ t2 := -99 b~ ii t_p0: loop to__ r t3 := t3+l ~ 1 \nt_o t3+20; tl := t3 uc[tl] := ua[t2+100] * ub[tl]; repeat; t3 := t3 - 21; t3 := t3 + 20; repeat; \nScalar prepagation eliminates the remaining tl references, subsumption and constant computation simplify \nthe t3 computation, and devolution eliminates the inner iteration loop: t3 := O; loop fo__[ t2 := -99 \n~ 11 t_o_ O: t3 := t3 + i; ft3 := t3 + 20; sg_ Lo ~; loop: uc[t3] := ua[t2+lO0] * ub[t3];  t3 := t3 \n+ 1; :while (t3 -ft3) < 0 repeat; t3 := t3 -1; repeat; Subsumption between loop iterations again simpli- \nfies the t3 computations: t3 := i; loop fo___~ t2 := -99 ~_Z ii t_o0: ft3 := t3 + 20; Sg_ !9 ~; loop: \nuc[t3] := ua[t2+lO0] * ub[t3]; t3 := t3 + i;  ~: while (t3 - ft3) < 0 repeat; repeat; Devolution eliminates \nthe outer iteration loop: t3 := i; t2 :=-99; 9.9.,. t.._9_ ~ ~; ioo p: ft3 := t3 + 20; g_9_ t_~_ ~; \nl\u00b0\u00b0Puc[t3]- := ua[t2+100] * ub[t3];  t3 := t3 + 1; ~: while (t3 -ft3) < 0 repeat; t2 := t2 + 11; \u00a3\u00a3: \nwhile (t2) < 0 repeat; Finally devolution eliminates both Dahl loops: t3 :=i; t2 :=-99; g_%t._9_~ ~; \nlabel outer loop; ft3 := t3 + 20; g_q t_gq ~; label inner loop; uc[t3] := ua[t2+100] * ub[t3]; t3 := \nt3 + i; label ~; i_f (t3 -ft3) < 0 then 9_9_ to inner loop; endif; t2 := t2 + ii; label ~ ~; i_f t2 <_ \n0 then ~_q to outer loop; endif; The program is now in a form where, making only reasonable a ssunnptions \nabout the target machine, a naive code generation would produce good code. By \"naive\" we mean that code \ngeneration need look only at one statement at a time, not that it need not be clever. Indeed, it is \nvery important that code generation know how to handle the ob- vious special cases:  load a register \nwith a constant,  increment a register by a constant,  access an indexed memory location,  conditional \ntransfer of control,  etcetera.  Application of Higher Level Optimization In order to improve the \nobject code of an existing high level production language,the tech- niques of source-to-source program \ntransforma- tion were used. This language is a FORTRAN-like programming language designed specifically \nfor the processing of cryptologic problems. The lan- guage contains the following features: i) Ability \nto define new character string alpha- bets, 2) Computational operations extended to character strings, \nsuch as character by character addi- tion (modulo alphabet size), 3) High level cryptologic operators, \nsuch as monographic frequency count, 4) Ability to extend operations to digraphs and, in general, k-graphs, \n 5) Explicit within-statement indexing over strings in a replacement statement or conditional statement, \n 6) Complex relations over strings for use in con- ditiona I statements, 7) Compound indexing in loop \nstatements and within-statement indexing including multiple and parallel indices. To study this language \nwe used a language labora- tory consisting of a parser generator and program manipulator to construct \na model front end compi- ler for the language (Searisto, Sattley [Laboratory]). The Language Laboratory \nis a tool de- signed to assist in the development of optimiza- tion techniques for high level programming \nlan- guages. It is written in BCPL and designed so that experimental optimization techniques can be plugged \nin for testing and analyzing. The labora- tory is capable of syntactically analyzing a pro- gram, creasing \na structural representation of the program, and transcribing that representation into a new source program. \nIt also provides utilities to manipulate and print the structural representa- tion so that experimental \noptimization techniques can be quickly implemented, debugged, and ana- lyzed. The laboratory is composed \nof five parts: Syntax Analyzer, Working Tree and Table Genera- tor, Transcriber, Utilities, and Experiments. \nThere is also an interactive Command Interpreter to assist with on-line experimentation. The Labora- \ntory implementation was somewhat language de- pendent. Itnow appears that this language de- pendence \ncould have been removed from all parts of the laboratory except, of course, the experi- ments themselves. \n In this laboratory environment we were able at low cost to study the effect of various transfor- mations \non typical source programs. Our results allowed us to make suggestions for changes in the production \ncompiler. The introduction of high level loop optimizations in the production compi- ler allowed certain \ncritical language constructs to  be executed up to a factor of three faster. In working with a language \nnot specifically designed to allow source-to-source transforma-tions, we naturally had to consider a \nvariety of other issues. These included information gather-ing, target language considerations, and proces-sing \nof high level features. All techniques for program improvement re- quire that information about the \nprogram and its environment be available. Although the gathering of this information was not a central \nportion of our work, the availability of the information is a ne- cessity. The objectives of our study \nin this area have been to assure ourselves that the type of questions which need to be answered for source- \nto-source transformations can be answered by state of the art techniques, and to advance the state of \nthe art in this area. We have concerned ourself with three major areas: i) The problem of determining \nordering relation- ships among array references in loops (Saint  [Ordering] ), 2) P-graphs and their \nuse in optimization (Love- man and Faneuf [Optimization]), and 3) The general problem of gathering information \n and propagating it through a program (Karr [Information] ).  The language proved to be an inhospitable \ntarget language for source-to-source transforma- tions. Some study revealed that several appropri- ate \nlow level constructs were not present and that the production compiler's code generator often missed \nmany special cases. A number of simple changes were made in code generation as a result of this study. \nBesides defining and tuning the non-pessimizing subset of the language, the spe- cial-case analysis in \ncode select also made a noticeable improvement in the object code. We found that specific high level \nlanguage features can be handled in one of two ways:devo- lution of features or cunning algorithm design. \n The use of a language feature can be re- placed by its implementation in terms of lower level language \nfeatures (\"devolution of features\"). This technique was used very successfully in the handling of a large \nclass of complex string assign- ment operations and predicates over strings. It was also used to handle \nlogical operations in con- ditional statements and the processing of array references. Alternatively, \ncunning algorithms were sometimes devised to implement certain features. This was often machine dependent \nand was very useful when a given target machine had special- ized, powerful instructions. For example, \nwe de- signed a special algorithm for a PL/I like INDEX function (to search a string for an instance \nof a second string) which used the IBM 370 translate instruction and executed approximately 6 times faster \nthan a conventionally implemented INDEX function. We studied the problem of computation on elements in \nstrings and devised techniques for performing certain computations one word at a time (packed) rather \nthan one character at a time. We studied the problem of providing efficient for- matted output for languages \nusing the FORTRAN run time input-output package. The techniques developed allowed certain programs to \nrun up to three times faster than they had prior to applying these techniques. Conclusions The use \nof source-to-source transforma- tions appears to be very promising both for high level program optimization \nand as a model of the bulk of the compilation process. Obviously much remains to be done. Initial steps \nhave been taken at gathering information needed to evaluate the predicates in transformations (Wegbreit \n[Analysis], [Extraction], Karr [Information], [Inequalities], [Affine] and Cheatham [Evaluator]). Considerable \neffort is still needed in this area and in the speci- fication of the predicates themselves. Only a lit- \ntle work has been done on the ordering of transfor- mations (Loveman and Faneuf [Optimization], Wegbreit \n[Transformation]). Although a large num- ber of compilers have been constructed, and therefore the software \nengineering aspects have had to be addressed, viewing compilation as an integrated information gathering \nand transformation system poses a number of interesting software en- gineering problems. If the role \nof code generation in a compiler can in fact be reduced as much as we hope, so that semi-automatic production \nof code generators becomes possible, considerable work will be needed in this field. The use of a Language \nLaboratory in the development of transformation techniques was very valuable. A similar but more general, \nand more powerful, system should be produced. Such an effort would have many interesting software engi- \nneering problems of its own, and would provide a very powerful tool for future research. The tech- nology \nof information gathering and the tools pro- vided by the Language Laboratory make possible the idea of \nconstructing an interactive program tailoring system. Such a system would allow a user to interact with \nand to transform his program, taking advantage of known constancies of data (\"matrix A is lower left \ntriangular\") or environment (\"the compiled version of this program will run on a CDC 7600\"). Since the \ntransformations would preserve the correctness of his program, the user could concentrate on the job \nof tailoring his pro- gram so that it would, hopefully, meet his re- quired space and time constraints. \n Acknowledgement Several people have contributed to our work on source-to-source transformations. We \nwould like to express our appreciation in particular to Kirk Sattley and Michael Karr of Massachusetts \nComputer Associates. Bibliography Abrams, Philip: [APL], \"An APL Machlne\", Stan- ford Electronics Laboratory, \nSU-SEL-70-017, February 1970. ACM Sigplan: [Languages], Symposium on Very High Level Languages, Sigplan \nNotices, 9, 4 April 1974. ACM Sigplan: [Optimization], Proceedings of a Szmposium on Compiler Optimization, \nSigplan Notices, luly 1970. Allen, Frances E. and Cocke,lohn: [Catalogue], \"A Catalogue of Optimizing \nTransformations \", in R.Rustin (Ed.) Design and Optimization of Com- pilers, Prentice-Hall, 1972. Bearisto, \nDavid and Sattley, Kirk: [Laboratory], \"BETA Laboratory\", Final Report, Mass. Compu- ter Associates, \nInc., CADD-7312-3111, December 31, 1973. Beckman, Lenhart et al: [Evaluator], \"A Partial Evaluator and \nits use as a Programming Tool\", Dept. of Computer Sciences, Uppsala Univer- sity, Sweden, Noyember 1974. \n Burstall,R.M. and Darlington,J.: [Transforma- tions], \"Some Transformations for Developing Recursive \nPrograms\", Int. Conf. on Reliable Software, IEEE Computer Society, April 1975, 465-472. Cheatham, T.E. \n: [Evaluator], \"ELi Symbolic Evalu- ator-Design Notes\", working paper. Cheatham, T.E. and Wegbreit, \nBen: [Laboratory], \"A Laboratory for the Study of Automatic Program- ming\", AFIPS Conf. Proc., Vol. 70, \nSpring 1972. Gerhart, S. L. : [Transformation s], \"Correctnes s Preserving Program Transformations\" \n, Second ACM Symposium onPrinciples of Programming I_anguaqes, January 1975. Geschke, Charles M. : [Optimization], \n\"Global Program Optimizations\", Ph.D Thesis, Computer Science Department, Carnegie-Mellon University, \nPittsburgh, Pennsylvania, 1972. Hoare, C.A.R. : [Hints], \"Hints on Programming Language Design\", Computer \nScience Depart- ment, Stanford University, STAN-GS-73-403, December 1973. Karr,Michaeh [Information], \n\"Gathering Informa- tion About Programs\", Mass. Computer Associ- ates, Inc., CA-7507-1411, July 14, 1975. \n Karr,Michael: [Assignment], \"The Assignment of Scalar Variables to Memory Cells\", Mass. Com- puter Associates, \nInc., CAID-7501-0611, January 1975. Karr, Michael: [Inequalities], \"Proving Inequali- ties\", Mass. Computer \nAssociates, Inc., CA- 7406-1011, June 1974. Submitted for publica- tion. Karr, Michael: [Affine], \"Affine \nRelationships Among Variables of a Program\", Mass. Computer Associates, Inc., CA-7402-2811, ~une 1974. \nAccepted for publication by Acta Informatlca. Knuth, Donald: [Go To], \"Structured Programming with go \nto Statements\", Computing Surveys, Vol. 6, No. 4, December 1974. Lamport, Leslie: [Parallel], \"Parallel \nExecution on Array and Vector Computers\", Proceedings of the 1975 Sagamore Conferehce on Parallel Proces- \nsing, August 1975. Lamport, Leslie: [Hyperplane], \"The Hyperplane Method for an Array Computer\", Proceedings \nof the 1974 Sagamore Conference on Parallel Pro- cessing, August 1974. Lamport,Leslie: [Loops], \"The \nParallel Execution of DO Loops\", Comm. ACM 17, 2,February 1974. Lamport, Leslie: [Coordinate], \"The \nCoordinate Method for the Parallel Execution of DO Loops\", Proceedings of the 1973 Sagamore Conference \non Parallel Processing, August 1973. Loveman,David: [Transformation], \"Source-to- Source Transformation \nof Programs\", in prepara- tion. Loveman, David, Bearisto, David, Karr, Michael and Sattley,Kirk: [Development \n1975], \"Develop- ment of Compiler Optimization Techniques \", Mass. Computer Associates, Inc., CADD-7508- \n3111, August 31, 1975. Loveman, David and Faneuf, Ros s: [Optimization], \"Program Optimization-Theory \nand Practice\", Proceedings of a Conference on Programming Lan- guages and Compilers for Parallel and \nVector Machines, Sigplan Notices, Vol. i0, No. 3, March 1975. Loveman,David: [K in a Row], \"The 'K in \na Row' Problem\", Mass. Computer Associates, Inc., CADD-7412-0411, December 4, 1974. Loveman, David, \nSattley, Kirk and Bearisto, David: [Development 1974], \"Development of Compiler Optimization Techniques\", \nMass. Computer Associates, Inc., CADD-7407-2311, July 1974. Loveman,David: [Replacement], \"On the Compila- \ntion of the Beta Replacement Statement\", Mass. Computer Associates, Inc., CADD-7308-0911, August 9, 1973. \n Presberg~ David and Johnson, Nell: [Paralyzer], \"The Paralyzer, IVTRAN's Parallelism Analyzer and Synthesizer\", \nProceedings of a Conference on Proqramming Lanquages and Compilers for Parallel and Vector Machines, \nSigplan Notices, Vol. i0, No. 3, March 1975. Saint, Harry: [Ordering], \"The Partial Ordering of Loops \nand Array References\", Mass. Computer Associates, Inc., CADD-7312-3112, December 31, 1973. Schaefer, \nMarvin: [Optimization], A Mathematical Theory of Global Program Optimization, Pren- tice-Hail, 1973. \n Shapiro,Robert M. and Saint, Harry: [Algorithms], \"The Representation of Algorithms\", Applied Data Research, \nInc., Final Technical Report. RADC-TR-69-313, Vol. II, Rome Air Development Center, September 1969. \nWegbreit, Ben : [Analysis], \"Mechanical Program Analysis\", Comm.ACM 18, 9, September 1975. Wegbreit, \nBen: [Extraction], \"Property Extraction in Well-Founded Property Sets \", IEEE Transactions on Software \nEngineering, Vol. SE-i, No. 3, September 1975. Wegbreit, Ben: [Transformation], \"Goal-Directed Program \nTransformation\", Xerox Palo Alto Re- search Center, September 1975. Wegbreit, Ben: [Predicates], \"The \nSynthesis of Loop Predicates\",Comm.ACM 17,2, Feb. 1974. Wegbreit, Ben: [Closure], \"Procedure Closure \nin ELi\", The Computer 5ournal, Vol. 17, No. i, February 1974. Wegbreit,Ben: [ECL], \"The ECL Programming \nSys- tem\", Proc. FTCC, Vol. 39, pp.253-262, 1971. Wulf,W.A., Russell,D.B. and Habermann,A.N.: [BLISS], \n\"BLISS:A Language for Systems Program- ming\", Comm. ACM 14, 12, December 1971, 780-790.  \n\t\t\t", "proc_id": "800168", "abstract": "<p>We treat a program as an object of manipulation, determine items of program constancy, and simplify the program based on the constancy. Some motivation for program manipulation is presented, along with two examples of &#8220;higher level optimization&#8221; written in an Algol-like language. A collection of program transformations and a model of the compilation process in terms of source-to-source transformations are presented. Finally a description of the application of these ideas to an existing programming language is given.</p>", "authors": [{"name": "David B. Loveman", "author_profile_id": "81100166567", "affiliation": "", "person_id": "P62671", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/800168.811548", "year": "1976", "article_id": "811548", "conference": "POPL", "title": "Program improvement by source to source transformation", "url": "http://dl.acm.org/citation.cfm?id=811548"}