{"article_publication_date": "01-01-1976", "fulltext": "\n Automatic Design of Data Processing Systems Gregory R.Ruth Project MAC Massachusetts Institute of Technology \nCambridge, Mass.02139 Abstract Because the computations generally involve very simple calculations, \nthe bulk of the processing cost for such systems is An automated business data processing system designer \nusually due to I/O charges. Thus, an optimization strategy is described. Attention is centered on the \nI/O aspects of such that focuses mainly on I/O reduction is appropriate. But systems and the development \nof optimizing design heuristics. implicit in the Langefors model are two simplifying assumptions that \nseriously weaken it as a design optimization tool for The design of data organization and data accessing \n practical systems: (I) that IIO costs depend directly on total procedures for data processing systems \noperating on large keyed transport volume as defined above, and (2) that all files have a files of data \nis a common and recurrent activity in modern data single key. (Also ignored is the cost due to core residency \nprocessing applications. A considerable amount of charges, but in systems dominated by secondary-primary \nstorage understanding and expertise in the this area has been developed I/O activity, this is a second \norder effect). and it is time to begin codifying and automating this process. In most operating systems, \nthe user is charged primarily It should be possible to develop a system where the user has by the I/O \nevent (the transfer of a single second storage block to merely to specify the characteristics of his \n(input, output, and or from primary storage), rather than by total volume. intermediate) data objects \nand their interrelations I and the Furthermore, the number of I/O events necessary when a system will \nautomatically determine the data organizations and computation processes a file depends on not just the \ntotal accessing procedures that are optimal for his application. The volume of the file, but also (I) \nthe number of records in a block, optimizer for Protosystem I (an automatic programming system (2) the \nnumber of records used by the computation, and (3) the prototype at MIT's Project MAC) provides an example \nof how access method (e.g. sequential, random). such automation can be accomplished. This paper describes \nthe The limitation of one key per record is completely theory and algorithms behind the optimizer that \nis currently artificial and at variance with practical reality. Files operational at Project MAC. frequently \nhave more than one key in their records (cf. CoddS). This means that they can be sorted in more than \none way. By PreviousWork the same token, a computation that processes such a file can process its records \nin more than one order by their keys. This The first reported efforts in the area of formalizing affords \nthe possibility of conflict either between two data processing system design are those of Langefors 2'a. \nHe computations that process the records of a file in two different conceives a data processing system \nas a collection of processes or key orders, or between two files with different sort orderings computations \nthat operate on data files. In this model a file is that are processed by the same computation. The manner \nin a set of records, each with a single key and one or more data which such conflicts are resolved (e.g. \nre-sorting a file, using items. Each computation makes passes over all of the records -random instead \nof sequential access) will affect the total I/O cost of its input files to produce records of its output \nfiles. A of the system. The possib!lity of sort order conflicts also transport volume is associated with \neach file-computation pair complicates the evaluation of I/O savings due to file and (where the computation \nreads or writes that file). This is computation merging. defined to be the volume of the file (measured, \nsay, in bytes) The Protosystem I optimizer takes all of these design multiplied by the number of passes \nmade by the computation considerations into account. over that file, The design objective is the minimization \nof total system transport volume. Transport volume reduction is The Protosystem I MIS Model accomplished \nby merging computations (so that a file can be read once for all, rather than separately for each) and \nby Protosystem I is an automatic programming system for merging files (thus eliminating key redundancy \nand making the generating batch oriented MIS's. Such systems involve a resultant file physically smaller \nthan the sum of the sizes of its sequence of runs or job steps that are to be performed at component \nfiles). Nunamaker et al 4 have analyzed this specified times. They are assumed to involve significant \nI/O formulation and developed an algorithm for System optimization activity due to repetitive processing \nof keyed records from large that generates all ,feasible mergings (by implicit enumeration) flies of \ndata. Systems such as inventory control, payroll, and and corresponding system configurations to find \nthe one that employee or student record keeping systems are of this type. minimizes transport volume. \n50 Central to the design of such systems is the notions ol the data set. A data set is a collection of \nsimilar data that are to be processed in a similar way. An example is the set of all Inventory levels \nin a warehousing MIS. In the domain of Protosystem I a data set is assumed to consist of fixed format \nrecords (e.g. one for the level of each inventory Item). Associated with each record are date ttems and \nkeys. The key values of a record uniquely distinguish it (e.g. the inventory data set can be keyed by \nitem since there is only one level [record] per Item) and so can be used to select it. Thus, a data set \nis essentially the same as a Codd relation 5 and its keys are what Codd calls primary keys. The repetitive \napplication of an operation to the members of a data set or sets is termed a computatton. The order of \napplication of the operation by a computation is assumed to be unimportant to the user; in fact, he may \nthink of them as being performed in parallel. However, every computation does, in fact, process its inputs \nserially, according to a particular ordering (chosen by Protosystem l) on their keys. Computations typically \nmatch data items from different data sets by their keys and operate on the matching items to produce \na corresponding output data item. For example, an order filling computation matches the total orders \nfor a given item against the inventory level of that item and determines the amount of that item to be \nshipped. Computations may also group the members of a data set by common keys and operate on each group \nto produce a single corresponding output. Following our example, suppose that Item orders can come from \nseveral sources, so that both the item and the source of an order are needed (as keys) to distinguish \nit. To form the total of all orders for each item, a system must group the orders by item and sum over \nthe order amounts in each group. In this context, then, data processing system design is the process \nof constructing computations and data sets and determining the best access methods and organizations \nfor them. In the domain of Protosystem t only three types of file organizations and three types of accessing \nmethods are considered. The organizations are: Consecutive Recordsare organized solely on the basis of \ntheir successive physical location in the file. For Protosystem I records in a sequentially organized \ndata set are required to be completely ordered. Index Sequential Records are organized in such a way \nthat by reference to indices associated with the data set it is possible to quickly locate any record \nand read or update them. Additions or deletions do not require a rewrite of the file. Each index-sequential \ndata set has~a particular sort ordering (determined by !he optimizer) associated with it and usually \nthe records are nearly ordered in this way physically. Rel~lonal (2) Records are stored in a \"random\" \nmanner but there is a mapping function from the keys of a record to its physical address. The accessing \nmethods are: Sequential For regional (2) and sequential data sets, records can be read and processed \nin the physical order in which they appear in the data file. For an index sequential data set the operating \nsystem's sequential access software delivers records in the sort order associated with the data set. \nRandom Records are referenced (read, updated, added, or deleted) through the values of keys supplied \nby the computation. This method is applicable only for accessing regional (2) data sets or for readlng \nIndex sequential datasets. Core Table An input file is initially read into core in its entirety; an output \nfile, after being assembled in core, is sequentially copied onto the device where it will reside when \nfinished with. Records are accessed in core sequentially if their sort order agrees with the processing \norder of the computation; otherwise, by binary search. Directly organized data sets are accessed by hashing. \nT.he Optimization Criterion Optimization, as viewed by this part of Protosystem I is simply cost minimization. \nFurther, because the MIS's are assumed to be IIO intensive, this is equated with access minimization, \nAn access is defined as the reading or writing of a single secondary storage block, which corresponds \nto a single operating system I/O event. In Protosystem 1, for a particular data set a block consists \nof a fixed number of records. Example A simple example of an MIS in the domain of Protosystem I is the \nstore chain inventory and warehousing system shown in Fig. i (boxes indicate computations and arrows \nrepresent data flows). This system contains computations that update the Inventory levels file to reflect \nshipments received from suppliers, find the total amount of each item ordered by all stores, fill the \norders, determine the total reductions in Inventory, adjust the levels file accordingly, check for the \nnecessity of restocking orders, and make orders to the proper suppliers. This example illustrates some \nof the important factors In data processing design. Consider the ~Icc O~0ERS computation. For each item \nthat is ordered by some store a shipment quantity is determined. It could be implemented by considering \neach possible combination of values for the keys STORE and ITEM, determining whether the conditions are \nsuitable for producing a shipment record (viz. that the item in question has been ordered by the store \nunder consideration), and if so applying the associated operation to generate such a record. This requires \na test for every possible combination and each test involves a probe to one or more of the input data \nsets. Since on a given day not all of the stores will order and most that do will order only a fraction \nof the possible items, such an imr~lementation 51 .~.. w~ o ~. ~ ~ ~ -~ L m 0 m t~ ) ~ ,, ~ .-- O-I-* \no c o 3 o m m ~ g \"D > \"0 .N L U 0 e m ~ C 52 would involve an unnecessarily large number of useless \naccesses. In contrast, since it can be determined (by analyzing the FILL ORDERS computation) that the \noperation will be applicable only when there is a record in QUANTITIES ORDERED, the computation could \nbe designed to consider only those store and item combinations for which there are records in QUANTITIES \nORDERED. In this case the computation is said to be drtuen by QUANTITIES ORDERED, and QUANTITIES ORDERED \niS said to be its drtulng data set. Note that not every input to a computation is a suitable driver; \nfor the UPDATE INVENTORY computation only YESTERDAY'S FINAL INVENTORY will do--if SHIPMENTS RECEIVED \nwere used it might not contain a record for each item, causing BEGINNING INVENTORY tO be incomplete. \nAnother factor in optimization is illustrated by the UPDATE INVENTORY computation. If the records of \nSHIPMENTS RECEIVED are consecutively organized and sorted in the same order as those of the driver (YESTERDAY'S \nFINAL INVENTORY) it can be used in the order in which it is stored. This means that blocking can be used \nto minimize accesses; if there are B records per block and N records in shipments received, the number \nof accesses necessary will be no greater than N/B 1. However, if the two inputs are not sorted in the \nsame order, the computation will have to search (usually requiring more than one access) for each record \nof SHIPMENTS RECEIVED when it is needed. Alternatively, SHIPMENTS RECElVEO might be given regional (2) \norganization and accessed randomly; but the blocking advantage would be lost. These considerations of \ndriving data sets, consecutive data set organization, compatible sort orderings and blocking will figure \nimportantly in the design process. TheOptimizer In Protosystem I the optimizing designer of data set \norganization and data processing (called the opttmtzer) is given a relational description of the basic \ndata aggregates and the relations among them. (Fig. 2 is an example of such a description for the HIS \nof Fig. 1.) Its job is to (I) design the keyed files--in particular their (a) contents (information \ncontained) (b) organization (direct, consecutive, or index sequential) (c) storage device (d) associated \nsort orderings (by key values)  (2) design each job step of the HIS--namely (a) which computations \nit includes (b) its accessing methods (sequential, random, core table) (c) its driving data set(s) \n (d) the order (by key values) in which ',it processes the records of its input data sets  (3) determine \nwhether sorts are necessary and where they should be performed (4) determine the sequence of the ,job \nsteps  All design decisions are made in an effort to minimize the total number of accesses that must \nbe performed in the execution of the HIS. In preparation for this design process an analysis of the HIS \ndescription is performed in order to determine properties relevant to making good decisions. Among these \nproperties are the candidates for driving data sets for each computation and the average and maximum \nsizes of each data set (the latter may require some interrogation of the user). During the design process \nthe user may be asked to give further information 6 that can be used to determine the sizes of newly \ndesigned data sets. Access Minimization There are three mapr techniques that the Protosystem I uses \nin designing MIS's so as to minimize accessing: (I) making use of data set blocking, (2) aggregating \ndata sets, and (3) aggregating computations. Blocking Since an access is defined as a reading or writing \nof a block of records, accesses can be reduced if blocking factors greater than one are used and if proc~essing \nand data set organizations are arranged in such a way that the records of a block can be used at the \nsame time. Where core table access is not possible (i.e. when the whole file will not fit in available \ncore) this means that a data set must accessed sequentially (and therefore have consecutive or index \nsequential organization) and it must be sorted in an order that is the same as processing order(s) of \nthe computation(s) accessing it. When core table access is possible for a data set the number of accesses \nis just the number of blocks in the data set; regardless of whether the data set's sort order matches \nthe processing order of accessing computations, each block need be accessed just once to ge t it into \nor out of core. Because blocking is the most effective way of reducing accesses, the optimizer tries \nto make it possible for the sort orders of data sets that are too big for core table access to match \nthe processing orders of their accessing computations in as many cases as possible. However, unresolvable \nconflicts among sort order preferences can and will arise. Sometimes it is advantageous to introduce \nresorting computations so that a data set can have two or more differently sorted versions. Aggregating \nData Sets and Computations The relational description of the HIS identifies the bastc data entities and \noperations. The straightforward Implementation of such a description is to make one data set for each \ndata entity and one computation for each operation. This can be considered as the tntttat conflguratton \nto be manipulated by the designer. The designer combines or aggregate data sets and computations in order \nto reduce accesses. The aggregation of data sets produces'a data set in which there is one record for \neach set of records in the original data sets that match (by keys). For example, the aggregate of the \ndata sets in Figs. 3.a and 3.b is shown in Fig. 3.c (Nil, 3} IN21, 2} (N,i, ---, 3} (Ni2, 2} (N22, 4} \nINi2, N21, 2}. {---, Nzz, 4} male-emplogees female-emplogees male-&#38;-female- (dep,t) (dept) emplogeestdept) \n i a. b. c. Figure 3: Data Set Aggregation 53 where a record is represented by a tuple whose last element \nis the key value and whose first elements are the data items. Data set aggregation is advantageous when \ntwo or more data sets are read or written by the same computation. Accesses can be saved if the shared \ndata sets are aggregated and processing is arranged so that'a single record of the aggregate can be accessed \nwhere more than one record from each of the unaggregated data sets would have to be accessed without \naggregation. Two other ways of reducing the number of accesses are illustrated in Figs. 4.a and 4.b (circles \nrepresent data sets, boxes represent computations). It may be advantageous to combine computations that \nread the same data set so that they can all access a record at the same time; if their processing orders \nfor the shared data set agree, each record to be accessed can be read once for all, rather than once \nfor each computation. This is called ltortzontal aggregation (Fig. 4.a). Yertlcal aggregatton refers \nto combining two computations when the output of one is used as the input to the other and their processing \norders agree (Fig. 4.b); in that case, the second computation does not have to read the output records \nof the first--they can simply be passed from the one computation to the other as needed. The aggregation \nof computations is more than a simple merging. Looping redundancy is eliminated so that the composite \nresult is more efficient than its components separately. General Design of the Optimization The access \nminimizations techniques given above require that the key order of processing agree in a special way \nwith the organization of the data being processed. This is where the fundamental difficulty in optimization \nlies. A data set's organization and the accessing method of a computation using it cannot be determined \nindependently of each other or of other data set organizations and computation accessing methods. The \norganization of a data set limits the ways in which it can be practically accessed by a computation, \nind, conversely, the accessing method of a computation restricts the practicable organizations of a data \nset that it accesses. Furthermore, a data set is typically accessed by more than one computation with \npossibly conflicting preferences for its organization; and a computation accesses more than one data \nset with conflicting preferences for accessing methods. Finally, data set organization constraints tend \nto propagate through computations, because it is most efficient for a computation to write its outputs \nin the same key order in which it reads its inputs, since that is the order in which the output records \nwill be generated. So, optimization of the type we are considering is necessarily be a problem in global \ncompromise. The straightforward solution of evaluating the cost of every possible combination of assignments \nof sort order, device, organization, and access method for data sets and computations in every possible \naggregation configuration to determine the least expensive is ruled out by the sheer combinatorics involved. \nEven with mathematical and special purpose tricks it would be impossibly slow. To make optimization tractable \na heuristic approach G + Figure 4.a: Horizontal Aggregation of Computations &#38;#169; --> &#38; Figure \n4.b: Vertical Aggregation of Computations must be taken. First different kinds of decisions (e.g. choice \nof driving data sets, which objects to aggregate) must be decoupled wherever possible. Further decoupling \nmust be .judiciously introduced where it is not strictly possible, for the sake of additional simplicity. \nSuch forced decoupling does not mean, though, that decisions that are in fact coupled are treated as \nif they were independent. The decoupled decisions are still made with a certain awareness of their effects \non other decisions. Finally, as a first order approximation, the optimizer does what is reasonable locally, \nand then adjusts somewhat for global realities. 54 3. Parallelism A computation outputs records in the \nsame The Optimization Algorit.hm key order in which its inputs are read. Our optimization algorithm \nconsists of the following 7 steps: |. Development of maximal potential for reducing accesses through \nblocking for the Initial configuration. 2. Aggregating computations where advantageous in the current \ncontext. 3. Aggregating data sets where advantageous in the current context. 4. Iteration over steps \n2 and 3 until no further aggregation Is suggested. 5. Determination of driving data sets.  8. Determination \nof device and organization for each data set and of access method for each computation. 7. Determination \nof optimal blocking factors. Step I: Settinl~ Up the Initial Configuration to Take Advantage of Blocking \nThe determination of mutually agreeable sort orders for computations and data sets that will allow maximal \nadvantage to be taken of blocking (which requires matching sort orders and sequential accessing where \ncore table access is not possible) must be considered first in the optimization process. Aggregation \nand other optimization techniques are of little value if they force sorting or other methods of accessing \n(that can require orders of magnitude more I/O events than sequential accessing). As explained above \nit is necessary for the sort order of a data set that is not core table accessible to be made the same \nas the processing order of a computation that accesses it in order to reduce accesses by that computation \nby blocking. But there are user constraints on data set sort orders and inherent preferences of computations \nfor constraints on their sort orders that must be taken into account, too. As a result of these each \ncomputation and data set will have a sort order constraint (SOC) restricting the possible sort orders \nit may be given. For'example, a data set D(key l, key z, key 3) may have the SOC that its records be \nsorted on key z first.. Then the optimizer could decide to sort them first by key I and then by key 3 \nunder key2, or vice versa. The factors that affect SOC's are: I. User constraints The user may specify \nthat inputs or outputs have a particular associated SOC. 2. Uniqueness A particular computation presents \nthe same SOC to all of the data sets that it reads or writes; and a particular data set presents the \nsame SOC to all of the computations that read or write it 4. Computation Preferences: The optimizer prefers \nthat the input data set to a grouping computation be sorted first by the keys that distinguish the groups \nit operates on. That way, it can be designed to process one group at a time. Otherwise groups must be \nprocessed in parallel, requiring costly accesses if the core available for buffering is insufficient. \n 5. Sequential Access Preference (to take advantage of blocking)  Two SOC's for which there is a common \npossible sort order are called conststent. If a data set and a computation accessing it have consistent \nSOC's both can be assigned the same SOC (one that conforms to each of their original SOC's), thus insuring \npotential for sequential access and blocking. In general it will not be possible to satisfy all preferences \nand constraints simultaneously. Conflicts arise preventing the sequential accessing of every data set. \nAt best, therefore, the optimizer can only try to find the cheapest compromise. What it aims for is the \nmaximization of the total volume of data accessed sequentially. Even here it must to make concessions \nto practicality. That is, it tries to come as close as possible to this maximum without expending an \nunreasonable amount of effort. Its method is to follow the implications of the initial constraints and \npreferences throughout the network of computations until conflicts arise, and then try to resolve those \nconflicts as advantageously as possible. It tries to associate with each computation and data set a minimimally \nrestrictive SOC that is consistent with its own preference (if any) and with all of the constraints on \nand preferences for the SOC's of all of the immediately adjacent objects in the net (for data sets this \nis the set of all accessors,and for computations this is the set of all Inputs and outputs). Insofar \nas this is possible it has what it wants; otherwise it has discovered a conflict of interest, which it \nattempts to resolve in such a way that the greatest volume possible of records involved can be accessed \nsequentially. We have found that this simple SOC assignment algorithm produces good, if not optimal, \nresults in the systems we have tested. This occurs because typically (.I) there are very few conflicts \nand (2)those that do occur are generally simple and local phenomena. When an assignment of minimally \nrestrictive, \"consistent-as- possible\" SOC's has been determined for all of the objects in the system \nheuristic aggregation of computations and data sets can be performed. It is not advantageous to aggregate \ncomputations and data sets if this will increase the I/O cost by precluding sequential accessing that \nmay have been possible before aggregation. So, the aggregation of two data sets or computations will \nbe permitted only if their $OC's are consistent. On the other hand, unnecessarily restrictive SOC constraints \nwould prevent aggregations that would otherwise be possible. Thus the emphasis on finding minimally restrictive \nSOC's. 55 Step 2: Computation Aggregation Computations are considered for aggregaUon if they are have \nconsistent SOC's and access a common data set. This data set may be either a common input (horizontal \naggregation) or a common intermediate data set--that is, the output of one and and input to the others \n(vertical aggregation). In order to preserve existing sequential accessing potentials, the SOC of the \naggregate must be consistent with each of the SOC's of the aggregated computations. Further, to avoid \nunnecessarily precluding otherwise possible aggregations, the SOC assigned to the aggregate is the minimally \nrestrictive mutual restriction of the SOC's of the components. Nevertheless, it will not be uncommon \nfor the SOC of the aggregate to be more constrained than the SOC's of the computations that went into \nit. This means that the decision to make a particular aggregation may prevent other aggregations (by \nvirtue of SOC inconsistency) that were formerly possible. For example, computations A, B, and C may be \npairwise aggregatable but the aggregation of all three may be impossible; that is, if, say, A and B are \naggregated, the SOC of the result may be inconsistent with that of C, even though the SOC of A is consistent \nwith that of C, and the SOC of B is consistent with that of C, Finding an exact optimum by considering \nall possible combinations of all possible aggregations is again precluded by sheer combinatorics. The \nheuristic approximation is made that what is optimal locally is good for the system, and again we have \nobserved (and conjecture) that the simplicity of typical systems is such that this is a viable approach. \nThe optimizer considers aggregation candidates two at a time. As the order of treatment is significant, \nthe policy is (locally) to consider aggregations in order of the number of IIO events they are expected \nto save. Furthermore, classes of aggregations are performed in the following order: 1. vertical aggregations \n 2. horizontal aggregation of computations reading common system inputs  S. other horizontal computation \naggregations Vertical aggregations are considered to have a higher priority than the horizontal variety \nbecause the former may result in the entire elimination of data sets; that is, the data sets involved \nwill neither have to. be read nor written. Horizontal aggregations that eliminate reads of system inputs \nare preferred over other horizontal aggregations because such inputs often reside on such storage media \nas magnetic tape and cards, which are relatively inconvenient and costly to access repeatedly. Step 3: \nData Set Aggregations. Data sets are considered for aggregation if they have consistent SOC's, are not \nsystem inputs or outputs, and are both outputs of a common computation and inputs to another common computation. \nAs with computations, care must be taken so that the SOC of the aggregate is consistent with each of \nthe SOC's of the aggregated data sets, in order to preserve whatever sequential accessing potentials \nmay exist. As the introduction of arbitrary constraints can prevent otherwise possible aggregations here \ntoo, the SOC assigned to the aggregate is the minimally restrictive mutual restriction of the SOC's of \nthe components. But again, the SOC of the aggregate may be more constrained than the SOC's of the data \nsets that went into it, so a particular aggregation may prevent others, and the order in which they are \nperformed is important. The optimizer considers those computations with more than one output in order \nof the total volume in records that they process. For each computation its largest output data set is \nconsidered for aggregation with as many of the others (in order of their size) as possible, the criteria \nfor aggregation being that SOC's be consistent and that the total cost of reading the aggregate (by all \ncomputations for which it is an input) is lower than the total cost without aggregation. Step 4: Aggregation \nIteration Computation and data set aggregation are repeated until no further aggregation is possible. \nThis iteration is necessary because the aggregation of data sets may make further pairwise computation \naggregations feasible, and similarly, the aggregation of computations may make additional pairwlse data \nset aggregations feasible. Step 5: Driving Data Set Determination Choosing driving data sets for each \ncomputation Is straightforward. Empirically, about 85~, of the computations in the initial (before aggregation) \nsystem configuration are found to have a only a single candidate. Aggregation further decreases the number \nof cases where there are multiple candidates. For each of the (few) remaining computations for which \na candidate must be chosen, determinations of the total average accesses necessary for each are made \n(possibly requiring the user to supply information about data set sizes) and the accessing minimizing \ncandidate is chosen. Step 6: Devices Organization a and Access.Method Determination Some assignments \nof device and organization for data sets will already have been made by the user. Typically he has specified \nthe devices and organizations for the system inputs and outputs. Reports generally have device PRINTER \nand so must be consecutively organized. Additionally, driving data sets are constrained to be accessed \nsequentially by the computations they drive; and so their organizations are either consecutive or index \nsequential. Within this context the remaining assignments are made by cases. The optimizer considers \none data set at a time and binds its device and organization and the accessing methods of each accessing \ncomputation before considering the next. If a data set has no SOC conflicts with its accessors it is \ngiven consecutive organization, sequential access, and (unless otherwise specified by the user) device \nDISK. If it is a system input that is only partially sorted core table access is used if its size permits; \notherwise a sorting computation is inserted. If a data set has SOC conflicts with its accessors and it \n is core table size, all accessors access it by core table. If a data set has a SOC conflict and is too \nbig for core table access there are three alternatives: I. give it consecutive organization, have the \ncompatible computations access it sequentially, and insert sorting computations to produce versions that \ncan be accessed by the rest of the computations. 56 2. give it index sequential organization, have the \n 4. Kornfeld, W. Methodology for Optimization in Automatic compatible computations access it sequentially \nand the Programming Systems, unpublished B.S. thesis, Project MAC, others randomly MIT, 19'75. S. give \nit direct organization and have all accessors access it randomly Each alternative assignment is sent \nto the job cost estimator and the one that minimizes cost is chosen. Step 7: Determination of Blocking \nFactors Choice of blocking factors can be postponed to the very last step in the optimization process. \nThe cost of accessing as a function of the block size drops initially as block size increases; but then \nit hits a minimum and begins to rise as core residency charges become significant. However, for the costing \nscheme we have studied the minimum usually occurs either beyond the operating system's block size limit \nor so close to it that the core residency does not play a significant role in cost. So the optimal assignment \nof blocking factors by minimizing total system cost (as a function of the blocking factor) is determined \nsubject to the constraints that the blocking factor be less than the operating system prescribed upper \nlimit (e.g. no block can be larger than a single disk track) and that the total buffer space at any time \ncannot exceed the available core. Experience While this process will not find the true optimum, it produces \na good and usually near-optimal solution for real and honest problems. The reason for this is that, from \nour experience so far, in typical systems global effects are weak, Step 1 (SOC determination) does a \ngood job of bringing the consequences of these global effects down to a local level, and after that what \nis \"reasonable\" to do in a fairly local context is almost always good (if not best) for the overall system \ndesign. The approach described in this paper is wholly within the spirit of state of the art MIS design. \nThe goal is to get a system that works and works well. Designers do not expend inordinate amounts of \ntime and energy to get as close to an optimum as possible unless timing and/or cost constraints are absolutely \ncritical. Although our optimizer lacks the cleverness and special case knowledge of a competent system \ndesigner we believe that the indefatigable, meticulous application of: sound (though not perfect) optimization \nrules will more than compensate for this. Even with the few, relatively simple, systems that have been \nworked on so far, the optimizer has proposed designs which were not at all obvious, but which, under \ncareful scrutiny, proved to be quite good. References 1. Codd,E.F. A Relational Model of Data for Large \nShared Data Banks, Comm. ACM 13, 6 (June 1970), 377-387. 2. Early, J. Relational Level Data Structures \nFor Programming Languages, Comp. Scl. Dept., U. of California, Berkeley, 1975. 3. Hammer, M., Howe, \nW. and Wladawsky, I. An Overview of a Business Definition System, ACM SIGPLAN Nottces, 9, 4, April 1974. \n 5. Ruth, O. Internal Memo 16: Status of Protosystem I, Project MAC, MIT, 1975. 6. Ruth, G. Internal \nMemo 21: The New Question Answerer, Project MAC, 1975.  57  \n\t\t\t", "proc_id": "800168", "abstract": "<p>An automated business data processing system designer is described. Attention is centered on the I/O aspects of such systems and the development of optimizing design heuristics.</p> <p>The design of data organization and data accessing procedures for data processing systems operating on large keyed files of data is a common and recurrent activity in modern data processing applications. A considerable amount of understanding and expertise in the this area has been developed and it is time to begin codifying and automating this process. It should be possible to develop a system where the user has merely to specify the characteristics of his (input, output, and intermediate) data objects and their interrelations<supscrpt>1</supscrpt> and the system will automatically determine the data organizations and accessing procedures that are optimal for his application. The optimizer for Protosystem I (an automatic programming system prototype at MIT's Project MAC) provides an example of how such automation can be accomplished. This paper describes the theory and algorithms behind the optimizer that is currently operational at Project MAC.</p>", "authors": [{"name": "Gregory R. Ruth", "author_profile_id": "81100431536", "affiliation": "", "person_id": "P341034", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/800168.811539", "year": "1976", "article_id": "811539", "conference": "POPL", "title": "Automatic design of data processing systems", "url": "http://dl.acm.org/citation.cfm?id=811539"}