{"article_publication_date": "01-01-1976", "fulltext": "\n TECHNIQUES FOR THE AUTOMATIC SLECTION OF DATA STRUCTURES James Low and Paul Rovner Computer Science \nDepartment University of Rochester Rochester, New York 14627 We are all aware of the development of \nin-creasingly sophisticated, elaborate, and expensive computer programs, particularly in the fields of \nartificial intelligence, data base management, and intelligent systems. The need for techniques to deal \nwith such complexity has renewed interest in programming language research. Recent work on structured \nprogramming, intelligent compilers, automatic program generation and verification, and high-level optimization \nhas resulted. A pattern of approach similar to that of ear-lier research on programming languages is \nemerging. The work divides naturally into two parts: the search for good linguistic tools for expressing \nalgorithms and data, and the development of prac-tical methods for translating these to working computer \nprograms. Our emphasis in this paper is in the latter. For programs that are inherently complex and expensive, \nthe intelligent choice of implementa-tion-level representations for high-level data is a central problem. \nThis paper contains a dis- cussion of several powerful techniques for automating such intelligent choices \nfor a given program. Flow analysis, execution monitoring, and interactive sessions about the characteristics \nof the problem are considered. Built-in knowl-edge for a collection of data structures is assumed. For \neach data structure, this includes rules about its applicability and formulas for its memory space and \nexecution time requirements as functions of the properties of the data and the operations of the program. \nThe context for the discussion is an ex-perimental system [Low74, Rovner76~ which chooses data structures \nand algorithms for sets, lists, and associations in the ALGOL-60 based program-ming language SAIL [VanLehn7~. \n(A brief des-cription of SAIL is contained in Appendix I.) Similar abstract data structures are to be \nfound in other experimental programming lan- guages, including QA4]~Derksen72], MICRO-PLANNER ~Sussman70, \nBaumgart72~, SETL [Schwartz75], MADCAP LMorris73J, VERS2 [Earley73] and CONNIVER [Sussman72, McDermott72]. \nThis paper is organized in two parts. The first comprises an introduction to our methods and an overview \nof the selection system. The second contains, a discussion of our recent work on auto- matic selection \nof associative data structures. An appendix contains detailed examples of this work. System Overview \nThe selection system is based on the premise that there are many different ways of representing the sets, \nlists, and triples of a user's program. A fixed library of such representations is built into the system. \nA compiler (with occasional help from the'user) selects representations which will tend to minimize some \ncost function (such as amount of cpu time) for the entire program. It does this by predicting the costs \nof using the various ap-plicable representations for the user's abstract data structures. Then it chooses \nthose representa-tions which seem to minimize the toal cost. First it selects representations by acting \nas if the choices for the various abstract structures were independent. Later it considers how these \nrepre-sentations interact. For example, one cost function is the space time product. If the program has \ntwo abstract data structures, the compiler has to consider the cost component of the space oc-cupied \nby each data structure multiplied by the time of execution of operations on the other. The compiler may \nchange its choice if these inter-actions so dictate. The amount of storage needed for the repre-sentation \nwill influence the total cost of a representation. The storaqe cost for a repre-sentation is usually \nquite easy to estimate. Con-sider the representation of a set by a binary tree. We estimate the number \nof storage cells needed by a set of N elements by simply adding the amount of storage needed by a header \nnode to N times the number of storage cells needed for each node of the binary tree. A header might consist \nof two cells, a count of the number of elements in the set and a pointer to the root node of the tree. \nEach node of the tree contains three memory cells: one for the element of the set, and one each for the \nright and left links. The storage occupied by a set of four elements would be 14 memory cells: two for \nthe header and three each for the nodes containing the elements of the set. Note that this notion of \nstorage cost is an approximation. A more precise system would have to include such factors as the storage \nallocation algorithm (in a buddy system we might be able to only allocate blocks which are powers of \ntwo and thus each node in the tree above would require four cells instead of three), whether some form \nof virtual memory such as paging or segmentation was being used, and whether there are more than one \nkind of local storage such as a fast semiconductor memory and a slower extended core storage. The time \ncost of a representation is somewhat harder to estimate. The time cost of a repre- sentation is the sum \nof the costs of the individual primitive operations used in a given program. The time cost of a primitive \noperation may be estimated by looking at the routine which implements it. We count the number of machine \ninstructions (weighted by their individual execution times) which will be executed. This will be a function \nof a number of relevant parameters of the abstract structure. Consider the following routine for determining \nif an item is an element of a set which is repre-sented by an unsorted linked list terminated by a NIL \npointer. (a) LOAD the \"item\" being sought. (b) LOAD the \"ptr\" to the first node of the linked list. (c) \nCOMPARE \"ptr\" to NIL and (d) ilf equal RESULTIS false (e) COMPARE \"item\" to the data part of the node \npointed to by \"ptr\" (f) if equal RESULTIS true (g) LOAD the \"ptr\" from the link field of the current \nnode (h) JUMP to (c) The execution times of the instructions are: LOADs take two time units (microseconds); \nCOMPAREs take three time units; RESULTIS takes (for a LOAD and a JUMP) three time units; and JUMPs take \none time unit. We analyze the routine to see how many times each of its statements will be executed. \nParts (a) and (b) will be executed once independent of the size of the set. If the item is in the set \nthen part (c) will be executed from I to N times. It will be executed N+I times if the item is not in \nthe set. Part (d) will be executed no times if the item is in the set and once if the item is not in \nthe set. Part (e) will be executed from I to N times if the item is in the set and N times if the item \nis not in the set. Part (f) will be executed once if the item is in the set and no times if the item \nis not in the set. Parts (g) and (h) will be executed from zero to N-I times if the item is in the set \nand N times if the item is not in the set. We assume that if an item is in the set, it is equally likely \nto be in any node o ! the list. We thus estimate the average frequency of state-ments (c) and (e) when \nthe item is an element of the set as (N+I)/2. The frequency of (g) and (h) is similarly estimated as \n(N-i)/2. Thus, there are only two important parameters whiqh affect the execution time of this routine: \nN the size of the set and P the probability that the desired item is actually in the set. The total time \ncost of the routine is derived by summing the weighted instruction times: C(N,P) = 9N+IO-P*(9N+3)/2 We \nrepeated this type of analysis for each of the primitive operations implemented for representation in \nour library. See Table I for an example set of time cost functions. TABLE I PUT SET -insert item in set \nH = proportion of time item already in the set = average size of set M = maximum size of set REPRESENTATION \nSet Empty Set Non-Empty Linked list 100 64 -42\"~ + 6\"~ AVL tree 56 80-166~+i6.8*LOG2(~) Bit -Array 146 \n+ 3\"FM/321 48 Hash table 521 82-40H + 3~/16 Bit-string with unsorted 265 + 3~FM/32~ I04-53~ linked list \n Attribute bit 27 Sorted variabl( 140 96-80H+5.05\"~ + length array ~O.5*LOG2(~) -.3~*~ Our system has \na library of representations for sets, sequences and triples and a library of cor-responding space and \ntime cost functions. Before the compiler attempts to choose representations for a given program it must \nobtain information about the use of the abstract data structures within the program. This information \nis obtained in three ways: static analysis, monitoring, and user interrogation. The static analysis phase \nconstructs a flow graph of the program and does a symbolic evaluation of the graph. This gives a model \nof the potential contents of all the set and list variables and the store of triples in terms of both \npre-declared items and dynamically created items. By flow analysis, we can usually prove that variables \nwill never con-tain certain items and that the associative store will never contain certain triples of \nitems. Static analysis is also used to obtain a partition of variables into classes which should be considered \nas units when choices of representation are made. There is usually no inherent reason why any two sets \nof a program need to be represented in the same way. However, translation of representation is usually \nquite expensive and in general should be avoided. Our static analysis determines which variables are \nused in the same expressions (e.g. union and intersection) or are involved in the same assignment statements. \nSuch variables will be represented in the same way. The partitioning also finds which operations are \nperformed on mem- bers of each representation class. This informa-tion is used to eliminate certain representations \n59 as candidates for a class because they don't provide all the required primitives. Monitoring sample \nexecutions of the program with user supplied data sets and direct inter-rogation of the user provide \nthe frequency of each primitive operation as well as the parameters to the time and space functions. \nIn the current implementation, monitoring is used to determine the frequency of the various statements \nof the program. The compiler asks the user the values of such things as the average size of a set at \na given point in a program, amount of overlap between sets involved in a union or intersection and so \nforth. Thus the system can make reasonable guesses of the values of the cost functions. The final selection \nis done by choosing those representations which minimize the total expected cost of a program (space-time \nproduct) based on the information drived above. Interactions between choices are considered in the analysis \nof the tradeoffs of space and time for representations of individual abstract data structures. Selection \nof Associative Data Structures The selection system provides a general frame-work in which to study data \nstructure design. A large part of our recent effort has been directed towards the extension of the system \nto the selec-tion of associative data structures. In particula~ we are looking at several alternate data \nstruc-tures for the store of triples in SAIL (see Appen- dix II}. These structures consist of variations \nand combinations of four basic types: records, hash tables, property lists, and inverted files. There \nare many different associative data struc-tures. We have chosen a representative collection of structures \nto analyze (see Appendix II). We purposely chose enough structures to require the development of methods \nfor managing combinatoric explosion in the selection process. A real selection system would be impossible \nwithout such methods. In addition, the collection of structures is rich enough to allow the system to \nconsider the issue of sharing vs. redundancy as it selects a data structure. An analysis of these structures \nto yield rules for their applicability and formulas for their space and time costs is complete. A detailed \nexample of these formulas is contained in Appen-dix II. As in the case of sets and lists, selection of \na data structure for triples is based on the ap-plicability of the structure to the operations of the \nprogram, and on estimates of program execution time and storage requirements. These estimates are derived \nfrom a static analysis of the program, from monitoring example executions, and from ques- tions asked \nof the programmer. Appendix II contains examples of such questions. The Associative Model As the system, \nanalyzes a program, it builds a model of the store of triples and the operations that are performed on \nit. The model of the store of triples reflects the use of these operations by the program. The set of \n\"make\" operations is divided into classes. Two make operations are put into the same class if there is \nan \"erase\" or \"search\" operation that could match triples created by both. For each class, the associated \nerase and search operations are listed. This partitioning reduces the problem of analyzing the entire \nstore of triples for data structure selection to several smaller problems. No operation effects more \nthan one class. Within a class, the search and erase operations are categorized by their \"form\". This \nis deter-mined by the arguments to the operation: whether they are given items, \"bound variables\", or \n\"any\". There are 27 (3x3x3) forms. Proposing Candidate Data Structures The system uses the structure \ninherent in a set of associative operations to identify candidate data structures for the class of triples. \nFor example, assume that the form of all search and erase operations is such that the Attribute and Object \npositions are always specified and there are a fixed number of items (known at compile time) that could \nappear in the Attribute position. Each such item could be used to select a fixed field of the given Object \nin which to find a Value. More-over, if there will be only one Value for a given Attribute-Object pair, \nthen there is no need to allow for a set of Values. This data structure is termed a \"field selection \nrecord\". A similar col-lection of such rules is associated with each data structure that the system knows \nabout. One of the problems arising from our decision to deal with a large library of data structures \nis combinatoric explosion in the selection process. Much of our. effort was spent in devising methods \nto manage this. In addition to the early use of ap-plicability rules, the system computes and stores \nuseful information about each operation to avoid re-computing it as each data structure candidate is \nconsidered. Also, sets of candidates which differ in minor ways are treated as units whenever possible. \nFor instance, in the previous example, if there is more than one choice for which component of the triple \nform to consider the Attribute, the candidate structure is not split into separate candidates for each \nchoice unless a preliminary cost analysis indicates that different costs are associated with the different \nchoices. If one choice is worse than some other in both space and time, it is eliminated. The proposal \nof candidate structures proceeds in three phases. First, each search and each erase operation is analyzed \nto determine the set of ap-plicable associative retrieval techniques, These include the selection of \na field of an object record, hash table lookup, property list search, and searching various types of \nlists of triples. Next, each data structure is considered in a process of \"matching\": if the associative \nretrieval tech-niques provided by the structure realize the needs of the set of operations, and the applicability \nrules of the structure are satisfied, then the structure is proposed as a candidate for the set of operations. \nIt should be noted that each such candidate might represent several instances of a more general structure. \nFor each candidate, the process of matching identifies the associative retrieval technique to be used \nfor each operation, and the set of choices for realizing each operation. The third phase is the proposal \nof candidate structures that exhibit redundancy. The set of associative operations might be more efficiently \nrealized as two (or more) subsets, each dealing with a data structure that is well-suited to its operations, \nThe savings in time may outweigh the cost of storing more than one copy of the set of triples. One problem \nin this phase is combina-toric explosion of the number of candidates. Heuristics are required for selecting \nonly plausible partitions of the set of associative operations. The present system is working through \nphase two of candidate structure proposal. A detailed ex-ample of its output is contained in Appendix \nII. The immediate next steps in the research are to experiment with methods for phase three, and push \nthrough more examples. The task of finishing the code which implements the cost formulas and does final \nselection is straightforward, and will pro-ceed in parallel. Summary Our system demonstrates the feasibility \nof the automatic selection of high-level data structures such as the sets, lists, and triples of SAIL. \nIn combination, the techniques of analysis and in- formation-gathering have been successfully applied \n[Low74]to programs that use sets and lists. Our preliminary analysis of the Problem of selecting representations \nfor triples shows that there is much structure in a store of triples and in the associative operations \nof a program. Our current research is an attempt to understand how this structure should be used with \nthe other techniques to select associative data structures. APPENDIX I Brief Description of SAIL SAIL \nis an ALGOL-60 based artificial intel- ligence language. The abstract data structures of SAIL [Feldman69, \nVanLehn7~ include ITEMS, SETS of items, LISTS (sequences) of items, and TRIPLES (associations) of items. \n An ITEM is essentially a reference to a variable allocated from a heap. Items are normally used to represent \nabstract o~j~cts and the names of binary relations. A SET is an unordered collection of distinct items. \nA set variable is declared in the same way as any arithmetic variable. Set expressions in- clude the \nunion of sets, intersection of sets and explicit sets (e.g. {a, b, c} where \"a\", \"b\", and \"c\" are items). \n A LIST is an ordered collection of items. An item may appear in a list more than once. List operations \ninclude concatenation, sublist extrac- tion, and subscripting. The TRIPLE is used for general mappings \nbetween items. Triples consist of three items, termed (in order) \"Attribute\", \"Object\" and \"Value\". The \nfirst item, the Attribute, is often used as the name of a binary relation between an Object and a set \nof Values. The store of triples and the opera-tions on it are fully symmetrical, however. The language \nimposes no constraints on the interpre-tation of the position of an item in a triple. The syntax for \ntriple forms is (<item>-<item>=item). There are three kinds of operations on triples in SAIL: I. MAKE, \nwhich adds a given triple to the store of triples. 2. ERASE, which removes specified triples from the \nstore of triples. Each of the three arguments to the erase operator is either an item, or the key-word \n\"ANY\". The arguments to erase are treated as a patters which is matched against the store of triples. \nWhere \"ANY\" is used, any item will match. Triples which match are re-moved from the store of triples. \n 3. SEARCH, which locates specified triples in the store of triples. As for erase, the three arguments \nto search are treated as a pattern which is matched against the store of triples. In addition to items \nand the key-word \"ANY\", an argument to search can be a \"bound variable\". If the arguments are all items \nor \"ANY\", then search behaves like a Boolean-valued function, returning true if the pattern matches, \nfalse otherwise. If there are any arguments which are \"bound variables\", then search behaves like a \"generator\", \nenumerating the triples that are found to match the pattern. As each triple is found, the \"bound variables\" \nin the pattern will be assigned the corresponding items of the matching triple.  APPENDIX II Selection \nof Associative Data Structures: Examples This appendix contains several glimpses of detail from the experimental \nselection system. It has five parts: A. A description of the data structures which are considered by \nthe system. B. Time and space formulas for one of them. C. Examples of questions which the system asks \nthe programmer. D. Partial results of the system's analysis of an example SAIL program. E. The example \nSAIL program listing. A. The Associative Data Structures Known to the System The candidate associative \ndata structures con-sist of variations and combinations of four basic types: records, property lists, \ninverted files, and hash tables. From the many possibilities, we have chosen a representative collection \nof struc- tures to analyze. These are listed and described below. In the discussion, the three positions \nof the triple form are identified with particular roles in the various data structures. This is done \nfor expository reasons. The system recognizes symmetries and permutes positions of the triple form when \nappropriate. I. Field Selection Records (FSR) A field selection record is a block of con-tiguous storage \ncells. The address of the block is obtainable from one of the three items of the triple (the Object). \nAnother of the items (the Attribute) determines a storage cell within the record and a field of this \ncell, The third item of the triple (the Value) is stored in the specified field. FSR's are con-venient \nfor associative searches in which two of the three items are given, and the Attribute is from a fixed \nset of items. FSR's are usual-ly wasteful of memory space (compared to hash tables or inverted files) \nunless there is at least one Value for most Attributes of most Objects most of the time. There are three \ntypes of FSR's: a. FSRBIT: One bit is sufficient to represent the value. b. FSRIVAL: The binary relation \nis single-valued. c. FSRSET: There can be more than one value.  2. Property List This data structure \nassociates a set of (Attribute-Value) pairs with each Object, The address of a list of elements is obtained \nfrom the Object's record, Each element con- tains an Attribute and an ordered list of Values. The list \nof elements is ordered by Attribute. Property lists are superior to hash tables where space is at a premium \nand the number of triples varies widely with time. They are superior to FSR's when space is at a premium \nand most Attributes do not have values most of the time. 3. Inverted Files An inverted file is a list \nof triples. Each triple is represented as a block of storage cells which contains the three items of \nthe triple. The list threads all blocks that have a particular item in a given position. There are two \ntypes of inverted files: a. Simple: A one-way list of triples for one position, perhaps ordered by the \nitems in another position. b. Complex: multiply-threaded triples. Each triple block is an element in \n several lists. Each list can be ordered, and either one-way or two-way. Inverted files are convenient \nfor associa-tive searches in which only one item is given. They are also useful for stores of triples \nthat vary widely in size and in place of FSR's for cases where the set of Attributes is not fixed before \nprogram executionL They are wasteful of processing time for associative searches in which more than one \nitem is given. 4. Hash Tables A hash table is a block of contiguous stor-age cells. A function which \nmaps triples to cell addresses within this block is used both to insert new triples and for associative \nretrieval. Each cell contains a pointer to the list of triples which map to the cell. Associative retrieval \nvia hash tables requires a combination of address computation and search- ing. Care is required to match \nthe design of the function to the properties of the set of triples, to avoid large discrepancies in the \nlength of lists. Hash tables are convenient for associative searches in which more than one item is given, \nor the set of Attributes is not fixed and computation time is at a premium. Hash tables are not convenient \nwhen space is at premium, and the size of the store of triples varies widely as the program runs. There \nare two classes of hash tables: a. All three items are hash operands. There are four types of data structure \nhere: SIMPLE: each conflict list element (CLE) contains the three items of the triple. 2) POINTERS TO \nTRIPLE BLOCKS: each CLE contains a pointer to a triple block. This allows sharing with inverted file \nstructures. 3) SINGLE LINK: each CLE contains a pointer to a triple block and is threaded in a one-way \ninverted file. This allows more intimate sharing. 4) MULTIPLE TWO-WAY LINKS: each CLE contains a pointer \nto a triple block and is threaded in one or more two-way inverted files. b, Two items are hash operands. \nThere are twelve types of data structure here: l) SIMPLE: each CLE contains the two hash operands (Attribute \nand Object) and a pointer to a list of Values. 2) POINTERS TO TRIPLE BLOCKS: each CLE contains a pointer \nto a list of pointers to triple blocks. This replaces the pointer to a list of Values. As for I. above, \neach CLE represents a particular Attribute-Object pair, and each entry of the list reDresents one triple \nhaving that pair. 3) SINGLE LINK: each CLE contains the two hash operands and is threaded in a one-way \ninverted file. The key for the inverted file must be one of the hash operands. Each CLE contains a pointer \nto a list of Values. 4) SINGLE LINK WITH POINTERS TO TRIPLE BLOCKS: combination of 2 and 3 above. 5) \nMULTIPLE TWO-WAY LINKS: each CLE contains the two hash operands and is threaded in one or more two-way \ninverted files. The key(s) for the inverted file(s) must be from the two hash operands. Each CLE contains \na pointer to a list of Values. 6) MULTIPLE LINKS WITH POINTERS TO TRIPLE BLOCKS: combination of 2 and \n5 above. The remaining six types of data structure (7 through 12) are variations of l to 6 above. Each \nentry on either the list of Values or the list of pointers to triple blocks is threaded in a one-way \ninverted file. The key for this inverted file is the Value. B. Example of Time and Space Formulas: Field \nSelection Records Io SPACE (in 36-bit memory cells) = C. (NA*NO)*(\u00bd+X) where NA = the number of Attributes \nNO = the number of Objects X = the average number of Values for a given Attribute and Object. 2. TIME \nMAKE: Co+Ci+Q*x*(Co+C4) + M'C2 2 ERASE (3 items given) Co+CI%R*x*(C\u00b0+c4) + E'C3 2 (Value = ANY) Co+Ci+DELCOST(X) \nFIND (3 items given) Co+ C~+Z *x*(c\u00b0+c4) 2 (Value : ANY) Co+Cl 63 (Value a variable) Co+Ci+V*GENCOST(X) \nwhere C o time to compare two pointers Ci time to select a given field of a given Object record, and \npick up its contents C 2 = time to insert an element at a given position in a list C 3 = time to remove \na given element of a list C4 = time to pick up the pointer to the next element of a list and jump M = \nthe fraction of MAKE operations which create new triples Q = the fraction of MAKE operations that find \na similar triple (A.O=ANY) R = the fraction of ERASE operations that find a similar triple (A.O=ANY) \nE = the fraction of ERASE operations that find a triple to erase Z = the fraction of fully specified \nFIND operations that find a similar triple V = the fraction of FIND operations for (A.O=variable) that \nfind answers DELCOST(X) = the time to reclaim a list of length X GENCOST(X) = the time to generate elements \nfrom a list of length X Example Questions Asked by the System About the Store of Triples I. For a given \nAttribute and Object, what is the average size of the set of Values? 2. For a given Object, how many \nAttributes (on average) will have at least one Value? 3. What fraction of (A.O-V) questions would find \nanswers if they were (A.O=ANY) questions? 4. How many triples (on average) will have a given value? \n 5. What fraction of (A-O=ANY) questions have answers (on average)? 6. What fraction of MAKE operation \nexecutions would create a triple that already exists? 7. What fraction of MAKE operation executions \nwould find a triple which has the same Attribute and Object? 8. What fract#Qn of ERASE Qperation executions \n find sometnlng to erase: D. Examole Analysis 3. Value-Threaded Hash Table Entries (A4b7) This section \ncontains partial results of the system's analysis of an example SAIL program. These consist of a collection \nof candidate repre-sentations for subsequent cost analysis and final selection. After a brief introduction \nto the example program, the candidate representations are described. Each candidate is listed with a \nrefer-ence to the description of its prototype in section A of this appendix. The attached test program \nconstructs the minimal spanning tree for a given graph and prints out information about its \"cost\". The \nalgorithm deals with disjoint sets of nodes, and selects edges with the smallest cost which connect nodes \nfrom different sets. Each time an edge is found, the two sets are merged. The result is a set of edges \nwhich form the minimum cost tree which spans the graph. A set of nodes is represented by a (single valued) \nbinary relation of the following form: (SETOF- NODE = SET) The primitive associative operations of the \nprogram are : I. MAKE (three given items); 2. ERASE (given item .ANY= given item); 3. SEARCH (given \nitem .given item = variable); 4. SEARCH (given item -variable = given item).  The system finds that \nmost of the associative retrieval techniques are applicable to each of the \"erase\"and \"search\" operations, \nbut discovers only six candidate representations to propose for the operations taken together: I. Threaded \nTriple Blocks (A3b) Each triple is represented as a block of storage cells having two threads. Each thread \nis associated with a position in the triple, and is part of a list of triples that have the indicated \nitem in the indicated position. The first thread is in either the Attribute position or the Object position, \nand is used by operation 3. The second thread is in either the Attribute or the Value position, and is \nshared by operations 2 and 4. The first thread is two-way to expedite the re-moval of triples which are \nto be erased. Both threads are ordered. In this example, the first thread represents either a list of \nall triples, ordered by nodes, or the one triple which identifies the set containing a given node. The \nsecond thread represents either a list of all triples, ordered by sets, or a list of triples which identify \nnodes belonging to a given set. 2. Object-Threaded Hash Table Entries (A4b7) In this case, operations \n2 and 4 share a hash table to find a list of Objects, given the Attribute and Value. In the example, \nthis is a list of nodes which are in a given set. Each Object is threaded in a list of hash-table entries \nfor the Object. Operation 3 uses this list to find the set for a given node. This is a variation on candidate \n2. Opera-tion 3 uses the hash table to find a list of Values, given the Attribute and Object. In the \nexample, this list would have one entry: the set which contains a given node. Operations 2 and 4 share \nthe thread through Values. The thread represents a list of nodes for a given set. 4. Attribute-Threaded \nHash Table Entries (A4b3) This is similar to candidate 3, except that operations 2 and 4 share a thread \nthrough Attributes. In the example, this thread repre-sents a list of all triples. For non-single valued \nrelations, each element of the thread would represent the collection of triples that have a particular \nAttribute and a particular Object. 5. Hash Table and Threaded Triple Blocks(A4b2) In this case, operations \n2 and 4 share a hash table to find a list of Objects, given the Attribute and Value. As for candidate \n2, this is a list of nodes which are in a given set.\" The difference is that hash table entries are pointers \nto triple blocks, which are threaded either in the Attribute position or the Object position. Operation \n3 uses this thread. The thread is two-way, to expedite erases by operation 2. 6. Attribute Threaded Hash \nTable and Threaded Triple Blocks (A4b4) This candidate provides a hash table for operation 3, an Attribute \nthread for operation 2, and a two-way triple block thread for operation 4. We would expect the preliminary \ncost analysis to reject it in favor of other candidates. As the system analyzes its model, it asks questions \nof the user when it needs to do so. For this example, it asked two questions: I. For a given Attribute \nand Object, is there only one Value? 2. For a given Attribute and Value, is there only one Object? If \noperation 3 were not present in the example, the system would have proposed a candidate repre-sentation \nwhen provides a list of nodes for each set. Each Value would be represented by a record having a fixed \nfield for the SETOF Attribute. The field would contain a pointer to a list of NODE items. Operations \n2 and 4 would use the given Attribute to select this field of the given Value. If only operation 3 were \npresent, a similar candidate representation would associate a given node with its set. One of the next \nsteps in the research will be to consider multiple (redundant) representations as candidates. In our \nexample, the system will then be able to consider the combination of the above two representations as \na single candidate. One difficulty here is dealing with the erase operation. A general technique is 64 \nto convert erase operations to loops with two operations: a search with variables in place of \"ANY\"s, \nand an erase with all items specified. E. Example SAIL Program Listing: Minimal Spanning Tree Construction \nAlgorithm BEGIN \"SPNTRE\" REQUIRE 100 NEW!ITEMS; LIST EDGES; COMMENT THE PRIORITY QUEUE OF EDGES OF THE \nGRAPH; LIST ITEM A,B,C,D,E,F,G,H; COMMENT NODES OF HOMEMADE GRAPH; STRING ITEM NAMEA, NAMEB, NAMEC, NAMED, \nNAMEE, NAMEF, NAMEG, NAMEH; COMMENT NAMES OF NODES; LIST ITEM EDGEAB, EDGEAC, EDGEAH, EDGECE, EDGEBC, \nEDGEBH, EDGEEF, EDGEGH, EDGEEH, EDGEBE, EDGEDF, EDGEBG, EDGEFG, EDGECD, EDGEEG; ITEM SETOF; COMMENT \nSET MEMBERSHIP RELATION: (SETOF ELT = SET); COMMENT EDGE COSTS; INTEGER ITEM EcAB,EcAC,EcAH,EcCE,EcBC,EcBH,EcEF,EcGH,EcEH,EcBE,EcDF,EcBG,EcFG,EcCD,EcEG; \nSET SETOFVERTICES; COMMENT THE SET OF THE NODES OF GRAPH; SET TREESET; COMMENT SET OF EDGES MAKING UP \nMINIMAL SPANNING TREE; LIST ITEMVAR EDGETEMP; COMMENT WILL REFER TO AN EDGE ITEM; LIST ITEMVAR V,W,VERTEX; \nCOMMENT WILL REFER TO VERTEX ITEMS; INTEGER ITEMVAR Ec; COMMENT WILL REFER TO COST OF AN EDGE; INTEGER \nCOSTS; COMMENT COST SO FAR OF SPANNING TREE; INTEGER NVERTEXSETS; COMMENT NUMBER OF DISJOINT SETS OF \nNODES; BOOLEAN PROCEDURE DISJOINTUNION(ITEMVAR MEMBER1,MEMBER2); BEGIN \"DISJOINTUNION\" ITEMVAR SETNAMEI, \nSETNAME2, TEMPI; IF NOT (SETOF MEMBERI = BIND SETNAME1) AND (SETOF MEMBER2 = BIND SETNAME2) THEN ERROR; \nIF SETNAMEi = SETNAME2 THEN RETURN(FALSE); FOREACH TEMPI I (SETOF TEMP1 = SETNAMEI) DO MAKE (SETOF \n TEMP1 = SETNAME2); ERASE (SETOF ANY = SETNAMEi); DELETE(SETNAMEI); RETURN(TRUE); END \"DISJOINTUNION\"; \nCOMMENT START EXECUTION HERE; COSTS := O; TREESET := PHI; COMMENT HOMEMADE GRAPH; DATUM(A) := {{ NAMEA \n}}; DATUM(NAMEA) := \"A\"; DATUM(B) := {{ NAMEB }}; DATUM(NAMEB) := \"B\"; DATUM(C) := {{ NAMEC }}; DATUM(NAMEC) \n:= \"C\"; DATUM(D) := {{ NAMED }}; DATUM(NAMED) := \"D\"; DATUM(E) := {{ NAMEE }}; DATUM(NAMEE) := \"E\"; \nDATUM(F) := {{ NAMEF }}; DATUM(NAMEF) := \"F\"; DATUM(G) := {{ NAMEG }}; DATUM(NAMEG) := \"G\"; DATUM(H) \n:= {{ NAMEH }}; DATUM(NAMEH) := \"H\"; SETOFVERTICES := {A,B,C,D,E,F,G,H}; 65 COMMENT EDGES IS AN ORDERED \nLIST OF EDGES, ORDERED BY COST; EDGES := {{ EDGEAB, EDGEAC, EDGEAH, EDGECE, EDGEBC, EDGEBH, EDGEEF, EDGEGH, \nEDGEEH, EDGEBE, EDGEDF, EDGEBG, EDGEFG, EDGECD, EDGEEG }}; DATUM(EDGEAB) := {{ A, B EcAB }}; DATUM(EDGEAC) \n:= {{ A~ C EcAC }}; DATUM(EDGEAH) := {{ A, H EcAH }}; DATUM(EDGECE) := {{ C, E EcCE }}; DATUM(EDGEBC) \n:= {{ B, C EcBC }}; DATUM(EDGEBH) := {{ B, H EcBH }}; DATUM(EDGEEF) := {{ E, F EcEF }}; DATUM(EDGEGH) \n:= {{ G, H EcGH }}; DATUM(EDGEEH) := {{ E, H EcEH }}; DATUM(EDGEBE) := {{ B, E, EcBE }}; DATUM(EDGEDF) \n:= {{ D, F, EcDF }}; DATUM(EDGEBG) := {{ B, G, EcBG }}; DATUM(EDGEFG) := {{ F, G, EcFG }}; DATUM(EDGECD) \n:= {{ C, D, EcCD }}; DATUM(EDGEEG) := {{ E, G, EcEG }}; DATUM, EcAB := 1; DATUM, EcAC := I; DATUM, EcAH \n:= i; DATUM, EcCE := I; DATUM, EcBC := 2; DATUM, EcBH := 2; DATUM EcEF) := 2; DATUM'EcGH) := 2; DATUM, \nEcEH) := 3; DATUM'EcBE) := 3; DATUM EcDF) := 3; DATUM, EcBG) := 4; DATUM EcFG) := 6; DATUM, EcCD) := \n8; DATUM EcEG) := 9; COMMENT INITIALIZE SET OF DISJOINT SETS AND THE MAPPING BETWEEN A NODE AND THE DISJOINT \nSET IN WHICH IT APPEARS: FOREACH VERTEX I VERTEX IN SETOFVERTICES DO MAKE (SETOF VERTEX = NEW); NVERTEXSETS:=LENGTH(SETOFVERTICES); \n COMMENT NOW CONSTRUCT THE SPANNING TREE: WHILE NVERTEXSETS > i DO BEGIN EDGETEMP := LOP(EDGES); V := \nDATUM(EDGETEMP) [I]; W := DATUM(EDGETEMP) [2]; Ec := DATUM(EDGETEMP) [3]; IF DISJOINTUNION(V,W) THEN \nBEGIN COSTS := COSTS + DATUM(Ec); PUT EDGETEMP IN TREESET; NVERTEXSETS:=NVERTEXSETS-I; END; END; COMMENT \nPRINT OUT THE SET OF EDGES OF THE MINIMAL SPANNING TREE; OUTSTR(CRLF&#38; \"EDGES AND COSTS OF EDGES\"); \nFOREACH EDGETEMP SUCH THAT EDGETEMP IN TREESET DO BEGIN STRING ITEMVAR NODENAMEi, NODENAME2; V := DATUM(EDGETEMP) \n[i]; W := DATUM(EDGETEMP) [2]; Ec := DATUM(EDGETEMP) [3]; NODENAME1 := DATUM(V) [I]; NODENAME2 := DATUM(W) \n[I]; OUTSTR(CRLF&#38; DATUM(NODENAMEI)&#38;DATUM(NODENAME2) &#38; TAB &#38; CVS(DATUM(Ec))); END; OUTSTR(CRLF&#38; \n\"TOTAL COST OF SPANNING TREE =\" &#38; CVS(COSTS)); END \"SPNTRE\" 66 REFERENCES [BAUMGART72] B. Baumgart. \nMicro Planner Alternate Reference Manual. Stanford Artificial Intelligence Laboratory. Operating Note \n67, April 1972. [EARLEY71~] J. Earley. Comments on SETL (Symmetric Use of Relations). SETL Newsletter \n52. Courant Institute NYU. Sept. 1971. [EARLEY71b] J. Earley. Toward an Understanding of Data Structures. \nCACM, Vol. 14, lO, Oct. 1971. [EARLEY73] J. Earley. An Overview of the VERS2 Project. Electronic Research \nLaboratory, College of Engineering memorandum ERL-M416, Dec. 1973, University of California at Berkeley. \n[EARLEY74] J. Earley. High Level Iterators and a Method of Automatically Designing Data Structure Representation. \nElectronic Research Laboratory, College of Engineering memorandum ERL-M425, Feb. 1974, University of \nCalifornia at Berkeley. [FELDMAN69] J. Feldman and P. Rovner. An Algol-Based Associative Language. CACM, \nVol. 12, no. 8, August 1969. [LOW74] J. Low. Automatic Coding: Choice of Data Structures. Technical Report \nNo. I, Computer Science Department, University of Rochester, Rochester, N. Y. [MCDERMOTT72] D. McDermott \nand G. Sussman. The Conniver Reference Manual. AI Memo No. 259, M.I.T., May 1972. [MORRIS73] J. Morris. \nA Comparion of MADCAP and SETL. University of California, Los Alamos Scientific Laboratory, 1973. [ROVNER76] \nP. Rovner. Automatic Selection of Associative Data Structures. Ph.D. thesis, Department of Mathematics, \nHarvard University (in preparation). [SCHWARTZ75a] J. Schwartz. Automatic Data Structure Choice in a \nLanguage of Very High Level. Second Symposium on Principles of Programming Languages. Palo Alto, California, \nJan. 1975. [SCHWARTZ75b] J. Schwartz. Optimization of Very High Level [anguages--I. Value Transmission \nand its Corollaries. In Computer Languages, Vol. l, pp. 161-194, Pergamon Press, 1975. [SUSSMAN70] G. \nSussman, T. Winograd, and E. Charniak. MICRO-PLANNER Reference Manual. AI Memo 203. Project MAC, M.I.T., \nJuly 1970. [SUSSMAN72] G. Sussman. Why Conniving is Better than Planning. AI Memo 255. M.I.T. Artificial \nIntelligence Lab, Feb. 1972. [VANLEHN73] K. VanLehn. SAIL User Manual. Stanford Computer Science Technical \nReport STAN-CS73-373, July 1973.\n\t\t\t", "proc_id": "800168", "abstract": "<p>We are all aware of the development of increasingly sophisticated, elaborate, and expensive computer programs, particularly in the fields of artificial intelligence, data base management, and intelligent systems. The need for techniques to deal with such complexity has renewed interest in programming language research. Recent work on structured programming, intelligent compilers, automatic program generation and verification, and high-level optimization has resulted.</p> <p>A pattern of approach similar to that of earlier research on programming languages is emerging. The work divides naturally into two parts: the search for good linguistic tools for expressing algorithms and data, and the development of practical methods for translating these to working computer programs. Our emphasis in this paper is in the latter.</p>", "authors": [{"name": "James Low", "author_profile_id": "81100143334", "affiliation": "", "person_id": "PP39029569", "email_address": "", "orcid_id": ""}, {"name": "Paul Rovner", "author_profile_id": "81100413965", "affiliation": "", "person_id": "P342027", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/800168.811540", "year": "1976", "article_id": "811540", "conference": "POPL", "title": "Techniques for the automatic selection of data structures", "url": "http://dl.acm.org/citation.cfm?id=811540"}