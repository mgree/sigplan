{"article_publication_date": "01-01-1976", "fulltext": "\n INDUCTION VARIABLES IN VERY HIGH LEVEL LANGUAGES* Amelia C. Fong and Jeffrey D. Ullma~ Princeton University \nPrinceton, N. J. 08540 Abstract We explore the notion of an induction variable in the context of ~ \nset- theoretic programming languge. An aoorooriate definition, we believe, in- volves both the necessity \nthat changes in the variable around a loop be easily computable and that they be small. We attempt to \njustify these re- quirements and show why they are independent assumotions. Next the oues- tion of what \noperators on sets play the role of +, ~ and * for arithmetic languages is explored, and several theorems \nallowing us recursivelv to detect induction variables in a loop are given. It is shown that most of the \nusual set operations do fit nicely into the theory and help form induc- tion variables. The reason most \nvariables fail to be induction variables concerns the structure of control flow, more than it does the \noperators ap- plied. i. Background \"Reduction in strength,\" that is, the replacement of multiplication \nby addition in a iooo, and its attendant detection and elimination of induction variables (those whose \nva'lue assumes-an-ari~m~r~--~rogre s- sion at a ooint) forms a key optimization for arithmetic languages \nlike FORTRAN [1-4]. Recently, there has been consider- able interest in algorithms for performing this \nkind of optimization [5-7]. However, in the FORTRAN environment, there is never more than a constant \nfactor speedup avail'- able by these methods. On the other hand, recent proposals such as [8-9] have \ndealt with reduction in strength applied to set-theoretic languages. In this context, reduction in strength \nbecomes a method for altering al- gorithms to improve their asymptotic run- ning time, and order of magnitude \nimprove- ment is possible. Earley [8] proposes \"iterator inversion.\" which is a powerful techniaue for \nimproving alqorithms au- tomatically. Unfortunately, as [8] ad- mits, it is not clear how to tell in \nad- vance whether a transformation is helping or hurting the running time. Our answer to that problem \nis that a set of permissible transformations must be built up \"from the bottom,\" starting with a few \nobviously safe transformations and developing additional safe transformations recursively. Schwartz [9] \nhas a similar idea, based on the notion of \"continuous\" functions, where an expression e(xl, x~,..., \nx ) is said to be continuous in x~ zif sma~l changes in (~-esumably set-vAlued) variable x~ produces \na change in e which can be easily calculated from the old value of e and the old and new values of x.. \nWe propose a related idea, but one which we beiieve is more aeneral in its treatment of Boolean valued \nopera- tors on sets, such as the relation of set inclusion, and in its extension from ex- pressions to \nproarams. 2. The Model We assume a set-oriented language such as SETL [10]. The operations which we \nassume can be done in unit time are: (i) arithmetic on integers (2) insertion of an element into a set \n * Work partially supported by NSF grant DCR-74-15255. I04 (3) deletion of an element from a set  (4) \nselection of some member from a set  (5) testing whether an atom is in a set  These assumptions are \nvalid, at least in an expected time sense, if one uses a hash table representation for sets, such as \nin [10] , with elements which are sets represented by pointers to their values. We take it as a corollary \nto (4) that a set may be tested for emotiness in unit time. It is assumed further that we are presented \na program as a flow graph, with basic blocks consisting of three-address statements, e. g., A = B U C, \nbut not A = B U C ~ D, which would appear as: T = C ~ D A = B U T 3. Goals We are interested primarily \nin developing a theory of induction variables and reductions in strength for set- theoretic languages \nthat is analogous to the one for FORTRAN-like ones. However, in the environment of a set-theoretic language, \nwhere large amounts of time are already given up to system overhead, it does not make sense to concern \nourselves solely with constant factor speedups, as one does for FORTRAN. We orient our de- finitions \nso that induction variables are those for which an order of magnitude im- provement in the running time \nof the pro- gram is possible by properly evaluating its induction variables. 4. Induction Variables \n ..................... The canonical situation for a FORTRAN level induction variable is a loop in which \nstatements like: I=I+l J=2*I appear. If I is not changed elsewhere in the loon, it is clearly an induction \nvari- able. Moreover, J is an induction vari- able, at least at the point immediately after J = 2 * \nI. We can arrange to. main- tain the value of J by additions and sub- tractions only, if we create a \ntemporary T whose value is always twice that of I. Then follow I = I + 1 by T = T + 2, and where I is \ninitialized outside the loop, initialize T to twice I. Replace J = 2 * I by .~ = T, yielding the sequence: \n I=I+l T=T+2 J=T In many cases we can dispense with I and/or identify J with T, adding to our savings. \nIn any event, we have eliminated the \"expensive\" multiplication at the cost of several copies and additions-perhaps \na worthwhile change. Now let's repeat the above in the set-theoretic context. It is generally recognized \nthat the basic role played by statements of the form I = I + 1 and I = I -1 in the arithmetic world \nbelongs to S = S U {x} and S = S -{x} in the set world (see [8,9]). These statements are just insertions \nand deletions of elements, operations which we have taken as primi- tive. We might see in a loop the \npair of statements: A = A U {x} C = A U B where B is presumed constant within the loop for simplicity \nin our present infor- mal discussion. It is natural to suppose that we could create set T, whose value \nwill always be that of A U B. Then we could follow A = A U {x} bv T = T U {x} and initialize T to A U \nB outside the loop. If we replace C = A U B by C = T we have: A = A U {x} T = T U {x} C = T Have we \nsaved significantly here? The answer is that probably we haven't, since the operation o~ copying T and \nassigning the value to C takes the same order of time as computing A U B. Of course it is possible that \non ex- amination of the entire loop we would find that C and T could be identified, thus re- placing \nthe union of arbitrary sets A and B by the adjoining of one element x. This would definitely be an order \nof magnitude savings. However, it is possible that the value of C is used in the loop in a way that makes \nits identification with T im- possible. In that case, we propose the following. Definition: A loop is \na set of blocks with a heater which dominates all other blocks in the loop, i. e., access to the loop \nis via the header only. Definition: Define ~(A, p) , for identifier ~+ and point p to be the pair of \nsets (A, D) and ~-(A, p) , where 21+(A, p) is  105 the set of elements added to A and not re- moved \nfrom A since the last time control passed point p, and Z~-(A, p) is the set of elements removed from \nA and not added. That is, Z~+(A, D) and ZI-(A, D) are new set valued identifiers whose value it is \nDos- sible to maintain. Every time control passes point p, we set z~(A, n) to (~, ~) and alter ~(A, \nD) as the \"current\" value of A changes and as long as control does not again reach p. Definition: Call \nA an induction variable of loop L at point p if there is a con- stant upper bound on the work necessary \nto maintain the value of ~k(A, p) between any two consecutive times that control passes point p, staying \nwithin loop L. Returning to our informal example of the statements: A = A U {x} C = A U B we may let \nD be the point immediately fol- lowing C = A U B. If the only assignment to A in the loop is A = A \nU {x}, then surely ~(A, p) can be maintained in con- stant work by writing the Piece of program as: \n if x not in A --then Z~+(A, D) = ~+(A, p) U {x} A = A O {x} C = A U B  A+(A, p) = /~-(A, o) = \nIf B is a constant within the loop, we can use ~X(A , p) to simulate the assign- ment C : A U B. Technically, \nwhat happens is this. We observe that since B presum- ably does not change, the change in the expression \nA U B from point p is almost the same as ~(A, p). In particular, A+(AUB, O) = A+(A, P) -B and ~.~.~(AUB, \nD) = ~X-(A, p) -B. If both ~i (A, D) and ~-(A, D) are bounded, as they are in this example, then ~X+(AUB, \np) is easy to com- pute. In general, if A and B vary in the loop, we can compute ~X(AUB, p) from /~(A, \np) and A(B, p) , provided both are small. Now we assume that C is only assigned at C = A U B within \nthe loop. Therefore, ~(C, p) = ~(AUB, p) , and we can replace the above program by: 106 if x not in \nA then --A+(A, p) = ,K~(-~, p) u {x} if x not in B then --~+(AUB, p) =~\u00f7(AUB, p) U {x} A = A U {~} \nC = C U ~ (AUB, p) -~(AUB, p) Z~ +(A, p) = ~(A, p) = 96 ~k (AUB, p) = ~k- (AUB, p) = ~6 Note that \nthe ~-'s can be ignored here but were included for form. Also, depending on what goes on elsewhere in \nthe loop, we may drop consideration of ~(A, D) or even of A itself. We see that in the above simple \ncase we have been able to replace a union of arbitrary sets A and B by unions and differences of sets \nthat remain small, in fact they have at most one element. Thus an asymptotic order of magnitude savings \nhas been achieved. We would now like to formalize furth- er the two important factors in this type of \ncode improvement, (i) the ability to efficiently maintain f~(e, p) for expres- sions e and (2) the boundedness \nof these sets. Definition: We use ~(e, p), for expression e and point p, to stand for the pair ~+(e, \np) and ~(e, D). Zi+(e, p) represents the set of elements added to the set denoted by e and not removed \nfrom that set, since the last time control passed point p. ~-(e, p) represents the set of elements removed \nfrom and not added to that set since control last passed p. Note that this definition coincides with \nthe earlier definition of ~ in the case e is a single identifier. We say e is an ....... e_xpression \n induction of loop L at point p whenever we can main- ~in~(e, p) with a bounded amount of work between \nsuccessive times through point p, as long as control does not leave L. Definition: Let us say an expression \nis of limited perturbation at point D in loop L i~, ~--and ~-(e, ~ are of bounded size as lonq as control \nstays within loop L. It is important to note that the no- tions of \"induction expression\" and \"ex- \npression of limited perturbation\" are not the same, nor does one imply the other. For example, ~k+(e, \np) miaht be known to be either ~ or {a}, but we have to solve the halting problem for Turing machines \nto tell which. Thus an expression could be of limited perturbation yet not be an in- duction expression. \nConversely, consider the situation of Fig. i, where A could be q I,A = B'U C] r lA D U E I P Figure \ni.  assigned either B U C or D U E before con- trol reaches point p. Then if B U C and D U E are induction \nexpressions, at points q and r, respectively, we shall see that A is an induction variable (hence an \ninduc- tion expression) at point p. yet B U C and D U E can differ by arbitrary amounts, and we might \ntravel a path such as q...p...r...p, so ZI(A, p) is surely not of limited perturbation. 5. Building \nInduction Variables and  We shall now develop the mechanism whereby induction variables can be detect- \ned in a straightfoward manner and reduc- tion in strength performed on them where possible, retaining \nthe assurance that what changes to the proqram are made will actually improve things. The theorems presented \nhere encompass most of the stan- dard set oberators. The implication is that the reason reduction in \nstrength can- not be performed in many cases has to do with the structure of control flow in the program \nrather than the properties of the operators used in calculation. The first theorems enable us to con- \n struct new induction variables and expres- sions from old ones. These theorems will all be stated in \na simple form that ig- nores the possibility that two or more variables could be mutually dependent in- \nduction variables, e. g., in a loop con- taining assignments A = B U {x} and B = A U {y}, both A and \nB might be induction variables. Once the principles are under- stood, this type of extension is easy. \n Definition: Call an assignment incidental if it is of the form A = A U {x~ or A = A -{x}. Theorem \ni: If in iOOD L, identifier A has o61y-Tnc~dental assignments, then it is an induction variable and is \nof limited per- turbation at all points in L where there is a bound on the number of incidental as- signments \nto A encountered going from p to p in L. Proof: The size of Z~+(A, D) and /i-(A, p) changes by at most \none and can be updated by a bounded amount of work each time an incidental assignment of A is encountered. \nSince there is only a bounded number of incidental assignments of A ~oing from P to P within L, the size \nof ~T(A, p) and L~(A, p) , and the total work involved in maintaining ~(A, p) from p to p are also bounded. \nHence A is an induction variable and is of limited perturbation at p. The example in Figure 2 shows \na si- tuation where A is not an induction vari- able and is not of limited perturbation at a point p \nin loop L even though all as- signments to A in L are incidental. Theorem 2: If in lOOn L, A and B are \nin- duction variables and of limited perturba- tion at point p, then A U {x}, A -{x}, A U B, A I--I \nB and A -B (in general, any  binary Boolean operation on A and B) are \"induction expressions and of \nlimited oer- turbation at p. Proof: We shall show that if e is one of the expressions A U ix}, A -{x}, \nA U B, A I-I B, A -B, then ~r(e, p) and ~-(e, p) are bounded in size and can be obtained from ~(A, p) \nand /i(B, p) using a bounded amount of work. Hence e is an induction expression and is of limited perturbation \n at p.  First let e be the expression A U {x}. Then ~(e, p) can be calculated from ~(A, p) by the following \nprogram: / . o' Figure 2. 107 ~+(e, p) = ~x+(A, D) fi-(e, p) = Zx-(A, p) if x in ~-(A, D) -- then \nZ~-(e, P) = ZX-[e, p) -{x} %ise if x n0t in ~i~(e, D) teen Z~ +(e, p) : Z~ +(e, D) u {x} Since ~+(A, \no) and ~-(A, p) are  bounded in size and can be maintained us-inq a bounded amount of work, ~+(e, p) \nand ~-(e, p) are also bounded in size and can be obtained using a bounded amount of work. Similarly, \nlet e be A -{x}. ~i(e, p) can be calculated as follows: ~+(e, p) = ~x+(A, p) ~i-(e, D) ~ ~-(A, p) if \nx in Z~ (e, D) = z~i + --then ~i+(e, p) (e, p) {x} else if x not in ~i-(e, p) ...... t~en ~i-(e, P) = \n~i-(e, D) U {x} Let e be A U B. Then z~+(e, p) = (A, D) U ~+(B, D) and ~-(e, D) = (A, p) -v(b)] U ~-(S, \np) -v(A)], where v(A) and v(B) denote the current value of A and B respectively.  Note that both ~X-(A, \np) -v(B) and ~X+(B, p) -v(A) may be obtained in time proportional to the size of ~-(A, p) and ~+(s, p). \nFor example, let C = ~-(A, p) -v(B). C can be obtained by the following piece of code: c=~ for x in \n~-(A, p) do if x not in v(B) then C = C U {x} Since we have assumed that membership testing and insertion \ncan be performed in unit time, C can be obtained in time pro- portional to the size of ~-(A, p).  + \nLet e be A J--J B. ~hen ~+(e D) -[A (A, p) J-J v(B)] U [A' (B, o) J-[ V(A)? and ~-(e, p) = /k (A, p) \nU fi (B, p). Let e be A -B. Then z~i+(e, p) = [~X+(A' P)e, -v(B)]\" U [z~(Bi ! J-J viA)] and ZX- ( P) \n= ~-(A, ~) I-I (B, p) U  ES(A, -v(B) ] U EA IB, I viA) Before goin 9 on to Theorem 3, we need some definitions \nof operations on the Zi's. Definition: Let A,B,C,D be sets. De- fine (A,B) [+] (C,D) to be the pair \n ((A-D) U (C-B), (D-A) U (B-C)) Definition: Let A, B, E, F be sets. Define (E,F) [-] (A,B) to be the \npair ((E U B) -(F U A) , (F U A) -(E U B))  Lemma i: Let D, a, r De points in loop L. Suppose control \npasses from p to g through path ii, then from g to r through path 12. Let ~i I be the pair ~(A, p) at \na after con- trol ~asses from p to q through path ii. Let ZX2 be the pair IX(A, a) at r after con- trol \npasses from q to r through path 12. Let ZX-be ZX(A, P) at r after control passesSfrom D to r through \nii followed by 12. Then if we write each ~; , i = 1,2,3 as the ordered pair ~, ~) , \u00a3hen  (1) % :A \n1 [+]% (2) ~i 2 = ~i 3 [-] ~i  Proof: The proof of (i) is straightfor- ward-and is omitted here. TO \nDrove (2) , let ~I ' ~o, ~ be (A, B), (C, .D) and (E, F) resp@cti~elv. By (i) E = (A-D) U (C-B) F = \n(B-C) U (D-A) We claim that C = (E U B) -(F U A) , i.e., C = [(A-D) U (C-B) U B] -[(B-C) U (D-A) U A] \n= [(A-D) U C U BI -[(B-C) U D U A]. Let T1 = (A-D) U C U B and let T2 = (B-C) U D U A. We shall show \nthat C is a subset of T1 -T2, and T2 -T1 is a subset of C. For all x in C, x is in (A-D) U C U B, i.e., \nx is in Ti. Obviously x is not in (B-C). Also x is not in D because C and D are disjoint by definition \nof ~. Again x is not in A because A End C are disjoint due to the fact that ii and 12 are consecutive \npaths. Hence x is not in (B-C) U D U A, i.e. not in T2. Therefore x is in T1 -T2. i.e. C is a subset \nof T1 -T2. TO Drove that T1 -T2 is a subset of C, suppose the contrary, i.e. there exists x in T1 -T2 \nwhich is not in C. Since x is in T1 = (A-D) U C U B, and not in C, x must be in either (A-D)-C or in \n(B-C). In both cases x is in (B-C) U D U A, contrad- icting the assumption that x is not in T2. Hence \nT1 -T2 is a subset of C. Therefore  C = T1 -T2. That proved in D = (F U A) -(E similar fashion. U B) \ncan be Theorem 3: Suppose there is a unique as- signment A = e which is always the last non-incidental \nassignment to A before con- trol reaches p, and that there is a bound- ed number of incidental assignments \nto A going from assignment A = e to Point D.  108 If e is an induction expression and of limited perturbation \nat the point of as- signment A = e, then A is an induction variable and of limited perturbation at p. \n Proof: Let q be the point of assignment A = e . We want to show that ~(A, p) is of bounded size and \ncan be obtained using a bounded amount of work. Consider the path as control passes  from p the i-th \ntime to p the i+l-st time. Since q is always the last non-incidental assignment before control reaches \np, we can consider the following three paths: ii, followed by 12, followed by 13 where i. is the path \nfollowed as control passes f~om g to p the ith time. 1 o is the path followed as control passes from \np the i-th time to a. i~ is the math followed as control masses f~om q to p the i+l-st time. Let ~ \ndenote the value of ~(A, q)  after control masses from a to a throuqh math 11 followed by i~. ~ is \nbounded in Size ~nd can be maintained using bounded amount of work because A is an induction variable \nand is of limited perturbation at g. Let ~\" denote the value of Z~(A, q) at  p ~fter control passes \nfrom q to p through path 11 . Z~\" is bounded in size and can be maintained using a bounded amount of \nwork because there are only a bounded number of incidental assignments between q and p. Hence b v \n(2) of Lemma i, ~(A, P) at a after control masses from p to a through path 1 2 is given by Z~ [-] A \n Let ~\" denote the value of A(A, q) at p a\u00a3ter control passes from g to p through path i~. By (i) of \nlemma i, Z~(A, p) when controI passes from p to p throuqh path 1 2 followed by 13 is given by Z~ [-] \nA\" [+] Z~\" which is bounded in size anu can be ob- tained using a bounded amount of work. Theorem 3 \nillustrates a situation in which A can not be identified with the ex- pression e in the loop L, and shows \nhow it can be handled without using a temporary the size of A or copying between A and the temmorarv. \n We can extend part of Theorem 3 to the common case where A has a value outside lo0p L, and the first \ntime throuqh point p after entering L, the external assignment to A is the most recent non-incidental \nas- signment. In fact, a far stronger result is possible, as far as induction variables are concerned. \n Theorem 4: Suppose at point p in loop L there are k possible assignments to A, say A = e , A = e^,. \n., A = e. which could be z .\" K  the ~ast non-znczdental assignment to A, and suppose all e i for \nwhich A = e i is ac- tually in loop L are induction expressions and of limited perturbation at the point \nof assignment. Let qi be the point of as- signment A = e i. suppose there is a bounded number of incidental \nassignments of A going from qi to p. Suppose further that there is a bound on the number of as- signments \nto A encountered going from p to p in L. Then A is an induction variable of L at p. Proof: Intuitively, \nthe value of A at p switches among k expressions, each of which is an induction expression. If k copies \nof A are kept, where the i-th copy has the value of A after its most recent assignment to e i, Z~(A, \np) may be represented by a switch (an integer) and the list ~(e., p) ..... ~(ek, p) The switch is set \nto i to indicate that the i-th copy of A is currently applicable, i.e. any reference to A should be \nmade to the i-th copy. ~f the last non-incidental assignment to A before reaching p is A = e , then \n~(e~, p) = ~(e~ o) , if there is n~ assignments to A b~fween g and p. If there are incidental assignments \nto A between a and p, ~(ei, p) may be obtained as in THeorem 3. ~(e are reset only if theila~ n~in~nt~ \nassignment to A before reachinq p is A = e . The work to maintain the switch and a~l the Z~'s is bounded, \nsince each of the ~'s is maintained in bounded time, and the switch is changed a bounded number of times \nby the hypothesis of the theorem. Note that we are missing from Theorem 4 a statement about A being \nof limited perturbation of L at p. The reason, obvi- ously, is that one cannot always conclude such a \nstatement, and Fig. 1 provides the canonical example why not. Thus, while one can oscillate between Theorems \n2 and 3 to find more and more induction variables and expressions within a loop, one cannot do so between \nTheorems 2 and 4. We can, however, extend the idea of the switch to a generalization of Theorem 4. Theorem \n5: Consider a point p in loop L, and suppose that the finite collection e ,..., comprises all the formulas \nfor the val~ of variable A at p, in terms of the values of variables the previous time through point \np. If each of the e \"s are induction expressions and of limited per- turbation at p, then A is an induction \nvariable at p. 109 Proof: There are only a finite number of different sequences of assignments which \ncan affect the value of A as we travel from p to p within L, or else the set of expressions for A would \nbe infinite. Represent ~(A, p) by a switch and the set of ~(e ,p}-s for all 1 < i < k. k \"copieR\" of \nA have to be kept so ~hat each ~(e~, o) represents the change with respect to the i-th copy of A. 6. \nBoolean Valued Expressions  While we have stated our theory of induction variables in great ~enerality, \nwhen it comes to specific operators that held form induction exoressions we have only mentioned union, \nintersection and similar operations. In fact, the theory does extend nicely to the usual set theoretic \nrelations, such as inclusion. When dealing with an expression like A c B, whose value is Boolean, the \n~ nota- tion and the notion of limited perturba- tion are meaningless, and in fact the ac- tual value \nof the expression is no harder to maintain than it is to increment. Thus we define a Boolean valued expression \nto be an induction expression of L at p if its value (r-at~er than 1Ts increment) can be maintained \nwith a constant amount of work from p to p within L. Theorem 6: If A and B are induction vari- a--bfes~- \nthen A c B and A = B (as well as a variety of similar relations) are induc- tion expressions. Proof: \nConsider A c B. By Theorem 2, A - B--~s an induction exoression and of limit- ed perturbation. The value \nof A c B can be maintained bv computing A -B and test- ing it for emptiness when it changes, a task we \nassume takes unit time per change. In the actual implementation, however, only the cardinality of A -B, \nwhose value is 0 if and only if A c B, need be main- tained. Theorem 7: Boolean operations on Boolean \n valued induction variables yield induction expressions. 7. Applications to Iterators  We would like \nto extend the notion of induction variable to interesting itera- tors as discussed in [S] . Let us consider \nthe set former {x ~ A I S(x)} to be specific. Now in the intermediate code we use, this iterator is actually \na loop of its own, in which x runs through every element of A. Since x is not an induction variable here, \nthe exoression {x C A I S(x)} cannot be an induction expression for this loop except under the most \ntrivi- al of circumstances. Thus, the set former cannot be an induction expression in a loop outside \nits own internal loop. There is, on the other hand, a fairly broad condition under which we can prove \nan order of magnitude improvement in the calculation of the set former is possible. Theorem 8: Let L \nbe a loop containing set former {x C A l S(x)} at point p. Suppose A and @(x) for all x which could ever \nbe members of A are all induction expressions and of limited oerturbation at p in L. Then the value of \n{x ~ A I S(x)} can be maintained with work proportional to IAl + costS(x) , where by cost~(x) we mean \nthe work necessary to compute ~(x) for any x that may be in A. (Note that the straightforward evaluation \nof the set former requires IAl costS(x) work.) Proof: Between any two consecutive execu- tions of {x \n~ A I @(x)}, the number of x in A such that the value of ~(x) changes is bounded by IAI. Since each S(x) \nis an induction expression and is of limited perturbation at p, the total work involved in maintaining \nthem is bounded by IAI. Between any two consecutive executions of the set former, the number of new elements \nadded to or deleted from A is also bounded because A is an induction variable and is of limited perturbation \nat p. The work involved in the addition and deletion is proportional to costS(x). Hence the total work \nnecessary is proportional to IAI + costS(x). Theorem 9: Let L be a loop containing the predicate P(A) \nat p where P(A) is Vx G A : ~(x} or ~x C A : @(X). Suppose A and ~(x), for all x which could ever be \nmembers of A. are all induction expres- sions and are of limited perturbation at O in L. Then the value \nof P(A) at p can be maintained with work proportional to IAI + cost@(x), where costS(x) is the work necessary \nto compute @(x) for any x that may be in A. (The straiqhtforward evalua- tion of P(A) requires IAI costA(x) \nin the worst case.) Proof: Let P(A) = Vx ~ A : @(x). The value of P(A) at p can be obtained by com- \nputing t, the cardinality of the set {x C A l not @(X)}, which can be maintained with work proportional \nto IAI + cost@(x) , by Theorems 7 and 8. P(A) is true iff t = 0. Again in actual implementation , onlv \nt rather the set itself is maintained. Similarly, if P(A) = ~x G A : @(x), its value is true iff the \nset {x ~ A I ~(x)} is nonempty. In the actual imolementation, only r those x in A such that @(x) has \nactually changed should be updated. If the size of. II0 this subset of A is considerably smaller than \nIAI and that the mapping to obtain this subsets can be precomputed outside the loop, it may be desirable \nto have this mapping available. In some cases, this mapping always maps to a subset of A of constant \nsize, then the total work in- volved in maintaining the set ~former is proportional to cost~(x). We shall \nillus- trate this with examples. (i) Consider P(X) = Vy e B : f(y) = X. If B and f (consider a function \nas a set of ordered pairs with distinct first elements) are induction variables and are of limited perturbation \nat p, then the value of P(X) can be maintained by main- taining the value t = cardinality of the set \n{y C B I f(Y) # X} as follows: for y in ~+(B, p) do if f(y) # X the6 t t + 1 for y in Z~-(B, D) do if \nf(y) ~ X th~ t = t -1 A change of f, say f(y) = z is rewritten as: if y is in B then begin ~f .f(y) \n= x then t = t -1 if z = x then t = t + 1 end f(y) = z  (2) Consider e : {x e A I x c B }. If B \nis an induction variable and is of limited perturbation at D, then ~(x) = x c B is an induction expression \nand is of limited perturbation at p for all x in A. If A is also an induction variable and is of limited \nperturbation at p, then e is an induction variable and is of lim- ited perturbation at p. Let t(x) = \nIx-Bl Let FIND(y) = {x ~ A I Y ~ x} ~(e, p) can be computed as follows:  ~+(e, p) : ~-(e, p) = for \ny in ~k-(A,_p) 42 ~-(e, D) = f~ (e, D) O {v}  for y in ~+(A, p) do begin compute t(y) = ly-Bl if t~y) \n= 0 then ---~+(e, p) ~e(e, p) U {y} end for y in Z~-(B, p) do Oegin for z in FIND(y) do begin if t(z) \n= 0 then ~-(e, p) = ~-(e, p) U {z} t(z) = t(z) + 1 end end  for y in Z~+(B, p) do begin for z in \nFIND(y) do begin t(z) = t(z) -1 if t~z) = 0 then -- ~ (e, p) i~e(e, D) O {z} end end If FIND is computed \noutside the loop, then the work necessary to obtain ~(e, p) is proportional to c + costa(x) , where c \nis an upper bound on the size of FIND(y) for any y in A, and ~{x} is Ix- BI. In the worst case c is IAI. \nHowev- er, if c is a constant independent of IAI, then ~(e, p) can be computed using work proportional \nto costS(x). 111 Bibliography [i] F. E. Allen, \"Program Optimization,\" in Annual Review in Automatic \nProgram--m~ng, Vol. 5, Perqam--on, 1969, pp. 239-307. [2] F. E. Allen and J. Cocke, \"A Catalo- gue of \nOptimizing transformations,\" in Design and Optimization of Compilers (R. Rustin, ed.), P~E~ce Hall, \n1972, pp. 1-30. [3] J. Cocke and J.T. Schwartz, Programming Languages and Their Compilers, Courant Institute, \nNew York, 1971. [4] A. V. Ano and J. D. Ullman, The Theory of Parsing, Translation and Compili_n_g~-Vol. \nII, Comp~ng, Prentlce Hall, 1973. [5] F. E. Allen, J. Cocke and K. Ken- nedy, \"Reduction of Operator \nStrength,\" TR 476-093-6, Dept. of Math. Sciences, Rice Univ., Houston, Aug., 1974. [6] J. Cocke and \nK. Kennedy, \"An Algo- rithm for Reduction of Operator Strength,\" TR 476-093-2, Dept. of Math. Sciences, \nRice Univ., Houston, March, 1974. [7] A. C. Fong, J. B. Kam and J. D. Ull- man, \"Application of Lattice \nAlgebra to LoOp Optimization,\" Proc. 2rid ACM Sym_~. on Principles o~ Programming Languages, J~n.7--i~7 \n..... [8] J. Earley, \"High Level Iterators and a Method of Automatically Designing Data Structure representation,\" \nERL-M416, Computer Science Division, Univ. of Cal- if., Berkeley, Feb., 1974. [9] J. T. Schwartz, \"On \nEarley's Method of \"Iterator Inversion',\" SETL Newsletter, No. 138, Courant Instit~e~ 1974. [10] J. \nT. Schwartz, On Programming, Vols. I and II. Courant Instftute, 1971 and 1973.  \n\t\t\t", "proc_id": "800168", "abstract": "<p>We explore the notion of an induction variable in the context of a set-theoretic programming langugage. An appropriate definition, we believe, involves both the necessity that changes in the variable around a loop be easily computable and that they be small. We attempt to justify these requirements and show why they are independent assumptions. Next the question of what operators on sets play the role of +, &minus; and * for arithmetic languages is explored, and several theorems allowing us recursively to detect induction variables in a loop are given. It is shown that most of the usual set operations do fit nicely into the theory and help form induction variables. The reason most variables fail to be induction variables concerns the structure of control flow, more than it does the operators applied.</p>", "authors": [{"name": "Amelia C. Fong", "author_profile_id": "81100096482", "affiliation": "", "person_id": "P328981", "email_address": "", "orcid_id": ""}, {"name": "Jeffrey D. Ullman", "author_profile_id": "81100314798", "affiliation": "", "person_id": "PP39037330", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/800168.811544", "year": "1976", "article_id": "811544", "conference": "POPL", "title": "Induction variables in very high level languages", "url": "http://dl.acm.org/citation.cfm?id=811544"}